Comparing Methods to Extract Technical Content for Technological Intelligence

Abstract--We are developing indicators for the emergence of science and technology (S&T) topics.  We are targeting various S&T information resources, including metadata (i.e., bibliographic information) and full text.  We explore alternative text analysis approaches ? principal components analysis (PCA) and topic modeling ? to extract technical topic information.  We analyze the topical content to pursue potential applications and innovation pathways.

In this presentation we compare alternative ways of consolidating messy sets of key terms [e.g., using Natural Language Processing (NLP) on abstracts and titles, together with various keyword sets].  Our process includes combinations of stopword removal, fuzzy term matching, association rules, and tf-idf weighting.  We compare PCA results to topic modeling results.  Our key test set consists of 4104 Web of Science records on Dye-Sensitized Solar Cells (DSSCs).  Results suggest good potential to enhance our technical intelligence payoffs from database searches on topics of interest.



I. INTRODUCTION   Tracking technologies or trying to determine their state has always been a challenging task.  The globalization of research has only added to the difficulty.  In the past, analysts primarily used expertise augmented by research to assess the state of technologies.  However, the increasing availability of electronic information about technology has opened up new possibilities to invert this process.  Since the mid-1980s researchers at the Technology Policy and Assessment Center at the Georgia Institute of Technology have been investigating the use of text mining to aid in the assessment of technologies [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].

This research is based on the premise that computer processable electronic records (bibliographic journal abstracts, full text journal articles, conference proceedings, etc.) can be effectively text mined and that the results of that mining can help determine the state of a technology.  The process that evolved at Georgia Tech over 20+ years of development uses the output of text mining to good effect, but, overall, the techniques employed in the ?Tech Mining? process still require a fair amount of analyst judgment and expertise in text mining [11].  The question today is can this Tech Mining process be further automated by incorporating some of the new advances in text analysis to reduce the requirement for analyst input in the process.  To this end, this paper looks at the output of the current Tech Mining techniques compared to the output of Topic Modeling, using Dye-Sensitized Solar Cells as an example.  The comparison  was carried out by two teams ? the PCA-based Tech Mining team at Georgia Tech and the Topic Modelers at UC Irvine.



II. BACKGROUND   Over the years many techniques have been used to model the research found in technology databases.  This is done to analyze the structure of technical domains and enable analysts to solve technical problems, project the direction a technology is taking, and understand their own place within the technical domain. Latent Semantic Analysis (LSA), Principal Components Analysis (PCA), Support Vector Machines (SVM), and Topic Modeling (TM) are a handful of methods that have been utilized in the past [12, 13]. The Georgia Tech process has used a variant of PCA to facilitate text analyses, usually focusing on technology topics, but also addressing Management of Technology (MOT) per se [3, 38].

However, the full process requires significant human interaction, iteration, and has scaling issues. Is there a more automated scalable approach? How do the outputs of varying approaches compare with our PCA-based results?

Since any type of text clustering is based on co- occurrence of words, whether some type of keyword or words contained in a document or abstract, it would seem that the actual clustering algorithm chosen will not bring about large differences in the actual clusters developed. This hypothesis is supported by numerous projects [14, 15, 16].  The basis of these similarities is the fact that the objective of all clustering is to minimize associations between clusters and maximize the relationships within clusters. Different algorithms simply have different starting points. This statement does not necessarily mean that the results are the same. The details of the chosen clustering algorithm are important to the end result and must be determined by the end goal. The difference, however, is primarily based on factors such as the following: whether the clusters are term clusters or document clusters; whether the clusters are distinct groups or whether certain items can be excluded from any cluster; whether the location of certain words or documents have a distinct location in the space or can have multiple locations; whether the clusters remain the same each time the algorithm is run or, as in the case of probabilistic methods, the clusters may change each time the algorithm is run.

The cluster research contains a plethora of clustering techniques and additions to well-known methods designed to improve the ability to find either documents or bits of information, as well as to provide a general landscape of the   2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies.

documents. These techniques fall into a number of categories.

Hierarchical methods group items in a treelike structure. The methods can start with small groups and aggregate those clusters into larger clusters or start with one or more larger clusters and break those into smaller ones. In contrast, non- hierarchical methods simply break the corpus into subsets [17]. Partitioning clustering divides the data into disjoint sets.

Density-based clustering groups neighboring objects into a cluster, based on density criteria. A cluster is defined by a given density threshold. Statistical clustering methods, such as factor analysis, use similarity measures to partition documents [18]. While factor analysis is a more linear statistical approach, there are other statistical approaches, such as the probabilistic approach offered in Vinkourov and Girolami [19]. Bayesian Clustering is another probabilistic approach which uses Bayesian probability theory to calculate the probability that a certain object belongs in a certain group [20].  Kohonen Self-Organizing Maps is an artificial intelligence approach based on unsupervised neural networks.

In general, each of these methods is based on frequency of term co-occurrence. One unique method is offered by Shah.

In this method, the semantic relationships among words in the document are captured. The Kohonen Self Organizing Map is used to cluster documents that have the most similar semantic maps [21].

In the context of text mining, clustering can be utilized in a number of different ways for a variety of purposes.

Clustering may also serve as the basis for other types of analysis, such as those presented by Watts, Courseault, and Kapplin [22]. In this paper, an algorithm based on combining various clustering techniques is used to find emerging technologies that accomplish a particular function in a corpus containing over 10,000 publication records. Clustering may be used to discover topic hierarchies giving structure to a corpus and allowing an individual to explore the corpus in a more organized fashion [23].  Merkl and Rauber use the Self Organizing Map as the basis for an approach designed to uncover associations among documents. Their approach is intended to make explicit the associations between clusters [24].

Clustering can also be reapplied to the original document set to improve information retrieval. Ding applies a probabilistic model for dimensionality reduction to a corpus as a means of conducting word sense disambiguation and thus permitting the filtering of information and improving information retrieval. Therefore, if the user types in the word ?capital,? articles related to a city vs. venture capital can be separated and the user can then focus their search on the type of capital that is their interest [25]. Similarly, Kaji et. al. [26] present a method for generating a thesaurus using term clustering as a means to traverse a domain-specific corpus.

The thesaurus is designed to cluster generic terms first. Then, allow the user to ?zoom-in? to a cluster and identify more specific terms in that cluster by analyzing the statistical correlation between terms [26].  Beil et al [27] also present a method for term-based text clustering with the intent of  offering a method that better handles very large corpuses and improves the retrieval process. However, this method includes the added effect of cluster descriptions based on the frequent terms in the cluster. A hierarchical and a non- hierarchical approach are presented [27].

Most clustering methods use document clustering as a way to maneuver through documents, especially as clustering is being promoted as a visualization method for document retrieval [28]. The increased number of internet sites have sparked a greater interest in this area [29].  Therefore, much of the most recent research in this area is based on web pages.

Broder et al [30] offer a method for determining the syntactic similarity of web documents for the purpose of filtering search results, updating web pages, and identifying copyright violations. Zamir and Etzioni [29] evaluate clustering algorithms used on web documents and offer an algorithm called Suffix Tree Clustering, which analyzes phrases shared by multiple documents.



III. THE DATA   The topic used for the comparison was Dye-Sensitized Solar Cells (DSSC).  We selected this topic because the area is a topic of active research for the team [31].  The data source selected for the comparison was Web of Science (expressly, Science Citation Index).  The range for the data was 1991 to 2011.  The search strategy consisted of the following:   #1: 4000 records TS= (((dye-sensiti*) or (dye* same sensiti*) or (pigment-sensiti*) or (pigment same sensiti*) or (dye* same sense)) same (((solar or Photovoltaic or photoelectr* or (photo-electr*)) same (cell or cells or batter* or pool*)) or photocell* or (solar-cell*))) Annotation: #1 search term is various expression of Dye sensitized solar cell (pigment sensitized solar cell is a kind of DSSC)  #2: 1204 records (#2 not #1: 32 records) TS=((DSSC or DSSCs) not ((diffuse cutaneous Systemic sclerosis) or (diffuse cutaneous SSc) or (diffuse SSc) or (distributed switch and stay combining) or (Distributed Static Series Compensator*) or (decoupled solid state controller*) or (Active Diffuse Scleroderma*) or (systemic sclerosis) or (diffuse scleroderma) or (Deep Space Station Controller) or (Data Storage Systems Center) or(decompressive stress strain curve) or (double-sideband- suppressed carrier) or (Flexible AC Transmission Systems) or (DSS induced chronic colitis) or (Dynamic Slow-start) or (dextran sulfate sodium) or (disease or patient* or QSRR))) Annotation: It is the papers which include 1) DSSC but not includes #1 and relate to Dye sensitized solar cell and exculde 2) noisy data.

#3: 330 recordes (#3 not (#1 or #2): 54 records) TS=((((dye- Photosensiti*) or (dye same Photosensiti*) or (pigment- Photosensiti*) or (pigment same Photosensiti*)) same ((solar or Photovoltaic or photoelectr* or (photo-electr*)) same (cell or cells or batter* or pool*))) not (melanocyte* or cancer)) Annotation: #3 search term is 1) various expression of Dye photo- sensitized solar cell, and use 2) (melanocyte* or cancer) to exclude noisy data.

#4: 188 records (#4 not (#1 or #2 or #3): 18 records)   2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies.

TS = (((((dye adj (sensiti* or photosensiti*)) and (conduct* or semiconduct*)) same electrode*) and electrolyte*) not (wastewater or waste-water or degradation)) Annotation: #4 search term searches DSSC papers according to1) the component of DSSC and use 2) (wastewater or waste-water or degradation) to exclude noisy data.

Total: 4104 records #1 or #2 or #3 or #4   The 4,104 Web of Science records were downloaded and  imported into VantagePoint text mining software [www.thevantagepoint.com].  The software extracted the abstract and title phrases using the Natural Language Processing module.  The resulting phrase list contained 64,480 phrases.  These phrases were cleaned using the general clean up module within VantagePoint to reduce the list to 56,800 phrases.  This set of 56,800 DSSC phrases formed the basis of the comparison between the PCA-based approach and the Topic Modeling approach.



IV. THE PCA-BASED APPROACH   The PCA approach applied here followed a number of discrete steps.  These are of interest for their potential inclusion in a semi-automated methodology of ?term clumping? [32].  The aim of such an approach is to expedite the reduction of large compilations of term phrases from a document set, such as these 56,800 phrases, to a more manageable, highly informative subset that could be analyzed to gain insight into topical patterns.  This is a work in progress.  Key steps included here can be summarized as follows: a. Field Selection:  These Web of Science (WOS) records  offer four promising topical sources: titles, abstracts, and two types of keywords (a set deriving from the authors that are not always available ? covering only 52% of these records; and a set constructed by WOS based on cited reference title terms ? for 94% of the present set).  In addition, we explored ?borrowing? keywords from another source (EI Compendex) and extracting that controlled vocabulary from the WOS records.  Use of such meta-data, however, accentuates certain terms to the disadvantage of the raw records.  So, for the present case, we favored letting the ?records speak? with less intrusion, so we report on the combination of title and abstract phrases extracted using the Natural Language Processing (NLP) algorithm of VantagePoint.

b. Basic Cleaning:  The 64480 Title + Abstract phrases were reduced to 56800 by use of VantagePoint?s general.fuz ?fuzzy matching? routine.  This consolidates terms with shared stems and other phrase variations expected to be highly related concepts.  The basic cleaned set forms the basis of comparison to Topic Modeling.

c. Further Cleaning: reducing the set to 51960:  In VantagePoint, we applied several thesauri (?.the? files) to further consolidate term variations and cull ?noise.?  These include both very general and quite topic-specific collections: ? stopwords.the ? a standard thesaurus provided with the  software that uses Regular Expression (RegEx) to batch some 280+ stemmed terms as ?stopwords? [e.g.- - the, you, and, are, single letters, and numbers]  ? common.the ? over 48,000 general scientific terms ? trash term remover.the ? compiled from scanning such  WOS phrase collections, including the DSSC records, to remove some 500 noise terms [e.g. -- copyright 2011, references, United States Abstract, 650 nm]  ? topic variations consolidator.the ? combines variations on a few prominent DSSC terms  ? DSSC data fuzzy matcher results.the ? a compilation of phrase variations that VantagePoint?s ?List Cleanup? routine suggested combining [e.g. ? various singular and plural variations; hyphenation variations; and similar phrases such as ?nanostructured TiO2 films? with ?nanostructured TiO2 thin films?]  d. Additional Cleaning:  Ran VantagePoint?s list cleanup routine, again, using a variation of a routine provided for general use ? ?general-85cutoff-95fuzzywordmatch- 1exact.fuz?  As the title hints, this was derived by varying parameters offered by the software to adjust fuzzy matching routines. This reduced the set to 47842 phrases.

e. Consolidation:  Ran a macro devised by Cherie Courseault Trumbach and Douglas Porter of IISC to consolidate noun phrases of differing numbers of terms.

As described by Trumbach and Payne [33], this concept- clumping algorithm first identifies a list of relevant noun phrases and then applies a rule-based algorithm for identifying synonymous terms based on shared words.  It is intended for use with technical periodical abstract sets.

Phrases with, first, 4 or more terms in common; then 3; then 2; are combined and named after the shortest phrase, or most prevalent phrase. In case of conflict, it associates a term with the most similar phrase.  This reduced the set to 43074 terms.

f. Pruning:  Human scanned the term list and added a few terms to trash remover.the; removed a few more general DSSC terms ? reducing just a little to 43060; then removed phrases appearing in only a single record ? now down to 10350 (after removing 2 cumulative trash terms).

So, this is clearly the critical reduction, albeit an extremely simple one to execute.

g. Parent-Child Consolidation: Ran a macro devised by Webb Myers of IISC originally to consolidate junior authors under a more senior collaborator -- Combineauthornetworks.vpm.  This reduces the set to 7179 terms.  To give a sense of term prevalence, here are some frequency benchmarks: 4941 terms associated with 3 or more records; 1086 with 10 or more; 348 with 25 or more; 194 with 40 or more; 49 with 102 or more.

h. PCA:   VantagePoint?s ?factor mapping? [and/or matrix] routine that applies Principal Components Analysis (PCA) was then run on selected groups of the 7179 terms.

2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies.

TABLE 1.  PCA FACTORS.  EACH ROW SHOWS INFORMATION ABOUT ONE TOPIC, INCLUDING: A SHORT MACHINE ASSIGNED LABEL FOR THE FACTOR; THE PERCENT OF VARIANCE EXPLAINED BY THE FACTOR; THE PHRASES THAT LOAD HIGHLY ON THE FACTOR.

PCA 10 Factors  Factors Percent Coverage Terms  Voc  1.54% mA cm; fill factor; Voc; open circuit voltage Voc; Jsc ; photocurrent density Jsc; eta; open circuit photovoltage Voc; current density Jsc ; ISC density functional theory DFT  1.37% density functional theory DFT; electronic structures conduction band  1.22% TiO; sensitizer; photocurrent; electron injection; conduction band electron lifetime  1.08% electron transport; electron lifetime; electron diffusion coefficient transmission electron microscopy  1.00%  electron microscopy; transmission electron microscopy; electron microscopy SEM; X ray diffraction; X ray diffraction XRD; X ray photoelectron spectroscopy XPS  electron donor  0.98% MW cm irradiance; electron acceptor; photophysical; electron donor XRD  0.94% XRD; SEM; TEM counter electrode  0.88% counter electrode; Pt ionic conductivity  0.86% electrolyte liquid; ionic conductivity; polymer electrolytes; polymer gel electrolyte open circuit voltage  0.83% mA cm; fill factor; open circuit voltage; overall conversion efficiency   Several runs, as follows -- PCA on the top 194 terms; cleaned terms a bit more to 7164; reran PCA on the top 204 terms (occurring in 37 or more records) ? got 15 factors, but the human analyst thought those could be consolidated better, so reran PCA requesting 10 factors; got 10 that look pretty coherent.  The terms listed below in Table 1 are from this PCA analysis.

To check robustness, another PCA on terms appearing in  >=25 records, removing several uninteresting terms (human judgment), was based on 319 terms.  Initial result of 18 factors included several that seemed to warrant consolidation; preferred a result with 11 factors that were pretty similar to those presented below (based on the 10-factor solution based on 194 terms).



V. THE TOPIC MODELING APPROACH   The topic model is a statistical model that learns topical structure in a collection of text documents [34, 35].  The topic model assumes that each document in the collection exhibits a small number of topics.  It simultaneously learns a set of topics to describe the entire collection, and the topics assigned to each document.  Formally, each topic is a probability distribution over terms, and is typically displayed by listing the ten to twenty most likely terms.

Topic models are learned in a fully automated fashion.

Like other unsupervised methods, such as Latent Semantic Analysis (LSA) or Principal Component Analysis (PCA), there is no need for an ontology, thesaurus or dictionary.

Instead the topic model works directly and only from the text data by observing patterns of terms that tend to co-appear in documents, such as candidate and election, or warming and climate.  The topic model works at a very granular level, assigning a topic label to every word in every document.

Topic tags on a per-document level are obtained by aggregating these word-level topic assignments.

The topic model is also known as Latent Dirichlet Allocation, and it evolved as a Bayesian approach for LSA/PCA [34,35].  The topic model is possibly more suited  to text data since it is a model of discrete counts rather than real-valued data.  Furthermore, topics from the topic model can be easier to understand since topics can be interpreted as probabilities, whereas PCA factors can have positive and negative values (required for orthonormality).  But the bigger difference is that the topic model uses ?T? topics to explain the entire corpus, whereas PCA computes the top-T factors that account for the most variance in the data (that is, there exists no different set of T factors that accounts for more variance).  So learning a topic model with twice as many topics will result in finer-grained topics, whereas computing twice as many factors in PCA will produce the same top-T factors.

Topic modeling has been used in a variety of application areas, ranging from information retrieval to research portfolio analysis.  One application of relevance to science and technology is the characterization of NIH-funded research, available online as the NIH Map Viewer [36].  The topic model is also highly scalable, and can be run on internet-size datasets.  One can efficiently and quickly learn topic models on millions of documents, making it possibly preferential over PCA for large-scale analyses [37].



VI. TOPIC MODELING PROCESS   Topic modeling uses a bag-of-words representation of a corpus, where word counts in each document are preserved, but word order is discarded.  The pre-processing of raw text data for topic modeling can be nearly identical to that for LSA/PCA [although in this work the PCA is modeling phrases (multi- or single-word), not unigrams ? i.e., single words].  One difference is that the topic model uses integer term counts, and there is no term frequency / inverse- document-frequency (TF-IDF) weighting of term frequencies that is often used in LSA/PCA.

The basic version of the topic model is parameterized by a single input parameter, T, the setting of the number of topics to learn.  One can use heuristics to set T, for example based on corpus size or experience.  Note that this differs from LSA/PCA where one can request the top-T factors to be   2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies.

computed (and the selection of larger T does not change factors already computed).  There are also nonparametric versions of the topic model that use the data to learn an appropriate number of topics to explain the data.

Our preprocessing of the 4104 DSSC abstracts followed a slightly different procedure to that used for the PCA analysis.

We used all the text from title and abstract, did some simple normalization (lowercasing and removal of punctuation), and limited stemming.  Here our focus was on unigrams, so we did no chunking or noun-phrase extraction, except for some limited replacement of frequent bigrams (such as thin film).

We removed a short list of standard stopwords (e.g. the, and), as well as some non-technical frequently occurring terms (e.g. journal, isi, elsevier, all, rights, reserved).  After tokenization, there were on average 100 terms per document.

Given this relatively small collection, topic models were learned with T=10, 15 and 20 topics, running for 400 iterations.  The setting of T=15 topics seemed to have a reasonable resolution, and we present that model here for illustration purposes (for more detailed analyses, one might be interested in more fine-grained topics learned using a higher setting for number of topics, T).



VII. RESULTS ON 4104 DSSC ABSTRACTS   Table 2 shows the T=15 topics learned on the collection of 4104 DSSC abstract.  Each row shows information for one topic.  For each topic there is a human assigned label (in the first column) and percentage (second column).  The percentage indicates the overall prevalence of a topic, i.e., the percentage of all ~400000 tokens in the corpus that are assigned to that topic.

We see from the table that the topic model learns a range of topics or facets about this collection of DSSC abstracts.

The topic model divides various aspects of this technology  from topics related to PERFORMANCE (terms like performance, efficiency, effect) to topics related to CIRCUIT INFO (terms like current, voltage).  It also captures the divide of TiO2-based technology from ZnO-based technology.  While not shown here, we can examine any one of the 4104 abstracts and show what topics are discussed.

Likewise, we can find relevant abstracts for a particular topic, for example, we could rank all abstracts on their relevance to the topic on ELECTROLYTE TYPES.  Since topics need to represent every word in every document we do see a range of topic types, covering different technologies, aspects of the technology (PERFORMANCE), and topics accounting for various terms that appear that are less related to technical aspects (e.g. the two PUBLICATION INFO topics).



VIII. COMPARISON   The results of the two analyses are both intriguing and very different.  The PCA factors appear highly focused to specific subtechnologies within DCCS?s.  The percentage variance is relatively low but not unexpected since the pre- processing tended to move the analysis away from the top tier terms and more toward the middle.  This is intentional.  An analysis of high frequency general terms from other DSSC work at Georgia Tech finds a good degree of structure within the field and can explain much of the variance.  This would indicate that the technology is organized and relatively well developed (key terms are agreed upon, the lexicon is established, definitional battles are minimal).  The interest therefore rests with the middle frequency terms.  The PCA analysis shows some variability at the middle tier indicating that DSSC technology is still undergoing significant transformation.  For example, variability in microscopy terms (XRD, SEM, TEM) indicates that best practices are still under development.  However, these same terms show some   TABLE 2.  TOPICS LEARNED BY THE TOPIC MODEL.  EACH ROW SHOWS INFORMATION ABOUT ONE TOPIC, INCLUDING: A SHORT  HUMAN-ASSIGNED LABEL; THE PERCENT OF WORDS IN THE CORPUS ASSIGNED TO THAT TOPIC; THE MOST LIKELY TERMS IN THE TOPIC, IN ORDER OF LIKELIHOOD.

Topics Short Label % Topic terms  DSSC GENERIC 9% dye sensitizer DSSC ruthenium group acid complexes efficient nanocrystalline_TiO2 organic_dye PERFORMANCE & EFFICIENCY 9% TiO2 layer dye recombination performance surface efficiency effect increase electrode DEVICE DESCRIPTION 9% device material cell photovoltaic dsc organic application efficiency low high semiconductor cost ELECTROLYTE TYPES 8% electrolyte polymer solid_state ionic_liquid iodide polymer_electrolyte poly gel_electrolyte ELECTRON TRANSPORT 8% electron recombination charge transport diffusion spectroscopy electron_transport kinetic TiO2 FILM 8% TiO2 film TiO2_film temperature electrode particle layer prepared thin_film deposition MOLECULAR CHARACTERISTICS 7% state dye TiO2 surface molecular band absorption electronic level excited density functional TiO2 NANOSTRUCTURES 6% TiO2 nanotube arraytitania nanoparticle mesoporous anatase light structure scattering CIRCUIT INFO 6% cell current voltage short_circuit open_circuit dye photocurrent factor density TiO2 solid_state PHOTOELECTROCHEMICAL 6% dye light film absorption sensitization electrode photoelectrochemical sensitized photon visible SPECTROSCOPY 5% surface ray spectroscopy electron microscopy properties characterized temperature scanning film ELECTRODE 5% counter_electrode carbon electrode dsc substrate layer resistance performance glass fto PUBLICATION INFO 5% doi chem phy_chem chem_soc mater sol phy mat commun lett sci adv energ nature ZnO NANOSTRUCTURES 5% ZnO film nanowire nanorod oxide growth zinc deposition array thin_film nanoparticle nanostructure PUBLICATION INFO 4% chemical physics society applied letter american electrode conversion chinese material energy     2012 Proceedings of PICMET '12: Technology Management for Emerging Technologies.

of the weakness of non-expert trained systems ? note that the system does not understand that TEM and Transmission Electron Microscopy are the same thing.  The key issue with the PCA process is that much of the understanding of the state of the technology comes from the analyst?s journey through the process.  Table 1 in isolation is not particularly illuminating when disconnected from the process used to create it.

The Topic Model results paint a very different picture.

The topics present a somewhat clearer view of the state for the technology with a much shorter route to the end.  Like the PCA process, the Topic Modeling process shows some variability of the technology at the second tier. However, it is interesting to note, that key to effective representation of the topics is the labeling ? a very human process.  In this experiment, we only computed topics using unigrams and some frequent bigrams, so the absence of n-grams can make human interpretation challenging.  However, it is possible to post-process the topics, and print out significant (high likelihood) phrases in each topic.



IX. CONCLUSIONS   At the end of the day, this comparison, although simple and not comprehensive, is still quite helpful.  The Topic Modeling process appears to have some utility to reduce the cycle time, the complexity, and analyst input required for a technology analysis.  It also presents some intriguing possibility for layering different techniques together.  The scalability is very attractive as its ability to work in a variety of languages.  However, Topic Modeling is not a silver bullet that can be used in isolation.  The human analysts in concert with other techniques will still be required to produce an effective assessment of a technology.

ACKNOWLEDGEMENTS   This work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20155.

Earlier work was supported by the Defense Advanced Research Projects Agency (DARPA), the U.S. Army Tank- automotive and Armaments Command, and the U.S. Army Aviation and Missile Command under Contract DAAH01-96- C-R169.

The U.S. government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.

Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, DARPA, or the U.S. Government.

