Characterization of Internal Learning Parameters in Artificial Neural  Networks

Abstract- As modern computers become even more powerful, scientists continue to be challenged to use machines effectively for tasks that are relatively simple for humans. Based on examples, together with some feedback from a ?teacher?, we learn easily to recognize the letter A or distinguish a cat from a bird. More experience allows us to refine our responses and improve our performance.

Although eventually, we may be able to describe rules by which we can make more decisions, these do not necessarily reflect the actual process we use.

Even without a teacher we can group similar patterns together. Yet another common human activity is trying to achieve a goal that involves maximizing a resource (time with one?s family, for example) while satisfying certain constraints (such as need to earn a living). Each of these types of problems illustrates tasks for which computer solutions may be sought.

Traditional, sequential, logic based digital computing excels in many areas, but has been less successful for other types of problems. The development of artificial neural networks began approximately 50 years ago, motivated by a desire to try both to understand the brain and to emulate some of its strengths. [1]  Keywords- teacher; computer solution; artificial neural networks

I. INTRODUCTION  Artificial Neural Network [ANN], also called as Natural Intelligent System, is an information processing system that has certain performance characteristics in common with biological neural networks [1]. It uses specialized hardware or sophisticated software, to develop solution for problem domains where conventional programming techniques are not appropriate. An ANN comprises a number of active 'neurons' connected together to form a network. The 'strengths' or 'weights' of these links between the neurons is where the functionality of the network resides.

A Neural Network's usefulness is in its ability to mimic - or model - the behavior of real-world phenomena like the weather, the environment, the economy, etc. Being able to model the behavior of something, a neural network is able subsequently to classify the different aspects of that behavior, recognize what is going on at the moment, diagnose whether this is correct or faulty, predict what it will do next, and if necessary respond to what it will do next.

Neural Networks helps in situations:  ? Where we can?t formulate an algorithmic solution [11] for example pattern recognition  ? Where we can?t get lots of examples of the behavior we require [11] for example rainfall prediction, patient data for diagnosis  ? Where current technologies have been inadequate to make a system viable, for example speech recognition, text recognition, target analysis [12]

II.ANN APPLICATIONS Figure 1. Structure of a simple artificial neural network  2009 IACSIT Spring Conference  DOI 10.1109/IACSIT-SC.2009.92   2009 International Association of Computer Science and Information Technology - Spring Conference  DOI 10.1109/IACSIT-SC.2009.92

II. APPLICATIONS  Applications include [2]:  ? Image Processing and computer vision, including image matching, preprocessing, segmentation and analysis, computer vision (e.g circuit board inspection), image compression, stereovision, and processing and understanding of time- varying images.

? Signal processing, including seismic signal analysis and morphology.

? Pattern Recognition, including feature extraction, radar signal classification and analysis, speech recognition and understanding, fingerprint identification, character (letter or number) recognition, and handwriting analysis.

? Medicine, including electrocadiographic signal analysis and understanding, diagnosis of various diseases and medical image processing.

? Military systems, including undersea mine detection, radar clutter classification, and tactical speaker recognition.

? Financial Systems, including stock market analysis, real estate appraisal, credit card authorization, and securities trading.

? Planning, control and search, including parallel implementation of constraint satisfaction problems (CSPs), control and robotics.

? Artificial Intelligence, including abductive systems and implementation of expert systems.

? Power Systems, including system state estimation, transient detection and classification, fault detection and recovery, load forecasting, and security assessment.

III. HOW ANNS WORK?

Instead of being programmed to model a real-world process explicitly, the neural network is trained to model it implicitly. Instead of modeling a real-world process in terms of logical and mathematical functions, the neural network models it as a vast array of 'inter-neural connection weights? [3].



IV. DATA STRUCTURE  The data structure required for this network comprises a one-dimensional array to hold the output signals from each layer plus a two- dimensional array to hold the weights of the connections between each pair of layers. To take advantage of the pointer arithmetic capabilities of programming languages like C, we can organize the data arrays slightly differently. i.e.

we can make all the layer outputs accessible via a single pointer array.



V. TRAINING PROCEDURE ? THE LEARNING PARAMETERS  The training procedure is a matter of adjusting the weights of the network until the observed output is as close a match as possible to the correct output provided in the training data file. Finding out by how much and in which direction each weight must be altered and then doing the alteration must be done as a number of separate computational steps.

There are a variety of learning parameters involved that affect the performance and speed of the Neural Network. These parameters include initial weight adjustment, learning rate, momentum, activation function, number of hidden layers and neurons in them (i.e. the layer size) etc. Appropriate adjustment of the learning parameters result in faster response generation and better results.

A. Learning rate  Learning rate (?) relates directly to the speed of the neural network. If ? is small, network will take longer to train, but will give better results as compared to the case when the value of ? is large [4].

B.  Momentum  In order to further speed up the learning, the momentum parameter is also considered along with the learning rate.

C. Number of Hidden Layers and Number of Nodes:  Number of hidden layers and the neurons (nodes) in these layers affect the performance of the network. A network with one hidden layer will usually produce better results than networks with more hidden layers.  Just occasionally two hidden layers will be needed but a network with one hidden layer should always be tried first [10].  Usually, the learning algorithms are sensitive to the initial weights of the connections between the nodes. An appropriate choice of initial weights leads to faster results of better quality.



VI. PROBLEM DESCRIPTION  Neural Networks could be considered as a basis for Intelligent Software. These are data- driven models. Data of insufficient quality or quantity can prevent the application development from success.

Although appropriate data preprocessing is a key to success in employing Neural Networks, the user is not expected to understand thoroughly their internal functioning. However, in order to optimize learning, speed and performance, it is at times necessary to adjust some or all of these parameters[13]. The default values and the choices of software do not always produce the best results. Therefore, it is important to adjust the learning parameter values in order to optimize network results. The parameters considered include learning rate and the number and size of hidden layers.

This research paper is aimed at exploring the effects of internal learning parameters on Artificial Neural Networks. The unprocessed data is used to train the Neural Network.

Comparison tables are generated and error rates are analyzed. The various learning parameters considered are learning rate, momentum, no. of hidden layers and no. of nodes in hidden layers.



VII. METHODOLOGY AND RESEARCH APPROACH  The research work will encompass the analysis and comparison of the effects of internal learning parameters like learning rate, momentum, number of hidden layers and number of nodes in hidden layers. The simulation tool intended to be used is ?EasyNN?.

The ANN will be trained over the raw data using ?EasyNN? as a tool for learning, and the results will be generated i.e number of cycles, minimum, maximum and average error.

For the purpose of training real world data would be used. The data sets of Medical, Environmental, Monetary and Pattern recognition domains would be considered. Once these results would be obtained, the whole procedure will be repeated for the same data set by varying the above specified learning parameter one by one.

Microsoft Excel would be used to generate the comparison tables and error graphs of the different results obtained.



VIII. CONCLUSION  The research paper aims to find the effects of internal learning parameters on the Artificial Neural Networks. The target domains are intended to be prediction, classification and diagnosis. It is expected to find the effects of internal learning parameters that would result in an optimal performance (result quality and speed) by the Artificial Neural Networks.

REFRENCES  [1] Laurene Fausett, Fundamentals of Neural Networks, Architectures, Algorithms and Applications, Prentice Hall, Upper Saddle River, New Jersey 07458, 1994.

[2] Robert J. Schalkoff, Artificial Neural Networks, McGraw Hill International Editions, 1997.

[3] Lionel Tarassenko , A Guide to Neural Computing Applications, Amold- A member of the Hodder Headline Group London, Sydney, Aukland. Co published in North, Central and South America by     John Wiley & Sons Inc. New York, Toronto, 1998.

[4] Mohamad H. Hassoun, Fundamentals of Artificial Neural Networks, Prentice Hall of India, New Delhi 110001, 1998.

[5] Welstead, Stephen T., Neural Networks and Fuzzy Logic Applications in C/C++, New York: John Wiley & Sons, Inc. 1994.

[6] Valluru B. Rao, C++ Neural Networks and Fuzzy Logic, M & T Books, IDG Books Worldwide, Inc., 1995.

[7] S.Y.Kung, Digital Neural Networks, PTR Prentice Hall, Englewood cliffs, New Jersey 07632, 1993.

[8] Simon Haykin, Neural Networks (A Comprehensive Foundation), Prentice Hall of India, New Delhi, 2003.

[9] Stephen Wolstenholme ?EasyNN Help? Copyright 1999-2001.

[10] Smith Leslie, An Introduction to Neural Networks, Center for cognitive and Computational Neuroscience, Department of Computing and Mathematics, University of Stirling, Edinburgh, Scotland, February 1996.

[11] Humera Noor, Effect of Data Preprocessing in Neural Networks for Intelligent Software Design. Sir. Syed University of Engineering and Technology, Karachi- 2003.

[12] F. Salehi, R. Lacroix and K. M. Wade ?Effects of Learning Parameters and Data Presentation on the performance of Backpropagation Networks for Milk Yield Prediction? Transaction of the ASAE (American Society of Agricultural Engineers). 41(1): 253-259. 1998. pp. 16- 19.

[13] Andrew D. Back, Bill G. Horne, Ah Chung Tsoi, C. Lee Giles ?Alternative Discrete-Time Operators: An Algorithm for Optimal Selection of Parameters?, PROCESSING, 1999.

[14] Francesco Luna, Computable Learning, Neural Networks and Institutions, Department of Economics, Universit di Venezia  [15] William Silvert, Martin Baptist, Can Neural Networks be used in Data Poor Situations? International Workshop on Applications of Artificial Neural Networks to Ecological Modelling, Toulouse, France.

[16] Miroslav Holecy, Application of Neural- Fuzzy Systems in Financial Management, Department of Cybernetics and Artificial Intelligence Technical University of Koice, 1997.

[17] Zhanshou Yu, Feed-Forward Neural Networks and Their Applications in Forecasting, Department of Houston - December 2000.

