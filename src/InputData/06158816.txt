Data Mining Based Network Intrusion Detection System: A  Database Centric Approach

Abstract? Network security technology has become crucial in protecting   government   and   industry computing   infrastructure. Modern intrusion detection applications face complex requirements ? they need to be reliable, extensible, easy to manage, and have low maintenance cost. In recent years, data mining ?  based intrusion detection systems (IDSs) have demonstrated high accuracy, good generalization to novel types of intrusion, and robust behaviour in a changing environment. Still, significant challenges exist in design and implementation of production quality IDSs.

Instrumenting components such as data transformations, model deployment, and cooperative distributed detection remain a labour intensive and complex engineering endeavour. This  paper  describes a  database centric architecture that leverages data mining with .NET to address these challenges. It also offers numerous advantages in terms of alert infrastructure, security, scalability, reliability and also has data analysis tools. The database centric architecture is illustrated with a Data mining Based Intrusion detection system application prototype using .NET.

Keywords? Data mining, Intrusion detection   I INTRODUCTION  Intrusion detection is an area growing in relevance as more and more sensitive data are stored and processed in networked systems.   The goal of intrusion  detection system  (IDS) is to provide another layer of defence against malicious (or unauthorized)  uses of computer systems  by sensing a misuse or a breach  of a security policy and alerting  operators  to an ongoing (or, at least, recent) attack. A comprehensive IDS requires a significant amount of human expertise and time for development.   Data mining-based IDSs require less expert knowledge  yet must provide good performance [19, 1, 15, 6]. These  systems  are  also  capable  of generalizing  to new  and unknown   attacks.   Data mining-based    intrusion   detection systems  can be classified  into misuse  detection  and anomaly detection [19]. Misuse  detection  attempts  to match  observed activity  to  known  intrusion   patterns.  This  is typically  a classification    problem. Anomaly   detection attempts   to identify behaviour that does not conform to normal behaviour. This approach  has a better chance of detecting  novel attacks. IDSs can also be  distinguished  on the basis of the audit data source (e.g., network-based,  host-based).  Successful detection of different  types  of  attacks  typically  requires  a  variety of audit data sources [13].

Building  IDS is a complex  task of knowledge engineering that  requires  an elaborate  infrastructure.

An effective production-quality  IDS need an array of diverse  components and features, including:  ? Centralized view of data ? Data transformation capabilities ? Data mining methods ? Real-time detection and alert infrastructure ? Reporting capabilities ? Data analysis ? System availability   Earlier  proposals  [8, 7] have  highlighted  the  need  for an architecture   and  framework   specification   for IDSs.  While these proposals provide a good foundation, they are somewhat general   in   nature   and   focus either   on   a   methodology specification  or on component  interaction  specification.  The details  of the infrastructure  required  to support  these feature rich complex   frameworks   are  not  provided,   instead such details are proposed to be handled by system engineers responsible for the implementation.

This paper demonstrates  that the Oracle Database,  with its capabilities for supporting mission critical applications, distributed  processing,  and integration  of analytics, can be an appropriate  platform  for  an  IDS implementation.  Given  the data-centric  nature of intrusion  detection  process [13], leveraging   existing RDBMS   infrastructure    can   be   both efficient and effective.

The current paper presents slight change in the DAID (Database-centric  Architecture  for Intrusion Detection) which can  be  used  for  all  types  of  databases, including   Oracle Database.  This  RDBMS-centric framework  can  be  used  to build,  manage,  deploy, score  and analyse  data mining-based intrusion detection models. The described approach is adopted in the  Intrusion   Detection   Center   (IDC)   prototype   ? an application implemented in .NET.

The paper  is organized  as follows.  Section  2 outlines the proposed  architecture.  The  individual  components are described and illustrated with references to the IDC prototype and functionality available in the Database.

Section 3 presents the conclusions and directions for future work.



II.  A DATABASE-CENTRIC  ARCHITECTURE  DAID (Fig 1.) shares many aspects of the AMG (Adaptive Model  Generation)  architecture  [8]. As in AMG,  a database component  plays a key role in the architecture.  Unlike AMG, where  the  database  is only a centralized  data  repository,  in DAID, all major operations take place in the database itself    Fig.1. Database-Centric architecture for intrusion  detection  DAID   also   explicitly   address   data   transformations, an essential  component  in  analytics.  DAID  has  the following major components:  ? Sensors ? Extraction, transformation  and load (ETL)  Centralized data warehousing  ? Automated rule generation ? Real-time and offline detection ? Report and analysis   The  activity  in  a  computer  network  is  monitored  by an array of sensors  producing  a stream  of audit data.

The audit data are processed and loaded in a centralized  data repository (ETL). The stored data are used for rule generation.  The rule generation data mining methods are integrated in the database infrastructure  ? no data movement  is required.  The incoming audit data will be checked by the rules to detect the intrusions. The  stored  audit  data  can be also further  investigated  using reporting and analysis tools in .NET.

The key aspect to the described data flow is that processing is entirely contained  within the database.

With the exception of  the  sensor  array,  all  other components  can  be  found  in modern RDBMSs.

Among the major benefits of using such an integrated approach are improved security, speed, data management and access, and ease of implementation. The following sections discuss and exemplify the functionality and usage of the individual components.

Sensors A sensor is a system that collects audit information.  Many types of audit streams  can be used for detecting  intrusions  ? examples   include   network traffic   data,   system   logs   on individual   hosts,   and system   calls   made   by   processes. Network   sensors typically   filter   and   reassemble   TCP/IP packets in order to extract high level connection features (e,g., duration of connection,  service, number of bytes transferred). A  number  of  utilities  exist  to  assist  the user  in  this  data extraction process [20, 18]. Host sensors monitor system logs, CPU and memory usage on a machine. In a distributed architecture, an emphasis is placed on creating lightweight sensors since they are the only components  that must run on the system that is being protected. DAID also favours a lightweight sensor  approach  since  all  computation  intensive tasks (e.g., feature extraction,  Rule generation,  and detection) takes place in the RDBMS. In the IDC prototype, we simulate a network environment by streaming previously collected network  activity  data.

This dataset  was originally  created  by DARPA  and later used in the KDD?99  Cup [12]. The same dataset has already been successfully  used for demonstrating the capabilities of another intrusion detection framework [2].

ETL Typically,  sensor audit streams require further pre- processing and feature extraction before the data can be successfully   used   for   data   mining   rule generation.   For example,   temporal   statistical features   of  connections   and sessions   have   been found   informative   [13].   Other   more elaborate approaches (e.g., conceptual clustering) have also shown promise [11].

In the RDBMS  context,  SQL  and user-defined functions offer   a   high   degree   of   flexibility   and efficiency   when extracting key pieces of information from the audit stream.

An  easy   way  to  comply   with   the   conference paper formatting requirements  is to use this document as a template and   simply   type   your   text   into   it.

Fig.2.   shows   the transformation  of the network  audit data  connection  records into the database using the equation given below:          Fig. 2. Data transformation  T: R?I R ? i (a1, a2, ..., an) (1)   I is  the  connection  records  obtained  by the  sensors and each connection  record is transformed  into an attribute  a1, a2 through an.

The KDD?99 network activity data used in our study had already  been  suitably  pre-processed.   Therefore the  current version   of   the   IDC   prototype   does   not include   feature extraction  or other raw data transformations  at the processing ETL stage.

Data warehouse Using   the   RDBMS   databases   as a   centralized   data repository offers significant flexibility in terms of data manipulation.  Inputs from different  sources can be combined through joins.

Without replicating data, database views or materialized views can capture different slices of the data (e.g., data over a given time interval, data for a specific host). Such views  can  be  used  directly   for  rule  generation and  data analysis.  The RDBMS  databases  have the additional  benefits of data security,  high  availability and load support,  and fast response time [14, 4].

Rule generation A  number  of  data  mining  techniques have  been  found useful in the context of misuse and anomaly detection. Among the most popular  techniques are association  rules, clustering, support vector machines (SVM), and decision trees [1, 15, 17, 10].    In this  IDC  prototype  we  have  used  association  rule mining   technique.   Association   rule  mining  is  one of  the important topics in data mining research. This approach determines  interesting  relationships  between large set of data items.  This  technique  was  initially applied  to  the  so-called market  basket  analysis,  which aims at finding  regularities  in shopping   behaviour   of customers   of   supermarkets   [27]. Agrawal  in [28] has proposed  the Apriori  algorithm  to find quickly Boolean association rules. In contrast to Boolean association rules, which handle only simple item-based transactions, the next generation of association rules faced quantitative attributes which their values were elements of continuous  domains  such as real number domain  R. But, the typical Apriori algorithm  was not capable  of dealing directly with such attributes.

Therefore,  in [29] an algorithm has been proposed  to mine  quantitative  association  rules.  This algorithm starts by partitioning the attribute domains and then transforming  the  problem  into  binary  one.  This method  can solve problems introduced by quantitative attributes.

1)  Apriori    Algorithm:    As    mentioned    before, Agrawal proposed the Apriori algorithm that is known as a fundamental  algorithm  for mining  frequent itemsets  in a set of transactions.  He defined  the formal  statement  for association rules as follows: Let I = { i1, i2, i3, ..., im} be a set of items  and D be a set  of transactions,  where  each transaction  T is a set of items such that T I. A set of items  X I is called  an itemset.  It can  be said  that  T contains an itemset X, if X I. An association  rule is an implication  of the form X Y, where X I, Y  I and X Y =   . The rule X Y has Support S (S = support (X   Y)) in the transaction set D, of S% of transactions  in D  contain   X Y  and   it has   Confidence   C   (c  = support(X   Y)/support(X)) in the transaction set D, if C% of  transactions   in  D that  contain   X  also  contain   Y. Apriori algorithm works iteratively and has two steps: In the first step, it finds all the large itemsets and in the next step, it uses the large itemsets to generate effective association  rules.

A  more  detailed  description  of  these two steps can be found in [28, 29]. This algorithm  will scan the database many times to find the large itemsets.

Intrusion   Database   and  Preprocessing   of  Features:  In 1998,  Defense  Advanced  Research  Project  Agency (DARPA)  funded an ?intrusion  detection  evaluation program  (IDEP)?  in  Lincoln  Laboratory  at the Massachusetts   Institute  of  Technlogy  which  this dataset was  prepared  by  Stolfo  [12].  Since  1999, KDD?99  was built  based  on  the  data  captured   in DARPA?98.   This dataset consists of three component   Table1 Attack Types Found In Kdd?99 Darpa Dataset  Attack Types Class Attack Types Class Normal Normal guess_passwd            R2L  apache2        DoS  lmap back multihop land named  mailbomb phf neptune sendmail  pod snmpgetattack processtable snmpguess  smurf spy teardrop warezclient udpstorm warezmaster buffer_overflo        U2R  worm httptunnel xlock loadmodule xsnoop  perl ftp_write ps lpsweep    Probe  rootkit mscan sqlattack portsweep   xtrem  nmap saint satan           TABLE 2 NAME of 41 ATTRIBUTES in KDD DATASET    No Features No Features  1 duration 22 is_guest_login 2 protocol_type 23 count 3 service 24 srv_count 4 flag 25 serror_rate 5 src_bytes 26 srv_serror_rate 6 dst_bytes 27 rerror_rate 7 land 28 rrv_rerror_rate 8 wrong_fragment 29 same_srv_rate 9 urgent 30 diff_srv_rate  10 hot 31 srv_diff_host_rate 11 num_failed_logins 32 dst_host_count 12 logged_in 33 dst_host_srv_count 13 num_compromised 34 dst_host_same_srv_rate 14 root_shell 35 dst_host_diff_srv_rate 15 su_attempted 36 dst_host_same_src_port 16 num_root 37 dst_host_srv_diff_host_ 17 num_file_creations 38 dst_host_serror_rate 18 num_shells 39 dst_host_srv_serror_rate 19 num_access_files 40 dst_host_rerror_rate 20 num_outbound_cm 41 dst_host_srv_rerror_rate 21 is_host_login   and it contains a number of connection  records where each connection is a sequence of packets containing values of 41 features and labelled as either normal or attack. Attack types in  this  dataset  fall  into  four  main categories:  Denial  of Service  (DoS),  Probe,  User to Root (U2R)  and Remote  to Local   (R2L)   which   is detailed   in  Table1.   These   four categories summarize 22 subclasses of attacks.

DoS:  This  kind  of  attack  consumes   a  lot  of computing, memory  resources  and  denying  the legitimate  requests.  The means  of achieving  this are varied  from buffer  overflows  to flooding the system resources.

U2R:  Unauthorized  access to super user root privilege.  This kind of attack starts out with normal user accessing the system and gradually  exploiting  system  vulnerabilities to gain super user access.

Probe:  Attacker   scans  the  network  to  gather information about the network and find the system?s known vulnerabilities. These vulnerabilities  will be exploited to attack the system. R2L:  Unauthorized access  from  a remote  to local  machine. An  attacker who  does  not  have  an  account  exploits  some systems vulnerability to gain local access.

In   this   research,   we   use   KDD?99   [12]   to evaluate   our prototype. In this way, a subset of KDD dataset with the same distribution  is used for the purpose of training and a subset of ?Corrected KDD? dataset is used as the test set. The name and type of features in the KDD dataset are listed in Table 2.

The motivation for applying the association rules  algorithm to audit data is: Audit  data can be formatted into a database  table where   each   row  is  an  audit record   and  each column is a field of the audit records; There is evidence that program executions and user activities   exhibit   frequent   correlations    among system features.

We can continuously  merge  the rules from a new run to the aggregate rule set (of all previous runs).

Detection We are using  the apriori  algorithm  to generate  the rules and use these rules for finding the intrusions in the test data.

Reports and Analysis As we are using the database as the platform for the implementation   of  the  IDS,  it?s easy  to  generate  the  data analysis   results   and reports.   Collected   audit   data,   rules generated, finding the intrusions,  analysis  and reports can be seen using  crystal  reports  and  visualization  tools  in  .NET.

This allows the lengthy application  development process and provides  a standardized  and  easily customized  report generation.

The IDC prototype leverages the tools available in .NET to instrument the network activity reporting and analysis mechanism. On the rule generation page (Figure 3) admin can generate the rules by double clicking on the training data with a specific  type  of attacks  and  use this  rules  for  finding  the intrusions in the test data.

On the analysis page (Figure 4) users can monitor: Number of intrusions  ? Breakdown of the network activity into normal and specific types of attacks.

? Break down of the network data into specific types of protocols  ? Break down of the network data into specific types of services.

On the report generation page (Figure 5) users can take the copy  of  the  report  generated  showing  the  attack types, protocol   types   and  different   types   of services   in  the network data that has been tested for intrusions.

Fig. 3 IDC Prototype Showing Rule Generation Page          Fig.4. IDC Prototype Showing Data Analysis Page     Fig.5. IDC Prototype Showing Report Generation Page

III. PERFORMANCE  EVALUATION  The table given below shows the comparison between the IDC prototype  developed  and the general rules applied to the test data to detect the attacks in the network data.

TABLE 3 COMPARISON OF RESULTS BETWEEN IDC PROTOTYPE AND GENERAL RULES APPLIED  TO THE TEST DATA.

Attack Types Detected  By The  Data  Mining  Based Intrusion  Detection.

Attack  Types  Detected By  Applying   General  Rules.

ftp_write ftp_write guess_password pod httptunnel snmpgetattack named snmpguess pod sqlattack ps warezmaster rootkit snmpgetattack teardrop warezmaster xterm    We took some one thousand  records  of network data and used our prototype to find the intrusions in them and same set of network  data was taken and general rules were applied  to find the intrusions. The table 3 shows the attack types detected by our prototype  and we can clearly see that the attacks like guess_password, httptunnel,  named,  ps, rootkit,  teardrop  and xterm were not detected by applying the general rules.



IV. CONCLUSION  Database-centric IDSs offers many advantages over alternative systems. These include integration of individual components,   security,   scalability,   and high  availability.   It provides administrator  the privileges for finding the intrusions which is reliable, secure and fast. This IDS can be developed in a reasonably  short time ? frame and at a low development cost. It also provides a good GUI?s for the users to work with. This system also helps in securing the network.

The  system  developed  can  also  be  extended  to  find the host-based  intrusions  and also credit-card  fraud detection.  we can also take into account that the system can pre-process the sensor data available from sensors.

This will be investigated in future work.

