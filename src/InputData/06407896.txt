Association Rule Mining Algorithms and Genetic Algorithm: A Comparative Study

Abstract?Generally frequent itemsets are extracted from large databases by applying association rule mining (ARM) algorithms like Apriori, Partition, Pincer-Search, Incremental, and Border algorithm etc. Genetic Algorithm (GA) can also be applied to discover the frequent patterns from databases. The main advantage of using GA in the discovery of frequent patterns or itemsets is that they can perform global search and its time complexity is lesser compared to other Apriori-based algorithms as because it is based on the greedy approach. But the FP-tree algorithm is considered to be the best among the ARM algorithms, because its candidate sets generation procedure is completely different from Apriori-based algorithms. The major aim of this paper is to present a comparative study among ARM-based and GA-based approaches to data mining.

Keywords-Genetic Algorithm; Association Rule Mining; Frequent itemset; Support; Confidence; Apriori; FP-tree; Data Mining.



I. INTRODUCTION Data is considered to be the core element in this era of  information technology. Huge amounts of data have been collected routinely for transactional purposes in business, administration, banking, the delivery of social and health services, environmental protection, security and in politics.

Such data is primarily used for accounting and for management of the customer base. Typically, management data sets are very large and constantly growing and contain a large number of complex features. While these data sets reflect properties of the managed subjects and relations, and are thus potentially of some use to their owner, they often have relatively low information density. One requires robust, simple and computationally efficient tools to extract information from such data sets. The development and understanding of such tools is the core business of data mining. These tools are based on ideas from computer science, machine learning, mathematics and statistics.

Mining useful information and helpful knowledge from these large databases has thus evolved into an important research area [1, 2].

Data mining has attracted a great deal of attention in the information industry and in society as a whole in recent years, due to the wide availability of huge amounts of data and the imminent need for turning such data into useful  information and knowledge. The information and knowledge gained can be used for applications ranging from market analysis, fraud detection, and customer retention, to production control and science exploration.

Frequent pattern mining is an important area of Data mining research. The frequent patterns are patterns (such as itemsets, subsequences, or substructures) that appear in a data set frequently. For example, a set of items, such as milk and bread that appear frequently together in a transaction data set is a frequent itemset. A subsequence, such as buying first a PC, then a digital camera, and then a memory card, if it occurs frequently in a shopping history database, is a (frequent) sequential pattern. Finding such frequent patterns plays an essential role in mining associations, correlations, and many other interesting relationships among data. Moreover, it helps in data classification, clustering, and other data mining tasks as well.

The process of discovering interesting and unexpected rules from large data sets is known as association rule mining. This refers to a very general model that allows relationships to be found between items of a database. An association rule is an implication or if-then-rule which is supported by data. The association rules problem was first formulated in [3] and was called the market-basket problem.

A typical association rule resulting from such a study could be ?40% of all customers who buy Laptop or PC also buy antivirus software" ? which reveals a very important information. Therefore this analysis can provide new insights into customer behaviour and can lead to higher profits through better customer relations, customer retention and better product placements. The subsequent paper [4] is also considered as one of the most important contributions to the subject.



II. RELATED WORK The mining of association rules is a field in data mining  that has received a lot of attention in recent years. The main association rule mining algorithm, Apriori, not only influenced the association rule mining community, but it affected other data mining fields as well. Apriori and all its variants like Partition, Pincer-Search, Incremental, Border algorithm etc. take too much computer time to compute all the frequent itemsets. The subsequent papers [11, 13, 18]      contributed a lot in the field of Association Rule Mining (ARM).

All the traditional association rule mining algorithms were developed to find positive associations between items.

Positive associations refer to associations between items existing in transactions. In addition to the positive associations, negative associations can provide valuable information. In practice there are many situations where negation of products plays a major role. By using Genetic Algorithm (GA) the system can predict the rules which contain negative attributes in the generated rules along with more than one attribute in consequent part. In this regard the contributions of the papers [10, 15, 16, 17] are worth mentioning for finding association rules.

Several encouraging attempts at developing methods for association rule mining have been proposed. However, efficiency and simplicity is still a barrier for further development. Normally, pre-processing or post-processing is required for mining data, such as transforming the data from any given format to relational format. In this paper, an attempt has been made to present a comparative study among the ARM-based and GA-based approaches to data mining.



III. ASSOCIATION RULE MINING (ARM) Association Rule Mining aims to extract interesting  correlations, frequent patterns, or associations among sets of items in the transaction databases or other data repositories [8]. The major aim of ARM is to find the set of all subsets of items or attributes that frequently occur in many database records or transactions, and additionally, to extract rules on how a subset of items influences the presence of another subset. ARM algorithms discover high-level prediction rules in the form: IF the conditions of the values of the predicting attributes are true, THEN predict values for some goal attributes.

In general, the association rule is an expression of the form X=>Y, where X is antecedent and Y is consequent.

Association rule shows how many times Y has occurred if X has already occurred depending on the support and confidence value.

Support: It is the probability of item or item sets in the given transactional data base:  support(X) = n(X) / n where n is the total number of transactions in the database and n(X) is the number of transactions that contains the itemset X.

Therefore, support (X=>Y) = support(XUY).

Confidence: It is a conditional probability, for an association rule X=>Y and defined as:  confidence(X=>Y) = support(XUY) / support(X).

Frequent itemset: Let A be a set of items, T be the  transaction database and ? be the user-specified minimum support. An itemset X in A (i.e., X is a subset of A) is said  to be a frequent itemset in T with respect to ?, if support(X)T ? ?.

The main association rule mining algorithm, Apriori, is proposed by R. Agrawal and R. Srikant in 1994 for Boolean association rules [4]. The name of the algorithm is based on the fact that the algorithm uses prior knowledge of frequent itemset properties. This is also called the level-wise algorithm, makes use of a bottom-up search, moving upward level-wise in the lattice. This algorithm uses two functions (candidate generation and pruning) at every iteration. It moves upward in the lattice starting from level 1 till level k, where no candidate set remains after pruning [9].

The Apriori heuristic achieves good performance gained by (possibly significantly) reducing the size of candidate sets. However, in situations with a large number of frequent patterns, long patterns, or quite low minimum support thresholds, an Apriori-like algorithm may suffer from the following two nontrivial costs: (i) it is costly to handle a huge number of candidate sets. For example, if there are104 frequent 1-itemsets, the Apriori algorithm will need to generate more than 107 length-2 candidates and accumulate and test their occurrence frequencies. Moreover, to discover a frequent pattern of size 100, such as {a1, a2,?..,a100}, it must generate 2100 ? 2 ? 1030 candidates in total. This is the inherent cost of candidate generation, no matter what implementation technique is applied, and (ii) it is tedious to repeatedly scan the database and check a large set of candidates by pattern matching, which is especially true for mining long patterns.

Figure 1.  A sample FP-tree  The paper [13] describes the well-known FP-tree algorithm (a sample FP-tree is shown above in figure 1), for mining the complete set of frequent patterns. Here efficiency is achieved with three techniques: (i) a large database is compressed into a condensed, smaller data structure, FP-tree which avoids costly, repeated database scans, (ii) FP-tree-based mining adopts a pattern-fragment growth method to avoid the costly generation of a large number of candidate sets, and (iii) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined     patterns in conditional databases, which dramatically reduces the search space.

The FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent-pattern mining methods.



IV. GENETIC ALGORITHM (GA) Genetic Algorithms (GAs) are adaptive heuristic search  algorithm premised on the evolutionary ideas of natural selection and genetic. The basic concept of GAs is designed to simulate processes in natural system necessary for evolution, specifically those that follow the principles first laid down by Charles Darwin of survival of the fittest. As such they represent an intelligent exploitation of a random search within a defined search space to solve a problem.

GAs are one of the best ways to solve a problem for which little is known. They are a very general algorithm and so will work well in any search space. The Genetic Algorithm [5] was developed by John Holland in 1970. GA is stochastic search algorithm modeled on the process of natural selection, which underlines biological evolution [6].

GA has been successfully applied in many search, optimization, and machine learning problems. GA works in an iterative manner by generating new populations of strings from old ones. Every string is the encoded binary, real etc., version of a candidate solution. An evaluation function associates a fitness measure to every string indicating its fitness for the problem [7].

Standard GA apply genetic operators such selection, crossover and mutation on an initially random population in order to compute a whole generation of new strings. GA runs to generate solutions for successive generations. The probability of an individual reproducing is proportional to the goodness of the solution it represents. Hence the quality of the solutions in successive generations improves. The process is terminated when an acceptable or optimum solution is found.



V. PERFORMANCE MEASURE The main aim of association rule mining algorithms is  the derivation of if-then-rules based on the frequent itemsets. The Apriori algorithm uses a breadth-first search (BFS) approach, first finding all frequent 1-itemsets and then discovering 2-itemsets and continues by finding increasingly larger frequent itemsets. The data is a sequence x (1) ,?, x (n) of binary vectors. We can thus represent the data as an n by d binary matrix, where n is the number of data records and d is number of items [11]. The time complexity of Apriori algorithm is T = O(d2*n).  Thus we have scalability in the data size but quadratic dependence on the dimension or number of attributes.

But a genetic algorithm consists of a population and an evolutionary mechanism. The population is a collection of individuals which represent potential solutions through a mapping called a coding. The population is ranked with the help of fitness function. We apply genetic algorithm on the selected population from the database and compute the fitness function after each step.

The paper [14] proposes three different measures for performance evaluation of GAs based on observations in experiments by simulation: The likelihood of optimality, the average fitness value and the likelihood of evolution leap.

Based on the above measures, a term has been introduced, cut-off generation k, i.e., how many generations a GA should be executed in each run.

Considering C = k*r be the total computation cost given to execute the GA, where r is the number of repeated runs; the best cut-off generation is the number k of generations which maximizes the performance with respect to a particular measure. If C is fixed, we want to find k maximizing the term (1- p(k)) r , where p(k) denotes the probability that a GA produces an optimal solution within k generations. If the value of (1- p(k)) r is fixed, we want to find k minimizing C = kr. Surely this indicates that the GA based simulations can be completed in linear time [11].

Therefore it follows from the above theoretical discussion that GA based solution provides significant improvement in computational complexity in comparison with Apriori algorithm and all its variants. If we look at the performance of FP-tree algorithm [13], it is seen that this algorithm can also compute the frequent itemsets in linear time like GA, but additional data structure is required which leads to increased space complexity, so we stick to GA.



VI. METHODOLOGY In this paper the Genetic algorithm (GA) is applied on  large data sets to discover the frequent itemsets. We first load the sample of records from the transaction database that fits into memory. The genetic learning starts as follows. An initial population is created consisting of randomly generated transactions. Each transaction can be represented by a string of bits.

Our proposed GA-based method for finding frequent itemsets repeatedly transforms the population by executing the following steps:  (1) Fitness Evaluation: The fitness (i.e., an objective function) is calculated for each individual.

(2) Selection: Individuals are chosen from the current population as parents to be involved in recombination.

(3) Recombination: New individuals (called offspring) are produced from the parents by applying genetic operators such as crossover and mutation.

(4) Replacement: Some of the offspring are replaced with some individuals (usually with their parents).

One cycle of transforming a population is called a generation. In each generation, a fraction of the population is replaced with offspring and its proportion to the entire population is called the generation gap (between 0 and 1).

RESULTS We use different databases (for example, 10000 x 8 Database, Plant Cell Signaling, Zoo, Tic Tac Toe, Synthetic #1, and Synthetic #2 etc.) from this URL [12] to show the effectiveness of our proposed GA-based method by using MATLAB software (version R2010a). The initial population was 20 and crossover was chosen randomly. The mutation probability was taken 0.05. The frequent itemsets with user-specified minimum support (?) ? 20% was selected for simulation. A portion of the sample 10000 x 8 database is shown below:  0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0  ---------------- ----------------  Figure 2.  Sample 10000 x 8 Database  The proposed technique was implemented on the different datasets with satisfactory results. For example, when we tested our proposed GA-based method on the huge data set of 10000 x 8 Database (consisting of 10000 records) we also achieved success which surely proves the effectiveness of the proposed method.

CONCLUSION AND FUTURE WORK We have dealt with a challenging association rule mining problem of finding frequent itemsets using our proposed GA-based method. The method, described here is very simple and efficient one. This is successfully tested for different large data sets. The results reported in this paper are correct and appropriate. However, a more extensive empirical evaluation of the proposed method will be the  objective of our future research. In our present work, we used the random sampling based method. Use of some other sampling techniques like cluster-based sampling or regression-based sampling, a good sample could have been found. A perfect sample will also improve the correctness of the association rules generated by the proposed algorithm.

Moreover, the incorporation of other interestingness measures mentioned in the literature is also part of our planned future work.

