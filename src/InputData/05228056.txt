Effective Web Personalization Using Clustering

Abstract- This paper describes web personalization, the essence of personalization is the adaptability of information systems to the needs of the adaptability of information system to the needs of their users. This issue is becoming increasing important on the web, as non-expert users are overwhelmed by the quantity of information available online. The proposed approach aims to mine a reduced set of effective association pattern rules for enhancing the online performance of web recommendations and proposed approach is evaluated based on the efficiency and quality. Adaptive user clustering and profiling is essential to be able to accurately predict user actions. In this paper we present results of our clustering and personalization project.We compare several distance measures used in clustering.We introduce a new measure to assess the quality of clustering independent of the distance measure used in the clustering process. We also compare different strategies(such as clustering users vs, clustering urls),

I. INTRODUCTION  Recently, many researchers have investigated the generation of user profiles from raw web logs by using various data mining techniques. The technique of clustering individual user sessions is seen as a method to aid in collaborative filtering in [3], [4], [5], [6]. Incremental web-log mining to create adaptive  Web-servers have also been investigated [9]. Most of these approaches concentrate on either user or urI clustering, but not on employing both. Likewise, they concentrate more on the clustering/profile creation methods themselves. In this paper we look at employing both user as well as urI clustering information for better completeness and relevancy (these attributes are defined below) of the recommended set. We investigate several distance measures commonly employed in the literature and evaluate some others. We have also proposed a measure (the induced entropy) to assess the quality of clusters generated. This quantity does not depend on the distance metrics invoked by the clustering algorithm, it can be computed directly from the original data. Experimental data is presented and discussed.



II. PROPOSED ApPROACH  Clustering users seems to be an indirect approach. In a sense, it emphasizes the question "what have like minded users done". The primary emphasis should be on the question "which urIs is this particular user likely to visit T". Then as a secondary option, the user might be told what like minded people have done (Amazon.com does this: after a book purchase, they inform the user which other books were deemed interesting by people who bought the same book).

Intuitively it appears that urIs should be clustered first, and a list of the (top few) urI clusters closest to a user's usage pattern should be retrieved when the user signs on.

Accordingly a comprehensive strategy would consist of the following steps:  (1) Cluster the URLs as well as the users (note that clustering of URLs helps answer the question "how similar are url's i andj).

(2) For each user, maintain a list of the (closest few) urI clusters. When the user logs-in, first retrieve links from the urI clusters he/she is closet to.

(3) The last (auxiliary) step: search the user clusters to find what other urIs "like minded" users have visited (that might be missed by the urI clusters this particular user likes).

A. Distance metrics As shown in Figure 1, we create an "incidence matrix"(denoted by A), whose rows represent sessions and columns represent urIs. If a urI was visited in a session, the entry in that column is 1, otherwise it is 0 (as illustrated in figure 1). Note that the "1" could be replaced by the count of number of times a urI was visited. This gives more detailed information of visitation patterns. In case users have explicitly ranked urIs giving them scores of some sort, those scores can be entered instead of"I"s. We approximate a "session" to be a user. This is not necessarily true, but with cookies this approximation can be close to reality: a cookie  lAMA 2009    URIs  U1 ... UL.... Uj..... Un  can identify whether the session originated from the same browser (which is likely to be used by the same end-user). The incidence matrix is likely to  sIlO ... 1 ... 0....... 1  Likewise AAT is a measure of similarities between users. Note that each user could be considered a point in an m dimensional space with distance among users given by an m x m matrix (such as AA  T . However this space is not a normed space  (triangle inequality does not hold). Hence relational clustering must be used in this space. In the following, UrIs are dented by ii s (Le., iii corresponds to the ith column of A), whereas sessions are denoted by S (Le., Sj corresponds to the jth row of A). The distance between a pair of corresponding entities (represented by the corresponding vectors) is denoted by D.

The normal (Euclidean or 2) norm of a vector is denoted by 11.11, whereas the l-norm of a vector (sum of absolute values of its elements) is denoted by 1.1  We have tested the following distance measures.

S ssions/users00 1 1 1Sk  Si 00 0 1 1 (1)  "Max" is the maximum value of dot product for any pair of vectors. (the dot product itself measures similarity, subtraction from the max is one way to render it to be a distance or dissimilarity)  be sparse and all well known techniques to handle sparse matrices can be applied here. Each urI is an m dimensional vector and could be treated as a point in an m dimensional Euclidean space and any lustering algorithm can be applied.

Likewise each session could be treated as a point in an n dimensional Euclidean space for the purpose of clustering.

However, clustering methods that work directly with normed spaces are typically very slow when the number of dimensions (m or n) is large which is the case here. Using relational clustering methods that only need the relative distances could be a better approach. The important point is that the distance calculation (based on the huge incidence matrices) is not part of the clustering algorithm itself, (these are typically pre-computed), so the algorithm could be potentially faster. Note that metric space algorithms that update centers of clusters as they iterate will have to work with original spaces which have huge dimensionality. On the other hand, in relational clustering methods the entire information about a pair of points is summarized by a single number irrespective of the dimensionality of the underlying space. Hence relational methods can be expected to be faster and more compact. Moreover, relative distances are more amenable to incremental updating (as more data is collected over time). This coupled with the fact that a relational clustering algorithm could be potentially smaller and faster suggests that reclustering or incrementally updating the clusters would be easier with relational clustering methods. A matrix of pair-wise distances (or similarities among users or urIs) which is required for relational clustering can be easily generated from the incidence matrix. For instance ATA is an n x n matrix which can be thought to indicate the similarities between urIs (the i jth entry of ATA measures the number of sessions in which both the urI's (i andj) were visited, which can be thought to measure the similarity between the two urIs).

(ii) Distance Dij = (1- COS(Ui,Uj))2 (2)  where cos(iii,iij) = uTi. iij/lliiill.lliijll (3)  The main difference wrt. measure (i) above is that the raw popularities of the urIs are factored out by dividing by the norm. For instance if incidence values for urIs i.j, k, I in a row of incidence matrix A are 10, 10,25,25 then both the urI pairs in question are correlated. Pair (i, j) is correlated, so is pair (k,l) but in one case (pair (k,l)), the raw popularity of the urIs is higher and contributes more to the similarity.

(iii) Distance Dij = (1-Pij)2 where (4)  where (iii) is simply the average (mean) defined as the sum of all the element's of the vector ui divided by the number of elements and,<Jwi = standard deviation =~(iI>_<iii>2  (iv)Distance Dij = IIiii-iijII (Euclidean norm) (6)  (v) Distance Dij = P(Uii'y} + P(UiUj) (7)  where P(Ui uj)denotes the probability of the event [visiting urI i AND not visiting urlj]. Since    where P(uiujlk) is the conditional probability of the event [visiting urIs i AND j I session k] It is straightforward to verify that  We need to calculate the probability of the event [Ui is visited  AND Uj is visited]. Since the sessions k can be approximated to be mutually independent events which together cover the entire event space, using conditional probabilities we get  (9)  (10)  M. The next step is recommending urIs based on the clustering. The way urIs are recommended is this: a row from the incidence matrix is read off, corresponding to a session- cluster (or an individual session if no clustering has been done on sessions). The total incidence of that row is summed (this is simply the sum of all elements in that row). UrIs from urI clusters with the highest incidence in that row are recommended until the total incidence of the recommended urIs exceeds the a threshold. The threshold function used was(totalIncidence/numSessionsInCluster), where the numerator is the row-sum mentioned above and the denominator is the number of sessions that got clustered together (Le., the number of rows that have been collapsed into the current row).

A. Evaluation ojClusters  Completeness C = IRI n IVI / IVI (15)  (14)  (17)  R = IRI n IVI/IRI  simplestj = linear combination aR+~C-yB  Relevance  score/quality Q= j(R,C,B)  No matter how sophisticated (or simple for that matter) he distance measure and the clustering algorithm is, in the end what matters is the quality of clusters produced. The assessment of quality should be independent of the distance measure itself as far as possible (the clustering algorithm presumably already minimizes (optimizes) an objective function which includes the distances). Hence we used the following method. Since the end product is a set of recommendations, it is natural to evaluate how useful the recommendations were. Denote by R the set of urIs recommended to a user, and by V the set of urIs visited. Let lSI denote the cardinality of a set S. Then we define  The score could be some combination f relevance, completeness and the number of bad recommendations,  Bad recommendations B = IRI- ~RI n IV]) (16)  which is a linear combination of R,C,B where a, ~, yare constants. See [13] for another method of combining the scores into one value. We look at R,C,B separately without combining hem into one value. These attributes (R,C,B, etc.) can be judged subjectively since they can be assigned values/importance (at least subjectively). For instance, in some cases, completeness might be the main criteria whereas in some other cases, relevancy might be the chief attribute. As an example, consider a mobile user who asks for a list of restaurants within 1 mile of their current location. The businesses paying the recommendation service provider want relevancy: if their web-site is recommended to the user, they  (11)  The same method is used when sessions are clustered: now multiple rows (belonging to the same cluster) are collapsed into one row representing the cluster. This way, no matter how the matrix is reduced, the sum of all elements always remains  substituting from (10) and (11) into (9), we get  Clustering and Recommendation Since these two steps is not the main focus of our investigation,they are briefly outlined in this subsection. Each of the above distance measures was used to construct distance matrices for clustering urIs as well as users. The next step was to invoke a relational clustering algorithm (which used the distance various matrices). We used the fuzzy-C medoids clustering algorithm [10], [11], [3], [12].

Note that the algorithm itself is immaterial, any of he large number published in the literature could be used in this step.

We used the fuzzy-C medoids method because it seems to perform well and a robust implementation of that method was easily available to us. Reduced matrices are generated after clustering (this would be the whole point behind clustering).

For example, suppose only urIs are clustered. The reduced matrix would have fewer columns since all columns (urIs) belonging to the same cluster are combined into a single column. Each entry in the new column is the sum of corresponding entries in the columns that got collapsed into this column. For instance, suppose urI's i.] and k get clustered into 1 group. Then, the 3 columns i, j, k in the original incidence matrix A are replaced by a single column c where each element of (vector) c is the sum of corresponding elements of i.], k, i.e,  where, M = Li Lj aij (sum of all elements of matrix A ).

Invoking the assumption that visiting ui and uj are independent events, the conditional probability of visiting links i and j given session k is  Cr = ari + arj + ark, where r=I, ,m (13)    want the user to at least visit that web-site ((VnR)lR to be maximum). The recommendation service provider on the other hand would like to primarily worry about completeness: they'd not be very happy if the user took none of their suggestions which would signal to them that the user did not like anything they recommended and may not pay for their service next time around (they want (VnR)IV to be maximum)). For this reason, these scores are together referred to as "subjective scores" in the remainder of the manuscript.

We define the entropy E ofthe original matrix A to be  In a sense, E measures the spread of total incidence M among the sessions and urIs. If, and only if, only one entry is non-zero then E= 0 (since xlg x ~ 0 as x----. 0, x dominates 19 x). On the other extreme, if all entries are identical then E = 19(mn) where mn is the total number of elements in the (incidence) matrix. For any other distribution, the E is in between these two extremes. It is seen that E is biased by the dimensions of the matrix (for example consider two different matrices with all entries identical, one with dimensions (ml,nl) and the other with dimensions (m2,n2), even though they are both equally spread, their E values are different). The reduced matrix (generated by clustering) can be shown to have less E than the original matrix. This is intuitive, clustering is trying to reduce the spread by concentrating many incidence entries into one, and is also reducing the dimensions. Trying to minimize E is therefore not a good way to assess the quality of clustering (trivially group all sessions and all urIs into 1 cluster which will yield a 1 x 1 matrix whose E = 0, but this clustering is certainly not what is desired). Instead we look at the E of another matrix induced by the reduced matrix (the induced entropy is denoted by E). The induced matrix has the exact same dimensions as the original matrix. Suppose that columns (urIs) cl,c2, .. ..., ck got collapsed into 1 column because of urI clustering and rows (sessions) rl,r2, ,rl got clustered together. In the reduced matrix the k x I block in the original matrix is replaced by a single element whose value o is the sum of all the elements in the original k x I sub-block as explained above. To generate the induced matrix this sum o is uniformly spread over a k x 1sub-block, Le., each entry in the sub-block now has the value olkl . In other words the single entry in the reduced matrix is replaced by a k x 1sub-block.



III. EXPERIMENTAL RESULTS AND CONCLUSION  We tested the different distance metrics using the evaluation criteria described above. Real web access logs from our departmental web-server were parsed to generate an incidence matrix of dimension 2474 (sessions) x 3013 (urIs).

From this incidence matrix, 10 different distance matrices were generated, one set of 5 matrices for pair-wise distances among urIs, corresponding to each of the 5 distance metrics described above. The other set of 5 distance matrices was for pair-wise distances among sessions/ users. The fuzzy C- medoids relational clustering method was then used to cluster the data. This algorithm does not figure out the number of  E = - (~i ~j aij I MIg aijl M) (18)  clusters by itself, that. number must be provided as an input parameter to the algorithm.

For each value of number of clusters, the algorithm was run 30 times. The best clustering  ("best" decided based on the value of the objective function minimized by the algorithm) from each batch was used to evaluate the scores mentioned in section 2.3 above.

The whole thing was run 42 times for each distance measure and the average scores, with standard deviation in parentheses, are tabulated in Tables I-II . The first column lists the number of sessions (out of the original 2474) for which the intersection between the set of recommended urIs R and the set of visited urIs V was nonzero, Le., at leastone of the recommended urIs was visited. This is the primary of aspect of the subjective scores we are concerned with. For those sessions for which R nVi: 0, the average relevance and completeness is listed in columns 2 and 3 (the average was computed over the total number of sessions with non-zero intersection). In the tables, the 4th column (titled IBI) shows the average number of bad recommendations averaged over those sessions for which R nV = O. The fact that the average is 1 or 2 indicates that a small number of useless urIs were recommended when there was no intersection (between recommended and visited) which is good. The first 4 columns in the tables list the subjective scores. strategies. First strategy is to cluster on urIs alone (reduced matrix dimensions 2474x900). In this case all sessions had non-zero Rn v.

The last column lists the entropy of the induced matrix. In each case (row of the table), the induced matrix has the same dimensions as the original incidence matrix (Le., 2474 x 3013). The entropy of the original incidence matrix was E = 13.3582. Table 1 (looking at the "number intersected" column)demonstrates that when the number of clusters is large (Le,the distances between elements within a cluster are small), the Euclidean norm distance metric leads to (relatively) better subjective scores.Table 3 compares different  Hence there are no sessions to average the bad ecommendations over, which is why the corresponding entry indicates "not applicable". Second strategy is to cluster on sessions (reduced matrix dimensions 35x3013). The third row indicates the following strategy: cluster on urIs first to obtain a reduced matrix 2474x900, and then cluster this matrix on sessions (so that overall reduced matrix is 35x900). This clustering of sessions is in general different from the one obtained in in the second strategy. The session-cluster- membership mapping generated by this two step process is then applied to the original matrix to generate an alternate session-clustered matrix of size 35x3013 and the subjective scores and entropy for this reduced matrix are indicated in the table. The last two strategies yield overall reduced matrix of size 35x900 and test if the ordering of reduction steps matters. The table demonstrates that clustering urIs leads to good completeness, whereas clustering sessions leads to better relevance. For the most part the induced entropy is better (lower) when the    subjective scores are better. We are further investigating the few cases where the scores go one way and the entropy goes the other way.

