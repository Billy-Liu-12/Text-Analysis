First-order Frequent Patterns in Text Mining

Abstract-In this paper a universal framework for mining long first-order frequent patterns in text data is presented. It consists of RAP, an ILP system for mining maximal first-order frequent patterns, and two types of predefined background knowledge. Two methods of using generated patterns for solving text mining tasks are described: propositionalization and CBA (class based association). A new variant of the CBA rule based classifier is proposed. The framework is used for solving three text mining tasks: information extraction from biomedical texts, context-sensitive text correction of English and morphological disambiguation of Czech. The distributed mining of frequent patterns is described and its influence on mining in text is discussed. It is shown that frequent patterns as new features for propositionalization usually provide better results than CBA.

Index Terms-Artificial Intelligence, Neural Networks, Genetic Algorithms, Spiking Neuron Models, JASTAP

I. INTRODUCTION  There are many tasks in text mining and natural lan- guage processing (NLP) which can be efficiently solved using machine learning. For example, the instance based learner Timbl was used for morphological disambiguation while other classifiers like the Naive Bayes or SVM are frequently used for information retrieval and extraction.

Attribute-value (propositional) systems are the ones most typically used. Nevertheless, it is difficult to express text data in the form of feature-vectors. The transformation tools often produce a very high dimensional, sparse data. Many of these problems can be solved using relational data mining (RDM) [1]. Inductive logic programming (ILP) is concerned with RDM. It integrates machine learning and logic programming.

The ILP system generates a logic program from the set of positive and negative examples, and from a given a priori background knowledge in first-order logic. It has been shown that ILP systems can be successfully used in many NLP tasks [2], e.g. for the morphological disambiguation of Czech [3] and Slovene [4], [3]. Although almost all systems share some properties they are usually tuned for solving only one task.

It is known that many data mining tasks can be regarded as a mining offrequent patterns. A frequent pattern [5] is a conjunction of literals which covers at least or records from a database. The value a is referred to as a minimal frequency threshold as given by the user. WARMR, the first system for finding frequent patterns in first-order logic was presented in [6]. In our earlier work [7], we developed RAP, a system for mining maximal first-order frequent patterns in multirelational  data. We have shown that these patterns can be successfully deployed as new features in classification tasks.

In this paper we introduce a universal framework which utilizes long first-order frequent patterns for text mining. We describe a general background knowledge for the RAP system, and manner in which can be extended, for solving particular text mining tasks. We propose new method of applying found frequent patterns for constructing a classifier and compare it with propositionalization [8]. This new method is based on a CBA (class based association) rule based classifier presented by Liu et al. in [9]. Propositionalization is a method which transforms a relational problem into its attribute-value (propo- sitional) representation. We compare propositionalization and CBA classifier on information extraction, context-sensitive text correction and partial morphological disambiguation of Czech.

Mining in relational data is very expensive and it may be impossible to process large amounts of data. Distributed computing offers a viable solution. From our point of view, a distributed system is a parallel system whose nodes (computa- tional units - computers) do not share any resources (memory, peripheries, system clock, etc.). Joshi et al. in [10] provide an overview of the distributed mining of frequent attribute- value patterns. For multirelational bioinformatics data, the PolyFARM system [11] was developed. In our experiments we utilize dRAP [12], a new version of the RAP system for independent distributed mining. We show that distributed min- ing of frequent patterns improves accuracy of the classifiers.

The structure of the paper is as follows. In the next section we define terms used in this paper. Then RAP, dRAP and the general background knowledge are described (Section III).

In Section IV feature construction and the CBA classifier is proposed. The global settings of experiments are stated in Section V. The experiments are in sections VI, VII and VIII.

The results are discussed in Section IX and the last section brings conclusions.



II. FREQUENT PATTERNS  Frequent patterns were introduced by Aggarwal and Imielin- sky [5] as subsets of items in a transactional database. A much more general definition was given by H. Mannila and H. Toivonenem [13]. Frequent patterns are defined as conjunc- tions of literals. In this paper we will follow this definition.

Firstly, we need to specify what a coverage is.

Let us have background knowledge 13, database r and formula ~o. The formula o covers example e C r if and only  0-7803-9365-1/05/$17.00c2005 IEEE344    if B U {fo} e. The support of the pattern o is defined as supp(r,y5) ={e eeC rABU{}A e}U (it is the number of the covered examples).

Now we can define a frequent patterns and a frequent pattern  mining task. For a database r, language L of all possible formulas and a minimal frequency threshold or, we say that conjunction of literals o C L is a frequent pattern if and only if supp(r, 0) > v. The computation task is to find theory Th(L2 r, supp) = {f Co L or < supp(r<o)}.

We define a partial order -<q on patterns from L. We say  that 9oi q 02 is true if the pattern oi is more general than the pattern 02 (or (02 0-subsumes (oi) for 1, 2 C L.

The frequent pattern, whose extensions are not frequent, is called a maximalfrequent pattern. Obviously each frequent pattern is a generalization (sub-pattern) of some maximal one.

We also say that a frequent pattern is subsumed by a maximal pattern.



III. RAP, DRAP AND THE BACKGROUND KNOWLEDGE  A. RAP  In our earlier work we developed the RAP system [7]. RAP is a system for searching long first-order frequent patterns in dense data. Three different search strategies have been implemented in RAP. Pre-defined as well as user-defined heuristics can be employed, and many operators for pruning the search space are available.

B. dRAP  Although RAP exploits several improvements to increase the performance, it may be impossible to process large amounts of data in a reasonable time. The main problem is usually caused by using large volumes of data and too complex background knowledge.

It seems that parallel and distributed computing can help to partially solve this problem. By separating the data into several parts which are processed on independent computers, the time taken by the mining system can be reduced.

We implemented the dRAP-INDEPENDENT [12] algorithm  which is able to mine patterns in horizontally fragmented data.

It is based on the Savasere' distributed algorithm [14]. Each node first finds all the frequent patterns in its fragment of the database. Then each node sends these patterns to other nodes. At the end, nodes compute the global frequency of the patterns.

C. Background knowledge  We designed two different sets of general background knowledge predicates, B1 and B2. They consist of predicates which specify general properties of a given focus word (focu- Wordl21), for example, that a given position in the sentence is a punctuation (isPunct/2), a quotation mark (isQuot/2) or that the first letter is capital (begCap/2), etc.

1The meaning of the term focusWord/2 is: "focusWord is a two-place predicate symbol."  The difference between B1 and B2 lies in a manner of exploring the context of the analysed word. BU uses a lit- eral hasWord/3 whose first argument determines the relative position of a word with respect to the focus word (e.g. -3 means the third word to the left). The background knowledge BU does not use information about a position of a word in the sentence and only introduces an arbitrary word from a context.



IV. FREQUENT PATTERNS FOR CLASSIFICATION  There are two possible ways to use frequent patterns for classification, propositionalization [7], [15] and rule based classification [9].

A. Propositionalization  It has been shown that frequent patterns, especially frequent Datalog queries, can be successfully used as new boolean features [15]. Learning discriminant functions can be realized by reformulating the relational problem into an attribute-value form and then applying an efficient propositional learner.

This process is usually called propositionalization [8]. In the attribute-value representation, examples are usually rep- resented as feature-vectors of a fixed size in the form fi = vi A f2 = V2 A ... A fao Va where fi are the features and vi the values.

A new feature is defined [8] as a conjunction of (possibly  negated) literals  (1)  where i, ni C N, X refers to an individual (a line in a relational table or example) and Vk C N literal Liti,k is defined in background knowledge B.

Obviously, the right side of the equation (1) (the body of the clause) can be replaced with a frequent pattern o.

Features may be constructed to describe a subset of the training examples that are in some way interesting. Techniques for finding interesting subgroups that are disjunctive [16] are used here. Or features may be constructed to have the same value for a sufficiently large proportion of not necessarily disjunctive examples. Methods for finding frequent patterns (itemsets, datalog queries) can be employed. In our work, we follow the latter.

B. Classification Based on Associations  Another method is to use frequent patterns as a set of classification rules. This approach has been introduced by Liu et. al [9]. They designed a CBA classifier (Classification Based on Associations) which uses so called class association rules (CAR). In this work we define CAR as follows.

Let us have a database r, frequent pattern o, a set of possible classes C and a total one-place function Xc: r -> C which assigns a class from C to the example e C r. Further we define function p such that  p(c) supp(rc, o) supp(r-, W0)   fi (X) : Lt'ti, 1 . ... . Liti,ni -    TABLE I  INFORMATION EXTRACTION BASED ON PROPOSITIONALIZATION AND CLASSIFICATION WITH SMO (SVM), J48, NAIVE BAYES (NB) AND INSTANCE  BASE (IB 1) LEARNERS: PRECISION/RECALL/F1 MEASURES FOR POSITIVE INTERACTIONS OBTAINED BY USING MAXIMAL PATTERNS (DM), FREQUENT PATTERNS (DF) AND ALL PATTERNS WHICH COVER AT LEAST ONE EXAMPLE (DE) ARE DISPLAYED.

CIS.

SVM J48 NB IB1  Serial DM DF DE - - .34l.30l.32 - - .35l.20l.26  .14/.06/.08 .14/.15/.14 .18/.18/.18  .20/.19/.19 .21l.26l.23 .19l.26l.22  Distributed DF  .36l.071.12  .17/.13/.15  .40l.33l.36  .50/.03/.07  .27l.24l.25  .11/.17/.14  DE .31l.39l.35 .33l.22l.27 .34l.26l.29 .24l.31/.27  if supp(rc, ') :4 0, p(c) = oc in other cases and where r= {e C r Xc(e) = c} and r- = r \ rc. A class association rule is an expression o '-* c for a c C C such that p(c) maxccc{P(c) }- The generated class association rules form a committee which determines a class of unseen examples. We use two methods for determining class:  . classification by majority: the most frequent class is assigned. This method is appropriate for cases where we have equal number of classification association rules for each class, they do not cover examples from other classes and all the examples are covered.

. sequential classification: an ordering of classes and two values (MinCov and MinN um) for each class must be specified. The classifier then uses class association rules related to the first defined class c. The class c is assigned to a given example, if some rule o such that supp(rc, o) > MinCov covers the example or is covered with MinNum rules for c. In the next step the uncovered examples are processed in the same manner with rules for the next class.

This method gives better results than the classification by majority if the number of rules for each class is not equal or if an ordering on classes is defined (there are some preferred classes in the data).



V. DESCRIPTION OF EXPERIMENTS  We evaluated both approaches described in Section IV for the three different tasks: information extraction, context- sensitive text correction and partial morphological disambigua- tion of Czech.

All experiments were performed on AMD AthlonTM XP 2500+ computers with 756 MB of memory. The classifiers SMO (support vector machines [17]), J48 (an implementation of the Quinlan's decision trees algorithm C4.5 [18]), Naive Bayes (NB) [19] and instance base learner (IBi) [19] from the Weka2 system [20] were used with default parameters.

Distributed mining of frequent patterns was performed on four computers.

To evaluate the quality of generated models we utilized clas- sical metrics from machine learning and information retrieval: precision (ratio of the number of positive examples classified  2http://www.cs.waikato.ac.nz/ml/weka/  as positive and the total number of examples classified as positive), recall (ratio of the number of positive examples classified as positive and the total number of positive examples in the dataset), F1 and accuracy measures (see e.g. [21]). The description of experiments is in the following three sections.



VI. INFORMATION EXTRACTION FROM BIOMEDICAL TEXT  The information extraction (IE) task we solved was that one proposed in Learning Language in Logic 2005 (LLLO53) workshop. The aim was to learn IE rules which can be used to extract information on gene-protein interactions from biomedical text. The goal was to extract the agent (proteins) and the target (genes) of genic interactions from a sentence.

The dataset consists of words, their lemmas, lists of morpho- syntactic relations in a sentence, agents, targets and lists of interactions (pairs agent-target4). Data also contain a dictio- nary of all name entities, i.e. candidate agents and targets. We searched rules to determine whether words from a sentence which belong to the dictionary interact or not.

An example of an analysed sentence is: "GerE stimulates  cotD transcription and inhibits cotA transcription in vitro by sigma K RNA polymerase, as expected from in vivo studies, and, unexpectedly, profoundly inhibits in vitro transcription of the gene (sigK) that encode sigma K." It contains following interactions: GerE-cotD, GerE-cotA, sigma K-cotA, GerE- SigK and sigK-sigma K.

A. Background knowledge and settings For generating frequent pattern we added new predicates for  morpho-syntactic relations to the B2 background knowledge.

Each pattern begins with a literal which introduces two entities (genes or proteins). There are predicates for exploring words which are syntactically related to these genes and predicates such as noun/2, isobj/2, issubj/2, etc.

We used all 576 (103 positive, 473 negative interactions)  examples. The value of the minimal frequency was O ?O and the best-first search was employed. All frequent patterns up to the length of four literals which were subsumed by some known maximal pattern were pruned. The time of execution was limited to 15 minutes. The tests were performed on 660 gene-protein pairs.

3http://www.cs.york.ac.uk/aig/111/11105/ 4See http://genome.jouy.inra.fr/texte/LLLchallenge/  for details.

TABLE II  INFORMATION EXTRACTION WITH USING SEQUENTIAL CBA CLASSIFIER:  THE VALUES OF THRESHOLDS MinCovIMinNum ARE IN COLUMNS T+H (POSITIVE INTERACTION) TH (NEGATIVE INTERACTION).

PRECISION/RECALL/F1 MEASURES OBTAINED BY USING THE ALL  CANDIDATE PATTERNS WHICH COVER AT LEAST ONE EXAMPLE (DE) ARE  IN COLUMNS DIS+ (BOTH, NEGATIVE AND POSITIVE RULES WERE USED)  AND DIS WITHOUT NEGATIVE RULES.

Serial Dis- Dis+  .321.201.25 .17/.20/.19  .35/.11/.17 .12/.11/.11  Distributed Dis- Dis+  .361.281.31 .211.281.24  .48/.19/.27 .19/.19/.19  among (7,119 occurrences) and between (13,378 occurrences) for classification.

A. Background knowledge and settings  We added a predicate hasTag/3 to the background knowl- edge L1 and B2. It returns a part-of-speech tag of a given word. The new background knowledge are further reffered as Us and US.

Before learning we split data to training (16,398 words) and test set (4,999 examples). Left and right contexts of the length of five words were used. We set the minimal frequency threshold to the value of 100 of the training set. The mining time was restricted to 90 minutes.

B. Results  Ten frequent patterns were generated when the RAP system was used. Twenty two patterns were found in the distributed case. Table I displays results of the propositionalization. The symbol "-" means that the learner marked all interactions as negative. We generated new features not only from the maximal patterns (the columns marked as DM in Table I), but also from all found frequent patterns (DF) and all candidate patterns evaluated by RAP which cover at least one example (DE). We obtained 10/33/290 (DM DFIDE) features in the serial case and 22/60/705 features when distributed computing was performed.

We used all DE patterns for classifying with majority based  CBA classifier too. The values of the thresholds MinCov and MinNum were determined regarding to the best results obtained in [22]. As we can see in Table II, the negative rules do not improve the performance of the classification. The best results were for distributed mining and positive rules.

We also tried to exploit the found patterns as new features  for the Aleph5 system [22]. Only serial mining was performed and the value of minimal frequency threshold was set to 10 interactions. When frequent patterns were added to the Aleph' background knowledge, the recall and F1 measure increased (recall from 42.5 to 64.8 and F1 from 42.5 to 47.6). Precision decreased from 42.5 to 37.6. More details are available in [22].



VII. CONTEXT-SENSITIVE TEXT CORRECTION  Classical spell checkers do not use context and, therefore, are not able to detect errors like that a correct word is used in a wrong place. For example, in the sentence "I'd like a peace of cake.", the word peace is a correct English word but the intended word was piece.

We used a variant of TDT26 English corpus (Carlson et al.

[23]) which was morphologically tagged with SNoW-based part-of-speech tagger. The corpus consists of about one million English sentences from six news sources. We chose the words  5http://web.comlab.ox.ac.uk/oucl/research/areas/ machlearn/Aleph/aleph.html  6The original corpus is available on http: //www. ldc. upenn. edu/ Projects/TDT2/. The tagged version can be found on http://12r.

cs.uiuc.edu/cogcomp/Data/Spell/.

B. Results  The serial RAP algorithm found 18 (BU) and 15 (US) maximal patterns. An example of a pattern generated by using Us is:  key(A), focusWord(A,B), hasWord(1,B, C), begCap(A, C), hasTag(A,C,'NNP), hasWord(2,B,D), hasTag(A,D,'CC).

It covers 16 examples from the class "among" and 1,379 examples from the class "between". The pattern can be in- terpreted as a rule which says: "If the part-of-speech tag of the first right word which follows the focus word is 'NNP', begins with capital and the tag of the second right word is 'CC', then the focus word should be between." Precision of the rule is 98.9 and a fragment A of text covered by this rule is: "... . during/IN the/DT World/NNP Cup/NNP semifinal/NNP [between/IN]B England NNPC and CCD Germany/NNP in/IN 1990/CD... .

When the background knowledge US was used, the predi-  cates hasWord/3 were replaced with the predicate rightWord/2 and we get pattern:  key(A), focusWord(A,B), rightWord(B,C), begCap(A,C), hasTag(A,C,'NNP), rightWord(C,D), hasTag(A,D,'CC).

Because only the relative position is determined, the sup- port of the pattern increased. It covers 3,095 examples from the class "between" and 92 from the class "among". Pre- cision of this pattern is 97.11 00. A fragment of a text covered with this pattern is: ... the/DT last/JJ time/NN that/IN relations/NNS [between/IN]B the/DT United NNPC States NNPanother c and CCD China.NNP... ". As we can see, the precision of patterns may decrease in the case of US. On the other hand, the large increase of the support repays this lost and additional patterns may be generated. For example, RAP found pattern:  key(A), focusWord(A,B), leftWord(B, C), hasTag(A, C, 'TO), rightWord(B,D), hasTag(A,D,'CC), rightWord(D,E), isPunct(A,E),  which covers 237 examples from the class "between" and only 7 from the other one. There is no frequent pattern (for 1 %  7The indexes represent variables from the pattern.

T+ T-H H 4/2 3/2 5/3 3/2    TABLE III  CONTEXT-SENSITIVE TEXT CORRECTION BASED ON PROPOSITIONALIZATION, CLASSIFICATION WITH SMO (SVM), J48, NAIVE AND BAYES (NB)  LEARNERS: PRECISION/RECALL/F1 AND ACCURACY (ACC.) MEASURES OBTAINED BY USING FREQUENT PATTERNS ARE MENTIONED FOR EACH CLASS  (IW: AMONG -AM. AND BETWEEN -BET.).

Serial L31 Us2  Prec./Rec./F, Acc. Prec./Rec./F, .61/.80/.69 .691.801.74 .871.721.79 .881.811.84 .631.751.68 .691.801.74 .851.761.80 .881.81/.84 .601.781.68 74 .621.90/.73 .861.721.78 .93/.70/.80  Distributed  Acc.

.80  .80  .77  Prec./Rec./Fl Acc.

.69/.71/.70 79 .841.831.84 .721.731.73 .861.851.85 .81 .581.81/.67 73 .871.681.77  ,2 Prec./Rec./F1 Acc.

.701.781.73 .871.821.85 .711.771.74 .871.831.85 .581.831.69 74 .881.681.77  TABLE IV  CONTEXT-SENSITIVE TEXT CORRECTION WITH CBA (CLASSIFICATION BY  MAJORITY), 131 AND 10 % MINIMAL FREQUENCY THRESHOLD. NUMBER S  OF CLASS ASSOCIATION RULES (#), PRECISION (PREC.), RECALL (REC.),  F1 AND ACCURACY (ACC.) MEASURES ARE DISPLAYED FOR EACH CLASS.

Rules IW max am.

bet.

freq am.

bet.

Serial # Prec./Rec./F,    Acc.

.71/.86/.78 65  .65/1.0/.79 56  Distributed # Prec./Rec./F1 Acc.

9 .621.501.55  31 .761.831.80 .72  12 .591.641.62 38 .801.761.78 .71  minimal frequency) equivalent to this one in the case of the background knowledge US.

By using only maximal patterns for propositionalization  the accuracy was about 700% for all learners (the base line accuracy is 65.3 %). When distributed computing was per- formed, 40 and 55 maximal patterns were found and accuracy increased to 77.7% (J48) and 76% (SVM). Table III gives results for using all frequent patterns as new features. As we can see, the results are better. That is because more general patterns increase the recall of the theory. By using the Us background knowledge and distributed computing better accuracy with the J48 and SVM learners was obtained. On the other side, the distributed mining of patterns with US increased accuracy only for J48. In the case ofNaive Bayes, the accuracy was worse for both background knowledge when distributed computing was utilized. We observed an opposite effect when maximal patterns were used. In this case, accuracy increased from 69.2 ? (65.4 O) to 71.7 ? (66.8 O) for background knowledge L3 (y).

In fact, all (in the case of LUS) and 14 of 15 (US) frequent patterns were transformed to classification rules for the class "between" when the serial mining was used. Similarly, 31 of 40 (LUS) and 51 of 55 (US) class association rules for the class "between" were generated when distributed mining was performed. It means that too small number of class association rules were created for the class "among" such that pure results of CBA classifier were obtained. To get a large number of CAR for "among" we tried to generate another set of frequent  patterns. We set 1 as a value of minimal frequency threshold and limited the execution time to 90 minutes. Although the number of patterns related to "among" increased the results for CBA and for propositionalization were worse than in the case the value of minimal frequency threshold was O ?O. Table IV shows results for the background knowledge US. The best results with CBA classifier were obtained for US, 100% of the minimal frequency threshold and the distributed mining of frequent patterns. In that case, 21/61 class association rules for the class "among"/"between" were generated and the value of accuracy measure was 74 0.



VIII. MORPHOLOGICAL DISAMBIGUATION OF CZECH  We evaluated both approaches described in Section IV also for the morphological disambiguation of Czech [24]. For simplicity, the task was reduced such that only word je was processed. This word has two readings8 which are: pronoun them (e.g. "I see them.") and verb to belis (e.g. "He is a driver"). For classifying we used sentences from DESAM [25], an annotated corpus for Czech. We used two versions of this data - unambiguous and ambiguous. The second one was generated by adding all possible grammatical readings to each word.

A. Background knowledge and settings We designed a literal for assigning one or more grammatical  readings to the words introduced by the predicates from L1.

We used the full range of tags from the DESAM corpus (it covers part-of-speech, gender, number and case). The contexts consisted of two left and right words closest to the word je (focus word).

All the experiments were performed with the following setting of the RAP and dRAP systems. The value of the minimal frequency threshold was set to 10% of the size of the learning set and all prefixes of frequent patterns which had already been shown to be frequent were pruned. We used 50 examples from each class (noun - k3, verb - k5) when RAP was used and 200 examples when distributed mining on 4 nodes was performed. The pruning criterion was relaxed for distributed mining. The pruning was applied only to the  8There is in fact a third reading ofje - it can be an interjection.

CIs. IW SVM am.

bet.

J48 am.

bet.

NB am.

bet.

TABLE V MORPHOLOGICAL DISAMBIGUATION OF CZECH BASED ON  PROPOSITIONALIZATION AND SMO (SVM), J48 AND NAIVE BAYES (NB) LEARNERS.

Cls. Sense SVM k3  k5 J48 k3  k5 NB k3  k5  Serial Prec./Rec./F, Acc.

.85/.64/.73 760 .71 .88 .79 760 .98/.47/.64 730 .65 .99 .79 73.0% .76 .807 7785% .79/.75/577  Distributed Prec./Rec./F1 Acc.

.801.891.84 83 ? .881.781.83 5/? .68 .90/.77 73.8% .85/.58/.69 .861.791.82 8 .811.871.84 83.0%  TABLE VI  RESULTS OBTAINED BY USING THE CBA CLASSIFIER (CLASSIFICATION  BY MAJORITY) FOR MORPHOLOGICAL DISAMBIGUATION.

Serial Prec./Rec./F, 781.751.77 .821.681.74  Acc.

71.2% 35  Distributed Prec./Rec./F1 Acc.

.86/.71 .77 767% .79/.83/.81 7.0  patterns which consisted of 5 or more literals. The time of execution of dRAP was shortened to be the same as in the serial case. None of the training data contains any ambiguity.

The evaluation of the generated models has been performed on 300 unseen examples from each class. This data was ambiguous.

B. Results  By using the serial RAP algorithm 42 frequent patterns have been generated (all these patterns were used for propositional- ization). The execution took approximately 120 seconds. When the p criterion has been exploited (see Section IV-B), 20 class association rules were generated for the class k3 and 19 for the class k5. There have been 3 patterns for which p(k3) = p(k5).

They have not been used for constructing the classifier. When dRAP was employed, the number of patterns increased to the value of 72. It is 1.82 more than in the serial case, but we must bear in mind that 400 examples have been used, not 100. 35 (k3) and 36 (k5) class association rules were generated.

The results of classification are displayed in Tables VI and

V. The precision, recall, F1 measure and accuracy obtained by classifying test data are shown. Columns # give a number of class association rules.

As we can see in both tables, the distributed processing  leads to much better results. There are two reasons for this.

Firstly, we have used a larger sample of data and so stronger associations could be found. The second reason is that we generated more features and classification rules. There is only one exception, namely the line related to the J48 classifier and the class k5. It is most likely caused by using a similar measure for generating frequent patterns as the classifier uses.

The relatively low accuracy has been achieved because of the small number of patterns. We can see, that the generated theory does not cover 5-160% of learning examples.



IX. DISCUSSION  The results above display some interesting aspects of ap- plying long first-order frequent patterns in text mining. The most important observation is that large number of frequent patterns used as features for the propositionalization or for constructing of a CBA classifier lead to better results. The only exception was in the case of context-sensitive text correction and classification with Naive Bayes classifier.

Maximal patterns and patterns that they subsume are better  for solving the problem with propositionalization. It is also possible to use infrequent patterns (candidate patterns gener- ated by the RAP system). On the other hand, only maximal patterns seems to be better for constructing the CBA classifier (see Table IV). When all frequent patterns are utilized it is more difficult to determine values of the thresholds MinCov and MinNum.

In our experiments the propositionalization approach out- performs CBA. The main problem of the CBA classifier is that its performance strongly depends on the number of the class associations rules and values of thresholds given by user.

It seems that long frequent patterns we generated are not sufficient for solving the problems we mentioned. Neither CBA nor propositionalization provide better results than meth- ods mentioned in the stat-of-the-art. For example, Carlson et al. [23] presented algorithm for solving the problem of context-sensitive text correction which performs at 98 00 level of accuracy9. Obviously the results of our approach may be improved by enlarging the background knowledge such that they would utilize all the information in data. Lastly, frequent patterns as new features (predicates) for another relational algorithm (e.g. Aleph) can improve its performance [22].



X. CONCLUSIONS  In this paper we have presented two methods of using first- order frequent patterns in text mining, the propositionalization and the classification, based on association (CBA). These methods were compared on three different tasks: informa- tion extraction from biomedical texts, context-sensitive text correction and morphological disambiguation of Czech. We showed that distributed computing can improve performance of both methods, because of mining in a larger sample of data and a larger number of features. In our experiments the propositionalization outperformed CBA.

We plan to design and evaluate new criteria for generating  CAR and CBA. We will design and evaluate automatic method for setting parameters such as minimal frequency threshold, MinCov and MinNum.

