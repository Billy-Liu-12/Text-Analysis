Literature Characterization and Similarity Retrieval Based on Hierarchical Clustering

Abstract  The growing number of literature in journals database raises a new and challenging search problem: locating de- sired literature. Traditional keyword search is insufficient: the specific literature users require is possibly not captured.

We introduce a new algorithm of hierarchical clustering.

With this algorithm, we cluster the keywords into a concept tree, then we turn every literature into an induced tree. We propose a new method for theses retrieval, which based on concept similarity. This method improves in recall and pre- cision.

1. Introduction  Researchers are increasingly relying on web as method- ology for literature retrieval. The growing number of litera- ture available on the Web raises a challenging search prob- lem: locating desired literature. Currently, these databases provide only simple keyword search. As one considers search for literature in more detail, it becomes apparent that the keyword search Paradigm is insufficient for four rea- sons.

(1) Keywords do not express the accurate user need. As involving semantic mapping of queries and user need, users are difficult to accurately select keywords for search. Cur- rent literature search engines return literature results if liter- ature contains the keywords in the search, such search may miss results.

(2) Keywords do not suffice for accurately specifying the contents of literature. If keywords were too extensive, it would be difficult or impossible to retrieve the relevant doc- uments.

(3) Polysemy. As the keyword matching technology is difficult to solve the ambiguity of the term, it will return a large number of irrelevant documents.

(4) Synonyms. User?s search keywords sometimes does not appear directly in the text, but with synonyms of key- words or other word-building way, so that documents can not be retrieved.

To address the challenges involved in searching for lit- erature, this paper describes the novel techniques we have developed to support these types of searches, and experi- mental evidence that shows the high accuracy of our algo- rithms. In particular, our contributions are the following:  (1) We describe algorithms for supporting similarity search. The key ingredient of our algorithm is a novel clus- tering algorithm that groups keywords of literature into se- mantically meaningful concepts. These concepts are then leveraged to determine similarity of keywords of literature.

(2) We describe a novel algorithm for supporting simi- larity search of binary tree.

(3) We describe a detailed experimental evaluation on a set of 30 papers. The evaluation shows that we can pro- vide both high precision and recall for similarity search, and that our techniques substantially improve on naive keyword search.

2 Keywords clustering algorithm  The main components of Literature are the following: ti- tle, abstract, keywords, text, additional information (author of the literature, publications information), and keywords is most effective representation of the theme of the litera- ture. As keywords tend to be highly varied given the use of synonyms, hyponyms, and different naming rules, retrieval based on keywords matching is not an effective way.

We exploit the keywords in literature Collection to clus- ter terms into meaningful concepts. As we shall see later, using these concepts, it greatly improves our ability to iden- tify similar keywords and hence find similar documents.

We begin to describe the keywords collection prepro- cessing and then describe the clustering algorithm.

2.1 Keywords collection preprocessing  Keywords preprocessing is a procedure which can be di- vided mainly into three operations:  (1) Selection of all literature keywords to construct key- words collection.

World Congress on Software Engineering  DOI 10.1109/WCSE.2009.124   World Congress on Software Engineering  DOI 10.1109/WCSE.2009.124   World Congress on Software Engineering  DOI 10.1109/WCSE.2009.124   World Congress on Software Engineering  DOI 10.1109/WCSE.2009.124     (2) Elimination of stop words with the objective of fil- tering out words with very low discrimination values for retrieval purposes.

(3) Stemming of the remaining words with the objective of removing affixes (i.e., prefixes and suffixes).

2.2 Hierarchical clustering algorithm of keywords  We base our clustering on the following heuristic: key- words tend to express the same concept if they occur to- gether often. The cohesion of a concept-the connections be- tween parameters inside the concept-should be strong; the correlation between concepts-the connections between pa- rameters in different concepts-should be weak. This heuris- tic is validated by our experimental results. We use it to cluster keywords by exploiting their conditional probabili- ties of occurrence in literature.

Frequent and rare keywords should be left unclustered; strongly connected keywords in between are clustered into concepts. Firstly, not clustering frequent keywords is con- sistent with the IR community?s observation that such tech- nique leads to the best performance in automatic search expansion. Secondly, leaving rare keywords unclustered avoids over-fitting.

Specifically, we are interested in association rules of the form:  t1 ? t2(s, c)  In this rule, t1 and t2 are two terms. The support, s, is the probability that t1 occurs in a literature, i.e., s = P (t1); The confidence, c, is the probability that t1 occurs in an input or output, given that t2 is known to occur in it; i.e., c = P (t1|t2).

Agglomerative clustering is a bottom-up version of hier- archical clustering. Each object is initialized to be a cluster of its own. In general, at each iteration the two most similar clusters are merged until no more clusters can be merged.

In our context, each term is initialized to be a cluster of its own; i.e., there are as many clusters as terms. The algorithm proceeds in a greedy fashion. It sorts the associ- ation rules in descending order first by the confidence and then by the support. Infrequent rules with less than a mini- mum support ts are discarded. At every step, the algorithm chooses the highest ranked rule that has not been considered previously. If the two terms in the rule belong to different clusters, the algorithm merges the clusters. Formally, the condition that triggers merging cluster I and J is  ?i ? I, j ? J, i? j(s > ts, c > tc)  where i and j are terms. The threshold ts is chosen to control the clustering of terms that do not occur frequently.

2.3 Improved clustering algorithm  The basic agglomerative algorithm merges two clusters together when any two terms in the two clusters are closely associated. The merge condition is very loose and can easily result in low cohesion of clusters. To illustrate, suppose there is a concept cluster {web mining,data mining}, and a concept cluster {web page analysis}. If, when keyword ?web mining? occur in a literature, ?web page analysis? often occur in the same literature as well, as a result, the basic algorithm will inappropriately combine the two clusters.

The cohesion of a cluster is decided by the association of each pair of terms in the cluster. To ensure that we obtain clusters with high cohesion, we merge two clusters only if they satisfy a stricter condition, called cohesion condition.

Given a cluster C, a term is called a kernel term if it is closely associated with at least half of the remaining terms in C. Our cohesion condition requires that all the terms in the merged cluster be kernel terms. Formally, we merge two clusters I and J only if they satisfy the cohesion condition:  ?i ? I ? J, ||{j|j ? I ? J, i ?= j, i ? j(c > tc)}|| ? 0.5? (||I||+ ||J || ? 1)  where ||I|| is the number of terms in cluster I .

The keywords in a cluster may not be synonymous, but  have a greater relationship. While the cluster does not con- stitute a true sense of the ?concept?, but it have little impact on the retrieval performance. Algorithm is described as fol- lows:  3 Literature characterization  With the above algorithm, we get C = {c1, c2, . . ., ci, . . .}. Then, we adopt a threshold t ?c < tc and run the cluster- ing algorithm repeatedly. At each step, we adopt a smaller     threshold, re-collect association rules, and then re-run the clustering algorithm. This process continues until finally one cluster is generated.

In the above process, a concept tree is generated as shown in Fig.1.

Figure 1. An example of concept tree  For example, after processing of a literature collection, a concept tree is generated as shown in Fig.2.

Then, each literature is represented as a sub-tree of the above concept tree, keywords of literature is corresponding to leaf nodes of the sub-tree. Thus, for literature which can be represented by keyword vector (t3, t4, t11, t30, t39), the corresponding tree representation is shown as Fig.3.

4 Literature similarity retrieval  4.1 Computing Literature Similarities  In traditional cosine similarity computing methods, the unit vectors of all leaf nodes are vertical to each other, among which the dot matrix result of any two unit vectors is 0 and 1 if the two vectors are the same. There are some great defects of the method, for example, the similarity com- puted using this method between literature th1(t1, t2, t3) and th2(t4, t5, t6, t7) is 0, but the two pieces of literature have great similarity, which we don?t wish sim(thi, thj equals to 0.

Here we describe a new method for computing similari- ties of literature.

The depth of node ti can be defined as the length from root node to this node in the tree, that is, the amounts of the total edges. The height of the tree is represented with h. To two leaf nodes ti and tj of tree U , LCA(ti, tj) represents their smallest common ancestor. Two leaf nodes have at least a common ancestor node ? root node. In figure2, LCA(t1, t7), LCA(t1, t8) = C1.

The unit vector of the leaf node t is defined as ??t and we don?t wish ??t1 ? ??t2 equals to 0, therefore in this model defi- nition ??t1 and ??t2 are not vertical to each other. Considering  Figure 2. Creating a concept tree  Figure 3. Tree representation of keyword vector (t3, t4, t11, t30, t39)  the hierarchical structure, we make ??t1 ???t2 = 23 , because the distance from root node to their common ancestor occupies 3 of the total distance. In the similar way,  ?? t1 ???t8 = 13 , while??  t1 ? ??t9 = 0.

To any two leaf nodes, we define ??ti ? ??tj =  depth(LCAT (ti, tj))/h, whose value is continuous, range from 0 to 1. When LCA(ti, tj is the root node, its value is 0 and 1 when ti = tj .

The vectors corresponding to literature A and B are rep- resented as  ?? A =  ? i ?? ti and  ?? B =  ? j ?? tj . Their dot ma-  trix is defined as ?? A ? ??B = ?ni=1  ?n j=1  ?? ti ? ??tj and the  similarity between A and B is defined as sim(??A ? ??B ) =?? A ???B???  A ???A ???  B ???B .

4.2 Literature retrieval  To the literature user selected, the system intensively cal- culates the similarity of every one, ordering by their relativ- ities and returns those whose sim > tsim.

The clustering algorithm applied in this paper is suitable for similarity retrieval of literature. It can also improve the retrieval based on matching of search items. The process were as follows: 1) Making hierarchical clustering men- tioned in this paper of collections of the keywords collected from the system, generating the concept tree, and represent- ing the literature with concept sub tree. 2) Preprocessing the search keywords user input, including extraction of word roots and deletion of non-use words. 3) Computing the     Table 1. Experimental data Retrieval methods Ret Retrel R(RRE) P(RRC) MSIR 8 7 58.3% 87.5% SIMR (tsim=0.1) 19 12 100% 63.2% SIMR (tsim = 0.3) 16 11 91.7% 68.8% SIMR (tsim=0.6) 6 6 50% 100%  MSIR: Matching search items retrieval; SIMR: Similarity retrieval;  similarity between search vector and literature vector. 4) Returning literature according to the value of the similarity.

5 Experimental evaluation  This experiment is used to compare the algorithm brought out by this paper with the retrieval algorithm based on the matching of items to be located. The latter one match user input with the subject words of literature, in- cluding title, abstract, keywords and text etc. Here we use the term ?RRE? (The Ratio of Retrieval Enough), in which r = RetrelRel , and RRC (The Ratio of Retrieval Correctly),  in which p = RetrelRet , to evaluate the performance of the retrieval methods. The notions ?Rel?, ?Ret?and ?Retrel? represent the collections of relevant literature, user retrieved literature and user retrieved relevant literature respectively.

30 pieces of literature are contained in the experiment, of which 12 pieces are relevant to ?Web Page Ordering? and 18 pieces are relevant to ?Information Retrieval Model?. To- tally, there are 176 keyword items. Firstly, we made clus- tering to the keywords with ts=0.33, tc=0.5. 98 keyword items were clustered to 13 concepts and the remaining 78 items contained 72 low-frequency items, presented at most in 2 pieces of literature, and 6 high-frequency items, pre- sented at least in 5 pieces of literature.

1) Retrieval based on matching of items to be queried User?s Objective was ?Webpage ordering algorithms?  and he inputted ?Page rank? as the keyword to search. 8 results were returned, in which 7 pieces were relevant to the objective.

2) Retrieval based on similarity of concepts Results were relevant to the value of the threshold. When  relevance degree threshold was set to tsim=0.3, user se- lected one of the relevant literature to have a relevance search, 16 pieces were returned, in which 12 pieces were relevant. When set tsim=0.1, tsim=0.6, the results from the experiments were as follows: Table 1  Experimental analysis: literature retrieval effect based on similarity were tested and compared with that based on matching search items. It was obvious from the data in ta- ble 1, that the effect of literature retrieval based on simi- larity was better than that based on matching search items.

Especially, when user could not submit appropriate search items, the RRC and RRP were both very low with tradi- tional retrieval method. By contrast, the retrieval effect with this method was obviously higher, because it determines the similarity between two pieces of literature according to the category that they belong to, not the amounts of the same keywords. Meanwhile, it could be seen from table 1, the performance of literature retrieval based on similarity con- cept is relevant the value of tsi. If the value of the threshold is appropriate, its performance could be much higher than that of keywords search. The next step could be to deter- mine the appropriate value of the threshold through experi- ments on huge amounts of data.

6. Conclusion  In this paper, we propose an algorithm of hierarchical clustering. In this algorithm, the keywords are clustered into a concept tree, then every literature is turned into an induced tree. Based on concept similarity, our method is to implement these retrievals. Our method improves efficiency and precision of recall.

