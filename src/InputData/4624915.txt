Hash Partitioned Apriori in Parallel and Distributed Data Mining  Environment with Dynamic Data Allocation approach.

Abstract  Parallel system is mainly composed of parallel algorithms which are cost optimal. In this paper a parallel algorithm the Hash Partitioned Apriori (HPA) is taken into consideration. HPA partitions the candidate itemsets among processors using a hash function, like the hash join in relational databases. HPA effectively utilizes the whole memory space of all the processors, hence it works well for large scale data mining in a parallel and distributed environment. The optimization technique of dynamic data allocation is discussed for the execution of this application. This technique is applied in a parallel and distributed environment.

Writing parallel data mining algorithms in a distributed environment is a non-trivial task. The main purpose of the proposed method is to meet certain challenges associated with parallel and distributed data mining such as i) minimizing I/O ii) Increasing processing  speed iii) Communication cost.

Keywords   Data Mining, Partitioning, Hash Partitioned Apriori, Dynamic Data Allocation.

1. Introduction   Data mining has attracted a lot of attention  from both research and commercial community, for finding interesting trends hidden in large transaction logs. Although there exist workable sequential  algorithms for data mining (such as Apriori, above), there is a desperate need for a parallel solution for most realistic-sized problems [5].  The most obvious argument for parallelism revolves around database size. The databases used for data mining are typically extremely large, often containing the details of the entire history of a company's standard transactional databases. As these databases grow past hundreds of gigabytes towards a terabyte or more, it becomes nearly impossible to process them on a single sequential machine, for both time and space reasons: no more than a fraction of the database can be kept in main memory at any given time, and the amount of local disk storage and bandwidth needed to keep the sequential CPU supplied with data is enormous. Additionally, with an algorithm such as Apriori that requires many complete passes over the database, the actual running time required to complete the algorithm becomes excessive. [13] The basic approach to parallelizing association- discovery data mining is via database partitioning.

Each node in a parallel system is assigned a subset of the database records, and computes independently on that subset, usually using a variation on the sequential Apriori algorithm described below.

2. Related Works  Algorithms for mining association rules from relational data have been well developed.

Several query languages have been proposed, to assist association rule mining such as [17]. Four different parallel algorithms exist for mining   DOI 10.1109/ICCSIT.2008.63    DOI 10.1109/ICCSIT.2008.63     association rules on shared nothing parallel machines to improve its performance. (NPA, SPA, HPA and HPA-ELD). In NPA, candidate itemsets are just copied amongst all the processors which can lead to memory overflow for large transaction databases. The remaining three partitions the candidate itemsets over the processors. HPA partitions the candidate itemsets using a hash function to eliminate broadcasting which also reduces the comparison workload significantly [18].

3. Mining of association rules   One of the best known problems in data  mining is mining of association rules from a database, so called ?basket analysis?[7], [8]. Basket type transactions typically consist of transaction identification and items bought per transaction.

Association rule mining has a wide range of applicability such Market basket analysis, Medical diagnosis/ research, Website navigation analysis, Homeland security and so on. Association rule and frequent itemset mining became a widely researched area, and hence faster and faster algorithms have been presented. Numerous of them are Apriori based algorithms or Apriori modifications. Those who adapted Apriori as a basic search strategy, tended to adapt the whole set of procedures and data structures as well. Since the scheme of this important algorithm was not only used in basic association rules mining, but also in other data mining fields. Hence compared with other data mining algorithms association rule is preferred in this research work. An example of an association rule is ?if customers buy A and B, then 90% of them also buy C?. The best known algorithm for association rule mining is the Apriori algorithm proposed by R. Agrawal, designating some parameters, such as the number of the transaction, the number of different items, etc [5].

4. Parallelization of association rule mining with HPA   In order to improve the quality of the rule, the  analysis is made on very large amounts of transaction data, which requires considerable computation time. Several parallel algorithms are analyzed for mining association rules, based on Apriori. One of these algorithms, called Hash Partitioned Apriori (HPA), is implemented and evaluated. The data allocation among the processors is also taken care dynamically during the execution  of the HPA algorithm. Depending upon the available size of the memory and contents the swap in, swap out process is done. HPA partitions the candidate itemsets among processors using a hash function, like the hash join in relational databases.

HPA effectively utilizes the whole memory space of all the processors, hence it works well for large scale data mining. The steps used in the research work are as follows.

1. Generate candidate k-itemsets:  All processors have all the large (k ? 1)- itemsets in memory when pass k starts. Each processor generates candidate k-itemsets using large (k ? 1)-itemsets, applies a hash function, and determines a destination processor ID. If the ID is the processors own, the itemset is inserted into the hash table, otherwise it is discarded.

2. Scan the transaction database and count the support value:  Each processor reads the transaction database from its local disk. It generates k-itemsets from those transactions and applies the same hash function used in phase  The processor then determines the destination processor ID and sends the k- itemsets to it. When a processor receives these itemsets, it searches the hash table for a match, and increments the match count.

3. Determine large k-itemsets: Each processor checks all the itemsets it has  and determines large itemsets locally, then broadcasts them to the other processors. When this phase is finished at all processors, large itemsets are determined globally. The algorithm terminates if no large itemset is obtained.

The Pseudo code for the HPA algorithm is {C1p}:= All items assigned to the p-th processor based on hashed value   forall t ? Dp do  forall items x ? t do  Determine the destination processor Id by  applying  the same hash function which is used in item partitioning and send that item to it. If it is its own ID, increment the support _count for the item.

Receive the item from the other processors and increment the support_count for the itemset.

Receive k-itemset from the other processors and increment the support_count for that itemset.

end end  {L1p}:= All the candidates in C1p with minimum support.

Send L1p to the coordinator Receive L1 from the coordinator  While (Lk-1 ? ?) do {Cpk} := All the candidate k-itemsets, whose  hashed value corresponding to the p-th processor forall t ? Dp do  forall k-items x ? t do  Determine the destination processor ID by  applying the same hash function which is used in item partitioning and send that k-itemset to it. If it is its own ID, increment the support_count for the itemset. Receive k-itemset from the other processors and increment the support_count for that itemset.

end end { Lkp}:= All the candidates in Cpk with  minimum support.

Send Lkp to the coordinator Receive Lk from the coordinator k:=k+1 end  Table 1. HPA algorithm abbreviations  Lk Set of all large k-itemsets.

Ck Set of all the candidate k itemsets.

Dp Transactions stored in the local disk of  the p-processor.

Cpk Set of k itemsets assigned with p  processor.

Lkp Set of large k itemsets derived from Cpk   5. Execution of Hash Partitioned Apriori   Each node of the cluster has a transaction data  file on its own hard disk. The produced data was divided by the number of various nodes in the distributed environment, and copied to each node?s hard disk. At each node, two processes are created and executed. One process makes candidate itemsets from previous large itemsets, and sends them to the other process, which puts the data into a  hash table. In the data counting phase, one process generates itemsets by scanning the transaction data file, and sends them to the other processes on the nodes selected by the hash function. The target processes check and increment their hash table values accordingly. During the execution of HPA, itemsets are kept in memory as linked structures that are classified by a hash function. That is all itemsets having the same hash value are assigned to the same hash line on the same node, and their structures are connected with each other to form a list. As an example, a result of HPA in distributed data mining is shown in Table 2.

Table 2. The number of candidate and large  itemsets at each step   Passes Candidate Itemsets  Large Itemsets  Pass 1 NIL 850 Pass 2 312543 25 Pass 3 17 30 Pass 4 5 5 Pass 5 1 0   In this execution, the number of transactions is 10,00,000, the number of different items is 5,000, and the minimum support is 0.5%. It is known that the number of candidate itemsets in pass 2 is very much larger than in other passes, as can be seen in the table. This often happens in association rule mining. Initially the large itemsets are generated and so there is a NIL in the Candidate itemsets.

6. Dynamic Data Allocation in the developed model   It may happen that the number of candidate  itemsets increases dramatically in this step so that the requirement of memory becomes extremely large. When the required memory is larger than the real memory size, part of the memory contents must be swapped out. [3]. The number of candidate itemsets in pass 2 is very much larger than other passes in association rule mining.

For example, when a memory available node does not have enough memory space, the shortage of memory is detected by application processes, so that another node is chosen as a swapping destination afterward. On the other hand, if some other processes begin their execution on a memory     available node which already accepted swapping operations, the swapped out data must be migrated to other memory available nodes to make space on its memory for the new processes [9]. A Data warehouse D contains different data sets like {D1, D2, D3,? Dn} in a geographically distributed environment  {D1, D2, D3,? Dn} ? D where these datasets are assigned to the  different processors {P1, P2, P3 ?Pn}. The dataset assigned to these processors may vary dynamically depending upon the mining process carried out using HPA. If there is any processor idle, then the load will be properly balanced to every processor, so that the communication overhead will be reduced.

7. Performance Evaluation   Transaction data is evenly spread over the  processors local disks. HPA transmits the itemsets to reduce the communication costs. The execution time along with the minimum support is considered as the first parameter. Figure 1 shows the execution time with varying minimum support values. The execution time increases sharply as and when the minimum support increases but after a certain point of time there is a decline in the same which means the number of candidate itemsets generation decreases in each and every passes.

Figure 1. Execution time Vs. Minimum Support Count   The next parameter is the speed ratio for pass2  varying the number of processors with a minimum support value of 0.3%. Figure 2 shows the Number of processors with varying speed.  The graph below  shows a satisfactory speed ratio. This algorithm focusses on the item distribution of the transaction file and picks up the extremely frequent occuring items. It is a linear graph which shows that as and when the number of processors are increased the speed ratio gives a linear increase.

Figure 2. No of Processors Vs. speed  The third parameter is the communication cost  of HPA algorithm is based on the itemsets of the transaction that are transmitted to the limited number of processors instead of broadcasting. Since the size of the transaction data is usually much larger than that of the candidate itemset which is focused on data transfer. In addition during the last phase of the processing, each processor sends the support count statistics to the coordinator where the minimum support condition is examined. This also incurs communication overhead. The total amount of communication for HPA at pass k can be expressed as follows   MkHPA = ?p=1 N?i=1 Tp tipCk * k * ?kip   P: Processor id (p=1,2,3?N) N: Number of processors Tp: Number of pth processor transactions.

tip: Number of items in i-th transaction of p-th processor.

One transaction generates tipCk candidates with a very small value for ?. To know whether a data mining technique is worth executing in parallel, its complexity has to be analyzed.

8. Conclusion   In this paper the parallel algorithm Hash Partitioned Apriori is examined with various performance evaluation factors like communications costs, processor speed up and execution time. Dynamic data allocation is done with the memory available nodes. During execution of the application, this data is accessed by all processes concurrently. The contents of the data are divided by the number of nodes almost equally and each node of the cluster reads its own portion during the execution. The important challenges as execution time, processor speed up, and communication cost are analyzed and the results are given in the performance evaluation.

The implementation of HPA in a parallel and distributed environment with various parameters and dynamic data allocation is clearly discussed. As a method of experiments, a limit value for memory usage of candidate itemsets is set at each node.

When the amount of memory usage exceeds this value during the execution of HPA program, parts of contents are swapped out to available remote nodes? memory. That is to say, the application execution node acquires memory area dynamically from one of memory available nodes when it is needed.

