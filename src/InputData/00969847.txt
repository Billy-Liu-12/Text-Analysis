Quantification And Granulation

Abstract Data mining holds the promise of extracting un-  suspected information from very large databases.

Methods have been developed to build association rules from categorical data. However, often many fme grained rules are generated. Additionally, much real data is not only categorical; it is quantitative. In forming association rules, quantitative values are often r&d to categorical values; this may overly simplify results.

The concern of this work is considering how fme grained rules might be aggregated and the role that non- categorical data might have. It appears that soft com- puting techmques may be useful.

Keywords Association rules, quantitative, granulation, soft com- puting, conceptual hierarchies  1. Introduction Data mining is an advanced tool for the manage-  ment of large masses of data. Data mining is the proc- ess of secondury analysis of large databases. Data m n - ing is secondary analysis because the data were not collected to answer questions now posed. The data is examined in an attempt to discouver if there are pat- terns beyond those that were hypothesized before the data was collected. For example, perhaps we are at looking at long distance telephone call records. The records were originally collected for billing. Secondar- ily, they can be examined to recognize patterns involv- ing such things as: call length, time-of-day, customer calling plan, from where-to-where, etc.

Broadly speaking, data mining is part of the general data exploration activity. The consideration of repeti- tively presented data, such as in a database, is called knowledge discouvery in databases (KDD). KDD in- cludes traditional data exploration techniques such as classical statistical analysis and statistical learning (such as Baysian learning); as well as a variety of other methods. Some methods, are too computationally complex to apply to an entire large data set.

Data mining methods often include elements of ar- tificial intelligence such as learning andor methods to help handle impreciseness. The most common data mining result representations are rules and ussocia- tions. For example:  Rules are the older representation. In data mining, they are often formed from a graph built by some form of inductive learning. Associations are often called association ndes. While this terminology may be un- fortunately confused with the different ?rules? result, it is what we have to work with. Associations are built by directly examining tabular data.

2. Association Rules The problem of finding association rules was intro-  duced by Agrawal [ 11. Association rules meet a neces- sardata mining subgoal of having efficient data struc- tures and algorithms. Their algorithms also have the advantage of being linearly upwardly scaleable.

Once processing begins, discouvering association rules often follows methods based on Apriori [ 2 ] .  A number of other authors have considered efficiently de veloping association rules [16] [21] [29] [34].

Finding association rules is somewhat of an unsu- pervised discouvery process. It may not be ?pure? un- supervised learning as there is often a predefined hierar- chical concept hierarchy structure that is applied to pre- cluster the data; this would be an aspect of supervised discouvery. For example, all brands of beer might be grouped into a single class called: ??Beer.?  Predefmed concept hierarchies may not serve the purposes of data analysis well. Perhaps, implicit in the data, there may be more useful concept hierarchies that go undiscouvered because of pre-processing assump- tions. As with much real world data, the clusters that concept hierarchies represent can possibly be best formed by fuzzy sets; or alternatively, rough sets.

It is a speculation of this work that it may be pos- sible to increase the granularity of discouvered associa- tion rules either through (a) using discouvered concept hierarchies, (b) soft clustering the rules themselves, or (c) increasing the granularity of the data before forming association rules.

Association Rules represent positive associations between attributes. Most commonly, the rules are de veloped from data that is expressed as coIumns in a 011 Boolean categorical matrix. For example, consider the transactions shown in Figure I .

Parts of this work were performed while the author was a visiting at BISC, Computer Science Division, EECS Department, University of California, Berkeley  0-7803-7087-2/01/$10.00 0 2001  mailto:mazlack@uc.edu   transaction  t, tl t2 t 2  t2 t2 t 2  t3 t 3  item quantity chips 1 Miller 1 chips 2 mustard 1 sausage 3 Coke 5 Tuberg 1 Miller 1 Tuberg 2   t, t 2 t3  Chips -ard -age Drink Beer 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1  tl t2 t3 t4 t5  t 7  t o t9 tl0  t6  Chips -ard -age Drink Beer 1 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1  1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1    This is possibly an undesirable oversimplification.

In contrast to Boolean attributes, the values of quanti- tative attributes corresponding to other records can be distinct; i.e., one quantitative value for a quantitative attribute may closely associate with another attribute while a different quantitative value may not.

There are several kinds of information that might be obscured (a) differences of behavior due to diffixing quantities and (b) trends that might be recognizable if data magnitudes were considered.

Several strategies have been pursued to deal with scalar, non-categorical data when mining data and form- ing association rules. The results are usually called quantitative association rules. Some methods reduce data magnitudes, others try to cluster by finding opti- mized ranges in a decision tree.

Dequantification (Discretization): Reducing Data Magnitudes  If the quantitative rule problem can be usefully mapped into the Boolean categorical rules problem, any algorithm for finding categorical association rules can be used to find association rules.

Srikant [38] discussed mining association rules over quantitative and categorical attributes. They dealt with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. For example, a rule might contain a quantified age range: Age = [30,39]; the re- sulting rule might be:  (<Age: [30,39] > and 44arried: Yes>) - Their method first determines the number of parti-  tions for each quantitative attribute and then maps all possible values for each attribute. They partition val- ues into equal intervals.

Their method is illustrated by Figures 3 and 4.

F i p r e  3 shows an example table with three non-key attributes. X and Z are quantitative attributes while Y is a categorical attribute with two enumerated values (Yes, No). The table in F i p r e  4 contains as many Boolean categorical fields as the number of attribute values/i@ervals for each quantitative or categorical attribute. To reduce the count of values to consider, values have been partitioned into intervals. Then, the value of any Boolean categorical field that corresponds to a attribute,v> would be ?1? if the attribute had value v in the original record, and ?0? otherwise. A rule that is present in the table in Figure 4 is:  <NumCars: 2>  {X: 20..29) and fl=Yes) * {Z:2)  Figure 3. Exam&  Figure 4. Example table recast into categorical data.

Generally, this type of method can be summarized *Determine the number of partitions for each quantita- tive attribute. Here lies a decision for each attribute: partition or not? And, if partitioning is decided, what should be the number of partitions?

*Map values to partitions. After this point, only the mapped values are used.

*Find support for each value of all attributes. To avoid minimum support problems, combine adjacent values as long as their support is less than the minimum support threshold.

*Find all sets of items whose support is greater than the minimum support. These are the fiepent itemsets.

*Generate association rules from the frequent item sets.

This simple mapping approach leads to two problems: *Support: If the number of valuedintervals for an at- tribute is large, the support for any particular value can be low. That means, a large number of intervals can cause some rules involving this attribute may not satisfy minimum support.

*Confidence: As the size of the intervals becomes larger, information loss can be encountered.

These two concerns are adversely complementary: if the intervals are too large, some rules may not have minimum confidence; if they are too small, some rules may not have minimum support. To break this chain, when processing each particular valuehterval, all pos- sible ranges over that value/interval need to be con- sidered Unfortunately, this approach is overly computa- tionally complex. Additionally, while it may be a way of dequantifyins values, it will generate many fine grained rules.

Starting with Srikant, a number of workers have addrased the question of optimizing interval design.

Often, but not always, the results have been called optimizedassociation rules [I31 [141 [25] [26] [331 [71.

A satisfactory resolution .has not yet been reached. It may be that dequantification into intervals is not a viable approach to quantitative association rules.

Some workers have focused on trying to discouver the more interesting rules from the numerous rules generated from dequantified association rules. [3 1 ] [32] [38] [39] [I71 [25] [20] [30] [35] [27] Some of the interest approaches work better than others. They all struggle with knowledge acquisition issues. None of them have been outstandingly successful.

.

de table.

Directly Dealing With Magnitudes I n Quantified Association Rules  Cai [8] considers transactions with quantities as supporting ?weighted? rules. Cai wanted to balance be- tween weight and support. Wang [40] also considered weighted association rules. Wang first finds association rules by converting values to Boolean categorical val- ues, then builds the weights onto the association rules.

Aumann [6]  was concerned with the distribution of quantitative attributes. They looked for events that differ significantly from a statistical norm and were then considered to be interesting. Their approach is to look for a subset and its mean (or median, variance) andcompare it to the mean of the whole. This might identify an interesting population subset. Yao [42] presented another statistical approach.

Soft Methods There is a considerable history applying either  fuzzy or rough set techniques to databases. To name a few approaches, there are: databases of fuzzy values, fuzzy query rules, and rough set partitioning. Both fuzzy and rough sets have been used in various data mining efforts. Our focus is on the relatively unex- plored area of rule aggregation, the lightly explored clustering antecedents andor consequents, and the role of quantification.

There have been a number of fuzzy learning algo- rithms for inducing rules from sets. Considerable work has also been done on decision trees. Both concerns are reviewed in Mazlack [24].

Chan 191 presents an algorithm for increasing data granularity that eliminates the need for user-supplied thresholds for support and confidence, and to find nega- tive as well as positive association rules. Using f imy set theory, linguistic terms are used to find the degree to which they characterize records in the database.

Zhang [43] extended the equi-depth (equal sized) par- tition algorithm to mine fuzzy quantitative association rules. They considered Y + /r, where an item +z,v> represents either a crisp value, an interval (if numeri- cal), or a fuzzy term and a is an attribute with value v.

If v is fuzzy, the item is fuzzy. They used a minimum support for each attribute.

Kuok [ 181 mines fuzzy association rules to avoid either ignoring or overemphasizing the elements near the boundaries in the mining process. A user-supplied threshold is used to test each side of the rule as to be- ing ?satisfied.?  Other efforts at association rules with fuzzy antece- dents and consequents include Au [4] [5], Fu [ 1 11, Lee [ 191. Au [4] and Chan [9] are working on the same problem. Fu?s work is closely related to [18]. Du [lo] has a somewhat different approach to h i t i e d  ranges.

Somewhat related is work on incomplete data [27] [3].

The difficulty with these hzzy methods is that they are overly complex in the face of very large data sets.

However, the potential of soft computing is just be- ginning to be applied to data mining.

4 .  Reducing Association Rules Generated:  Value Aggregation Another approach to quantification that might  prove to be useful would be to granulate values, then apply quantified association rule techniques. For exarn- ple, if academic grades are recorded from 100 to 0, they often are granulized into A, B, C, ... .

This approach would have the substantial advantage that it would reduce the count of rules generated. How- ever, the problem is identifying what the extents of the appropriate granules. Possibly the mountain method [41 J might be applied to form the granules. The moun- tain method may also me useful in clustering associa- tion rules together  Granularity, Concept Hierarchies An unanswered research question is Granularity: It  is unclear whether concept (hierarchy) trees should be part of the initial formation of the transaction matrix (categorical or non-categorical) or dealt with later. For example, should all beer brands be grouped together as a single class under all cases? Or, should they be pre- sented separately?

Consider Figure I shown earlier. Should Miller and Tuberg be grouped together into a single class called beer? Should all beverages be grouped together; i.e., from example I ,  should Miller, Tuberg, and Coke be grouped together? Can items be grouped together d e pending on context; i.e., perhaps sometimes beers should be in one group and Coke in another; and, sometimes they should all be grouped together (e.g., picnic supplies). Is there some way of computationally deciding what should be grouped together?

Extending the example further: Consider the case where we have more beer brands, say: Budwiser, Miller, Sam Adams, and Tuberg. Most regular beer drinkers would agree that there is a substantial taste difference between the four of them. (There may also be price differentiation.) -Should Budwiser, Miller, Sam Adams and Tuberg be grouped into a single category; or  *Should each be grouped separately; or, *Should Budwiser & Miller form one group while Adams & Tuberg form another group?

Currently, concept hierarchy descriptions are done by a human expert. This is not entirely satisfactory because (a) potentially useful groups may go unrecog- nized and (b) the expert may make an error.

Granules  41 5    Overlapping Concept Hierarchies Concept hierarchies group similar attributes to-  gether. For example, various types of wine might be all be placed into to concept of ?wine.? For example, all of the following are wines.

If the transactions were: preinium wines E Carminet, Chalone, Caneros white wines E Chardonnay red wine E Burgundy, Pinot Noir wine type E alcohol, alcohol-jiree  basic wines E Meier, Gallo  Meier Carminet store Jean alcohol alcohol Chalone  green brand D?Arc cottage free free Gallo Pinot Almadin Carneros grapes cheddar brie cheese Burgundy Chardonnay Burgundy Noir Chardonnay Chardonnay 1 0 1 0 0 0 0 1 0 1  1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0  Figure 5. Wine and cheese transactions for items of diflering perceived quality.

A human observer might not be able to easily  re^  ognize that the premium, alcohol wines can be grouped together [22]. They are all associated with both green grapes and brie (a premium cheese). Similarly the basic alcohol wines can be grouped together as they are all associated with basic cheddar cheese.

However, if a predefined hierarchy was used to group all wines together, the association between type of wine and type of cheese would not be discouvered; and, the Figure 6 would result. This would result in loosing the information that premium wines are related to the purchase of brie cheese and green grapes. Why might this be important to a store? Well, if the store stopped carrying green grapes or brie, you may also lose your premium wine customers.

store Jean brand D?Arc cottage  i ~ ~ ~ ~ ~ s  cheddar brie cheese wine 1 0 1 0 1 1 0 1 0 1  I 1  0 1 0 1 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1  0 1 .  0 0 1 l o  1 0 1 1  1 0  1 0 0 1  Figure 6. All wine grouped together.

green grapes cheese wine  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1  1 1 1 1 1 1  Figure 7. All cheese and wine grouped together.

It gets even worse if all of the cheese varieties are grouped together as well, as in Figure 7. Now, the analysis would tell us that any cheese and any wine are associated. These results are clearly over-generalized.

Now, the analysis would tell us that any cheese and any wine are associated. These results are clearly over- generalized.

Ahierarchy such as: items E food E wine can be many levels deep. There are two ways of establishing hierarchies: (a) have a human predefme them or (b) learn them. There are a number of different ways that they can be learned.

As part of a rough-set based approach, Han [15] used crisp concept hierarchies provided by the user before the initiation of data mining. Alternatively, Mazlack [22] developed and tested a methodology to learn useful concept hierarchies. Potentially, the same methodology could be applied here.

5. Link Analysis Link analysis is an extension of simple association  rules. It recognizes sequential patterns. The goal is to detect when one set of items is followed by another set of items over a specified time period. Each set of items is distinct from the other. In a shopping example, each set (or transaction) represents a separate visit to a store.

Some of the same issues of granularity and quanti- fication that occurs in simple association rules also appear in sequence rules. For example, How are con- cept hierarchies formed? How are quantitative values handled? A comparative question is: Are the dferences in handling quant@cation and granulation when form- ing Link Rules as opposed to simple association rules?

6. Summary Data mining holds the promise of extracting new in-  formation from very large databases. One difficulty of     data mining is that traditional ways of discouvening new information from data are largely drawn from clas- sic machine learning methods. The work involved in these methods increases geometrically with the amount of data considered. Consequentially, the use of these methods is problematic in very large databases.

Association rules are a linearly complex, user under- standable way of doing data mining. The results are particularly useful when it is important that both the rules and the methodology behind the rules? develop- ment are understandable.

There are several open questions across all types of association rules. One of the most immediately en- gaging is how to identify interesting rules. The heuris- tic most commonly used is to identify rules that OCCUT more often than others; or, rules that occur above a specific threshold. However, it might be that there are high value rules that would not be recognized when using a threshold. A more fundamental question is ds termining whether or not an association is casual.

Bibliography [ I ]  R Agrawal, T. hielinski, A. Swami, ?Mining Association  Rules Between Sets Of Items In Large Databases,? Proceedings of ACM SIGMOD Conference on Management of Data (SIGMOD- 93), P. Buneman, S. Jajodia (eds), Washington, DC, May, pp. 207- 216, 1993.

[2] R Agnwal, R. Srikant, ?Fast Algorithms for Mining Asso- ciation Rules, VLDB Proceedings, pp. 487-499, 1994.

[3] K. Ali, S. Manganaris, R. Srikant, ??Partial Classification Us- ing Association Rules,? Knowledge Discovey In Databases And Data Mining Proceedings, Newport Beach, Ca, August, 1997.

[4] W.H. Au, K.C.C. Chan, ?An Effective Algorithm For Dis- covering Fuzzy Rules In Relational Databases,? PAKDD-98 Pro- ceedings, Melbourne, pp. 1314-1319, 1998.

[5] W.H. Au, K.C.C. Chan, ?Farm: A Data Mining System For Discovering Fuzzy Association Rules,? IEEE International Fuzzy Systems Conference Proceedings, Seoul, August, pp. 1217-1222, 1999.

[6] Y. Aumann, Y .  Lindell, ?A Statistical Theory for Quantita- tive Association Rules,? ACM SIGKDD Proceedings, San Diego,  [7] S .  Brin, R. Rastogi, K. Shim, ?Mining Optimized Gain Rules For Numeric Attributes,? ACM SIGKDD Proceedings, San Diego, pp. 135-144, 1999.

[8] C.H. Cai, A. W.C. Fu, C.H. Cheng, WW. Kwong, ?Mining Association Rules With Weighted Items,? Proceedings of IEEE Database And Engineering Applications, July, pp. 68-77, 1999.

[9] K.C.C. Chan, W. Au, ?Mining Fuzzy Association Rules,? and Knowledge Management, pp. 209-21 5,1997.

[IO] X.Du, Z. Liu, N. Ishii, ?Mining Association Rules On Re- lated Numeric Attributes,? PAKDD-99 (Third Pacific-Asia Confer- ence, Beijing, pp. 44-53, 1999.

[ I l l  A.W. Fu,M.H. Wong, S.C. Sze, W.C. Wong, W.L. Wong, W.K. Yu, ?Finding Fuzzy Sets For The Mining Of Fuzzy Associa- tion Rules For Numerical Attributes,? IDEAL 98, 1st Intem?onal Symposium On Intelligent Engineering and Learning, October, pp.

263-268, 1998.

[I21 T. Fukuda, Y. Morimoto, S. Morishta, T, Tokuyama ?Constructing Effcicnt Decision Trees By Using Optimized Nu- meric Association Rules,? Proceedings VLDB, Bombay, 1996.

[13] T. Fukuda, Y. Morimoto, S. Morishita, T, Tokuyama ?Mining Optimized Association Rules for Numeric Attributes,? Proceedings ACM SIGACT-SIGMOD-SIGART Symposium on Principles ofDatabase Systems, pp. 182-1 91, 1996.

[I41 T. Fukuda, Y. Morimoto, S. Morishita, T, Tokuyama, ?Data Mining Using Two-Dimensional Optimized Association Rules: Schemes, Algorithms, And Visualization,? Proceedings Of the ACM SZGMOD Conference on Management of Data, June, pp. 13- 23,1996.

pp. 261-270, 1999.

[15] J. Han, Y. Cai, N. Cercone, ?Knowledge Discovery in Da- tabases: An Attribute-Oriented Approach,? VLDB Proceedings, Vancouver, B.C. Canada, pp. 547-559, 1992.

[ 161 J. Han, Y. Fu, ?Mining Optimized Association Rules For Numeric Attributes,? VZDB Proceedings, Zurich, 1995.

[I71 M. Hemetinen, H. Mannila, P. Ronkainen, H. Toivonen, A.I. Verkamo, ?Finding Interesting Rules from Large Sets Of Dis- covered Association Rules,? CIKM-1994, 1994  [18] C.M. Kuok, A. Fu, M.H. Wong, ?Mining Fuzzy Association Rules In Databases,? Proceedings Of the ACMSIGMOD Confer- ence on Management ofData. pp. 4146, 1998.

[I91 D.H. Lee, M.H. Kim, ?Database Summarization Using Cybernetics -Part B, Vo. 27, No. 1 ,  pp. 68-77, 1997.

[20] B. Liu, W. Hsu, Y. Ma, ?Pruning and Summarizing the Dis- covered Associations,? Proceedings of the fifth ACM SIGKDD (XDD-99) Proceedings, pp. 125-134, 1999.

[21] H. Mannila, H. Toivonen, A.I. Verkamo, ?Efficient Algo- rithms For Discovering Association Rules,? AAAI Workshop on Knowledge Discovery and Databases, U.M. Fayyad, R Utharusamy (eds), Seattle, Washington, July, pp. 181 -1 92, 1994.

[22] L. Mazlack, ?Approximate Reasoning Applied To Unsuper- vised Database Mining,? International Journal of Intelligent Sys- tems, Vol 12, No. 5, May, pp. 391-414,1997.

[23] L. Mazlack, ?Clustering Mined Association Rules,? NMIPS 2000 Proceedings, July, 2000  [24] L. Mazlack, White Paper: Granulation Of Quantitative Association Rules, U. Cincinnati, 30 pages, 2001.

[25] RJ. Miller, Y. Yang, ??Ahxiation Rules Over Interval Data,? ACM SIGMOD, Vol. 26, No. 2, May, pp. 452461,1997.

[26] R.T. Ng, J. Han, ?Efficient And Effective Clustering Meth- ods For Spatial Data Mining,? VLDB Proceedings, pp. 144-155, 1994.

[27] V. Ng, J. Lee, ?Quantitative Association Rules Over In- complete Data,? IEEE Conference On Systems, Man, And Cyber- netics, San Diego, pp. 2821-2826, 1998.

[28] R.T. Ng, L. Lakshmanan, J. Han. ?Exploratory Mining And F?nming Optimizations Of Constrained Association Rules,? SIGMOD-98 Proceedings, 1998.

[29] J.S. Park, M. Chen, P.S. Yu, ?An Effective Hash Based Al- gorithm For Mining Association Rules,? ACM SIGMOD Proceed- ings, San Jose, May, 1996.

[30] B. Padmanabhan, A. Tuzhilin ?A Belief-Driven Method For Discovering Unexpected Patterns,? KDD-98 Proceedings, 1998.

[31] G. Piatetsky-Shapiro. ?Discovery, Analysis, and Presenta- tion Of Strong Rules,? in G. piatetsky-Shapiro, W.J. Frawly (eds): Knowledge Discovery In Databases, AAI Press/MIT Press, Cam- bridge, MA, pp. 229-248, 1991  [32] L. Rendell, H. Ragavan, ?Improving The Design Of Induc- tion Methods By Analyzing Algorithm Functionality And Data- Based Concept Complexity,? IJCAI-93 Proceedings, Chambrey, August, pp. 952-958,1993.

[33] R. Rastogi, K. Shim. ?Mining Optimized Gain Rules For Nu- meric Attributes,? Technical Report, Bell Laboratories, Murray Hill, 1998.

[34] A. Sawsere, E. Omiecinski, S .  Navathe, ?An Efficient Al- gorithm For Mining Association Rules In Large Databases,? VLDB Proceedings, Zurich, pp. 144-1555,1995.

[35] A. Silbershantz, A. Tuzhilin, ?What Makes Patterns Inter- esting In Knowledge Discovery Systems,? IEEE Transaztions On Knowledge And Data Engineering, ?? Vol. 8, No. 6,1996.

[38] R. Srikant, R. Agrawal, ?Mining Quantitative Association Rules in Large Relational Tables.? ACM SIGMOD Proceedinm. - .

pp. 1-12, 199%.

With Item Constraints,? KDD-97 Proceedings, 1997.

[39] R. Srikant, Q. Vu, R. Agrawal, ?Mining Association Rules  [40] W. Wang, J. Yang, P.S. Yu, ?Efficient Mining Of Weighted Association Rules,? KDD-2000 Proceedings, pp. 270-274,2000.

[41 J RR. Yager, D.P. Filev, ?Approximate Clustering Via The Mountain Method,? IEEE Transactionr On Systems, Man, And Cybernetics, Vol. 24, No. 8, pp. 1279-1284, 1994.

[42] Y.Y. Yao, N. Zhong, ?An Analysis Of Quantitative Meas- ures Associated With Rules,? PAKDD-99 (Third Pacific-Asia Con- ference), Beijing, pp. 479487, 1999.

