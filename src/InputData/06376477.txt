Image Mining by Data Compactness and Manifold Learning

Abstract? One important issue in image mining is how to analyze the compactness of image data and apply it to image mining. In this paper we study the class compactness and boundary compactness of image data, which are used in image classification and data confining. The data confining results in relevance graph, which is used in calculating the distances between images. Manifold learning techniques are applied in the computation of distances between images and manifolds of images.  Image retrieval is based on these distances.

Experiments are reported to show the effectiveness of our approach.

Keywords- class compactness; boundary compactness; manifold learning; image mining; image classification; image retrieval

I. INTRODUCTION With the advance of multimedia technology and growth  of image collections, image mining has gradually become the focus of attention. Image mining is a technique to extract patterns, implicit knowledge, and/or image data relationship which are not explicitly stored in images [1]. This technology can be widely used in image classification and retrieval, medical diagnostic imaging, satellite image analysis, and mineral forecast.

The frequently-used image mining techniques include: image similarity search, image association rule mining, image classification, image clustering, and neural networks.

When manual image annotation became more and more unfeasible, image search based on content similarity became popular. A lot of image retrieval systems adopt the similarity-based paradigm, including QBIC (IBM Query by Image Content) [2], VisualSEEk [3], Virage?s VIR Image Engine [4], and Excalibur?s Image RetrievalWare [5]. Image association rules provide information in image databases, such as interesting but non-obvious spatial or temporal causalities. Many association rule mining methods [6][7] have been proposed for image databases. The objective of image classification is to decide whether an image belongs to a certain category or not. Uehara et al. [8] used a binary Bayesian classifier to achieve a systematic image classification, where images are divided into two types: indoor and outdoor. Vailaya et al. [9] proposed a hierarchical classification of vacation images: at the highest level, images are classified as indoor or outdoor; outdoor  images are further classified as city or landscape; finally, a subset of landscape images is classified into sunset, forest, and mountain classes. Image clustering methods partition images into clusters such that the images within the same cluster are similar to each other. Unlike classification, image clustering is unsupervised. Many image clustering methods [10][11] have been successfully used to better organize, represent, and browse images. A neural network is a massively parallel distributed processor consisting of simple processing units, each of which has a natural propensity for storing experiential knowledge and making the knowledge available for use [12]. [13] presents a noteworthy research work that applied neural network to image mining.

Typical image mining frameworks can be divided into two categories: function-driven and information-driven.

MultiMediaMiner [14], a system developed by researchers from Simon Fraser University, used a framework belonging to the first category. The system constructs a multimedia data cube facilitating multiple dimensional analyses of multimedia data, primarily based on visual content, and the mining of various kinds of knowledge, including summarization, comparison, classification, association, and clustering. J. Zhang et al. proposed an information-driven framework for image mining [15]. The framework distinguishes four levels of information: the pixel level, the object level, the semantic concept level, and the pattern and knowledge level. High-dimensional indexing schemes and retrieval techniques are also included to support the flow of information among the levels. This framework makes a first step towards capturing the different levels of information present in image data and addressing the question of what are the issues and challenges of discovering useful patterns/knowledge from each level.

Although many methods have been proposed for image mining, the study of image mining is not very mature yet, and many theoretical issues remain to be further investigated. One important issue is how to analyze the compactness of image data and apply it to image classification and image understanding. We see the world, classifying and analyzing various scenes; in this process, data compactness plays an important role. In the ever- changing world, we observe objects of different types; the appearance of objects in each type has a relatively stable   DOI 10.1109/ICINIS.2012.53     and compact model. The stability and compactness in appearance indicate data compactness.

In this paper we will study the class compactness and boundary compactness of image data. The theoretical results will be used in image classification and data confining. The data confining results in relevance graph, which is used in calculating the distances between images. Manifold learning techniques are applied in the computation of distances between images and manifolds of images.  Image retrieval is based on these distances.

The rest of the paper is organized as follows. In Section II, we introduce a mathematical model of image mining.

Section III presents our approach for image classification by class compactness. Section IV proposes our scheme for data confining by boundary compactness. Image retrieval by manifold learning is elaborated in Section V. Section VI reports the experiments. Section VII concludes.



II. MATHEMATICAL MODEL OF IMAGE MINING Let Rd be the feature space of the image set,  d n RxxxX ?= },...,,{ 21 be the vector representation of the  images in the feature space. The scenes of the images are divided into m categories: },...,,{ 21 m???=? . The image clustering or classification can be represented as a function  ????Xg : . In the real world, scenes in the same category are very different under different conditions, the images of these scenes form a manifold in the feature space. For large quantities of image data, we can use a manifold learning approach to investigate the problem of the semantic representation of images. Manifold Learning pursuits the goal to embed originally high-dimensional data in a lower dimensional space, while preserving characteristic properties. For an image scene ?i, a manifold learning is a mapping di  ii Rgg ???? ? )(: 1 . Put together, the scene space  can be formulated as di i  m  i  RY ??= =  }{ ? , and the goal of image  mining is to find a mapping from the image set X to the scene space Y, YXh ???: , where )),(),(()( xgxgxh i= and  ixg ?=)( . The image association rule mining aims to find a set of association rules to reveal and represent the occurrence frequency of a group of objects/features, or their relationship. A typical association rule can be written as %]%,[ csQP ??? , where P and Q are predicates, s% is the support of the rule, and c% is the confidence. It is common to use P for low level features and Q for semantic features so that we can use association rules to infer image semantics from low level features. Thus dRp ? , YQ ? , and every association rule is represented as a point in the association rule space )()( YRd ??? , where )(?? represents the power set of a set.



III. IMAGE CLASSIFICATION BY CLASS COMPACTNESS One main problem of image classification is that the  relationship of inter-class distance and intra-class distance is not fully explored. To address this problem, we introduce an iCluster Tree model. An isolation cluster, or icluster, is a connected subset whose inter-subset distance to its complement (ECD) is longer than its intra-subset distance (ICD). The compactness of an icluster is defined as ECD/ICD. We can prove that all iclusters form a rooted tree, called the iCluster Tree.

We apply this model to image classification. Let the training and test sets respectively be  Xtn = {x1, x2, ? , xt} and Xtt = {xt+1, x t+2, ? , xn}. The predefined class set is Y = {Y1, Y2, ? , Ym}. On the training set Xtn, there is a mapping: ftn: Xtn ? {1, 2, ? , m} which assigns each training point to a preset class. Supervised classification aims to extend the mapping ftn to the test set Xtt.

We combine the two sets into one set: X = Xtn ? Xtt = {x1, x2, ? , xt, xt+1, x t+2, ? , xn}, and let G be a graph constructed on X and T be the icluster tree of G. Each icluster C has a training histogram, which is a m-tuple: ( |C ? Y1|, |C ? Y2|, ? , |C ? Ym| ). The training histograms of iclusters can be computed recursively. Initially we compute the histogram of each leaf iclusters, i.e., an icluster with one point. Let the point be v and the histogram is (|{v} ? Y1|, |{v} ? Y2|, ? , |{v} ? Ym|). For each non-leaf node, we add the histograms of all its children to get the histogram of this node. The concentrantion ratio of C is defined as:  ConcentrationRatio(C) = ||  |)(|1 nt  imi  X YCMax ??? .

An icluster C is called concentrated on class Yk if its concentration ratio is no less than a given threshold T_ratio and |C ? Yk| = |)(|1 imi YCMax ??? .

We make a top-down search to find all concentrated iclusters: we start with the root node; for each node, if it?s concentrated (with respect to a given threshold) then add it to the result queue, otherwise repeat the process on its child nodes. The result queue has a property that the concentrated iclusters in the queue do not overlap. For a test point p in a concentrated icluster (concentrated on class Yi) in the queue, we let f(p) = i, where f is the extended function of ftn.

However not all points appear in these concentrated iclusters.  Let Z be the set of points classified thus far and suppose Z = Z1 ? Z2 ??? Zm such that f(Zi) = i for 1? i ? m, i.e., points in each Zi are assigned to the class Yi. For any point q in X?Z, we calculate its distances to the centroids of Z1, Z2, ? , Zm respectively and add q to the set Zj if its distance to the centroid of Zj is the shortest. The whole data set X is finally classified as Z1, Z2, ? , Zm, which are corresponding to the m preclasses, respectively.



IV. DATA CONFINING BY BOUNDARY COMPACTNESS In real applications, high-dimensional image data is  difficult to interpret as it requires more dimensions to represent. As a dimension reduction method, manifold learning provides an explicit representation for the useful implicit information hidden in the original feature space.

But the internal topological and differential structure has disappeared in the dimensionality reduction process. On the other hand, existing surface reconstruction methods only work for low-dimensional, mostly 2D or 3D, data. We introduce boundary compactness to study the shape of the data set in the original high-dimensional feature space.

Given a K-dimensional set of n data points, to construct the Delaunay diagram, we discuss two cases: (1) K?2 and (2) K >2. When K?2, there exist O(n log n) algorithms (that is optimal) to compute the Voronoi diagram and Delaunay triangulation [16]. In dimension K >2, Delaunay triangulations can be computed in O(n?K/2?) time [17]. For the boundary-fitting-by-erosion process, the graph to be ?eroded? depends on K. When K?3, we choose Delaunay triangulation; when K >3, the complete graph is used. In each case we call the graph to be eroded the fat graph.

For 3D and higher dimensional data, the erosion process is based on local density and controlled by boundary compactness. When the erosion process stops, we get the data shape (Figure 1e). Given an n-dimensional (n > 2) data set X and some x in X, we select m points closest to x in the MST, and compute the average length ?(x) of the MST edges connecting the m points and x. ?(x) indicates the local density at x. For any edge pq in the fat graph, we define its boundary compactness as  2/))()(( qp qp  ?? + ? . We remove from  the fat graph the edges of boundary compactness greater than a threshold and get a graph, called the relevance graph.

In the relevance graph, points connected by an edge represent closely related instances in real world.

(a) (b)  (c) (d)  (e) (f) Figure 1: Data confining by cutting the Delaunay triangulation at ?good? boundary gaps: (a) the data set, (b) its MST, (c) its Delaunay triangulation, (d) removal of boundary gaps of big boundary compactness, (e) the shape of the data, and (f) the boundary.

Once the relevance graph is created, it can be used for surface reconstruction and investigation of the topological and differential structure of a data set. In this paper the data confining process results in a relevance graph for each data class, which is then used for image retrieval.



V. IMAGE RETRIEVAL BY MANIFOLD LEARNING An image retrieval system is a computer system for  browsing and retrieving images from a large image base. In this paper we apply manifold learning techniques on image retrieval. Many machine learning systems tend to be very slow when operating on high-dimensional data, as is known as the curse of dimensionality. In many applications, the observed data are found to lie on a low-dimensional manifold embedded in the higher-dimensional space.

Manifold Learning, also referred to as non-linear dimensionality reduction, is a technique to find the intrinsic structure of high dimensional data by mapping them to a lower dimensional manifold, while preserving characteristic properties.

A variety of manifold learning methods can be found in literature. Topologically Constrained Isometric Embedding [18]  uses both local and global distances to learn the intrinsic geometry of flat manifolds with boundaries. The algorithm filters out potentially problematic distances between distant feature points based on the properties of the geodesics connecting those points and their relative distance to the boundary of the feature manifold, thus avoiding an inherent limitation of the Isomap algorithm. RankVisu [19] is a mapping method designed to the preservation of neighborhood ranks rather than their dissimilarities. A mapping of data is obtained in which neighborhood ranks are as close as possible according to the original space.

In our approach, image retrieval is implemented by computing distances between points, between a point and a manifold, and between manifolds. Using the scheme introduced in [20], a manifold M is represented as a set of subspaces M = {C1 C2 ? Cm}. We define the following distances:     (1). d(x1, x2) = 21 xx ? , where x1 and x2 are two points, and  21 xx ?  is the distance in the relevance graph.

(2). d(x, C) = 'min xxyx Cy  ?=? ?  , where C is a subspace,  and x? is the projection of the point x on C.

(3). d(x, M) = "minmin),(min xxyxCxd iii CyMC  iMC ?=?=  ??? , where  M is a manifold, and x? is the projection of the point x on M.

(4). d(C, M)= ),(min iMC CCdi? .

(5). d(M 1, M 2)= ),(minmin),(min  2 jiMCMCiMC CCdMCd  jii ??? = , where M1 and  M 2 are two manifolds.



VI. EXPERIMENTAL VALIDATION In this section, we report on experiments designed to test  the effectiveness of our approach. We used a web spider to collect 2000 images in the TUTE web site (www.tute.edu.cn). A subset of 200 images was randomly selected as the training set. The training images were manually classified into the following 10 classes: (Opening Ceremony), (Military Training), (Join the party), (Red Song Contest), (School- enterprise cooperation), (Lecture), (International Exchange), (Equipment), (Classroom), and (Campus Scenery). We then applied the proposed classification algorithm to classify the 2000 images, and used the data confining algorithm to compute the relevance graph. The manifolds of the image classes were constructed.

In the experiments, given an query image q, we search a best match image by finding the image m in the image base with the smallest d(q m). Given a set of query images, we first construct a manifold M1, We search a best match image class by finding the image manifold M2 in the image base with the smallest d(M1 M2). The overall accuracy of the best match image testing is 83%, and the overall accuracy of the best match class testing is 87%.



VII. CONCLUSION In this paper we investigated the class compactness and  boundary compactness of image data. Manifold learning techniques are applied in the computation of distances between images and manifolds of images.  The introduced techniques were tested by experiments.

