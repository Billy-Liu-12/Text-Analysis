Mining Top-K Frequent Closed Patterns without Minimum  Support.

ABSTRACT In this paper, we propose a new mining task mining top-k frequent closed patterns of length no less than m i n l ,  where k is the desired number of frequent closed patterns to he mined, and minl is the minimal length of each pattern.

An efficient algorithm, called TFP, is developed for min- ing such patterns without minimum support. Two meth- ods, closdnode-count and descendantsum are proposed to effectively raise support threshold and prune FP-tree both dunng and a f t w  the construction of FP-tree. During the mining process, a novel top-down and bottom-up combined FP-tree mining strategy is developed to speed-up support- raising and closed frequent pattern discovering. In addition, a fast hash-based closed pattern verification scheme has been employed to check efficiently if a potential closed pattern is really closed.

Our performance study shows that in most cases, TFP outperforms CLOSET and CHARM, two efficient frequent closed pattern mining algorithms, even when both are run- ning with the best tuned minsupport. Furthermore; the method can be extended to generate association rules and to incorporate user-specified constraints. Thus we conclude that for frequent pattern mining, mining t o p k  frequent closed patterns without minsvpport is more preferable than the traditional minsupport-based mining.

1. INTRODUCTION As m e  of several essential data mining tasks, mining fre-  quent patterns has been studied extensively in  literature.

From the implementation methodology point of view, re cently developed frequent pattern mining algorithms can be categorized into three classes: (1) Apriori-based, hori- zontal formatting method, with Apriori [I] as its represen- tative, (2) Apriori-based, verticai formatting method, such as CHARM 181, and (3) projection-based pattern growth method, which may explore some compressed data struc- ture such as FP-tree, as in FP-growth [3].

The common framework is to use a minuupport threshold to ensure the generation of the correct and complete set of frequent patterns, based on the popular Apriori property [I]: every subpattern of a frequent pattern must be firpent (also called the downward closurr property). Unfortunately, this framework, though simple, leads to the following two problems that may hinder its popular use.

First, Setting mixsupport is quite subtle: a too small  The work was supported in part by U.S. National Science ' Foundation, University of Illinois, and Microsoft Research.

threshold may lead to the genemtion of thousands of pat- terns,  whewos a too big one may often genemte no an- swers. Our awn experience at  mining shopping transaction databases tells us this is by no means an easy task.

Second, frequent pattern mining often leads to the gen- eration of a large number of patterns (and an even larger number of mined rules). And mining a long pattern may unavoidably generate an exponential number of subpatterns due to the downward closure property of the mining process.

The second problem has been noted and examined by re- searchers recently, proposing to mine (frequent) closed pat- terns 15, 7, 81 instead. Since a closed pattern is the pattern that covers all of its subpatterns with the same support, one just need to mine the set of closed patterns (often much smaller than the whole set of frequent patterns), without los- ing information. Therefore, mining closed patterns should be the default task for mining frequent patterns.

The above observations indicate that it is often preferable to change the task of mining frequent patterns to mining top- k frequent closed patterns of minimum length m i n l ,  where k is a user-desired number of frequent closed patterns to he mined (which is easy to specify or set default), top-k refers to the k most frequent closed patterns, and m i d ,  the min- imal length of closed patterns, is another parameter easy to set. Notice that without min.!, the patterns found will he of Iength one (or their corresponding closed superpatterns) since a pattern can never occur more frequently than its cor- responding shorter one. (i.e., subpatterns) in any database.

I n  this paper, we study the problem of mining topk fre- quent closed patterns of minimal length minl efficiently without minsupport, i.e., starting with minsupport = 0.

Our study is focused on the FP-treebased algorithm. An efficient algorithm, called TFP, is developed by taking ad- vantage of a few interesting properties of t o p k  frequent closed patterns with minimum length m i d ,  including (1) any transactions shorter than m i d  will not be included in the pattern mining, (2) minsupport can be raised dynami- cally in the FP-tree construction, which will help pruning the tree before mining, and (3) the most promising tree branches can he mined first to raise minsupport further, and the raised rninsupport is then used to effectively prune the remaining branches.

Performance study shows that TFP has SWprisingly high performance. In most cases, it is better than two efficient frequent closed pattern mining algorithms, CLOSET and CHARM,  with the best tuned minsupport.

Moreover, association rules can be extracted by minor ex- tension of the method, and constraints can &o be incorpe  0-7695-1754-4/02 $17.00 0 2002 IEEE 211  mailto:tzvetkov}@cs.uiuc.edu   rated into topk  closed pattern mining.

Therefore, we conclude that mining tOpk frequent closed  patterns without minimum support is more preferable (from both usability and efficiency points of view) than traditi0n.d minsupport-based mining.

The remaining of the paper is organized as follows. In Section 2, the basic concept of t o p k  closed pattern mining is introduced, and the problem is analyzed with the related properties identified. Section 3 presents the algorithm for mining t o p k  closed patterns. A performance study of the algorithm is reported in Section 4. Extensions of the method are discussed in Section 5, and we conclude our study in Section 6.

2. PROBLEM DEFINITION In this section, we first introduce the basic concepts of t o p  k closed patterns, then analyze the problems and present an interesting method for mining t o p k  closed pattern.

Let I = { i , , i z , .  . . , i")  be a set of i tems. An i temset X is a non-empty subset of I. The length  of i temset X is the number of items contained in X,  and X is called an l-itemset if its length is 1. A tuple (l id,  X) is called a transaction where tid is a transaction identifier and X is an itemset. A transaction database TDB i s  a set of transactions. An  itemset X is contained in transaction ( t i d , Y )  if X 2 Y .  Given a transaction database TDB, the suppor t  of an itemset X, denoted as sup(X) ,  is the number of transactions in TDB which contain X.

DEFINITION 1. ( t o p k  closed i t emset )  A n  itemset X is a closed i temset if there ezists no itemset X'  such that (1)X c X ' ,  and (2) V tmnsaction T ,  X E T - X' E T .  A closed itemset X 1s (I top-k closed i t emset  of minimal length  m i n d  iJ them enst' no more than ( k  - 1 )  closed itemsets oJ length at least m i n S  whose support is higher than that of X .  .

Our task is to mine t o p k  closed itemsets of minimal length minl efficiently in a large transaction database.

EXAMPLE 1 (A transaction dataset example).  Let Table 1 be our transaction database T D B .  Suppose our task is to find top-4 Jreguent closed p a l t e m  with m i n d  = 4 .  m  Items I Ordered Items I  Table 1: A transaction database TDB.

FP-growth?' Without minsupport threshold, one can still use Apriori to mine all the I-itemsets level-by-level for I from 1 to m i n l .  However, since one cannot use the downward closure property to prune infmpent Citemsets for genera- tion of (I + l)-itemset candidates, Apriori bas to join all the length 1 itemsets to generate length I + 1 candidates for all 1 from 1 to m i n l -  1 .  This is inefficient. CHARM loses its pruning power as well since it has to generate transaction idJist for every item, and find their intersected transaction idlist for every pair of such items since there is no itemset that can be pruned. Will the fate he the same for FP-growth?

Since FP-growth uses a compressed data structure FP-tree to register TDB, all the possible itemsets of a transaction and their corresponding length information are preserved in the corresponding branch of the FP-tree. Moreover, FP-tree pre- serves the support information of the itemsets as well. Thus it is possible to utilize such information to speed up mining.

Of the three possible methods, we will examine FP-growth in detail.

The question then becomes, "how can we eztend FP-growth for eficient top-k f m p e n t  closed pattern mining?' We have the following ideas: (1) O-mindupport forces us to construct the "full FP-tree". however. with t o p k  in mind, one can capture sufficient higher support closed nodes in tree con- struction and dynamically raise minaupport to prune the tree; and (2) in FP-tree mining, one can first mine the most promising subtrees so that high support patterns can be de- rived earlier, which can be used to  prune low-support s u b trees. In  the following section, we will develop the method step-by-step.

3. MINING TOP-K FREQUENT CLOSED PAT- TERNS: METHOD DEVELOPMENT  In this section, we perform stepby-step analysis to de- velop an efficient method for mining t o p k  frequent closed patterns.

3.1 Short transactions and 1-counts Before studying FP-tree construction, we notice, REMARK 3.1 (Short  transactions).  If a tmnsaction  T contains less than m i n d  dlstinct items, none of the i t e m in T con contribute to a pattern of minimum length m i n l .

For the discussions below, we will consider only the trans- actions that satisfy the minimum length requirement.

:$ ~  Our first question is "which mining methodologg should be chosen fmm among the thme choices: Apriori, CHARM, and 'Since there could be more than one itemset having the same support in a transaction database, to ensure the set mined is independent of the ordering of the items and transactions, our method will mine every closed itemset whose support is no less than the k-th frequent closed itemset.

b) Count and a) The FP-tree constructed from TDB. 1-count of items.

F igu re  1: FP-tree and i t s  header table.

Let the occurrence frequency of each item be stored as Here we introduce in count in the (global) header table.

the header table another counter, l(ow)-count, which records the total occurrences of an item at  the level no higher than m i n l  in the FP-tree, as shown in Figure 1 b).

REMARK 3.2 (1-count). If the 1-count of an item t is lower than minsupport, t connot be used to genemte Jw F e n 1  i temet of length no less than mind.

Rationale. Based on the rules forgenemtion offrepent item- set in FP-tree [Si, only a node residing at the level mind  or lower (i.e., deeper in the tree) moy genemte a pmfiz path no shorter than minl. Based on Remark 3.1, short prefLzpoths will not contebute to the genemtion of itemset with length greoter or equal to m i n l .  Thus only the nodes with I-count no lower than minsupport may generate lkrpuent i temet of length no less than m i n l .  .

People may wonder that o u  assumption is to start with minsupport = 0, how could we still use the notion of minsupport? Notice that if we can find a goad number (i.e., no less than k )  of clased nodes with nontrivial s u p port during the FP-tree construction or before tree mining, the minsupport can be raised, which can be used to  prune other items with low support.

3.2 Raising minsupport for pruning FP-tree Since our goal is to  mine t o p k  frequent closed nodes, in  order to  raise minsupport effectively, one must ensure that the nodes taken into count are closed.

LEMMA 3.1 (Closed node). At any time during the construction of an FP-tree, D node n, is ~1 closed node (rep- resenting D closed itemset) if it falk into one ofthe following three m e s :  (1) n, has more than one child and n, car ies more count than the sum of its children, (2) nt comes more count than ib child, and (3) nl is a leaf node.

Rationale. This can be easily derived fmm the definition of closed i temet and the rules fo. construction of FP-tree [9]. A s  shown in Figure 2 a), D node (nt : 1) denotes an i temet  nl , .  . . , nt with support 1. Any later tmmaction (or prefiz-path) that contaim ezoetly the same set of i t e m  will be represented by the same node in the tree with inerrased support. If nt has more than one child and nt car ies  more count than the sum of its children, then n, cannot carry the same support as any of its children, and thus nt must be ~1 closed node. The same reason holds i fnt  c a r i e s  more count than its child. If nt is ~1 leaf node, the future insertion of brnnches will make the node either remain as D leaf node OT cam more count than its children, thw nt must be a closed node as well. .

To raise minsupport dynamically during the FP-tree con- struction, a simple data structure, called closednodemount array, can be used to  register the elurent count of each clased I-node with support node#, as illustrated in the left.

The array is constructed as follows. Initially, all the count of eacb node# is initialized to  0 (or only the non-zero 1- node is registered, depending on the implementation). Each clmed I-node with support m in the FP-tree has one count in the count slot of node# m. During the construction of an FP-tree, suppose inserting one transaction into a branch makes the support of a closed I-node P increases from m to  I-node# I count  U:?#'   , P  b) Count of closed I-node with support node#. a) Judging the closed node  Figure 2: Closed node and closed n o d e  count  array.

m + 1. Then the count corresponding to the node# m + 1 in the array is increased by one whereas that corresponding to  the node# m is decreased by one.

Based on how the closed-oderount array is constructed, one can easily derive the following lemma.

LEMMA 3.2 (Raise  minsupport  using closed.nnode).

At any time during the constmetion of an FP-tree, the mini- mum support for mining top-k closed itemsets will be at least equal to the node# if the sum ofthe closedxnoderount a m y fmm the top to node# i s  no less than k .  .

Besides using the closednodelount array to  raise mini- mum support, there is another method to  raise minsupport with FP-tree, called anchor-node descendant-sum, or simply descendant-sum, as described below. An anchor-node is a node at  level mind-  1 of an FP-tree. It is called an anchor- node since it serves as an anchor to  the (descendant) nodes at  level mind and below. The method is described in the following example.

/iz h18 .:,J / \  Figure 3: n o d e  of an FP-tree.

Calculate  descendantsum for an anchor  EXAMPLE 2. As shown in F i p r e  3, node b is on nnchor node since it resides at level m i d  - 1 = 2. At thzJ node, we collect the sum of the counts for each distinct iternset of b's descendants. For emmple, since b has two descendant d- nodes, (d : 25) and (d : 43), b's descendantsum f o r d  is (d : 68) (which means that the support ofi temet abd contnbuted from 6's descendants is 68). From the FP-tree presented in Figure 3, it is easy lo figurn out that b's descendantsum should be { ( c :  75) , (d :  6 8 ) , ( e  : 43),(h : 38),(f : 2O),(g : 20)). Such summaw infomation may mise minsupport     effectiectively. For ezample, minsupport f o r  top-3 closed nodes should be at least 43 bosed on b's descendantsum. .

(descendantsum).  Each distinct count in descendant-sum of an anchor node represents the minimum count of one dlstinct closed pattern that can be generated by the FP-tree.

Rationale. Let the path from the root of the FP-tree to on anchor node b be 0. Let D descendant ofb bed and its count be countd. Then based on the method for  FP-tree construc- tion, thew must ezist an i t e m e t  0 - d  whose count is ermntd.

I fP.  d zs a closed node, then it is the w i p e  closed node in the FP-tree with count countd; othemise, them must enst a closed pattern which is its super-pottern d t h  support countd.

Since another node in b's descendantsum with the same svpport countd may show such D closed node with 0. d ,  and atso another bmnch may contribute addition01 count to such a closed node, thus only D distinct count in  descendantsum of b may mpmsent the minimum count of a distinct closed pattern genemted by the FP-tree. .

We have the following observations regarding the two s u p port raising methods. First, the c l o s e d n o d e a u n t  method is cheap (only one array) and is easy to implement, and it can he performed at any time during the tree insertion proces.

Second, comparing with c l o s e d n o d e a u n t ,  descendantsum is more effective at  raising minsuppet, hut is more costly since there could he many ( m i d  - 1) level nodes in an FP-tree, and each such nude will ueed a deseendontsum structure. Moreover, before fully scanning the database, one does not know which node may eventually have a very high count. Thus it is tricky to select the appropriate an- chor nodes for support raising: too many anchor nodes may waste storage space, whereas too few nodes may not be able to register enough count information to raise minsupport effectively. Computing descendantsum structure for low count nodes could he a waste since it usually derives small descendantsum and may not raise minsupport  effectively.

Based on the above analysis, our implementation explores both techniques but at  difierent times: During the FP-tree construction, it keeps an closednnodexount array which raises minsupport ,  dynamically prunes some infrequent nodes, and reduces the size of the FP-tree to he constructed. Af- ter scanning the database (i.e., the FP-tree is constructed), we traverse the suhtree of the level ( m i d  - 1) node with the highest support to calculate descendantsum. This will effectively raise minsupport .  If the so-raised minsupport is still less than the highest support of the remaining level (minl-  1) nodes, the remaining node with the highest s u p port will be traversed, and this process continues. Based on o w  experiments, only a small number of nodes need to he 50 traversed (if k for t o p k  is less than 1000) in most cases.

LEMMA 3.3  3.3 Efficientminingof FP-treefor top-k patterns The raise of min-support prunes the FP-tree and speeds  up the mining. However, the overall critical performance gain comes from efficient FP-tree mining.

We have the following observations.

REMARK 3.3 (Item skipping).  Ifthe count ofan item in the global header table is less than minsuppor t ,  then it Is infrequent and its nodes should be mmoved from the FP-tree  ,Woxover, if the 1-count of an item in the global heoder ta- ble ts less than minsuppor t ,  the item should not be used to genemte any conditional FP-tree.

The TFP-mining with FP-tree is similar to FP-growth.

However, there are a few subtle points.

1. "Top-down" ordering of the items in theglobal header table for the generation of conditional FP-trees.

The first subtlety is in what order the conditional FP-trees should be generated for t opk  mining. Notice since FP-growth in [3j is to find the complete set of frequent patterns, its min- ing may start from any item in the header table. For t o p k mining, our goal is to find only the patterns wtth high sup- port and mise the minsupport as fast as possible to ovoid unnecessary work. Thus mining should start from the item that has the first non-zero 1-count (which usually carries the highest 1-count) in the header table, and walk down the header table entries to mine subsequent items (i.e., in the sorteditemlist order). This ordering is based on that i t e m with higher 1-count usually produce patterns with higher support. With this ordering, minsupport  can he raised faster and the t o p k  patterns can he discovered earlier. In addition, an item with 1-count less than mindupport do not have t o  generate conditional FP-tree for further mining (as stated in Remark 3.2). Thus, the faster the minsupport can he raised, the moIe and earlier pruning can he done.

2. "Bottom-up" ordering of the items in a local header table for mining conditional FP-trees.

The second subtlety is how to mine conditional FP-trees.

We have shown that the generation of conditional FP-trees should follow the order of the sorteditemlist, which can be viewed as topdown walking through the header table.

However, it is often more beneficial to mine a conditional pattern tree in the "bottom-up" manner in the sense that we first mine the i t e m  that are located at the low end of a tree branch since it tends to produce the longest patterns first then followed hy shorter ones. It is more efficient to first generate long closed patterns since the patterns containing only the subset items can be absorbed by them easily.

3. Efficient searching and maintaining of closed patterns using a pattern-tree structure.

The third subtle point is how to efficiently maintain the set of current frequent closed patterns and check whether a new pattern is a closed one.

During the mining process, a pattern-tree i4 used t o  keep the set of current frequent closed patterns. The structure of pattern-tree is similar to that of FP-tree. Recall that the items in a branch of the FP-tree are ordered in the support- decreasing order. This ordering is crucial for closed pat- tern verification (to be discussed below), thus we retain this item ordering in the patterns mined. The major difference between FP-tree and pattern-tree is that the former stores transactions in compressed form, whereas the latter stores potential closed frequent patterns.

The bottom-up mining of the conditional PP-trees gener- ates patterns in such an order: for patterns that share p r e fixes, longer patterns are generated first. In addition, there is a total ordering over the patterns generated. This leads to our closed frequent  p a t t e r n  verification scheme, presented as follows.

Let (i,, , . . ,it,, . . , ij,. . . ,in) he the sortehilemlist, where ii is the first non-zero I-count item, and ij he the item whose     conditional FP-tree is currently being mined. Then the set of already mined closed patterns, S, can be split into two subsets: (1) SDz.i, obtained by mining the conditional trees corresponding to items from il t o  ij-, (i.e., none of the item- sets contains item if), and (2) s,,, obtained so far by min- ing i j ' s  conditional tree (i.e., every itemset contains item i,). Upon finding a new pattern p during the mining of ij's conditional tree, we need to perform new pattern checking (checking against Sj) and old pattern checking (checking  The new pattern checking is performed as fallows. Since the mining of the conditional tree is in a bottom-up man- ner, just like CLOSET, we need to check whether (1) p is a subpattern of another pattern pi, in S,, , and ( 2 )  supp(p) supp(p+,). If the answer is no, i.e., p passes new pattern checking, p becomes a new closed pattern with respect t o  S.

Note that because patterns in Sold do not contain item ij, there is no need to check if p is a subpattern of the patterns in Sold.

The old pattern checking is performed as follows. Since the global FP-tree is mined in a topdown manner, pattern p may he a super-pattern of another pattern, p,(d. in Sold with supp(p) supp(p.rd). In this case, parr cannot be a closed pattern since it is absorbed by p .  Therefore, if p has passed both new and old pattern checking, it can he used to raise the support threshold. Otherwise, if p passes only the new pattern checking, then it is inserted into the pattern- tree, but it cannot be used to raise the support threshold.

The correctness of the above checking is shown in the following lemmas.

LEMMA 3.4  against Sold ) .

(New pattern checking). If a pattern p cannot pass the new pattern checking, there must e d t  n pattern, p, ,  , in S,, , which must &o contain item i, with  Rationale. This con be obtained directlyfmm the newpattern checking method. .

Let p r e f i s ( p )  be the prefix pattern of B pattern p (i.e., obtained by removing the last item ij from p).

(Old pa t t e rn  checking). For oldpattern checking, we only need to check if there ezists a pottern prefiz(p) in Sold with supp(prefis(p)) z supp(p).

Rationale. Since D pattern in Sol,+ does not contoin item i,, it cannot become D super-pottern ofp. Thus we only need to check if it is a subpattern of p. In fact we only need to check if them is (I potternprefis(p) in Sou with the Same support as p .  We can pmve this by eontmdiction. Let us assume there ls another subpattern ofprefix(p) that a n  be absorbed by p. If thls is the ease, omording to o w  mining order, we know this subpattern must have been absorbed by prefiz(p) either via new pattern checking or old pattern checking. 8  (Support  raise). If a newly mined pat- tern p can pass both new pattern checking and old pottern checking, then it is safe to w e  p to mise m i n s u p p a t .

Rationale. From Lemma 3.5, there will be two possibilities for p. First, it is D wal closed pattern, i.e., it will not be absorbed by any patterns later. Second, it will be absorbed by a later found pattern, and this pattern can only absorb pattern p. In this case, we will not use the later found pot- tern to mise support because it has already been wed to mise  SUPP(P<, 1 = SUPP(P).

LEMMA 3.5  LEMMA 3.6  support when we found pottern p (OT p's precedents). Thus it is safe to w e  p to mise minsuppart. .

To accelerate both new and old pattern checking, we in- troduce a tw-level index header table into the pattern-tree structure. Notice that if a pattern can absorb (or be a b sorbed by) another pattern, the two patterns must have same support. Thus, our first index is based on the sup- port of a pattern. In addition, for new pattern checking, we only need to check if pattern p can be absorbed by another pattern that also contains ij; and for old pattern checking, we need to check if p can absorb p r e f i s ( p )  that ends with the second-last item of p .  To speed up the checking, our second level indexing use5 the last item.ID in a closed pat- tern as the index key. At each pattern-tree node, we also record the length of the pattern, in order to judge if the corresponding pattern needs to be checked.

The twelevel index header table and the checking process are shown in the following example.

EXAMPLE 3 (Closed pattern verification). Figure 4 shows a two-level indezing structure for venfiiention of closed patterns. Based on the lemmas, we only need to indez into  U  Figure 4: Twc-level indexing for veriRcatlon of closed patterns.

the fimt structure based on the itemset support, and based on its matching ofthe last two items in the indez structurr to find whether the eomspnding closed node is in the tree.

3.4 Algorithm Now we summarize the entire mining process and present  ALGORITHM 1. Mining top-k jwepuent closed itemsets with  Input:  (1) A tmnsoction database DB, (2) an integer k, i.e., the k most freguent closed itemsets to be mined.

and (3) mind ,  the minimal length of the frequent closed itemsets.

Output .  The set offreguent closed items& which satisfy the wquirement.

Method.

the mining algorithm.

minimal length m i n d  in a large tmwaetion database.

1. Initially, minsupport = 0;  2. Scan DB once', Collect the mcumncefreepueney (count) ' This scan can be replaced by a sampling process, which re- duces one database scan but increases the chance that items may not be ordered very well due to biased sampling, which may hurt performance later. Thus such scan reduction may or may not improve the performance depending on the data characteristics and the ordering of transactions.

of every item in tmnsactions and sort them in fre- quency descending order, which fonns sorteditedist and the header of FP-tree.

3. Scan D B  again, construct FP-tree, updote the I-count in the header of the FP-tree, use closed node count a m y  to mise minsuppol-t, and use this support to prune the tree. After scanning DB,  tmuerse FP-tree with descendantsum check, which mises minsuppmt further, and the mised minsuppol-t is used to prune the tree.

4. %e mining is perfomzed b y  tmuersing down the header table, storting with the item with the fimt non-zero 1- count and genemte the conditional FP-tree fov each item, M along as its 1-count is no less than the cumnt minsupport.  Each conditional FP-tree i s  mined in ?bottom-up? (i.e.. long to short) manner. Each mined closed pattern is inserted into a pottern-tree.

5 .  Output pattern fmm pattern-tree in the order of their support. Stop when it outputs le patterns. .

4. EXPERIMENTAL EVALUATION In this section, we report our performance study of TFP  In particular, we compare the efficiency of TFP with CHARM over a variety of datasets.

and CLOSET, two well known algorithms for mining frequent closed itemsets. To give the best possihle credit t o  CHARM and CLOSET, our comparison is always based on assigning the best tuned minsuppmt (which is difficult to obtain in practice) to the two algorithms so that they can generate the same t o p k  closed patterns for a user-specified k d u e  (tu- der a condition of m i n d ) .  These optimal minsuppol-t are obtained by running TFP once under each experimental con- dition. This means that even if TFP has only comparable performance with those algorithms, it will still he far more useful than the latter due to its usability and the difficulty to  speculate minstqpol-t without mining. In addition, we also study the scalability of TFP.

The experiments show that (1) the running time of TFP is shorter than CLOSET and CHARM in most cases when mind is long, and is comparable in other cases; and 2) TFP has nearly linear scalability.

4.1 Datasets Both real and synthetic datasets are used in experiments,  and they can be grouped into the following two categories.

1. Dense datasets that contain many long frequent closed patterns:  1) pumsb census data, which consists of 49,044 transactions, each with an average length of 74 items; 2) connect-4 game state information data, which consists of 67,557 transactions, each with an average length of 43 items.

and 3) mushmomcharacteristic data, which consists of 8,124 transadions, having an average length of 23 items. All these datasets are obtained from the UC-Irvine Machine Learning Database Repository.

2. Sparse datasets: 1) gazalle click stream data, which consists of 59,601 transactions with an average length of 2.5 items, and contains many short (length below 10) and  some very long closed patterns, (obtained from BlueMartini Software Inc.), and 2) TlOI4DlOOKsynthetic data from the IBM dataset generator, which consists of 100,000 transac- tions with an average length of lO.items, and with many closed frequent patterns having average length of 4.

4.2 Performance Results All experiments were performed on a 1.7GHa Pentium-4  PC with 512MB of memory, running Windows 2000. The CHARM code was provided to us hy its author. The CLOSET is an improved version that uses the same index-based closed node verification scheme as in TFP.

We compared the performance of TFP with CHARM and CLOSET on the 5 datasets by varying m i d  and K. In most cases K is selected to be either 100 or 500 which covers the range of typical K ~ I u e s .  We also evaluated the scalability of TFP with respect to the size of database.

For the dense datasets with many long closed patterns, TFP performs consistently better than CHARM and CLOSET for longer m i n l .

Dense  Datasets:  WWl l  a) K = 100 ?.dWPI  b) K = 500 Figure  5: Performance on Connect-4 (I)  Figure 5 shows the running time of the three algorithms on the connect-4 dataset for K fixed at  100 and 500 respec- tively and m i n l  ranging from 0 to 25. We observe that, TFP?s running time for K at 100 and 500 remains stable over the range of mind. When mind reaches 11 or 12, TFP starts to outperform CHARM and the same for CLOSET when m i n l  reaches 17 or 18. The reason is that for long patterns, the minsupport is quite low. In this case, CHARM has to retain many short frequent patterns before forming the required longer patterns, and the FP-tree of CLOSET would also contain a large number of items that takes up much mining time. On the other hand, TFP is able to use the m i d  length restriction to cut many short frequent pat- terns early, thus reduce the total running time.

Figure 6 shows the running time of the three algorithms on the connect-4 dataset with m i n l  set to 10 and 20 respec- tively and K ranging from 100 to 2000. For the connect-4 dataset, the average length of the frequent closed patterns is above 10, thus mind at 10 is considered to be a very low length restriction for this dataset. From a) we can see that even for very low length restriction such as 10, TFP?s perior- mance is comparable to  that of CLOSET and CHARM when it runs without giving support threshold. For m i n l  equal to 20, the running time for TFP is almost constant over the full range, and on average 5 times faster than CHARM and 2 to 3 times faster than CLOSET. We also noticed that, even for very low m i n d  as K increases, the performance gap between TFP, CLOSET, and CHARM gets smaller.

,-    ....... *.. .... ....* '.+---+--f ........... ,..... *... ~* ...., I i ...... * .........

0- 0- m m 100 10 ,am 1m 1110 lam ,m am m n am a0 la0 IBO ,Eo Im 1100 am  a) minl  = 10 b) mind = 20 Figure  6: Performance on Connect-4 (11)  Figure 7 shows the running time of the three algorithms on the mushroom and pumsb datasets with K set to 500 and mind ranges from 0 to 25. For the mushroom dataset, when m i n l  is less than 6 all three algorithm have simi- lar low running time. TFP keeps its low running time for the whole range of mind and starts to outperform CHARM when m i n l  is as low as 6 and starts to outperform CLOSET when m i n l  is equal t o  8. Pumsh has very similar results as connect-4 and mushroom datasets.

i - 1  >:m j i:  s  2 1  d i  I, 2).

U ,..*,*  I IO I3 1 1 I IO I$ I 1l  *.ndWRI Y U d W R ,  a) Mushrwm b) Pumsb Figure  7: Performance on Mushroom and Pumsb  Sparse Dataset:  Experiments show that TFP can effi- ciently mine sparse datasets without minsupport. It has comparable performance with CHARM and CLOSET for low m i d ,  and outperforms both on higher mind.

Figure 8a) shows the running times of TFP, CHARM, and CLOSET on T1014D100K with K fixed at  100 and m i n l ranges from 1 to 10. Again, it demonstrates TFP's strength in dealing with long m i n l .  At m i n l  = 8, the performance of CHARM and CLOSET starts deteriorating, while TFP re- tains its good performance. Figure 8b) shows the perfor- mance on the same dataset but with m i n l  fixed at  8 and varying K from 200 to 2000. The curves show that when K is above 400, the running times of CHARM and CLOSET are around 3 times slower than TFP.

The experiments on the gazelle dataset are shown in Fig- ure 9. For smaller K, TFP outperforms both CHARM and CLOSET for minl  greater than or equal to 5. For K = 500, TFP continues to outperform CLOSET for mind greater than or equal to 5, and has similar performance as CHARM.

Rom this performance study, we conclude that TFP has good overall performance for both dense and sparse datasets.

I ts  running time is nearly constant over a wide range of K and mind values for dense data. Unlike CHARM and CLOSETwhose performance deteriorates as mind increases,  b) L = 8 Figure  8: Performance on T1014D100K  a) K = inn b) K = 500 Figure  9: Performance on Gazelle  TFP's running time stays low. The reason is inherent from the mining strategy of TFP, CHARM, and CLOSET. In mast time, the support for long patterns is lower than that of short patterns. Thus even with the optimal support given, both CLOSET and CHARM are unable to prune short fre- quent patterns early, thus causing much time spent on min- ing useless patterns. On the other hand, TFP is able to use the minl  length restriction to cut many short frequent patterns early, thus improves its running time instantly. In addition, TFP does not include any nodes that reside above m i n l  level to participate in the mining process. As mind increases, more nodes reside above the minie level of the tree means that less conditional FP-trees need to he built, thus keeps the running time low.

Besides the good performance over long m i n l  values, the performance of TFP over short m i n l  values (even when mind = 1, i.e., no length constraint) is still comparable to that of CLOSET and CHARM. In such cases, the run- ning times between the three do not differ much, and both CLOSET and CHARM were run with the optimal support threshold while TFP was not given any support threshold.

Scalability Test: Our performance tests showed that the running time of TFP increases linearly with increased dataset size.

5. DISCUSSION In this section, we discuss the related work, how to gener-  ate association rules from the mined t o p k  frequent patterns, and how to push constraints into the mining process.

5.1 Related work Recent studies have shown that closed patterns are more  desirable [5] and efficient methods for mining closed pat-     terns, such as CLOSET [7] and CHARM [B], have been de- veloped. However, these methods all require a user-specified support threshold. Our algorithm does not need the user to provide any minimum support and in most cases runs f a t e r than two efficient algorithms, CHARM and CLOSET, which in turn outperform Apriori substantially [7, 81.

Fu, et al. [Z] studied mining N most interesting item- sets for every length 1,  which is different from our work in several aspects: (1) they mine all the patterns instead of only the closed ones ; (2) they do not have minimum length constraintssince it mines patterns at  all the lengths, some heuristics developed here cannot be applied; and (3) their philosophy and methodology of FP-tree modification are also different from ours. To the best of our knowledge, this is the first study on mining t o p k  frequent closed patterns with length constraint, therefore, we only compare our method with the two best known and well-performed closed pattern mining algorithms.

5.2 Generation of association rules Although t o p k  frequent itemsets could he all that a user  wants in some mining tasks, in some other cases, sjhe wants to mine strong association rules from the mined t o p k  fre- quent itemsets. We examine how to do this efficiently.

Items in the short transactions, though not contributing to the support of a t o p k  itemset of length no less than mind ,  may contribute to the support of the items in it.

Thus they need to be included in the computation which has minimal influence on the performance. To derive cor- rect confidence, we have the following observations: (1) The support of every Liternset is derived at  the start of min- ing. (2) The set of t o p k  closed itemsets may contain the items forming subsetjsuperset relationships, and the rules involving such itemsets can be automatically derived. (3) For rules in other forms, one needs to use the derived t o p k itemsets as probes and the known minsupport as threshold, and perform probe constrained mining to find the support only related to those itemsets. (4) As an alternative to  the above, one can set mind= 2, which will derive the patterns readily for all the combinations of association rules.

5.3 Pushing constraints into TFP mining Constraint-based mining 14.61 is essential to t o p k  mining  since users may always want to put constraints on the data and rules to be mined. We examine how different kinds of constraints can be pushed into the t o p k  frequent closed pattern mining.

deep into the TFP-mining process. he succint constraints should be pushed deep to select only those itemsets hefore mining starts and the anti-monotonic Constraint should be pushed into the iterative TFP-mining process in a similar way as FP-growth.

Second, for monotone constraints, the rule will also be similar to that in traditional frequent pattern mining, i s , if an itemset mined so far (e.g., o k d )  satisfies a constraint "sum 2 loo", adding more items (such as e) still satisfies it and thus the constraints checking can be avoided in further expansion.

Third, for convertible constraints, one can arrange items in an appropriate order so that the constraint can be t r ans formed into an anti-monotone one and the anti-monotone constraint pushing can he applied.

First, succinct and anti-monotone constraints can be pushed  Interested readers can easily prove such properties for t o p k frequent closed pattern mining.

6. CONCLUSIONS We have studied a practically interesting problem, mining  top-k frequent closed patterns of length no less than mind, and proposed an efficient algorithm, TFP, with several opti- mizations: (1) using closednodexcount and descendantsum to raise mindupport before tree mining, (2) exploring the topdown and bottom-up combined FP-tree mining to first mine the most promising parts of the tree in order to raise rninsupport and prune the unpromising tree branches, and (3) using a special indexing structure and a novel closed pattern verification scheme to perform efficient closed pat- tern verification. Our experiments and performance study show that TFP has high performance. In most cases, it out- performs two efficient frequent closed pattern mining algo- rithms, CLOSET and CHARM, even when they are running with the best tuned minsuppwt .  Furthermore, the method can be extended to generate association rules and to incor- porate user-specified constraints.

Based on this study, we conclude that mining t o p k  fre- quent closed patterns without minsupport should be more preferable than the traditional minsuppwt-based mining for frequent pattern nuning. More detailed study along this direction is needed, including further improvement of the performance and flexibility at  mining topk frequent closed patterns, as well as mining t o p k  frequent closed sequential patterns or structured patterns  Acknowledgements. We are grateful to Dr. Mohammed Zaki for providing the code and data conversion package of CHARM and promptly answering many questions  7. REFERENCES [I] R. Agrawal and R. Srikant. Fast algorithm for mining  [2] A. W.-C. Fu, R. W.-W. Kwong, and J. Tang. Mining  [3] J. Han, J. Pei, and Y. Yin. Mining frequent patterns  [4] R. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang.

association rules. VLDB'94.

n-most interesting itemsets. ISMIS'W.

without candidate generation. SIGMOD'OO.

Exploratory mining and pruning optimizations of constrained esociations rules. SIGMOD'SR.

Discovering frequent closed itemsets for association rules. ICDT'99.

161 J. Pei, J. Ha", and L. V. S. Lakshmanan. Mining frequent itemsets with convertible constraints.

ICDE'O1.

[7] J. Pei, J. Han, and R. Mao. CLOSET An efficient algorithm for mining frequent closed itemsets.

DMKD'OO.

[8] M. J. Zaki and C. J. Hsiao. CHARM An efficient algorithm for closed itemset mining. SDM'O2.

[5] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal.

