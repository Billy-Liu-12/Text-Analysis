CLUSTER BASED DATA REDUCTION METHOD FOR TRANSACTION DATASETS

Abstract The common feature of transaction datasets is that it is very huge in size, so it is important to develop a technique for dataset reduction. The process of dataset reduction must not change the features of the original dataset; this will increase the effectiveness and efficiency of extracting association rules from these datasets without affecting the original data. Disjoint clusters that have different number of transactions will be introduced in order to minimize the search space, this in turn will decrease the time required to mine the desired rules by dealing with each cluster individually. The support and confidence measures will be used to determine the frequent item sets and exclude the others.

Keywords: dataset reduction, Association rules mining, Instance selection  1. Introduction Recently, an explosion of information  technology has been occurred due to the  generation of electronic data, widespread of  transferring data through Internet, the extensive  use of bar codes, and the improvement in  computer systems, all these factors besides the  available capabilities of database systems lead to  produce and accumulate huge amounts of data,  these data caused a lot of problems as the data  overload and lack of scalability. The tools used  to deal with huge datasets are stronger than  those used to collect and store the data [1, 2].

There is a vital requirement for data mining  techniques and tools that can generate useful  information and knowledge from huge volumes  of data [3, 4].

Association rules mining (ARM) has  come out as an essential method for constructing  knowledge base systems [5], and has been used  to solve real-world problems such as intrusion  detection [6], customer relationship management  (CRM) [7], and Web-based decision support  problems [8], and many other problems [9].

Data reduction is an important step of  data pre-processing [10], one of its main goals is  preparing data for mining.  In transaction  processing systems, data reduction enables  decision makers to know the real dimensionality  of the enterprise database(s), which in turn leads  to an informed decision making.

The huge size of transaction datasets  makes the process of generating association  rules very difficult task and constructing  association graph is impossible or in best cases  infeasible, so it is highly required to use a  technique that improves the process of frequent  itemset generation and reduces the size of the  dataset before constructing the association graph  to extract the desired association rules, this  technique is called dataset reduction based on     clustering (DRBC). DRBC best utilizes the  memory space needed to store the generated  frequent itemsets.

In this work, we investigate the  implementations of Instance selection method as  among the dataset reduction methods that are  commonly use, we proposed dataset reduction  based on clustering (DRBC) technique, which is  considered as an improvement to the traditional  dataset reduction methods.

The rest of the paper is organized as  follows: Section 2 describes the data reduction  methods and approaches that have been  employed in previous works. Also, provide an  example to give an extra explanation to our  proposed algorithm. Section 3 concludes the  work presented in this paper.

2. Data Reduction Methods Evolutionary algorithms, or genetic algorithms  (GA), and instance selection (IS) are among the  dataset reduction methods that are commonly  used in the literature. These two methods aim to  reduce the search space since data reduction in  the process of knowledge discovery in databases  (KDD) can be viewed as a search problem. The  following subsections illustrate briefly these  methods.

2.1. Evolutionary Algorithms (EAs) Data reduction in KDD may be considered as a  search problem, it may be solved using  evolutionary algorithms (EAs). Data reduction  can be thought as a strategy in data  preprocessing. EAs give promising results when  they are used to solve the IS problem [11].

Evolutionary algorithms used to optimize the  rules generated by an association rule mining  algorithm. Overall, the system can expect the  rules that hold negative attributes when the EAs  are used over the association rules that are  produced by an ARM technique; this is since the  rule mining techniques do not take in  consideration the pessimistic appearances of  attributes. EAs do a global search and deal with  attribute interaction better than the greedy rule  induction algorithms normally used in data  mining, and this is considered as the main  motivation for using EAs in the discovery of  high-level prediction rules [12], as shown in Fig.

1.

2.2. Instance Selection (IS) Instance selection method aims to minimize the  number of rows in a transaction dataset [10].

Instance selection method is a central task in the  data training phase of KDD [13]. It is one of the  effective means of data reduction. There are  different strategies of IS that can be followed,  some of these strategies are: (1) sampling, (2)  boosting, (3) prototype selection (PS), and (4)  active learning. Prototype selection (PS)  algorithms can be classified as displayed in  figure 1.

Figure 1 Prototype Selection (PS) Algorithms (Cano et al. 2003)  The main two strategies that IS  participates in are: (1) Instance Selection for  Prototype Selection (IS-PS) and (2) Instance  Selection for Training Set Selection (IS-TSS). In  the IS-PS approach, the analysis of the results  obtained when choosing prototypes (instances)  for a nearest neighbor (NN) algorithm, while in  the IS-TSS approach, the selected instances are  first used to construct a decision tree, after that  the tree is used to organize new patterns.

2.2.1. DRBC Method Figure 2 presents the steps of the  proposed dataset reduction based on clustering  (DRBC) technique, which is considered as an  improvement to the traditional dataset reduction  methods  Algorithm DRBC Input: Set of m clusters, m is the length of the longest transaction record Number of transaction records N Output: Minimized set of clusters For (k = 1; k ? N; k++) Determine infrequent 1-itemsets; For (k = 2; k ? m; k++) Use upward closure property to remove infrequent k itemsets;   Figure 2 The Proposed Dataset Reduction Method Algorithmic Steps   As shown in Fig. 2. After building the clustering  matrix and assigning each transaction to the  proper cluster according to their length and  building the clustering matrix, the clustering  matrix consists of rows and columns, where the  rows are transaction records and the columns are  the items, the value 0 is put in the cross of  transaction  j with item k if this item is not exist  in that transaction, otherwise the value 1 is  recorded, so all the items are represented as bit  vectors to simplify the process of computing the  support value for each item and identify the  frequent 1-itemsets from other itemsets available  in the cluster matrix.

Infrequent 1-itemsets are determined  and their supersets in the kth clusters, k  2, are  ignored from further processing, which may lead  to a minimized set of frequent itemsets with less  loss of knowledge, and this improves the  efficiency and scalability of the process of  mining association rules from the association  graph that will be constructed using a graph  based association rules mining algorithm.

2.2.2. Example of database transactions  We provide an example to give an extra  explanation to our proposed algorithm; the  minimum support threshold is 20%. There are  20 transactions and 5 different items in the  database. We assume ? as in most of sequential  rule mining algorithms ? that the items are in  lexicographical order. A transaction database  example is shown in Table 1; the items are  represented by letters rather than numbers to  deal with some worst cases, where the  numbering step is required. The first step, as we  mentioned earlier in the previous section, is  scanning the database to determine the length of  each transaction, the length means the number of  items in a transaction, and at the same time,  assigning numbers to the items, item A will be  given the number 1, item B the number 2 and so  on. This will help us in constructing the cluster  matrix, after that we don?t need to rescan the  database, as we will move to deal with the  clustering matrix that can be easily reside in the  main memory.

In our example, the maximum transaction length  is 4, and so, there will be at most four clusters.

Since there are no transactions of length 1, the  total number of clusters is 3 as shown in Fig. 3,  the table contents are 0s or 1s to denote absence  or existence of an item in a transaction, after  constructing the matrix, each column is the bit  vector for the corresponding item, and so, no  need to make further contrasts with the cluster  matrix. These bit vectors are used in identifying  the infrequent 1-itemsets and exclude them with  their supersets from further processing in order  to simplify the process of mining the desired  association rules.

Table 1: an example of database transactions  TID Items TID Items TID Items TID Items  10 A, B, C 60 C, E 110 A, B, C, E 160 A, C, E 20 B, C 70 B, C, E 120 C, D 170 C, E 30 A, E 80 A, B, C, D 130 B, C, D 180 A, C, D 40 A, C, D, E 90 C, E 140 A, C, E 190 A, E 50 A, C 100 A, B, D 150 B, D, E 200 C, E    By counting the number of 1s in each bit vector,  we determine the support for each candidate  itemset of length 1, as the following: support  ({1}) = 60%, support ({2}) = 40%, support  ({3}) = 70%, support ({4}) = 45%, and support  ({5}) = 0.55. Thus the frequent             1-  itemsets are: {{1}, {3}, {4}, {5}} as their  support is not less than 45%, itemset {2} and all  its supersets, i.e. {2, 3}, {1, 2, 3}, {2, 3, 5}, {1,  2, 3, 4}, {1, 2, 4}, {1, 2, 3, 5}, {2, 3, 4} and  {2, 4, 5}will be eliminated from the database of  transactions.

Shaded transactions are those that will be  ignored from further processing which will  reduce exponentially the time required to  generate all frequent itemsets. After that, the  frequent 2-itemsets are generated by partially  scanning the clusters, i.e. the kth clusters, where  k  2, only the frequent 2-itemset is {3, 5}  whose support is 0.5 which is greater than the  minimum support threshold, i.e. 0.45. So, {1, 4}  will be removed from clusters in figure 3 (b).

The last itemset is {1, 3, 5} is frequent since its  support is 100%, thus all frequent itemsets are  generated.

Figure 3 (a) The cluster tables for the database given in table 1 (b) The reduced clusters     1 2 3 4 5  T2 0 1 1 0 0  T3 1 0 0 0 1  T5 1 0 1 0 0  T7 0 0 1 0 1  T10 1 0 0 1 0  T12 0 0 1 0 1  T14 0 0 1 1 0  T19 1 0 0 0 1  T20 0 0 1 0 1  T1 1 1 1 0 0 T6 1 0 1 0 1  T8 0 1 1 0 1  T11 1 1 0 1 0  T15 0 1 1 1 0  T16 1 0 0 1 1  T17 0 1 0 1 1  T18 1 0 1 1 0  T4 1 0 1 1 1 T9 1 1 1 1 0  T13 1 1 1 0 1   1 3 4 5  T3 1 0 0 1  T5 1 1 0 0  T7 0 1 0 1  T10 1 0 1 0  T12 0 1 0 1  T14 0 1 1 0  T19 0 1 0 1  T20 0 1 0 1  T6 1 1 0 1  T16 1 0 1 1  T18 1 1 1 0  T4 1 1 1 1  Item Item  Cluster 4  Cluster 3  Cluster 2  Transaction Transaction  a b     3. Conclusion and Future Work DRBC takes benefit of the representation of data  items as bit vectors to simplify the extraction of  infrequent 1-itemsets, i.e. itemsets that have only  one item, and converts the data matrix into  cluster matrices of variable dimensions, such  that each one of these matrices (rows are the  transactions? identifiers and columns are items)  is processed separately to generate frequent item  sets of a specific length, i.e. local frequent  itemsets, then by using the upward closure  property, it can easily eliminate infrequent  itemsets from all cluster matrices in an efficient  way. As a future work, DRBC can be expanded  to cover different types of data, for example,  time series data, mix mode data, etc.

4. References [1] U. Fayyad and R. Uthurusamy, "Evolving  data into mining solutions for insights," Communications of the ACM, vol. 45, pp. 28-31, 2002.

[2] X.-B. Li and V. S. Jacob, "Adaptive data reduction for large-scale transaction data," European Journal of Operational Research, vol. 188, pp. 910-924, 2008.

[3] D. Anbarasi and K. Vivekanandan, "Dimension Reduction Techniques for Market Basket Analysis."  [4] S. M. Weiss, Predictive data mining: a practical guide: Morgan Kaufmann, 1998.

[5] M. C. Okur and M. Buyukkececi, "Big data challenges in information engineering curriculum," in EAEEIE  (EAEEIE), 2014 25th Annual Conference, 2014, pp. 1-4.

[6] Y. Peng, et al., "A descriptive framework for the field of data mining and knowledge discovery," International Journal of Information Technology & Decision Making, vol. 7, pp. 639-682, 2008.

[7] M. J. Shaw, et al., "Knowledge management and data mining for marketing," Decision support systems, vol. 31, pp. 127-137, 2001.

[8] J. P. Shim, et al., "Past, present, and future of decision support technology," Decision support systems, vol. 33, pp.

111-126, 2002.

[9] L. A. Kurgan and P. Musilek, "A survey of Knowledge Discovery and Data Mining process models," The Knowledge Engineering Review, vol. 21, pp. 1-24, 2006.

[10] J. R. Cano, et al., "Using evolutionary algorithms as instance selection for data reduction in KDD: an experimental study," Evolutionary Computation, IEEE Transactions on, vol. 7, pp. 561-575, 2003.

[11] C. R. Reeves and D. R. Bush, "Using genetic algorithms for training data selection in RBF networks," in Instance selection and construction for data mining, ed: Springer, 2001, pp. 339-356.

[12] M. Saggar, et al., "Optimization of association rule mining using improved genetic algorithms," in Systems, Man and Cybernetics, 2004 IEEE 3725-3729.

[13] A. I. R. L. Azevedo, "KDD, SEMMA and CRISP-DM: a parallel overview," 2008.

