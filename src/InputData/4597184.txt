Text Categorization based on Boosting Association Rules

Abstract  Associative classification is a novel and powerful method originating from association rule mining. In the previous studies, a relatively small number of high-quality associa- tion rules were used in the prediction. We propose a new approach in which a large number of association rules are generated. Then, the rules are filtered using a new method which is equivalent to a deterministic Boosting algorithm.

Through this equivalence, our approach effectively adapts to large-scale classification tasks such as text categoriza- tion. Experiments with various text collections show that our method achieves one of the best prediction performance compared with the state-of-the-arts of this field.

1 Introduction  The associative classifier uses the association rules pro- duced by a frequent pattern mining algorithm [1], [7]. Since the associative classifier is a rule-based classifier, humans can easily understand its operation, and the prediction re- sult provides a simple and direct interpretation. Moreover, it can exploit the combined information of multiple fea- tures as well as a single feature, while Na??ve Bayes and Support Vector Machine(SVM) classifiers only consider a single feature. This means that text categorization can use phrase occurrence information as well as word occurrence information.

In text classification problems, large-scale is inevitable due to the large number of word features, class labels and example documents in the text corpus. Many high-order1  association rules are generated in the induction procedure for the associative classifier. Generally, high-order rules are more informative; hence the classification with those rules has a better performance [15], [22]. However, as the order  1In associative rules, order means the number of words occurring in the rule.

of the rules grows, the number of generated rules increases very rapidly so that the computational complexity is unbear- able.

The paradigm of the previous methods of associative classification was to use a small number of high-quality rules regardless of the order of rules (or preferring high- order rules) [12], [21]. To reduce the time of rule mining, they limited the number of word features by eliminating the words that were estimated as not useful in the classification.

In contrast, our method uses most words in the vocabulary except for stop-words as features. To avoid generating an excessive number of rules, the order of the rules is limited to less than a prescribed threshold.

Since the number of those generated rules can be sev- eral millions, a small number of rules should be selected for prediction in real situations. When predicting for test instances, the selected rules votes as much as their individ- ually weighted scores. We propose a new rule selection al- gorithm based on validation by training examples. In addi- tion, we show that this rule selection process is equivalent to a deterministic AdaBoost algorithm [6]. This analogy is utilized to filter the generated rules, which greatly im- proves the training and generalization errors. Our method fits well to text corpora because a large number of low-order rules can cover the high dimensional feature space of test instances.

This paper is structured as follows. Section 2 introduces the associative text classification and provides the problem formulation. Section 3 describes the classification rule min- ing. Section 4 explains the algorithm of boosting associa- tion rules and presents an analytical justification. Section 5 proposes our method of handling multi-label predictions.

Section 6 shows experimental results and the analyses. Fi- nally, the paper concludes in Section 7.

DOI 10.1109/ICSC.2008.70    DOI 10.1109/ICSC.2008.70     2 Associative Classification  2.1 Application to Text Categorization  The overall system architecture of the associative classi- fication for text documents is shown in Fig. 1. The left-hand side of the figure denotes a training process and the right- hand side a testing process.

Training Documents eg.) 20newsgroups  Pre-processing: -Feature Selection  -Into Word-vector format  Pre-processed  Training: -Frequent Pattern Mining  -Classification Rule Extraction Rule Database  R1: p1 -> c1, R2: p2 -> c2,  ? Rn: pn -> cn  New test document  Conversion Into  a Pattern  Pattern Matching  Decide the Classes  Alt.atheism or Talk.religion.misc  Rule-1, Rule-2, ? Rule-k  Figure 1. Associative Classification ? Train- ing and Testing  First, raw text data is pre-processed for rule mining.

Each document is converted into a transactional record for- mat. From this pre-processed database, we mine frequent patterns, i.e. classification rules. Because the number of mined rules is very large, we filter out some useless or re- dundant rules and select a small number of well-qualified rules. This process is called Pruning. Finally, we construct a classification-rule database with these selected rules.

When a new document comes in to be classified, we con- vert it into a pattern of words and search the database for matching rules. With matched rules, we decide which class the test document is assigned to.

2.2 Formal Definition  The database for a classification task consists of at- tributes and their instances. The instances of attribute val- ues constitute training (or testing) examples. If we denote the set of features as A = {A1, A2, ..., Ad}, where d is the dimension of the feature space, then a set called do- main can be represented as X = A1 ? A2 ? ... ? Ad. Let Y = {c1, c2, ..., c|Y |} be the set of class labels, then the set of training examples D is  D = {(xj , yj) | xj ? X, yj ? Y }.

An item is defined as the instance value of an attribute.

Itemset(or pattern) is a term denoting the instance of sin- gle or combined features. Association rule mining extracts frequent itemsets from a transactional database. We de- fine the support of a pattern as the number of training ex- amples in which the pattern occurs. The pattern pj ? Ai1 ? Ai2 ? ... ? Aik is called frequent if the support of the pattern exceeds some given threshold value min sup.

A classification rule rk is a mapping from a set of fea- tures to the set of class labels:  Ai1 ? Ai2 ? ... ? Aik ? Y. (1)  If the rule (1) is frequent, then it is called a class associa- tion rule(CAR) [12]. We define the confidence of a rule as the support of the rule divided by the support of the pattern of the left-hand side of the rule. From application to appli- cation, the extent of CAR may be confined to those of which confidence values exceed a given threshold min conf.

For example, assume that in the document collection about sports games, the following word phrase and the as- sociated game was mined using a rule mining algorithm,  run, diamond ? baseball (5, 0.71),  where the numbers within the parenthesis denote the sup- port and confidence of the rule, respectively. This rule says that the co-occurrence of run and diamond in a document implies the theme of the document to be the baseball.

Let R = {r1, r2, ..., r|R|} be the final classification rule set. When we have a test example x, we apply the rules to x. Let sij be the score which rule ri produces supporting that the class label of x is cj . Then, Sj , the total score for cj when the entire rule set is considered can be written as:  Sj = ?  ri?R and cj?Y sij . (2)  Then, the final prediction label c? for x is determined such that  c? = arg max cj?Y  Sj .

3 Generating Class Association Rules  3.1 Transactional Representation for Doc- ument Examples  Most text corpora have a high-dimensional feature space.

Since they are not relational databases, there is no pre- determined length for an example record. The average num- ber of words in a document is far less than the vocabulary size, which shows the sparsity of word distribution in text collections. A document can be modeled statistically as the     Dataset msup / mconf # rules  Sick from UCI 1 / 50 71,828 e-mail 5 / 0 42,182  20 newsgroups 0.02 / 5 8,185,780  Table 1. The number of generated rules  events of the words that constitute the document. There are two different document models: the Multi-variate Bernoulli Model and the Multinomial Model [13]. The former ignores the count information of words and uses only binary infor- mation (present or not-present), while the latter includes the information of word count.

In the multinomial model, a document d which is a se- quence of words, ?w1, w2, w4, w2?, is modeled as the set {w1, w2, w4} and their occurrence counts. But it is known that information about the multiple occurrence of words does not yield much additional assistance for an exact clas- sification [4], [19]. Thus, as the input format of a docu- ment to the mining process for association rules, we choose a ?transactional? format which has no occurrence count in- formation.

3.2 Mining Classification Rules  The frequent pattern mining was done with the method of Han et al. [7]. CBA [12] and CMAR [11] took 1% and 50% as threshold values of min sup and min conf respec- tively. Such high threshold values cause a relatively small number of generated rules, which could raise the classifying precision. But this may miss useful information contained in the rules which have lower confidence or support than the thresholds.

Our approach adopts the opposite direction to the above heuristic. We lower the values of min sup and min conf to the lowest possible. Hence, we collect many rules that are considered slightly better than a random guess. Table 1 lists the thresholds in % and the numbers of rules mined from raw texts. The first data is used in CBA, and the second is an e-mail collection by Itskevitch [8]. With 20 newsgroups we produced twenty times more rules compared to Itskevitch.

Lowering association mining thresholds can make it im- possible to generate rules within a reasonable time unless another constraint is applied. We limit the order of gener- ated rules to less than a small number. In the case of text corpus, the order is limited as less than or equal to 2 (or 3). Despite this limitation, the number of generated rules is still too large. The next section considers the rule selection process for better classification performance.

4 Boosting Association Rules  4.1 Analogy to Boosting  According to PAC-learning theory [9], a strong PAC- learning algorithm is an algorithm that, given ?, ? > 0 and access to random examples, outputs with probability 1?? a hypothesis with error at most ?. A weak PAC-learning algo- rithm satisfies the same conditions but only for ? ? 1/2?? where ? > 0.

Schapire [17] proved that any weak learning algorithm can be efficiently transformed or boosted into a strong learn- ing algorithm. Generally, Boosting refers to producing a very accurate prediction rule by combining rough and mod- erately inaccurate rules-of-thumb. AdaBoost [6] is a very effective and efficient boosting algorithm, where the weak learner produces the hypotheses with any ?t ? [0, 1] and the hypotheses are boosted adaptively.

A class association rule with a low confidence value may fall under the class of weak classifiers. The pruning pro- cedure of our classification method can be thought as the boosting procedure where the weak classifiers are not re- trained but selected from the set of association rules accord- ing to the modified distribution of training examples.

4.2 A New Algorithm based on Boosting  Fig. 2 represents our new algorithm to boost weak as- sociation rules. This algorithm is a modification of the database coverage pruning [12] based on the principle of Boosting. To simplify our analysis, we begin with a binary classification problem where Y = {0, 1}. A class asso- ciation rule in Fig. 2 corresponds to a weak hypothesis of AdaBoost.

In Fig. 2, the main loop iterates for the rule index t. For each iteration, rt is applied to all of the training examples remained so far. If the prediction is correct, then the cover count vi is increased as much as the confidence value of rt at Line 2-b-ii. This is equivalent to the weight update step of AdaBoost [6]:  wt+1i = w t i?  lti = wti exp(l t i ln ?),  where wti is the weight of example xi at t round and ? is a weight update factor. lti , the loss of xi for rt, corresponds to the confidence value which is added to vi by rt. Boost- CARs is not an adaptive algorithm since ? is fixed to some value less than 1 for all t = 1, ..., T .

Next, if the updated vi exceeds the threshold cvth, then the example (xi, yi) is removed from the database. This corresponds to the modification in the distribution of the ex- amples in AdaBoost. Finally, after all T iterations, Boost- CARs yields a final hypothesis hf combined with the se- lected hypotheses rt?s.

Algorithm BoostCARs Input Class Association Rules: {r1, r2, ..., rT },  Database: {(x1, y1), (x2, y2), ..., (xN , yN )}, cvth (database coverage threshold)  Initialize the cover count: v1i = 0 for i = 1, ..., N , sort the rules {rj} in the descending order of  confidence For t = 1, 2, ..., T  1. CorrectPred = false 2. For i = 1, 2, ..., N  (a) Apply rt to xi.

(b) If rt predicts yi, then  i. CorrectPred = true ii. vi = vi + conf(rt)  iii. If vi > cvth then delete (xi, yi) from the database.

3. If CorrectPred = false then conf(rt) = 0  Output the final hypothesis  hf (x) = arg max y?Y  ? t:ht(x)=y  conf(rt)  Figure 2. Algorithm of Boosting weak associ- ation rules  4.3 Training Error  It can be shown that the training error of BoostCARs approaches to zero exponentially fast as the training pro- gresses.

Theorem 1. Suppose BoostCARs chooses the class asso- ciation rule whose error ?t ? 1/2 ? ? for some ? > 0 at each round t (t = 1, ..., T ). Then, the error ? of the final hypothesis hf output by BoostCARs is bounded above by  ? = |{i : hf (xi) ?= yi}|  N ? exp(?T?2/2). (3)  Proof Sketch. First, we convert BoostCARs into an equiv- alent form of the deterministic version of AdaBoost, where the weight update factor ?t is the same for all t = 1, ..., T .

We choose ? to be 1??. Also, we change rt in BoostCARs into h? : X ? [0, 1] such that  h?t(xi) = {  conf(rt) if yi = 1 1 ? conf(rt) if yi = 0 .

Then, the remining proof procedure becomes equivalent to that of Hedge(?) algorithm of the on-line allocation model [6]. If we apply the analysis to BoostCARs, then the error bound (3) can be directly derived.

The theorem says that, if we collect many weak associa- tion rules all of which performances are slightly better than  random guessing by ?, then the error will decrease exponen- tially fast. This analysis can be easily extended to the case of multi-class problems if we adopt the techniques in [6].

4.4 Generalization Error  In our approach, since the min sup and min conf pa- rameters are set to the lowest, the number of the omitted features is minimized. Thus, these features can cover the attributes of the test domain better. In addition, since Boost- CARs has abundant low-order classification rules, it can cover the word patterns of test documents better than the approach which adopts high-order rules. This property en- ables BoostCARs to show minimized generalization errors.

Schaprie et al. [18] defined the classification margin of a training example to be the difference between the num- ber of correct votes and the maximum number of votes re- ceived by any incorrect label. They proved that maximizing the margin can improve the generalization error of a classi- fier and Boosting tends to increase the margins of training examples when the final classifier is combined from weak hypotheses. Although the final classifier becomes larger, its test error constantly decreases.

Based on the margin theory, we adjust parameter cov- erage threshold (cvth) in Fig. 2 so that BoostCARs can achieve the minimum generalization error. It can be achieved if the whole range of generated rules are selected evenly according to the principle of Boosting and there are no remaining rules and examples when the rule selection process is completed. Then, the margin of the examples is maximized and the value of cvth is selected as the value of that case.

5 Multi-Label Classification  Our associative classifier yields the prediction scores for all class labels at once. Thus, it is necessary to set a threshold for the scores to determine whether the predic- tion is right or wrong [20]. We propose a novel threshold- ing scheme that is similar to ?RCut? in [24]. RCut always predicts k class labels with the highest scores for each test document.

We assume that, regardless of the number of answer la- bels, the prediction score of an answer label occupies larger than some ratio in the total prediction score of a document.

Let ? be such ratio threshold. If we denote Sj as the pre- diction score of class cj as in (2), then the estimated class labels {c?} is determined by this relation:  c? ? {cj | cj ? Y and Sj? ci?Y Si  ? ?}. (4)  The uneven distribution in the number of training exam- ples of different class labels invokes another problem. In     such circumstance, the class label which has a large num- ber of training examples will have much more classification rules than the class label which has a small number of train- ing examples. Thus, the prediction score of the latter would always be less than the score of the former even when the latter is an correct label. The sum of prediction scores Sj for class cj in (2) would no longer denote a correct prediction score.

We introduce a normalized prediction score model to compensate for such uneven distribution of training exam- ples. First, we define a weight function w of a class label cj as  w(cj) =  |Y | ?|R|  i=1 conf(ri)? k?Rj conf(rk)  , (5)  where conf(ri) is the confidence value of ri and Rj is the set of the rules with the label cj as their consequents. When w(cj) is multiplied to the final prediction score Sj , the score S?j is effectively normalized between the class labels:  S?j = w(cj)1/2 ? Sj , (6)  where the square root of w(cj) smoothes further the effect of w. When we predict on a severely imbalanced corpus, we replace Sj in (4) with this S?j before judging correct labels.

6 Experiments  6.1 Test Collections  Reuters-21578 and 20 newsgroups [10] are multi-class and slightly multi-label text collections. Reuters-21578 is a collection of articles from Reuters newswire. We used the ModApte split version [2] from which we further selected the documents with the top 10 TOPICS categories, which are earn, acq, money-fx, grain, crude, etc. This final set is the same as the one used in HARMONY [21]. 20 news- groups is a collection of USENET mail postings whose cat- egory set includes the names of the 20 discussion groups, for example, comp.os.ms-windows.misc. Some statistics on the collections are listed in Table 2, where we can find that 20 newsgroups is larger and more complex than Reuters- 21578.

OHSUMED [16] is the collection of citations to med- ical journals from the year 1987 to 1991. Instead of the original OHSUMED collection, many researches have used ?Heart Diseases (HD)? subset of MeSH(Medical Subject Headings). It is also referred to as ?HD-119?. HD-119 con- tains 16,595 documents in 107 distinct categories. We di- vided this sub collection according to the publication year: from 1987 to 1990 for training, 1991 for testing.

We used BOW-toolkit [14] for pre-processing of the doc- uments. The header part except for the title was removed  Reuters-21578 20 newsgroups  # documents 9,979 19,997 # classes 10 20  vocabulary size 22,424 90,833 # words /doc 49.2 77.3 # labels /doc 1.10 1.05  Table 2. Statistics on the text databases in our experiment  from the training documents. We filtered out general stop words and conducted no stemming. We prepared the train- ing input according to the document model in Section 3.1.

We implemented our associative classification system with C++. Our codes were executed The program was run on a Linux machine with 4GB memory and 2.8GHz CPU speed.

6.2 Parameter Selection  Fig. 3(a) represents the prediction accuracy of our asso- ciative classifier for various min sup and min conf thresh- olds when applied to Reuters-21578 collection. In all re- sults shown in this paper, min sup is represented with ab- solute numbers not with ratios. The accuracy is represented with Breakeven point (BEP) between the recall and the pre- cision measures from the Information Retrieval community.

As the min sup and min conf thresholds become lower, the number of rules grows larger and the performance improves accordingly. These results agree with our intuitions in the previous sections.

In Fig. 3(b), the x-axis represents the highest order of rules included in the rule set, and the y-axis BEP or F1 score. F1 is the harmonic mean of the precision and the recall. BEP is used in the Reuters and 20 newsgroups, and F1 in the OHSUMED collection. As the order exceeds some number, the generalization performance starts to drop due to overfitting. The order at which the performance decreases differs to text corpora. Most large-scale text collections show the best performance at the order of two. The reason why such high-order rules do not assist in raising the pre- diction accuracy is that high-order word phrases in the rules have lower probability to occur in the test set than low-order phrases.

Fig. 3(c) shows the selection process of optimal score ratio thresholds in the multi-label associative classification.

As the average number of class labels per document be- comes larger, the ratio decreases. We select optimal values through validation using training examples.

Fig. 4 shows a detailed process of BoostCARs algorithm for 20 newsgroups data. The x-axis represents the id of the generated rules which are sorted in the descending order of confidence values. The y-axis represents the number of re-             min sup (reuters)  B E  P (  % )  min_conf=3% min_conf=10% min_conf=20%  (a) minimum support and confidence  1 2 3 4         rule order  B E  P /F  (%  )  reuters 20 news ohsumed  (b) rule order  0 10 20 30 40 50        score ratio  B E  P /F  (%  )  reuters 20 news ohsumed  (c) score ratio threshold  Figure 3. Selection of Classification Performance Parameters  0 2 4 6 8  x 10      rule id (20 news)  # ex  am pl  es  cvth=1,     BEP=82.43 cvth=25,   BEP=89.87 cvth=100, BEP=90.51 cvth=400, BEP=90.38  Figure 4. Selection of coverage threshold  maining training examples when rule rt predicts for the ex- amples remaining at round t?1. If the coverage threshold is low, then the training examples are exhausted prematurely.

If the coverage threshold is high, then the probability for incorrect rules to be selected increases because inappropri- ate training examples still remain. The coverage threshold should be selected so that the training examples can be ef- fectively used in the process. The selection of 100 as cvth value shows the best performance in this case.

Fig. 5 represents the learning curve of our associative classifier for Reuters. The x-axis represents the number of rules included in the prediction. As the size of the combined classifier grows, the training error decreases constantly. At the same time, the test error continues to decrease without overfitting to the training examples. From this, we find that the margin theory on the generalization error also applies well to our boosting algorithm.

6.3 Performance Comparison  Table 3 lists the classification accuracies of HARMONY and SVM for Reuters-21578. The result of linear SVM classifier is from [5]. The performance for each class is  0 2 4 6 8 10 12 14  x 10           # rules applied (reuters)  er ro  r (%  )  testing training  Figure 5. Learning curve of Associative Clas- sifier  measured with BEP, and finally averaged for all the test in- stances. BCAR with min sup threshold 5 shows the best BEP, 93.5%. This performance is obtained when the order of rules is set to three and the min conf threshold set to 20, which is slightly better than random guessing.

In addition to such excellent classification accuracy, our approach shows good efficiency in computation. Table 4 shows the time spent from mining association rules up to predicting test instances for Reuters-21578. The comput- ing time of BCAR grows as min sup value is lowered. Al- though the time of BCAR with min sup of 5 is several times of that of HARMONY, BCAR with min sup of 20 shows comparable execution time to HARMONY while its accu- racy is kept better than HARMONY.

Table 5 lists the prediction performances of previous studies with 20 newsgroups. The classification accuracy is measured with micro-averaged BEP. The BEPs are ob- tained by 4-fold cross validation. The computing time of all the classifiers is a single-fold time elapsed during training and testing phase. The result of Naive Bayes is obtained using the Rainbow tool [14]. Recently, the classifiers based     Harmony SVM BCAR (msup=60) (linear) (msup=5)  acq 95.3 93.6 97.8 corn 78.2 90.3 82.2 crude 85.7 88.9 88.1 earn 98.1 98.0 97.4 grain 91.8 94.6 86.5  interest 77.3 77.7 83.5 money-fx 80.5 74.5 84.4  ship 86.9 85.6 92.6 trade 88.4 75.9 89.8 wheat 62.8 91.8 79.9 Total 92.0 92.0 93.5  Table 3. Classification performance of Reuters-21578  Harmony BCAR BCAR (msup=60) (msup=20) (msup=5)  BEP 92.0 92.8 93.5 time(sec) 73 99 333  Table 4. Computing time of Reuters-21578  on SVM have shown state-of-the-art results for text cate- gorizations. The result of SVM-1 in the third column is from [3]. They conducted a feature selection based on clus- tering of the words appeared in the corpus. SVM-2 [25] is a hierarchical model of base SVM classifiers constructed with the 3-level hierarchy of the 20 categories. Except for SVM-1, the rest three classifies used the same feature set for the training and testing procedures.

BCAR of Table 5 is trained with very low support and confidence thresholds: 3 and 5% respectively. We limit the order of the rules k to 2. Let us consider the complexity of generating association rules. It grows exponentially to the number of words in the vocabulary. Thus, if we gen- erated high-order association rules, then the computational complexity would be unbearable. Fortunately, the classifi- cation performance did not improve anymore for the orders higher than two. The value of BEP 90.5 is the best ever reported among the results for 20 newsgroups. In addition, the computing time has also decreased compared with that of SVM-2.

In Table 6, we compare the performances of the clas-  Naive SVM-1 SVM-2 BCARBayes clustering hier.

BEP 83.2 88.6 89.0 90.54 time 8.3 mns 4 hrs 5.3 hrs 4.9 hrs  Table 5. Classification performance of 20 newsgroups  LLSF SVM BCARhierarchical w/o weight w/ weight 55 58.7 53.2 61.6  Table 6. Classification performance of OHSUMED  sifiers which have been tested on OHSUMED. The per- formance is measured by the F1 averaged for all the class labels. Yang [23] conducted several classification exper- iments with various kinds of classifiers on OHSUMED.

Among them, we have put the result of Linear Least Square Fit (LLSF) classifier in the first column. Yoon et al. [25] reported a better classification result using the hierarchical SVM classifier. The weighing on prediction scores in (6) is very important to OHSUMED where the number of train- ing examples is unevenly distributed with respect to the cat- egories; the third column that applies no weighting shows a poorer performance than the weighted one, the fourth col- umn.

7 Conclusion  Our approach is different from others in that it generates as many rules as possible including the rules whose pre- diction accuracy is moderate or even worse than random guessing. We proposed a new selection method that fil- ters rules on the principle of Boosting. In addition, the new scheme for multi-label classification was provided based on score-ratio thresholding. By many experiments with rep- resentative test collections, we showed that our approach achieves excellent classification performance in the large- scale sparse data such as text corpora.

We need to decrease the number of class association rules to lessen the learning time while keeping the classi- fication performance the same as the original one. In addi- tion, the parameter setting in the associative classification depends deeply on the distribution of the training database.

For further study it is worth investigating which aspects of the distribution affect the performance.

