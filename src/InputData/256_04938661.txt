Discovering Relational Knowledge from Two Disjoint Sets of Literatures Using Inductive Logic Programming

Abstract? Literature-based discovery for hypothesis genera- tion is a subarea of text mining that aims to discover novel or previously-unknown knowledge from two complementary but disjoint (CBD) sets of literatures. The discovery approach is based on Swanson?s discovery models where indirect con- nections between two disjoint sets of literatures A and C could be found through a set of common terms B extracted from A and C. In this paper, we report an application of an Inductive Logic Programming (ILP), specifically the WARMR algorithm, to the field of literature-based discovery.

The application extends Swanson?s closed discovery model to uncover potentially meaningful knowledge in forms of relational frequent patterns that may exist after the connections between the two sets of literatures are found. We conducted an experi- ment between two pairs of topics: Raynaud?s disease and fish oils, and Down syndrome and cell polarity. The experimental results demonstrate that our method can be used to enhance a literature-based discovery approach by providing potentially meaningful knowledge in addition to the indirect connections.



I. INTRODUCTION  L ITERATURE-based discovery for hypothesis genera-tion is a subarea of text mining that aims to discover novel or previously-unknown knowledge from literature. It was first introduced in 1986 by Swanson, who discovered links between two sets of unrelated literatures by finding common terms or concepts present in both sets [1]. The key feature of the method is stated as: given two unrelated topics A and C, A is indirectly connected with C if there exists at least a term B that commonly occurs in retrieved literature set A and retrieved literature set C.

A number of interesting studies have been conducted that extended Swanson?s ABC discovery methods. Lindsay and Gordon used lexical statistics for extracting and ranking terms in the common B-term list [2]. Pratt and Yetisgen- Yildiz employed biomedical concepts instead of words or phrases [3]. Srinivasan proposed a concept profile for knowl- edge discovery [4]. Jin and Srihiri introduced concept chains [5]. Hristovski et al. exploited frequent item sets and associa- tion rules in their knowledge discovery approach [6], whereas Stegmann and Grohmann utilized co-word clustering for hypothesis generation [7]. Finally, Wren extended the mutual  Supphachai Thaicharoen is a Ph.D. student in Computer Science and Engineering Department at University of Colorado Denver. (email: sup- phachai.thaicharoen@email.ucdenver.edu).

Tom Altman is a professor in Computer Science and Engineering Depart- ment at University of Colorado Denver. (email: tom.altman@ucdenver.edu).

Katheleen Gardiner is a professor in Department of Pediatrics at Uni- versity of Colorado Denver. Her research interests are Down syndrome and chromosome 21. (email: katheleen.gardiner@ucdenver.edu).

Krzysztof J. Cios is a professor at Virginia Commonwealth University and affiliated with IITiS Polish Academy of Sciences. His main research interests are bioinformatics and data mining. (email: kcios@vcu.edu).

information for ranking terms in the common B-term list [8].

In this paper, we present a further enhancement of Swanson?s method by discovering additional meaningful knowledge after the indirect connections are found. Mining knowledge from two sets of literatures can thus be transformed into a problem of relational data mining.

Relational data mining is an area of data mining that is used to extract knowledge or patterns from a relational database. It could be typically attained by the use of Inductive Logic Programming (ILP). ILP is an inductive machine learning technique at the intersection of machine learning and logic programming [9]. Given examples and background knowledge as inputs, an ILP system generates output as a relational description of the examples using predicate logic.

There are two main categories of ILP tasks, predictive and descriptive. A predictive ILP system generates hypothesis in terms of relational rules that can be used for classifi- cation of new examples. A descriptive ILP system aims to discover regularities in a set of examples. The advantages of ILP over typical data mining methods are its expressive data representations and ability to incorporate background knowledge. PROGOL [10], FOIL [11], and WARMR [12], [13] are among the existing important ILP algorithms.

In this study, we are interested in extracting knowledge in forms of relational frequent patterns which correspond to frequent Prolog queries. Discovery of relational frequent patterns was introduced by Dehaspe and Toivonen [12].

Several studies and applications have used this approach. Li- akata and Pulman employed WARMR to automatically learn domain theories in terms of association rules for company succession events from a parsed text corpus, and then used Finite State Automata (FSA) to construct a graphical repre- sentation of the learned theories [14]. Blat?a?k implemented a relational rule mining algorithm called dRAP for mining long first-order frequent patterns from distributed text data [15]. He transformed the mined patterns into propositional representation before solving two text mining tasks: context- sensitive text correction of English language and morpho- logical disambiguation of the Czech language. King et al.

applied WARMR to find frequent patterns from a database of chemical compounds tested for carcinogenicity in rodents [16]. In their study, each compound was described by a combination of various numbers of the following five logical relations, atom-bond description, generic structural groups, genotoxicity, mutagenicity and structural indicators. King et al. encoded the frequent patterns generated by WARMR as attributes and used them for classification. They found that methods which used these encoded WARMR outputs as     attributes in their predictive models were among the top three most accurate classifiers.

We propose here an extension of literature-based discovery for hypothesis generation using WARMR to find relational frequent patterns between two sets of disjoint literatures.

With our approach, two tables were generated from a pair of complementary but disjoint (CBD) sets of retrieved litera- tures, and a third table was built from the set of common terms extracted from the two literature sets. First-order logic representation was then used to represent attributes in a table. We performed experiments using two pairs of topics: Raynaud?s disease and fish oils, and Down syndrome and cell polarity. The experimental results demonstrate that our approach is complementary to a typical literature-based discovery method, and can be used to discover potentially meaningful knowledge in addition to indirect connections.

The paper is organized as follows. We present the technical background in Section II, namely, the WARMR algorithm and Swanson?s ABC discovery models. Then, we describe the data and the pre-processing tasks, including our method, in Section III. Next, we present results from performed ex- periments and discuss them in Section IV. Finally, concluding remarks are given in Section V.



II. TECHNICAL BACKGROUND  A. WARMR  WARMR is an ILP data mining algorithm introduced by Dehaspe and Toivonen [12], [13]. It is an extension of the APRIORI association rule mining algorithm [17] for discovering frequent queries in a relational database.

Given a relational database D and a declarative language bias definition L, WARMR discovers queries that have the number of returned records greater than or equal to a user- specified minimum support (minsup), and generates rela- tional association rules with a confidence value greater than or equal to a user-specified minimum confidence (minconf ).

The declarative language bias definition is used for defining admissible query patterns, which limits the search space of WARMR.

In WARMR, Prolog is used to represent queries, patterns, rules and database, and a term query extension is used for relational association rule.

A query extension is an existentially quantified implica- tion. Given a Prolog clause,  ?- l1, ..., lm. ??- l1, .., lm, lm+1, ..., ln., where 1 ? m < n, a query extension can be formulated by Equation 1.

?- l1, ..., lm ? lm+1, ..., ln., (1)  where ?- l1, ..., lm. is called the body, lm+1, ..., ln. the head and ?- l1, .., lm, lm+1, ..., ln. the conclusion of the query extension.

WARMR is a level-wise algorithm based on breath-first search in the lattice spanned by a specialization operator or a subsumption relation ?. A pattern ?p1 is more general than  p2? or ?p2 is more specific than p1?, if p1 ? p2. In addition, p1 ? p2 if and only if p2 |= p1, where |= is the logical implication operator.

Similar to APRIORI, WARMR iterates between candidate generation and candidate evaluation phases. In the candidate generation phase, candidates are derived from frequent pat- terns generated from the previous steps using a refinement function. These new candidates are valid if they (i) conform to the declarative language bias definition L, (ii) are not more specific than infrequent patterns, and (iii) are not the same as already-generated frequent queries. In the candidate evaluation phase, frequencies of candidates are calculated with respect to the database. Candidates with frequencies greater than or equal to the user-specified minimum support are added to the frequent pattern set. Otherwise, they are added to infrequent pattern set.

In contrast to APRIORI algorithm that employs a subset relation, WARMR utilizes the notion of ?-subsumption re- lation for pruning candidates. ?-subsumption is a stronger variants of ? relation. A query q1 ?-subsumes q2 if and only if there exits a set of variable/value pairs, {X1/x1,X2/x2, ...,Xn/xn} of the query q2 such that every atom in the query q2 resulted from variable/value substitu- tions occurs in query q1. Note that if the query q1 ? q2, then the query q1?-subsumes q2. However, if the query q1?- subsumes q2, it does not always mean that q1 ? q2.

B. Swanson?s ABC discovery models  Swanson and Smalheiser introduced two literature-based discovery models, open discovery and closed discovery, for hypothesis generation from two complementary but disjoint sets of literatures [18].

In the open discovery model, a starting topic a is used as a keyword search to retrieve a set of relevant literatures from a database. This set is called the first literature set.

Subsequently, a set of terms B = {b1, b2, ..., bQ} is extracted from the first literature set, and then each bq is used as a keyword search to retrieve another set of literatures called the second literature set of bq. Next, a set of terms Cbq ={  c bq 1 , c  bq 2 , ..., c  bq W  } are extracted from the second literature  set of bq for all terms bq in B. Finally, the starting term a is used together with each term cbqw in Cbq as a keyword search for q = {1, ..., Q} and w = {1, ...,W}. If there is no literature returned, a new association between a and cbqw through bq is discovered, a ? bq ? cbqw .

In the closed discovery model, topics a and c are assumed to be unrelated. It means that there is no literature returned when a and c are used together as a keyword search. Two literature sets, A-set and C-set, are retrieved from a database.

The A-set uses a as a keyword search, and the C-set uses c as a keyword search. A new association is discovered if there exists a term b that belongs to terms extracted from both the A-set and the C-set. The open discovery model is usually used for generating hypotheses and the closed discovery model is used for testing hypotheses.

The notion of two complementary but disjoint sets of literatures was introduced by Swanson [19]. Two separate sets of literatures are complementary, if, when combined, the relationship between them gives some important inferences and insights. In addition, two separate sets of literatures are disjoint, if there are no common articles between them and they do not cite or mention each other.



III. DATA AND METHODS  A. Data sets  Inspired by Swanson?s discovery of the indirect connec- tions between Raynaud?s disease and fish oils, we retrieved sets of biomedical abstracts from NCBI PubMed using the following search strategy:  ? Raynaud?s disease and fish oils Search keyword: ?raynaud disease?, Publication date: from 1980 to 1985, Field: Text Word and Number of returned abstracts: 902 Search keyword: ?fish oils?, Publication date: from 1980 to 1985, Field: Text Word and Number of returned abstracts: 187  In addition, based on our interest in Down syndrome, we used a second pair of topics, Down syndrome and cell polarity, and retrieved sets of biomedical abstracts using the following search strategy:  ? Down syndrome and cell polarity Search keyword: ?down syndrome?, Publication date: 2008, Field: Text Word and Number of returned ab- stracts: 881 Search keyword: ?cell polarity?, Publication date: 2008, Field: Text Word and Number of returned abstracts: 930  B. Methods  Discovery of relational knowledge from a pair of CBD sets of literatures can be achieved using a relational data mining approach where data are usually stored in a relational database.

We constructed a relational database in first-order logic.

A table was created for each CBD set of literatures where a record corresponds to an abstract, and the PubMed ID of the abstract is used as a primary key. To find links between two tables, we extracted terms that are common to the two literature sets and used them as foreign keys. Accordingly, for a pair of CBD sets, A and C, and a set of common terms B, three tables are constructed: Topic A table, Topic C table and Common Term B table. The relational knowledge to be discovered is frequent patterns of common terms that link to biomedical abstracts containing co-sentences between two terms associated with a set of user-selected verbs. For two terms to be co-sentenced, one term must be in a subject part and another term must be in the object part of a sentence, where the subject and object parts are separated by a verb.

Details follow.

1) Pre-processing:  ? Given two unrelated topics, retrieve two sets of biomedi- cal abstracts from NCBI PubMed, one set for each topic.

Two topics are assumed to be unrelated if they have never been co-mentioned in the same article.

? Eliminate identical abstracts if any exists to ensure disjointedness.

? Perform sentence boundary detection and part-of-speech tagging on each abstract using Lingpipe (available at http://alias-i.com/lingpipe) and Genia Tagger [20], [21], respectively.

? Extract important terms (noun phrases) from each set of abstracts tagged by Genia Tagger [20], [21], and carried out stop word removal using a combined set of English and PubMed stop words.

? Compute weights of each extracted terms using a term weighting scheme such as document frequency, term frequency, or TF ? IDF .

? Rank terms in each set in descending order based on their numerical weights, and select a subset of terms using a threshold. Most single-word phrases with high ranks are too general: therefore, eliminate all single- word phrases.

? Extract common terms between the two sets of terms.

? Compute and ranked the common terms based on rela-  tive document frequency formulated by Equation 2. This is similar to Lindsay and Gordon?s study [2].

RDFt = DF 1t + DF  t  N1 + N2 , (2)  where RDFt is the relative frequency of a common term t, DF 1t is the document frequency of term t in the first literature set, DF 2t is the document frequency of term t in the second literature set, N1 is the total number of documents in the first literature set, and N2 is the total number of documents in the second literature set.

? Construct a relational database in first-order logic as shown in Fig. 1.

2) Co-sentence in first-order logic: We employed first- order logic for representing our relational database. Based on King et al. [16], we represented each abstract by a set of co-sentence predicates. In other words, a topic of interest is analogous to a compound, each retrieved abstract is analogous to atoms constituting the compound, and the co- sentence predicates extracted from the abstract are analogous to the properties of atoms that constituted the compound. Our motivation to use co-sentence, rather than co-mentioned, in the same abstract is based on Wren et al. [22] who found that the relatedness between two entities co-mentioned in the same sentence is higher than that co-mentioned in the same abstract.

In addition, since our goal is to uncover co-sentence information in an abstract in which at least one common term is present, we assume that when a common term occurs in an abstract, it relates to every sentence in that abstract in the same way that it relates to the abstract.

To construct a co-sentence predicate we divided a sentence into three parts based on verbs that occur in the sentence: subject phrase, verb phrase and object phrase. Let np be a noun phrase and v be a verb,  Sentence 1: np1 v1 np2.

=? Subj(np1), Verb(v1), Obj(np2).

Then, if a subject phrase or object phrase contained more than one noun phrase as shown in Sentence 2, an enumeration of all different subject/verb/object combinations was generated.

Sentence 2: np1 v1 np2 np3.

=? Subj(np1), Verb(v1), Obj(np2).

=? Subj(np1), Verb(v1), Obj(np3).

Similarly, if a sentence contained more than one verb as shown in Sentence 3, all possible different combinations of subjects, verbs, and objects were generated.

Sentence 3: np1 v1 np2 v2 np3.

=? Subj(np1), Verb(v1), Obj(np2).

=? Subj(np1), Verb(v1), Obj(np3).

=? Subj(np1), Verb(v2), Obj(np3).

=? Subj(np2), Verb(v2), Obj(np3).

Finally, a relational table for a set of literatures was constructed using PubMed abstract IDs as primary keys and co-sentence predicates as attributes.

In addition, based on the notion of co-sentence, types of noun phrases, subject or object phrases, within the same sentence are not important. In other words, a co-sentence of Subj(np1), Verb(v1), and Obj(np2) is the same as that of Obj(np2), Verb(v1), and Subj(np1). As a result, noun phrases are alphabetically ordered in a co-sentence predicate.

Moreover, verbs in the passive voice are converted into the base form.

Example: Given an abstract in A-set with PubMed ID 1234567 containing a a sentence np1 v1 np2, if np2 is alphabetically preceding np1, a co-sentence predicate of this sentence can be expressed as follows.

Co-sentence predicate: topicA co sentence(?1234567?, ?np2?, ?np1?, ?v1?).

3) Relational database in first-order logic: For a pair of complementary but disjoint sets of literatures A = {a1, a2, ..., ap, ..., aP } and C = {c1, c2, ..., cr, ..., cR}, whose abstract contains one or more sentence patterns, and a set of common terms B = {b1, b2, ..., bq, ..., bQ}, three tables in first-order logic can be created as follows:  Let T = {t1, t2, ..., ti, tj , ..., tN} be a set of terms, V = {v1, v2, ..., vs, ..., vS} be a set of verbs extracted from all abstracts of both sets of literatures, and B ? T  Topic A table: topicA co sentence(?a1?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?a1?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?a1?, ?ti?, ?tj?, ?vs?).

...

topicA co sentence(?a2?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?a2?, ?ti?, ?tj?, ?vs?).

...

...

topicA co sentence(?ap?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?ap?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?ap?, ?ti?, ?tj?, ?vs?).

...

...

topicA co sentence(?aP ?, ?ti?, ?tj?, ?vs?).

topicA co sentence(?aP ?, ?ti?, ?tj?, ?vs?).

...

Topic C table: topicC co sentence(?c1?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?c1?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?c1?, ?ti?, ?tj?, ?vs?).

...

topicC co sentence(?c2?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?c2?, ?ti?, ?tj?, ?vs?).

...

...

topicC co sentence(?cr?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?cr?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?cr?, ?ti?, ?tj?, ?vs?).

...

...

topicC co sentence(?cR?, ?ti?, ?tj?, ?vs?).

topicC co sentence(?cR?, ?ti?, ?tj?, ?vs?).

...

Common term B table: common term(?b1?).

common term(?b2?).

common term(?b3?).

...

...

common term(?bq?).

...

...

common term(?bQ?).

Note that within the same co sentence predicate, ti ?= tj , and across different co sentence predicates, ti or tj in one predicate may or may not be the same as ti or tj in other predicates. Similarly, vs in one predicate may or may not be the same as vs in other predicates. In other words, each predicate is unique within a biomedical abstract. A graphical description of a relational database constructed from a pair of complementary but disjoint sets of literatures is shown in Fig. 1.

Fig. 1. A relational database in first-order logic built from two complementary but disjoint sets of literatures.

4) ACE data mining system: We used WARMR in ACE data mining system for searching for relational frequent patterns and rules. ACE is a data mining software tool that provides a common interface to a set of relational data mining algorithms. It was developed by the data mining research group at Katholieke Universiteit Leuven [23]. As of version 1.2.14, it consists of a number of useful algorithms such as TILDE (the relational version of the decision tree learner C4.5), WARMR (the relational version of APRIORI association rule mining algorithm) and ICL (the relational version of CN2 rule learner). ACE data mining system can be run in a Prolog system such as SWI-prolog (available at http://www.swi-prolog.org/) under the Linux operating system.



IV. EXPERIMENTS AND RESULTS  To search for frequent relational patterns using WARMR, a key pattern to be counted must first be specified. We used the common term predicate as the key pattern. For the ACE data mining system, two input files are necessary: knowledgebase (.kb) file which contains predicates of relational database and settings (.s) file which contains user-defined key predicates and other necessary parameters such as minimum support and confidence. An input file, background knowledge (.bg) file used to provide knowledge about the domain, is optional.

To create a knowledgebase file, the database needs to be formatted in a way suitable for the data mining system used. Two possible formats are available in ACE data mining system: key format and models format. In the key format, every predicate must contain a key pattern as one of its argument. For the models format, the key pattern is specified by the begin and end predicates. We used the key format for constructing the knowledgebase.

We constructed a knowledgebase file using the top 15  common terms listed in Table 1 and 25 user-selected action verbs listed in Table 2 as constraints. These user-selected common terms and action verbs serve as two functions: (i) they are used for extracting only knowledge of interest and (ii) they are used to reduce the problem space.

An example of knowledgebase file for Down syndrome and cell polarity is shown below.

common term(?epithelial cell?).

common term(?plasma membrane?).

common term(?gene expression?).

...

...

common term(?essential role?).

cell polarity co sentence(?18256323?, ?key role?,  ?itch pathology?, ?presence?, ?correlate?).

...

...

down syndrome co sentence(?17517397?, ?gene expression?,  ?cytochrome p450 aromatase activity?, ?mullerian inhibiting substance?, ?inhibit?).

...

...

The first argument in the cell polarity co sentence and down syndrome co sentence predicates corresponds to the PubMed ID of the abstract that contains the common term in the second argument, the third argument to the first noun phrase, the fourth argument to second noun phrase and finally the fifth argument to a verb that separates these noun phrases in the same sentence.

The experimental results are presented in the rest of this section.

TABLE I  LIST OF THE TOP 15 MOST COMMON TERMS.

No. Raynaud?s disease and fish oils Down syndrome and cell polarity  1 rheumatoid arthristis epithelial cell 2 platelet aggregation plasma membrane 3 blood pressure gene expression 4 vascular disease molecular mechanism 5 platelet function actin cytoskeleton 6 blood viscosity cell division 7 peripheral vascular disease cell migration 8 beneficial effect mental retardation 9 plasma viscosity cell proliferation  10 platelet count transcription factor 11 systolic blood pressure key role 12 connective tissue key regulator 13 essential hypertension crucial role 14 autoimmune disease early stage 15 epidemiological study essential role  TABLE II  LIST OF 25 USER-SELECTED ACTION VERBS.

No. Action verbs  1 activate 2 affect 3 associate 4 cause 5 control 6 correlate 7 decrease 8 enhance 9 exhibit 10 improve 11 increase 12 induce 13 inhibit 14 influence 15 interact 16 involve 17 lead 18 prevent 19 promote 20 raise 21 reduce 22 regulate 23 stimulate 24 suppress 25 treat  A. Raynaud?s disease and fish oils  A subset of relational patterns generated by WARMR on Raynaud?s disease and fish oils data set with the minimum support set to 0.2 is described below.

? freq(2, 3, [common term(A), fish oils co sentence(B, A, blunted circulatory response, membrane phospholipid, associate)], 0.266666666666667).

The format of this frequent pattern output is described as follows. The freq means ?frequent query? or ?frequent pattern?. The first argument corresponds to the tree depth level. In this case, the tree depth level is 2. The  second argument is the frequent pattern number gener- ated by WARMR. Accordingly, this frequent pattern is the third pattern generated in level 2 by WARMR. The third argument is the frequent pattern which consists of common term/1 and fish oils co sentence/5 predicates.

Finally, the fourth argument is the support of this frequent pattern. Since the minimum support is set to 0.2 and the support of this pattern is 0.267, the pattern is considered frequent.

In addition, this output result can be interested as fol- lows: 26.67% of the common terms link to the fish oils abstracts that contain the co-sentence pattern describing ?blunted circulatory response either associates with or is associated with membrane phospholipid.?  ? freq(2, 5, [common term(A), fish oils co sentence(B, A, blunted circulatory response, omega 3, associate)], 0.266666666666667).

26.67% of the common terms link to the fish oils abstracts that contain the co-sentence pattern describing ?blunted circulatory response either associates with or is associated with omega-3.?  ? freq(2, 28, [common term(A), ray- naud disease co sentence(B, A, aggrega- tion platelet retention, collagen, induce)], 0.2).

20% of the common terms link to the Raynaud?s disease abstracts that contain the co-sentence pattern describing ?aggregation platelet retention either induces or is in- duced by collagen.?  ? freq(2, 26, [common term(A), fish oils co sentence(B, A, adp, platelet aggregation,induce)], 0.2).

20% of the common terms link to the fish oils abstracts that contain the co-sentence pattern describing ?adp induces or is induced by platelet aggregation.?    B. Down syndrome and cell polarity  A subset of relational patterns generated by the WARMR on Down syndrome and cell polarity data set with the minimum support set to 0.2 is shown below.

? freq(2, 16, [common term(A), cell polarity co sentence(B, A, increase cell migration, intercellular interaction, correlate)], 0.2).

20% of the common terms link to the cell polarity abstracts that contain the co-sentence pattern describing ?increase cell migration either correlates with or is correlated with intercellular interaction.?  ? freq(2, 33, [common term(A), cell polarity co sentence(B, A, apicolat- eral tight junction, cell proliferation, affect)], 0.2).

20% of the common terms link to the cell polarity abstracts that contain the co-sentence pattern describing ?apicolateral tight junction either affects or is affected by cell proliferation.?  ? freq(2, 293, [common term(A), down syndrome co sentence(B, A, dyrk1a dose sensitive reduction, early endodermal mesodermal differentiation, cause)], 0.2).

20% of the common terms link to the Down syndrome abstracts that contain the co-sentence pattern describing ?dyrk1a dose sensitive reduction either causes or is caused by early endodermal mesodermal differentia- tion.?  ? freq(2, 306, [common term(A), down syndrome co sentence(B, A, prema- ture expression, undifferentiated trisomy es cell, cause)], 0.2).

20% of the common terms link to the Down syndrome abstracts that contain the co-sentence pattern describing ?premature expression either causes or is caused by undifferentiated trisomy es cell.?  In addition to relational frequent patterns, WARMR can be used to generate relational association rules. We modi- fied a setting file by allowing verbs to be a variable with input/output mode, and set both minimum support and con- fidence to 0.1. We then reran WARMR on Raynaud?s disease and fish oils data set. A subset of rules generated by WARMR and their interpretations is shown as follows.

? rules(([fish oils co sentence(B, A, blunted circulatory response, omega 3, C)] :- [common term(A)]), bodyfreq(1.0), sup(0.266666666666667), conf(0.266666666666667), lift(none)).

The generated rule consists of two parts: head and body, which is separated by a ?:-? symbol. The head part corresponds to the fish oils co sentence/5 predicate and the body part corresponds to the common term/1 predicate. It can be interpreted as follows. For all common terms A it holds that if a fish oil abstract that contains a common term A, it will also contain a co-sentence between blunted circulatory response and  omega 3 in which a verb C is a separation between them.

? rules(([raynaud disease co sentence(B, A, aggregation platelet retention, collagen, C)] :- [common term(A)]), bodyfreq(1.0), sup(0.2), conf(0.2), lift(none)).

For all common terms A it holds that if a Raynaud?s disease abstract that contain the common term A con- tains a common term A will also contain a co-sentence between aggregation platelet retention and collagen in which a verb C is a separation between them.

? rules(([fish oils co sentence(D, A, blunted circulatory response, omega 3,E)] :- [common term(A), raynaud disease co sentence(B, A, treatment, vessel patency rate, C)]), bodyfreq(0.2), sup(0.133333333333333), conf(0.666666666666667), lift(none)).

For all common terms A, it holds that if a Raynaud?s disease abstract contains a co-sentence between treat- ment and vessel patency rate, that a fish oils abstract will also contains a co-sentence between blunted circulatory response and omega 3.

? rules(([fish oils co sentence(D, A, omega 3, reactive platelet, E)] :-[common term(A), raynaud disease co sentence(B, A, treatment, vessel patency rate, C)]), bodyfreq(0.2), sup(0.133333333333333), conf(0.666666666666667), lift(none)).

For all common terms A, it holds that if a Raynaud?s dis- ease abstract that contains the common term A contains a co-sentence between treatment and vessel patency rate, that a fish oils abstract that contains the common term A will also contain a co-sentence between omega 3 and reactivate platelet.



V. CONCLUSIONS  In the paper we report an application of the Induc- tive Logic Programming technique, the WARMR algorithm, to the literature-based discovery domain. Our application method extends the closed discovery model of Swanson to the discovery of potentially useful knowledge in the forms of relational frequent patterns from two complementary but dis- joint (CBD) sets of literatures after the indirect connections are found. The experimental results show that our approach can be used to provide information additional to standard literature-based discovery, and that it can be used as an exploratory method in studies. In addition, the results have shown that Inductive Logic Programming is well suited for text mining because it provides flexibility in defining the textual patterns to be discovered and has good expressive power of the outputs.

Several issues need to be considered. For two different topics, the number of returned literatures may be unbalanced for a number of reasons. With unbalanced data sets, patterns in the small literature set may never be included in the output because they do not pass the threshold. In addition, the WARMR setting file is critical. It allows users to provide    constraints to reduce the search space, and to specify queries to be refined.

In the future, our approach can be improved in several ways. First, biomedical concepts can be used instead of words or phrases. Using concepts not only gives semantics to the discovered knowledge, but also helps reduce the search space. Secondly, other types of predicates may be added in addition to the predicate, verbs may be used as a predicate name instead of one of the arguments. Finally, background knowledge can be added to provide additional information or used as constraint of the search space such as information about variations of gene symbols and names.

