Formal Concept Analysis for Digital Ecosystem

Abstract  Formal Concept Analysis (FCA) is an effective tool for data analysis and knowledge discovery. Concept lattice, which is derived from mathematical order theory and lattice theory, is the core of FCA. Many research works of various areas show that concept lattices structures is an effective platform for data mining, machine learning, information retrieval, software engineer, etc. This paper offers a brief overview of FCA and proposes to apply FCA as a tool for analysis and visualization of data in Digital ecosystem, and also discusses the applications of data mining for Digital ecosystem.

1 Introduction  Concept is the base of human?s thinking and logic. We can distinguish and understand the various different objects of the real world by the concepts that describe the categories and attributes of the objects. Concept is also important for data analysis in computer science. Formal Concept Analy- sis (FCA) is a method for data analysis, information man- agement and knowledge representation that takes advantage of the features of formal concepts.

The core of FCA is concept lattice. Theoretical foun- dation of concept lattice founds on the mathematical lattice theory [3, 11]. Lattice is a popular mathematical structure for modeling conceptual hierarchies. Concept lattice is a method for deriving conceptual structures out of data. For concept lattice, we study the relations between objects and attributes in a formal context, and how objects can be hi- erarchically grouped together according to their common attributes. Certain object subset and the set of their com- mon attributes can represent each other, such duality sets form a formal concept, which the attribute subset is called intent and the object subset is called extent. Among the for- mal concepts, it exists an order relation, they form a com- plete lattice: concept lattice. Each node in the lattice is a concept and the corresponding graph (Hasse diagram) is  considered as the generalization and specialization relation- ships between concepts. Such graphical structure represents directly and visually the relations of conceptual hierarchies.

It allows us to analyze and mine the complex data for such as classification, association rules mining, clustering, etc.

The application of concept lattice has been an area of ac- tive and promising research in various fields such as knowl- edge discovery, information retrieval, software engineer and machine learning. For example, in the context of associa- tion rule mining [1], which consists in finding all associa- tions and correlation among data items with a certain crite- rion (degree of support and confidence), it?s a hard problem to find all frequent sets in a large data. The frequent sets generation is the most important step for association rules.

Concept lattice structure [13] has shown to be an effec- tive tool for finding frequent concepts for association rules [17, 20, 18, 24]. The problem of finding frequent sets from data for association rules can be reduced to find frequent concepts with concept lattice. And it?s possible to prune the number of rules produced without information loss using closed set lattice [16].

We propose to use FCA as a tool for data analysis, infor- mation management and knowledge representation in Digi- tal ecosystem.

Digital ecosystem (DE) is a new concept. In the EU re- search community, the Digital Ecosystem aims at provid- ing to small and micro enterprises (SMMEs) ICT appli- cations and services which improve their efficiency, busi- ness integration and synergies within EU territories, but also enabling their integration of local value chains within the global market. In the natural world, an ecosystem is a sys- tem whose members benefit from each other?s participation via symbiotic relationships. For the digital ecosystem, it is a ?digital environment? populated by ?digital species? or ?digital components? which can be software components, applications, services, knowledge, business processes and models, training modules, contractual frameworks, law, etc.

A digital component is any useful idea, expressed by a lan- guage (formal or natural), digitalised and transported within the ecosystem, and which can be processed by humans or by  0-7695-2735-3/06 $20.00  ? 2006    computers. The digital ecosystem infrastructure supports the description, compositions, evolution, integration, shar- ing and distribution of the digital components and of knowl- edge.

The digital ecosystem initiative aims at fostering a cul- tural change in enterprise networking and in business prac- tices. It innovates and impacts on three aspects: technology, business practices and knowledge.

In the digital ecosystem, some knowledge is existing, but some knowledge is previously unknown, implicit, hidden in large data. So we should extract the knowledge from large data. The techniques of data mining (DM) are widely used in research and application to look for relationships and knowledge that are implicit in large volumes of data and are interesting in the sense of impacting an organization?s practice. Hence, we propose to use the techniques of data mining to extract knowledge for the digital ecosystem. We will discuss main issues of application of data mining for the digital ecosystem.

Contributions. This paper makes the following contri- butions: propose to use the technologies of data mining to extract knowledge for digital ecosystem and propose and describe a new tool, formal concept analysis, for digital ecosystem to analyze and represent data and knowledge.

2 Knowledge extraction in DE  Digital Ecosystems are emerging as a novel approach for the catalysis of sustainable development driven by networks of micro- and small enterprises, enabled by ICT services and intelligent cooperative solutions, linking multitudes of socio-economic actors and ICT solutions that are afford- able, trustworthy, adaptive and evolutionary. Dynamic and remote collaboration and interaction in structured and un- structured environments are catalysed by new approaches and ICT technologies, which consider the knowledge, ICT solutions and services as digital ecosystem entities which exhibit the behavior of natural organisms. Thus, the in- frastructure and the services supporting organisational in- teraction and networking will form a digital ecosystem con- sidered a pervasive common infrastructure carrying knowl- edge, models and services, where complex heterogeneous, human and digital entities and systems are themselves com- posed of simpler subsystems.

In order to extract the implicit knowledge from the dig- ital ecosystem, and help companies to provide better, cus- tomized services and support decision making. We discuss the main issues of application of data mining for the digital ecosystem in this section.

2.1 Main aims of DM for DE  For the digital ecosystem, computer science research fo- cuses on the seamless management of distributed, multi- centric and pervasive ICT networks carrying services and representations of knowledge, enabling the creation of a self-organising environment that supports the continuous evolution of business models and software services, by ex- ploiting paradigms from biology and economics.

The application of the data mining techniques should consider the features of the digital ecosystem. The pur- pose of the applications of DM is to develop efficient DM algorithms that scale up large distributed data sets, inte- grate efficient DM algorithms and techniques, and P2P dis- tributed computing environment for extracting knowledge from large amounts of large and complex distributed data for DE. The main research works will focus on :  ? Robustness, reliability, sustainability, and scalability of distributed data mining techniques for DE  ? Automatic, autonomic and dynamic DM processes for DE  ? Dynamic Peer-to-peer (or abbreviated P2P) architec- tures for heterogeneous distributed and complex data  ? Seamless interoperability with service oriented plat- forms  ? Recursive, reflexive, and self-reinforcing knowledge discovery  We should provide dynamic, scalable and flexible DM algorithms for extracting knowledge efficiently from the heterogeneous, high dimensional and distributed data. We can use various specific algorithms or approaches for the same task, because many algorithms or approaches may ex- ert efficient performance on specific data (particular in size, density, and type etc.) and distributed environment. The system should analyze the characteristics of the data, and then automatically choose a befitting algorithm. Different sites may perform different algorithms or approaches for the same task depending on different characteristics of each site. For example, there are lots of clustering algorithms, but most of them is only suitable for one type of data. Maybe many different types of data in the same or different sites for distributed data. Hence each site can use one suitable clustering algorithm or combination of several clustering al- gorithms.

According to the application, we can unify some data mining tasks. For example, if we need both clustering and association rule mining for huge distributed transaction data, we can use some techniques to unify these two tasks for avoiding large amounts of repetitious computing.

0-7695-2735-3/06 $20.00  ? 2006    2.2 Challenges of DM for DE  In recent years, DM has attracted a lot of attention among the fields of research and applications. Many techniques and systems of DM have been proposed. However, the data and infrastructure of DE are very complex. There are many challenges of the applications of DM for DE such as het- erogeneous data, complex data, security, privacy and auton- omy of local databases, network topology and transmission scheme. We need to develop more scalable and more effi- cient techniques of DM for DE.

Data mining and knowledge discovery can benefit from the use of distributed data mining (DDM) techniques to improve mining performance of huge data or distributed data. Although there are many efficient algorithms and techniques for mining centralized data sets, it?s inefficient or incapable to deal with huge data sets or distributed data sets.

There are two main reasons to choose DDM. The first one is that data is very large. If data is too large, it?s hard to store it at a single site, or it?s inefficient or incapable to mine such large data at a single site. In such a case, data may be decomposed into some parts that are distributed at different sites. Then we perform the data mining operations for each site. At the end, the mining results of each site are combined to gain global results. This will optimize centralized data mining since the work load is distributed among the sites.

The second reason is that we need to deal with inher- ent distributed data sets. In fact, various wired and wireless networks such as internet, intranets, local area networks, ad hoc wireless networks and sensor networks etc. produce many distributed resources of data. These distributed data need to be mined to gain global patterns, models or knowl- edge. The straightforward solution is to transfer all data to a central site, where data mining is done. However, even if we have enough capacity to handle the data storage and data mining at a central site, it may be too expensive to transfer the local data sets to the central site. On the other hand, the privacy issue is playing an important role in the emerg- ing distributed data. The distributed data sets may not be transferred because of privacy, security or autonomy of the data sets. Therefore, DDM is an effective and scalable solu- tion for mining huge and distributed data sets in distributed computing environments.

2.3 Complex data in DE  There are large amounts of distributed data in DE. Most of distributed data is heterogeneous, complex and noisy. It?s hard to deal with heterogeneous and complex data. Dis- tributed data can be divided into two categories: homo- geneous and heterogeneous. In homogeneous data, the databases located at different sites have the same attributes  and in the same format, while in heterogeneous data, the at- tributes at each site are different or in different format. Het- erogeneous data is more complex than homogeneous data for DDM tasks.

Most studies on DDM assume that local databases are homogeneous. So many DDM algorithms only deal with homogeneous data. If the local databases are heteroge- neous, we need to adopt different techniques to deal with them. Integrating local models of heterogeneous data is hard for many data mining tasks. Therefore, developing DDM algorithms that can handle heterogeneous data is be- coming increasingly important.

Many real data are high dimensional, high dense, non static, unbalanced. Increasingly complex data sources, structures, and types (like natural language text, images, time series, continuous data streams, multi-relational and object data types etc.) are emerging. It requires the develop- ment of new methodologies, algorithms, tools, and services to mine such complex data. One solution for managing the complex data for DDM is to unify different data. For exam- ple, we can use XML to present complex data.

Sometimes, complexity of data rests with noise in the data. Real world data is dirty and noisy. In a large database, many of the attribute values will be inexact or incorrect, or there are some missing attributes and missing attribute values. Data noise may affect DDM results, so high quality data for DDM is needed. One solution is data preprocessing such as data cleaning, data transformation, data reduction.

2.4 Complex infrastructures in DE  Distributed environment is the base of DE. DDM needs effective infrastructures for distributed large-scale and high- performance computing and data processing. Various wired and wireless networks offer the distributed computing en- vironment. Recently, P2P or grid is considered as more and more important distributed computing environment in DDM.

In centralized data mining, the main concern for the ef- ficiency of a data mining algorithm is its I/O and/or CPU time. In a distributed environment, the communication cost should be considered, it may be a bottleneck in DDM [2, 19]. The cost of transferring large blocks of data may be prohibitive and result in very inefficient implementations in DDM. For a slow network, the communication cost will dominate the overall cost. The communication cost is deter- mined by the infrastructures of the distributed environment, the network bandwidth and the number of messages that are sent across the network. In order to reduce the communica- tion cost, many DDM methods are used to minimize the number of messages sent. Some methods also attempt to load-balance across sites to prevent performance from be- ing dominated by the time and space usage of any individ-  0-7695-2735-3/06 $20.00  ? 2006    ual site. We consider that one important method is to choose a suitable distributed infrastructures and computing service.

The distributed computation infrastructure of P2P or grid is very suitable for DDM. P2P or Grid can provide an effective computational support for DDM applications.

A grid is a geographically distributed computation in- frastructure composed of a set of heterogeneous machines that users can access via a single interface. A grid environ- ment provides high performance computing facilities and transparent access to them in spite of their remote location, different administrative domains and hardware and software heterogeneous characteristics. Grid computing provides a novel distributed environment, computational model, and unprecedented opportunities for unlimited computing and storage resources. It?s distinguished from conventional dis- tributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high- performance orientation. Grids can be used as effective infrastructures for distributed high-performance computing and data processing [7].

DDM on grid, although is a fairly new research topic, has been very active in data mining community. The main disadvantage of grid is that grid software and standards are still evolving. The development of DDM on grid isn?t easy.

P2P architecture is a type of network in which each workstation has equivalent capabilities and responsibilities.

This differs from client/server architectures. Generally, P2P networks are used for sharing files, but a P2P network can also mean Grid Computing. Techniques and applications of P2P for DDM can be found in [23].

The primary disadvantage of P2P is the tendency of com- puters at the edge of the network to fade in and out of avail- ability. Also, accountability for the actions of network par- ticipants could be a difficult problem. Several high-profile implementations have shown that architecture, security, and systems management issues are difficult to control. For these reasons, system managers often prefer to operate P2P systems as separate isolated entities. But, doing so is often impossible for practical applications.

3 Concept lattice  Concept lattice [11] and Closed itemset lattice are based on order theory and lattice theory [4, 22]. They are used to represent the order relation of concepts or closed itemsets.

Concept lattice describes the character of the set pair: intent and extent of concept. Closed itemset lattice emphasizes the representation of the character of itemset.

In this section, we define some basic notions: Data con- text, Closure operator, Formal concept, Concept lattice, Closed itemset and Closed itemset lattice.

Definition 3.1 Data context is defined by a triple (G;M ;R), where G and M are two sets, and R is a relation  a1 a2 a3 a4 a5 a6 a7 a8 1 ? ? ? 2 ? ? ? ? 3 ? ? ? ? ? 4 ? ? ? ? 5 ? ? ? ? 6 ? ? ? ? ? 7 ? ? ? ? 8 ? ? ? ?  Figure 1. An example of data context  between G and M . The elements of G are called objects or transactions, while the elements of M are called attributes or items.

A data context is usually represented by the binary data, but in practice, the values of attribute are not binary, we can transform many-valued data context to binary values context by concept scaling [11].

Definition 3.2 Given a subset A ? G of objects from a data context (G;M ;R), we define an operator that pro- duces the set A? of their common attributes for every set A ? G of objects to know which attributes from M are com- mon to all these objects:  A? := {m ? M | gRm for all g ? A}.

Dually, we define B? for subset of attributes B ? M , B?  denotes the set consisting of those objects in G that have all the attributes from B:  B? := {g ? G | gRm for all m ? B}.

These two operators are called the Galois connection  for (G;M ;R). These operators are used to determine a formal concept.

So if B is an attribute subset, then B? is an object subset, and then (B?)? is an attribute subset. We have: B ? M ? B?? ? M . Correspondingly, for object subset A, we have: A ? G ? A?? ? G.

Thus we define two closure operators as B ? B?? for set M and A ? A?? for set G.

For example, Figure 1 represents a data con- text. G(1, 2, 3, 4, 5, 6, 7, 8) is the set of objects, and M(a1, a2, a3, a4, a5, a6, a7, a8) is the set of items. The crosses in the table describe the relation R of G and M .

Definition 3.3 A formal concept of the data context (G,M,R) is a pair (A,B) with A ? G, B ? M, A = B? and B = A?. A is called extent, B is called intent.

Definition 3.4 If (A1, B1) and (A2, B2) are concepts, A1 ? A2 (or B2 ? B1), then we say that there is a hi- erarchical order between (A1, B1) and (A2, B2).

0-7695-2735-3/06 $20.00  ? 2006    All concepts with the hierarchical order of concepts form a complete lattice called concept lattice.

For example, (68, a1a3a4a6) is a concept of the data con- text of Figure 1. a1a3a4a6 is intent of (68, a1a3a4a6), and 68 is extent of (68, a1a3a4a6).

Definition 3.5 An itemset C ? M is a closed itemset iff C ?? = C.

Thus, a closed itemset is intent of a formal concept. This definition is very important for closed itemset algorithm.

A formal concept or closed itemset describes more a stricter relation between objects and attributes than itemset of association rules mining. For itemset, one attribute set maps to an object set, it?s injective; but for formal concept, there is a bijection between one attribute set and one object set. The intent of a concept is a closed itemset and it?s a maximal itemset.

For example, {a1,a7} is a closed itemset of the data con- text of Figure 1.

Definition 3.6 If C1 and C2 are closed itemsets, C1 ? C2, then we say that there is a hierarchical order between C1 and C2.

All closed itemsets with the hierarchical order of closed itemsets form of a complete lattice called closed itemset lattice.

4 Analysis and visualization of data with FCA in DE  Formal Concept Analysis provides a natural platform for data analysis and knowledge representation. FCA is differ- ent from some of the traditional, statistical means of data analysis and knowledge representation because of its fo- cus on human-centered approaches. Formal concept pos- sesses the same features as philosophical concept. From the formal concepts, we can analyze data such as reveal- ing stronger association or relation between itemset and the set of their common objects, classifying objects, generating implications of attributes or knowledge rules, extracting the hierarchical relation among formal concepts, etc.

FCA also provides an effective tool of knowledge vi- sualization. Concept lattice can show how objects can be hierarchically grouped together according to their com- mon attributes, and the relations between the formal con- cepts. Concept lattice facilitate discussion and exploration of conceptual structures. FCA has been examined with re- spect to principles of knowledge representation. Wille [21] identifies ten functions of knowledge processing (explor- ing, searching, recognizing, identifying, analyzing, investi- gating, deciding, improving, restructuring and memorizing) and investigates how these are supported by FCA.

Several algorithms were proposed to generate concepts or concept lattices on a data context, for example: Bordat [5], Ganter (NextClosure algorithm) [10], Chein [6], Norris [14], Godin [12] and Nourine [15], etc. We can use the formal concepts or concept lattices to analyze and represent the data. Concept lattice can be also applied to distributed data to analyze and represent knowledge in DE [8, 9].

For example, using lattice algorithms, we can generate concepts or concept lattices on a data context (see Figure 1). All formal concepts and the concept lattices on the data context (see Figure 1) are shown in Figure 3. The closed itemset lattice of the data context of Figure 1 is presented in Figure 4. The Figure 2 shows intents of the data context (see Figure 1).

No. Intent No. Intent 1 {a1} 10 {a1a7a8} 2 {a1a2} 11 {a1a2a4a6} 3 {a1a3} 12 {a1a2a7a8} 4 {a1a4} 13 {a1a3a4a5} 5 {a1a7} 14 {a1a2a3a4a6} 6 {a1a2a3} 15 {a1a3a7a8} 7 {a1a2a7} 16 {a1a2a3a4a6} 8 {a1a3a4} 17 {a1a2a3a7a8} 9 {a1a4a6} 18 {a1a2a3. . .a7a8}  Figure 2. All intents of the data context (see figure 1)   1234 34678 12356 5678  234 568  34 123   23 68  3 7 6  ?  a1  a1a7 a1a3  a1a2 a1a4  a1a7a8 a1a4a6  a1a3a7a8 a1a2a7 a1a2a3  a1a3a4  a1a2a4a6  a1a2a7a8 a1a3a4a6  a1a2a3a7a8 a1a3a4a5 a1a2a3a4a6  a1a2a3a4a5a6a7a8  Figure 3. An example of concept lattice  0-7695-2735-3/06 $20.00  ? 2006    a1a2a3a4a5a6a7a8  a1a3a4a5 a1a2a3a4a6  a1a3a4a6  a1a2a4a6  a1a4a6 a1a3a4  a1a2a3  a1a2a3a7a8  a1a2a7a8 a1a2a7  a1a3a7a8  a1a7a8  a1a7 a1a3 a1a4a1a2  a1  Figure 4. An example of closed itemset lattice  5 Conclusion  Formal Concept Analysis provides a natural effective platform for data analysis and knowledge representation. In this paper, we propose FCA as a tool of data analysis and representation for digital ecosystem. We also propose to use the technologies of data mining to extract knowledge from huge and complex heterogeneous distributed data in the DE.

Furthermore, some issues of the applications of data mining for Digital ecosystem is discussed.

Acknowledgements  This work is supported by the project of EU IST Network of Excellence ?OPAALS?.

