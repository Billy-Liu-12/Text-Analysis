Information Theory for Atypical Sequences Anders H?st-Madsen, Elyas Sabeti

Abstract?One characteristic of the information age is the exponential growth of information, and the ready availability of this information through networks, including the internet ? ?Big Data.? The question is what to do with this enormous amount of information. One possibility is to characterize it through statistics ? think averages. The perspective in this paper is the opposite, namely that most of the value in the information is in the parts that deviate from the average, that are unusual, atypical. Think of art: the valuable paintings or writings are those that deviate from the norms, that are atypical. The same could be true for venture development and scientific research.

The paper first discusses what exactly should be understood by ?atypical.? This is by no means straightforward. It has to be a well defined theoretical concept corresponding to some intuitive idea of atypicality, which when applied gives useful results.

This is followed by a simple example of iid binary sequences.

This example is simple enough that complete algorithms can be developed and analyzed, which give insights into atypicality. We finally develop a more general algorithm based on the Context Tree Weighing algorithm and apply that to heart rate variability.



I. INTRODUCTION  Information theory is generally a theory of typicality. For example, compressing data using the Asymptotic Equipartion Property (AEP) [1] can be done by throwing away all se- quences that are not typical. Our perspective in this paper is that the value of data lies not in these typical sequence, but in the atypical sequences. With the enormous amount of data generated with modern technology and available through data networks and the internet (?Big Data?), our perspective is that most of this data is background noise. What is valuable are the outliers from this background noise. We do not claim that in every application only atypical data is of interest. In many cases, such as consumer behavior, average data is highly interesting. In fact, those perspectives may supplement each other. Statistics about typical data can be complemented by individual examples of atypical data. For example, in ECG there are known patterns indicating heart disease [2]. What we are looking for are unknown patterns that could be warning of heart problems. In passive acoustic monitoring (PAM) [3] of oceans, there are years of recordings with mostly silence (or noise). The goal is to find the rare sounds of marine mammals.

One approach is to look for sounds of known animals, for example dolphins. Our approach is to look for sounds that cannot be classified in any other way. What is essential here is that we do not know what we are looking for, we do not  This work was supported in part by NSF grants CCF 1017823 and 1017775.

know its statistical properties or other properties. We therefore need an abstract method of classifying atypical data. Our main focus is on atypical sequences: large collections of samples that have a special ordering, e.g., according to time or space.

What is an atypical sequence? Consider throwing a fair coin.

If we get a sequence a 100 consecutive heads, we would be surprised. If we were in a casino, security would scrutinize our gambling. Yet, a sequence of 100 consecutive heads does not contradict the laws of probability for a fair coin. In fact, for a fair coin the sequence of 100 consecutive heads has exactly the same probability as any other sequence. Now suppose we instead used a biased coin with probability 0.99 of getting a head. A sequence of 100 heads would not be unexpected; in fact a sequence with 99 heads would be the most likely. The causes of the two outcomes are different. Yet, for a casino both sequences would be worthy of scrutiny. We call the first type of sequence intrinsically atypical, while the second type will be called extrinsically atypical. In detection theory, e.g., anomaly detection, an intrinsically atypical sequence would be called a false alarm. From the point of view of localizing atypical data, we would definitely want to be able to find extrinsically atypical sequences, but intrinsically atypical sequences are of equal interest, in particular since we often do not know the laws governing data.

A. Relationship to prior research  The research in this paper is inspired by several branches of prior research. First, information theory and universal source coding has been used successfully in anomaly detection, e.g., [4], [5]. The approaches haves mostly been heuristic. What we would like is to develop a more fundamental approach to this problem. In particular, we would like to have as few assumptions on the atypical sequence as possible. As an example, we do not assume any knowledge of the length of an atypical sequence.

The second topic of inspiration is Rissanen?s work on minimum description length (MDL) [1], [6], [7]. The concept of MDL, and it?s relationship to Kolmogorov complexity, is exactly the tool we need to make a fundamental theory of atypicality. Instead of using entropy or relative entropy as in prior work, we use MDL.



II. PROBLEM STATEMENT  Consider a sequence of random variables {x[n], n = ??, . . . ,?} from a finite alphabet A. The sequence is    generated according to a probability law P (at first vaguely specified). In this sequence is embedded (infrequent) finite subsequences Xi = {x[n], n = ni, . . . , ni + li ? 1} from the finite alphabet A. which are generated by an alternative probability law P??. The probability law P?? is unknown, but it might be known to be from a certain class of probability distributions, for example parametrized by the parameter ?.

Each subsequence Xi may be drawn from a different proba- bility law. The problem we consider is to isolate these sub- sequences, which, following the above, we will call atypical subsequences. In this paper we limit the discussion to binary sequences, A = {0, 1}.

In the following we consider a very simplified model where the typical sequences are iid (identically, independent distributed) so that the probability law P is specified by the single parameter p = P{x[n] = 1}. The alternative law P?? is also iid with ? = P{x[n] = 1} 6= p. In section IV we consider the more general problem of non-iid binary sequences.

A. Information Theoretic Typicality  First we will discuss the information theoretical concept of atypicality [1]. A sequence X of length l is called strongly ?-typical if  ?a ? X : |p?i ? pi| ? ?  |X | (1)  where p?i = 1lN(i|X ) with N(i|x n) the number of i-s in the  sequence. If p = 12 , a sequence with half zeros would be typical, while one with all zeros would be atypical.

Essentially, a sequence is strongly typical if the distance between the empirical probability mass function and the expected one is small. A relevant question then is how to measure that distance. A guidance can be found by looking at the probability that a {0, 1} sequence is atypical by definition (1). According to large deviation theory [1] the probability of an intrinsically atypical sequence is  P (n) ?=2?nD(p??p) + 2?nD(p???p), px = (p(0) + x, p(1)? x)  where ?= means equal to the first order in the exponent, and D is relative entropy [1]. This indicates that a measure of atypicality could be D(p??p), and a criterion for atypicality could be  D(p??p) >? (2)  for some threshold ? .

B. Hypothesis testing  Another point of view on atypicality is that it is a sequence that comes from a probability distribution q 6= p. This can be stated as the hypothesis test problem  H0 :q = p H1 :q 6= p  This problem does not have an UMP (universal most powerful) test. However, a common approach to solving this type of  problem is the GLRT (generalized likelihood ratio) , which in this case is  L = log ?1 i=0 p?  N(i|xl) i?1  i=0 p N(i|xl) i  = l 1? i=0  p?i log l N(i|xl)? l  1? i=0  p?i log pi = lD(p??p) (3)  Thus, the GLRT is determined by comparing the relative entropy with a threshold. This strengthens the idea of using relative entropy to measure atypicality.

C. Descriptive length What we have seen is that D(p??p) could be a reasonable  atypicality measure. However, this does not really solve the problem. Suppose we in fact know the value of ? and that ? > p. The optimum detection solution is then to simply mark all (or a percentage of) 1s as being atypical sequences ? of length one. This contributes no new information to our understanding of the sequences. The problem is that although the problem seem well-posed and precise, it is in fact not. We have not specified the distribution of the lengths li. Without that, there is no penalty in simply assuming that li = 1 for all i. The issue is that in most applications of this mathematical problem, there is no way to know the distribution of the li in advance.

In essence, for each length li we have a separate detection problem with a threshold ?i.

We will therefore approach the problem from a different perspective. Consider an optimum code (satisfying the Kraft inequality) for the sequence {x[n], n = ??, . . . ,?}, and denote the code length of a sequence of length l by L(l). If the atypical subsequence Xi is coded with an optimum code according to the law P??i with a resulting length L?i(li), then clearly L?i(li) < L(li), which could be a detection criterion.

Since the exact probability law P??i is not known it must be estimated as P???i . Again, L??i(li) < L(li); however, this is true for any subsequence, atypical or not, so this will not work.

On the other hand, if we think of this as a source coding problem, the receiver does not know P???i and therefore the codebook used by the transmitter; this information needs to be also conveyed. This is problem similar to Rissanen?s minimum descriptive length (MDL) [6], [7]. MDL is used to find the model order in estimation problems, and is basically done by minimizing K(P???i) + l??(li), where K(P???i) is a measure of the complexity of describing the random model. This principle can be used to refine the descriptive length classification of atypicality, namely by defining that a sequence is atypical if  L(li)?min ??i  ( K(P???i) + L??(li)  ) > 0 (4)  Definition 1. A sequence is atypical if it can be described (coded) with fewer bits in itself rather than using the (optimum) code designed for typical sequences.

This definition is central to our approach to atypicality.



III. SIMPLE EXAMPLE: BINARY IID SEQUENCES  In this section we analyze the simple model of iid binary sequences. In the textbook [1] a universal code for this problem is given: the source encodes first the number of ones k; then it enumerates the sequences with k ones, and transmits the index of the given sequence. For analysis it is essential to have a simple expression for the code length. We can therefore use Lp?(l) = lH(p?) + 12 log l, where p? =  l  ? Xi. This is an  approximation which is good for reasonably large l. We notice that it reaches the lower bound in [7]. The decoder also needs to know when an atypical sequence starts. We can achieve this by inserting an extra symbol ?.? every time an atypical sequence starts. The length of the corresponding codeword is approximately ? = log 1P (?,?) ; that is, the code length of atyp- ical sequences is increased by a constant independent of the length. Of course, we do not know P (?.?) in advance, so we can think of ? is a parameter to adjust the rate of atypical data. The decoder also needs to know when the atypical sequence ends.

The most natural way is to encode the length of the atypical sequence with a universal code. As argued in [6], the length of the codeword is log?(l) = log l+log log l+log log log l+ ? ? ? , where the sum continues as long as the argument to the log is positive. However, for ease of analysis and implementation we will, for now, replace log? l with log l. We then end up with the following length of the universal code for atypical sequences  Lp?(l) = lH(p?) +  log l + ? (5)  The code length for the sequence coded according to the optimum code for the the typical probability law P is approximately (ignoring integer constraints) l ( p? log 1p + (1? p?) log  1?p  ) . This leads to the following hy-  pothesis test for atypical sequences  l  ( p? log  p  + (1? p?) log 1 1? p  ) ? (lH(p?) + 3  log l) > ?  or  D(p??p) > ? + 32 log l  l (6)  Thus, as predicted by (2) and (3) atypicality is determined by relative entropy. What (6) gives us is the threshold as a function of l. The inequality (6) gives two thresholds for p?,  p? > p+ or p? < p?  Where 0 < p? < p < p+ < 1. It is impossible to find explicit expressions for p?, but it is clear that p? ? p as l ? ?.

Therefore, for l large, we can replace D(p??p) with a series expansion. We then end up with the more explicit criterion  |p?? p| > ?? ?= ? pq ln 4 l  ? ? +   log l (7)  In the following we will use this as it is considerably simpler to analyze. Our main interest is the dependency on l, which is given by the following Theorem  Theorem 2. Consider an iid {0, 1}-sequence with P (X = 1) = p. The probability PA that a sequence of length l is classified as atypical according to (7) is bounded by  PA ?2??+1 l3/2  K(l, ?), ?? : lim l??  K(l, ?) = 1 (8)  Proof sketch: A direct application of the the Chernoff bound (e.g., [1]) and some calculations yield  ln PA ?l ln  ( lq  lq ? b  ) + (?lp? b) ln  ( q(lp+ b) p(lq ? b)  ) =l ln  ( 1 +  b  lq ? b  ) ? (lp+ b) ln  ( 1 +  b  p(lq ? b)  ) ? b2 ( 3l2q2p+ lb(7p2 ? 6p? 3) + b2(6p+ 3)  ) 6p2(b? lq)3  (9)  ?? b  2lpq +O  ( b3  l2  ) (10)  =? ? ln 2? 3  ln l +O  ( ln3/2 l?  l ?3/2  ) , (11)  where we have used x ? x  2 ? ln(1 + x) ? x ? x2  2 + x3  3 for x ? 0 in (9). The equation (11) directly leads to (8).

Figure 1 compares the upper bound with simulations.

The complete problem we set out to solve in Section II,  however, is more ambitious: we would like to find atypical subsequences of variable length of a long sequence. The question therefore is how to choose the ?correct? length of an atypical sequence, or stated differently, where exactly does an atypical sequence start and end? Also for this problem de- scriptive length can provide an answer. Namely, the sequence should be divided into typical and atypical segments so as to minimize the total code length [8].

Theorem 3. Consider the case p = 12 . The probability PA(Xn) that a given sample Xn is part of an atypical subsequence of any length is upper bounded by  PA(Xn) ? (K1 ? ? +K2)2?? (12)  for some constants K1,K2.

Proof sketch: Using the union bound and symmetry we can write  PA(Xn) ?2 n?  n1=??  ( 1?  ?? l=n?n1  P  ( A c (n1, l)  ????? l?1?  `=n?n1  A c (n1, `)  )) where A(n1, l) is the event  A(n1, l) =  { n1+l?1? i=n1  (2Xi ? 1) > ? l ln 2(2? + 3 log l)  } (13)  For ease of notation define  ?(l) = ??  l ln 2(2? + 3 log l) ?    Now consider P ( A c (n1, l)  ????l?1`=n?n1 Ac(n1, `)) = 1 ? P ( A(n1, l)  ????l?1`=n?n1 Ac(n1, `)). We can think of what hap- pens as a simple random walk [9], based on (13). Consider the latter probability. What it means is that we have a path that has not passed the threshold in (13) up until time l ? 1, but then at time l passes the threshold. Since the random walk can only increase by at most one, and since the threshold is increasing with l, that means that at time l we must have Sl =  ?n1+l?1 i=n1  (2Xi? 1) = ?(l). And we can strengthen this: the time l is the first time the random walk reaches ?(l). Then  P  ( A(n1, l)  ????? l?1?  `=n?n1  A c (n1, `)  )  ?P (Sl = ?(l) first time) = ?(l) l P (Sl = ?(l))  where the last equality is from [9]. Here [1, 13.2]  P (Sl = ?(l)) = (  l 2 (l + ?(l))  ) 2?l  ? ?  l  ? 14 (l + ?(l))(l ? ?(l)) lH  ? 1 2 (l+?(l))  l  ? 2?l  We can bound  l  ( H  (  + ?(l)) l  ) ? 1 ) ?? 2  ln 2 l  ( ?(l)) l  )2 =? ? ? 3  log l  Then P (Sl = ?(l)) ? ?  ?2 ?? l?2 and  ?(l) l P (Sl = ?(l)) ?  ? ln 2(2? + 3 log l)  l  ? ?  2?? l?2  ? ?  2? ln 2 l  ? ?  2?? l?2  +  ? 3 ln l l  ? ?  2?? l?2 ? 0.7  Thus  ln  ( ?? l=n?n1  P  ( A c (n1, l)  ????? l?1?  `=n?n1  A c (n1, `)  ))  ? ??  l=n?n1  ln (  1? ?(l) l P (Sl = ?(l))  )  ? ln 0.3 0.7  ?? l=n?n1  ?(l) l P (Sl = ?(l))  ?? k12?? ?  ln(n? n1) (n? n1)3  ? k22??Erf?1 (?   ln(n? n1)  )  ? k32?? ?  ?  (n? n1)3 (14)  .=S(n? n1, ?) (15)  Then  PA(Xn) ?2 n?  n1=?? 1? eS(n?n1,?) (16)  ?2 n?  n1=?? ?S(n? n1, ?) (17)  =(K1 ? ? +K2)2?? , (18)  as it can be checked that all three sums, when (14) is inserted in (17), are convergent.

Fig. 1 compares the upper bound (12) with simulations done minimizing total code length.

101 102 103 10?6  10?4  10?2   l  P A  Theorem 1. The probability that a sequence of length l is classified as atypical     simulated upper  0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 10?3  10?2  10?1  P A  (X n)  Theorem 2. The probability of a sample being atypical versus      simulated upper  Figure 1. The top figure shows the probability of a sequence of length l being atypical with ? = 1, p = 1  . The bottom figure shows the probability of  a given sample being part of an intrinsically atypical sequence of any length.

A. Hypothesis testing interpretation  The solution (6) may seem arbitrary, but it has a nice interpretation in terms of hypothesis testing. Return to the solution in Section II-B. The problem is that it does not reconcile tests for different l, and it therefore does not work to find subsequences of a long sequence; as argued in Section II-C, we can just choose length 1 sequences. One way to solve that issue is to introduce a prior distribution for l, P (l). The equation (3) now becomes  L = log ?1 i=0 p?  N(i|xl) i P (l)?1  i=0 p N(i|xl) i P (0)  = lD(p??p) + logP (l)? logP (0)  The hypothesis test is  D(p??p) > (? + logP (0))? logP (l) l  (19)  Compare that with (6) without the approximations,  D(p??p) > ? + 12 log l + log  ? l  l (20)  To the term log? l corresponds a distribution on the integers, namely Q(l) in [6, (3.6)]. Except for the term 12 log l, the equa- tions (19) and (20) are identical if we use the prior distribution    P (l) = Q(l). Rissanen [6] argues that the distribution Q(l) is the most reasonable distribution on the integers when we have really no prior knowledge. The term 12 log l is the ?penalty? for one unknown parameter as argued by Rissanen. We therefore have a statistical explanation for (6).



IV. UNIVERSAL SOURCE CODING A natural way to extend the simple example to more  complex data models and non-iid data is universal source coding, directly following from Definition 1. The most popular and simplest approach to source coding is perhaps Lempel- Ziv [1], [10], but Lempel-Ziv is poor for short sequences [11]. Here we will use the Context Tree Weighing (CTW) algorithm [12], which is a natural extension of the simple example considered in Section III.

We will use the setup and notation from Willems? original paper [12]. In particular we will use the following notation  P?w Block probability at the root of the context tree.

Typical Encoding. The block probability is a function of the sample time n, which we denote P ??w (n). The typical code length of a sequence X = {X(n1), . . . , X(n2)} then is  LT (X ) =? log(P ??w (n2)) + log(P ??w (n2))  Atypical Encoding. The code length is minimized over the maximum depth D of the context tree. The code length is given by (log? is defined in Section II)  LA(X ) = min D  ( ? logP?w(D) + log  ?D )  + log? l + ?

V. APPLICATION EXAMPLE: HRV AND ARRHYTHMIA One of the cardiac conditions HRV can be an indicator for  is arrhythmia [2]. There are many types of arrhythmia that cannot be found by current algorithms [13]. Subtle arrhythmias and complex cardiac diseases make it difficult to assess the direct role of HRV analyses in the prediction or diagnosis of cardiovascular risk. In research settings, genetic variations that are known to induce arrhythmias have been altered in labora- tory animal models [14]. Surprisingly, existing algorithms are unable to corroborate the animal data with the human genetic correlates of disease [14]. The common issue is that it is not known exactly what to look for in the data to find the arrhyth- mia and the potential for incorrect conclusions. Confounding the analyses is the large amount of data generated from weeks of ECG recording using implantable devices. Our aim for this application is to use atypicality to localize signs of subtle and complex arrhythmia. As a first step the goal of this paper is more modest: to see if our algorithm can localize a simple type of known arrhythmia, namely premature beats, Fig. 2.

We work with the difference between interbeat times (mea- sured as an integer number of sample times) instead of the original time series, ?t(i) = t(i)? t(i?1), which is encoded with a Huffman code to obtain a bit stream . For every bit of the data we calculate the minimum difference between the atypical and typical code lengths,  ?L(n) = min l LA(X (l))? LT (X (l))  where X (l) = {X(n? l), . . . X(n)}, and where LA(X (l)) is calculated with ? = 0.

The ECG signals that we used were downloaded from MIT- BIH database [15]. We used ?MIT-BIH normal sinus rhythm database (nsrdb)? and ?PAF (paroxysmal atrial fibrillation) challenge database (afpdb)?.

0 200 400 600 800 1000 1200 1400 1600 1800 2000 ?50        beats      L(n) (bits) HRV (samples)  273 274 275 276 277 278 279 280 281  ?100      Time (sec)  Normal Sinus Rhythm  1658 1660 1662 1664 1666 1668  ?100      Time (sec)  Arrhythmias (Premature Beats)  Figure 2. HRV signal with premature beats.

