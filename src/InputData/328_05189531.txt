A Modified Apriori Algorithm with Its Application in Instituting Cross-Selling  Strategies of the Retail Industry

Abstract?Retail industry accumulates a large number of retail sales data. Using Apriori algorithm we can find the association rules among commodities and institute cross-selling strategies so that we can improve the profits of retail industry. Based on the analysis of the efficiency of the typical Apriori algorithm we provide a modified method to improve the performance of the Apriori algorithm by reducing the scale of the candidate itemset Ck and the spending of I/O. The paper also describes the application of the modified Apriori algorithm in search of the association rules of sales data of commodities by combining with the actual sales data, so that the feasibility of the algorithm is proved.

Keywords- Apriori algorithm; cross-selling; association rules; I/O spending; candidate itemsets

I.  INTRODUCTION It is called cross-selling that service personnel promote  other products or services when they provide a service or a product to customers. Using various analysis techniques of data mining we can find manifold related demands of customers and by satisfying their demands we can sell a variety of related products and services to customers. The general ideology of mining association rules in data mining originates from the applications of "shopping basket data".

These data are usually recorded in the selling database of stores. Each data record represents a basket of commodities.

Each variable is used to indicate whether a certain commodity is bought or not. The Apriori algorithm, which is proposed by Rakesh Agrawal in 1993, is the most classical algorithm for mining  association  rules among data mining technologies. Association rules of commodities can be found by Apriori algorithm in the process of commodity sales in retail industry. As a result, the relevant demands of customers can be found and cross-selling can be processed.



II. DESCRIPTION OF  THE  CLASSICAL  APRIORI ALGORITHM [1]  Mining association rules is to find out strong association rules that satisfy minimum support and minimum confidence in the transaction database D. It can be decomposed into two steps. First, find out all the frequent itemsets in transaction database D. Second, generate strong association rules from the frequent itemsets. The key to the discovery of strong association rules is finding out frequent itemsets, while  Apriori algorithm is the most classical algorithm for the search of frequent itemsets.

Apriori employs an iterative approach known as a level- wise search, where k-itemsets are used to explore (k+1)- itemsets. First, the set of frequent 1-itemset L1 is found. Next, L1 is used to find frequent 2-itemset L2. Then L2 is used to find frequent 3-itemset L3. Iterate like this until no more frequent k-itemset can be found. The finding of Lk requires a full scan of the database.

To improve the efficiency of the level-wise generation of frequent itemsets, an important property called the Apriori property, presented is used to reduce the search space.

Apriori property: All nonempty subsets of a frequent itemset must also be frequent. A two-step process is used to find the frequent itemsets: join and prune acions.

(1) The join step To find Lk, a candidate k-itemset Ck can be generated by  joining Lk-1 with itself.

(2) The prune step If a (k-1)-dimension subset of X in a candidate itemset Ck  is not frequent, then according to the property of Apriori algorithm, we know that X is not frequent either and should be deleted from Ck.



III. IMPROVED APRIORI  ALGORITHM [1,2] The object of association rules mining is large-scale  databases. Low efficiency of the mining algorithm first results in the large number of generated candidate sets.

Secondly, so many records of database result in too many I / O spending. Based on above analysis, in this paper, from two aspects we will propose an optimized method for Apriori algorithm.

A. Optimized Algorithm of  Rreducing Candidate Itemset Ck [3,4] Apriori algorithm generates frequent itemset Lk from  candidate itemset Ck by scanning the database and calculating each candidate's support count respectively. After the generation of Ck, most of improved algorithm will first generate all (k-1)-item subset of each element X in Ck and compare with Lk-1. If a (k-1)-item subset is not the element of Lk-1, then it is not a frequent itemset. According to the property, X is not frequent, either. So X would be deleted from Ck. This algorithm needs to search Lk-1 for k times for each element X in Ck. In this article, we will introduce a   DOI 10.1109/ECBI.2009.121    DOI 10.1109/ECBI.2009.121     more efficient way to achieve the pruning operation. The algorithm only needs to search Lk-1 one time to complete deletion and remaining of each element X in Ck. The ideology of the algorithm is as follows.

Inference 1: Tk is a k-dimensional itemset. If the number of (k-1)-dimensional subsets of all (k-1)-dimensional frequent itemset Lk-1, which contains Tk, is less than k, then Tk is not a k-dimensional frequent itemset.

Proof: It is clear that the number of (k-1)-dimensional subsets of Tk is k. If the number of (k-1)-dimensional subsets of frequent itemset Lk-1 which contains Tk is less than k, then there exists a (k-1)-dimensional subset of Tk that is not frequent itemset. According to the property, Tk is not a k- dimensional frequent itemset.

As a result, the improved algorithm only needs to compare the count of each element of Lk-1with the count of each element (X) of Ck (each element X has a count). If the count of the element X equals to k, then maintain X.

Otherwise X must be a non-frequent itemset, it should be deleted.

B. Deduce I/O Spending Inference 2: T is a transaction record in transaction  database D. If the total number (m) of all the valid data in T is less than k (dimension of frequent itemset Lk), then we won?t find any elements X of frequent itemset Lk in T.

Proof: Obviously as we known that if transaction record T contains 2 valid data (m=2) and the dimension of frequent itemset L3 is 3, then any element X in L3 will have at least 3 items. In any case, we can not find an element with 3 items in the record with only 2 valid data.

As a result, compressing transaction database can be considered in two ways. If one data of the transaction database D no longer appears in frequent itemset Lk, then this data will not appear in any other k+n (n>1) frequent itemset Lk+n. So this data could be revised to 0 or null (invalid value); In addition, when frequent itemset Lk has been found and if the number of the current transaction record is less than k+1, according to inference 2, we can see that any (k+1)-dimensional subsets of frequent item Lk+1 could not be found in this transaction. So this transaction record could be deleted.

C. Description of the Algorithm Input: D: Database of transactions; min_sup: minimum  support threshold Output: L:frequent itemsets in D Method: 1) L1=find_frequent_1-itemsets(D); 2) For(k=2;Lk-1??; k++){ 3)  Ck=apriori_gen(Lk-1, min_sup); 4)  for each transaction t?D{ 5)     Ct=subset(Ck,t); 6)     for each candidate c?Ct 7)      c.count++; 8)   } 9)  Lk={ c?Ck |c.count?min_sup }; 10) if(k>=2){ 11)  delete_datavalue(D, Lk, Lk-1);  12)  delete_datarecord (D, Lk);  } 13) } 11) return  L=UkLk ; procedure apriori_gen ( Lk-1: frequent (k-1)-itemsets;  min_sup: minimum support threshold) 1) for each itemset l1? Lk-1 { 2)  for each itemset l2? Lk-1 { 3)  if(l1 [1]= l2 [1])? (l1 [2]= l2 [2]) ???(l1 [k-2]= l2  [k-2]) ?(l1 [k-1]< l2 [k-1]) then { 4)    c=l1  l2; 5)   for each itemset l1?Lk-1 { 6)    for each candidate c ?Ck { 7)         if l1 is the subset of c then 8)          c.num++; }}}}} 9)  C'k={ c?Ck |c.num=k}; 10) return C'k; procedure delete_datavalue (D:Database; Lk: frequent  (k)-itemsets; Lk-1: frequent (k-1)-itemsets) 1) for each itemset i ?Lk-1 and i ?Lk{ 2)   for each transaction t?D{ 3)     for each datavalue?t{ 4)        if (datavalue=i) 5)          update datavalue=null; 6) }} Procedure delete_datarecord (D: Database; Lk: frequent  (k) -itemsets) 1) for each transaction t?D{ 2)   for each datavalue?t{ 3)       if(datavalue!=null and datavalue!=0 ){ 4)          datarecord.count++; }} 5)   if(datarecord.count<K){ 6)     delete datarecord;} 7)}

IV. AN  EXAMPLE  OF  THE  ALGORITHM The sales data of a retail enterprise is shown in Table 1 as  an example of the algorithm. In order to simplify the description, Table 2 is introduced and Ii represent commodities respectively. Suppose the minimum support count of the transaction as min_sup = 3 support and the minimum confidence as min_conf = 50%.

TABLE I.  SALES   DATA OF  A  RETAIL  ENTERPRISE  TID Items  T001 T002 T003 T004 T005 T006 T007 T008 T009  T0010  shampoo, toothpaste, soap shampoo, toothbrush, toothpaste, gargle cup, toothpick shampoo, toothbrush, toothpaste toothbrush, tissue toothbrush, toothpaste, gargle cup, tissue toothbrush, toothpaste, toothbrush, toothpaste, soap toothbrush, toothpaste, gargle cup, toothpick shampoo, toothpaste shampoo     TABLE II.  LOGO   ITEMS    Algorithm steps are as follows. Figure 1 shows the  results of the algorithm steps respectively.

Step 1: In the first iteration of the algorithm, each item is  a member of the set of candidate 1-itemsets, C1. The algorithm simply scans all of the transactions T in order to count the number of occurrences of each item.

Step 2: Because of min_sup=3, the set of frequent 1- itemset, L1 can be determined. It consists of the candidate 1- itemset, C1, satisfying minimum support min_sup.

Setp 3: Since the support count of I5, I6, I7 don?t satisfy minimum support min_sup, they won?t appear in frequent itemset L1. According to the property of the algorithm we know any itemset that includes these infrequent itemsets is not frequent.  So delete these data from transaction database D. In addition, when L1 is generated, we find there is only one data in the record T010 in D. According to inference 2, there won?t exist any elements of candidate 2-itemsets C2 generated by the join L1  L1 in the record during the course of searching frequent 2-itemset, L2. Therefore T010 can be deleted. In addition, the number of valid data becomes one when I5 is deleted from T004. Therefore, T004 is deleted and we obtain transaction database D1.

Step 4: To discover the set of frequent 2-itemsets, L2, the algorithm uses the join L1 L1 to generate a candidate set of 2-itemsets, C2.

Step 5: The transactions in D1 are scanned and the support count of each candidate itemset in C2 is accumulated.

Step 6: The set of frequent 2-itemsets, L2, is then determined, consisting of those candidate 2-itemsets in C2 satisfying minimum support.

Figure 1.  An example of improved Apriori algorithm  Step 7: After L2 is generated, we can find the number of the transaction records of T001, T006, T007, T009 are two in D1. According to inference 2, there won?t exist any element of C3 in the records generated by the join L2 L2. Therefore, these records can be deleted and we obtain transaction database D2.

Step 8: To discover the set of frequent 3-itemsets, L3, the algorithm uses the join L2 L2 to generate a candidate set of 3-itemsets C3, where C3= {{I1, I2, I3}, {I2, I3, I4}}.

According to the property of the algorithm, we prune C3. We can validate that the 2-item subset { I1, I2} of { I1, I2, I3} is not the element of L2. So delete it from C3. While the 2-item subsets {I2, I3}, {I2, I4} and {I3, I4} are all the elements of L2, they should remain in C3.

Step 9: The transactions in D2 are scanned and the support count of each candidate itemset in C3 is accumulated.

Use C3 to generate L3.

Step 10: L3 has only one 3-itemset so that C4 =?. The algorithm will stop and give out all the frequent itemsets.

Step 11: Calculate the confidence The confidence of 2-item set in L2 can be calculated as  follows.

I1 => I3 , confidence=4/5=80%; I2 => I3 , confidence=6/7=85.71%; I2 => I4 , confidence=3/7=42.86%; I3 => I4 , confidence=3/8=37.50%; The confidence of 3-item set in L3 can be calculated as  follows.

I2 => I3 ? I4, confidence=3/7=42.86%; I2 ? I3 => I4, confidence=3/6=50%; As a result, I1 => I3 ?shampoo => toothpaste?, I2 => I3  ? toothbrush => toothpaste ? , I2 ? I3 => I4 ? toothbrush ? toothpaste => gargle cup?are all strong rules. According to the above mentioned, we can guide the design of cross-selling strategy.



V. CONCLUSION The mining of association rules is to find out the  relationship of the data from abundant existing data. It is the most extensive problem in the field of data mining. Using Apriori algorithm of data mining technology we can find the knowledge of association rules in retail sales, based on which we can institute cross-selling strategy and improve sales performance [4]. In addition, the large number of the generated candidate itemsets and data records in database leads to excessive I/O spending. These two factors lead to low performance of typical Apriori algorithm. The modified algorithm provided in this paper can improve the performance of Apriori algorithm and mine association rules more efficiently.

