<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">On the  Computational Complexity of Two Frequent Set Generation Algorithms

Abstract Frequent set generation is normally the first step toward association rule generation, and is usually considered the bottleneck step. Accordingly, many algorithms to generate frequent sets have been proposed. Evaluation of these al? gorithms is normally done empirically: their running times have been compared on a few or a substantial number of data files. Evaluation of the computational complexity of these algorithms has not much been pursued except to note that every algorithm requires exponential time, because the output size can be exponential in terms qf' the input size.

We develop an alternate method qf' evaluating algorithms for frequent set generation: we propose evaluating the al? gorithms according to their computational complexity on several abstractly defined infinite families qf' data files. We perform this evaluation for Apriori and F P-growth on two infinite families qf'data files, and show that on both families, a variant of F P-growth is unboundedly faster than Apriori as the data file size increases.

1 Introduction In this paper we use computational complexity to shed light on the relative execution speed of frequent set genera? tion algorithms, as the input data file size grows unbound? edly. In particular we study Apriori [3, 4, 2] and a variant of FP-growth [7, 6]. This variant differs from the original FP-growth in two ways: (1) the order on items is static, not modified at each conditional tree; (2) conditional trees are constructed directly from the parent tree, not by first con? structing the "conditional pattern base" [7, 6]. We assume main memory execution.

Algorithms to generate frequent sets have been a con? cern because of (1) the importance of frequent sets as the first step toward generating association rules, and (2) the large amount of computation performed by these algo? rithms. Analysis of the computational complexity of these algorithms has commonly ended with the observation that execution time is exponential in terms of the input size (be? cause output size is exponential in terms of the input size).

A rare paper that pushes beyond this is [5]. Instead, com? parison of algorithms has normally been done by comparing the execution time of implementations, on benchmark data sets. A recent systematic initiative in this direction is the workshop "Frequent Itemset Mining Implementations" that took place in November 2003 [1] .

We will quickly study the computational complexity of Apriori; the number of incrementations of support will form the basis for the lower bound derived for the computational complexity of Apriori. We will study the computational complexity of a variant of FP-growth. As may be expected, the number of nodes in the FP-trees will play an important role in that complexity.

The trouble is that the number of support incrementa? tions and the number of FP-trees are incommensurate in general. However, they may be related to each other for specific data files and specific families of data files. We will define certain infinite families of data files (each fam? ily containing infinitely many data files; each data file is  finite). One family models certain dense data files: in it, all small (and perhaps medium sized) itemsets are records.

The other family models data files arising from rectangu? lar arrays: there are t attributes, and each record contains one value for each of the t attributes. We will show that on both these families, the variant of FP-growth is unbound?    both these families, the variant of FP-growth is unbound? edly faster than Apriori as the data files grow unboundedly large, unless the size of each record is bounded.

Thus, like comparisons of run time on benchmark files, our comparison is primarily on specific data file families.

Unlike comparisons of run time on benchmark files, our method gives clear extrapolations to arbitrarily larger files.

This method of evaluating algorithms forms a complement to empirical testing.

After some definitions, we describe the families of data files in Section 2. Section 3 establishes a lower bound on the execution time of Apriori on the two families. A variant of FP-growth and its computational complexity are the topic of Section 4, and the next two sections specialize this to the two families of data files. Section 7 contains the major results, and the final section concludes.

Because of space constraints, most proofs for one of the data file families are omitted.

1.1 Preliminaries We will describe a data file as a pair (1, V) where I is a set of items, V is a set of records (or transactions), and each record is a subset of I. (More precisely, since multiple records can contain exactly the same set of items, V is a list of records rather than a set of records.) The number of records is written IVI; size(V) = L {Irl : rEV}. A usual, an itemset is a subset s of I, and where lsi = j, s is also called a j -itemset.

We let I represent the power set of I - the set of all itemsets. The set of j-itemsets is written Ij.

The support of an itemset s is the number of records r such that s ? r; we write this as suppt( s) . s is called a frequent set if suppt( s) 2: 0", where 0" is an integer known as the minimum support threshold. We represent the set of all frequent item sets by F (or by F(CY) in situations where we are balancing two minimum support thresholds). Also, Fj or FjCY) is the set of all j-itemsets that are frequent: Fj = F n Ij.

We extend the support notation as follows: where J is a set of item sets, suppt(J) = LSEJ suppt(s).

Both Apriori and FP-growth rely on the existence of a linear ordering on items. We let I = {al, a2, ... , aIII}' where ai &lt; aj iff i &lt; j. Commonly the items are small integers: ai = i.

00, no, and eo have their standard meanings: o (f (n)) is the set of functions 9 (n) such that for some constant c, g(n) ::; f(n) for all large n. When g(n) sat? isfies this condition, we write g(n) E O(f(n)). Similarly, n (f (n)) is the set of functions 9 (n) such that for some positive constant c, g(n) 2: cf(n) for all large n. Lastly, e(f(n) )  = O(f(n)) n n(f(n)).

2 The Data Files In this section we describe two mathematically defined families of data files. Each family has infinitely many data files, of increasing size. Because they are mathematically defined, it is possible to predict the behavior of algorithms on them, and to establish the computational complexity of algorithms on them.

2.1 Limited Power Set Let n be a positive integer and 1 ::; t ::; n. The limited power set data file ?P(n,::; t) is the data file (1, V) where  III = n, and V is all itemsets s such that lsi::; t.

We define the integer C;;t) as follows: Where (1, V) is the ?P(n, ::; t) data file, the number of records is (;;t), and size(V) = L?=o j G)? Since j G) = nG=i), siz?(V) = n(?t-=-\)? Note that if the minimum support threshold 0" is set at 1, then the set of frequent sets is equal to V, so the sum of sizes of frequent sets is n (;;t-=-\) . Section 3 will make use-of the following proposition.

Proposition 1 (a) For 0 ::; j &lt; t, suppt(Ij) G) (;;2J (b) ForO::; j::; t/2, suppt(Ij) 2: suppt(It_j).

(b) ForO::; j::; t/2, suppt(Ij) 2: suppt(It_j).

Proof [9] is omitted.

2.2 Cartesian Product Some naturally arising data files are basically rectangular arrays. Each row is a record and each column is an attribute.

Hence each record is a sequence of n data values, each data value being in the domain of that attribute. If each attribute has the same number of data values, the data file has the general structure of the Cartesian product data file that we define next.

Let c and t be positive integers. Let G = {I, 2,'" , c}.

Let A = {I, 2"" , t}; each member of A will be called an attribute. A function f : A --+ G maps each attribute to a value in G; we wish to consider f as a set of t ordered pairs (i,j) with i E A,j E G. Define 1= {(i,j) : i E A,j E G}, and call members of I items. Finally, we let V be the set of all functions from A to G (so "record of the data file" and "function from A to G" are equivalent notions). Then IVI = IGIIAI = d. Hence size(V) = tlVI = tct.

(V, 1) will be denoted by CP (c, t) and be called a Carte? sian product data file. Wherec = 2 andt = 3 and item (1, 1) is written as aI, item (1,2) is written a2, item (2, j) is writ? ten as bj (for j = 1,2), and (3, j) is written Cj, V is the set of 8 records {{al, b1, cl}, {al, b1, c2}, {al, b2, cl}, {al, b2, c2}, {a2, b1, cl}, {a2, b1, c2}, {a2, b2, cl}, {a2, b2, c2}}.

In a 3-question questionnaire where attribute 1 is gender (male or female), attribute 2 is education (low, high), and attribute 3 is income (low, high), the 8 records in V corre? spond to the results of the polling of 8 respondents where by some coincidence the 8 respondents were distributed equally among the 8 possible outcomes.

Where the minimum support threshold is set at 1, the number of frequent sets of size j is G) ej, so the number of frequent sets is L:?=o G) ej = (e + 1) t. Also, the sum of sizes of frequent sets is L:?=o j G)ej = te(e + 1) t- 1.

suppt( Fk1)  = G)et because the number of frequent k? sets is G) ek, each having support et-k. (The support of each frequent k-set s is et-k because s has a determined value in k attributes; there are t - k attributes on which the value is not determined, so there are et-k ways to extend s to a function from A to C.) Finally, suppt( F (1)) = 2td (by computing the sum L:i=o suppt(Fk1))).

3 A lower bound for Apriori In this section we develop a lower bound on the compu? tational complexity of Apriori. This bound takes two forms: one for arbitrary minimum support threshold (J", and one specifically for (J" = 1. From this we will obtain a lower bound for the two data file families.

Apriori's method of generating frequent sets is to gener? ate candidates and then to read the data file to compute the support of the candidates. It computes the support of candi? dates by initializing support at zero, and then incrementing support whenever a record containing the candidate is en? countered. Accordingly, the computational complexity of Apriori is at least as great as the number of incrementations that are performed, namely L: sEF suPpt(s). (This ignores candidates that are not frequent.) We state this as a propo? sition: Proposition 2 Using Apriori, the execution time to gener? ate all frequent sets is D (suppt( F) .

Where (J" = 1, this takes the following form: Corollary 3 Where the minimum support threshold (J" is 1, the execution time of Apriori is D(L:rEV 2Irl). Also, suppt(F) = L:rEV 21rl.

Proof. This follows from Proposition 2 by counting pairs {(r,s): r E V,s E F,s &lt;;;; r}, as follows.

Organize the count by frequent set s: s is the second member of suppt( s) pairs. Therefore the number of pairs is L: {suppt(s) : s E F}.

Organize the count by record r: each record r is the first    Organize the count by record r: each record r is the first member of 21rl pairs, since (where (J" = 1) every subset of r is in F.

Thus, L:{suppt(s) : s E F} = L:rEV 21rl. D We note that both bounds hold for any frequent set gen? eration algorithm that determines support of a frequent set by a succession of incrementations.

Also, both bounds hold regardless of whether Apriori uses or omits pruning while generating candidates.

Let V be the data file CP(e , t). We will compute a lower bound on the computational complexity of Apriori on V, where the minimum support threshold (J" ::; et/2. We start with a lemma.

Lemma 1 !I' V CP(e , t) and (J" &lt; et/2, then suppt(F(CI) )  ? suppt(F(1) )/2.

Proof. Let s be a k-itemset, and s be contained in at least one record (hence the items of s are from distinct attributes of the Cartesian product file). In Section 2.2 it was established that suppt( s) = et-k. Hence if k ::; t /2 then suppt(s) ? et/2, so suppt(s) ? (J". In conclusion, if s E .1'(1) and lsi::; t/2 then s E F(CI).

From Section 2.2, suppt( Fk1)) etm. Hence suppt( F(1)) is the sum of values on row t of the Pascal tri? angle (multiplied by et), while suppt( F(CI) ) is the sum of values on the left half of row t of the Pascal triangle (multi? plied by et). Hence suppt( F(CI) ) ? suppt( F(1) )/2 . D Proposition 4 !I'V = CP(e , t) and (J" ::; et/2, then the time complexity of Apriori is D(2tet).

Proof. By Lemma 1 and Section 2.2, suppt( F (CI) ) ?

suppt( F(1))/2 = 2td /2. Since suppt(F(CI)) is a lower bound on the computational complexity of Apriori, then Apriori takes D(2tet) time. D Next we establish a lower bound on the time compexity of Apriori on the limited power set data file.

Proposition 5 !I'V = ?P(n,::; t) and (J" ::; (n;://}), then the time complexity of Apriori is D(2t G)).

Proof. (t /2)-itemsets are frequent (since their support is easily seen to be (n;:// 2)), and likewise smaller itemsets are frequent.

Hence suppt(F(CI)) ? L:;?2o suPPt(Ii). Since for each i ::; t/2, suPPt(Ii) ? suppt(It-i) (by Proposition l (b)), then suppt( F(CI) ) ? suppt(F(l)) /2. Then the execution time of Apriori on ?P (n,::; t) with (J" ::; (n;:// 2) is D (suppt( F(CI) )), so is in D(L:rEV2r), so is in D(G)2t). D 4 FP-growth In [7] Han, Pei and Yin introduced the new algorithm FP? growth. Two of its features are that the algorithm avoids the candidate generation of Apriori, and that its data structure (the FP-tree) tends to be much more compact than the data file itself.

In [7] it is suggested that items be ordered from most frequent to least. However, in the variant of FP-growth we treat here, items are in an arbitrary order, which is fixed throughout execution. Where b is an item, we use I &lt;b to represent the set of items that precede b. Also, we will refer to suffixes of itemsets: let s, s' be subsets of I; s' is a suffix of s if (letting the first element of s' be b) s - I &lt;b = s', i.e., when writing elements of s in ascending order, the tail of the list equals s'.

The data file is represented by an FP-tree, which basi? cally is a prefix tree or trie. In addition, there is a list of headers, one per frequent item, plus item links or horizontal links which form all nodes of the tree corresponding to an item b into a linked list having the header for b as head of the linked list.

In [7] it is established that the number of nodes (exclud? ing the root) in a FP-tree is never more than the size of the    ing the root) in a FP-tree is never more than the size of the data file. The notion of prefix-closed will enable us to say more. A database D is said to be prefix-closed if for each record in D, every prefix of it is also in D.

Proposition 6 If D is prefix-closed and has no repeated records, then the number of nodes in the FP-tree for D equals the number of records in D.

Proof. This is most easily seen if one constructs the FP-tree, processing from the shortest records in D, progres? sively to the longest. The first record to process would then be 0, i.e. the record containing no items; after processing it, the FP-tree has 1 node, the root. Each subsequent record would cause 1 additional node to be adjoined to the tree. D Although most databases are not prefix-closed, this proposition is still useful for them, as follows. If one can count the number of records that must be added to make the database prefix-closed, this proposition allows us to deter? mine the number of nodes in the FP-tree (since the number of nodes in the FP-tree does not change while making the database prefix-closed).

This principle may also be stated: Proposition 7 For any data file D, the number of nodes in the FP-tree for D equals the number of distinct prefixes of records in D.

Let D be a database and T be the FP-tree for D. For an item b, the conditional database or residual database con? ditioned on b will be represented Rb(D), and is defined as the set of truncated records r n I &lt;b where r is a record of D containing b: Rb(D) = {r n I&lt;b : r E D, b E  r}. The con? ditional FP-tree or residual FP-tree conditioned on b will be represented as Rb(T), and is the FP-tree corresponding to Rb(D).

FP-growth is a Divide and Conquer algorithm. The di? vide step is to create a series of residual FP-trees. There is nothing to do in the conquer step. As any divide and conquer algorithm, this algorithm solves "small" problem instances directly, and smashes "large" problem instances into several smaller ones (and makes a recursive call for each). In a little greater detail, a recursive call will be given a FP-tree T and have as its goal the generation of all fre? quent sets having some itemset a as suffix. If T is not a path (i.e., if this problem instance is "large"), then for each item b that is frequent in T, the residue Rb(T) is formed and a recursive call is made, for the purpose of generating all frequent sets having {b } U a as suffix.

The main routine of this divide and conquer algorithm constructs the initial FP-tree to represent the database, and makes the initial call to the recursive subroutine. The pseu? docode here is the same for FP-growth and its variant; they differ from each other in the procedure for constructing residual trees.

FP-growth(D) 1. /* Goal: given DB D, to generate all frequent itemsets. */ 2. Construct the FP-tree for database D; call it T.

3. a = O.

4. recursive-FP-growth(T, a). /* The goal of this call is to generate all fs. having a as suffix, i.e. all f.s. */ recursive-FP-growth(T, a) 1. /* Goal: generate all fs. having a as suffix.

T is the FP-tree representing the set of records containing a as subset. */ 2. If T is a single path then 3. Generate all fs. of the form j3 U a where j3 is a subset of the item headers of T.

4. Else 5. Print "f.s. a has support" T.root.support.

6. For each item header in T: 7. Let b be the item of that header.

S. Construct the residue Rb (T); call it Tb.

S. Construct the residue Rb (T); call it Tb.

9. recursive-FP-growth(Tb1 b U a).

/* Goal of the recursive call: generate all fs. having b U a as suffix. */ We give an overview of the process of constructing a residual FP-tree. In [7, 6, p.242] it is specified that the residual tree be constructed by first constructing the residual database, and then from the residual database constructing the residual tree. Instead, we will construct the residual tree directly.

Let T be a FP-tree and b an item appearing in T. The residue Rb(T) may be constructed by starting at the item header for b and following the horizontal links so as to en? counter all the b-nodes in T. At each b-node, travel upward to find and mark all its ancestors. Sometimes (for example, when the minimum support threshold 0" is 1), the residual tree is isomorphic to the set of marked nodes and the copy? ing process is straightforward. Other times, consolidation of paths occurs, because some items in J &lt;b become not fre? quent. An algorithm to construct the residual tree is pre? sented in [9], and is linear in the number of nodes marked in the parent tree, even when consolidation occurs. This lin? earity may fail to hold in the standard FP-growth algorithm, because of sorting of items when constructing the residual tree.

4.1 Time complexity of the variant of FP-growth Proposition 8 The time complexity of the variant of FP? growth on a database D equals the sum of the following three quantities: ? size(D), i. e. ,  the sum (?f the number qf items in all records.

? the sum of the number of nodes in unconsolidated residual FP-trees.

? the size of the output, i.e. the sum of the number of items in all frequent sets.

Proof. From the pseudocode above, it is clear that the computational complexity of the execution time is the sum of the three quantities: ? time to read data file D and construct the original FP? tree. The time to read D is 8(size(D)) ,  and the origi? nal FP-tree can be constructed in the same time bound.

? time to construct all residual FP-trees. By choosing to construct each residual FP-tree directly from its parent FP-tree, the time to construct it is linear in the number of nodes of the parent tree marked during the process of constructing it. If no consolidation occurs, this num? ber equals the number of nodes in the FP-tree being constructed.

? time to write frequent sets.

Each of these three quantities is linear in the correspond? ing quantity in the statement of the proposition. D 5 Complexity of the variant of FP-growth on CP(c,t) Let D be the Cartesian product data file CP(c, t). We will perform the analysis supposing that the ordering on items is what we call the standard ordering: if items a and b are of attributes i and j respectively and i &lt; j, then a &lt; b.

Also, throughout this section, the minimum support thresh? old is 1.

The number of nodes in the FP-tree for CP (c, t) equals the number of distinct prefixes occurring the the input file.

The prefixes of size 1 are precisely the items of attribute 1: each record contains 1 item of attribute 1, and it will be the first item in ct-l records. In general, the prefixes of size j are all j-tuples of items, where the ith coordinate is filled by an item of attribute i (1 ::; i ::; j). Hence the number of prefixes occurring in the input file is L?=o cj = (cHI -1)/(c - 1), and the number of nodes in the FP-tree for the file is the same.

Let T be the FP-tree for CP(c, t). For an itemset 8 we simplify the notation for the residue of 8: Rs(T) will    we simplify the notation for the residue of 8: Rs(T) will be written as R(8), and where 8 is a singleton {b}, we write R(b). Let 1 ::; k ::; t. For any item b of attribute k, the residual FP-tree R(b) is isomorphic to the FP-tree for CP(c, k - 1). Inductively, then every residual FP-tree encountered in the execution of the variant of FP-growth on CP(c, t) (with the standard ordering) is isomorphic to CP(c, u) for some u &lt; t.

Proposition 9 The sum of the sizes of all residues (exclud? ing the original FP-tree) is L?=l c( C + 1 )t-j (cj -1) / (c- 1). This number is no more than tc(c + l)t-l.

Proof. Where 8 ? J, a residue R(8) is computed iff 8 contains at most one item of each attribute (hence 8 is contained in at least one record) and 8 =I- 0. Let 8 = {bl, b2, ? . ?  bj} where bl &lt; b2 &lt; ... bj, and let bl be of attribute k. Then R( 8) is isomorphic to the FP-tree for CP(c, k - 1) so R(8) has (ck - 1) / (c -1) nodes. Also, the number of subsets 8' of J such that R( 8' ) is isomorphic to R( 8) is c( c + 1 )t-k, for every itemset having first item in attribute k is such a 8' and 8' can be constructed by choosing an arbitrary item in attribute k, and for each attribute after k either choosing an item of that attribute or choosing no item of that attribute (c + 1 alternatives). Hence the sum of nodes in all residues is L?=l c(c + l)t-k(ck -1)/(c - 1) .

To establish an upper bound on this quantity: let a j = c( c + 1) t-j (cj - 1) / (c - 1). Then the quantity we are in? terested in is L?=l aj. It may be verified that the sequence aI, a2, ... at is a decreasing sequence. Hence L?=l aj &lt; tal = tc(c+ l)t-l. D Proposition 10 The time complexity of the variant of FP? growth on CP(c, t) (with 0" = 1, and the standard ordering on items) is O(tc(c + l)t-l).

Proof. The execution time of the algorithm on CP (c, t) with the standard ordering on items is the sum of 3 quanti? ties:  D ? size(V) (namely tct); ? the sum of the sizes of all FP-trees generated (namely the (ct+1 - 1) I (c - 1) nodes in the original FP-tree plus at most tc(c + l)t-l nodes in all residues); ? the sum of the number of items in all frequent sets (namely tc(c + l)t-l ).

6 Complexity of the variant of FP-growth on ?P(n,:::; t) In this section we establish the computational complex? ity of the variant of FP-growth on the limited power set data file. In this section, the minimum support threshold 0" is 1. Results of this section then provide an upper bound on the time complexity with higher support thresholds. Most proofs are omitted in this section.

The FP-tree for ?P(n, ? t) will be denoted by T(?P(n, ? t)). The number of nodes in T(?P(n, ? t)) is (2t). This is because the database is prefix closed and has fhat many records.

We will say that two FP-trees are isomorphic if they are isomorphic as trees, and in addition their horizontal links are preserved. In short, they have the same structure. By contrast, the supports of nodes need not be preserved If two FP-trees are isomorphic and their corresponding data files are prefix closed, then the data files have the property that, after deleting duplicate records, some bijection from the items of the first data file to the items of the second car? ries the first data file to the second.

We will want to know the size (and structure) of residual FP-trees that are constructed during execution of variant? FPgrowth on ?P (n, ? t). It turns out that residual FP-trees fall in the same family as their parent tree: Proposition ll The ak residue qlT(?P(n,? t)) is iso? morphic to T(?P(k - 1, ? t - 1)), and has (??l) nodes.

Proof. The residual data file consists of all records of ?P(n, ? t) that contain ak. The residual FP-tree represents    ?P(n, ? t) that contain ak. The residual FP-tree represents the intersection of each such record with I &lt;ak Each such record (intersected with I &lt;ak) is a subset of I&lt;ak and has at most t - 1 items (since each initially con? tained ak). Hence each record is in ?P(k - 1, ? t - 1).

Conversely, for each record 7" in ?P(k-l, ? t-l), 7"U{ ak} is a record in ?P(n, ? t) that maps to 7". D This result may be generalized to the following.

Proposition 12 Where a is a set ql items (1 ? lal ? t), having a k as its first item, the a residue ql T (?P (n, ? t)) is isomorphic to T(?P(k -1, ? t -Ial)), and has Ut--:1",) M? - We define: the size of a FP-tree is the number of nodes in it (including the root).

Proposition 13 The total number of nodes in all residues generated by variant-FPgrowth on the data file ?P(n, ? t) . (n-l ) (n-2) lS n ::;t-l - ::;t-2? Proposition 14 The computational complexity of the vari? ant of FP-growth on ?P(n, ? t) is 8(n (?t-=-\)).

Proof. The computational complexity of this algorithm ?P(n, ? t) is the sum of three quantities: ? The size of the input file. This quantity is n (2t-=-\) .

? The sum of the sizes of residual FP-trees. This quan? tity is n (2t-=-\) - (2t-=-22), by Proposition 13, so it is 8(n(?t-=-1 ?)). - ? The sum of the sizes of the frequent sets. This equals the size of the input file.

7 Results In this section we compare the computational complexity of Apriori and variant-FPgrowth on the two families of files.

First we compare the two algorithms on the Cartesian product file CP(c, t), when the minimum support threshold 0" is no more than ct/2. We will show that as t increases unboundedly, the ratio of the execution time of Apriori ver? sus the execution time of variant-FPgrowth also grows un? boundedly. More exactly, where fA and f F are respec? tively the execution time of Apriori and variant-FPgrowth on CP(c, t) (with the minimum support threshold at most d/2), we will show that fA E O( (4/3)t iF It).

Proof. By Proposition 4, fA E O(2tct).

It has been shown in Proposition 9 that, when the min? imum support threshold is 1, the number of nodes in all FP-trees generated by variant-FPgrowth (and hence the ex? ecution time of variant-FPgrowth) is O(tc(c + l)t-l). It is easy to see that, as the minimum support threshold in? creases, the number of nodes and execution time of variant? FPgrowth decrease. Hence fF E O(tc(c + l)t-l). Lastly, fAI iF E O((4/3)t It) because the ratio 2c/(c + 1) is at least 413 for all c. D Next we compare the computational complexity of the two algorithms on the limited power set data file ?P(n,::; t), when the minimum support threshold is no more than C??;}) . Analysis will be limited to t ::; . 4n.

By Proposition 14, the computational complexity of variant-FPgrowth on ?P(n,::; t) with (J" 1 is e(n(?t?\)), which equals e(t(?)) (since t ::; .4n). It is clear that as (J" increases, the execution time of variant? FPgrowth does not rise; hence for all values of (J", variant? FPgrowth is an O( t (?)) algorithm.

Since the time complexity of variant-FPgrowth is o (t G)) while by Proposition 5 Apriori is n (2 t G) ), Apri? ori is unboundedly slower than variant-FPgrowth, as t be? comes large (and t ::; . 4n).

8 Conclusion We have shown that a variant of FP-growth is unbound? edly faster than Apriori on two families of data files, as the data files become unboundedly large and the largest record simultaneously becomes large.

It has been observed [8] that there are two basic families    It has been observed [8] that there are two basic families of frequent set generation algorithms. One family, includ? ing Apriori, generates candidates and then determines their support by reading the data file. The other family (including FP-growth) computes intersections of records. The lower bound (Proposition 2) that we computed for Apriori will be valid for any algorithm in the first family that maintains the data file as a list of records. This suggests that algorithms in the first family might consider maintaining the data base as a trie, such as the FP-tree.

