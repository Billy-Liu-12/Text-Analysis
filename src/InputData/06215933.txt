An Effectual Algorithm For Frequent Itemset

Abstract? Association mining is one of the most researched areas of data mining and has received much attention from the  database community. Association rules are interesting  correlations among attributes in a database. It plays an  important role in generating frequent item sets from large  databases. Many industries are interested in developing the  association rules from their databases due to continuous  retrieval and storage of huge amount of data. The discovery of  interesting association relationship among business transaction  records in many business decision making process such as  catalog decision, cross-marketing, and loss-leader analysis. It is  also used to extract hidden knowledge from large datasets. The  Association Rule Mining algorithms such as Apriori, FP-Growth  requires repeated scans over the entire database. All the  input/output overheads that are being generated during repeated  scanning the entire database decrease the performance of CPU,  memory and 110 overheads. In this paper, We have proposed An Effectual Algorithm for mining Association Rules in  Generalized data set that is fundamentally different from all the  previous algorithms in that it uses database in transposed form  and database transposition is done using Parallel transposition  algorithm so to generate all significant association rules number  of passes required is reduced. We will compare proposed  algorithm with Apriori algorithm for frequent item sets  generation. The CPU and 110 overhead can be reduced in our proposed algorithm and it is much faster than other Association  Rule Mining algorithms.

Keywords-Data Mining, Association Rule Mining (ARM), Association rules, Apriori algorithm, Frequent pattern.



I. INTRODUCTION  The rapid development of computer technology, especially increased capacities and decreased costs of storage media, has  led businesses to store huge amounts of external and internal  information in large databases at low cost. Mining useful information and helpful knowledge from these large databases  has thus evolved into an important research area [3, 2, 1]. Data  mining commonly involves four classes of task:  Classification - Arranges the data into predefined groups.

For example an email program might attempt to classify an  email as legitimate or spam. Common algorithms include nearest neighbor, Naive Bayes classifier and neural network.

Clustering - Is like classification but the groups are not  predefined, so the algorithm will try to group similar items together.

Regression - Attempts to fmd a function which models the data with the least error. A common method is to use Genetic  Programming.

Association rule learning - Searches for relationships between variables. For example a supermarket might gather  data of what each customer buys. Using association rule learning, the supermarket can work out what products are  frequently bought together, which is useful for marketing  purposes. This is sometimes referred to as "market basket analysis".

Association rule mining (ARM) [18] has become one of the  core data mining tasks and has attracted tremendous interest among data mining researchers. ARM is an undirected or  unsupervised data mining technique which works on variable  length data, and produces clear and understandable results.

Association Rule Mining (ARM) algorithms [17] are defined  into two categories; namely, algorithms respectively with candidate generation and algorithms without candidate  generation. In the first category, those algorithms which are similar to Apriori algorithm for candidate generation are  considered. Eclat may also be considered in the first category  [8]. In the second category, the FP-Growth algorithm is the best-known algorithm. Following table defines the comparison  among these three algorithms [9].

Algorithm Scan Data Structures  Apriori M+1 HashTable &Tree  Eclat M+1 HashTable &Tree  FP-Growth 2 PrefixTree      The main drawback of above discussed algorithms is the  repeated scans of large database. This may be a cause of  decrement in CPU performance, memory and increment in 110 overheads. The performance and efficiency of ARM  algorithms mainly depend on three factors; namely candidate sets generated, data structure used and details of  implementations [8].

In this paper we have proposed an Algorithm which uses these three factors. Transactional database is considered as a  two dimension array which works on generalized value  dataset. The main difference between proposed algorithm and  other algorithms is that instead of using transactional array in  its natural form, our algorithm uses transpose of array i.e. rows and columns of array are interchanged and transposition is  done using parallel matrix transpose algorithm. The advantage  of using transposed array is to calculate support count for particular item. There is no need to repeatedly scan array. Only  by finding the row sum of the array will give the required support count for particular item, which ultimately results in  increased efficiency of the algorithm. The major Advantages  of our proposed algorithm are as follows:- ? Candidate generation becomes easy and fast.

? Association rules are produced much faster, since  retrieving a support of an itemset is quicker.

? The original file isn't intluenced by the pruning  process where its role ends as soon as data is stored  in 2-d array.

? It reduces the 110 overhead ? The retrieval of support of an itemset is quicker  The remainder of this paper is organized as follows: Section 2 provides a brief review of the related work. In Section 3, we  explain Frequent Itemset and Association Rule Mining through  Apriori Algorithm. In Section 4, we introduce our proposed algorithm. An illustration of the algorithm and experiment  analysis is presented in section 5 and section 6 respectively.

Finally, we concluded our work.



II. RELATED WORK  One of the most well known and popular data mining techniques is the Association rules or frequent item sets  mining algorithm. The algorithm was originally proposed by  Agrawal et al. [4] [5] for market basket analysis. Because of its significant applicability, many revised algorithms have been  introduced since then, and Association rule mining is still a  widely researched area.

Agrawal et. al. presented an AIS algorithm in [4] which  generates candidate item sets on-the-tly during each pass of the database scan. Large item sets from previous pass are  checked if they are present in the current transaction. Thus  new item sets are formed by extending existing item sets. This algorithm turns out to be ineffective because it generates too  many candidate item sets. It requires more space and at the  same time this algorithm requires too many passes over the  whole database and also it generates rules with one consequent  item.

Agrawal et. al. [5] developed various versions of Apriori  algorithm such as Apriori, AprioriTid, and AprioriHybrid.

Apriori and AprioriTid generate item sets using the large item  sets found in the previous pass, without considering the  transactions. AprioriTid improves Apriori by using the database at the first pass. Counting in subsequent passes is  done using encodings created in the first pass, which is much  smaller than the database. This leads to a dramatic  performance improvement of three times faster than AIS.

Scalability is another important area of data mining because of its huge size. Hence, algorithms must be able to "scale up"  to handle large amount of data. Eui-Hong et. al [16] tried to  make data distribution and candidate distribution scalable by Intelligent Data Distribution (lDD) algorithm and Hybrid  Distribution (HD) algorithm respectively. IDD addresses the issues of communication overhead and redundant computation  by using aggregate memory to partition candidates and move  data efficiently. HD improves over IDD by dynamically partitioning the candidate set to maintain good load balance.

Different works are reported in the literature to modify the Apriori logic so as to improve the efficiency of generating  rules. These methods even though focused on reducing time  and space, in real time still needs improvement.

Much work has been carried out on improving the efficiency  of the apriori algorithm by reducing the 110 time and minimizing the set of candidate item sets. However, all these works suffer from problem of scans over the database at least  once. The efficiency of these algorithms can still be improved by reducing the time required for counting the supports of  candidate item sets. We aim to obtain an efficient algorithm  which reduces the time needed to count the supports of candidate item sets.



III. FREQUENT ITEM SET AND ASSOCIATION RULE  The aim of Association rule mining is exploring relations and important rules in large datasets. A dataset is considered as  a sequence of entries consisting of attribute values also known  as items. A set of such item sets is called an item set. Frequent item sets are sets of pages which are visited frequently together  in a single server session. Only the list of session IDs and URLs is used during this process. Support is often utilized to  limit the number of discovered patterns. [17] Support of the  subset {i 1... in} from a set D is defined as in equation (1).

S (i.l iJ = count ( { it, in } t: D) I Count(D)---- (1)  Once the frequent item sets are discovered, we calculate for  each item set the interest to objectively rank them. Interest is defined as in equation (2).

1 (1: ... , iJ = SOh? .. , iJ I nj=l SCU)------(2) Set of n frequent items are broken into n separate  Association rules. The confidence of an association rule (as in  equation (3)) is the fraction of sessions where the subsequent and the antecedent are present and sessions where only the  subsequent is present.

For the rule ia --+is1 ? ? ?  .isn it is C(ia --1 ns! ... isJ :;;;;5(1a --1 is! , ... isJ f S(iJ -.-(3)  A. Apriori Algorithm  ARM is one of the promising techniques of data mining to  extract interesting correlations, frequent patterns, associations or casual structures among sets of items in the transaction  databases or other data repositories. The advantage of the algorithm is that before reading the database at every level, it  prunes many of the sets which are unlikely to be frequent sets  by using the Apriori property, which states that all non empty subsets of frequent sets must also be frequent. This property  belongs to a special category of properties called anti? monotone in the sense that if a set cannot pass a test, all of its  supersets will fail the same test as well. Using the downward  closure property and the Apriori property the algorithm works as follows. The first pass of the algorithm counts the number  of single item occurrences to determine the L 1 or single  member frequent item sets. Each subsequent pass, K, consists of two phases. First, the frequent item sets Lk-l found in the  (k-l )th pass are used to generate the candidate item sets Ck, using the Apriori candidate generation algorithm. Therefore,  the database is scanned and the support of the candidates in  Ck is determined to ensure that Ck item sets are frequent item sets [10].

Apriori Algorithm: 1) LI = { large 1- itemsets} ;  2) for (k= 2; Lk_1 *" 0; k++) do begin 3) Ck = apriori-gen(Lk_a; II New candidates 4) forall transactions tED do begin 5) Ct = subset(Ck,t); II Candidates  contained in t  6) 7)  forall candidates c E Ct do  8) end c.count ++;  9) Lk = { C E Ck I c.count ? min sup } 10) end 11) Answer = Uk Lk ;  We will use Apriori algorithm for ARM as a basic search strategy in our proposed algorithm. The proposed algorithm  will adapt the whole set of procedures of Apriori but the data structure will be different. Also, the proposed algorithm will  use the transposition of transactional database as data  structures and transposition is done using Parallel transposition algorithm.

ISBN: 978-81-90

IV. PROPOSED ALGORITHM  In Apriori algorithm, discovery of association rules require repeated passes over the entire database to determine the  commonly occurring set of data items. Therefore, if the size of disk and database is large, then the rate of inputloutput (1/0) overhead to scan the entire database may be very high. We  have proposed A Mining Algorithm, which improves the Apriori algorithm for repeated scanning of large databases for  frequent item sets generation. In our proposed algorithm,  transaction dataset will be used in the transposed form  (Transposition is done using Parallel Transposition algorithm)  and the description of proposed algorithm is discussed in the following sub-sections.

A. Transaction dataset transposition  The idea of our algorithm is quite simple. Since the diagonal  elements are not affected during the transposition, that is,  element aij of A equals element aii of AT, the data in the diagonal processors will stay stationary. Those below the  diagonal are sent to occupy symmetrical positions above the diagonal (solid arrows in Fig. 1). Simultaneously, the elements  above the diagonal are sent to occupy symmetrical positions  below the diagonal (dashed arrows in Fig.l).

Figure 1. Matrix to be transposed, stored in mesh of processors.

B. Matrix Transpose Algorithm  Procedure TRANSPOSE (A)  Step 1: do steps 1.1 and 1.2 in parallel (1.1) for i = 2 to n do in parallel  for j = I to i- I do in parallel C(i- i,j) (a,j, i)     (2.3) for i = 1 to n - 1 do in parallel for j = i + I to n do in parallel  C. Candidate Generation Algorithm  In the candidate generation algorithm, the frequent itemsets are discovered in k-l passes. If k is the pass number, Lk_1 is the set of all frequent (k -1) itemsets. Ck is the set of cand idate sets of pass k and c denotes the candidate set. l)'b .. .Ik are the itemsets[19]. The candidate generation procedure is as follows:  Procedure Gen_candidate_itemsets (Lk-I)  Ck = <!>  for all item sets II E Lk_1 do for all itemsets b ELk_I do ifIl[l] = 12 [1] /\ II [2] = 12 [2] /\ ... /\ II [k-l] < 12  [k-l] then c = II [1], II [2] ... II [k-l], 12 [k-l]  Ck = C k U {c} End Procedure  D. Pruning Algorithm  The pruning step eliminates some candidate sets which are  not found to be frequent.

Procedure Prune(Ck)  for all CE Ck for all (k-1 )-subsets d of c do ifd ? Lk-1  then Ck = Ck - {c} End Procedure  neighbors do (i) if (ak., m, k) is received from P(i,) + 1)  then send it to P(i,) - 1)     E. Proposed Algorithm Description  Our algorithm uses candidate generation and pruning algorithms at every iteration. It moves from level 1 to level k  or until no candidate set remains after pruning. The step-by? step procedure of our algorithm is described as follows.

Procedure EPT AO / / Transpose the transactional database  l. Transpose(Data Set)  2. Read the database to count the support of C 1  to determine L 1 using sum of rows.

3. L)= Frequent 1- item sets and k:= 2  4. While (k-l * NULL set) do Begin  Ck: = Call Gen_candidate_itemsets (Lk-l)  Call Prune (Ck)  for all item sets i E I do  Calculate the support values using dot?  mUltiplication of array;  support;  k:=k+l  End  All candidates in Ck with a minimum  S. End of step-4  End Procedure

V. AN ILLUSTRATION  Suppose we have a transactional database in which the user  transactions from T 1 to TS and items from A 1 to AS are stored  in the form of boolean values, which is shown in Table I(Figure 2).

Consider the transpose of transactional database of Table 1  is stored in Table 2 by applying Parallel Transposition that can be used in our proposed algorithm. Assume the user specified  minimum support is 40%, and then the steps for generating all frequent item sets in proposed algorithm will be repeated until  NULL set is reached. In our algorithm, transactional dataset  will be used in the transposed form. Therefore, candidate set and frequent itemset generation process will be changed as  compared to Apriori algorithm.

Then the candidate 2-itemset will be generated by  performing dot-multiplication of rows of array, as array consist  of generalized values, the resultant cell will be produce in the form of 1. If the corresponding cells of the respective rows  have 1, otherwise 0 will be in the resultant cell. In this  approach, we will receive a new array consisting of candidate 2-itemsets to get the higher order of itemsets. The above  process between rows of array can be performed to fmd out the  results.

TABLE I: TRANSACTION DATABASE  Parallel Transposition  TABLE 2  Logical Dot Product  Logical Rowwise Sum  Figure 2: An Illustration of our approach

VI. EXPERIMENTAL EVALUA nONS  The performance comparison of our mining algorithm (EGA) with classical frequent pattern-mining Apriori algorithm is shown in the Figure 3. All the experiments are performed on 1.50 Ghz Pentium-IV desktop machine with 256 MB main memory, running on Windows-XP operating system.

The program for Apriori and our proposed algorithm were developed in Java JDK1.5 environment. We report the experimental results on three synthetic generalized datasets.

The datasets consists of generalized values as shown in table 1.

TABLE!. RESPONSE TIME COMPARISON OF ALGORITHMS WITH DATABASE  Response Time (in ms)  Support Count (in Apriori  Our  .-..

'" 8 800 =  :.:, 600 ? 8 E= 400 ? 200 = 8.. 0 '"  ?  %) Algorithm 50 50 10  40 100 40 30 150 130 20 300 60 10 500 80  +-______ -I-____ -+-Apriori  +-__ ?? .. ?------- ---EGA  50 40 30 20 10  Support Count (in %) Foigure 3:Performance analysis of algorithms.



VII. CONCLUSIONS  ARM algorithms are important to discover frequent item sets and patterns from large databases. In this paper, we have designed An Algorithm for generation of frequent item sets similar to Apriori algorithm. The proposed algorithm can improve the efficiency of Apriori algorithm and it is observed to be very fast. Our algorithm is not only efficient but also very fast for fmding association rules in large databases.

The proposed algorithm drastically reduces the 110 overhead associated with Apriori algorithm and retrieval of support of an itemset is quicker as compared to Apriori algorithm. This algorithm may be useful for many real-life database mining scenarios where the data is stored in generalized fonn.

