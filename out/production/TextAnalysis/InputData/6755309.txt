Topics and Terms Mining in Unstructured Data  Stores

Abstract?One of the major challenges of the ?Big Data? epoch is unstructured data mining. The problem arises due to the storage of high-dimensional data that has no standard schema.

While knowledge discovery in database (KDD) algorithms were designed for data extraction, the algorithms best fit for structured data storages. Moreover, today, at the data storage level, NoSQL databases have been deployed in response to accommodate the unstructured data. However, the over- reliance on multiple APIs by NoSQL storages hampers efficient data extraction from different NoSQL storages. Also, there are limited numbers of tools available that can perform KDD tasks on NoSQL data stores. In this work, we explore the trend in unstructured data mining and detail the future direction and challenges. Then, focusing on topics and terms extraction from NoSQL databases, we propose a tool called TouchR2, which algorithmically relies on bloom filtering and parallelization. Using the CouchDB data storage as the test case, the evaluation of TouchR2 shows high accuracy for terms extraction and organization within a much optimized duration.

Keywords- Unstructured Data Mining; Big Data; Bloom Filtering, Terms; Topics; NoSQL; Association Rules.



I. BIG DATA: THE 5V MODEL There is a growing amount of high-dimensional data in  the modern data-driven economy; a trend that some stakeholders describe as ?big data? [1]. Researchers and practitioners have explored the field by looking into big transaction data, big interaction data, and big data processing [30]. While the beginning of the big data era attracted three views or models as shown in Fig. 1, the model has been extended to five more recently. We explain the 5V model as follows.

Volume: The actual size of data is increasing and continuous to grow at an exponential rate. It is believed that the amount of digital data generated within the last two years is more than the total electronic data ever created. Besides, Twitter alone generates 12 Terabytes of data daily [1] and other social media communities produce more or similar volume of data. Thus, the consideration of data size is shifting from Terabytes to Zettabytes.

Variety: The high-volume of data being generated comes in heterogeneous formats and multiple sources. Besides, the data has no standard schema as digital asset storage shifted from structured style data storage to contain semi-structured and unstructured data.

Velocity: Practically, the concept of data sets (batch) is very fast moving to data streams. This is due to the fact that the amount of data coming in and moving out is happening speedily.

.

Figure 1. The initial 3Vs of Big Data [29]  Value: While this concept has been confused between quality and cost, we identified value with the latter (i.e., cost). Data today is ?money? and there are so many enterprises that are in possession of various data that has different price values.

Veracity: Taking out the ?noise? and ?pollution? in the data in order to guarantee quality. Also, veracity enforces that the data we are looking for is correct or not. This is the area that has witnessed studies on provenance, data pedigree, and sanitization.

In this work, our focus is on ?Variety? and the challenges that it poses to data mining. This is discussed further in the next section. The entire paper is structured as follows. After describing the concept of unstructured data generation in the next section, we highlight the adopted approaches on unstructured data mining. Then we propose a tool which is discussed in Section IV on unstructured data mining from NoSQL databases. The evaluation of the tool is discussed in Section V and the paper concludes in Section VI   DOI 10.1109/CSE.2013.129    DOI 10.1109/CSE.2013.129

II. VARIETY: THE RISE OF UNSTRUCTURED DATA Primarily, user generated content has taken a whole new  dimension where all forms of data is being archived as digital assets. To this effect, enterprises are re-aligning their services in such a way that paper-based requirements are all being digitized. To a great extent, there are benefits that big data brings along. Data and information accessibility is made easier and highly available for third parties at minimal or no cost. However, there is a major challenge that we face today with the enormous amount of data that we have. The challenge is that, the data is unstructured since:  1. It is in multiple formats (documents, multimedia, emails, blogs, websites, textual contents, etc) [26]  2. It is schema-less because there is no standardization or constraints placed on content generation.

3. It is from diverse sources (e.g., social media, data providers such as Salesforce.com, and mobile. [25])  4. It requires multiple APIs to aggregate data from the multiple sources; and the APIs are not standardized nor have SLAs.

The fact that the data is unstructured has made it difficult to perform data mining activities. Without data mining, we cannot deduce any meaning out of the high-dimensional data at our disposal. The existing data mining techniques have been designed to work with schema-oriented databases which are structured [2]. However, these existing techniques are no longer relevant to the modern challenges of information extraction. To enhance the data mining process, scientist in both the academia and industry are beginning to explore the best methodologies that can aid in the unstructured data mining process. Thus, we have witnessed some methodologies such as: information retrieval algorithms based on templates [3], association rules [4], topic tracking and topic maps [6], term crawling (terms) [7], document clustering [8], document summarization [9], Helmholtz Search Principle [10], re-usable dictionaries [11], and so on. On the whole, most of the methodologies are based on Natural Language Processing (NLP) which is inherited from Artificial Intelligence and Linguistics [12-14].

While it appears the issue of unstructured data mining is receiving recognition, most of the methodologies are based on single data sources; which means the ability to apply the techniques to distributed data in silos is hampered. This later requirement is actually what most enterprises are aiming for in order to support data extraction from multiple sources.

Also, there is the need to provide frameworks that will perform the data mining task within a significantly lower amount of time, which is our major goal in this paper. But, before we delve deeper into our goal, we need to understand the modern style of data storage.

In an attempt to accommodate the growing high- dimensional data, NoSQL databases have been proposed and this style of database has received high enterprise adoption [15]. NoSQL databases support schemaless, semi-structured, and structured data storage while some accommodate textual and file attachments (e.g., CouchDB). Further, there are some of the NoSQL storages that stores only files such as  DropBox, MEGA, Amazon S3, and so on. The NoSQL platforms that support actual data (e.g., MongoDB, Google Bigdata, etc) allow the users to store data in formats such as JSON, XML, or other textual formats.

The problem is that, these NoSQL databases are products of different vendors so their query styles vary. Moreover, in order to aggregate the data from all of these sources, a mashup service has to be deployed which presents the researcher with two problems. Firstly, multiple APIs has to be studied and secondly, the returned data from the individual data silos are in different structures [15]. Further, the querying of the individual data sources can be time consuming and computationally demanding.

In view of the fact that graphing [16-19] methodologies have achieved significant success, emerging NoSQL databases have adapted the idea. In essence, graph NoSQL databases can be employed to build data storages in a way that can aid in simple queries as well as establishing relationship between the data without writing relational database queries. Relational databases rely on keys and join operations which makes searching tedious in data silos.

Further, most of the previously deployed NoSQL databases rely on MapReduce functions which cannot be shared but are specific to each NoSQL database. However, graph databases allow direct access to the data source of interest without any join or MapReduce operation. Moreover, graph databases are transactional and are optimized for cross-record connections.

At the moment, some NoSQL graph databases as listed in [20] are Neo4J, Infinite Graph, InfoGrid, HyperGraphDB, GraphBase, and so on.

Despite the efforts being made to deploy these graph- based NoSQL storages, there are challenges; which form the bases of our research.

A. Our research goal Though NoSQL databases are available to accommodate  the storage of unstructured data, there is the need to start looking into ways that can aid in mining data from such storage locations. Currently, the NoSQL storages have their own MapReduce functions that can be used to query the database for specific artifacts. However, Knowledge Discovery in Database (KDD) process is beyond just queries.

It?s about building relationships between records, associations, and determining new knowledge.

Currently, there are only a limited number of tools available for performing unstructured data mining; and as of the time of our work, we could not find any tool that performs data mining from textual NoSQL such as CouchDB (which is our test case database). Further, tools that are being developed for the mining of text-based data takes significant amount of time to accomplish the tasks with no indication on the quality and accuracy of the extracted data.

Though this goal is similar to some of our previous studies [5] on unstructured data mining, this paper seeks to explore the question based on the bloom filtering algorithm.

This algorithm will be compared to the parallelization approach which we found reliable for term extraction accuracy and speed in a data space [5]. The next section expounds the issue of text-based mining.



III. UNSTRUCTURED DATA MINING (TOPICS AND TERMS EXTRACTION)  It is important to state that big data (or the high- dimensional data) is only useful if we can perform knowledge discovery in database (KDD) processes on it. The KDD process as well as the information extraction (IE) techniques aid in the interpretation of the data. The challenge however is that, previous IE techniques are deployed for structured and schema-based data storages. But, most of these techniques are non-applicable in the modern data era for data extraction largely because the data is unstructured.

In an attempt to make sense of the unstructured data, researchers especially from Artificial Intelligence and Software Engineering have proposed data extraction methodologies which are based on Natural Language Processing (NLP), Linguistics, and Ontological Semantics.

In summary, we provide Table I which gives an overview of the methodologies that have been proposed by researchers after we surveyed the existing methodologies, tools, and applications. Our aim in this paper is to provide an enhanced terms extraction methodology from a NoSQL so we discuss some of the existing works in the area of terms, topics and association rules.

TABLE I. SUMMARIZED METHODOLOGIES OF UNSTRUCTURED DATA MINING [5]  Information Extraction  Association Rules Topics Terms  Document Clustering  Document Summarization  Re-Usable Dictionaries  Goal Data Retrieval, KDD Process  Trend discovery in  text, document or file  Topic Recommendation  Establish associative  relationships between terms  Organize documents into sub-groups with closer identity  Noise Filtering in Large  Documents  For Knowledge  Collaboration and Sharing  Data Representation  Long or short text, semantics and ontologies,  keywords  Keywords, Long or short  sentences  Keywords, Lexical chaining Keywords  Data, Documents  Terms, Topics, Documents  Words, Sentences  Natural Language Processing  Feature extraction  Keyword extraction Lexical chaining  Lemmas, Parts-of- Speech  Feature extraction  Lexical chaining  Feature extraction  Output Documents  (Structured or semi-structured)  Visualization, Summary  report Topic linkages Visualization, crawler depth Visualization  Visualization, Summery  report  Summary report  Techniques  Community data mining,  Tagging, Normalization  of data, Tokenizing,  Entity detection  Ontologies, Semantics, Linguistics  Publish/Subscribe , Topics-  Associations- Occurrences  Linguistic Preprocessing,  Term Generation  K-means clustering,  Hierarchical clustering  Document structure analysis,  Document classification  Annotations, Tagging  Search Space Document, Files, Database Document,  Files, Database Document, Files,  Databases Topics vector  space Documents, Databases  Databases, Sets of documents  Databases, Sets of  documents  Evaluation metrics  Similarity and relatedness of Documents,  sentences and keywords  Similarity, Mapping, Co- occurrence of  features, correlations  Associations, Similarity, Sensitivity, Specificity  Frequency of terms,  Frequency variations  Sensitivity, Specificity, Balanced Iterative  Reducing and Clustering (BIRCH)  Transition and preceding  matrix, Audit trail  Word extensibility  Challenges and Future  Directions  Lack of research on  Data Pedigree  Applies varying approaches to different data sources. E.g., relational, spatial, and transactional data stores  Identification of Topics of interest  may be challenging  Community based so  cross-domain adoption is challenging  Can be resource intensive  therefore needs research on  parallelization  Needs research on Data Pedigree  Lack of research on dictionary  adaptation to new words  A. Terms Extraction Terms extraction unlike topics publishing focuses on  establishing a network of associative relationships between terms [6]. A typical example is the presence of concurrent terms in a World Wide Web (WWW) document. The main benefit of term mining is that, it optimizes the search space vector thereby reducing processing time [21-22]. Term  crawling for example aims at indicating the relevance of information gathered in an unstructured document by showing the interdependencies between the terms. Fig. 2 shows a map created based on term crawling by Abramowicz et al. [6] where XML is the main search term.

Figure 2. Term crawler showing two level crawling depth  The work by Feldman et al. [22] extended on the Knowledge Discovery in Databases (KDD) by shifting from topic tracking and word tagging to term identification in unstructured documents. The authors push for term normalization based on lemmas with corresponding part-of- speech tags such as stock/N, market/N, annual/Adj, interest/N, and rate/N where N is a noun and Adj is an adjective. A proposal for term extraction module is put across which is primarily responsible for the labeling of extracted terms from a particular textual document. Overall, the term extraction module performs Linguistic Preprocessing, Term Generation, and Term Filtering. What is also important to highlight in the work of Feldman et al.

[22] is that, the authors provide a score test based on relevance in the document using the following techniques: Deviation-Based Approach?calculate the frequency of a term in the document based on the relative standard deviation; Statistical Significance Approach?to test the significance of the relative frequency variation of a term; and Information Retrieval Approach?using the maximal term frequency ? inverse document frequency (tf-idf) to assign a score for information retrieval. The work further explores the option of building a taxonomy constructor based on terms.

The use of taxonomy constructors facilitates the search for specified terms which have association rules in an optimal time rather than searching through an entire document.

There are other ways of using terms such as the conversion of terms into a vector space and calculating the cosine distance between the search terms [23]. The idea is to understand the size of the cosine distance, the frequency of the search term, and the document correspondent.

One of the areas that terms are dominant to a software process is Online Analytic Processing (OLAP) applications [7]. Further, based on term occurrences as central root, a star architecture can be established using the following child dependencies: time, location, term, document, and category [24]. The star architecture proposed by [24] is similar to Figure 1 but the root element is term occurrences.

Also, as the computing landscape is venturing into the adaptation of methodologies from related and non-related fields, term extraction can be a very optimal approach to perform some of the time consuming tasks. For example, there is enough room to explore terms applicability to traceability, readability of bugs, maintainability, and so on.

As an extension on Table I, terms extraction can be challenging due to linguistic issues. For instance, similar words in different languages can mean different things and even similar written words in the same language can mean different things.

B. Topics Topics tracking is generally employed to recommend  subjects of interest to a user. Most online subscription systems (e.g., hotel booking, flight, news articles etc) are based on topics methodologies where keywords are extracted from users? subscription to form a basis for the users? interests [6, 32]. Based on these key words, a mechanism of grouping the related keywords (called Lexical Chaining) can be employed to extract subsequent published messages [6].

Topic Maps (ISO/IEC 13250:2003) on the other hand focuses on the representation and knowledge interchangeability in a repository [6]. Topic maps deal on the following elements (commonly referred to as TAO) as representations: topics?refers to the entities being referred to which are mainly names, events, application component and modules etc.; associations?graphical links between the topics of interest; occurrences?refers to the relevant linkages of the information to topics. Abramowicz et al. [6] researched on the automation of topic map creation and outline four procedures which can be adopted such as: subject recognition, information extraction and preparing, RDF modeling, and mapping RDF model into a topic map.

The basic idea is that the information extraction is initialized after the topic of interest is identified. Information Extraction at this point from the unstructured data source can be done by exploring other techniques such Natural Language Processing and Shallow Text Mining. RDF models are triples of the format subject (e.g., resources), property (e.g.

property type of the subject), and value (e.g., URL). RDF models label the corresponding objects to topics in a map and their occurrences and associations. However, mapping RDF model into a topic map has various processing procedures to follow which include One-time Processing, Repeated Processing, and Continuous Processing [6].

C. Association Rules (AR) Moreover, the Association Rules (AR) is applied in  discovering text with co-occurrences in features [6]. Simply put, association rules are employed to achieve pattern discovery in the text, document or file. AR works by identifying a set T of transactions which is a subset of a bigger transaction I.

FORMALISM: To formalize the AR transaction, Delgado et al. [4] express I and its subset T as associative if A ? C with A, C ? I, A, C ? null and A ?C = null, where |C| = 1; to mean that ?C? is present in every transaction that ?A? is present in.

Association rules are closely related to information extraction since AR aims at establishing relationships between records [31]. As further outlined by Delgado et al.

[4], one application of associative rule is trend discovery, a     Unstructured Data Semi-structured Data Structured Data  Keyword Extraction  Topics Terms  Search Algorithm  NoSQL  Serializer  Clustring Data (JSON)  Search Criteria  Visualization  Topics Clustering  Terms Clustering Association Rules  Thesaurus Dictionary  Thesaurus Engine  Association Rules  Figure 3. The architectural overview of TouchR2.

very salient measure in accomplishing search tasks. Trend discovery can enable us understand from the textual content what reports are being generated and how those reports are being handled. This is achieved by building relationships between the T transactions (or activities) and labeling these transactions using timestamps and other identifiers.



IV. THE DEPLOYMENT OF TOUCHR2 The TouchR2 1  framework as illustrated in Fig. 3 is  deployed based on our vision which seeks to aid topics, terms, and association rules mining in NoSQL data silos. The implementation is done in the Erlang programming language [27] with the user interface being browser-based. In the next section, we discuss the architectural overview of the framework.

A. The Choice of NoSQL In this work, we chose the CouchDB [33] platform for  the use case because it has the pros and cons of most NoSQL databases; with a lot more advantages that are lacking in others. The platform is heavily adopted by the enterprise community and its features are well advanced over the past years. Its storage follows the document append style where the data is stored as JSON; hence, the platform stores structured, semi-structured, and unstructured records. To each record, a file attachment can be added as shown in Fig.

4; and this record can be retrieved over HTTP using the REST request standard.

However, there are challenges with the platform too just as other compatriots like Hadoop. Firstly, there are no links between different (or multiple CouchDB) databases so, MapReduce queries can only be applied to specific tables.

This means there are no relational queries which can lead to discovering knowledge in multiple CouchDB databases.

Secondly, the attachments can contain useful information for data extraction but these files can be in formats that may not be easily accessible (or readable) by a computer program. In   1 R2: The initial of the author?s first names  some instances, these files can contain useful information for the extraction of topics and terms.

Figure 4. Single record from CouchDB with attachments  B. Architectural Flow TouchR is a standalone application that has Erlang  processing backend and an HTML5 interface to enhance user interaction. Currently, the user interface relies on the visualization framework called JavaScript InfoVis Toolkit [28]. From the user interface, the user can specify the Search Criteria. The search criteria is to enable the user specify whether the extracted artifact should be a topic or terms (i.e., Keyword Extraction). The specification of a topic or terms is the keyword which should be used for the extraction.

Once the keyword is specified from the user, the request is sent to the Thesaurus Engine which is the first layer of the back-end. This layer is a complex database component that is implemented using two storage frameworks in Erlang; DETS     and ETS. The DETS component write to disk so it introduces longer read time. Hence, we deploy the ETS component which stores previously searched artifacts in memory for fast read. The thesaurus engine consists of the Thesaurus and the Dictionary. The former contains the topics and terms that are stored to be extracted. The thesaurus can be pre-populated or populated during the search process. For instance, the thesaurus can contain artifacts such as ?Contract?. The Dictionary on the other hand contains the meanings of the artifacts in the thesaurus. When necessary, each artifact in the thesaurus will have a corresponding meaning in the dictionary which can be a synonym. This is very important because sometimes, users can specify a synonym of an actual topic (artifact). For example, a corresponding synonym in the dictionary for ?Contract? from the thesaurus can be ?Agreement?. Also, the dictionary contains the antonyms of the topics in the thesaurus. This concept will be clarified when the association rule is discussed. In the case of the thesaurus engine, we treat a topic as the specific keyword being searched for in which case the tool only focuses on the exact keyword specified by the user. Regarding terms, we consider both the specific keywords specified by the user plus other dependency keywords such as synonyms and antonyms.

After the specification of the keywords, the Association Rules component is activated. One of the major challenges with topic and terms mining is the presence of semantic and linguistic variation issues. Let?s consider the scenario below: Assuming a user specifies a topic/term such as ?Contract? to be extracted, we can think of this ?Contract? to mean ?Agreement?.

The truth is, in the English language, ?Contract? can also mean ?Acquire?; which is completely different from agreement. So, when a user specifies an artifact such as ?Contract?, what do we search for? There are other complex scenarios where a single keyword can mean three or four different things. At the machine level, the term ?Contract? means the same thing so how do we train the system to tell the difference. When this issue is not dealt with, a search result will return a lot of unwanted results (i.e., False Negatives) that will corrupt the result.

In order to address this issue, we employ the association rule between terms based on the existence of synonyms and antonyms. Thus, when a user specifies a keyword, the same user is given the option to choose corresponding synonyms.

So, as highlighted in the above scenario, the user can specify ?Contract? and a synonym such as ?Agreement? so that all meanings related to ?Acquire? will be eliminated. Similarly, the user can specify ?Contract? as a topic/term and add an antonym such as ?Acquire? so that the system will know that the user wants the keywords which are related to ?Agreement?. The synonyms and antonyms are suggested to the user so the user does not have to memorize all these words. The TouchR2 tool will be made open source for enterprise adoption so we build this flexible association rule.

However, when adopted, the association rule can be modified to fit the particular enterprise requirements. For instance, the entire thesaurus engine can be modified to adapt to the medical domain where health related jargons can be added.

After the association rule is established, the Search Algorithm component comes to play. Currently, TouchR2 is built on two search algorithms; Parallel Search and Bloom Filtering. Since there is no relationship between CouchDB databases, we built pointers to the various databases from Erlang. Firstly, following the directed graph approach, we implement processes that run as linear functions where the search is carried out by traversing the databases one after the other (i.e., sequentially). In a search space, linear functions have a complexity cost of O(n). However, linear traversal of the databases can take significantly high amount of time.

Hence, the concept of the parallel search is to fire-up multiple linear functions at the same time to the various database nodes. In this case, there is no waiting to traverse one database record before going to the other. The second search algorithm is the Bloom Filtering technique where the pointers are built to enable us test whether the terms are present or not within a particular database. In this case, we built hash functions that map the existence of a keyword to a particular database.

The selection of a search algorithm directly leads to the interaction with the NoSQL (i.e., the CouchDB) database. As already posited, the NoSQL database can have structured, semi-structured, or unstructured records. In both search criteria, we pass the extracted keyword artifacts to a Serializer. The functionality of the serializer is to filter duplicate keywords and re-organize the keywords. For example, using the parallel search, the various processes can return data sets U such that U1={d1, d2, d5, d6}, U2={d2, d7, d8}, and U3={d5, d7, d9}. In this case, the serializer will generate a universal set Un such that, Un={d1, d2, d5, d6, d7, d8, d9}; and this is the data that is passed to the visualizer. However, the frequency of occurrence of the terms is recorded for statistical purposes. The final serialized data is modeled as JSON which facilitates whether the extracted artifacts should be represented as topic clustering or terms clustering with an association rule. The final output is displayed on the visualization display.



V. EVALUATION To test the performance, we deployed the TouchR2 tool  on an Amazon EC2 instance. Then we take sample in-house data on student records and replicate them so that we can generate high volumes. We could have tested the tool with any open source data or multiple in-house data but that will not help us to conclude whether the tool is correct or not. We can only validate the performance of the tool if there are existing results which we can compare our results with.

Since this is not the case, we considered a sample in-house student records which contains contracts, bio data, projects, etc. and we reproduce the records into our CouchDB database. The replication is beneficial for the test purpose because it enables us to have the statistical results of what we want and later, a validation of the tool can be made against this result. For instance, if the file contains the topic ?Agreement? once, and it is replicated 1 million times, then when the topic extraction is done with the tool, we expect the tool to find and return 1 million occurrences of ?Agreement?.

Overall, we created 1 million records each in 37 different     TABLE II. NUMBER OF RECORDS  1 Million 10 Million 25 Million 37 Million  PS BF PS BF PS BF PS BF True Positive 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00%  False Positive 2.70% 12.80% 3.84% 15.70% 7.12% 22.32% 10.06% 29.32%  True Negative 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00%  False Negative 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%  Precision 97.37% 88.65% 96.30% 86.43% 93.35% 81.75% 90.86% 77.33%  Recall (Sensitivity) 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00%  Specificity 97.37% 88.65% 96.30% 86.43% 93.35% 81.75% 90.86% 77.33%  Accuracy 98.67% 93.98% 98.12% 92.72% 96.56% 89.96% 95.21% 87.21% Type I Error 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% Type II Error 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%  CouchDB databases; and an average of 9 databases are hosted on a single Amazon EC2 instance.

Firstly, we analyzed the latency for searching through the databases based on the two search algorithms. This result is shown in Fig. 5.

Figure 5. The Search Duration Plot  From Fig. 5, we found that the Bloom Filtering (BF) algorithm completes the topics and terms extraction plus determining any associations in a shorter period compared to the Parallel Search (PS). The duration is almost double the search time between the two algorithms especially as the number of records increases. However, the returned results show very interesting trends. Though the BF technique is much quicker, it introduces a lot of False Positive results which ended up affecting the Precision, Specificity, and Accuracy of the search. In Table II, we report the outcome of these indices. True Positive (TF) refers to the extraction of desired terms. False Positive (FP) refers to the extraction of desired terms from the unstructured data but in reality, those are unwanted terms, True Negative (TN) is when the term is not found because it does not exist. False Negative (FN) is when the term could not be found but it exists in the data source. Using these four indices, we provide Table II above.

Because the FP is high in the BF, the accuracy is affected.

Besides, we also observed that as the number of records  increases, the FP increases which makes the accuracy and precision less.

Overall, Table III is provided to explain the most searched for terms in the training data set and their existing associations.

TABLE III. ASSOCIATIONS IN SAMPLE TERMS  Sample Artifacts  Difficulty Level  Success Ratio (%)  Term Frequency ? inverse document frequency (tf-idf)  Projects 1 100 0.00012 Assignments 3 98 0.00029  Duties 1 100 0.00014 Tasks 2 98 0.00032  Contracts 5 82 0.0087 Agreements 4 92 0.0045  Though there are a lot more terms (about 1200 unique artifacts), we focus on the most frequently accessed. The difficulty level from high to low (i.e., 1-5) is which sample artifacts (or, terms) are easily accessed even if a synonym is specified instead of the actual word. Also, the tf-idf as reported gives an indication of the most terms that appeared the most. The lesser the tf-idf, the higher number of times the sample artifact occurs. From Table III, we realized that terms that are standardized or commonly used among the documents (such as Duties) are easily accessed. However, a synonym of the same word which is ?Tasks? has higher difficulty level. Also, ?Project? has a low difficulty index but we realized that synonyms such as ?Contracts? and ?Agreements? have very high difficulty indices. From these outcomes, we can conclude that the more frequently occurred terms are easily extracted with high success ratio.



VI. CONCLUSION Today?s data economy (a.k.a., ?Big Data?) is following  the 5V model. The 5Vs are: Volume, Variety, Velocity, Value, and Veracity. In this work, we focused on the concept of variety which is the rise of unstructured data. The stream of data being generated comes in heterogeneous formats and from multiple sources. This hampers data mining which facilitates knowledge discovery in the data sources. The     problem arises because previously proposed methodologies and available tools work best with schema-oriented databases.

Our effort in this work focuses on extracting topics and terms from unstructured data sources. A tool called TouchR2, is proposed, designed, and implemented which seeks to perform the data mining from NoSQL databases.

The tool is built on two algorithms; Parallel Search and Bloom Filtering and it is tested with CouchDB NoSQL database. The strength of TouchR2 is its ability to retrieve topics and terms in an optimal duration with the identification of associations within the terms. Future works will focus on whether the search criteria be more adaptable to a particular user?s need.

