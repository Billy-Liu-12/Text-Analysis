The Research on Association Rules Algorithm based

Abstract?Association Rules Algorithm is very important in  Data Mining. Apriori algorithm is analyzed, which is classic one  in the Association Rules Algorithms and summarizes problems  existing in the algorithm. Study the frequent itemsets problem for  Association Rules in data mining and a new Association Rules  Algorithm based on Minimum item supports called MSOA is  proposed. In this algorithm, the itemsets are ordered by  ascending order instead by lexicographic order. It can greatly  reduce the candidate frequent itemsets, keeps the completion of  frequent itemsets, and reduces the cost of computing. Experiment  results show that the algorithm is a high efficient algorithm which  can mine all the frequent itemsets by scanning the source database only once.

Keywords-Association Rules; Improved Apriori algorithm;  Candidate frequent itemsets; Frequent itemsets

I. FOREWORD  Data Mining is a technique that aims to analyze and understand large source data and reveal knowledge hidden in the data. It has been viewed as an important evolution in information processing. During the past decade or over, the concepts and techniques on data mining have been presented, and especially in the latest few years, some of them have been discussed in higher levels.

Data Mining uses the classification, association, sequence analysis, clustering analysis, machine self-study and other statistic approaches to find potential, unaware and useful information and knowledge from large database. It is a new subject that involves a lot of subjects and develops with these subjects[1]. Data Mining system can find lots of patterns, in which Association Rules that describe the interesting relations among the items in given data sets are the important area[2].

Association rules is one of the main research fields in data mining, and to gain the more valuable rules is a focus in this field. This paper deeply researches the Association Rules Algorithm, which is important in the Data Mining. It analyses Apriori algorithm, which is classic one in the Association Rules Algorithms, and the improved algorithms of Apriori, and summarizes problems existing in these algorithms.

A new Association Rules Algorithm based on Minimum item supports called MSOA is proposed in this paper, it is a unique way to calculate the supporting degree of every candidate itemsets by marking the transactions related to each item when scanning the database. It is a high efficient algorithm which can mine all the frequent itemsets by scanning the source  database only once, so is more effective than those of Apriori.

Support and confidence of rules are two rules interested measurement. If itemsets is fit for min-support, it is called Frequent itemsets. Frequent k2 is usually called Lk, if it is more than min-support threshold and min-confidence threshold, Association Rules is interested. The computing of the two measurements is involved in frequent itemsets, Apriori algorithm is a basic algorithm to find out Frequent itemsets in data base.



II. RELATED WORK  A. Apriori Character  All no NULL subset of Frequent itemsets must be frequent, contrarily, if a set is not Frequent itemsets, its supersets are not Frequent itemsets. Apriori algorithm uses iteration method which searches by the level to get Frequent itemsets, itemsets k- is for finding out ( k+1)-.First ,frequent temset 1- i is found out , labelled L1 . Then L1 finds out frequent itemsets 2- labelled L2 ,and L2 finds out L3 ,and so on . Lastly it halts until frequent itemsets k2- can?t be found out. To find out each Lk must scan data base once[3]. Apriori algorithm consists of Join and Prune.

Join. In order to find out Lk, it joins LK-1 to generate Candidate itemsets k-, labelled Ck . Assume I1 and I2 are itemsets of Lk?1, Ii[j] is the item j of Ii. In order to reduce the computing time, itemsets are arrayed as ascending order according to frequency of their items in total database, and are not arrayed as lexicographic order; it is the main improved method and is pivotal way to enhances efficiency. Execute join  Lk-1 Lk-1 , If the k-2 items is the same before them, the elements can be connected in Lk?1.  it is also said that if  (I1[1]=I2[1]) (I1[2]= I2[2]) ? (I1[k-2] = I2[k-2]) (I1[k-1]<>I2[k-1]) then elements I1 and I2 can be connected in Lk?1, the condition I1[k-1]<>I2[k-1] ensure that they are not iterative, the connected result of I1 and I2 is that itemsets are Candidate itemsets I1[1]I1[2]?I1[k-1]I2[k-1].

Prune. The superset of Ck and Lk is or not frequent, but all frequent itemsets k- are included in Ck . scan database and identify the count of each Candidate in Ck, and Lk is identified.

Sometimes Ck may be very large and the computing time is also very long. In order to compress Ck , the Apriori character can be used as below, any non frequent itemsets ( k-1)-, they are not the subsets of frequent itemsets k-. So, if a subset of     (k-1)- of Candidate itemsets k- isn?t in Lk-1, the Candidate isn?t frequent and can be deleted in Ck .

B. Apriori Algorithm  Definition 1 The product of min-support and count of database total records is called min-support affair[4].

Definition 2 If there are k elements in set D, the set consist of k?1 elements of them is called  reduced power subset of D[4].

Figure 1 is affair database D of a branch of electronic equipments shop[5], there are 9 affairs in the database, it is |D| = 9; 5 kinds of items , they are I1,I2,I3,I4,  TID Items  T100 I1,I2,I5 T200 I2,I4 T300 I2,I3 T400 I1,I2,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I1,I2,I3,I5 T900 I1,I2,I3  Figure 1. Affair Database D of a Branch of Electronic Equipments Shop  Item set  Support Item set  Support Item set  Support  {I1} 6 {I1} 6 {I1,I2} 4 {I2} 7 {I2} 7 {I1,I3} 4 {I3} 6 {I3} 6 {I1,I4} 1 {I4} 2 {I4} 2 {I1,I5} 2 {I5} 2 {I5} 2 {I2,I3} 4  {I2,I4} 2 C1.

Scan D and count each Candidate item.

L1.

Candidate supports  compare with minimum supports  {I2,I5} 2  {I3,I4} 0  {I3,I5} 1  {I4,I5} 0  C2.

L1 generates C2   Item set  Support Item  set Support  Item set  Support  {I1,I2} 4 {I1,I2,I3} 2 {I1,I2,I3} 2  {I1,I3} 4  {I1,I2,I5} 2 {I1,I2,I5} 2 {I1,I5} 2 {I1,I3,I5} 1  {I2,I3} 4  {I2,I3,I5} 0 L3 {I2,I4} 2  {I2,I4,I5} 1 Candidate supports  compare with min supports  {I2,I5} 2  {I1,I2,I3} 0  L2 C3 Candidate supports compare with min supports  L2 generates C3  Figure 2 Apriori Algorithm Seeks Frequent Itemsets in D  1) Each item is the member of C1 of Candidate itemsets 1- at the first iteration of the algorithm. Scan all affairs ,count the time of each item occurring.

2) Assume the count of min affairs supports is 2 . L1 of frequent may be identified itemsets 1-. It consists of min-support of candidate itemsets 1-.

3) To find out the set L2 of frequent  itemsets 2-, L1 L2  generates C2 of Candidate  itemsets 2-. C2 consists of  1Lc  itemsets 2-. Scan the affairs of D and compute the support of candidate itemsets in C2.

4) Identify set L2 of frequent itemsets 2-. Each item is arrayed as lexicographic order .

5) Computing C3. Set C3 = L2 L2 = {{ I1,I2,I3} ,{ I1,I2,I5} ,{ I1,I3,I5} ,{ I2,I3,I4} ,{ I2,I3,I5} ,{ I2,I4,I5}}. All subsets of frequent itemsets must be frequent according to Apriori character. it may be determined the latter 4 Candidates are not frequent, deletes them.

6) Scan affairs of D , Identify L3 .

7) Computing C4. set C4 = L3 L3 = {{ I1,I2,I3,I5}} , delete subset { I2,I3,I5} for it isn?t frequent, then C4 = , the algorithm halts , all Frequent itemsets are found out.

Frequent itemsets are gotten to scan database in candidate itemsets according to Apriori algorithm and count Candidate items. The count of scanning database equals to the number of items in max frequent itemsets, the expenses are enormous to scan some large database or the data warehouse many times.



III. MSOA ALGORITHM  Current Association Mining algorithm include two main stages:  1) All frequent itemsets are found out, the frequency of itemsets is not less than min-support.

2) Strong Association Rules are generated according to frequent itemsets, these rules must fit for min-confidence threshold.

Minimum support of rules is replaced by Minimum item supports of items of rules, each item has value of minimum item supports in database in the algorithm , it is set by user.

MIS(i) describes the value of Minimum item supports of item

I. Minimum item supports of Rule R is looked as the Minimum  value of all minimum item supports. Rue R: i1 ,i2 , ., im im+1 ,?, ir ,  ij I, if actual support of rule is more than or equals to min (MIS(i1), MIS(i2) ,. . . ,MIS(ir)) , then the rule fits for Minimum supports. Minimum item supports may achieve the following goals, rules with higher min supports including frequent items , rules with lower min supports including non frequent items.

Apriori is the basic algorithm to generate frequent itemsets.

the algorithm uses an level order search cycle method to complete the mining of frequent itemsets. the cycle method uses itemsets k- to generate itemsets (k+1)-. In order to enhance the level order search and to have the frequent     itemsets processing efficiency, the algorithm uses up to down closed character as follows, any subset of frequent itemsets also should be frequent itemsets.

The MSOA algorithm is based on Apriori, if frequent itemsets are found by Apriori, it isn?t fit for up to down closed character. So a pivotal operation of MSOA algorithm is to array as the ascending order of value of item I, and the following operations are completed as the order, these steps may effectively solves these questions.

Level1- M is ordered by ascending order  1.    M = sort(I,MS) ; //according to MIS(i)'s stored in MS;  2.    F = init - pass (M, T) ;  //Make first pass over database T, only keep  frequent 1- itemsets  3.    L1 = { < f > | f F, f.count MIS(f) };  4.    for ( k = 2; Lk-1  ; k++ ) do  //k represents the pass number  5.      if k = 2 then C2 = level2 - candidate - gen(F)  6.         else Ck = candidate - gen (Lk-1 )  7.    end  8.    for each transaction t T do  9.       Ct = subset ( Ck , t ) ;  10.      for each candidates c Ct do c.count ++ ;  11.      end  12.   Lk = { c Ck | c.count (MIS(c[1]))  //Itemset must larger than the minimum  MIS  13. end  14. Answer = UkLk ; Level2 - Candidate ? gen function to find out frequent 2  itemsets.

1. For each item f in F in the same order do  2.    If f.count MIS(f) then  3.      For each item h in F that is after f do  4.         If h.count MIS(f) then  5.           Insert < f, h > into C2 6.       end  7. end  Level3 ?Candidate function, the input parameter is Lk-1 (All frequent(k-1) - itemsets), and return set Lk of all frequentk  ? itemsets.

Step 1 Join  1.    Insert into Ck 2.    select p.item1, p.item2, ., p.itemk-1 , q.itemk-1  from Lk-1p, Lk-1q  3.     where p.item1 = q.item1 ,...,  p.itemk-2 = q.itemk- 2 ,p.itemk-1 < q.itemk-1 ;  step 2 Prune  If subsets of ( k-1) - exist in c and are not included  in Lk-1,delete all itemsets c Ck. and describe the case of  s unenclosed c[1].

1.     for each itemsets c Ck do  2. for each ( k-1) - subsets s of c do  3. if ( c[1] s) or (MIS(c[2])=  MIS(c[1])) then  4. if (s(Lk-1) then delete c from Ck;  5. end  6. end According to this algorithm , if the frequency of every item  of itemsets in total database is arrayed by ascending order in  figure 2, then L2={{ I5 I1} ,{ I5 I2} ,{ I4 I2} ,{ I3 I1} ,{ I3 I2} ,{ I1 I2}}. C3 = L2 L2 = {{ I5 I1 I2} ,{ I3 I1 I2}}. If L2 joins itself and the second item from last is different in the two itemsets then the joining is given up for the generated itemsets are repeat or non frequent . So many redundant itemsets are reduced than in figure 2 and all frequent itemsets are included. The computing time is obviously reduced.



IV. EXPERIMENT  In order to test the validity of the algorithm, the actual data is used to carry on the experiment. There are 55 items and 700 transactions records included in dataset. Each transaction record includes 14 - 16 items, some items appear in 500 transactions, some only appear in 30 transactions. In the experiment, the MIS value of items is established in the dataset as following formula.

( ) ( ) ( )  M i M i LS MIS i  LS otherwise  ,       > =  , (1)  ( ) ( ) (0 )M i f i? ?= ?    ? ?1             (2)  f(i) is the item I actual purchasing frequent, LS is special  minimum item supports, [ 0, 1 ] , the parameter controls relation of the value of MIS and the actual item purchasing frequency. Therefore, it needs two parameters to establish the  MIS , they are and LS . if c = 0 then  a minimum support LS is only necessary, it is the same as traditional Association Rules mining.

Figure 3. Quantity of Candidate Itemsets and Frequent Itemsets  In order to test how to affect the quantity of frequent  itemsets, set =1/a, a? [1,20]. In the experiment, set LS=1%, discovered quantity of Candidate itemsets and frequent itemsets is shown as Figure 3. The experiment shows that the  Candidate Itemsets and Large Itemsets  Candidate itemsets    Large itemsets     =  =  =  =  =  =  =  =  =  =  =   =  =   =  =   =  =   =  =   =   Candidate temsets  Large itemsets     MSOA algorithm obviously reduces the quantity of Candidate itemsets and the frequent itemsets. the comparison of running time is shown in percentage as Figure 4. Running time is set to100% in the unified minimum support, the method proposed in the paper obviously reduces the running time.



V. CONCLUSIONS  The new MSOA algorithm based Apriori is proposed in the paper, it only scan source database once and all frequent itemsets can be identified. It enormously enhances the algorithm efficiency according to the examples. It has made up the Apriori shortcoming, avoided the repetition scanning and saved the computing time, is one effective good algorithm.

