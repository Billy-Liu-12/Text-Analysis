<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2004 8th  lntematlonal Conference on

Abstract In this paper we present an approach to that tries to alleviate the main item-based collaborative filtering (CF) drawback - the sparsity and the first-rater problem.

By combining the contcnts of items into the item-based CF to find similar items and use the combined similarity to generate predictions. The first step concentrates is using association rules mining methods to discover new similarity relationships among attributes. The second step is to exploit this similarity during the calculation of item similar. Finally, combines new similarity and rating similarity measures to find neighbor item in item- based CF algorithm and generating ratings predictions based on a combined similarity measure. The experiments show that this novel approach can achieve better prediction accuracy than traditional item-based CF algorithm.

1 Introduction In e-commerce environment, Collaborative Filtering (CF) is the most popular recommender system technology to date such as Amazon.com and CDNOW provide interesting and powerful recommendation services. The goal of a CF algorithm [1,2] is to recommend new items or predict the utility of a certain item for a certain user based on the user?s preference and other user?s opinions. In a CF scenario there is a list of m users U = {ur,lr2, ..., U,,,} and a list of n items I = {il,i2 ,..., in}. Each user has a rating vector R, where rk represents the user?s rating for item ik (The user?s opinion and preference are explicitly given by the rating score.) In this paper we will concentrate on predicting the rating score for an unrated item i k  (called target item). Currently, there are mainly two types of CF algorithms, user-based and item-based. User-based CF algorithm uses some statistical techniques to find a set of users called user neighbors for the target user. Then different methods can be adopted to combine the user neighbors? ratings to product a prediction rating for the target user. Recently, a new class of item-based CF algorithm has been proposed to deal with the scalability problem in user-based CF algorithm [3], The idea is rather than find similar users (user neighbors) this algorithm tries to find similar items that are rated by different users in some similar ways. Then for a target item predictions can be generated by taking a weighted average of the target user?s ratings on these similar items. However the item-based CF algorithm still suffer from the problems associated with data sparsity, and they still lack the ability to provide recommendations or predictions for new or recently added items. The contribution of this paper is an approach to solve these problems that still exits in item-based CF algorithm in two distinct stages: We use association rule mining methods to discover new similarity relationships among attributes of items Finally we combine the attributes simiIarity and the rating similarity measures to find item neighbors and predictions based on a combined similarity measure.

predictions based on a combined similarity measure.

Item-based CF Algorithm Item-based CF algorithm looks into the set of items the target user has rated and computes how similar they are to the target item i and ?hen selects k most similar items {i,,i2, ..., ik). At the same time their similarities are also computed. Once the most similar items are found and then, the prediction is computed by taking a weighted average o f  the target user?s ratings on these similar items. Item-based CF algorithm consists of two processes, finding similar items and generating rating prediction based on similar item?s ratings.

Sim(ij) = ?

I 2  J !A I Figure 1. Similarity computation 2.1 Finding Similar Items The first step in computing the similarity of items i and i temj  is to identify all the users who have rated both item i a n d j  (as show in Figure.1) illustrates this step, where the matrix rows represent users and the columns represent items, There are a number of different methods to compute the similarity between items. The most popular methods are given below: - The cosine-based similarity is presented in Equation (1). In this case, two items i and j  are considered as two column vectors in the user ratings matrix R.  The similarity between them is measured by computing the cosine of the angle between these two vectors.

where U means the set of users who both rated i and j , Rq,; means the rating of user U on item i.

- The correlation-based similarity is presented in Equation (2).

where Ri means the average rating of the i-th item.

- The adjusted cosine similarity is presented in Equation (3) - where R ,  means the average rating of user U.

2.2 Prediction Computation The next step is to select a set of most similar items to the target item and generate a predicted rating for the target item using weighted sum as show in Equation (4).

where P,,,i means the prediction rating of target user II on item i. Only the K nearest neighbors of item i.

Our approach, we integrate the similarity attributes and item ratings to calculate the item-item similarity in the finding item neighbors step.

Overview of Adding Item Content L 1 ~ ~ ~~ ~ Figure 2. Added item-based CF Gamework In Figure 2 shows the detail of our approach describes as follows: - Represents the contents of items as 3 set of boolean values, each row represents an item and each column represents a unique attribute value. Then convert the contents of items into transaction of attributes.

- Apply association rules mining to discover similarity relationship among attributes of items, and exploit this new simiIarity to find similar items.

- Compute the similarity: firstly, calculate the similarity of item content using new similarity that will be describe in the section 3.2, then calculate the similarity of item ratings using user-item rating matrix that described in section 2.1. At last, the total similarity is the linear combination of the above two.

- Make a prediction for an item by performing a weighted average of deviations from the neighbour's mean.

3.1 Mining Similarity Attributes In this section, we describe applying association rules    In this section, we describe applying association rules mining, in particular the Apriori algorithm [5] to extract association rules between each pair of attributes in transaction of amibutes. The association rules [6] are of the form X 3 Y, where Xand Yare sets of items (item contents). The tmnsaction T contains a certain itemset (set of attributes) X,  then the transaction probably contains another itemset Y. The probability that a given rule holds, the confidence, is the percentage of tran~actions containing Y given that Xoccurs: The support of itemset X is defined as the fraction of transactions supporting X with respect to the entire database. The support of a rule X 2 Y is the probability that both itemsets occur together in a transaction: The measure of rule confidence is related to support, and can be computed as follow: Rule Action 3 Drama Action 3 Comedy Action 3 Romance support(x U Y ) Support (x) Confidence(X 3 Y )  = (7) Support ~ Confidence ~ 25% 34% 7% 11% 14% 17% The association rules that satisfy user specified minimum support threshold and minimum confidence threshold are called strong association rules.

Treating item contents as transactions and the item attributes therein as itemsets, the Apriori algorithm can be used to derive a set of attribute-attribute rules and associated confidence levels. We have limited this initial phase of the work to rules with single-attribute antecedents and consequents. Table 1 shows some sample rules that were generated by running Apriori on the EachMovie dataset. The confidence values are taken as probabilities and used to fill in the attribute-attribute similarity matrix, as shown in Table 2, which provides the additional similarity attributes necessary to find similar items.

Comedy=.Romance I 5% Table 1: Sample association rules 7% Table 2: A simple attribute-attribute similarity matrix Action Comcdy Drama Romancc Action Comedy Drama Romance 3.2 Finding Similar Items Using Similarity Attributes In this section we descript how attributes similarity can be usefully exploited to fmding similar items. The item similarity metric is the mean of the similarities between the attributes in the target item case t and the compare item case c as follows.

As for, The computation of the similarities between the attributes in the item cases t and c consists of two cases as show in Equation (9) and (10).

ASim(t,, c, n )  = 1, f i t i  f t : t ;  = c j  (9) In the situation where there is a direct correspondence between an attribute in the target, ti and the compare, c,, the maximal similarity is assumed, otherwise, the similarity value of attribute in the target item is computed 3s the mean similarity between this attribute and the n most similar attributes in the compare item c (cl, c2 ,..., cn). Figure 3 shows how the find similar items using similarity attributes works on given movies, with a solid arc showing attribute overlap and dashed arcs    a solid arc showing attribute overlap and dashed arcs representing the new similarity through association rules.

fil,~~-=il:~A,, , Moviej comedy drama romance ASimYactiod?, j, 3) = action 3 comedy action 3 drama action a romance ASim(%omedy?, j, 3) = comedy 2 comedy comedy 3 drama comedy romance - ISim(i, j, 2) 0.31 0.34 0.1 1 0.17 - 1 0.66 0.7 0.65 - Figure 3. Finding similar items using similarity attributes 3.3 The item similarities measure ISim(ij], for a pair of item i and j ,  is computed using the similarity attributes as described in Section 3.2. Similarly, we compute item similarities based on the user-item matrix as described in Section 2.1, we denote the rating similarity between two item i a n d j  as RSim(ij3. Finally, for each pair of items i and j ,  we combine these two similarity measures to get CSim(iJ) as their linear combination as show in Equation (1 1).

? Prediction Based on a Combined similarity where a is a combination parameter specifying the weight of similarity in the combined measure. If a = 0, the CSim(ij) = S i m ( i j ) ,  in other words we have the standard item-based CF. On the other hand, if a = 1, then only the attribute similarity is used which, essentially, result in a form of content-based filtering.

In order to compute predicted ratings, we use the weighted sum approach described in Section 2.2.

Specifically, as show in Equation (1 2).

The only difference is that, here we will use the combined item sindarity CSim(ij) instead of Sim(i,j) to generate prediction rating.

4 Experiments and Results  4.1 Dataset We perform experiments on a subset of movie rating data collected from the EachMovie dataset [7]. The EachMovie dataset deals with movie items using data from the EachMovie collaborative filtering site depIoyed by Digital Equipment Research Center from 1995 through 1997. The EachMovie dataset consists of 2,811,983 preferences for 1,628 movies rated by 72,916 users. User preferences are represented by means of numeric values from 0 to 1.0, such as 0, 0.2, 0.4, 0.6, 0.8, 1.0. In EachMovie dataset, genre can have I O different values such as action, animation, art-foreign, classic, comedy, drama, family, horror, romance, and thriller. In our experiment, we retrieved 200 users and 200 movies. Furthermore, we randomly chose 90% of the ratings data as training dataset and the 10% as test dataset 4.2 Evaluation Metria In our experiments, we used MAE (Mean Absolute Error) as our evaluation mehics to measure prediction accuracy by comparing the numerical prediction scores    accuracy by comparing the numerical prediction scores against the actual user ratings in the test data. The MAE is calculated by summing these absolute errors of the corresponding ratingprediction pairs and then computing the average as show in Equation (23) .

the number of rating-prediction pairs between the test data and the prediction result. The goal of an algorithm is to achieve as low a MAE as possible.

4.3 Experimental Results The first experiment is the quality comparison of three different similarity algorithms such as cosine, correlation and adjusted cosine as described in Section 2.1. We implemented three different similarity algorithms with value of combination parameter U and tested them on our dataset. We performed an experiment where we varied the value of combination parameter a from 0 to 1 in an increment of 0.1 and determine the size of neighbors = 10. We ran these experiments on our training dataset and used test dataset to compute MAE. In Figure 4 shows the results of first experiments. We can observe that the value of combination parameter a does, affect the quaIity of prediction. When combination parameter a = 0, which means traditional item-based CF. When a between 0.1- 0.9 means combining attributes similarity with rate similarity. When U = 1, which means oniy using attribute similarity to find similar items and predict rating. Especially, the adjusted cosine similarity given the best result when a = 0.4, after this point the error values start increasing. Hence, we select the adjusted cosine similarity for the next experiment.

-+- Cosine -i%- Adjusted cosine .- - Correlation 1 I 1.02 0.98 2 0.96 0.94 0.92 0.9 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Combination parameter Figure 4. Comparison of prediction quality The second experiment is the size of the neighbourhood.

We performed an experiment where we varied the number of neighbors to be used for adjusted cosine similarity and computed MAE. In Figure 5 shows the results of our experiments. We can observe that the size of neighborhood does affect the quality of prediction.

where Pu.j means the user U prediction on item i and Ru,i means the user U rating on item i in the test data, N is 30 Neighbors +@- 40 Neighbors 0.99 0.97 2 0.95 0.93 w 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Combination parameter Figure 5. Experiments with neighborhood size The final experiment, we implemented two different similarity attributes methods - one is Euclidean distance and our new similarity methods. In Figure 6 shows the results of experiments. We can observe that our approach for new similarity has a trend to show better performance than the Euclidean distance method, especially, when a = 1 the difference is so much, otherwise the difference is negligible I + Euclidean Distance I : Our new similarity    I 0.98 $? 0.96 E 0.94 0.92 0.9 5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Combination parameter Figure 6. Comparison of our approach and Euclidean distance at the size of neighbors = 30 5 Conclusion and Future Work In this paper, we combine the strengths of content-based filtering techniques with item-based CF to provide more accurate predictions which solves the sparsity and the first-rater problems. The results of our experiments show that the combination of attributes similarity and rating similarity can achieve better prediction accuracy than traditional item-based CF algorithms. For future work, we will continue to evaluate the effectiveness of our approach across a? variety of collaborative filtering datasets and to make comparative evaluations to other techniques.

