

Analysis on Probabilistic and Binary Datasets through  Frequent Itemset Mining  Robin Singh Bhadoria   Ram Kumar         Manish Dixit Dept of CSE    Dept of CSE         Dept of CSE  IITM,Gwalior(MP)-INDIA             Rayat Bahra IITM,Sonepat(Harayana)-INDIA  MIST,Gwalior(MP)-INDIA  Email: ssr_robin@yahoo.co.in       Email: ram.amu786@gmail.com        Email: dixitmits@gmail.com  Abstract--Association rule mining is the process of discovering  relationships among the data items in large database. It is one of  the most important problems in the field of data mining. Finding  frequent itemsets is one of the most computationally expensive  tasks in association rule mining. The classical frequent itemset  mining approaches mine the frequent itemsets from the database  where presence of an item in a transaction is certain. Frequent  itemset mining under uncertain data model is a new area of  research. In this case the presence of an item is given by some  likelihood measure. In this paper, we have developed a hyper  structure based pattern growth method for frequent itemset  mining from uncertain data. We have also developed a maximal  clique based candidate pruning method for uncertain data. We  have implemented and analyzed the performance of the well  known algorithms for frequent itemset mining for both binary  and uncertain data model. Our empirical results show that in case  of dense binary datasets, FP-growth outperforms all other  algorithms, whereas in case of sparse data H-mine outperforms  other algorithms.



I. INTRODUCTION  Over the last two decades, with advances in storage  media technology, storage devices have become larger and more  economically viable. As a result, businesses, large corporations,  etc., have started storing and archiving various types of data in  the form of large databases. The purpose of storing data is two-  fold: first, to access these data in the future, and second, for  analysis and discovery of co-relation or relationships among data  items stored in database. The task of determining association  and/or co-relation among the data items is known as association  rule mining.

The main motivation comes from market basket analysis  (or MBA) [1, 8, 14]. When in a supermarket a customer comes to  buy some items, how likely is she to buy some other specific  items or how often customers are purchasing a set of items  together?

Recently, researchers have started finding rules for data  where existence of an item is not certain [11, 18]. In this case, an  item is present in transaction by some likelihood measure. For  example, consider the database D of research papers from various  fields and a black box algorithm B. Algorithm B takes as input a  research paper from D and outputs a set of subjects that this  paper may belong to, along with their corresponding  probabilities. The results of algorithm B may be based on  different parameters such as frequencies of words, phrases, etc.

The output can be viewed as a transaction. For example B, may  process papers and output the probabilities, as shown in Table  1.1. Analysis of this type of database may used to classify a  research paper as belonging to a specific field. For example, a  paper with high probabilities of subjects like algorithms and  biology may belong to bioinformatics, and a paper with high  probabilities of biology and chemistry may belongs to  biochemistry field.

Table 1.1: Example of a research papers database.

Association rule mining has been recognized as one of the big  research areas in the field of data mining.



II. FREQUENT ITEMSET MINING & ALGORITHM FOR  DATA MODEL  To compute the support count of an itemset we require  to access the database. The database can be stored in memory in  the following ways.

? Horizontal representation: This is the basic representation of  transactional database. In this, each row consists of transaction  identifier followed by set of items present in the transaction.

? Vertical representation: In this type of representation, each row  of database consists of an item followed by a set of transaction  identifiers in which the item is present. The main difference in  this to approaches is, in case of horizontal representation the  computation of support count of an itemset requires complete  scan of database, whereas in vertical representation the support  count of an itemset is calculated by intersection of transaction  sets of items in itemset.

? Bit-vector representation: This is the more general  representation of database. In this, database is stored in the form     of 2-dimensional bit vector. Table 2.1 shows the bit vector  representation of the transactional database.

Table 2.1: Bit-vector representation of transactional database.

A.    Apriori Algorithm  Apriori is a well known algorithm for frequent item  mining which employs level wise search, i.e., breath first search,  where it uses frequent k-itemset to discover the (k+1)-itemset. In  preprocessing of Apriori a scan of database is performed to find  out the support count of each item. Then all those items whose  support count is less the minimum support threshold a, i.e., all  infrequent 1-itemset, are removed from the database.

? Apriori property [14]: ?All nonempty subsets of a frequent  itemset must also be frequent?.

This property can be described as follows. If X is an  itemset and support count of X is less than minimum support  threshold, a, then X is an infrequent itemset. If we add an item A  to the itemset X, then the support count of resulting itemset X   A  cannot be more than support count of X. Therefore, the resulting  itemset cannot be frequent if X is not frequent. Whenever a k-  itemset is found it checks each of its size (k-1) subsets. If all the  size (k-1) subsets are frequent then only scans the database to  check the support of that k-itemset. The Apriori follows two step  process to find out the frequent itemset. A two-step process  consisting of join and prune actions:  1. Join step: In this step, it joins Lk-1 with itself to get Lk , which  is the set of frequent               k-itemset. This join operation is a  simple natural join. Resulting set after join is called as candidate  set and is denoted by Ck.

2. Prune step: Clearly, Ck is the superset of Lk  Some candidates of set Ck may not be frequent. As Ck is the  superset of Lk, it contains all possible frequent k-itemset and  some infrequent k-itemset. To determine Lk (i.e., set of k  frequent k-itemsets ) a scan of database is required to find the  support count of each candidate in Ck. All those candidates  whose support count does not  satisfy the minimum support threshold a, has been removed from  the Ck.



III.  FP-GROWTH  Apriori algorithm generates candidates and tests the  occurrence frequency of generated candidate set. This process  required scanning of database for each candidate. Candidate set  generation and test in huge database requires significant amount  of time. In short, we can say that in Apriori -like algorithms the  bottleneck is candidate set generation and test. FP-growth is  known as one of the fastest algorithms of frequent set mining  [15]. It uses a compact data structure called FP-tree, which is an  extended prefix tree. The main advantage of FP over Apriori is  that FP does not generate candidate sets, instead it uses pattern  growth method to generate frequent itemsets. FP-growth  approach first represents the frequent item in the form of  frequent pattern tree (FP-tree), which is a compressed structure.

This compressed database is divided into set of projected  databases, known as conditional database. All the frequent  itemsets can be generated mining this conditional database.

A.     FP-tree construction  First we build the header table which consists of item  name and link field corresponding to each item. All link entry of  header table is initially set to null. Whenever an item first time  added into the tree, the corresponding entry of header table is  updated. The root, labeled as ?null?, is created. Children are  added by scanning the database.

Table 3.1: Transactional database after  preprocessing.

First a path that shares same prefix is require to be  search. If there exist a path that is same as any prefix of current  items of transaction (in  ist order) then the count of prefix portion  is incremented by one in the tree, remaining items of same  transaction (which do not share the path), are added from the last  node of sharing portion in  ist order and their count value is set to  1. If items of a transaction do not share any path of tree then they  are added from the ro ot in  ist order. Each path of prefix tree  (FP-tree) represents a set of transactions. The FP-tree for  transactional database of Table 3.1 is shown in Figure 3.1  Figure 3.1: FP-tree for above example.

264 2011 World Congress on Information and Communication Technologies    B.      H-mine : Another pattern Growth Method H-mine is another pattern growth method for frequent  pattern mining [22]. In case of sparse data, the FP-tree is not able  to achieve good compression. We found that in case of sparse  data H-mine empirically performs better than FP-growth. Also  the space requirement of H-mine is polynomial. In order to  achieve pattern growth H-mine uses a hyper structure called as  H-struct. In H-struct frequent item projection of each transaction  of database D is taken as an element of a queue.

In Table 3.1, the right column shows the frequent item  projections of each transaction. H-mine represents frequent item  projection of each transaction in a particular order which is called  the flist order.

figure 3.2 : H-struct  H-mine uses divide-and-conquer strategy to mine all the frequent  patterns. The general consideration is that the frequent patterns  can be partitioned into different sets. The partitioning can be  done as follows. First partition contains all patterns that contain  the first item of  flist, second partition contains all the patterns  which  contain second item but not the first item, third partition contain  all the patterns that contain third item in flist order but not first  and second item and so on.

C.      Probabilistic pattern Growth: P-Hmine  The general idea of P-Hmine is that it represents the  database in form of a new structure called P-Hstruct. P-Hstruct is  similar to H-struct of [22], with the additional capability of  handling uncertain data. In P-Hstruct, we represent the database  as a set of queues.

Figure 3.3: P-Hstruct

IV. EXPERIMENTAL ANALYSIS & RESULT  We analyze the running time of algorithms on both  synthetic and real datasets. The synthetic dataset generator is  taken from IBM Almaden website.

A.    Datasets The dataset mushroom is a description of hypothetical  samples corresponding to different species of mushrooms. This  dataset consists of 8124 instances of 119 attributes which are  derived from 24 species. It is a dataset containing long pattern  with significant repetitions and overlaps between transactions,  i.e., a dense dataset. The chess dataset is also a dense dataset  containing of 3196 instances and 74 items. The synthetic data  generator takes as input the following parameters:  ? T is the average length of a transaction.

? N is the number of items in a dataset.

? D is the number of transaction in the dataset.

To study the performance of U-Apriori,       UFP-growth and P-  Hmine, we have used the same datasets discussed above. To  introduce uncertainty we assigned random existential probability  to each item of the transactions by randomly assigning value Pi, 0  < Pi <= 1, to them. High probability values are assigned to retain  the long patterns. In our experiments, we have used the value of  P is greater than 0.6.

B.     Analysis on Binary Datasets  On mushroom dataset FP-growth is faster than all the  other algorithms as shown in Figure 4.1. The performance of H-  mine is not as goo d as FP-growth on dense dataset, because in  case of dense datasets a lot of traversal is required between  different queues. Apriori generates huge number of candidates;  testing of these candidates results high execution time for  Apriori.

Figure 4.1: Performance of algorithms on Mushroom  dataset  If the support threshold is high, the performance of  Apriori is similar to H-mine, because in case of high support  threshold the number of candidates that needs to be examined  and the number of maximal cliques is very less. As shown in  figure, when the support threshold is above 10% all the  algorithms execute very fast.

2011 World Congress on Information and Communication Technologies 265    Figure 4.2: Space requirements of algorithms on Mushroom datasets  In Figure 4.2, when the support threshold is above 15%, the  space requirement of Apriori and UFP-growth is camparable  with other two algorithms.

Real dataset mushroom contains a lot of repeated long  patterns (very few items differ between consecutive transactions)  which results in very compact FP-trees. Due to repeated patterns  most of the transactions get merged and a single branch of FP-  tree represents a large set of transactions.

C.   Analysis on Probabilistic Datasets  When the data is very dense, the performance of FP-  growth is better than H-mine. However, the results obtained on  dense probabilistic data shows that, the behavior of UFP-growth  is not so closely related to            P-Hmine. This is because in  case of probabilistic data, it is possible for a data item to have  different probability values in different transactions, due to which  the basic data structure of UFP-growth, UFP-tree, has large  number of no des and large number of leaves which results in big  UFP-tree.

Figure 4.3: Performance of algorithms on uncertain Mushroom dataset.

In Figure 4.3 shows that performance of all the  algorithms on uncertain mushroom dataset. This can be seen  from the figure, in case of low support threshold UFP- growth  requires high execution time than P-Hmine.

The reason for this is larger UFP-trees. When the  support threshold is above 6%, the execution time of FP- growth  is similar to   P-Hmine. In case of low support threshold both U-  Apriori  generate huge number of candidates; testing of this  candidates required scanning of the database.

Figure 4.4: Space requirement of algorithms on uncertain Mushroom.

Figure 4.4 shows the memory requirements of all the algorithms.

In case of uncertain data the UFP-tree contains large number of  nodes and leaves. The large UFP-tree also generates large  conditional FP-trees. Due to this large structure, UFP-growth  requires more memory. The bigger UFP-trees generates bigger  conditional UFP-trees which is one of the time consuming tasks  of UFP-growth. Mining of large UFP-tree results in high running  time. P-Hstruct does not depends on different probability values  of an item and do not require much space in recursive  steps,requires almost constant amount of memory. When the  support threshold is above6% the space requirement of UFP-  growth becoming similar to P-Hmine.



V. CONCLUSION  In this paper, we extended H-mine for uncertain data.

We have also developed algorithm based candidate pruning  for  uncertain data. Finally, we analyzed the performance of frequent  pattern mining algorithms on few benchmark metrics, over both  binary and uncertain datasets. In case of binary dense data (data  contains a lot of repeated long patterns) model, FP-growth  performs better than other algorithms, because the dense data  results in very compact FP-trees and this requires very less  amount of memory. whereas H-mine fails in achieving good  division of data and patterns in this case. In case of sparse  datasets, data contains small patterns and there are not much  repetition of patterns in dataset, H-mine performs better than FP-  growth. The reasons for this are, as mentioned earlier in case of  sparse data the size of FP-tree will be bigger and FP-growth  spends a lot of time in building and traversing the conditional FP-  trees.

On the other hand, in this case the H-mine will be able  to achieve good division of data and patterns. Apriori like  algorithms, i.e., algorithms which are based on candidates  generation and test, generate a lot of candidates and thus, require  high running time. Candidate pruning based approach such as  Hmine and P-Hmine saves a lot of scans of the database and  achieves better performance than Apriori on all the tested  datasets. In case of uncertain data, empirical results show that  this hyper structure based algorithm, P-Hmine, outperforms all  the other algorithms in most of the cases that we have examined.

266 2011 World Congress on Information and Communication Technologies    In case of dense uncertain datasets UFP-tree does not attain good  compact structure and requires large memory, and this results in  high execution time for UFP-growth. We also found that P-  Hmine is scalable for both large number of data items and large  number of transactions. We found that there exists no such  algorithm that outperforms all other algorithms in all cases. In  practice, we can switch the algorithms according to the  applications.

