Preliminary Results from a Machine Learning Based Approach

Abstract  This work describes a possible approach to the problem of extracting knowledge from the analysis of questionnaires through Machine Learning. The idea guiding our research was to investigate the existence of association rules among the topics covered in a course. The data used came from the questionnaires administered to the freshmen in Electronic Engineering attending the course of Foundation of Computer Science at our University. Each questionnaire was coded into feature vectors that  were classified with respect to the grade obtained by the student and analysed with C4.5.

Some statistical results and hints for further work are  discussed in the paper.

1. Introduction  This work is aimed at identifing the existence of  correlations among the formative deficiencies emphasized  by the presence of wrong answers to Multiple Choice  Questions (MCQ) related to different topics. If such  correlations do exist and can be elicited, a more  appropriate feedback can be provided to the students.

Furthermore, the identification of unexpected or  unforeseen correlation among topics may help the teacher  to revise the didactic process.

In our experiments we used the C4.5 package, a  classification system based on top-down induction of  decision trees [2]. C4.5 provides a set of software tools  able to learn inductively models of different concepts  (classes) from a testbed, to build a classifier in the form of  a decision tree and to use it for unknown data  classification. Furthermore, C4.5 provides a tool for re-  expressing the classification tree via production rules in  the simplified form L -> R, where L is a conjunction of  attribute-based tests and R is a class. Each invocation of  the classifier over a set of cases produces an output  containing its performance on the cases from which it was  constructed. Thus, after classes induction, C4.5 applies the  classification to the same cases used for training, and  evaluates the percentage of errors made: the less this error  is, the better the model is. The package contains heuristic  methods for simplifying decision tree, with the aim of  producing more comprehensible structures without  compromising accuracy on unseen cases. Both tree (or  rules) and error values related to the simplified tree are  printed to the output stream. Finally, C4.5 provides a  module that allows the evaluation of the accuracy of a  classification model through cross-validation. In this  procedure, the available data is divided into N blocks so as  to make each block?s number of cases and class  distribution as uniform as possible. N different  classification models are the built, in each of which one  block is omitted from the training data, and the resulting  model is tested on the cases in that omitted block: In this  way, each case appears in exactly one test set. Provided  that N is not too small the average error rate over the N  unseen test set is a good predictor of the error rate of a  model built from all the data.

2. Description of the Data Sample  The data used for knowledge discovery came from 1322  questionnaires administered to the freshmen in Electronic  Engineering attending the course of Foundation of  Computer Science during the academic years ranging  from 1995 to 1998. The data set has been filtered by  removing low-end and high-end achievers in order to  obtain a subset containing information free from border  effects. After this filtering, the data set obtained contained  436 questionnaires. Ten MCQ with four answers  constituted a questionnaire. Each question was given a  weight ranging from 1 to 5 points, representing the  relative importance of the question inside the curriculum.

Thus, the minimum score that may be obtained by a  student is zero and the maximum is 30. The number of different questions in the database is 150. They have been classified according to the topic covered with a taxonomy  of 15 items derived from the course syllabus. Each  questionnaire was transformed in a set of feature vectors  (fv):  A1, A2, A3, ?, A15, P  where:  ? Ai is 1 if the question containing the topic i is correctly answered, 2 if the answer to the question  containing topic i is wrong or missing and 0 if no      question addressing topic i is present in the  questionnaire;  ? P is ?poor? if the score to the questionnaire is between 12 and 17, ?average? if the score of the  questionnaire is between 18 and 20, and ?good? if the  score is between 21 and 26.

The 436 questionnaires allowed generating 7884 fv: this is  due to the fact that the same topic may be covered by  different MCQ in a questionnaire. This required to  generate all the possible combinations of different answers  to the same topic The analysis conducted with the C4.5  program allowed constructing a decision tree that  classifies the fv under examination with an error rate of  11.9%. Furthermore, a list of about two hundred  production rules was generated from the decision tree. The  rules allow to classify the fv with an overall error of  13.4%. Thus, for instance, the following rule has been  generated:  (Rule 1) Scope of variables = 2  Parameters = 2  -> class ?poor?  [98.7%]  This rule allows to infer that if a student does not answer  correctly to questions regarding the topics: ?Scope of  Variables? and to ?Parameters? is ranked ?poor? with a  confidence factor of 98.7%. Therefore, the two topics  seem to be related.

3. Discussion  Three classes of production rules have been identified by  our analysis. The first category puts in relation topics that  according to our knowledge are strongly related. Rule 1 is  an instance of this class and is further strengthened by the  existence of a complementary rule stating that "if a  student answers correctly to questions regarding the same  topics, his/hers questionnaire will obtain a score  comprised between 21 and 26 with a confidence factor of  99.8%". Both rules represent two examples of  straightforward associations, since we strongly believe  that a student failing to catch the concept of ?Scope of  variables? will have difficulties in understanding the  correct use of ?Parameters? and vice versa.

The second category of rules puts in relation topics that  although not being predictable by our experience, may be  understood after a deeper analysis of the questions. Thus,  for instance, a rule does exist allowing to infer that a  student is ranked ?poor? with a confidence factor of  96.6% if his/hers questionnaire contains wrong answers to  questions related to Grammars & EBNF, to Side Effects  and to Recursion. While we believe that it is possible to  put in relation the use of functions containing Side Effects  and Recursion, so that a misconception on each of the  topics may affect the other, it appears difficult to  understand in which way the topic Grammars & EBNF  should be put in relation with the formers. After a careful  analysis, we discovered that most of the questions related  to this latter topic contain excerpts of production rules  involving recursive definitions. Thus, the conclusion that a  student failing to understand the concept of recursion may  have difficulties in answering questions involving  recursive production rules does not appear as an extremely  odd idea.

The last category of rules put in relation topics that do not  appear to be related each other in any way. Thus, it is very  hard to understand the meaning of a rule that allowa to  infer that if a student answers correctly both to a question  related to the use of ?Integers and Reals? and to a question  covering the topic ?Side Effects?, is ranked good with a  confidence factor of 96.1%.

We believe that this last category of rules deserves further  investigation, since it may be useful to obtain a better  understanding on the way students perform.

We decided to adopt the cross-validation utility of C4.5 to  verify that this last class of rules is not due to some sort of  statistical error. The results obtained by dividing the data  set into ten block of randomly selected samples are very  encouraging since the average error rate is 15,9% if the  decision tree is applied and 16.2% if the rules are used.

This seems to indicate that the inferred rules are slightly  independent from the adopted data set.

4. Final Remarks and hints for future work  There is a number of open questions about the approach  discussed in this paper that call for further investigation.

Thus, for instance, the same topic is covered by questions  having different weights. We need to explore how, and to  what extend, this aspect may influence the elicited rules.

Furthermore, the same question may cover different  topics. In fact, a number of questions aimed to assess the  scoping of variables are written using in Pascal. How the  knowledge of the programming language may affect the  answer to these questions has not yet been investigated.

Moreover, for each topic there exist questions aimed at  verifying different levels of competence, ranging from  Knowledge to Evaluation [1]. How this aspect may be  related to the results obtained, is still a matter of  investigation. Finally, we need to gain a deeper  understanding of the rules that put in relation topics that  appear to be completely independent each other.

