Mining Weighted Negative Association Rules from Infrequent Itemsets Based on  Multiple Supports

Abstract?The existing researches for association rule mining most use the single minimum support. However, in practical applications, the occurrence frequency of each itemset is different. We should set different minimum support for itemsets. In association rule mining, if the given minimum support is too high,then the items with low frequency of appearance couldn?t be mined. Otherwise, if the given minimum support is too low,then combination explosion may occur. We support the technique that allows the users to specify multiple minimum supports to reflect the natures of the itemsets and their varied frequencies in the database. It is very effective for large databases touse algorithm of association rules based on multiple supports. The existing algorithms are mostly mining positive and negative association rules from frequent itemsets. But  the negative association rules from infrequent itemsets are ignored. Furthermore, We set different weighted values for items according to the importance of  each item. Based on the above three factors, an algorithm for mining weighted negative association rules from infrequent itemsets based on multiple supports(WNAIIMS) is proposed in this paper.

Keywords-multiple minimum support, infrequent itemset, weight, negative association rule

I. INTRODUCTION In association rule mining, two threshold values  should be set firstly: minimum support and minimum confidence. The entire data mining is processing under the two threshold value restriction. Therefore, support and confidence play the important role in the mining process.

Support threshold value becomes a key factor in association rules mining. In statistical significance, minimum support defines the lower limit value that association rules need to cover the number of events samples.This lead to setting the only one support for different attributes to the data sample in date mining. That is to say different attributes of the data sample have the same minimum probability. For example, we are interested in transactions that some may occur very frequently, but others may occur very sparse. Then there is a problem in the existing association rule mining, if the given minimum support is too high, then the items with low frequency of appearance can?t be mined. Otherwise, if the  given minimum support is too low,then a large number of non-meaningful data association will be filled in the mining process.Usually the mining strategy is to cover the greater probability affairs but abandon the small probability transactions. To solve this problem, We set different minimum supports based on different  probability of the datasets  The existing algorithms are mostly mining positive and negative association rules from frequent itemsets. Literature [1] is mining positive and negative association rules from the frequent itemset on  multiple supports.But the negative association rules from infrequent itemsets are ignored . In fact, more negative association rules exist in the non- frequent item set.

According to the importance of different items. set different weight values for each items.This can make the association rules more meaningful.

Based on the problems offered above,an algorithm for mining weighted negative association rules from infrequent itemsets based on multiple supports(WNAIIMS) is proposed.Experimental results show that the algorithm is efficient and feasible.



II. PROBLEM STATEMENT AND RELATED WORK  A. Weighted Association Rules Weighted Association Rules is as follows: there are n  records and m items. Let I ={i1,i2,....,im}be a set of items and W={w1, w2,?, wm}(wi [0,1] be a set of non-negative real numbers. A pair(X, w(X)) is called a weighted item where X I is an item and w(X) W is the weight associated with

X. A transaction is a set of weighted items, each of which may appear in multiple transactions with different weights.

A dataset may therefore be defined as a set D of transactions.

Given a weighted itemset X and a set of transactions, referred to as D, we say X has support in D if s% of transactions in D support X. The weighted support of an itemset X in a dataset D, denoted as wsup(X)= countD(X)/|D|*w(X), where countD(X) is the number of transactions in D containing X. Note that the support of a   DOI 10.1109/ICICEE.2012.32     weighted itemset X is always less than or equal to the support of any of its generalization.

There are several different formulations for the weighted support of the rule X?Y according to the different demand, where X?I, Y?I, and X?Y=?. The left-hand side(LHS)of the rule is called the antecedent, while the right-hand side(RHS)is called the consequence. In this paper the following form is used:  sup(X?Y)= countD(X?Y)/|D| wsup(X?Y) = avg{ w1, w2,?, wp } * sup(X?Y) where p is the items? count of the itemset X, avg{ w1,  w2,?, wp } is the average value of the weight of itemset X?Y and sup(X?Y) is support of itemset X?Y before the item is associated with weight.

An itemset is said to be weighted-frequent (large) if its support is larger than a given value (also called minimum weighted support (wminsup)).

The confidence of the WAR denoted as conf is the ratio of the weighted support of X Y over the weighted support of X. The weighted support of X Y (wsup) in the transactions is larger than wminsup, furthermore when X appears in a transaction, Y is likely to appear in the same transaction with a probability conf. X?Y is a valid rule if its weighted support wsup(X?Y) and weighted confidence wconf(A?B) meet minimum weighted support (wminsup) and minimum weighted confidence (wminconf).

B. The introduction of multiple minimum supports Under multiple minimum supports conditions,recursive  nature of apriori algorithm  is no longer valid and the existing algorithms cannot be applied. The following example can prove this.

Suppose that there is a transaction database DB containes four properties <A, B, C, D>. The multiple minimum support for each property is MIS(A)=0.15, M IS(B)=0.25, MIS(C)=0.06, MIS(D)=0.08. By scan two-high- frequent itemsets,we can obtain sup ({A,B})=0.12,Obviously,{A,B} is less than the required minimum support A and B.According to Apriori algorithm, <A, B> will be abandoned as a non-high-frequent itemset.However, after introducing the multiple minimum support strategy, from the new definition,we can find that the minimum support required of attributes C and D are only 0.06 and 0.08 respectively,they are both less than 0.12, < A ,B, C> and< A,B,D > should remain three-high- candidate-frequent itemsets.This shows that apriori nature is no longer valid after introducing the multiple minimum support strategy.The single minimum support cannot fully express the difference between mining object its own characteristics and the actual frequency.The minimum support set too high or too low cannot achieve good mining results.So in this paper we introduce the multiple minimum support strategy in data mining. The approach not only avoid generating too many rules of no practical significance,but also can mining low frequency rules effectively.

The multiple minimum support strategy is a data mining method that on the basis of maintain previously support define,set different minimum supports for different transaction itemsets.According to the characteristics of their own affairs,each requires its own minimum support.

C. The definition of multiple minimum support For a given transaction database let I ={i1,i2,....,im}as  the set of all transaction items. setting a minimum support MIS(im) for each item im , MIS(I)={MIS(i1)MIS(i2) ......

MIS(im)}.



III. WNAILMS ALGORITHM All association rule mining algorithms similar to  classical Apriori algorithm.The generating candidate sets are all based on Apriori nature. Generating k itemset by connecting K-1 frequent itemsets gradually. In multiple minimum support association rules mining,the subset of frequent item may not be frequent, Apriori nature is no longer apply, when using Apriori nature in multiple minimum support association rules.Algorithm must satisfy the frequent itemsets and candidate itemsets follow minimum item support (MIS) from small to large order.

Items  that have the same MIS values, sort  according to the priority of their appearance.

WNAIIMS algorithm describe as follows: Input: DB : transaction database Output: All weighted frequent and infrequent itemsets Septs:  Set multiple minimum support  MIS(i) for each item I, The value can be given by experts or we can adjust the value according to practical necessity;  Sort items according to the weighted minimum support (sort from small to large);  Weighted according to each project is the order minimum support;  For each i in I, when sup(i)> = WMIS (i) , added  i in weighted  frequent 1- itemsets  WL1 ,Otherwise added i in weighted infrequent 1_itemsets WNL1;  for(k=2; WLk-1??; k++)do if k=2 then C2=candidate_gen_level2(I) else Ck=candidate-gen(WLk-1);  candidate_gen_level2 for each item i in the same  order do  if i.count>=WMIS(i) then for each item h in I that is after i do  if h.count>= WMIS(i) then insert{i,h} into  C2  else  insert{i,h} into WNL2 candidate_gen()  for each itemset I in WLk-1 in the same order do for each itemset m in Lk-1 that after I do  if l?m=l.the last item p in l And 1?m=m.the last item q in m then     if p<q then insert l? q into Ck: else insert m? p into Ck:  getWFrequent() and getWINFrequent() getwsupport(Ck); for each itemset c in Ck do  if SC.count>=c.MS then insert c into WLk; else insert c into WNLk.

In candidate_gen_level2, we use I, not weighted frequent 1_itemsets generating the 2-candidate itemsets ,because the subset of frequent item may not be frequent In multiple minimum support association rules mining, A weighted frequent two itemset is composed of a weighted frequent one item and  non-weighted  frequent one item.Therefore, a weighted frequent two itemset that composed of weighted frequent one items  is  incomplete, potentially useful weighted frequent two- itemsets.

Itemset I is sorted  from small to large  based on the value of  WMSI. For any  item i in the itemset I, determine whether the support of j that after i in the itemset I meet WMIS of the current item.if meet the condition, Both will be composed of two sets into two focused on the candidate two itemset that composed of the two items  insert into candidate two itemset.

In candidate-gen,first, take out an itemset i from Lk-1 in order, Then compare with all items  that  behind I one by one.If you concentrate on two items, in addition to other items outside the last one are the same.If in addition to the last item,the other items are all the same in two items. then Link  these two items  in order. And inserted the itemsets into Ck. After generating the candidate items,we also need pruning step. remove itemsets than users not interest from the candidate itemsets.

Pruning processing algorithm is as follows:  for each itemset c?Ck do for eaeh (k-1)_subsets of c do  if (c[1] 	s) or (WMIS(c[2])= WMIS(c[1])) then if (s	WLk-1) then delete from Ck:  For any k-1 subset of candidate k itemset,If the subset of current k-1 itemsets contains the current candidate k or when the two smallest of current k candidate items have the same value of WMIS,and is a non-frequent itemset, its supersets (the current k candidate itemsets) cannot be frequent,then remove the current k candidate itemsets from the candidate itemset.

If SC.count is equal or greater than c.MS in the functions of getWFrequent() and getWINFrequent() , item i in candidate itemsets is outputed to WLk as a frequent itemset, otherwise, i is outputed to WNLk. Rules' generation is similar to references[2].



IV. EXPERIMENTAL RESULTS We  run the algorithms on a simulated  dataset to test the  WNAIIMS algorithm. We assume that wmc is equal to 50%, mininterest is equal to 10, the values of multiple minimum support are as follows in Table 1 and the result of weighted frequent and infrequent itemsets as follows:  TABLE I. THE NUMBERS OF WEIGHTED FREQUENT AND INFREQUENT ITEMSETS  The method of rule generation is similar to references[1].

The number of generated rules is shown in Table 2.

TABLE II. THE NUMBERS OF RULES  ALGO RITH MS  WNR from WF WNR from WIF    A??   B A?? B A?B   A??  B A? B A?B  WNAII MS 34 18 8 14 40 1  WPNI MS 41 25 8 9 36 7  WNIM S 45 33 11 6 34 2  In the two tables above, weighted positive association rules(WPR), weighted negative association rules(WNR) , weighted frequent itemsets(WF) and weighted infrequent itemsets(WIF) are listed. By comparing the results of algorithms WNAIIMS, WPNIMS[2], WNIMS[3], we can prove that the number of rules from WNAIIMS algorithmsis obviously less than ones from WPNIMS and WNIMS algorithms. Experimental results show that the algorithm of WNAIIMS is efficient and feasible.

