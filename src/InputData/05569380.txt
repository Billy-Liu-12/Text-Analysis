978-1-4244-5934-6/10/$26.00 2010 IEEE                                 1668

AbstractPrivacy has become an important issue in Data  Mining. Many methods have been brought out to solve this  problem. This paper deals with the problem of association rule  mining which preserve the confidentiality of each database. In  order to find the association rule, each participant has to share  their own data. Thus, much privacy information may be  broadcasted or been illegal used. These issues can be divided  into three categories: data hiding, knowledge hiding and data  mining results publishing. This paper reviews the major  method of privacy preserving on each category and choose  some of them to complete our system. At the end, an  improvement of sensitive rules hiding is proposed to make it  more accuracy and security.

Keywords-privacy preserving; association rule; data hinding;  knowledge hiding; results publishing

I.  INTRODUCTION  Association rule mining, as a very important technique,  has already been applied in a wide range of areas. However,  it breaks out many privacy issues. From a general point of  view, we may classify privacy issues into two broad  categories. The first is related to the data hiding, the second  concerns the information, or else the knowledge, that a data  mining method may discover after having analyzed the data,    and is known as knowledge hiding.

Data hiding tries to remove confidential or private  information from the data before its disclosure. In this case,  many randomization methods have been addressed. The  randomization method has been traditionally used in the  context of distorting data by probability distribution. The  miner uses the perturbation data to get the association rule. In  this case, the data miner does not know the raw data and also  can get the similar result. Which the key point for data miner  is how to reconstruct the raw data distribution. Agrawal R  Gave out a method named Bayes Reconstruction Method [1].

Randomization method will change the raw database,  there will be many negative impacts, just can be classify into  two parts:  y Useful rules have been lost;  y New rules have been produced artificially.

In fact, there will be a contradiction between accuracy  and security. In other word, if we disturb the raw data so  much that the mining result will be poor. In the same, if we  disturb it so little that the hacker will get the data they want  more easily. To solve this problem, Secure multi-party  computation [2] is addressed, it use security communication  protocols and cryptography, make it possible to mining rules  on the raw data, this can reduce the negative impact. But this  will cause huge communication costs. Further more,  complex cryptography will make the algorithm low-  efficiency.

Knowledge hiding, on the other hand, is concerned with  the sanitization of confidential knowledge from the data. As  a result of association rule mining, many useful association  rules will be discovered, but at the same time, many privacy  rules will also be exposed which do not want others to know.

To solve this, we have to limit the mining process, in order to  keep these sensitive rules being hidden. There are so many  methods to solve this problem. Which we used is just one  kind of them, called Support-based and Confidence-based  Blocking Schemes [3, 4, 5].

The main disadvantage of a blocking algorithm is the fact  that the dataset, apart from the blocked values, is not  distorted. Thus, an adversary can disclose the hidden rules by  identifying those generating itemsets that contain question  marks and lead to rules with a maximum confidence that lies    above the minimum confidence threshold. If the number of  these rules is small then the probability of identifying the  sensitive ones among them becomes high. Chris Clifton  presents a method to avoid this problem [6].

At the end of association rule mining, the frequent  itemsets or the association rules with confidence and support  factors are published. There is an approach to reconstruct a  dataset from the published frequent itemsets, and then use  the reconstructed dataset to analyze privacy [13]. This field  is called inverse frequent itemset mining [11]. In association  rules data mining, it is a convention to conceptually split  datasets into two parts: Quasi-identifier (QI) attributes and  Sensitive Attributes (SA). In the mining results publishing  process, while form as SAQI ? rules published, adversaries  may derive the value of SA according to the threshold (c, s),  which are confidence and support respectively. If the results  This research was supported by Natural Science Foundation from  Nanjing University of Information & Science Technology (20080302),  Natural science fund for colleges and universities in Jiangsu Province  (08KJD520018), Jiangsu oversea study scholarship and Jiangsu Youth  Project.

published as )','( scSAQI ? , adversaries may inductive the  value of SA more precisely, in which, )','( sc means exact  confidence and support.

In this study, we use existed approaches to construct the  privacy preserving association rule data mining system,  which can protected privacy from data to knowledge.



II. APPROACHES  In this section, the main approach used in our system is  presented. Apriori algorithm is adopted to find the frequent  itemsets, and Secure Multiparty Computation (SMC) is used  to get the global support and confidence without privacy  disclosure. On the Knowledge hiding, an improved algorithm  is mentioned, which can get satisfy result. For mining results  hiding, two scenarios are considered.

A. Secure computation  The history of the multi-party computation problem is  extensive since it was introduced by Yao [7] and extended by  Goldreich et al [8]. The basic idea of SMC is that a  computation is secure if at the end of the computation, no    party knows anything except its own input and the results.

Security Sum [6, 9] is a very simple and useful method  which base on SMC. It is used for get the sum of data from  the different site. It works as follow:  Assume that the value ?

=  =  n  l  lvv   to be computed is  known, which lies in the range [0, n]. One site is designated  as the master site, numbered 1. The remaining sites are  numbered 2 n. Site 1 generates a random number R,  uniformly chosen from [0, n]. Site 1 adds this to its local  value v1, and sends the (sum R+v1 mod n) to site 2. Since the  value R is chosen uniformly from [1, n], the number (R+v1  mod n) is also distributed uniformly across this region, so  site 2 learns nothing about the actual value of v1. For the  remaining sites i = 2n, the algorithm is as follows:  Site i receives ??

=  +=    mod  i  j  j nvRV  then computes  ?

=  +=+=  i  j  ij nVvnvRV   mod)()mod(  and send to  site i+1.

At last each site do the steps above, and sends the final  result back to the site 1, the site 1 minus R and get the sum.

B. Association rule hiding    Wang and Jafari [3] proposed two modification schemes  that aim at the hiding of predictive association rules, i.e. rules  containing the sensitive items on their LHS (left-hand-side).

Both algorithms rely on the distortion of a portion of the  database transactions to lower the confidence of the  association rules. Compared with the work of Saygin et al.

[4, 5], the algorithms presented in [3] require a reduced  number of database scans and exhibit an efficient pruning  strategy. However, by construction, they are assigned the  task of hiding all the rules containing the sensitive items on  their LHS, while the algorithms in the work of Saygin et al.

can hide any specific rule. In the first strategy, called  Increase Support of LHS (ISL), the confidence of a rule is  decreased by increasing the support of the itemsets in its  LHS. In the second approach, called Decrease Support of  RHS (DSR), the confidence of the rule is reduced by  decreasing the support of the itemsets in its RHS (right-hand-  side).

C. Mining results publishing  Although association rules only contain aggregate  information about original datasets, they have potentials to  disclose the private information in the original datasets.

There are two potential scenarios while association rules  publishing. In the first scenario, data owners withhold the  exact support and confidence scores. That is, users (and  adversaries) only know the published association rules  together with the thresholds for support (s) and confidence  (c). In this situation, for an association rule SAQI ? , we  only know the following:  cQISAP ?)|( and sQISAP ?),( , where P means  probability.

As )(/),()|( QIPQISAPQISAP = , so,  ))(,max(),( QIPcsQISAP ??                                   (1)  In the second scenario, to give users more information  about the association rules, data owners also publish the  exact support and confidence scores for the association rules.

That is, when publishing an association rule )','( scSAQI ? .

We can derive the following:  ')|( cQISAP =  and '),( sQISAP =                              (2)  Assuming that adversaries have all the data of the QI  attributes, adversaries also know the domain of the sensitive  attributes, i.e., they know all the possible values of the    sensitive attributes. From the equation (1) and (2),  adversaries may induce the possible value of SA. Obviously,  the value of SA in equation (1) is more difficult to deduce  than in equation (2). Adopting which mining results form to  publish, it is depends on the privacy information disclosure  in each form.



III. OUR ALGORITHMS  A. The Association rule mining System  Fig.1 gives a general structure of out system, which can  be divided into three steps:    First step: Security Sum is used to get the global support  and confidences, which keep the participant dont know the  local support of each sit.

Second step: SMC is adopted to send all itemsets from  different site to one center, without data leakage.

Third step: In this step system does the sensitive rules  hidden, its a very important step, building up a Sensitive  rules Database, Which contain the rules should be hidden.

Then something should do to hide these rules, which is  discussed in next section.

Fig.1.association rule mining system  The first two steps are belonging to the data hiding  process and the third step is the knowledge hiding process, a  detailed description is given in the next section.

B. Data hiding  Center M produces a random data matrix, which meet the  average distribution, then sends this matrix to site A. Follow  the Security Sum method, we can get the global support and  confidence of each item from the local support and  confidence on each sit.

Assume the items on Site A which have a support matrix  XA , center M will produce a random matrix Y which have the  same order with X, and Y can be distribution-free. Site A get  Y and send ZA= X+Y to Site B, Site B with its own support  matrix XB, make ZB=ZA+XB. At the end of this process, M  will get the global support by minus Y. Each site cant know  the local support of the other site.

The RSA encrypt algorithm is used, Data miner produce  the center encrypt key which denoted C-e and the key-pair of    each site denoted as (ei,di), ei is encryption key and di is  decryption key. Data miner will send di to Disturb Center  and send ei and C-e to each Site. Disturb Center will produce  an ID for each site. While, each site will use the ID, ei and C-  e to encrypt their own frequent itemset and send the result ID,  ei(C-e(frequent itemset)) to Disturb Center. The Disturb  Center use the di and the ID decrypts the data from different  site. After this step, Disturb Center will change the data to C-  e(frequent itemset) and disrupts the order then sends C-  e(frequent itemset) to Data miner. Data miner decrypts the  itemsets and use the Apriori algorithm and the global support  and confidence form M to analyze the itemsets and get the  association rules. The detail of the method is described as  follow:  Encrypt step:  1) Data Miner produce public encryption key C-e;  2) Data Miner produce key-pair of each site (ei,di);  3) Send ei to each site, di to Disturb Center;  4) Disturb Center send ID to each site;  5) Each site Encrypt their frequent itemset to ID ei(C-  e(frequent itemset));  Decrypt step:  1) Each site send their data ID ei(C-e(frequent  itemset)) to Disturb Center;  2) Disturb Center use di decrypt the data from each  site and remove the ID;  3) Disturb Center disrupts the order of the data ,send  C-e(frequent itemset) to Data Miner;  4) Data Miner decrypts the data, getting frequent  itemsets.

C. Knowledge hiding  In traditional methods, to hide a sensitive rule, some item  should be deleted or use an unknown data to change the raw  data. In terms of association rule mining, rule A=>B will be  discovered, as long as satisfy these two conditions:  1. Min_sup B) andP(A B)Support(A ?=? ;  2.

confMin _  support(A)  B)support(AB)|P(AB)(AConfidence  ?

?

==?

Properly, decreasing the support of B, decreasing the  support of A?B or increasing the support of n can realize  hiding the rules. But there is a conflict, when a support of  items is changed some other insensitive rule will be hidden,  at the same time or other new forgery rule will be produced.

S.-L.Wang [3] proposes two data mining algorithms for  hiding informative association rules, namely Increase  Support of LHS (ISL) and Decrease Support of RHS (DSR).

The first algorithm tries to increase the support of left hand  side of the rule.  The second algorithm tries to decrease the  support of the right hand side of the rule. The detail of the  ISL algorithm is described as follow.

Algorithm ISL    Input:  (1) a source database D,  (2) a min_support,  (3) a min_confidence,  (4) a set of predicting items X  Output: a transformed database D, where rules  containing X on LHS will be hidden  1. Find large 1-item sets from D;  2. For each predicting item x?X    3. If x is not a large 1-itemset, then X: = X-{x};  4. If X is empty, then EXIT;// no rule contains X in LHS  5. Find large 2-itemsets from D;  6. For each x?X {  7. For each large 2-itemset containing x {  8. Compute confidence of rule U, where U is a rule like  x?y;  9. If conf(U) < min_conf, then  10. Go to next large 2-itemset;  11. Else {//Increase Support of LHS  12. Find TL = {t in D|t does not support U};  13. Sort TL in ascending order by the number of items;  14. While {conf(U) ? min_conf and TL is not empty }{  15. Choose the first transaction t from TL;  16. Modify t to support x, the LHS(U);    17. Compute support and confidence of U;  18. Remove and save the first transaction t from TL;  19. }; // end While  20. }; // end if conf(U) < min_conf  21. If TL is empty, then {  22. Can not hide x?y;  23. Restore D;  24. Go to next large-2 itemset;  25. } // end if TL is empty  26. } // end of for each large 2-itemset  27. Remove x from X;  28. } // end of for each x?X  29. Output updated D, as the transformed D    Through the above method, the sensitive rules will be  hidden, but some insensitive rules may have been hidden  also, and many new rules may have been produced  artificially. To solve this problem, system should use the  mining results to restraint the choosing process (which  choose item to modify). Our method like this:    Fig 2 sensitive rules hiding process    In the modification the choose process, we choose other  item as sacrifice item, in order to get a better results. Then  we add some noise rules to increased security.

In the ISL, the short itemset is chosen as sacrifice item,  because it has least item, negative impact cause by modify it  may be the smallest. Our method first step is the same with  ISL or DSR, then check the mining result, in this step, using  privacy quantify to measure the result, if its dissatisfied,  return to change the choose policy. The items with support  more close to Min_sup as unsettled itemsets, the other is  settled itemsets. While choosing the sacrifice item, the  item in settled itemsets will be chosen first.

As the main disadvantage of a blocking algorithm is the  fact that the dataset, apart from the blocked values, is not  distorted. So create some noise rules is necessary, to make  the dataset distortion, which can be deleted in pruning step.

D. Result hiding  As discussed in section 2.3, there are two forms for  publishing the association rules mining results. From the    viewpoint of privacy preserving, we would like to adopt the  form as SAQI ? . But if we are sure that there is less  privacy disclosure, we can releases the mining results as  form )','( scSAQI ? . Whats form is the best choice? We  consider it from the disclose information from the mining  results.

As we know, the adversaries only get the information  from equation (1), (2). As )(QIP  is publicity, so the  probability of ),( QISAP is induced. Further more, the  probability of )(SAP can be reconstructed from  the ),( QISAP , noted as )(* SAP . If the )(* SAP is similar as  real probability )(SAP , we say, the privacy disclosure is  serious. The method of reconstruct )(* SAP  from  ),( QISAP is Maximum Entropy Modeling [11]. The  similarity between )(* SAP and )(SAP is measured by  standard Kullback-Leibler (KL) Divergence [12]. Zutao Zhu  discussed the effect of exact confidence value c, and it gave  the data owners a useful guideline to decide whether to  publish the exact confidence values [11]. We adopt the idea  of Zhus.



IV. CONCLUSIONS  In this work, we have reviewed some approaches about  privacy preserving while mining association rules,  constructed a system for data mining, in which the data,  knowledge and rules publishing are considered. Using  Secure computation and RSA encryption technology, avoid  data leakage which cause by data sharing. On the side of  knowledge hiding, using ISL and DSR achieve sensitive  rules hiding, and an optimization method is presented for  better result, with smaller negative impact. For the mining  results publishing, the privacy information is measured in    two scenarios. The published result form depends on the  privacy information disclosure by the published association  rules.  What ever data hiding, knowledge hiding or mining  results publishing, it is based on small data. How to increase  the algorithms efficiency for mass data is future works.

