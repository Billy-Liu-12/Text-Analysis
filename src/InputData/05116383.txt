Efficient Algorithms with Time Fading Model for   Mining Frequent Items over Data Stream

Abstract?Efficient algorithms with time fading model for mining frequent items over data stream are presented. Our algorithm FC2 can detect?-approximate frequent items of a data stream using O(??1) memory space and the processing time for each data item is O(1). Experimental results on several artificial data sets and real data sets show that our methods have high precision, require less memory and consume less computation time than other similar methods.

Keywords-data mining; data stream; frequent items; time fading model

I.  INTRODUCTION Mining frequent item sets from stream data is an  important problem with ample applications in stream data analysis. It has applications in many areas such as sensor data mining, business decision support, analysis of web query logs, direct marketing, network measurement, monitoring and traffic analysis.

Recently algorithms for identifying frequent items and other statistics in the entire data stream have been proposed.

Lossy Counting [1] by G. S. Manku et al was the first algorithm for finding frequent items from a data stream.

Lossy Counting is a one-pass algorithm that provides an accuracy guarantee on the set of frequent data items and their frequencies reported. Karp, Shenker and Papadimitriou [2], and independently, Demaine, Lopez-Ortiz and Munro [3], applied a deterministic MG algorithm by Misra and Gries [4] to detect frequent stream data, they decreased the processing time of MG algorithm to O(1) by managing all counters in a hash table. The algorithm can easily be adapted to find ?-approximate frequent items in the entire data stream without making any assumption on the distribution of the item frequencies. This algorithm needs only 1/? counters for the most frequent data items in the stream. Processing the arrival data items involves only the  operations of increment or decrement of some of these counters.

Many algorithms for frequent item counting use random sampling. Flajolet and Martin [5] and Whang et al. [6] proposed probabilistic algorithms to estimate the number of distinct items in a large collection of data in a single pass.

Golab, DeHaan, Lopez-Ortiz and Demaine [7] gave an algorithm for the problem when the item frequencies are multinomially-distributed. Gibbons and Matias [8] presented sampling algorithms to recognize top-k queries.

H. Liu [9] et al presents an error-adaptive and time-aware maintenance algorithm for frequency counts over data stream. G. S. Manku et al [1] advanced a sampling based algorithm called ?sticky sampling? for computing an ?- deficient synopsis over a data stream of singleton items.

The algorithms mentioned above do not discount the effect of old data, all data items in the whole history of the data stream are given equal weight. This is undesirable in solving many application problems. One way to handle such problem is using sliding window models. An algorithm which works over sliding windows can ignore the out of date data and only consider the recent data, which have greater importance in many cases. Recently several data mining algorithms over sliding windows are proposed.

Based on MG algorithm, Arasu and Manku [10] gave a deterministic algorithm for finding ?-approximate frequent items; it supports ( )11 log ?? ??O  query and update time and uses ( )121 log ?? ??O  space. Golab, DeHaan, Demaine, Lopez-Ortiz and Munro [11] gave some heuristics for the problem to identify frequent items over sliding window. Lee and Ting [12] proposed an algorithm which achieves O(?-1) space. Their algorithm needs O(?-1) processing time for update and query. L. Zhang and Y Guan [13] have proposed a stream data frequency estimation algorithm over sliding windows. Their algorithm requires O(?-1) memory space and O(1) time to process a each item data and answer a query.

DOI 10.1109/IIS.2009.48    DOI 10.1109/IIS.2009.48     Other recent works on data stream algorithms have been surveyed in [14, 15].

In this paper, we advance the time fading model for frequency measures in data stream. In this model, the entire stream is taken into account to compute the frequency of each data item, but more recent data items contribute more to the frequency than the older ones. This is achieved by introducing a fading factor 0<?<1. One advantage of this model is that it takes into account the old data items in the history. In the fading model, the frequency changes smoothly when more data arrive continuously without a sudden jump. Based on such time fading model, we design two efficient frequency estimation algorithms which improve the space and time cost. Firstly, we propose an algorithm called FC1 which can detect ? -approximate frequent items in data stream. FC1 requires O(?-1) memory space and the processing time for each data item is O(?-1).

Furthermore, we also proposed an improved algorithm FC2 which also requires O(?-1) memory space but processes each data item in O(1) time. We verify the effectiveness of our approach empirically using real datasets and synthetic datasets and show our methods have high precision, require less memory and consume less computation time than other similar methods.



II. DENSITY OF DATA ITEM In many data stream applications, recent data are more  important than the older ones. To capture this, we use a fading factor )1,0(??  in calculating the data items? support counts. Let the input data stream be X=(x1, x2, x3,?). For each data item x, its support count decreases as x ages. We call such modified support counts as the density of the data item. In each time step, the density of the data item will be reduced by a fading factor ? .

Suppose t>ts, here t is the current time and ts is the latest time data item x being received. It is obvious that  sttstxDtxD ?= ?),(),( . (1)  Although the density of a data item should be constantly modified by a fading factor ? , it is unnecessary to update the density values of all data records at every time step.

Instead, it is possible to update the density of a data item only when an identical new data item is received from the data stream. The time when the last data item x was received should be recorded so that density of x can be updated when a new data x arrives. Assuming a new data item x is received at time tn, and the time when the last same data item x arrived is tl (tn > tl), the density of x can be updated as follows:  1),(),( += ? l tt  n txDtxD ln? . (2)  Lemma 1: Let X(t) be the set of all the data items that arrive from time 1 to t, we have:  a) ??  ? ?  ? ? ?=?  ? 1  1),(  )(  t  tXx txD , for any t=1,2,?..

b) ??  =? ??? 1  1),(lim )(tXxt  txD  Proof  of  lemma 1 is omitted due to the limited space.

Let )1,0(?S be a user specified threshold, at time t, a  data item x is a frequent one if its density ??  ?  ),( StxD .

Let )1,0(??  be a user specified error bound and ?<S. We are asked to discover the data items with density at least  ? ?  ? ?  S . The estimated densities output by our algorithm are  less than the complete densities by at most ?  ? ?1  .



III. THE ALGORITHM FC1  A. The Framework of the Algorithm FC1 In FC1, for each data item x, a characteristic vector is  defined as C(x)= [x, D(x, ts), Da(x, ts), ts], where ts is the last time when data item x is received, D(x, ts) is the relative density of data item x at time ts. Da(x, ts) is the complete density of data item x at time ts assuming its density values are not deleted.  The algorithm FC1 processes the coming data from stream and updates a list D where each entry is the characteristic vector of a data item. The maximum size of D is L=?-1.

When the algorithm receives a new data item x from the stream, the algorithm modifies the list D by creating or updating a characteristic vector in D. Whenever the size of D goes beyond L, the entry with the smallest density should be deleted. If the characteristic vector of data item x already exits in D, the algorithm modifies its density and ts value.

Otherwise, the algorithm creates a new entry for x and inserts it into D.

Algorithm FC1: Input:  X: the data stream;  ?: fading factor;  ?: density error bound; S: density threshold; Output: D: a list of potentially frequent data items; Begin  1. t=0; L= ?/1 ; 2. While not terminate do 3.     Receive a data item x from the stream; 4.     If there is no entry corresponding to x in D then 5.         Create a new entry ],1,1,[ tx ; 6.         If  |D| ? L  (D is full) then 7.              delete entry ]'),','(),','(,'[ ssas ttxDtxDx  where )','( stxD ' st t? ? is the smallest;  8.               q?= )','( stxD ' st t? ?  9.               For each entry [x,D(x,ts),Da(x, ts),ts] in D 10.                           '),(1 qtxDq s  tt s ?=  ?? ,     sttsa txDq ?= ?),(2  11.                      replace [x,D(x,ts),Da(x, ts),ts] by ],,,[ 21 tqqx 12.               End for 13.       End if 14.            Insert new entry ],1,1,[ tx in D; 15.     Else  /* Assuming the corresponding entry of x in  D be [x,D(x,ts),Da(x, ts),ts] */ 16.             1),(1 +=  ? stt stxDq ? ,  1),(2 += ? stt  sa txDq ? 17.             replace [x,D(x,ts),Da(x, ts),ts] by ],,,[ 21 tqqx ; 18.       End if 19.       t=t+1; 20.   End while  End The algorithm does not multiply the densities by the  fading factor? for all the entries in D at each time step, instead line 11 in the algorithm updates the density of a data item using ts only when an identical new data item is received from the data stream.

We will show that if x is not listed in D, it could not be a frequent one even if add all its deleted densities.

Theorem 1: Suppose an entry C(x)=[ x, D(x, ts), Da(x, ts), ts] of data item x is deleted from D at time t, its real density  d(x, t) is less than ?  ? ?1  .

Proof  of  theorem 1 is omitted due to the limited space.

Theorem 2: Suppose at time t there exists an entry C(x)=[  x, D(x, ts), Da(x, ts), ts] of data item x in list D, its real density d(x) at time t satisfies  stt aa txDtxdtxD  ?  ? +?? ?  ? ?  ),(),(),( .

Proof  of  theorem 2 is omitted due to the limited space.

To answer a query request at any time t, only the frequent  data entries [x,D(x,ts),Da(x,ts),ts] satisfying  ? ??  ? ?>?  ),( StxD sttsa   can be output. It is obvious that all  items whose complete density exceeds ??1  S  must be output  and no item whose complete density is less than ? ?  ? ?  S  is  output.  From theorem 1, we know that the estimated densities of the output entries are less than the complete  densities by at most ?  ? ?1  .

B. Complexity of the Algorithm FC1 Since the maximum size of list D is L=?-1, our algorithm  requires no more than O(?-1) memory space. In each step, the algorithm processes one data item by the operation of inserting a new entry, or modifying and moving an old entry in D, or deleting an entry from D. All these operations can be  performed in O(?-1) time. Therefore, time complexity for the algorithm to process one data item is O(?-1).



IV. THE ADVANCED ALGORITHM FC2  A. The Framework of the Algorithm FC2 In algorithm FC1, when an entry is deleted from list D,  the densities of all the other entries should be reduced by an identical value. This requires O(L) computation time, where L=|D|=?-1. In order to reduce the complexity of processing each item, an improved algorithm FC2 is advanced. FC2 improves the characteristic vector, and introduces the parameter d? and the time it is produced.

To avoid the operations of subtracting an identical density value from all the entries, we can store this density value instead of subtracting it from all the densities in D. To keep this deleted density value, the algorithm uses a record defined as follows.

Definition 1 (Record of the Deleted Density): FC2 saves the record of the deleted density ],[ dtd? , where d? is the accumulated deleted density in the algorithm, td is the time when d? is created or modified.

Since the algorithm keeps a record of the deleted density, it is not necessary to modify the relative density when an entry is deleted from D. Therefore, the relative density in each entry in D does not reflect its actual value.

Suppose the relative density of a data item x recorded in D is D(x, ts) at time t, then its actual value should be  ds tttts dtxDtxD ?? ??= ??),(),( . (3)  When a new entry is inserted into the list D, its relative density reflects its real value, and should not be subtracted by the deleted density d? . To make a uniform computation using (3) to get the actual relative densities of all the entries in D, we add a correction value dx and its produced time tx in the characteristic vectors.  In FC2, characteristic vector is defined as follows  Definition 2 (Characteristic Vector in FC2): The characteristic vector of a data item x is C(x)= [x, D(x, ts), dx, ts, tx], where ts is the last time when data item x is received, D(x, ts) is the relative density of data item x at time ts, dx is the density correction value of x, tx is the time when the entry of data item x is created.

Using the characteristic vector, the actual density value of a data item in the list D can be calculated. Suppose at time t, the characteristic vector of x is ],,),,(,[ xss ttdxtxDx , then, its actual relative density is  )(),( xds tttttts dxdtxD ??? ??? ??? . And its complete  density is sttstxD ??),( . When a new entry is inserted into  the list D, its initial density correction value is set equal to the current d?  value, and the value of tx  is set equal to the current time t.

The framework of FC2 is as follows.

Algorithm FC2:     Input:   X: the data stream;  ?: fading factor; ?: density error bound; S: density threshold;  Output:  D: a list of potentially frequent data items; Begin  1. t=0; L=?-1; 2. While not terminate do 3.     Receive a data item x from the stream; 4.     If there is no entry corresponding to x in D then 5.        Create a new entry ],,,1,[ ttdx ? ; 6.        If  |D| ? L  (D is full) then 7.             delete entry ],','),','(,'[ 'xss ttdxtxDx  where ')','( sttstxD ??  is the smallest;  8.          replace the record ],[ dtd?  by  ],).','(.[ ' ttxDd sd tts tt ?? +? ?? ;  9.        end if 10.        Insert new entry ],,,1,[ ttdx ? in D; 11.     Else 12.        replace the entry [x, D(x, ts), dx, ts, tx] by ],,,1),(,[ x  tt s ttdxtxDx s +  ?? ; 13.     End if 14.     t=t+1; 15. End while  End When an entry x in D is deleted, line 8 in FC2 modifies  the value of d? by adding ')','( sttstxD ??  which is the  deleted density value of x, instead of subtracting it from the densities if all the other entries. At time t, the real value of  d?  is just dttd ?? ? , the real deleted value of x is xd tttt dxd ?? ?? ?? .. . Therefore, the actual relative density  value of x is )(),( xds tttttts dxdtxD ??? ??? ??? .  And its  complete density is sttstxD ??),( .

To answer a query request at any time t, only the frequent data entries  [x,D(x,ts),Da(x,ts),ts] satisfying sttstxD  ??),( >  ? ?  ? ?  S   can be output. It is obvious that all items whose  complete density exceeds ??1  S  must be output and no item  whose complete density is less than ? ?  ? ?  S  is output.  By  Theorem 1 we know that the estimated densities of the output entries are less than the complete densities by at most  ? ? ?1  .

B. Complexity of the Algorithm FC2 Since the maximum size of queue D is L=?-1, FC2 also  requires no more than O(?-1) space. FC2 does not update all the entries at each time, but uses d?  to save the value which should be subtracted from each entry. Also the algorithm does not multiply the densities by fading factor?  for all the entries in D at each time step, instead it updates the density of a data item using ts only when an identical new data item is received from the data stream. To accelerate the process of updating D, it is stored in a hash table using a hash function H, namely for a data x, its address is H(x). Once a new data item x is received from the stream, the algorithm can find its entry in D in O(1) time.

Therefore, time complexity for the algorithm to process one data item is O(1). Since the maximum size of list D is L=?-1, it is obvious that computation time for answering each query is O(?-1).



V. EXPERIMENTAL RESULTS We evaluate the quality and efficiency of FC2 through  extensive experiments on the synthetic data sets and the real data sets compared with ?-LC which is revised version of LC algorithm by adding time fading model on it. We focus on the algorithms' computation time, recall and precision in handling the data streams.

All experiments are carried out on 1.7GHz processor and 512 MB memory running Windows XP. In our experiments, we set ?=0.1S, ?=0.99.

A. The Synthetic Data Sets We randomly generate four datasets with Zipf  distribution. The parameters are respectively 0.5, 0.75, 1, 1.25. The size of each dataset is 1000K. We set ?=0.0005 or 0.001 and compare the space, time costs, recall and the precision of FC2 with ?-LC algorithm.

Fig. 1 shows the comparisons of the memory requirement of the two algorithms on datasets with ?=0.0005. Since the space cost of FC2 is O(?-1), the memory requirement of FC2 is a constant when ? is fixed. But in ?- LC, its memory requirement is ( ))log(1 nO ?? ? , where n is the length of the data received from the stream. From Fig. 1, we can see our algorithm requires less memory than ?-LC.

The time costs of the two algorithms with different ? are shown in Fig. 2 and Fig. 3. It is obvious that FC2 is much faster than ?-LC. The recalls of the two algorithms are both 100% which empirically prove that the outputs of the algorithms have no false negatives. Fig. 4 shows the comparison of the precisions of the results by the two algorithms. It can be seen from Fig. 4 that the precision of FC2 is higher than that of ?-LC at each step.

B. The Real Data Sets In this section, we adopt the log file data of visiting the  1998 Word Cup official web as the real data set[16]. This log file records all of the visit requests for the word cup official web in the period of competition of 1998 Word Cup.

Each request consists of 8 attributions including visit time, IP address, the ID of the visit web and so on. We pick up all of the IDs of the visit webs as the experimental dataset and find out the most frequent visit webs. Fig. 5 shows the comparisons of the memory requirement of the two algorithms on dataset with ?=0.0005. From Fig. 5 we can see our algorithm requires less memory than ?-LC.

0.5 0.75 1 1.25  Zipf parameter  M e m o r y ( n u m b e r o f  e n t r y )  ?-LC FC2   Figure 1. Comparison of the memory requirement of the two algorithms  on different distribution data(1000K, ?=0.0005)           0.5 0.75 1 1.25  Zipf parameter  T i m e  c o s t( m s / K )  ?-LC FC2   Figure 2. Comparison of time cost of the two algorithms on different  distribution data(1000K, ?=0.0005)  The time costs of the two algorithms with different ? are shown in Fig. 6 and 7. It is obvious that FC2 is much faster than ?-LC.

The experimental results also show that the recalls of the two algorithms are both 100%.  Furthermore, we also show the results of the recall and the precision of FC2 on different sizes of World Cup 98 datasets in Fig. 8. From the figure we can see that FC2 could efficiently discover the frequent items with high quality.



VI. CONCLUSIONS In this paper, we present two algorithms for computing  frequency counts based on the time fading model.

Traditional stream mining algorithms, such as Lossy Counting (LC), were generally designed to handle all data items in the streams with equal weights. To emphasis the importance of the more recent data items, a fading factor ? is used. FC1 can detect?-approximate frequent items of a data stream using O(?-1) memory space and the processing time for each data item is O(?-1). The improved algorithm FC2 limits the processing time for each data item in O(1).

Experimental results show our methods have high precision, require less memory and consume less computation time than other similar methods.

