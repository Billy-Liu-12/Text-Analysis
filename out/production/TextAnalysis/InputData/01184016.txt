Implementation of a Least Fixpoint Operator for Fast Mining of  Relational Databases*

Abstract  Recent resenrch has focused on computing large item sets for association d e  mining using SQL3 least Erpoint computa- tion, and by exploiting the monotonic nature of the SQL3 aggregate functions such as sum and create view recursive constructs. Such approaches allow us to view mining as an ad hoc querying ezercise and treat the eficiency issue as on optimization pmblem. In this paper, we present a recursive implementation of a recently proposed least Erpoint operator for computing large item sets from object-relational databases.

We present experimental evidence to show that our implemen- tation compares well with seweml well-regarded and contempo.

mry algorithms for large item set genemtion.

1 Introduction  the recursion. Two approaches were proposed. In [3, 41, we have investigated computation of large item sets using a computationally expensive recursive join computation. But it was shown that unlike apriori like approaches, it can be made to compute only non-redundant large item sets. In [5], an apriori like fixpoint computation was also proposed.

This approach required the development of a Tp like operator usually found in deductive database literature. The idea for such an operator was captured in the following representative SQL3 view definition for large item sets called the ILtable.

In the following SQL3 expressions t.table(TranID, Items) is supplied as the input transaction table where TranID is the transaction ID and Items is a set of items (readers may refer to [5] for a complete discussion on these expresions and the fixpoint computation approach).

create view f-table as iselect Items. countiltems\lm as Suooort  I, ..

from 1-table group by create sequence seq increment by create view recursive lztoble as ((select Items, sum(Suppor1) as Support from (flatten(se1ect sub(ltems, {}, I) as Items, Support  erouD bv Items  The importance and need for integrating data mining with relational databases have been addressed in several articles such as [7, 91. The authors convincingly argue that without such integration, data mining technology may not find itself in a viable position in the years to come. To be a successful and feasible tool for the analysis of business data in relational databases, such technology must be made available as part of database query engines in a declarative way.

Start with 1;  from f-table))  Declarative computation of association rules were iuvesti- gated in works such as (6, 8, 91. Meo et al. [6] proposes a new SQL like declarative query language for association rule mining which ap ears to be heavily oriented towards trans- action databasff . In their extended language, they blend a rule mine operator with SQL and other additional features.

The series of research reported in [8, 91 spear headed by IBM researchers mostly addressed the mining issue itself. They attempted to compute the large item sets by generating can- didate sets testing for their admissibility based on their MC model, combination, and GatherJoin operators. Essentially,  .these works represent a faithful encoding of apriori like algo- -rithms using SQL, and like [6], become too specific.

In a series of recent research 13, 4, 51, we have shown that association NI= can be easily computed as ad hoc SQL3  , queries without requiring any specialized relational operators or language Constructs. SQL3's recursive view construct was particularly useful in developing the least fixpoint queries that involved the monotonic aggregate function s u m  inside  * Research supported in pan by National Science Foundation grants EPS-0082979 and EPS-0132618.

'It is important to note that association ruler may be computed for virtually my type of database, transaction or not.

P  0-7695-1754-4102 $17.00 Q 2002 IEEE 633  - . I  having sum(Support) 2 6,) union all (select t.ltems, sum(t.Support) as Support from f-table as U,  (flatten distinct(se1ect sub(f.ltems, Lltems, i.Degree)  from f-table as f, Ltable as 1, as Items  (select Seq.Neztwal as Degree from iteration) as  i,

I.ltems c f.ltems)) as t where siseof(l.ltems) = i.Degree - I and  where t.ltems E u.ltems group by t.ltems having sum(t.Support) 2 6,)); What is interesting about these two approaches, specially  the approach in (3, 41, is that unlike those in [6, 8, 91, only traditional SQL3 coustructs were used and, in particular, no mining specific features were needed. And as such these approaches offer excellent opportunities for query optimization. Even the expressions above (and in (51) use only one user defined function s u b  which implements the selection of a set with a specific cardinality and membership from a power set of a given set of elements.

http://jamilQcs.msstate.edu   Initial experiments show that the SQLS apriori approach just outlined outperforms the recursive join approach in [3, 41 as the farmer incorporates the stronger apriori pruning heuristics. In 151, the following recursive relational operator Q was also proposed to abstract the apriori type hpoint encoding above.

Definition 1.1 (Fixpoint Operator)  Let I t  and Su be two column names in a transaction database V with corre- sponding item set and support type domains, 6, be the min- imum support threshold, and n be the maximum cardinality of the item sets in I t  of ?D. Then the large item sets operator e is defined as follows:  e:t,su(D) = ~S,>S,(I~~S,=,,,(S,,(~:~(~)))  e?t.s.(D) = e;&(v) U (0se6,( I:~s?=.?,(s?,((~;t(~D)) w? &&(V))))  The large item set operator Q is defined in terms of two other operators <* (called s t e p )  and W k  (called distance-k item set join) defined as follows. Let 1 be the set of all item and + be a total ordering on the labels of the items in Z. Also let k-sub be a function that for any given item set s Z and k such that 15 k 5 IsI, it returns all distance-k subsets of s.

For any two sets S and s, s is said to be a degme-k subset of S i f s  E P ( S )  and Is1 = k. Additionally, s is called a distancek subset of S if k = (IS1 - 1.1). Using this function we define the degree-k subsets of an item set relation as follows.

Definition 1.2 Let T be a relation such that its scheme includes a set valued attribute Items. Then for any k 2 0, the degree-k subset of r is defined as  f : tems(T)  = { t  I %,v(u E T A  v E k-sub(u[ltems]) A t = (U, u[R \ Items]))}  For any two Sets of item sets SI and s2 in 9, SI is said to be a strict distance-k superset of sz if sz c SI. Is,] - Is21 = k, and V u , v ( u  E SI A v E s2 A D  # U + v < U). Intuitively, st is an increasing superset of sz. For example, abc is a strict distance1 superset of ab, but not ac, when we consider the ordering a < b + c < . . .. Let 5r be a binary Boolean operator that returns true if SI is a strict distance4 superset of SZ, i.e., s15ksz = true. Finally, we define a distance-k item set join, w*, operator as follows.

Definition 1.3 Let T and s be two relations on scheme (Items, Support). Then the distance& item set join of T and 8 is defined as  r w!be,~ = f l - . l tmv.supso-t(T Wr,ltema,ka.ltcma - S )  Notice that T W k  s returns all tuples in r such that there is a tuple in s that has an item set for which the item set in tuple of r is a distance-k superset. Other tuples are not selected.

There is a more subtle issue here. By defining and using the notion of strict distance-k supersets, we have practically facilitated a ?beamed? join of item sets. That is, item sets will join with only another item set that has an increasing cardinality and order. This is largely due to  the ordering relation we insisted upon the item sets in 1 as a technical requirement.

Intuitively, the item set join based on strict distance-k supersets works as follows. Consider an input transaction table t-table and the corresponding large item set table Ltable as shown below for a support threshold 6, : 0.25.

,le Support __  .29  .71  .43  .29  .43  .29  .29  .43  .29  .29 - large i t em set tab  Notice that, in this example, when a is a large item set (in table t-table), we need to generate the candidates ab  and ac as b and e are also large. Also because there are database entries that contain ab  and ac. This suggests that we need to consult the large item sets and the database entries to generate these candidates. In this way, we will not generate ad or a f ~ for example. Again, when we consider b as a large item set, we may want to consider generating ab one more time because it is a possibility. But it is not necessary as we have already created ab  as part of processing a.  The question that remains, however, is how do we implement this ?memory? in a set based setup such as relational algebra or SQL? One way to accomplish this is to apply SQL?s distinct feature and make them unique, but this is an inherently expensive and wasteful operation. The strict distance-k supersets based item set join we have proposed help capture the idea by mimicking the affinity of the item sets (large or candidate) as shown in figure 1 during join processing in a set based setting.

Figure 1: beamed candidate generation path.

Exploration tree of input t-table showing  In the remainder of the paper, it is our goal to present a procedural implementation of Q and compare its performance with two leading approaches - FP-tree [2] and apriori [l].

We proceed as follows. In section 2 we present a recursive implementation of the operator followed by a comparative analysis in section 3, and conclusion in section 4.

2 Implementation of the Fixpoint Operator  In this section, we present a simple algorithm that implements the least fix point operator Q with the hope of demonstrating that even this primitive implementation performs well and is     comparable to leading algorithms and approaches in its class available in the literature (such as 19, 8, 61).

2.1 A Depth-First Recursive Algorithm The algorithm we are about to  present exploits the so called "beamed" join technique for W k  operator that not only facilitates faster join, but also allows us to explore the item set lattice (discussed in [3]) in a depth first manner (in fact it reduces to a forest of trees). Thus we trade time for space to avoid memory swaps. This also allows us to explore the forest one tree at a time. Let us first explain the basic idea of the algorithm using the example tables t-table and ILtable, and a support threshold 6, 2 0.25. The large item set operator  necessitates that a set of distinct operations need to be performed in a sequence to compute the large item sets. First, we need to compute the large one item sets (the exit rule).

We do so by scanning the database once and creating the list shown in figure 2. In this list, for every one item set, we list the record pointers to which an item set belongs. Notice that e is not a large item set since its support is less than 6, (marked with a box around the node), and thus is taken out of the list L1 (layer one list).

Figure 2: Generation of Litem sets.

Then we take one node at a time in the list LI and explore its subtree in a depth first manner. Note that a node created in such a fashion will never be created in another branch in the forest and hence, there is no duplication of node expansion.

To create the next level of a node we proceed as follows. We scan the tid list of the node X and fetch the item sets one at a time, We generate a new node with id Y only if the node id Y is larger than the current node id X in the < relation, and it has not been created already. In the figure 3 below, we expand the tree for node a, followed by the tree for bin figure 4. Note that when we are at level 2 (figure 3), we discard node e, but continue to  keep node b when we are exploring c.

We delete both band c when we exit from a, but keep a until we move below a  B Figure 3: Exploring the a-tree - depth first expansion, shown from left to right.

The generation of next level nodes the way we did by checking for the satisfaction of the order relation < among  the items, implements W' and E L  when we are at level k.  The grouping is achieved by structuring the nodes in a tree, as we did, that always pushes an item set in one single branch of a tree. The summing of the support is done hy counting the tids in the tid list and then checking if the node met the minimum threshold. If it did, then we expand it further; if not, we mark it dead (marked with a box around a node) and return to the parent node. If the node is large, we output the sequence from the root to  the node as a large item set with its support count. The algorithm we have just described is presented in figure 5.

T Figure 4: Exploration of the  btree.

3 Performance Comparison In this section, we present experimental evaluations of our framework as proof of its viability as a computing paradigm for association rules in relational databases. In our performance study, we compared our algorithm with apriori and FP-tree, two leading competitors. We have also implemented apriori and FP-tree ourselves to be fair, by eliminating the experience factor of the programmer in the comparison.

We developed our algorithm in C++ on a PC with AMD Athlon Processor 1.2G processor running Windows NT with 196 MB memory, 764 MB bytes virtual memory and lOGB hard drive. We have used the IBM data sets T25110DlOK and T25120D100K for our evaluation. These synthetic datasets were extensively used almost as a benchmark to compare relative performance of most association rule mining algorithms. These data sets feature long patterns even for very small thresholds such as 0.1% and as such algorithms must explore almost all possible combinations of item sets.

Also, these data sets intersect with each other (overlap) so heavily that the possible set of combinations is truly huge.

The graphs in figure 6 show the total execution times of our algorithm on these data sets and confirm that our algorithm performs better than apriori and almost approximates FP- tree. But so do many other algorithms. The real question is, does our algorithm perform better than other algorithms in its class - that is, those which also take an approach based on relational computation of large item sets? We would like to answer these questions in two steps.

First, we think our approach can only be compared with works such as [SI. Unfortunately, we could not run a comparative analysis using their data sets. But from the published literature, their method appears to be more expensive than our algorithm. Their running times are listed in excess of 10,000 seconds in some cases while ours are expected to be lower. As can be seen, even for the large T25120D100K data set, our total running time is less than     algorithm: forest.expension; input: A transaction table D, support threshold 6,.

output:  Large item sets Lz,  (ILtable).

begin  scan 'D and generate a large 1-item set list L I  while L I  is not empty do with a tid list for every item in which they appear;  call the node pointed by L I  X ; call procedure node-expansion; remove node X from LI  endwhile; end.

algorithm: node-expansion; input: A node X and item list in the path I ; output: Void.

begin  let the item in node X he U; count the tid list T; if the count is greater than 6, then  output large item set I Uu and its count; for every tid in the tid list T do  fetch the item set s at T from D; for every item i in s do  if U < i and there is no child node Y at  create a node Y with item i; copy the ti& in Y from T in which  call procedure node-expansion;  X with item i then  I U U U i appears;  else  return; mark X dead;  end  Figure 5: Algorithms for the large item set operator.

8,000 seconds for 0.1% support threshold (note, however, that the hardware used is different).

Secondly, we believe that novel indexing techniques will help improve the performance of our algorithm significantly.

We are currently developing an indexing scheme called the S+ trees for set valued domains that aids fast set operations such as c, E, 3, > , = , \ , n a n d  U. Recall that W' require such a join based on strict distance-k superset relationships. Our hope is that such an indexing scheme will help us locate specific portions of the database that is relevant for the computation at hand thereby increasing performance.

4 Conclusions It was our goal to demonstrate that association rules can be computed using existing SQL3 machineries, which we believe we have done successfully. We have, of course, used a couple of built-in functions for set operations that current SQL systems do not possibly support, but we believe that future enhancements of SQL will. These functions can be easily implemented using SQL's create function statements as we have done. We have demonstrated that SQL's create view  Figure 6: Total execution time of fixpoint on the data sets T25110DlOK and T25120D100K as compared to apriori [l] and FP-tree [2].

recursive clause can simulate apriori effortlessly once the idea of a least fixpoint operator for apriori was at hand.

As a second step, we have also attempted to support our conviction with an implementation of the large item set operator e that we have proposed. The initial performance results show that our algorithm does better than apriori and possibly better than [9]. We believe that new techniques for computing W k  based on set indexing would be useful and efficient. In this connection, we are current1 investigating query optimization issues involving e and W operators in relational algebra expressions.

