Intuitive Topic Discovery by Incorporating Word-Pair?s Connection into LDA

Abstract?We demonstrate a generative model that incorporates word-pair connection into the smoothed LDA model to intuitively discover people?s wish related activities.

The widely used model, LDA topic model, generally generates clusters in the form of separate words. However, this form is not intuitive enough to express people?s activities. Therefore, we consider the word-pairs led by verbs can better describe users? intentions and activities, and we prefer to present this collocation under topics as the clustering results. We mathematically present the relatedness between verbs and non-verb words through association rule, and build the physical connection of word-pairs and possible topics. By incorporating the connection lattice into the smoothed LDA, the word-pair LDA model is created. In the experiments, Twitter posts about ?new year?s resolutions? were chosen as the data source. The results show that the proposed model performs well on perplexity, and presents excellent intuitive character.

Keywords- intuitive expressions, connection lattice, twitter posts, LDA model, association rules

I. INTRODUCTION In the field of natural language processing, there is a  considerable interest in topic clustering to discover people?s real world activities [6,7,8,9,10,11]. Many online services, such as personalized recommendations and web searches, strongly benefit from explicit arrangement of categories of users? intentions or activities [21,22,23]. Generally, in the clusters generated by the previous topic models, most the items are separate words, especially nouns. However, the question that whether such expression is suitable for representing people?s intentions and activities hadn?t been discussed before in detail.

Everyday experience suggests that effective language communication about people?s intentions and activities generally requires the alliance of verbs and the objects which are usually nouns. For example, the word ?food? doesn?t  intelligibly express what a person?s intention or activity is, since it can be ?eat food?, ?cook food? or any other similar acivity, while the form of a verb plus a noun, ?eat food? or ?cook food?, delivers more accurate information, and it can intuitively represent people?s intentions or activities. Besides the combination of verbs and nouns, there are some other collocations that may also play important roles in communication, such as a verb plus an adjective.

Therefore, using such word-pairs as cluster items is supposed to intuitively express people?s intentions or activities, and give us a clearer picture of corresponding subjects in topic discovery. In case some functional expressions miss, we choose the combinations of verbs and non-verb words as the collocations for clustering.

The standard LDA(Latent Dirichlet Allocation) [1], as well as its many derivative versions, is one of the most widely employed topic models due to its superior features such as being regularized and directly dealing with topic probabilities for each word. Most models assume that the corpus is generated by separate words. However, for our purpose of generating intuitive results, this assumption causes the problem that the words, which are supposed to be set as collocations for intuitively representing the subjects, might be assigned to different topics. Even the ones that consider the relatedness of words only increase the chances that the words frequently co-occurring share the same topics, but they cannot deal with fixed part of speech collocations.

David Andrzejewski et al. used a Dirichlet Forest prior to present words links associated with domain knowledge [2], and they assumed the linked words appear with a similarly high probability in the same topics; Hanna M. Wallach [3] and Xuerui Wang et al. [4] considered textual order and built n-gram topic models.

Therefore, we propose a model that can build ?hard link?, that is, the words in collocation must be assigned with the same latent topics, and we call it word-pair LDA model (wpLDA). By part of speech (POS) tagging in advance, it   DOI 10.1109/WI-IAT.2012.205     can assign topics for any desired collocations, but in this paper, we just focus on the intuitive expressions of people?s intentions and activities, which are the word-pairs composed of verbs and non-verb words. The proposed model allows us to reasonably combine verbs and non-verb words by incorporating the word-pair connection lattice into the LDA model, and generate these word-pairs under topics.

The rest of this paper is organized as follows. In Section , we briefly introduce related work about LDA variants,  especially those in which the relatedness among words or documents was considered; in Section , we review the LDA model which is the basis of the proposed wpLDA, explore the appropriate collocation for expressing people?s intentions and activities, and build the structural model for such connection in corpus; in Section , we give the details of the proposed topic model as well as its inferences; in Section , the results of the experiments conducted on two datasets are analyzed to validate the capabilities of our model in clustering word-pairs, and to evaluate the performance of verb-non-verb collocation in representing people?s intentions and activities; in the final section, we conclude the work we?ve done and discuss possible directions for future research.



II. RELATED WORK Since it was raised in 2003, great importance has been  attached to the LDA generative probabilistic model. Then, Thomas L. Griffiths and Mark Steyvers improved the algorithm by using Collapsed Gibbs Sampling [12] to skip direct estimation of the topic distribution for documents and the word distribution for topics, focusing on the posterior distribution over the assignments of words to topics instead.

This modified LDA model has found widespread use as a basic framework for developing various derivatives to satisfy different information needs.

Amr Ahmed et al. built a hierarchical Bayesian model for delivering news [13]. This scalable probabilistic model assumed a distinction between topics and story lines whose strength was built as a non-parametric model over time. To extend the LDA model to online applications, AlSumait et al.

[14] proposed an online LDA model using the empirical Bayes method. This online model is able to be incrementally updated according to the new stream of data without referring to the previous. Therefore, it is capable of efficiently tracking topics and detecting the emerging topics in real time.

Some interest is being shown in incorporating the relatedness between words or documents to enhance the clustering effects of LDA model. Domain knowledge is the relatedness that is generally extracted from outside sources instead of the corpus itself. Zhu et al. used the external documents as the domain knowledge source on which the LDA parameters can be estimated [15]. As for the relatedness of documents, citation between two documents is considered as a rich feather that indicates topical similarity [16,17,18,19], and even the authoritativeness of the cited documents [20]. Andrzejewski et al. used the Must-Link and Cannot-Link to denote words? relatedness [2], and built a  general model that could be applied into various relational structures. The part of Must-Link in their model seems to satisfy our purpose of building words? relatedness, however, their assumption that the linked words appear with a similarly high probability in the same topics is not exactly what we need, and a word may be associated with any other word, instead of their ?intuitive expression partners?.



III. FRAMEWORK ELEMENTS OF MODEL  A. LDA topic model The LDA model assumes that documents are generated  as random mixtures over latent topics, while each topic is characterized by a distribution over words. The graphical model of LDA is given in Figure 1, and the associated variables are illustrated in Table .

Using the Collapsed Gibbs Sampling, we can develop the following recursion equation of topic probability for each word in the corpus[12]:  ???(?,?)???(?,?), 	; , ?? = ??(?,?),??(?,?),?;?,?????(?,?),?;?,??  ? (??,(?)?,?(?,?) + ?) ?(?),??,?  (?,?)??? ? (?(?),??,?  (?,?)???)??? , (1)  where ?(?,?) denotes the topic of the ?!" word token in the #!" document, ??(?,?) denotes the topics for all the words in the corpus except the ?(?,?).

Figure 1. Plate notation of the smoothed LDA  TABLE I. DEFINITION OF VARIABLES IN THE MODEL  Variables Meaning $ Number of topics %& Number of words in a document ' * Number of documents  Parameter of the Dirichlet prior on - ? Parameter of the Dirichlet prior on .

- Per-document topic probability distribution . Per-topic word probability distribution ? Topic for a word  ??,(?)?,?(?,?) Number of words assigned to the /!" topic in the #!" document, except the ?!" word  ?(?),1?,?(?,?)/?(?),2?,?(?,?) Number of word 3 (or word 4 ) assigned to the /!" topic     B. Word-pairs Generally, a verb and an object can clearly describe  people?s intentions or activities. The object is usually denoted as a noun, which can be a person, a place or a thing, while the function of verb is to make the action expressed understandably. One of the advantages of combining these two kinds of words as the expression is that this combination is more understandable than using any one of them on its own[22,23,24]. Therefore, it is reasonable to set verb-noun pair as a candidate of the collocations. However, there are some other words that may also play an important role in identifying the messages conveyed, especially adjectives. For example, if a person?s wish is to ?eat healthier food?, then we can more correctly identify his intention through the verb-adjective pair ?eat healthier? than the verb-noun pair ?eat food?. Therefore, we treat a verb and a non-verb word as the form of collocations for constrained clustering, and encourage the words in the collocations to be assigned to the same topics.

Therefore, the relatedness between verbs and non-verb words in the vocabulary needs to be explored. Considering the association rules [5] are the straightforward ways to do that, we choose the concept of ?confidence? to measure this connection. Let 5677(8) be the support of set 8, then the confidence of the word-pair (9:, 9<)  is  >??@(9:, 9<) = ABCC(DE?DG)ABCC(DE) .

Then, we use the smoothed confidence H:,< as the weight to measure the relatedness between the verb 9: and the non- verb word 9<:  H:,< = ABCC?DE?DG??IABCC(DE)?I?K  ,                           (2)  where L is the smoothing coefficient.

Figure 2. Word connection  In the LDA model, the probability of words assigned to each topic is Dirichlet distributed. In our research, this assumption has been perpetuated, but with the incorporation of word connections, the topic-specific distribution of word- pairs can be represented by Figure 3, in which %1, %? and %C are the numbers of verbs, non-verbs and pairs in the vocabulary, respectively.

According to Figure 3, when the word distribution for topic /  is .? , we assume the probability that a word-pair (9:, 9<) assigned to the topic / is  M?(E,G) = N ? .?,: ? H:,< ? .?,< ,                    (3)  where  N is the normalization constant.

k  Topic  (a)  (b) Figure 3. (a) Word-pairs distribution (b) Word-pair distribution represented  by the connection lattice  It could be understood in a sensible way that the word- pair deserves to have high probability within a topic if both of words in the pair are likely to belong to this topic and they are intensely related. And we treat all these three factors, .?,: , H:,<  and .?,< , as contributing to the weight M?(E,G) equally.



IV. WORD-PAIR LDA MODEL Considering the importance of word-pair connection, we  prefer to assume that the documents in the corpus are generated by word-pairs, instead of separate words. The proposed model is built on the basis of LDA model, and we call it the word-pair LDA model (wpLDA). The generation factor graph of the proposed model is shown in Figure 4, and     the generative process for each document O is similar to the standard LDA: 1. Choose P<~QO4( ), where R ? {1, ? , *} and QO4( )  is  the Dirichlet distribution for parameter ; 2. Choose .?~QO4(?), where / ? {1, ? , $} and QO4(?) is  the Dirichlet distribution for parameter ?; 3. For each of word-pair 74<,! , where the word-pair token  V ? {1, ? , %&}, and %&  is the number of word-pairs in document R:  a. Choose a topic ?<,!~*6WVO??#OXW(P<); b. Create the connect lattice and calculate M?G,Y  by (3); c. Choose a word-pair 74<,!~*6WVO??#OXW(M?G,Y).

Figure 4.  Plate notation for word-pairs generation  According to the model, the total probability of generating the corpus is  ?(?Z, ?, -, M, .; , ?, H) = ? ?(.:; ?)?(M:|.:; H)\:]^ ? ??P<; ?K<]^  ? ? ?(?<,!|P<)?(74<,!|M?G,Y)_!]^  ,                           (4) where ?Z denotes all the word-pairs in the corpus.

Equation (2) suggests that if the corpus is chosen, the relatedness of words in this corpus is fixed. Therefore, we can know that M totally depends on . by (3), and thus, we have ?(M:|.:; H) = 1.

Then, by integrating out both . and -, (4) is written as follows:  ?(?Z, ?; , ?, H)  = ? a(? ?EbE? )? a(?E)bE? ? a(?G,(?)E ??E)bE?  a(? (?G,(?)E ??E))bE? K<]^  ? ? a(? ????? )? a(??)??? ? a(?(?),?E ???)???  a(? (?(?),?E ???))??? \:]^ ? H:,2c  ?(?),?c Ede  2c]^ .  (10)  where f and  fC are the numbers of words and word-pairs in the vocabulary.

Let the pair symbol in the pair vocabulary of the ngh pair in document m be 3?, and it corresponds to the 3!?" word and the 3j!"  word in the vocabulary. By the Collapsed Gibbs Sampling, the recursion formula is obtained:  ???(?,?) = /???(?,?), ?Z; , ?, H? ? ???,(?)?,?(?,?) + ?? ?  k?(?),? ?,?(?,?)??? lk?(?),?o  ?,?(?,?)???ol ? k?(?),??,?  (?,?)???l??? k? k?(?),??,? (?,?)???l??? ?^l    ? H?,(1 ,1o) ?(?),?c?,?  (?,?) ,                                                                (15)  in which ??,(?)?,?(?,?) is the number of words assigned to the topic / in the document #, except the words in pair 3?. The symbol ?(#, ?) indicates all qualified words, except the words in the ?!" pair in document #.

The conditional probability for ?(?,?)  has similar structure with the smoothed LDA, but we need to treat verbs and other words separately. Compared to the recursion formula of the LDA, the additional data we need to apply (15) are the numbers of times that the two words in the word- pair are assigned to the topic, as well as the number of times the topic occurs in the document to which this word-pair belongs.

We do the sampling for each word-pair in two steps: 1. Sample the verb of this pair. We calculate the  relatedness parameters between this verb and all the non- verbs; for each topic class, we use (15) to work out the probabilities of all the pairs this verb involves; sample the verb;  2. Sample the non-verb word of this pair. We calculate the relatedness parameter between this non-verb word and all the verbs; for each topic class, we use (15) to work out the probabilities of all the pairs this non-verb involves; sample non-verb word.



V. EXPERIMENTS  A. Datasets and Tasks In the past few years, social networks like Twitter have  been experiencing explosive growth, and they become hotbeds for generating various information. By exploring this naturally occurring data, we could discover abundant information, especially about people?s intentions and activities.

In this paper, we focus on twitter posts, and try to explore people?s intentions through this online data source. Though informal, Twitter has characteristics for knowledge exploration: open to the public, abbreviated, easy to analyze and generated constantly. Considering many people posted their New Year?s resolutions on Twitter during the New Year?s period, we use the tweets related to New Year?s resolutions to discover people?s wish related activities.

We collected 1488 pieces of tweets about ?new year?s long-term goal?, with duplicates eliminated. By using Helmut Schmid?s ?TreeTagger? tool to conduct the POS tagging (part-of-speech tagging), we separated verbs from other words, and used this tagged data as the input of the     wpLDA model. For example, if the tweet is ?eat more vegetables and to read the news more frequently?, then the tagged data is: eat (VV), more (JJR), vegetables (NNS), and (CC), to (TO), read (VV), news (NN), more (RBR), frequently (RB). The taggers are described in [24,25]. In the experiments, we use  L = 1 in (2) to calculate the relatedness between two words.

B. Sensitivity Analysis To evaluate the sensitivity of the proposed model, we  varied the number of topics from 5 to 50, and used the perplexity as the evaluation criterion.

In this paper, the perplexity of the resolution corpus is defined as follows:  7p47WpqOVr = pq7 s? ? tuv(C2w)xw? ? _wxw? y  where ?(74&) is the generative probability of all the word- pairs in the document '.

Figure 5 presents how the perplexity changes over the number of topics. We can see that the value of perplexity decreases by the increasing topic number at the opening phase, indicating a higher value for the number of topics benefits clustering by higher likelihood. But around the point of  %?. ?@ V?7O>5 = 20, the perplexity begins to converge.

Therefore, we choose 20 as the value of the number of topics, since the perplexity in this point is near to the minimum.

Figure 6 offers the picture about the effect of the iterations on the perplexity when the number of topics was set on 20. As expected, the perplexity decays at the first phase, but with an easing decline. And after about 15 iterations, the perplexity enters a stable state, suggesting that the algorithm converges after about 15 iterations? operation.

C. Performances The proposed model has a higher value of perplexity than  the standard LDA does, which means the words under the same topics share less likelihood. This is due to the scheme of the generative process, in which we assume that the documents are generated by word-pairs instead of separate words. It would definitely lead to the problem that the two words in a pair may be more likely to belong to different topics. Therefore, the compactness of words under each topic would not be as good as the case of word-by-word generative models, causing a higher value of perplexity.

Therefore, we built a baseline model as the control group to figure out how the relatedness affects the clustering of word-pairs. The baseline model differs from the wpLDA model in two ways: H:,<  takes the minimum of (2), which equals to 1/2* ; the constant value 1/f  is used as the probability of non-verbs? topics instead of (15), which is the minimum probability that  a word is assigned to a topic.

These settings have blunted the relatedness of non-verbs to the verbs in the baseline model.

5 10 15 20 25 30 40 50  iteration=20 iteration=15  Perplexity  No. of topics  Figure 5. Perplexity vs. number of topics        1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  No. of topics=20  Perplexity  Iterations  Figure 6. Perplexity vs. iterations  Figure 7 shows the comparison of the baseline and the wpLDA on perplexity. It illustrates that the baseline model has higher value of perplexity. This is because the baseline model treats the relatedness between words equally, and meanwhile it greatly weakens such relatedness, causing the situation that the two words sharing low likelihood have the same probability of being a pair as the closely related words.

Compared to the baseline, the wpLDA model has better performances on perplexity, and it indicates that it is reasonable to set the two words that tend to occur together as a pair, and they are far more likely to share the same topic.

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  Baseline wpLDA  Perplexity  Iterations  Figure 7. Perplexity of the baseline and the wpLDA (No. of topics=20)     To choose the best word-pairs in each topic, we used ?pair-weight? as the criterion to measure the compactness of pairs. The pair-weight under the topics is defined as follows:  7XO4 ? ?pO??V(9:, 9<) = >??@(9:, 9<) ? %DE ? %DG where 9:  is a verb,  9<  is a non-verb word, and %D  is the number of appearance of 9 in this class.

Table 2 and Table 3 are some samples selected by applying the criterion of pair-weight to the clustering results of the baseline model and the wpLDA model, and the pairs that have the highest pair-weight have been listed.

The baseline model ignores the relatedness between words, though it links the verbs and non-verb words together.

This ignorance causes some unexpected words to be set as pairs. For example, the pairs ?cut_gym? and ?beat_best? are meaningless, which are just possible combinations of verbs and nouns.

The proposed model demonstrates excellent intuitive character by considering the relatedness of words, and the form of word-pairs makes sense in expressing people?s intentions and activities. In addition, the results show that the average value of pair-weights in the classes of wpLDA is higher than the baseline?s by 3.2%.

TABLE II. WORD-PAIR CLASSES: THE BASELINE WORD-PAIR GENERATIVE MODEL (SNIPPETS)  Topic1 Topic2 Topic3 Topic4 cut_week study_week write_week start_week cut_days cut_gym cut_way cut_just cut_food study_food study_gym study_days study_just study_way write_gym  lose_diet lose_weight quit_weight quit_diet help_weight help_diet lose_smoking watch_diet watch_weight quit_smoking fit_weight fit_diet help_smoking lose_bit lose_book  stop_best stop_people stop_life stop_time stop_word stop_fat stop_fucking beat_best beat_people shit_people shit_don shit_best finish_people finish_best fit_best  like_person like_people like_phone like_best talk_person live_person love_person talk_best talk_phone talk_people think_person live_phone live_best look_person live_people  TABLE III. WORD-PAIR CLASSES: THE WORD-PAIR GENERATIVE MODEL (SNIPPETS)  Topic1 Topic2 Topic3 Topic4 love_things like_things love_person love_getting know_things live_things read_things work_things like_getting like_person start_things love_really love_lot love_just love_actually  exercise_diet start_diet eat_diet exercise_healthier exercise_life live_healthier live_life eat_healthier eat_life start_healthier start_life exercise_blog exercise_book exercise_possible play_healthier  quit_smoking quit_finally eat_good stop_good stop_smoking stop_finally watch_smoking watch_good watch_finally quit_person quit_lot eat_person start_good eat_lot start_smoking  spend_time spend_friends spend_god gain_time buy_time spend_actually spend_internet spend_food spend_reading spend_car gain_family buy_family learn_family gain_god learn_friends  D. Computational Cost Comparing to the LDA, the wpLDA model requires  some extra work to generate word-pair classes: calculate the relatedness of each word-pair; treat verbs and non-verbs as independent variables when counting the number of times the word is assigned to a topic and the number of times the topic occurs in a document.

For the calculation of relatedness, we need to consider each word in all the possible pairs in the vocabulary under different topics. If the word to be sampled is a verb, we should calculate the relatedness between it and all the non- verbs in the vocabulary, and the number of calculations about verbs in this document is %&,1 ? %?, where %&,1 is the number of verbs in the document ', and %? is the number of all the non-verb words in the vocabulary; if the word to be sampled is not a verb, we should calculate the relatedness between it and all the verbs, that is %&,? ? %1, where %&,? is the number of non-verbs in the document ', and %1  is the number of all the verbs in the vocabulary. Therefore, the total number of calculations for the whole corpus is $ ? * ? (%&,1 ? %? + %&,? ? %1). In addition, we also need to count ?(?),1 ?,?(?,?) and  ?(?),1o?,?(?,?), respectively, for each topic.



VI. CONCLUSIONS We have presented a word-pairs generative model to  intuitively discover people?s wish related activities. The word-pair, which is composed of a verb and a non-verb word, is supposed to provide more intuitive information than the separate words do. Therefore, we developed the wpLDA model to generate this intuitive expression in topics which could assist in our understanding of people?s intentions and activities.

To generate the word-pairs, we need to build the connection of the two words in pairs, and we use the association rule to discover this kind of relation. By combining the relatedness of words in pairs with the topic probability distributions of separate words, the topic-specific word-pair distribution can be denoted by the connection lattice which is the core part of generating word-pairs.

The LDA model is the most popular model used in the field of clustering, and from it various kinds of plug-in versions have been derived to accommodate new environments and new requirements of users. In this paper, we also choose the LDA model as the basis to build our word-pair generative model. We incorporate the connection lattice into the LDA model, and assume that the documents in the corpus are generated by word-pairs, instead of separate words, and thus, the topics assigned to the words in pairs are the same.

In the experiments, we used the dataset about people?s ?new year?s resolutions? as the corpus, in which data were retrieved from twitter posts and were POS tagged. The results show that the proposed model performs much better than the baseline model on perplexity, and presents more intuitive character to create a better understanding of people?s intentions and activities.

The main work and contributions we made in this paper are as follows:     1. Explored an appropriate form, a verb plus a non-verb word, to express people?s activities;  2. Built the relation between word-pairs and topics, which is represented as the connection lattice;  3. Incorporated the connection lattice into the standard LDA model;  4. Proposed a new topic model which could cluster corpus in the form of word-pairs.

In addition, the association rules we used to build words? relatedness is replaceable, and the word-pair could be any combination of words, which means the proposed model is a general model and could be used to support many other tasks of topic discovery.

Our future work will focus on contributions of various collocations in expressing people?s intentions and activities, optimization of pair selection, and the reduction of computational cost.

