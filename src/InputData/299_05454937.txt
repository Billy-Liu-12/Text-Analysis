Research on Time-validity and Incremental  Association Rules

Abstract?Association rule mining is an important research field in data mining. Current association-rule methods mainly depend on the support-confidence framework. The strategy is not quite effective in consideration of the time-sensitive factor and correlation problem between antecedent and consequent of rules.

To solve this problem, a novel association rules framework has been proposed: time-validity support and time-validity match.

First, we define a new match as the substitution of confidence for coping with the correlation problem between antecedence and consequence of rules. Moreover, by embedding the time-entropy factor into the new support-match framework, the time-sensitive can be solved. Finally, an example is given to prove the feasibility and superiority of the new method. On the basis of this, we propose a new incremental algorithm and the idea of performance. Experimental results and comparisons with traditional incremental method demonstrate the effectiveness of the proposed framework.

Keywords-data mining; association rules; time-validity support; time-validity match  I INTRODUCTION Association rule mining is an important research field in  data mining. R.Agrawal et al [1] first put forward mining association rules in transaction database and also presented the corresponding theory of support-confidence framework. From then on, many algorithms were developed for mining association rules and they were all based on this framework, including Apriori algorithm, AprioriHybrid algorithm, FP-tree algorithm and mining closed frequent itemsets algorithm [2-5] those were the main algorithms that based on efficiency improvement. They focus on reducing the amount of the generated candidate itemsets or the passes over the transaction database. The maintenance of association rules includes updating the association rules when adding new data or support is changing. The purpose to study on algorithms is to make full use of the existing mining results and reduce the passes over the transaction database. Rule correlation analysis obtains rules whose antecedent and consequent are highly correlated mainly by adding parameters such as interestingness, lift, match and using linear regression and reverse  authentication method. However, many mining processes only consider that the data sets contain the same value and don?t take account of the time factor. Actually, the value of data is changing over time. Therefore, in this paper we did some research on data?s time validity as well as the correlation of the generated rules. We combined the advantages described in [6] and proposed a new framework of time validity support and time validity match. By adopting this new framework, the generated rules can reflect the time validity of data. Besides, the antecedent and consequent of rules will have high correlations. Further more, the paper proposed the corresponding association rules mining algorithm based on time validity and increment. This incremental algorithm has effectively solved the rule maintenance problem based on the framework of time validity support and time validity match.

II FRAMEWORK OF TIME-VALIDITY SUPPORT AND TIME-VALIDITY MATCH(TSTM)  Mining association rules are aimed to find correlation among the different items in a database. Some relevant definitions about association rule are given in [1].

A Definitions of TSTM The classical definition of support can?t reflect the time  factor of data. Therefore, the paper defines time-validity support based on [7]. According to the statistical significance of time entropy, after effect of events is decreasing exponentially as time interval is increasing. Consider there are n observation points (T0, T1, T2, T3?) on time axis whose time interval are same or close.  T0 is the observing point which is called the benchmark observation point. The interval time between occurrence time of transaction I Th and occurrence time of transaction T0 is ?T=?Th-T0?. The information value Vh transaction I contains is associated with ?T. The smaller the time interval ?T is, the greater the information value Vh is.

Conversely, the greater the time interval is, the smaller the information value Vh is. Assume when ?T=0, information value on the benchmark observation point is I, the decision-making value Vh a transaction contains declines as  This work was supported in part by the National Natural Science Foundation of China under Grant No.J07240003,  60773084, 60603023 and the National Research Foundation for the Doctoral Program of Higher Education of China  under Grant No.20070151009)      index )0( >?? ?? Te . So according to transaction I with the occurrence time Th, the information value it contains is Vh= 0hT Te ?? ? .

Assume there are m transactions {I0,I1, I2? Im-1} that all support rule (A), and their respective occurrence time are {T0,T1,T2,?,Tm-1}. For each transaction, we compute the product between information value Vi and the corresponding support Si. Then the products? sum divides the sum of m information value Vi. The obtained result is called time-validity support of rule (A). That is to say  i 0 i 0  1 1 1 m-1 - | | - | |1 1  i 0 0 0 i=0  ( ) ( ) ( ) ( e ) ( s e ) m m m  T T T T  i i i i i i  TSRule A v s v ? ? ? ? ?  ? ?? ?  = = =  = ? = ?? ? ? ?       (1)  Based on time-validity support, we absorb the advantages presented in [6] and propose a new method to generate rules.

For each transaction, we compute the product of information value Vi and the corresponding match Mi. Then the products? sum divides the sum of m information value Vi. The obtained result is called time-validity match of rule (A).

Here we give the definition of match: Assume P(A) denotes the probability of A?s emergence, P(B) denotes the probability of B?s emergence. P(AB) denotes the probability that A and B occur simultaneously. P( A B) denotes the probability that A and B occur simultaneously. P( A ) denotes the probability that A doesn?t occur. We define the match of A=>B like (confidence of A=>B)-(confidence of A =>B).

That is to say M= 1 1P(A) P(AB) P(A) P(A B)? ?? . Hence time-validity match is defined as  0 0  1 1 1 1 | | | |1 1  0 0 0 0  ( ) ( ) ( ) ( ) ( )i i m m m m  T T T T  i i i i i i i i  TMRule A B V M V e M e? ? ? ? ? ?  ? ? ? ?? ?  = = = =  => = ? = ?? ? ? ?        (2)  The minimum time-validity support mins and minimum time-validity match minm are specified by decision-maker.

Rules that satisfy TSRule(AB)>mins and TMRule(A=>B)> minm are called strong association rules. We believe knowledge that satisfies strong association rules is interesting.

B Significance of TSTM Framework  First of all, by adding a parameter i 0 - | |e T T? ? , transaction  value can reflect the time factor. By taking the sum of information value as denominator, the generated rules can better embody the time significance of rule values. That is to say, we consider the ratio between information value of data in each transaction and value of the total information. Besides, the scope of the ratio is [0,1]. But the time-validity framework proposed in [6] has drawbacks that its support results are too small and the definition of minimum support is not easy to understand.

Here we give an example. Give three observation points and their respective support. Assume the data sets are with the same size for each observation point. Minimum support=0.6.

Table 1 Item support  T2 T1 T0  Sup2(A)=0.2 Sup1(A)=0.7 Sup0(A)=0.

Sup2(B)=0.6 Sup1(B)=0.6 Sup0(B)=0.

Sup2(C)=0.8 Sup1(C)=0.6 Sup0(C)=0.

(1) When not considering the time factor, the items A, B and C have the same support, all 0.6.

(2) When considering the time factor and using the calculation method presented in [7], Let ? ?0.5, Ti-Ti-1=1.

We can get:  TSRule(A)=0.47 ,TSRule(B)=0.42,TSRule(C)=0.353  (3) When considering the time factor and using the calculation method proposed in this paper, Let ? ?0.5, Ti-Ti-1=1. We can get:  TSRule(A)=0.675 ;TSRule(B)=0.6; TSRule(C)=0.51  From the calculations above, we can learn that when not considering the time factor, the three items have the same support and it is unable to distinguish them. However, it is obvious that the frequencies of the three items at different time are significantly different. Item A significantly increases in the recent time while item B is very stable and item C significantly decreases in the recent time. Such differences often have decision means for decision makers. But the traditional association rules can?t distinguish such differences.

When considering the time factor and using the calculation method presented in [8], although we can see the differences between the three items, but they are non-frequent sets and also can?t be distinguished. Moreover, the supports of items are all less than 0.6 and the results are not easy to understand.

When suing the framework the paper proposed, we can easily distinguish frequent itemsets and the generated support is easy to understand. When the observation points are stable, even if we add the time factor, time-validity support still keep unchanging. It shows that stable items don?t impact by time.

Through the calculation, the algorithm proposed in the paper can accurately reflect the time significance of rules.

III TIME-VALIDITY INCREMENTAL MINING  A Time-validity Incremental Mining Framework Based on TSTM framework, the paper proposed a new  incremental algorithm to mine association rules. If the minimum time-validity support and minimum time-validity match are both constant, when updating association rules after new database is added, two main issues need to consider:  (1) Association rules updating when adding database D01 at the benchmark observation point T0.

(2) Association rules updating when taking the time adding database as the benchmark observation point.

Considering that the information values of transactions at observation point Ti are all similar, we assume the time interval between adjacent observation points is ? , the information value of one transaction at a certain observation point can be expressed as ??ii eV  ?= . In the process of incremental association rules mining, it only aims at new data behind the benchmark observation point. Do the weighted sum     with the previous result and then we can obtain the overall time-validity degree.  Therefore, incremental mining can take full advantage of the previous result to take a quick update.

Aiming at the two issues above, we take the time-validity support as an example to give the formulas and its proofs.

First of all, description of symbols is given as follows:  m: Time-validity accuracy interval observer specified;  TS0: Overall time-validity support of original itemsets (A);  S0: Support of itemsets(A) in D0;  S01: Support of itemsets(A) after adding database D01;  TS1: Overall time-validity support of itemsets (A) before observation point T0;  TS: Overall time-validity support of itemsets (A) after updating database;  Assume ? ?  =  ?=   m  i  ieM ?? .

Association rules updating when adding database D01 at the benchmark observation point T0: Overall time-validity support of itemsets (A) is computed as  1 1  0 0  TS ( ) ( ) m m  i i  i  i i  e S e? ? ? ? ? ?  ? ? ?  = =  = ?? ?  i0 0    1 m i  i  DS DS S e TS  M M M ? ?  ? ?  =  = + ? = +? ,  Which 1 1  0 0 0 0   0 0  DS =  S D S D  D D  ? + ?  + , 0  1 0 M  S TS TS= ?  Association rules updating when taking the time adding database D01 as the benchmark observation point: Overall  time-validity support is computed as   TS  S e TS  M ???= + .

Proof: time-validity accuracy interval is m, so we  consider  i   M m  m  i  e e? ? ? ? ?  ? ?  =  ? +? .

i i0  0 1  1 1m m i i  i i  S TS S e S e  M M M ? ? ? ?? ?  = =  = ? = + ?? ?  1 11 1 ( 1)0 0  0 0  1 m mj j i j  j j  S S e S e S e  M M M M  ??  ? ? ? ?  ?? ? ? + ?  = =  = + ? = + ?? ?    S e TS  M ???= + ?  The algorithm description of association rules updating when adding database D01 at the benchmark observation point T0 is as follows:  (1) Compute the support S01 of itemsets in D01 using traditional algorithm;  (2) Compute the support DS0 in D0+D01 using fast incremental updating algorithm [7-9];  (3) Adopt the formula TS? 0  DS  M TS+  to compute the  overall time-validity support;  (4) Determine whether TS satisfies the minimum time-validity support.

When taking the time adding database D01 as the benchmark observation point, to any itemsets (A), there may be two cases in the original database:  TS1 has been obtained and itemsets (A) satisfy the minimum time-validity support;  TS1 hasn?t been obtained and itemsets (A) don?t satisfy the minimum time-validity support;  In order to take full advantage of the existing TS1 to obtain the time-validity association rules after adding database, we design the incremental algorithm as follows:  1) Compute the support S01 of itemsets in D01 using traditional algorithm;  2) When meeting the first case, use formula   ' 0 TSeM  S TS  ??? +=  to compute overall time-validity support;  3) When meeting the second case, find the support for itemsets (A) in every database. And then compute TS1 and obtain overall time-validity support by using the  formula   TS  S e TS  M ???= + ;  4) If TS1 has been obtained but it doesn?t appear in the new database, overall time-validity support is  TS e TS???= .

5) Determine whether TS satisfies the minimum time-validity support.

B Performance Analysis of Time-validity Increment The paper proposed a new incremental updating algorithm  based on TSTM framework. The algorithm fully takes account of the time effect when adding new database, and has given two formulas. According to the first updating issue, it can adopt the corresponding fast updating algorithm and it can greatly improve the updating efficiency. According to the second updating issue, it only has to do the mining in the new database, and then do weighted sum with previous result to obtain the overall time-validity support; In the process of updating frequent itemsets, it only needs to use vector method to find the corresponding itemsets to do ?and? operation. It doesn?t need to scan the entire database. Time-validity incremental updating algorithm proposed in [7] only considers association rules updating when taking the time adding database as the benchmark observation point. The formulas? simply is based on the premise that the numbers of transactions are roughly equal at each observation point. And updating itemsets with different length has to repeatedly scan the database. The algorithm proposed in the paper fully takes account of the different time validity after adding new database to calculate the formula. It does not need to consider the condition that the numbers of transactions are roughly equal at each observation point. It also doesn?t have to scan the database many times. Therefore, the incremental updating algorithm proposed in this paper not only has generality, but also can greatly improve the updating efficiency.

IV EXPERIMENTAL RESULTS The paper uses adult database to verify the algorithm?s     time comparison        4000 5000 6000 7000 8000 size of whole database  p e r f o r m  t i m e ( s )  incremental time  whole time    efficiency and adopts the algorithm to calculate frequent itemsets at single observation point. Experiments are performed at an IBMR52 notebook with 512M RAM and 1.86G main frequency. We extract 8000 records from adult database and transform them to a Boolean database. The target is to test the efficiency. Experimental parameters are set as follows: there are four transactions 0 1 2 3{ , , , }D D D D initially. The transaction size is 1000 records. Their respective occurrence time is {T0, T1, T2, T3}. The time difference Ti-Ti-1=1. ? =0.5?mins=0.5. 1000 records for each update.

Compare the recalculation time of frequent itemsets between incremental updating algorithm and original algorithm.

Association rules updating when adding database D01 at the benchmark observation point T0. ?0+?1+?2+?3=4000 is original database. The effect is shown in Fig.1.

Figure. 1  Test the updating algorithm at the new observation point  Association rules updating when taking the time adding database as the benchmark observation point. ?0+?1+?2+ ?3=4000 is original database. The effect is shown in Fig.2.

Figure. 2  Test the updated algorithm in the benchmark observation point  Experimental results show that under the original TSTM framework, regardless of association rules updating when adding database at the benchmark observation point or association rules updating when taking the time adding database as the benchmark observation point, the incremental updating algorithm proposed in this paper can quickly update frequent itemsets and effectively reduce the passes over the transactions. Hence, the algorithm is an effective safeguard to rules update under this framework.

V CONCLUSIONS The presentation of TSTM framework in this paper has  overcome the shortcoming that traditional association rules can?t reflect the time validity of data. It reflects the time significance of data as well as the correlation changes between antecedent and consequent of rules. Besides, vector method can be used for mining association rules to improve the efficiency of algorithms. Time-validity incremental updating algorithm is proposed based on TSTM framework. This algorithm improves the maintenance of rules under this framework, and at the same time can make use of other fast incremental updating algorithms such as FUP [10], VFUP [11] etc to improve the efficiency of maintenance. Therefore the presentation of this algorithm has a certain theoretical and practical significance.

This is a summary of issues for future research. We would like to analyze the time-validity significance of data further.

Besides, we want to discuss the optimization of related algorithms.

