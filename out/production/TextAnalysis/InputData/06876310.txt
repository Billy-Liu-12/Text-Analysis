ICTON 2014  Mo.D1.3

ABSTRACT The massive big data transfer requirements in the cloud age are posing unprecedented challenges on todays? network infrastructure. Delivering data in a cost effective and scalable manner is of critical importance to support the continued growth of cloud services. In this paper, we revisit the problem of delivering large data on top of dynamic circuit switched networks, with a particular focus on how it can be used together with packet switching in an integrated data delivery model. We argue that the Software Defined Networking (SDN) concept and flow based switching are crucial to realize scalable and energy efficient integrated data delivery. We propose to use built-in or public storage to resolve contentions between large data flow transfers.

Keywords: big data, optical circuit switching, data aggregation, network performance.

1. INTRODUCTION The dramatic decrease in the cost of computing and storage, and the proliferation of powerful smart handheld devices, have brought about the continued growth of digital data at an unprecedented pace. According to the study done by IDC, from 2005 to 2020, the amount of digital data created, replicated and consumed every year will increase by 300 folds, from 140 Exa Bytes to 40,000 Exa Bytes (40,000 ? 1018 bytes) [1]. During the meantime, the need to share and distribute data is also increasing. Take for example, the genomic data produced by DNA sequencers, needs to be delivered to its clients and other research institutions geographically distributed across the world. The data is so large that it can hardly be transmitted through the current network infrastructure.

Instead, it is stored in disk drivers and delivered through FedEx [2]. The Large Hadron Collider (LHC) at CERN has 10 ? 100 Tera Bytes of data to be distributed globally on a daily basis. In the commercial domain, the widespread deployment of data centers has also created a fast growing need to replicate and distribute data among them. It is reported that by 2015, the traffic between Data Centers will reach 1 Zetta Bytes (1021 bytes).

While the demand of data movement in the research and commercial domains will undoubtedly be strong in the coming years, the data movement demand generated by end users is also experiencing tremendous change.

The information generated or received by a typical Internet user is increasingly dominated by high definition photos, videos and the like. As a result, mainstream Internet Email providers, such as Gmail, Yahoo! Mail and MSN Mail, start to provide email services that support very huge attachments, under the support of readily available personal cloud storage services. This, on the first hand, reflects the transition of data delivery demand of typical Internet users, from exchanging small and interactive messages, to generating and consuming bulk media files. On the other hand, this also will unquestionably promote the exchange of large files among users, through an application that is most accessible and so commonly used.

Given the enormous dynamics and unpredictability in the number of active users and their behavior, the characterization of network traffic has been extremely difficult. This has partially resulted in the situation that packet switched networks are probably still the most economical, scalable and efficient way to deliver data, especially in access and metro area networks. When the data flows are predominantly short and end systems are responsive to network signals, network congestions rarely happen and the inbuilt mechanisms work well to provide good quality of experience to end users. However, it becomes increasingly difficult, as line rate continues to increase and transmission of large data flows become commonplace. This either leads to the situation in which large file transfers are not able to get enough bandwidth and thus unable to finish within a predictable time, or, they grab too much network bandwidth and result in constant network congestions. The difficulty in network control and bandwidth sharing is rooted at the nature of the Internet, by which each packet is processed separately. As we are entering a time when a single flow may consume a significant portion of link bandwidth, for a considerably long duration, the per-packet forwarding concept, and the mechanisms built upon it, is no longer effective.

