DRAC: A Direct Rule Mining Approach for Associative Classification

Abstract The application of associative rule mining in  classification (associative classification) has demonstrated its  power in recent years. The current associative classifier  building often adopts three phases: Rule Generation, Building  Classifier and Classification. Unfortunately, in rule  generation phase, a large number of rules are usually  produced, which could not only slow down the mining process  but also bring challenge to pruning and storing such  magnitude of rules.

In this paper, we propose the DRAC, a Direct Rules  mining approach for Associative Classification, to tackle the  efficiency of associative classification problem. DRAC can  mine the high quality non-redundant rule set directly. At the  same time, it also adopts the multiple strong class association  rules to classify the unlabeled dataset correspondingly. The  experimental results show that DRAC is more efficient than  traditional approach CBA without losing of accuracy.

Keywords: associative classification; generator;  non-redundant rule set

I. INTRODUCTION  Association rule mining and classification are two  important data mining techniques used in amount  application areas, including finical market, bioinformatics,  web analysis, and so on. The goal of association rule  mining is to find the rules in the database with confidence  and support greater than the user specified threshold [1].

Classification is used to build a classifier by analyzing the  ____________________  * Corresponding Author: mazhx@lzu.edu.cn.

The work was supported by the Fundamental Research  Funds for Central Universities under grant No.

lzugbky-2010-91.

given training datasets with a class label, and predict the  unlabeled objects. In recent years, a new approach called    associative classification is proposed to integrate  association rule mining and classification, such as CBA[7],  CMAR[8]. This new technique was found to be  competitive with traditional classification methods, such as  C4.5 and SVM due to its higher accuracy.

Unfortunately, recent studies show that associative  classification also suffer efficiency problem inheriting from  association rule mining, the large number of rules  discovered. First, mining so large number of rules is time  consuming due to the exponential combination of  attribute-pairs. Especially, when the datasets scale is large  or minimum support threshold is low, it cannot be  completed. Moreover, the last classifier building is always  using a small high quality rule set, so we have to prune the  class association rules (CARs)[7] discovered. There is no  doubt so large number of rules will bring challenge to the  prune phase.

The computational cost raised by traditional association  rule mining motivates us to investigate an alternative  approach: instead of mining the complete set of CARs,  directly mining the high quality non-redundant rules. This  leads to our proposal direct mining high quality rules for  classification, DRAC.

Our contributions are summarized as follows:  (1) In this paper, we propose a new approach  (DRAC) to build a classifier efficiently without  loss of accuracy. The experimental results show  that DRAC is more efficient and accurate in  companion with CBA.

(2) DRAC can solve the problem inheriting from the  association rule mining, a large number of rules.

DRAC extends the efficient generator mining   978-0-7695-4225-6/10 $26.00  2010 IEEE  DOI 10.1109/AICI.2010.155   algorithm, GrGrowth[2], to mine a compact set  of high quality non-redundant rules directly  from the training set in comparison with  traditional CBA[7], CMAR[8] which generate  all CARs satisfying the support and confidence  threshold. And we also adopt the multiple strong    class association rules to classify the unlabeled  dataset to avoid bias caused by single rule.

(3) DRAC adopts the generator to form the rule,  which makes the classifier compact and efficient.

Moreover it uses hash table to eliminate the  redundant rules during the mining process.

The outline of this paper is as follows: Section 2  introduces the relate works. Section 3 describes the rule  generation process in DRAC. Section 4 discusses how to  build the classifier. The experimental results are presented  in section 5 and we conclude the study in section 6.



II. RELATE WORKS  In general, an association rule is the form of X ?Y,  expressing the semantics that occurrence of X is  associated with occurrence of Y, where X and Y are  collections of data items. However, the task of  classification is to build a classifier from the training  dataset so that it can be used to predict class label of the  unknown object with high accuracy. In the training dataset,  except the common attributes, there is a class label. So, in  associative classification the rule is the form like X ?c  called class association rule. X is the attribute sets, and the  c is a value of class label. Let D be the training set, I be the  set of all attributes values in D, and C be the set of class  label. We say that a data object d contains X, a subset of I,  if X?d. Given a class association rule R: X ?c, where  X?I, and c? C, the number of data objects in D  containing X and having class label c is called the support  of R, denoted as of sup(R). The ratio of the number of  objects containing X and having class label versus the total  number of objects containing X is called the confidence of  R, denoted as conf(R).

Two important association rule based classifiers are  CBA [7] and CMAR [8]. CBA first generates all the class  association rules greater than the given support and  confidence thresholds as candidate rules. It then selects a  small set of rules from them to form a classifier. When it  predicts the class label for an unlabeled object, the best  rule (i.e., with the highest confidence) whose body is  satisfied by the object is chosen. CMAR generates and  evaluates rules in a similar way as CBA (but uses a more  efficient algorithm FP growth [11]). A major difference is    that it uses multiple rules in prediction, using weighted ?2.

The traditional associative classification methods  always generate rules from the frequent items  (attribute-pairs). Obviously, the number of rules generated  is much larger due to the exponential combination of  attribute-pairs. Especially, when the datasets scale is large  or minimum support threshold is low, it cannot be  completed. Moreover, the rules generated contain a large  number of redundant ones and are needed to prune before  building the classier. So, they are time consuming.

Previous study ACCF[10] uses frequent closed itemsets  to form the rules to lessen the number of rules successfully.

Frequent closed itemsets is a concise representation of the  frequent itemsets, so its number is always smaller.

Mannila et al.[4] first propose the notion of condensed  representation. Several methods[2],[3],[5] for concise  representation of frequent itemsets have been previously  proposed to eliminate the redundancy including frequent  closed itemsets, generators and generalizations of  generators. Frequent closed itemsets are the maximal  among the itemsets appearing in the same set of  transactions and generators are the minimal. Frequent  closed itemsets mining has been well studied and several  efficient algorithms have been proposed. However, little  effort has been put on developing efficient algorithms for  mining generator based representations.

The concept of generators is first introduced by Bastide  et al. [6]. They use generators together with frequent closed  itemsets to mine minimal non-redundant association rules.

In fact, generators are more preferable than closed itemsets  in classification, because the later contains some redundant  items that are not useful for classification and it violates  the minimum description length principle.

In the previous study, [2] has proposed a new concise  representation of frequent itemsets using generators and  positive borders. In this paper we extend their method  (GrGrowth) to mine the high quality non-redundant rules   for classification. Our experimental results show that  number of rules we mined is much smaller and our  classifier is more compact and general.



III. GENERATING RULES FOR CLASSIFICATION    In this section, we extend GrGrowth for frequent  generators mining to discover the high quality  non-redundant rule set.

Definition 1 (Generator):  Itemset l is a generator if  there does not exist l, such that l?l and support (l) =  support (l).

According to the definition, generator is the minimal  representation of the equivalent class compared with the  closed itemset which is the maximal. As described in the  previous section, the generator is more representative and  it is more preferable than closed itemset in classification.

Property 1 (anti-monotone property): If l is not a  generator, then ?  l? l, l is not a generator.

Property 1 implies that if itemset l is a generator, then  all of its subsets are generators.

Definition 2 (General rule):.Let R(X?C) denotes the  class association rule. We say that R1 (X1?C1) is more  general than R2 (X2?C2), if and only if X1?X2.

Definition 3 (Non-redundant rule set): Let the rule set  R= {R1,......, Rn}.we say that R is non-redundant if and only  if ,given a rule Ri(Xi?Ci), there does not exist one rule  Rj(Xj?Cj), such that Xj?Xi and conf(Rj)>=conf(Ri). In  other word, there does not exist one rule which is more  general and with higher confidence than Ri.

It is clear that if there exists such a rule Rj then Ri does  not convey any new information. Thus we say that Ri is  redundant. To guarantee this non-redundant rule set, first  we use generator to form the rules. According the  definition of generator, it is the minimal representation of  the equivalence class. In the example 1, there are 4  itemsets {a, ad, ae, ade} in the equivalence class, we only  generate one rule using the generator a?y and the other  three rules are redundant because they do not convey any  new information. Moreover, in the generator mining  process, we use the hash table to check whether there  exists at least one more general rule with higher  confidence.

Example 1 (Mining class association rules) Given a  training data set T as shown in Table1. Let the support  threshold is 2 and the confidence threshold is 50%. DRAC  mines the class association rule as follows.

Table 1. A training data set    Tid  Transactions          Class  1     a b c d e g            y  2     a b d e f              y  3     b c d e h i             n  4     a d e m               n  5     c d e h n              y  6     b e i o                y  DRAC extends the GrGrowth algorithm to find  frequent generators and generate rules in one step. To  achieve this purpose, when constructing the conditional  FP-tree, we add the distribution of various class labels  among data objects for every node, as shown in Figure(1).

Figure 1. FP-tree in Example 1  The GrGrowth[2] algorithm explores the search space  using the depth-first-search strategy. During the mining  process, the GrGrowth algorithm needs to check whether  an itemset is a generator by comparing the support of the  itemset with that of its subsets. To guarantee that all the  subsets of a frequent itemset are discovered before that  itemset, the GrGrowth algorithm traverses the search space  tree in descending frequency order. In the example shown  in the Figure(1), the conditional database of item d is first  processed, and then the conditional databases of item b,  item a and so on. The conditional database of item i is  processed last.

When mining itemset ls conditional database Dl, the   GrGrowth algorithm first traverses Dl to find the frequent  items in Dl, denoted as Fl={a1, a2, , am}, and then  construct a new FP-tree which stores the conditional  databases of the frequent items in Fl. According to the  anti-monotone property, there is no need to include item  aj? Fl into the new FP-tree if l?{aj} is not a generator.

Non-generators are identified in two ways in the  GrGrowth algorithm. One way is to check whether  support(l?{ai})=support(l) for all ai? Fl. This checking is  performed immediately after all the frequent items in Dl  are discovered and it incurs little overhead. The second  way is to check whether there exists itemset l such that  l? (l?{ai}) and support(l)=support(l?{ai}) for all ai  such that support(l?{ai}) < support(l). During the mining    process, the GrGrowth algorithm maintains the set of  frequent generators that have been discovered so far in a  hash table to facilitate the subset checking. Further details  can be obtained in[2].

Differently from the GrGrowth, we form the rule using  the generator discovered so far. If the rules confidence is  greater than the threshold and it is non-redundant as  defined in definition3, then we put it into the hash table.

However, if the rule does not satisfy the above two  conditions but the rule body is a generator, then we only  put the generator into the hash table. For example, we find  the rule <bd?y,2/6,2/3> is redundant according the  Definition 3 because in the hash table there exists a rule  <b?y,3/6,3/4> which is more general and has higher  confidence, so it only stores the generator bd in the hash  table. In the generator checking process, we check whether  the rule generated is non-redundant or not at the same time,  so there is no extra time consuming. Thus, we directly  mine the non-redundant rule set efficiently for building the  classifier.

We define the form of rule in DRAC as <rule, support,  confidence>. In this example we only generate 7  non-redundant rules:  <d?y,3/6,3/5>,<b?y,3/6,3/4>,<a?y,2/6,2/3> ,<c?y,2/6,  2/3>,<h?n,1/6,1/2>,<i?n,1/6,1/2>,<ab?y,2/6, 1>

IV. BUILDING A CLASSIFIER  After the non-redundant rule set is mined, as discussed  in Section 3, DRCA is ready to build a classifier based on  database coverage similar to CBA[7]. Firstly, it sorts the  rules according the following definition. Secondly, it  chooses a set of high precedence rules in R to cover  training data.

Definition: Given two rules, ri and rj, ri ?  rj (also  called ri precedes rj or ri has a higher precedence than rj) if  (1) the confidence of ri is greater than that of rj or  (2) their confidences are the same, but the support of  ri is greater than that of rj or  (3) both the confidences and supports of  ri and rj are  the same, but ri is generated earlier than rj .

DRAC for building the classifier and predict an unseen  case is similar to the CBA, but there are two major  differences:    (1) DRAC direct mines the non-redundant rule set.

For example, CBA discover 35 CARs but DRAC  only 7 rules without lose of information.

(2) Given the unlabeled object t, CBA seeks the first  rule r?c whose body is satisfied by t, that is t?r.

However, DRAC use the best k rules of each class  for prediction like CPAR[9] to avoid bias caused  by single rules: 1) select all rules whose bodies  are satisfied the new object; 2) select best k rules  for each class; and 3) compare the average  expected accuracy of the best k rules of each class  and choose the class with the highest expected  accuracy as the predicted class.



V. EXPERIMENTAL RESULTS AND PERFORMANCE STUDY  In this section, we present some experimental results  and also compare them with CBA. We conduct the  experiments using the same parameter originally proposed  by its authors. The support threshold is set to 1% and the  confidence threshold is set to 50%. Also, we adopt the  same method used by CBA to discrete continuous  attributes.

Table2 gives the comparison of accuracy and the  number of rules between DRAC and CBA. The results was  obtained by 10 fold cross validation over 15 datasets from  the UCI ML repository [12].

As can be seen from the table 2, DRAC outperforms  CBA on accuracy and it produces a much smaller number  of CARs. The average accuracy increase from 0.8418 for   CBA to 0.8544 for DRAC and out of the 15 datasets,  DRAC achieves the better accuracy in 10 ones (marked  with * in table2). Moreover, the average number of CARs  decreases from 23955.9 for CBA to 2396.7 for DRAC.

Table 2  Experiment results  dataset  CBA  CARs  DRAC  CARs  CBA  accuracy  DRAC    accuracy  heart* 52309 5975.8 0.8295 0.8519  tic 7063 1395 0.9886 0.9852  led7* 464 656.6 0.695 0.814  glass 4234 425.9 0.7698 0.7687  horse* 62745 6973 0.8061 0.812  diabetes* 3315 164 0.7587 0.773  breast* 2831 401.7 0.9599 0.971  wine 38070 1531 0.9833 0.9622  iris* 72 72.7 0.9391 0.953  labor 5565 349 0.9499 0.933  pima* 2977 741.5 0.7601 0.769  crx* 42877 4859 0.8449 0.8591  vehicle* 23446 6012 0.7195 0.7275  hepatic 63134 2321.4 0.8576 0.8327  auto* 50236 4072 0.7656 0.803  average 23955.9 2396.7 0.8418 0.8544

VI. CONCLUSIONS  In this paper, we proposed a novel classification  algorithm DRAC that mines high quality class association  rules directly. Thus it resolves the problem inheriting from  association rule mining, a large number of rules, which  could not only slow down the mining process but also  bring challenge to the pruning and storing the rules. We  extend the GrGrowth algorithm to produces the  non-redundant rule set directly and build a compact  classifier. Our experiments on UCI datasets show that  DRAC is consistent, highly effective and has better  average classification accuracy compared to CBA. As a  future work, we will continue comparing DRAC with other  associative classification approaches and we will also  study the other uses of the generator.

