Quantitative Association Rules over Incomplete Data

ABSTRACT This paper explores the use of principle component analysis (PCA) to estimate missing values during the mining of quantitative association rules. An exam- ple of such association may be ?15% of customers spend $100 - $300 every month will have 2 cable out- lets at home?. In our algorithm, instead of imputing missing values before the mining process, we propose to integrate the imputation step within the process.

The idea is to reduce the unnecessary imputation ef- fort and to improve the overall performance. First, only attributes with enough support counts and with missing values are required to perform imputations.

Thus, effort will not be wasted on unimportant at- tributes. Further, rather than estimating the actual value of a missing data, the possible range of the value is guessed. This will not affect the resultant quantitative association rules much but will cut down the guessing effort.

1 INTRODUCTION  Many excellent studies on knowledge discovery have been conducted and reported [l, 2, 5, 61. Piatetsky- Shapiro, et. al., defined the goal of studying KDD and its general requirement [4]. In general, data min- ing algorithms can be classified into two categories: concept generalization-based discovery and discov- ery at the primitive concept levels. The former relies on the generalization of concepts (attribute values) stored in databases and then summarization of the data regularities at a higher conceptual level, such as the DBLearn system developed by Han [6]. The latter relies on the discovery of strong regularities (rules) from the databases without concept general-  The work was supported in part by the Polytechnic Uni- versity Research Grant 0351/638.

ization. Association rule is a good example of this approach.

Agrawal, et. al., first defined the problem of min- ing association rules in their pioneer work [l]. The al- gorithm proposed there can be applied to sales trans- action records from large retailing companies. Rules like ?[Children?s Hairliners] => [Infants and Chil- dren?s Wear] (66.55%)? were discovered which is in- terpreted as the rule ?among all the customers, who had bought something in the Children?s Hardliner Department, 66.55% of them also bought something in the Infants and Children?s Wear Department?.

However, data mining algorithms are difficult to ap- ply directly when data is incomplete. Further, in addition to binary and categorical data, quantitative data are often analyzed, which cannot be dealt with effectively by the boolean association rules. This pa- per explores the use of principle component analysis (PCA) to estimate missing values during the mining of quantitative association rules. An example of such quantitative association may be ?15% of customers spend $100 - $300 every month will have 2 cable out- lets at home?.

In the next section, we will first review the recent work in discovering quantitative association rules.

Section 3 will discuss what imputation techniques have been suggested and used in data mining. In Section 4, we will present our definition of the miss- ing information problem in the mining of quantita- tive data. In Section 5, we present three algorithms.

The Pre-Guess algorithm will perform the imputa- tion before mining. The Post-Guess algorithm will utilize principle component analysis to estimate the missing values after the mining process. In the last algorithm, Deviation algorithm, it does not estimate the actual value of a missing data, rather the possi- ble range of the value is guessed. This will not affect the resultant quantitative association rules much but  2821 0-7803-4778-1 198 $10.00 0 1998 IEEE    the resultant quantitative association rules much but will cut down the guessing effort. Preliminary exper- imental results are presented in Section 6 and Sec- tion 7 will conclude our work.

2 QUANTITATIVE ASSOCIATION RULES  Srikant and Agrawal, has reported their work on mining quantitative association rules (QAR) in 1996 [lo]. A QAR extends the classical association rules to include range predicates like (value1 5 I t e m  5 valuez). A quality metric, partial-completeness, is developed to find out what are good intervals for the rules. In their work, quantitative attributes are first partitioned according to  the values of the attributes and adjacent partitions are then combined as nec- essary. The partial-completeness is used to ensure that intervals are neither too big, nor too small with respect to the set of rules they generate.

In developing the measure, quantitative properties of the intervals, such as the relative distance between values, or the density of an interval, are not con- sidered. The algorithm works well for ordinal data where there is no semantic meaning between the val- ues. It does not work well on highly skewed data since it may separate close values that have the same behavior.

A distance-based measure has been recently de- veloped to consider the quantitative properties of an interval [9]. The distance-based association rules can be discovered by first finding out clusters which rep- resent the interesting intervals and then applying a standard association technique. The Birch algorithm [Ill is used as the clustering algorithm because it can incrementally identify and refine clusters in a single pass over the data.

3 IMPUTATION TECHNIQUES Imputation has been an interesting area in Statistics for many years [3]. It has caught much attention, particularly, when people are working on multivari- ate analysis. In general, there are three approaches to deal with missing data. The simplest and most direct approach is to include only those records with complete information. This approach is fine if data are missing at random; otherwise the results from any analysis will be biased. A second simple alter- native for missing data is to  delete the offending at- tributes. In many situations where a nonrandom pat- tern of missing data is present, this may be the most  efficient solution. However, after the deletion, the analyst may only leave with a handful of attributes and the analysis?s results are uninteresting.

A third approach to deal with missing data is through various imputation techniques. Imputation is the process of estimating the missing value based on valid values of other attributes and/or records in the data. There are four common techniques used to replace missing values.

1. Record substitution: this is to replace a record with missing data by another record not in the current data set.

2. Mean substitution: this replaces the missing value of an attribute by the mean value of that attribute based on all complete records.

3. Cold deck imputation: constant values obtained from previous studies are used to  replace missing values of attributes in the current data set.

4. Regression imputation: regression analysis is used to predict the missing values of an attribute based on its relationship to other attributes in the data set.

In data mining, method (1) and (3) are not ap- propriate as there may not be any external data nor previous study. Even when there is external data available, the discovered patterns will be heavily bi- ased towards the external information. Method (2) is feasible, but one has to deal with non-integral values in the data mining algorithms and the interpretations of the discovered patterns afterwards. Amongst the four methods, method (4) demands more effort but will have better predictions of the missing values.

The concern, though, is related to the performance and applicability when working with huge data set.

Lakshminarayan et al. have proposed to use machine-learning based methods to deal with miss- ing data [8]. The idea is similar to method (4) above, except that two well-known machine-learning tech- niques are adopted. The first technique adopts a Bayesian approach to cluster the data into classes by using an unsupervised clustering strategy. Au- toclass is used to generate a set of resultant classes which are then be used to predict the missing values of attributes. The second method predicts the miss- ing values via a decision tree-based classifier, C4.5.

Before the predictions, supervised induction is used to  train the classifier with a sample set first. How- ever, for both techniques, they are only dealing with a relatively small set of data. Their performance and applicabilities on large volume of data are not tested.

A more promising approach is the use of princi- ple component analysis (PCA) to estimate the miss- ing values [7]. Korn et. al. proposes a single-pass mining algorithm to find linear rules from a set of quantitative data. Rules like ?Customers that spend between 7 to 15 dollars on coffee beans and 3 dollars on milk will likely to spend 1 dollar on sugar? are found. The algorithm is based on the results of per- forming a PCA on the data. The linear rules found can also be used to guess any missing values in the data. When comparing with mean substitution, the linear rules can achieve up to 5 times smaller guess- ing error.

4 PROBLEM STATEMENT  In the work of [7, 81, it has been assumed that there are two states of a quantitative item within a given transaction. The item can either be having a numeric value, or its value is missing. However, if we consider the popular supermarket scenario, we can observe that, in fact, there is one more possible state - ?the item is not in the transaction?. For example, suppose there are only three items available in a supermarket: bread, milk and butter. If we bought $3 of milk and $5 of bread, it does not imply that we have bought some butter as well.

Let V = 1 1 ,  I2,. . . , IM be a set of quantitative items, and T be the transactions of a database D.

For each transaction t ,  t[k] > 0 means that t con- tains item I k  with the value t[k]. t[k]=O means that I k  does not exist in t while t[k]= -1 means that Ik?s value is missing. In our work, we are interested in finding all quantitative association rules, X => Y , that satisfy the following conditions:  X and Y are intersections of predicates like ( V k  = valuel) or (valuel 5 V k  5 value2).

The clusters of items represented by the predi- cates in X and Y are disjoint.

The frequencies (support counts) of the trans- actions satisfying the predicates in X and Y are greater than a given threshold.

X and Y may contain 1 or more item with miss- ing information.

ALGORITHMS  Their differences are mainly due to how we apply the imputation technique before, after or during the min- ing step. We have adopted the distance-based associ- ation (DBA) algorithm [lo] for the mining of quanti- tative association rules because of its efficiency. Only a single pass of the data is needed in its first phase of clustering the database. In its second phase, a clus- tering graph is formed and maximal cliques of the graph will correspond to  large itemsets in the data.

From the large itemsets, the quantitative association rules are then derived. Due to the space limitation, we will not cover the details of the algorithms, but will highlight their major ideas.

5.1 Pre-Guess Algorithm The first algorithm is based on the pre-processing approach. That is, we will first impute any miss- ing values and then perform the mining step after- wards. The imputation method utilizes the principle component analysis as proposed in [7]. Eigenvec- tors are first computed from the covariance matrix of the database D. They are then used to predict any missing value. However, this method only han- dles transactions with a constant number of items. In our database, the number of quantitative items varies for different transactions. This implies that we may need different covariance matrices for different types of transactions. In order to save the computational and book-keeping effort, in the Pre-Guess Algorithm, we propose to construct one covariance matrix only.

In Figure 1, we show the steps in finding out the covariance matrix. After forming the matrix, we will scan the database D again. When a transaction with missing values is encountered, columns and rows cor- responding to its non-empty items in the matrix are pulled out. The sub-matrix is used to compute the corresponding eigensystem to do the predictions af- terwards. The system is saved for other transactions with the same items containing missing values later.

Several eigensystems may be resulted but only two passes of D are required.

After the prediction of all the missing values, we use the DBA algorithm to find the quantitative as- sociation rules with no modification.

5.2 Post-Guess Algorithm It is realized that not all items in a database would have sufficient support counts. Therefore, there is a waste to guess values of those items which will not  In this section, we will discuss three different ap- proaches to support mining on missing information.

be included at the end. The Post-Guess Algorithm is designed not to guess the missing values at the     Given a database D with T transactions and M quanti- tative items. Find out the covariance matrix C.

1. Fori = 1 to M  0 Initialize colavg[i] = 0.0 0 For j = 1 to M  - Initialize C[i]Ij] = 0.0  2. For each transaction t in D  0 colavg[i] = colavg[i] + t[i] for all (t[i] > 0) 0 For j = 1 to M  - C[i]Ij] = C[i]lj] + t[i]*tIj] if ((t[i] > 0) and (th] > 0))  3. For i = 1 to M  0 colavg[i] = colavg[i]/ ID1  4. Fori = 1 to M  0 For j = 1 to A4 - C[i]b] = C[i]Ij] - ID/ * colavg[i]*colavg~]  Figure 1: Pre-Guess Algorithm: Finding the Covari- ance Matrix.

beginning, but to  do it after the initial clustering step.

Suppose we consider each transaction as a multi- dimensional point and each point can be of different dimensions. In this case, the direct application of the clustering distances cannot be used [11]. We propose to normalize the distance measurements in order to allow clustering points (transactions) of different di- mensions. For example, the Mahanttan distance D1 between two clusters Cl[X] and C2[X] of dimension k is re-defined as:  Dl(Cl[X], C2[X]) = 1x01 - XO2I/k  where XO1 and X02 are the centroids of the clusters C1 and C2 respectively.

As in the DBA algorithm, there are two phases in the mining step for the Post-Guess Algorithm. Dur- ing the clustering phase, when transactions are added to the CF-tree, the single covariance matrix similar to that in the Pre-Guess Algorithm is computed. In addition, the support counts of individual items are recorded. When a transaction with missing values is encountered in this phase, it is put aside in a sep- arate storage area (5?). After the CF-tree is built, each transaction in S is taken out. Its missing val- ues are predicted by utilizing the covariance matrix as described before, and the transaction is inserted into the closest cluster in the CF-tree finally. Note  that for items whose support counts are less than the threshold, their values will not be estimated. Af- ter the final clustering, we continue with the second phase as described to discover the quantitative asso- ciation rules.

5.3 Deviation Algorithm  Estimating the missing values require the construc- tion of the covariance matrix, finding of the eigen- system and then the final prediction steps. All these steps demand noii-trivial computational effort. In discovering quantitative association rules, we are not really interested in the actual values associated with the items. Instead, a good guess of its possible range is often sufficient.

In the Deviation Algorithm, we propose to drop the PCA but to  exploit the clustering property offered by Birch in the first phase of the mining step. The idea is similar to the work done by Lakshminarayan et al.

except that we are not using decision trees nor the Bayesian approach. We assume that there will not be a lot of missing values in the database; otherwise it will be impossible to  do any meaningful data min- ing. Further, if values are missing systematically, it would be easier to discover and correct them accord- ingly. On the other hand, if only a small number of values are missing randomly, then they can be easily approximated by their nearby information (clusters).

In the clustering phase, when a transaction with missing values is encountered, it is put aside in a sep- arate storage area ( S )  as in the previous algorithm.

However, after the CF-tree is built, each transaction in S is inserted into the closest cluster in the CF- tree directly. Any missing values of the transaction will be imputed by the centroids the corresponding dimensions. The method is similar to  the mean sub- stitution, but it only considers transactions within the same cluster rather than the complete database.

After the final clustering, we continue with the sec- ond phase to discover the quantitative association rules afterwards.

6 EXPERIMENTS  In order to assess the performance of the algorithms, we have implemented them in C++. The prelim- inary experiments have been run on a Sun Ultra workstation with 64 Mbytes of main memory. Ini- tially, a data set of 100,000 transactions is created.

Each transaction contains two groups of data items.

Each item in the first group contains a pair of num-     0 Let P be the predication accuracy and set it to 0  0 For each rule R, in A initially.

- Find a rule Rj in B with the largest overlap- ping of items.

- For each pair of predicates involving the same item in R, and Rj ,  calculate the ratio of their interval overlaps. Add up all these ratios and divided the sum by the number of predicates in R3.

- Add the resultant value to P.

0 P=P/IBI  Figure 2: Prediction Accuracy.

bers generated randomly. The first number repre- sents the item number and the second value repre- sent the item's value over the interval [O,lOOO]. The items in the second group is similar to those in the first group except their item values are calculated with the item values in the first group by using some linear formulae.

During the experiments, we randomly select 10% (i.e. 10,000) of the transactions to have missing val- ues. In the first experiment, the 10% transactions will have only 1 item with missing value. The second experiment will have two items with missing values and so on, until there are 5 items with missing values at the end. During the experiments, we memure the total times needed for the mining and the prediction accuracies of the quantitative rules when compared with those discovered from the complete data set.

Suppose we have two rule sets A and B discovered from the incomplete and complete data sets respec- tively. We can then compute the prediction accuracy as shown in Figure 2.

The results of our experiments are shown in Fig- ure 3 and Figure 4. From these results, we can ob- serve that the Deviation algorithm takes less time in mining the quantitative association rules, but its accuracies are comparable with the PCA algorithm.

25 - P .E - P 2 0    I -  - - 'I * -  *  0 1 2 3 4 5 Number 01 kems wlm mlssmg values  Figure 3: Total mining times.

1 , 'Prebuess' f  'Devlalwn' -+- 'PoslGuess' t  0.8  0.2  o o o  Number of items wah missing values  Figure 4: Prediction accuracies.

analysis to estimate the missing values after the min- ing process. The last algorithm, Deviation algorithm, does not estimate the actual value of a missing data, but the possible range of the value is guessed. Pre- liminary experimental results have shown the compa- rable accuracies of the algorithm and its great savings in the computational effort.

