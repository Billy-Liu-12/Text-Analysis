OSSM: A Segmentation Approach to Optimize Frequency Counting

Abstract Computing the frequency of a pattern is one of the  key operations in data mining algorithms. We describe a simple yet powerful way of speeding up any form of fre- quency counting satisfying the monotonicity condition. Our method, the optimized segment support map (OSSM), is a light-weight structure which partitions the collection of transactions into m segments, so as to reduce the number of candidate patterns that require frequency counting. We study the following problems: (1) What is the optimal num- ber of segments to be used; and (2) Given a user-determined m, what is the best segmentation/composition of them seg- ments? For Problem 1, we provide a thorough analysis and a theorem establishing the minimum value of m for which there is no accuracy lost in using the OSSM. For Problem 2, we develop various algorithms and heuristics, which effi- ciently generate OSSMs that are compact and effective, to help facilitate segmentation.

Keywords: Data mining, frequent patterns, support count- ing, data structure, performance analysis  1. Introduction and motivation Computing the frequency (or support) of a pattern is one  of the key operations in data mining algorithms. For the algorithms to discover frequently occurring patterns, they have to investigate specific patterns to find cardinalities of subgroups or significance of deviations, etc. Typically, the patterns, whose frequencies are needed, are conjunctions of atomic patterns. A prime example is given by the frequent set concept underlying association rules [2, 3]. Moreover, the patterns defined for correlation [6, 7], causality [18], sequential patterns [4], episodes [13], constrained frequent sets [11, 14, 19], long patterns [1, 5], closed sets [16], and many other important data mining tasks have the same basic form. In all these cases, we have instances of the follow- ing abstract problem. Given a collection P of atomic pat- terns or conditions, compute for collections S ? P the sup- port sup(S) of S. The monotonicity condition sup(S) ? sup(fag) holds for all a 2 S, since the frequency or support of the collection S is defined to be the number of observa-  tions in the data for which all atomic patterns in S are true.

In this paper, we describe a simple yet powerful way of  speeding up any form of frequency counting satisfying the monotonicity condition. Our method, the optimized seg- ment support map (OSSM), is based on a simple observa- tion about data: Real life data sets are not random. More concretely, the frequencies of patterns will be different in different parts of the data set. Computing the frequencies separately in different parts of the data set makes it possi- ble to obtain better/tighter upper bounds for the frequencies of the collections of patterns and thus enables one to prune much more effectively. The results will be useful to the mining of any of the above classes of patterns.

The frequencies of patterns are computed typically in a collection of ?transactions?, i.e., T = ft1; : : : ; tng.1 Most algorithmsmining the types of patterns mentioned above do so by generating candidate patterns S1; : : : ; Sw, and check- ing their frequencies against T .2 The OSSM is a light- weight, easy-to-compute structure, which partitions T into m segments, i.e., T = T1 [ : : :[ Tm and Ti \ Tj = ;, with the goal of reducing the number of candidate patterns (i.e., w) for which frequency counting is required. We make the following specific contributions: ? In Section 3, we present the OSSM and show how it pro- vides a light-weight mechanism to reduce the number of candidate patterns.

? In Section 4, we consider the segment minimiza- tion problem. Given an OSSM Mm of m segments T1; : : : ; Tm, the OSSM provides for any set S of items an upper bound for the support of S, denoted by dsup(S;Mm). The segment minimization problem is to find the smallest value of m such that the upper bound, in fact, becomes the actual support of S. In other words, there is no loss of accuracy using the OSSM. We show that that the minimum value of m, denoted as mmin, can be quite high in the general case.

1Transactions may come in different forms. In the case of association rules, a transaction is a set of items [2]. In the case of episodes, a transac- tion corresponds to a sequence of events in a sliding time window [13].

2There are algorithms that do not rely on candidate generation (e.g., FP-growth [8], CHARM [21], and GenMax [20]).

? Given that the OSSM created with mmin segments will consume too much space, in Section 5 we con- sider the constrained segmentation problem. Given an input value muser (which is smaller than mmin), the constrained segmentation problem seeks to find the muser segments that minimize the loss of accuracy.

In other words, with the number of segments fixed to muser , we seek to find the best way to partition T into T1; : : : ; Tmuser . Finding the optimal solution turns out to be computationally hard. We, thus, focus on develop- ing heuristic segmentation algorithms. We present three such algorithms and analyze their complexities. To fur- ther optimize the segmentation process, we propose to run some of these algorithms in a hybrid fashion, and propose a heuristic called the bubble list.

? Given the heuristic nature of our algorithms, we present experimental results in Section 6 evaluating the effec- tiveness of the segmentation. We show that all our seg- mentation algorithms bring about significant savings in frequency counting. For example, for an OSSM that occupies only 0.3 megabytes in total size, it can bring about a speedup of 50 times. Furthermore, with the help of our proposed heuristics, this OSSM can be con- structed in less than 10 seconds of total time, for 5 mil- lion transactions.

An outline of the paper is as follows. The next section dis- cusses the related work. Section 3 introduces the OSSM and the way it reduces the number of candidate patterns.

Section 4 analyzes the segment minimization problem, and Section 5 studies the constrained segmentation problem.

Section 6 shows experimental results. Section 7 discusses the additional benefit of using the OSSM in conjunction with efficiency/performance improving algorithms such as DHP [15], Partition [17], and DepthProject [1]. Section 8 presents conclusions.

2. Related work The most relevant piece of related works is our case  study [10] focusing on the use of a SSM structure to fa- cilitate the Carma algorithm [9] for online mining. With such a structure, the collection of transactions is divided ar- bitrarily into several partitions. Such a case study is differ- ent from our current paper as follows. First, in the current paper, we discuss how the OSSM is applicable to general pattern discovery algorithms (e.g., Apriori, and many of its variants and extensions). Second, the collection of trans- actions is divided into segments in the OSSM according to some optimization criteria (e.g., to preserve accuracy when dealing with the segment minimization problem; to mini- mize the loss in accuracy when dealing with the constrained segmentation problem). Third, the key technical focus of the current paper is on the segment minimization problem and the constrained segmentation problem, which have not been studied previously.

The next group of related works are partitioning-based data mining algorithms (e.g., the DHP algorithm [15], the Partition algorithm [17]). On the surface, the hashing-based partitioning done by these algorithms looks similar to the  OSSM. However, there are several key differences. For ex- ample, the DHP algorithm uses a hash table to partition the itemsets of the same cardinality (e.g., 2) into buckets; and the hash table is created dynamically for each run of the algorithm. In contrast, the OSSM partitions transactions, not itemsets; and the OSSM is intended to be a static struc- ture. The Partition algorithm does not set up any structure like the OSSM to reduce the number of candidate patterns.

Moreover, the segment minimization problem and the con- strained segmentation problem are not considered in the DHP algorithm or the Partition algorithm. (As a preview, we will discuss in Section 7 how the OSSM can be used in conjunction with these two partitioning-based data mining algorithms, and will show the additional benefit brought by the OSSM to these two algorithms.)  The third group of related works are the FP-growth al- gorithm [8] and its variants. The FP-growth algorithm is an example of a framework not requiring candidate genera- tion. The algorithm constructs a prefix-tree structure called the FP-tree to mine frequent patterns in a depth-first fash- ion. Contrarily, the OSSM framework proposed in the current paper ? like most studies on frequent pattern com- putation ? relies on candidate generation. The two frame- works are different in at least the following key aspects.

First, the FP-tree structure is query-dependent; the tree is constructed based on a user-defined support threshold of the query. In contrast, the OSSM structure is ?static?, i.e., oper- ating with any support threshold. Second, the FP-tree struc- ture is main-memory based. If the structure is too large to fit into main memory, recursive projections and partition- ing are required, causing additional overhead. On the other hand, once the FP-tree resides in memory, it does not take advantage of additional memory space. In contrast, given a small amount of memory space, an OSSM can still be constructed (using fewer segments) and can bring a signif- icant speedup. Given a large amount of memory space, the OSSM can take advantage of the additional space by using more segments; this further reduces the number of candi- dates for which frequency counting is required (i.e., leads to more effective pruning). In sum, the OSSM is designed to work with any amount of memory space. Its size is inde- pendent of the size of the itemset lattice. In fact, the main focus of the current paper is to explore how to construct an OSSM, given a pre-determined amount of space.

3. The optimized segment support map and its utility  Let the collection of transactions T be partitioned into m segments, not necessarily of equal size. In later sec- tions, we will discuss how to determine the composition of the m segments. An OSSM is a structure consisting of supi(fag) for all singleton itemsets fag, where supi(fag) denotes the support of fag in the i-th segment Ti, for 1 ? i ? m. The support of fag is then  Pm i=1 supi(fag).

While the OSSM Mm only contains the segment supports of singleton itemsets (for the m segments T1; : : : ; Tm), it can be used to give an upper bound on the support of an     arbitrary itemset S:  dsup(S;Mm) = mX i=1  minfsupi(fag) j a 2 Sg (1)  Example 1 Suppose there are 4 segments in the OSSMM4 , and the (ac- tual) segment supports for items a; b and c are as shown:  T1 T2 T3 T4 T  fag 20 10 40 40 110 fbg 40 40 40 10 130 fcg 40 20 20 20 100  By equation (1), csup(fa; bg;M4) is minf20; 40g + minf10; 40g + minf40; 40g+minf40; 10g, for a total of 80. Similarly, by equation (1), the support of fa; b; cg is bounded from above by 60. On the other hand, if we do not use the OSSM (i.e., just by the last column of the above table), then the upper bound on the support for fa; bg isminf110; 130g = 110, while that for fa; b; cg isminf110; 130; 100g = 100.

The above example shows how the OSSM can provide valuable filtering by reducing the number of candidate sets that need to be counted for their frequencies (e.g., when the support threshold is less than 100). For many of the frequent pattern algorithms, such as frequent-set discovery methods or episode discovery methods, one of the performance bot- tlenecks is the number of candidates that have to be con- sidered: Even in cases when the true number of frequent patterns is small, there can be a huge number of candidate patterns. Another important thing to note is that for many algorithms (e.g., those based on hashing), skewed data can be problematic. In contrast, the more skewed the data, the more effective the OSSM is.

Clearly, the upper bounddsup(S;Mm) provided by the OSSMMm can be made tighter by increasing the number of segments m.3 The amount of storage space required is then increased linearly. In the hypothetical extreme case, when the number of segments m equals the number of transactions n, the upper bound becomes the actual support count of S.

We also note that knowledge discovery is typically an iterative process: One first computes certain patterns, in- vestigates them, and then re-computes using perhaps dif- ferent thresholds. In this context, an advantage of the OSSM is that it a fixed structure that can be computed once at ?compile-time? (pre-processing), and can be used regardless of how the support threshold is changed dynami- cally during ?exploration-time? (query execution). In other words, the OSSM is query-independent. This is different from such mining algorithms as DHP [15] or FP-growth [8], which are query-dependent; they construct summary struc- tures (e.g., hash table or FP-tree) based on a given support threshold of the query (cf.: Section 2).

Finally, we note that there is no searching involved when the OSSM is used. Once the singleton itemsets are enumer- ated based on some canonical ordering, the itemsets them- selves (i.e., the first column of the above table) need not be  3An alternative way to tighten csup(S;Mm) is to generalize the OSSM by storing not only the actual segment supports of singleton patterns or itemsets, but also those of itemsets of higher cardinalities (i.e., itemsets of sizes greater than one).

Symbols Meanings  T the collection of reference transactions m the number of segments mmin the minimumm without loss of accuracy muser the user-specified number of segments T1; : : : ; Tm them segments of transactions S a set of items a; a1; : : : ; ai; b; c individual items k the total number of individual domain items Mm an OSSM consisting ofm segments sup(S) the actual support of S with respect to Tcsup(S;Mm) upper bound on sup(S) based onMm p the number of pages occupied by T  Figure 1. Definitions of the main symbols  stored, and direct addressing into the OSSM makes the use of equation (1) very efficient.

4. The segment minimization problem While the previous section highlights the potential bene-  fit of the OSSM, there are two main questions that need to be addressed: 1. What is the best number of segments (i.e., the value  of m)?

2. What is the best composition of them segments?

The primary focus of this section is the first question. In particular, we consider the segment minimization problem, as stated below.

Definition 1 Given a collection of transactions T , the seg- ment minimization problem is to determine the min- imum value mmin for the number of segments in the OSSM Mmmin , such thatdsup(S;Mmmin) = sup(S) for all itemsets S, i.e., the upper bound on the support for any S is exactly its actual support.

To make it easier to understand our analysis, we begin with the simple (hypothetical) situation where there are only 2 individual items. Then we present the key theorem giv- ing the mmin value for the general case with k ? 2 items.

Finally, we generalize the analysis to cover a more practi- cal and relaxed notion of accuracy. For reference, Figure 1 gives a summary of the main symbols used in this paper.

4.1. Lessons learned from the situation with only two items  For ease of understanding, we begin our analysis for the hypothetical situation when there are only two items a and b.

Example 2 Suppose there is a collection of transactions as shown:  Transactions in T Contents  t1 fag t2 fa; bg t3 fag t4 fag t5 fbg t6 fbg  Let us consider the following OSSMM6 containing 6 segments, each of which consisting of a single transaction.

T1 T2 T3 T4 T5 T6  fag 1 1 1 1 0 0 fbg 0 1 0 0 1 1  While it is impractical to consider one transaction per segment, this ex- ample does illustrate an important observation. Consider the following OSSMM2, which contains 2 segments. Specifically, the first segment T 01 consists of all transactions containing a (i.e., t1; t2; t3 and t4), and the second segment T 0  consisting of all transactions containing b but not a  (i.e., t5 and t6).

T 0  T 0  fag 4 0 fbg 1 2  Now with this OSSM M2, it is easy to verify that the upper bound csup(fa; bg;M2) is given byminf4; 1g+minf0; 2g = 1, which is the exact support of fa; bg. Furthermore, suppose that the segmentation is done slightly differently ? with t4 moved from segment T 01 to T   . The  resulting upper bound is then given by minf3; 1g + minf1; 2g = 2, which is no longer the exact support of fa; bg.

The following analysis shows that what we have seen from the above example is not a coincidence.

The definition of segment T 0 in Example 2 necessitates  that sup(fag) ? sup(fbg) in the segment. From now on, we use the notation (a ? b) to describe this situation, and we refer to this description as the configuration of the seg- ment. Similarly, we denote the configuration of segment T 0   as (b ? a).

The following lemma shows that when we merge two  segments Ti and Tj that are of the same configuration, we preserve the configuration and do not change the upper bound due to the two segments.

Lemma 1 Let Ti and Tj be two segments of the same configuration from a general collection of transactions.

If we merge Ti and Tj into one combined segment T 0, then the merged segment is the same configuration, and dsup(fa; bg; Ti) +dsup(fa; bg; Tj) =dsup(fa; bg; T 0).

To understand this lemma, let Ti and Tj be of the config- uration (a ? b). Then we have:  Ti  fag v1 fbg v2  Tj  fag v3 fbg v4  with v1 ? v2, and v3 ? v4. Now with Ti and Tj merged, we have:  T 0 = Ti [ Tj  fag v1 + v3 fbg v2 + v4  It is easy to verify thatdsup(fa; bg; Ti) +dsup(fa; bg; Tj) =dsup(fa; bg; T 0). The case for two segments of the configu- ration (b ? a) is similar.

4.2. Generalization to the situation with k items In general, for k items a1; : : : ; ak, the configuration  of a segment is of the form (ai1 ? : : : ? aik ), where (ai1 ; : : : ; aik ) is a permutation of the k items. This de- scriptor indicates that in this segment, it is the case that  sup(fai1g) ? : : : ? sup(faikg).

4 Clearly then, there are  k! possible configurations of segments.

A close examination reveals that the number of possi-  ble configurations can be reduced. Given a collection of transactions of k items, it produces at most 2k ? 1 distinct non-empty itemsets. Among these itemsets, k of them have the same configuration. Hence, the number of possible con- figurations of segments can be reduced to 2k ? k.

Theorem 1 Let T be a general collection of n transactions for k items. If we allow T to be rearranged, then the min- imum number of segments mmin required in the general case for the upper bound on sup(S) to be exact for all S, is the number of segments with distinct configurations, i.e., mmin ? minfn; 2k ? kg.

To understand the above theorem, suppose there are ini- tially more than 2k ? k segments. Then there must be at least two segments with the same configuration, say (ai1 ? : : : ? aik ). Then the reasoning articulated in Lemma 1 ap- plies. Thus, even if there are initially more than 2k ? k seg- ments, repeated merging would reduce the number of seg- ments to be no more than 2k ? k.

To go one step further, let us consider merging two seg- ments T1; T2 with different configurations. While the gen- eral case can be proved by induction, let us focus on the fol- lowing simple case to understand the situation ? the con- figurations being:  (ai1 ? : : : ? aij ? aij+1 ? : : : ? aik ) for T1, (ai1 ? : : : ? aij+1 ? aij ? : : : ? aik ) for T2,  i.e., two successive items swapped in the permutation. The following shows the part of the OSSM corresponding to T1 and T2:  T1 T2  faij g v1 v3 faij+1 g v2 v4  for some non-negative integers v1; : : : ; v4. Because aij ? aij+1 in T1, it is the case that v1 ? v2. However, in T2, it is the other way, namely v4 ? v3. If we merge these two segments, we get:  T 0 = T1 [ T2  faij g v1 + v3 faij+1g v2 + v4  Now consider estimating the support sup(faij ; aij+1g).

It is always the case that minf(v1 + v3); (v2 + v4)g ? (v2 + v3). The inequality is strict, unless v1 = v2 and v3 = v4. This shows that merging segments with differing configurations can cause the upper bound on sup(S) for some set S to be inexact. Hence, if the transactions themselves give rise to all 2k ? k possible configurations, mmin is exactly 2k ? k.

4To be more precise, if there are ties (i.e., sup(faij g) = sup(faij+1g)), then we adopt the following convention. Without loss of generality, we assume that there is a canonical enumeration of the indi- vidual items. Ties are broken by following the canonical enumeration, i.e., ij < ij+1 .

4.3. A relaxed notion of accuracy: The page version The above analysis addresses the segment minimization  problem formally. However, from a utility standpoint, the results obtained indicate clearly that segment minimization is hard, i.e., requiring in the general case 2k ? k segments to ensure that the upper bound on sup(S) is exact for all S.

The number 2k ? k is simply too huge to be practical.

A natural question to ask is: If we were to relax the no- tion of accuracy in segment minimization, would we be able to reduce the number of required segments to a much more manageable number? Specifically, as analyzed so far, the segment minimization admits no loss of accuracy, i.e., the upper bound from the OSSM on sup(S) exactly matches the actual support for all S. As transactions are stored in pages, one natural form of relaxation of the segment min- imization problem is to consider the page version of the problem. Recall that in the earlier analysis, we have made the implicit assumption that initiallywe know the frequency of every item in every transaction in the segment. The page version of the segment minimization problem begins with a higher granularity level. Initially, we have the aggregate fre- quency of every item per page. Suppose that the collection of transactions T is physically organized into p pages, de- noted by T 01; : : : ; T   p. Then the page version of the segment minimization problem can be described as follows.

Definition 2 Given a collection of transactions T orga- nized in p pages T 0  ; : : : ; T 0p, the page version of the seg-  ment minimization problem is to determine the min- imum value mmin for the number of segments in the OSSMMmmin , such thatdsup(S;Mmmin) =dsup(S;M0p) for all itemsets S, i.e. the upper bound suffers no loss of accuracy compared to the initial p segments in M0p = hT 01; : : : ; T   pi.

Corollary 1 Let T be a collection of transactions for k items in p pages T 01; : : : ; T   p. If we allow these pages to be rearranged, then the minimum number of segments mmin required in the general case for dsup(S;Mmmin) =dsup(S;M0p) for all S, is the number of segments with dis- tinct configurations, i.e.,mmin ? minfp; 2k ? kg.

For academic interest, this corollary of Theorem 1 is a positive result because it indicates that all the previous anal- ysis carries over to the page version of segment minimiza- tion. However, from a practical perspective, this corollary is negative saying that even if we were to relax the notion of accuracy from a per-transaction basis to a per-page ba- sis, we will still end up with a huge number of segments for segment minimization. This motivates us to explore the constrained segmentation problem.

5. The constrained segmentation problem Recall that concerning the creation of an OSSM, there  are two issues: (i) the number of segments, and (ii) the com- position of the segments. The segment minimization prob- lem we have analyzed so far begins by focusing on having the minimum number of segments. In this section, we look  at these issues from a different angle. Here we study the constrained segmentation problem, as stated below.

Definition 3 Given a collection of transactions in p pages and a fixed number of segments muser ? mmin (and muser ? p) to be formed, the constrained segmenta- tion problem is to determine the best composition of the muser segments that minimizes the loss of accuracy.

We first introduce a way to quantify the loss of accu- racy when merging two segments of differing configura- tions. Given the properties of this quantification, we argue that finding the optimal solution is hard. Thus, in the rest of this section, we focus on developing different heuristics to solve the constrained segmentation problem. These heuris- tics will be compared in the next section.

5.1. Properties of merging segments of differing  configurations Given the p pages of the transactions, we assume with-  out loss of generality that they are all of different configu- rations. (If this is not true, then we can simply merge those segments with the same configuration into one combined segment, due to Lemma 1.) To reduce p to muser , there is no avoidance of merging segments with differing configu- rations together. Thus, in the following, we explore various aspects of this operation.

When it comes to merging segments with differing con- figurations, there exists one immediate complication ? namely, the resultant segment may have a totally different configuration. This can be seen by a simple example below.

Example 3 Suppose for 3 items, the configurations for segments T1 and T2 are (a ? b ? c) and (c ? b ? a) respectively. This implies that T1 satisfies the condition that sup1(fag) ? sup1(fbg) ? sup1(fcg), and T2 satisfies sup2(fcg) ? sup2(fbg) ? sup2(fag). But once these two segments are merged, the rank ordering of [sup1(fag)+sup2(fag)], [sup1(fbg) + sup2(fbg)], and [sup1(fcg) + sup2(fcg)] can be of any one of the 6 possible permutations.

Apart from this complication, we know from the earlier analysis that merging segments of differing configurations leads to loss in accuracy. To solve the constrained seg- mentation problem, we need to capture this loss precisely, leading to the definition of the following quantity. Let T = fT1; : : : ; Tvg be a set of segments with v ? 2. Then:  subop(T ) = X ai;aj  [dsup(fai; ajg;M01) ?dsup(fai; ajg;M0v)] (2)  The first term is the upper bound on sup(fai; ajg) based on M0  , which consists of 1 combined segment formed  by merging all v segments in T . The second term is the upper bound based on M0v, which keeps the v seg- ments T1; : : : ; Tv separated. The difference between the two terms quantifies the amount of ?sub-optimality? on the set fai; ajg to have the v segments merged. The quantity subop(T ) then sums over all pairs of items to measure the total loss.

The quantity subop() satisfies several natural properties, as stated in the following lemma.

Algorithm Greedy INPUT: p initial segments in T = fT1; : : : ; Tpg OUTPUT: T with onlymuser < p segments 0. initialize a priority queue 1. for each pair of segments T1; T2 2 T do  compute subop(fT1; T2g) and insert it into the priority queue 2. for (i = p downto m + 1) do 3. obtain the pair T1; T2 having the min subop() in the queue 4. merge T1; T2 to give Tnew = T1 [ T2 5. remove all pairs in the priority queue involving T1 or T2;  remove T1; T2 from T 6. for all remaining segments T 0 2 T do  insert subop(fTnew ; T 0g) into the queue, add Tnew to T  Figure 2. A skeleton of the Greedy algorithm  Lemma 2 Let T be a set of segments: (a) If all segments are of the same configuration, then subop(T ) = 0.

(b) If there exist at least two segments with differing con- figurations, then subop(T ) > 0.

(c) If T 0 is another set of segments such that T ? T 0, then subop(T ) ? subop(T 0).

While the above lemma shows that the quantity subop() is well-behaved, a complication with the constrained segmen- tation problem is that there are really too many possibilities to formmuser segments from p pages. The total number of possibilities can be enumerated exactly; but because it is not a closed form formula, we omit the details here. We only show below a simple example to illustrate the explosion.

Example 4 Suppose p = 5 and muser = 3. Then the total number of combinations is given by: (i) the number of combinations for one seg- ment to be of size 3 pages, and the other two of size 1 page each, plus (ii) the number of combinations for two segments to be of size 2 pages each, and the other segment of size 1 page. Thus, there are 25 possible combinations. For muser = 3, if p is raised to 6 and to 7, the number of combinations quickly jumps to 90 and to 301 respectively.

5.2. Heuristic algorithms for the constrained seg- mentation problem  Given p initial pages/segments, the constrained segmen- tation problem amounts to finding muser final segments to minimize the total subop() quantity. Partly due to the com- plication illustrated in Example 3, and partly due to the huge number of possibilities to make up the muser segments, the optimal segmentation is too expensive to be computed.

Thus, in the remainder of this section, we focus on develop- ing heuristic algorithms.

One obvious heuristic is the Greedy algorithm shown in Figure 2. Given the p initial segments, it first com- putes the subop(fT1; T2g) quantity for each pair of seg- ments T1; T2. Then the pair of segments with the mini- mum subop() value is selected and merged. However, the merged segment Tnew = T1[T2 may be of a different con- figuration (see Example 3). Thus, it is necessary to com- pute subop(fTnew; T 0g) for every remaining segment T 0.

Then the next pair of segments with the minimum subop() value is selected and merged, and so on. This continues untilmuser segments are obtained.

Algorithm RC INPUT: p initial segments in T = fT1; : : : ; Tpg OUTPUT: T with only muser < p segments 1. for (i = p downto m+ 1) do 2. pick a random segment T1 2 T 3. find the closest segment T2 to T1 , i.e.,  subop(fT1; T2g) = minfsubop(fT1; T 0g) j T 0 6= T1g 4. remove T1; T2 from T , but add (T1 [ T2) to T  Figure 3. A skeleton of the RC algorithm  Let us determine the complexity of the Greedy algo- rithm. According to equation (2), each computation of the subop() quantity for a pair of segments requires O(k2) ef- fort, due to O(k2) pairs of items to sum over. Because there are O(p2) pairs of segments to compute, the complexity of Step 1 is O(p2k2). For Step 2, each insertion and dele- tion from the priority queue takes O(log p2) = O(log p) effort. But for each iteration of the loop, the total effort is O(p? (k2+log p)). Because the loop in Step 2 is executed (p?m) times, the total complexity of the Greedy algorithm becomes O(p2k2 + p2 log p). Note that segmentation is a compile-time operation, i.e., it is done only once and the result can be used by many different mining queries.

Next we describe another heuristic method, the RC (Random Closest) algorithm for finding the muser seg- ments that minimize the total subop() quantity. See Fig- ure 3. During each iteration of the loop in the RC algorithm, a segment is randomly selected. Then the segment closest to it is found, i.e., in terms of the smallest subop() value.

Merging two segments that are close to each other is a nat- ural strategy because this merge does not incur a high level of ?sub-optimality?, i.e., it has a low subop() value. On this issue, the RC algorithm is similar to the Greedy algo- rithm. The key difference, though, is that the RC algorithm starts with a random segment, and does not require a prior- ity queue to be maintained to find the pair of segments that give the lowest subop() value. In terms of complexity, each iteration takes O(pk2) effort. Because the loop is executed (p ? m) times, the total complexity of the RC algorithm is O(p2k2).

From the Greedy algorithm to the RC algorithm, we re- lax in the latter algorithm the requirement in the former al- gorithm that the pair of segments with the absolute mini- mum subop() value be found. To go in the same direction one step further, we can even relax the requirement in the RC algorithm that the closest segment to T1 be found. This leads to the Random algorithm. It is almost identical to the skeleton shown in Figure 3, except that Step 3 is now replaced by a simple random selection of a segment T2.5  Before we dismiss this algorithm as too simplistic, there are two reasons why we include this heuristic in our discus- sion and experimentation. First, the Random algorithm is fast, with a complexity of O(p). So in our experimentation, it forms a good baseline for us to judge whether the other more sophisticated heuristics are cost-effective. Second, as  5Similar to the construction of the SSM structure [10], the Random algorithm constructs the OSSM by arbitrarily/randomly partitioning pages of transactions into segments.

will be shown very soon, the Random algorithm, because of its speed, can form part of a hybrid strategy for efficient segmentation.

5.3. The bubble list optimization: Dealing with the k2 factor  Note that in the Random algorithm, there is no need to compute any subop() value. This is why there is no k2  factor in the complexity of the Random algorithm. But in the other two algorithms, because selection of segments is based entirely on the subop() values, k2 becomes a domi- nant factor in their complexity. To eliminate the k2 factor, one heuristic is to find a constant number of items which are ?on the bubble?. These are the items whose frequencies barely satisfy, and are the closest to, the support threshold.

Then in the subop() calculation, we restrict the summation in equation (2) only over all pairs of items in the ?bubble list?. This is reasonable because the filtering offered by the OSSM is expected to be the most applicable to those items whose frequencies are closest to the threshold. For exam- ple, there may be k = 1000 items in all, but the bubble list may contain only w = 100 items. In this case, for each computation of subop(), the number of pairs considered in the summation reduces from (k  ) = 499 500 to (w  ) = 4950.

Note that the content of the bubble list depends on a support threshold. This bubble list then turns the focus of the segmentation squarely on the items in the list. But it is important to remember that while segmentation with the bubble list requires some support threshold, the OSSM pro- duced can be used for any support threshold. In the next sec- tion, we will evaluate the effectiveness of an OSSM that was produced with a bubble list based on one support threshold, but is used dynamically to handle different support thresh- olds.

5.4. Hybrid segmentation strategies: Dealing with the p2 factor  With the bubble list heuristic, the k2 factor disappears from the complexity of the Greedy and RC algorithms.

What about the p2 factor? As a heuristic to cope with situations when p is large, we offer hybrid segmentation strategies. Examples include Random-RC and Random- Greedy. That is, for a large initial value p, in the first phase we use the Random algorithm to merge the p pages to pmid segments (where pmid > muser). Then in the final phase, the RC or the Greedy algorithm is used to merge the pmid segments into themuser segments. In the next section, we will evaluate whether these hybrid strategies work at all.

6. Experimental evaluation In this section, we provide extensive experimental evalu-  ation of all the segmentation algorithms and heuristics pro- posed above. Specifically, we provide empirical evalua- tion on: 1. the speedup an OSSM provides at query execution or  exploration time; and 2. the segmentation cost of producing the OSSM, with or  without the bubble list and the hybrid strategies.

We conclude this section with a recommended recipe for different circumstances.

6.1. Experimental setup  The segment minimization and the constrained segmen- tation problems apply to the many algorithms that con- duct mining of various kinds of patterns based on candidate generation (see Section 1). The results reported here are based on a specific instance of these algorithms ? namely, the classical Apriori algorithm for association rule mining.

Nonetheless, the presented ideas and algorithms apply to other instances as well.

All the experiments were performed on a time-sharing environment on a 700 MHz machine. All the programs were written in C. There are two kinds of execution time.

The first is the segmentation time, which includes all CPU and I/O costs incurred for segmentation. The second is the runtime of Apriori with or without the OSSM, which in- cludes all CPU and I/O costs to find all the frequent sets.

The reported figures are based on the average of multiple runs. There are three data sets we used: 1. Nokia data set, which is a real data set from Nokia on  a sequence file containing about 5000 transactions of about 200 distinct types of telecommunications network alarms. For proprietary reasons, we cannot describe this data set further.

2. regular-synthetic data set, which is a synthetic data set generated using the program developed at IBM Al- maden Research Center [3]. The exact number of trans- actions is not important, because the key parameter is the number of pages p. In our experimentation, p varies from 200 to 50 000, and the number of items is k = 1000.

3. skewed-synthetic data set, which is a synthetic data set that has skewed ?seasonal? behavior. Specifically, 50% of the items have a higher probability of appearing in the first half of the collection of transactions, and the other 50% have a higher probability of appearing in the second half. The reason for using this data set is that in the real world, there are many databases that do not follow a regular or uniform distribution (e.g., a super- market database consisting of transactions over a few months from summer to winter).

6.2. Runtime speedup The first set of experiments evaluates whether the OSSM  provides significant speedup. The speedup for segmentation algorithmX is defined to be the ratio of the execution time of the Apriori algorithm without the OSSM, to that with the OSSM produced by algorithm X (where X is the Ran- dom, RC, or Greedy algorithm). This gives the y-axis of the graph in Figure 4(a). The x-axis shows the number of segmentsmuser allowed in the OSSM.

Segmentation algorithms aside, the first observation from Figure 4(a) is that the OSSM is a useful structure.

It is light-weight. For muser = 100 segments, and for 1000 items, the OSSM consumes only about 0.2 megabytes.

But it can bring about a speedup from around 7 times to about 22 times. And for muser = 150 segments (thus con- suming 0.3 megabytes), the speedup can be about 50 times.

The second observation is that as expected, the larger the           20 40 60 80 100 120 140 160  S pe  ed up  r el  at iv  e to  A pr  io ri  w ith  ou t t  he S  S M  Number of segments  "Regular" Synthetic Data  Greedy RC  Random  (a) Relative speedup   0.1  0.2  0.3  0.4  0.5  0.6  0.7  20 40 60 80 100 120 140 160  N um  be r  of c  an di  da te  -it  em se  ts r  el at  iv e  to A  pr io  ri w  ith ou  t t he  S S  M  Number of segments  "Regular" Synthetic Data  Random RC  Greedy  (b) Fraction of candidate 2-itemsets not pruned  Figure 4. Effectiveness of the segmentation algorithms  value of muser , the higher is the speedup. Interestingly, even the Random algorithm can offer a speedup that is bet- ter than an order of magnitude. In terms of the speedup, the rank in descending order is always the Greedy algorithm, the RC algorithm and the Random algorithm. The gaps be- tween them open up gradually asmuser becomes larger.

The speedup can be directly linked to the number of can- didates that require frequency counting, i.e., candidates that are not pruned based on equation (1). Figure 4(b) com- pares the number of candidate 2-itemsets required with or without the OSSM. Again the ratio is shown on the y- axis, the ratio 1 being the case without the OSSM. Clearly, the OSSM provides significant pruning. For example with muser = 150, and the OSSM produced by the Greedy al- gorithm, only about 3% of candidate 2-itemsets checked by Apriori ordinarily are not pruned by the information kept in the OSSM. Figure 4(b) only shows the reduction for can- didate 2-itemsets. The OSSM applies to candidate sets of higher cardinalities. Given our synthetic data sets, the re- duction for higher cardinalities is minimal. But this is not as negative as it may seem, because it is well known that the main bottleneck of the Apriori algorithm is on its process- ing of candidate 2-itemsets [15]; this is precisely the area where OSSM excels.

The results reported in Figure 4 are based on the regular- synthetic data set, and a support threshold of 1%. In addi- tion, we have experimented with other data sets mentioned  Pure strategy Segmentation time Speedup  Random 0:02? 0:00s 2:6? 0:1 RC 2791 ? 7s 5:5? 0:4  Greedy 5439 ? 6s 5:9? 0:3 (a) Pure strategies with p = 500  Hybrid strategy Segmentation time Speedup  Random-RC 521? 2s 4:3? 0:0 Random-Greedy 1051 ? 1s 5:2? 0:1  (b) Hybrid strategies with p = 50 000, pmid = 200  Figure 5. Segmentation costs: Pure and hy- brid strategies withmuser = 40  above and various support thresholds. Please refer to our technical report [12] for more details.

6.3. Segmentation cost  In light of the significant speedup offered by an OSSM, the natural question to ask is whether this speedup is achieved by a high ?compile-time? segmentation cost. The key parameters here are p and muser . While we experi- mented with many combinations, we only report some of them below, due to a lack of space. For example, Figure 5(a) reports the results for p = 500 andmuser = 40.

Effectiveness of the pure segmentation strategies: As ex- pected, both the RC and the Greedy algorithms take a long time to produce the OSSM. But is it too long? Is the ad- ditional one-time cost of 5439 seconds worthwhile to in- crease the speedup from 2.6 to 5.9 for each mining query?

The answer to this question is subjective, depending on the relative importance of segmentation cost to dynamic query execution cost for the specific application. It also depends on the amount of space the OSSM can occupy, i.e., the value of muser . In figures, we deliberately take a smaller valuemuser = 40 to allow the segmentation process to take longer. If the application can afford a larger value ofmuser , the segmentation cost drops while the speedup increases.

In this case, the choice becomes more obvious ? namely, it pays off to use a more elaborate segmentation algorithm to produce a higher-quality OSSM.

Effectiveness of the hybrid segmentation strategies: Given the numbers in Figure 5(a), a natural question to ask is how well the more elaborate algorithms scale up with re- spect to p. If it takes 5439 seconds for the Greedy algo- rithm for p = 500 pages, how long does it take for p = 50 000 pages? (For a page size of 4 kilobytes, each page can contain roughly 100 transactions. Thus, 50 000 pages correspond to 5 million transactions.) This is an important question because data mining applications are supposed to have a huge number of transactions (and pages).

Fortunately, the Random algorithm comes to the rescue.

Recall from Figure 5(a) that the Random algorithm takes a negligible amount of segmentation time, but still delivers reasonable speedup. This opens the possibility of hybrid segmentation strategies for large values of p. Figure 5(b) shows the situation for the hybrid strategies Random-RC and Random-Greedy. More specifically, the initial value of p is 50 000. The Random algorithm is used to quickly re- duce p to pmid = 200 segments. From that point on, either     0.1      0 10 20 30 40 50 60  S eg  m en  ta tio  n co  st (  in s  ec on  ds )  Size of bubble list (in percentage of domain items)  "Regular" Synthetic Data  Random-Greedy Random-RC  (a) Segmentation time   3.5   4.5   0 10 20 30 40 50 60  S pe  ed up  r el  at iv  e to  A pr  io ri  w ith  ou t t  he S  S M  Size of bubble list (in percentage of domain items)  "Regular" Synthetic Data  Random-Greedy Random-RC  (b) Speedup  Figure 6. Effectiveness of the bubble list op- timization  the RC or the Greedy algorithm is used to select the final muser = 40 segments.

Notice that for Random-RC, the total segmentation time for 50 000 pages is only 521 seconds, as supposed to 2791 seconds for only 500 pages using purely the RC al- gorithm. Yet there is a minimal drop in speedup. Similar observations can be obtained for Random-Greedy. In gen- eral, given a large initial p value, a good strategy is to use the Random algorithm to reduce p to a much smaller pmid value (e.g., between 100 to 500 pages), and then to use an elaborate algorithm to complete the segmentation.

Effectiveness of the bubble list optimization: Apart from the hybrid strategies, we also propose the bubble list heuris- tic, with the objective of focusing the computation to the subop() value only to those items that are on the bubble list. Figure 6 shows the situation when the bubble list was formed based on the support threshold 0.25%, and yet dur- ing query evaluation, the actual support threshold is 1%.

The x-axis shows the varying size of the bubble list, ex- pressed as the percentage of k, the total number of items in the domain. The key observation is that the segmenta- tion cost is drastically reduced. For example, the Random- Greedy hybrid strategy with the bubble list can produce an OSSM in about 10 seconds of total time, for 5 million trans- actions (i.e. 50 000 pages), as opposed to 1051 seconds for that without the bubble list (cf.: Figure 5(b)). A similar reduction applies to the Random-RC hybrid strategy. This  H H H H H HH  ? ? ? ? ?  ?? H H H H H HH  ? ? ? ? ?  ?? H H H H H HH  ? ? ? ? ?  ??  Random  Random-RC Random-Greedy  Greedy  large muser and skewed otherwise  segmentation cost an issue otherwise  very large p otherwise  Figure 7. Recommended recipe  shows that the bubble list heuristic is very effective in re- ducing the segmentation cost.

Figure 6(b) shows that even though the segmentation time is significantly reduced, the quality of the OSSM pro- duced by the hybrid strategies is not compromised signif- icantly. Furthermore, even though the support threshold used during segmentation is different from the one used at query execution time, the speedup offered by the OSSM is still significant. As expected, the longer the bubble list, the higher is the quality of the OSSM, and thus the speedup.

6.4. Summary: A recommended recipe In sum, we have provided extensive experimental results  evaluating the various proposed segmentation algorithms and heuristics. Figure 7 shows a ?recipe? we recommend for deciding on which segmentation algorithm to use for various kinds of applications. First, if the application can afford a lot of space for the OSSM (i.e., muser is large), and the data is skewed, the Random algorithm, which is the simplest, is sufficient for segmentation. Otherwise, if the segmentation cost is not an issue at all, the Greedy al- gorithm with the bubble list is the choice. However, if the number of initial pages p is large, then we recommend either the Random-RC or the Random-Greedy algorithm, with the bubble list.

7. Discussion In previous sections, we have shown how the OSSM  helps to improve the efficiency of the Apriori algorithm (an instance of pattern discovery algorithms). Being a generic data structure, the OSSM can be equally applicable to the discovery of sequential patterns, episodes, constrained fre- quent sets, etc. As mentioned in Section 2, the OSSM tech- nique is rather different from the DHP algorithm [15] and the Partition algorithm [17]. But an observant reader may wonder whether the OSSM provides better pruning than the two existing algorithms. However, this is the wrong ques- tion to ask because the OSSM can be made to work in con- junction with the two algorithms.

Recall that the DHP algorithm hashes a k-itemset (e.g., k = 2) to a bucket, which may eventually be pruned due to an insufficient number of itemsets being hashed into such a bucket. However, if an OSSM is used simultaneously, then known infrequent k-itemsets are not generated in the first place. Itemsets that pass through the pruning by the OSSM can now be further pruned by the DHP algorithm. A pre- liminary result, presented in the following table, shows the     additional benefit brought by an OSSM (constructed using the Random-RC algorithm with m = 40 segments) to the DHP algorithm with 32 768 buckets.

Algorithms Runtime No. of C2 DHP without the OSSM 4:01? 0:13s 292 DHP with the OSSM 1:96? 0:01s 142  Here, when the DHP algorithm is used in conjunction with the OSSM, the number of candidate 2-itemsets (C2) is about half and the speedup is about 2 times (when compared to the DHP algorithm without the OSSM).

Similarly, the OSSM can bring additional benefits to some other algorithms. For lack of space, we briefly dis- cuss below how the OSSM can be applicable to the Partition algorithm [17] and to the DepthProject algorithm [1].

For the Partition algorithm, if an OSSM is built for each partition, the execution time for each partition will be sig- nificantly reduced because known local infrequent itemsets are pruned by the OSSM. To improve the performance fur- ther, if the OSSMs for all the partitions are available, then many of the global candidates (i.e., itemsets that are locally frequent in a partition), which are known to be globally in- frequent with respect to the OSSMs, can in fact be pruned.

Recently, a pattern discovery algorithm, called Depth- Project, was proposed to generate long patterns by using depth-first search on a lexicographic tree of itemsets. More precisely, at each step, the algorithm generates possible fre- quent lexicographic extensions (i.e. candidates) of a tree node and tests for frequency. If an OSSM is used simul- taneously, then known infrequent candidates can be pruned before the frequency counting.

8. Conclusions In this paper, we proposed a light-weight structure called  optimized segment support map (OSSM). In addition to im- proving the pruning in pattern discovery algorithms, it also provides direct information about the variability of frequen- cies in different segments of the transactions. Unlike many algorithms which cannot handle skewed data, the strength of the OSSM is to exploit the variability. Concerning the OSSM, we studied two main problems: (i) the minimum number of segments for an OSSM to incur no loss in accu- racy (the segment minimization problem), and (ii) the best composition of the segments given a user-determined num- ber of segments (the constrained segmentation problem).

For the first problem, we provided a thorough analysis and hardness results, showing that no loss in accuracy using the OSSM requires too many segments in general. For the second problem, we developed the heuristic segmentation algorithms called the Random, RC, and Greedy algorithms.

To further reduce segmentation cost, we proposed optimiza- tions that use a bubble list and run the algorithms in a hybrid fashion. Our experimental results are strong indicating that for a small OSSM (e.g., 0.3 megabytes), the speedup can be very significant (e.g., 50 times), yet the segmentation cost is small (e.g., 10 seconds for 5 million transactions). We concluded by presenting a recommended recipe for various  circumstances. While our experiments were based on find- ing frequent sets using the Apriori algorithm, it is impor- tant to remember the generality of the OSSM: It is applica- ble to many pattern discovery algorithms (many of which are listed in the introduction). We expect the OSSM to be equally effective for those algorithms.

