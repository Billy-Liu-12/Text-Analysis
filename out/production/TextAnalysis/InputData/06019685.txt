

Abstract?This paper points out the bottleneck of classical Apriori?s algorithm, presents an improved association rule mining algorithm. The new algorithm is based on reducing the times of scanning candidate sets and using hash tree to store candidate itemsets. According to the running result of the algorithm, the processing time of mining is decreased and the efficiency of algorithm has improved.1    Keywords- Data mining; association rule; Apriori algorithm; frequent  itemset

I.  INTRODUCTION Mining association rule is one of main content of data  mining research at present, and emphasizes particularly on finding the relation of differ items in the database. It was first proposed by R. Agrawal et al.

The basic concept is described in Table 1:  Name Explain Formula  Confidence Probability of set Y appear only if X appear P(Y|X)  Support Probability of set X and Y appear simultaneity P(Y?X)  Expected Confidence Probability of set Y appear P(Y)  Lift Ratio of confidence to expected confidence P(Y|X)/P(Y)  Table 1 Basic concept of association rules   1 Supported by Science-Technology Development Fund Project in Colleges and Universities of Tianjin Education Commission (No.20080817) 2 Supported by Research Project of University of International Business and Economics (No.10QD07)  Our purpose is searching for association rules with the smallest support degree and the smallest confidence degree.

The classical algorithms can mine those rules by scanning database repeatedly. Its efficiency is very low.



II. ASSOCIATION RULES DESCRIPTION   Given a transaction database DB, I={i1, i2, i3?in} is a set of items with n different itemsets in DB, each transaction T in DB is a set of item (i.e.itemsets), so T? I.

? Definition 1: Let I={i1, i2, ? , in} be a set of items, then D={ <Tid, T>| T? I} is a transaction database, where Tid is an identifier which be associated with each transaction.

? Definition 2: Let X? I, Y? I, and X?Y= ?, we called X?Y as association rule.

? Definition 3: Suppose c is the percentage of transaction in D containing A that also contain B, then the rule X?Y has confidence c in D. If s is percentage of transactions in D that contain A?B, then the rule X?Y has support s in D.

? Definition 4: Hypothesis X? I, minsup is the smallest support. If the frequency of X is greater than or equal to minsup, then X is called as frequent itemset, otherwise X is called non-frequent itemset.

Classical Apriori algorithm:  (1) C1 = {candidate 1-itemsets};  (2) L1 = { c? C1 |c.count?minsup}};  (3)   for (k=2; Lk?1??; k++) do begin  (4)      Ck=apriori-gen(Lk-1);  (5)      for all transactions t?D do begin     (6)         Ct=subset(Ck,t);  (7)          for all candidates c? Ct  do  (8)               c.count++;  (9)      end  (10)     Lk={c ?Ck |c.count?minsup}  (12)  end  (13) Answer=?Lk;    The association rule mining is a two-step process:  ?   Find all the frequent itemsets in the transaction database. If support of itemset X, support (X) ? minsup, then X is a frequent itemset. Otherwise, X is not a frequent itemset.

?   Generate strong association rules from frequent itemsets. For every frequent itemset A, if B?A, B??, and support (A)/support (B) ? minconf, then we have association rule B? (A-B).

The second step is relatively easy, and its algorithm generation can be found in the reference  The present focuses in research are to find highly efficient algorithm in the first step. In this paper, we discuss how to mine association rules quickly.



III. THE ALGORITHM ANALYSIS AND IMPROVE   In Apriori algorithm, from Ck to Lk have two steps process:  ? Pruning itemsets according to Lk-1  ? Pruning itemsets according to missupport  The question of Apriori algorithm is that the set of candidate itemsets Ck is generated from Lk-1. Every itemset in Ck is tested whether its all k-1 subsets constitute a large k- 1itemset or not, if one of k-1itemsets is not in Lk-1 itemsets, the super itemset of this k-1 itemset can be deleted. That is, every time a k itemset is constituted, Apriori must scan all Lk-1 itemsets.

Analyzing the process of mining rules, we find out a way to reduce the times of scanning frequency itemsets Ck.

The theory can be described as below:  ? Theorem 1: Support X?T, Y?T, and X?Y. If Y is a frequent itemset, then X must be a frequent itemset.

That is (?X)(?Y)((Y?T ? X?Y ? Y.count ? mincount)?(X.count ? mincount ) ).

Where X.count and Y.count stands for X?s count and Y?s count in the database, and mincount is the minimum support degree count.

? Proof: Let T={t1,t2,?,tn} be records of D. According to the definition of the minimum support degree, we get the formulation mincount=minsup?|D|. Since Y is a frequent itemset, there are at least mincount records included Y. Since X?Y, there are at least mincount records included X from the transitivity of the inclusive relationship. So, X.counter ? mincount. That is, X must be a frequent set.

? Theorem 2: Given X?T, Y?T, and X?Y. If X is a non-frequent itemset, then Y must be a non-frequent itemset. That is (?X)(?Y)((Y?T ? X?Y ? X.count < mincount)?(Y.count < mincount ) ).

? Proof: This theorem can be proved using disproof.

Suppose Y is a frequent itemset, that is, Y.count ?mincount. Because of X?Y, X must be a frequent itemset by theorem1, that is X.count ?mincount. This is contradictory with X.count < mincount. Therefore, if X were a non-frequent itemset, Y must be non-frequent itemsets.

According to the above theory, when set of candidate itemsets Ck is generated, we scan the Lk-1 itemset only one time and test whether any Lk-1 item X is subset of item Y in Ck or not, if true, count the number of Y itself. That is, in Lk-1 itemset we statistic the k-1subset number of any item Y in Ck, then reduce the item in Ck according to the counting result. If the subset number of Y in Lk-1 is less than K, Y can be deleted from Ck. For example, let Lk-1={AB, AC, BC, AE}, then Ck={ABC, ABE, ACE}, the k=3, we can contribute large itemsets L3 by scan L2 one time. The first item AB is subset of ABC and ABE, so record number in array is {1,1,0}, item AC is subset of ABC and ACE, the array becomes {2,1,1}, finally, after scan BC and AE, the array is {3,2,2}. Only item ABC has three subsets, so it remains in C3, and ABC, ACE can be deleted.

Support |Lk-1| is the itemset number in Lk-1, and p?|Ck| express the itemset number in Ck, every item in Ck has m subsets(m=1..|Ck|). As we know, from Ck to pruning Ck  classical Apriori algorithm need ? ? = =  ?  |k|  1i  m  1j  c |)L(| 1K  times  account, but new algorithm only need p?|Lk-1| times..

Another bottleneck of Apriori algorithm:  Every item in Ck need to be tested whether it can append in Lk, this process is another bottleneck   of Apriori algorithm.

This processing of counting the candidate itemsets spend most of the processing time. We can count candidate itemsets number quickly by contributing a hash tree data structure.

We use the hash tree to store Ck, every node of tree includes a itemset or a hash table (called inner node). Every hash table points to another node. The depth of root node is defined 1. A     node?s depth is d+1, which is in d layer. Itemsets are stored in those leaf nodes.

Suppose n k-itemsets and average size of matter is t, matching one matter and a candidate itemset need time O(t*k).

If this matter matches all candidate itemsets, need time O(n*t*k). After contributing a hash tree, the matching time only need O(n*k), because of the tree height is no more thank.



IV. EXPERIMENT RESULTS   We use anonymity user visiting log records of www.microsoft.com to be testing data source. In this anonymity web database, we choose records of user accessing website in one week.

? Data snippet:  A,1287,1,"International AutoRoute","/autoroute"  A,1288,1,"library","/library?C,"10164",10164  V,1123,1  V,1009,1  ? Result of association rules mining (part 1):  minSupport: 0.3  minCofidence : 0.7  Association Rules:  : ({CS610} => {CS611} / 0.7727272727272727)  : ({CS611} => {CS610} / 0.8429752066115702)  : ({CS610} => {CS612} / 0.7727272727272727)  : ({CS612} => {CS610} / 0.8360655737704918)  : ({CS610} => {CS611} / 0.7727272727272727)  : ({CS613} => {CS610} / 0.8329452057536782)  ???..

Time: 2281  ? Result of association rules mining (part 2):  minSupport: 0.04  minCofidence : 0.15  Association Rules:  {1001}=>{1018} 4.8057227%  35.317905%  {1008}=>{1009} 7.6640887%  23.135843%  {1008}=>{1018} 5.9154415%  17.857143%  {1001}=>{1018} 4.8057227%  35.317905%  {1008}=>{1009} 7.6640887%  23.135843%  {1008}=>{1018} 5.9154415%  17.857143%  ????..

Time: 351722  Choose confidence 50%, 70%, and analyze curve of processing time with the support change. It is obvious that, run time is reducing gradually when the support increases. It show new algorithm is constringency and stability. (Figure 1, Figure 2)     Figure1  Time change curve 1     Figure2  Time change curve 2    Figure3, Figure4 shows mining of the database with association rule and frequency itemset records by the new algorithm with different minimum support degree, the vertical axle shows the number of reducing records in same database.

The unit is one. The horizontal axle shows 4 different support degrees (in percentage).

Figure3 Reduced association rule records     Figure4  Reduced frequency itemset records    In order to compare mining process of the database, we used the identical the minimum confidence degree (15%) in the Figure5, Figure5 shows that the new algorithm is more effect than Apriori in mining databases, the horizontal axle shows support degree and the vertical axle shows the time of mining (the unit is second).

Figure5  Compares Apriori with new algorithm

V. CONCLUSIONS  In this paper, we analyze the Apriori algorithm of mining association rule according to the property of frequent items and transaction database, set forth 2 theorems of reducing the database and then put forward the new algorithm. In our test, the new algorithm is superior to the classical algorithm.

According to the experiment?s results, the efficiency of improved algorithm is better and has abroad applications in data mining.

