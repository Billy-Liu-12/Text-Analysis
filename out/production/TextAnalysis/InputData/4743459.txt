Video Event Modelling and Association Rule Mining in  Multimedia Surveillance Systems

Abstract  Event representation models provide a framework in which we can reason about events so as to interpret the collective behaviour of objects over time and space domains. Many are context-specific and lack flexibility when faced with unstructured video. This paper proposes a comprehensive event modelling framework for multimedia surveillance systems. An event detection model incorporates multimedia strings and a new predicate set for describing more complex event scenarios. Event classification performance is evaluated on benchmarked datasets.

1 Introduction  Events occurring in observed scenes are one of the most important semantic entities that can be extracted from surveillance videos. Many event recognition algorithms have been proposed in the literature [8,9,10]. However, the various event representations used are quite diverse and most systems represent video events as isolated entities rather than capturing the relations between them in a structured ontology.

Hence, there is a need to design an efficient framework which not only allows semantic annotation of video events but provides methods to store, retrieve and mine interesting information captured therein [1].

Semantic event modelling provides a framework in which we can reason about events so as to interpret the collective behaviour of objects over time and space domains.  This presupposes the existence of reliable low-level video processing techniques used for event detection. Such techniques include background modelling, object detection and labelling, shadow suppression, tracking and trajectory based behaviour recognition [2]. Users can browse and retrieve events of interest in a multimedia information system [20]. It is important to provide a query language capable of expressing complex semantic events alongside their spatio- temporal properties. The system should also provide browsing capabilities to enable navigation between event and object- centred views. The discovery of previously unknown events should be possible by applying different mining techniques.

In this paper, we propose a comprehensive event detection and modelling framework for multimedia surveillance video.

The performance is evaluated with benchmarked datasets in the context of our framework. Finally, we demonstrate the exploration of hidden knowledge in detected events using association rule mining.

2 Previous work  There is considerable work aimed at defining a representation model for describing video events [3-15]. Although many such models provide a framework to identify key semantic entities like objects, events and the specificaton of properties and relations between them, they are often context dependent and lack flexibility.

The use of stochastic context free grammars for describing events was proposed in [8]. The hierarchical decomposition of events into separate threads was described in [9]. A similar approach was presented in [10] by specifying the characters involved in the scenario. A tool set based on MPEG-7 was introduced for describing semantics in multimedia content [12]. While this work identifies key semantic entities and allows users to specify properties and relations between them, it does not fully explore the concept of structuring of events.

In [15], complex events are expressed in terms of primitive events and a set of predicates are provided for describing temporal relationships between them.

The work presented here builds on the EDF approach proposed in [1]. Here an event instance is viewed as the assertion that there exists a particular event of a given type with various roles in which other individuals participate.

Similar approaches are found in [11] and [15]. The advantage of EDF is that it provides a single template predicate for representing all events instead of defining a predicate for each event type. It also provides a set of predicates for describing spatial-temporal relationships between events and entities.

3 Multimedia semantic event modelling framework  3.1 Limitations to overcome  EDF [1] only describes the video content of a multimedia stream. However, any SEM framework should be capable of modelling supporting multimedia strings as well, e.g. audio,  c?2008 The Institution of Engineering and Technology Printed and published by the IET, Michael Faraday House, Six Hills Way, Stevenage, Herts SG1 2AY  426 VIE 08    text and static image. Although EDF provides a set of predicates for temporal relationships, it does not cater for different granularities of time interval. In addition it does not provide a mechanism to update/store a set of entities in the system. The number of entities can be large and it is prohibitive to store manually such entities. We address this problem using association rule mining between different event entities and supporting multimedia content such as text/audio.

In visual surveillance, there is a need to define virtual scene entities. These entities are not real observable objects but provide contextual scene information as a backdrop in which the events will take place. Two such examples are region of interest (ROI) and tripwires.

We have also introduced a new set of predicates for defining more complex events. In the following section, a set of classes for semantic annotation of multimedia data are described along with their properties and relationships. We then present a set of predicates for describing various relationships between events and entities. Finally, we present an example to illustrate the concept.

3.2 Entities  Entities are basic objects such as car, person, chair observable in a particular domain. Each of these entity classes are subclasses of the parent classes entity, e.g. wine bottle is sub-entity class of parent entity bottle. Thus, for each specific domain we will have hierarchy of entity classes.

Let PE be a set of parent entities PE={PE1, PE2?.. PEn}, each PE can have one or more sub entities class SE.

PE={PE1, PE2?.. PEn}, PEn={SE1, SE2?.. SEn}  (1)  These entity classes can have various properties associated with them and can also inherit the properties of its parent class entity; for example wine bottle can have colour, shape, and size as its image features.

SEn={F1=value, F2=value?? Fn=value}  (2)  where Fn is specific feature of an entity.

In addition to the entities reflecting real life objects, we introduce the user defined virtual entities;. These can be manually annotated using a graphical interface.

UDVE={UDVE1, UDVE2, UDVE3?.. UDVEn}  (3)   UDVEn ={F1=value, F2=value?? Fn=value}  (4)  For example, features of a tripwire are spatial locations of the endpoints  Two more classes are introduced to represent the supporting text/audio multimedia streams.

TE={TE1=value, TE2=value?.. TEn=value} (5)   AE={AE1=value, AE2=value?.. AEn=value} (6)  3.3 Action  An Action class refers to actions such as enter, leave, run, walk, that occur in a specific domain. Like entity classes, action classes can be organised in a hierarchy and also have features associated with them. Further, they have a patient specification which describes the entity towards which the action is directed to, e.g. a wine bottle passes over the tripwire1 where tripwire1 is the patient.

To handle advance spatial queries the concept of target object (TOBJECT) used in the Augmented Transition Networks (ATN) model [25] is adopted. TOBJECT is used as a reference object to store spatial/temporal information about all the entities in a specific event. It is important to see that each action will have at least one object entity associated with it. For example, enter action needs two entities (object which is going to enter and the place as Tripwire1 in which that specific object is going to enter)  3.4 Events  Events are divided into two main categories, that of simple events and composite events.

Simple Events (SEVENT): These are basically (Actor, Action) tuples, where actor is a set of entities that initiate the event and action is a set of actions performed during the course of the event. For example in the event of ?Wine bottle passes over tripwire1?. Wine bottle and Tripwire1 are entities and enter is the action.

Actor={E1,E2?.En}  (7) SEVENT=  ({Actor1,Actor2?.Actorn},Actionn, STIME, ETIME, TOBJECT )  Composite Events (CEVENT): Composite events combine two or more simple events and are specified using the predicate PROCESS whose first argument is the event being defined and whose second argument is the composition of other events. In addition to the predicates defined in [1], we have introduced two additional predicates (NOT IN, TERMINATE) which are useful in visual surveillance. The predicates are:  Predicate={SEQUENCE,AND, OR, NOT IN, TERMINATE}  SEQUENCE ? represents a set of events which happen one after an other is a temporal sequence AND ? represent a set of events with no particular temporal relationship between them.

OR ? represent a set of alternate events of which at least one should occur      As we have evaluated the video contents in detail, it was identified that these three predicates cannot comprehend all the events nature; therefore we introduce following two predicates as well.

? NOT IN: represent set of events which should not appear  between sequence of other simple events.

? TERMINATE: represent when the event detection  should be terminated  In complex events there can be multiple entities of same type at different time and space. Therefore, while modelling complex events the proposed framework should be able to reference entity which has been already defined in that event.

We call such entity as INTERNAL entity and referred it with its ID.

3.5 Temporal Association Framework  Different predicates were proposed in [1] for defining temporal relationships between two events (e.g. met-by, meets, finishes, finished-by, started-by, starts, during, after, before, overlaps, overlapped-by, contains, simultaneous).

Each of these predicates has two arguments and they can be either time intervals or events. Although these predicate cover most of the temporal relationship to be found in video events, it does not provide a mechanism to define different granularity of temporal associations. We therefore provide granularity as a feature of these predicates. Moreover, we introduce two new predicates which can be extensively used in events that are minimum gap (min-gap) and maximum gap (max_gap) with given granularity and its value.

TemporalPredicates=(Predicat1,Predicate2?. ??Predicaten) Predicaten(SEVENT/CEVENT,(G1=value,G2=value?.Gn=value))   We use the framework defined in [1] to classify spatial relationships into three types: topological, directional and metric.

3.6 Event modeling: an example  We now provide an example of a typical surveillance event in a retail store. This example is shown in Fig. 1 using the event modeller interface.

An object passes over the green tripwire and then passes over the white tripwire within 03 seconds.

SEQUENCE (Event1, Event2), Actor(Event1, class = object, size>=300, ID=E1), Action(Event1, ?Enter?, class=GreenTripwire,ID=E2), Actor(Event2, class = INTERNAL, ID=E1), Action(Event2, ?Enter?, class=WhiteTripwire,ID=E3), G=Second, value<=3, TOBJECT= E1)   .

Figure 1.  Event modeller interface  4 Event Detection Process  In addition to low-level object detection and tracking algorithms, we introduce three new concepts which have shown a positive impact on the overall event detection process.

As objects can change their appearance during events, simple colour or shape matching techniques cannot provide reliable results. To overcome this problem we utilise adaptive histogram mechanism presented in [24]. The main idea is to keep updating the object appearance model for each frame during event detection process.

Multimedia text streams are included for improving object tracking and background updating schemes, e.g. system should only update background between two given text strings (EndTransaction, StartTransaction). Supported multimedia streams can also be used to modify different parameters for the object tracking model as well.

Due to the sheer volume of video data, it is important that an event detection process should only store relevant information subject to the constraint that full evidence of detected event is provided as backup. As there is no prior information available as to when a specific event is triggered, the challenge is how to optimise the event storing mechanism. This can be overcome with an event buffering process, the main concept is to buffer specific number of frames and only store them permanently if specific event is triggered (this provides vital evidence just prior to the event being triggered).

Virtual Entities  Draw ROI Tripwires  Add Library Object Upload/generate new object  Temporal Properties Spatial properties     5 Experiments  The proposed framework for event modelling and detection is evaluated using sequences generated in laboratory (Fig. 2a) and also from realistic surveillance environment (Figs. 2b-d).

We used a static camera in all cases, although conditions differed significantly between views (lighting, camera angle, background variation).

(a)  (b)   (c)  (d)  Figure 2. Sample shots from each test scenario. Datasets are (a) D1, (b) D2, (c) D3, and (d) D4.

The experiments were conducted by formalising 4 different events with the help of the event modeller interface (Fig. 1).

For each event, the detection algorithm was applied to all test scenarios and the results were compared against ground truth.

5.1 EventID=1  Object (minimum size of 300 pixels) crosses entry line then received text stream and then same object crosses exit line within 3 seconds  SEQUENCE (Event1, Event2), Actor (Event1, class = object, size>=300, ID=E1), Action(Event1, ?Enter?, class=EntryLine,ID=E2),  TE=TRUE, Actor(Event2, class = INTERNAL, ID=E1), Action(Event2, ?Enter?, class=ExitLine,ID=E3), G=Second, value<=3, TOBJECT= E1    95.22  70.25 79.29  0.77 1.83 1.28    a b c  MultiMedia Datasets  Accuracy% False Detection%   Figure 3. Classification results for EventID=1 with datasets D1-D3.

Each dataset contains 150 different events in total, where  D1=20, D2=11 and D3=30 events that satisfy EventID=1 respectively.

5.2 EventID=2  A black colour object (minimum size of 200 pixels) crosses entry line and same object crosses exit line within 4 seconds  SEQUENCE (Event1, Event2), Actor (Event1, class = object, size>=300, Colour=Black, ID=E1), Action(Event1, ?Enter?, class=EntryLine,ID=E2), Actor(Event2, class = INTERNAL, ID=E1), Action(Event2, ?Enter?, class=ExitLine,ID=E3), G=Second, value<=4, TOBJECT= E1)   89.25   1.02 3.76    a b  MultiMedia Datasets  Accuracy% False Detection%   Figure 4. Results for EventID=2 with datasets D1, D2.

Each dataset contains a total of 500 item scans, out of which 10 scans from each dataset do not have an associated text stream.

5.3 EventID=3  Wine bottle crossing entry line and receiving text stream within 2 seconds  Event(Actor(class=WineBottle, size>=300, Colour=Black, ID=E1),Action(?Enter?,class=EntryLine,ID =E2),TE=TRUE, G=Second, value<=2, TOBJECT= E1)   97.11 79.25  0.87 4.25    a b  MultiMedia Datasets  Accuracy% False Detection%   Figure 5. Results for EventID=3 with datasets D1, D2.

5.4 EventID=4  Two objects of same type (minimum size of 400 pixels) are in the scene for more then 30 seconds     Event ( Actor(class=object, size>=400, ID=E1), Actor(class=INTERNAL, ID=E2), Action(?Remain?, class=E1, class=E2), G=Second, value=>30)   97.82 90.56  1.24 2.36        MultiMedia Data(a) MultiMedia Data(d)  MultiMedia Datasets  Accuracy% False Detection%   Figure 6. Results for EventID=3 with datasets D1, D4.

Each dataset contains a total of 20 different events; out of which 3 events matches EventID=4.

5.5 Performance Evaluation  There is little improvement in true detection rate by lowering the object matching threshold, however the false detection rate increases significantly (Fig. 7). Emprirically, a threshold of 0.7 produces the best results.

1 2 3 4 5 6 7 8  Hours  Accuracy % True Event Detection% False Event Detection   Figure 7. Event classification rates with object matching threshold=0.70  6 Multimedia video mining  Video data can be divided into two main categories ? structured (with content) and unstructured. The former include edited programs such as sports broadcast, movies and news. For unstructured "raw" video, e.g. visual surveillance, there is none or little content structure. Work presented in [16-23] provide alternative frameworks for indexing and mining structured/edited video content using both low level and semantic information.

Although mining unstructured data is a complex process, it can yield useful information that has broad applications in different domains including customer monitoring, marketing strategies, retail loss prevention, crowd management and public transportation systems monitoring. In [26,27], video mining efforts were concentrated on unstructured data.

However, only low level information (motion intensity and location) was used for the indexing and summarisation process. We propose to apply association rule mining on two different aspects of video semantic events.

Exploring association rules among different multimedia streams  Discover hidden knowledge based upon association among detected events.

In our work we have applied association rules on unstructured videos from two different perspectives.

6.1 Association rule mining among entities of events  By applying association rule mining algorithm on text string and detected objects, we propose a model which can automate the process of storing appearance model of real world entities.

The main notion of the process is to first define few simple events for a specific domain and then during event detection process store the entity features and corresponding text string.

Once sufficient data has been collected (depending upon the number of events and entities), detected objects associated with predefined events and text string entity are modelled into a transactional database structure to be applied for association rule mining. As object appearance can differ due to changes in environmental conditions, such as different lighting condition during day and night. Therefore by utilising temporal information during discovery of association rules, we can yield more effective association rules.

Object ID  Feature 1 (Colour)  Feature 2 (Size)  Text String TimeStamp  1 Black 202 Cola bottle 13:05:11 2 Black 300 Cola bottle 16:01:13 3 Black 400 Wine bottle 14:13:11 4 Grey 150 Cola bottle 13:45:31 5 Black 1000 Tissue Paper 18:15:00 6 Red 200 Toothpaste 22:51:11 7 Black 299 Cola bottle 14:35:55 8 Black 399 Cola bottle 20:35:55  ?? ?? ?.. ?.. ?..

N Blue 250 Cola bottle 16:30:01  Table 1.  Multimedia dataset for association rule discovery  By applying association rules on objects and text strings, we can discover following rule for determining the likelihood that a given object is a Cola bottle expressed as   Cola Bottle Object(Colour=Black, Size>200<300, Time(>1:05pm<4:31pm)) With   support=66%, confidence=83%  6.2  Association rule mining among different events  By applying association rule mining on detected events, previously unknown interesting events can be discovered.

This is useful when the events we are trying to detect in a video segment are not known a priori. This is often the case in visual surveillance where much video consists of long stretches of repetitive or ?uninteresting? parts occasionally interrupted by unusual or ?interesting? parts, which are too diverse to be anticipated in advance. Moreover, association rules among complex events can also reveal hidden     information which can be very useful for indoor marketing and administration activities.

7 Conclusion  In this paper, we have described a video event modelling, detection and mining framework for multimedia surveillance systems. The following contributions have been made:   An extension and modification to the semantic event modelling framework presented in [1] has been proposed to increase its capability to store events in any surveillance environment.

The utilisation of multimedia stream (text) for enhancing object tracking, background modelling, video shot and clip detection.

An automated mechanism to store real world entities by utilising association rule mining.

Adaptive appearance modelling to support object tracking (since object features can change during the tracking phase)  Mining association rules among event objects and among events themselves.

In future work we will concentrate on implementing association rule mining among multimedia events and among different multimedia streams in unstructured video.

