Parallel Data Reuse Theory for OpenMP Applications

Abstract  The development of multi-core processor technology makes parallel programming become more and more pop- ular. Similar to serial programs on single-core platforms, the locality optimization of parallel programs is and will be a hot-spot of research owing to the memory wall problem.

In this paper, we extend the famous data reuse theory to parallel domain and propose parallel data reuse theory for OpenMP applications. The parallel data reuse theory further classifies the reuse in parallel programs, from four classes to eight. This paper systemically discusses the intra-/inter- iteration reuse and intra-/inter-processor reuse in OpenMP programs, and gives the judging and solving method of each reuse class. Besides, this paper does the case study and analysis of SPEComp2001 benchmarks, using our parallel data reuse theory. We believe that parallel data reuse theory will have a big impact on the locality optimization of parallel applications.

Index Terms  Parallel; Reuse; Locality; OpenMP; Intra-processor; Inter-processor  1. Introduction  Along with the development of microelectronics, a chip can integrate more and more transistors. To better utilize the area of chips, the academic communities and industrial world pay more attentions to the multi-core technology, trying to put multiple processor cores into one chip. Thus, a lot of multi-core researches and products emerge [1], [2].

The parallel technology, which was only within the high- performance computing domain, begins to come into the desktop computers of casual users. On the other hand, the memory wall problem has always been the bottleneck of the computer performance. With the increasing of the processor frequency, the speed gap between processors and memories becomes larger and larger. Therefore, the techniques for alleviating or solving the memory wall, typically including locality optimization techniques, have always been very important.

The basic principle of the locality optimizations depends on the existence of reuses in programs. Wolf proposed the reuse and locality theory in 1991 [3]. And his reuse theory has a very big impact on the locality optimization domain.

By December 2008, his paper has been cited over 300 times in ACM Digital Library. However, Wolf?s reuse theory only aims at the serial programs on single-core processors. In the multi-core platforms, parallel programs exhibit their own data reuse characteristics different from serial programs.

We extend the data reuse theory into parallel programs, providing basic principles for the researches in multi-core locality optimizations.

This paper includes the following contributions: 1) The parallel data reuse theory for OpenMP applica-  tions is proposed. To our knowledge, our theory is the first systemically work of classifying data reuse in OpenMP programs.

2) The data reuse of the SPEComp2001 benchmarks [4] is analyzed using our parallel data reuse theory.

The rest of this paper is organized as follows. Section 2 gives the overview of the serial data reuse theory proposed by Wolf. Section 3 introduces our parallel data reuse theory for OpenMP applications. Section 4 describes the data reuse analysis of the SPEComp2001 benchmarks. Section 5 illustrates the related works and we conclude in section 6.

2. Overview of Serial Data Reuse Theory  In this section we briefly introduce the serial data reuse theory proposed by Wolf. Wolf?s theory distinguishes be- tween reuse and locality. A data item is reused if the same data is used in multiple iterations in a loop nest. Thus reuse is a measure that is inherent in the computation. If the cache catches the reuse of two references to one same data item, the locality happens. And if the two references both lead to cache miss, the locality does not happen. Therefore, a reuse does not guarantee locality. Second, the data reuse theory only concerns about the reuse, happened in the loop, in uniformly generated references. The following gives the definitions of iteration space and uniformly generated reference.

Definition 1: Iteration space Zn of a loop nest of depth n is a vector space in n dimensions. Each iteration in the  DOI 10.1109/SNPD.2009.92    DOI 10.1109/SNPD.2009.92     loop nest corresponds to a node in the iteration space, and is identified by its index vector ?p = (p1, p2, ? ? ? , pn); pi is the loop index of the i loop in the nest, counting from the outermost to innermost loop.

Definition 2: Let n be the depth of a loop nest, and d be the dimensions of an array A. Two references A[?f(?i)] and A[?g(?i)], where ?f and ?g are indexing functions Zn ? Zd, are called uniformly generated if  ?f(?i) = H?i + ?cf and ?g(?i) = H?i + ?cg  where H is a linear transformation and ?cf and ?cg are constant vectors.

Wolf classified the data reuse into four classes: self- temporal reuse, self-spatial reuse, group-temporal reuse and group-spatial reuse. If a reference within a loop nest accesses the same data location in different iterations, a self-temporal reuse happens. And if the reference accesses data on the same cache line in different iterations, it is said to possess self-spatial reuse. Furthermore, different references may access the same locations. So we can get the group-temporal reuse and group-spatial reuse.

? Self-Temporal Reuse: Self-temporal reuse happens when a reference A[H?i+?c] accesses the same data ele- ment in iteration?i1and?i2, whenever H?i1+?c = H?i2+?c.

