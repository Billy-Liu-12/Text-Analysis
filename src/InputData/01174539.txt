MINING CONSTRAINED CUBE GRADIENT USING CONDENSED CUBE

Abstract: Constrained cube gradient mining is an important mining  task and has broad applications. The goal of constrained cube gradient mining is to extract the interesting pairs of gradient- probe cell from a data cube. The constrained cube gradient mining faces the obstacle of large requirements in time and space for the generating of combination of gradient cells and probe cells. In this paper, we explore the condensed cube approach that is a novel and effEcient data organization technique to the mining of constrained cube gradient. A new algorithm based on the condensed cube approach is developed through the extension of the existing efficient mining algorithm LiveSet-driven. The results of experiments show our algorithm is more effective than the existing algorithm on the performance of mining constrained cube gradient.

Key words: Data cube; Constrained cube gradient; Condensed cube; Cube computation  1 Introduction  Many interesting applications may need to analyze the changes of measures in multidimensional space 16?. The problem of mining changes of measures in a multidimensional space was first proposed by Imielinski, et al. as a cubegrade problem [31, which can be viewed as a generalization of association rules [?I and data cubes [51. It studies how changes of measures (aggregates) of interest are associated with the changes in the underlying characteristics of sectors, where the changes in sector characteristics are expressed in terms of dimensions of the cube and are limited to specialization (drill-down), generalization (roll-up), and mutation (a change in one of the cube?s dimensions). Cubegrades are significantly more expressive than association rules in capturing trends and patterns in data because they use arbitrary aggregate measures, not just COUNT, as association rules do.

Cubegrades can also support sophisticated ?what if? analysis tasks dealing with behavior of arbitrary aggregates over different database segments. As such, cubegrades can be useful in marketing, sales analysis, and other typical data mining applications in business. However it poses serious challenges on both understandability of results and on computational efficiency and scalability. The constrained cube gradient mining r61 represents a confined but interesting version of the cubegrade problem. The goal of constrained cube gradient mining is to extract the  0-7803-7508-4/02/$17.00 02002 IEEE  interesting pairs of gradient-probe cell satisfying the specifying constraints from a data cube. Though the constraints can restrict the search space of cells, the constrained cube gradient mining still faces the obstacle of Iarge requirements in time and space for the generating of combination of gradient cells and probe cells since the data cube is always very large. Motivated by the condensed cube technique [41 that can reduce dramatically the size of cube itself and the cube computation time, we explore this technique to the mining of constrained cube gradient in this paper. A new algorithm named as eLiveSet based on the condensed cube approach is developed through the extension of the existing LiveSet algorithm. In our algorithm, we focus on the issue on how to derive the live set of gradient cells that are stored in a condensed format since the derivation of live set of gradient cells is a key issue for the LiveSet-style algorithm. There are other related works including the cube computation r1*21 and the mining of constrained association rules [?I. Recentla;, f~ investigated environment named as CubeExplorer online exploration of data cube is developed.

2 An overview of condensed cube approach  The semantics of the CUBE BY operator is to partition a relation into groups based on the values of the attributes specified in the CUBE BY operator and then apply aggregations functions to each of such groups, the CUBE BY operator computes GROUP BY corresponding to all possible combinations of attributes in the CUBE BY operator. In general, a CUBE BY operator on n attributes computes 2? GROUP BYs, or cuboids. The attributes of a relation table can be divided into dimension attributes and measure attributes. A tuple with dimension attributes and measure attributes in a data cube is called a cell. Given a set of dimension attributes S D c  A, if r is the only tuple in its partition when the relation table is partitioned on SD, we say tuple r is a single tuple on SD, and SD is called the single dimensions of r 14]. Assumed that relation table R containing three relation tuples with attributes (A, B, C, M) and M is the measure attribute and the aggregate function is SUM function: (0,1,1,50), (l,l,l,lOO) and (2,3,1,60). When R is partitioned on dimension A (i.e. cuboid A), the partition in which the dimension value of A is equal to 0 contains a single tuple (0, 1, 1, 50). Then (0, 1, 1, 50) is a single tuple and {A) is its SD. A single tuple and its SD can   http://yubliu0263.net    express many cells in a cube and these cells can be viewed as being condensed into the single tuple r. In general, given a single tuple r(r(al),..,r(an)) and its single dimensions SD=[a,,..,a,} (KiSjSn), the complete set of cells condensed by the single tuple r and it?s SD is denoted as ExpandSet(r) and the dimension value of any cell r?E ExpandSet(r) can be computed by the following Expand principles:  (1) r?(ak)=r(ak), if akE SD; (2) r?(ak)=r(ak) or r?(ak)=*, if b j ; (3) r?(ak)=*, if a# SD and l lkcj .

For example, suppose the relation table tuple r=(A=l,  B=2, C=3, D=4) is a single tuple, and its SD=[AC}*.

According to the Expand principles, each dimension value of the cells in ExpandSet(r) is as follows: A=l, B=*, C=3, and D=4 or D=*, where ?*? denotes the value ?all?, i.e., aggregated to the highest level on this dimension. In other words, the two cells: (A=l, B=*, C=3, D=4) and (A=l, B=*, C=3, D=*) in ExpandSet(r) are condensed into the single tuple r. In general, for the given single tuple r(r(al),..,r(an)) and its SD={ a,,..,a,) (l<i<jSn), there are the number of 2n-J cells contained in ExpandSet(r), in other words, 2?-? cells are condensed by the single tuple r. Notice that all the cells in ExpandSet(r) have the same aggregation value since all of them are only aggregated from the same single tuple. In addition, the operation Expand is a simple operation that requires copying the original tuple and replacing certain attributes value with a special value. Then the Expand operator needs no additional cost and no aggregation or other computation is required. According to the above Expand principles and the definition of single tuple, we also have two important properties on a single tuple r: (I) The non-* dimension values of the cells in ExpandSet(r)  ?are all derivedfrom the single tuple rand equal to the corresponding dimensions values of r.

dimension values on SD and the cells that have the same non- * dimension values on SD are contained in the set ExpandSet(r).

The properties of single tuple construct the basis of the condensed cube. In a condensed cube, we only need physically store the single tuple together with an extra field to store the single dimensions information of the single tuple. The cells can be expressed by the single tuple that are not stored physically. When needed, these cells can be generated through the expand operator of the single tuple.

The SD fields of these non-single tuples are equal to 0 in a condensed cube. These non-single tuples can be viewed as the general cells in a general data cube (i.e., non-condensed cube) since they don?t condense any cells. A condensed  (2) All of cells in ExpandSet(r) share the same non- * ?  Notice that we require SD#A, where A denotes the all  r(ak) denotes the value of dimension ak ( l lk ln) In this paper, whenever there is no confusion, we use the  concatenation of dimension names to represent the set consisting of those dimensions. For example, the {AB ) is a short of [ A,B ) .

dimensions of a cube.

cube can be computed through our BU-BST algorithm [41, which is basically a modified version of the original BUC algorithm [?I. Similar to the BUC algorithm, BU-BST algorithm explores the data cube space using a bottom-up, depth-first munner (i.e., the recursive partitioning manner).

3 The mining of constrained cube gradient  3.1 The problem definition  Given two distinct cells c1 and c2 of a data cube D of a given relation table R with n dimensions, c1 is an ancestor of c2 and c2 is a descendant of cl iff on every dimension attributes, either c1 or c2 shares the same value; or c1 has value ?*?, where ?*? indicates ?all?; CI is a sibling of c2, and vice versa, iff c1 and c2 have identical values in all dimensions except one dimension in which neither has value *. For simplicity, we sometimes say c1 is similar to c2 if c1 is a descendant, an ancestor or a sibling of c2. A significance constraint Csig is usually defined as conditions on measure attributes. A probe constraint c p r b  is usually defined as conditions on dimension attributes and is used to select a set of user-desired cells. A cell c is significant iff Csig(c)=true, and a cell c is a probe cell iff c is significant and Cp&(c)=true. Given a single tuple c. If c is significant, all of the cells condensed into c are significant because they have the same measure to c. In addition, we say c satisfies the probe constraint Cprb, if each dimension of c has the dimension values that satisfy the corresponding dimension value constraints of cprb. For example, assume that the cell c=(1,2,3,4) is a single tuple and its SD={B}. Assume that Cprb=(A=*, B=*, C=4, D=3). Then the cell c does not satisfy the probe constraint because the dimension of C and D of the cell c have no dimension values that satisfy the value constraints of dimension C and D in Cph. Assume that Cprb=(A=*, B=*, C=3, D=*). Then the cell c satisfies the probe constraint Cph and dimension C has dimension value 3 satisfies the corresponding dimension constraint of Cprb. The complete set of probe cells is denoted as P. The set of significant cells that may have gradient relationship with a set of probe cells, P, are called the gradient cells of P.

The gradient constraint has the form cgrad(cg. $) z(g(c,, cp) 0 v), where 0 is in {I, 2,< , >}, v is a constant value, and g is a gradient function. A gradient cell cg is interesting with respect to a probe cell C,EP iff cg is significant, cg and cp satisfy similar relationship and Cgrd(cg, %)=true. Formally, given a relation table R, a significant constraint Csig, a probe constraint C, and a gradient constraint cgrad(cg, c,), the constrained cube gradient problem is to find all the interesting gradient-probe pairs (cg, cp) such that Cgrd(cg, $)=true.

3.2 The LiveSet algorithm  The LiveSet-driven algorithm (short for LiveSet) is an efficient algorithm for the constrained cube gradient mining.

The main framework of LiveSet algorithm is as follows: (1)       Apply a cube computation algorithm, such as H-cubing [21 or BUC [?I, to compute the set of probe cells P from the relation table R using both the significance and probe constraints. (2) Use a cube computation algorithm to find all interesting gradient-probe cell pairs, that is, to determine which gradient cell should be associated with which probe cells. The live set method is used in second step. In general, the live set of a gradient cell cg, which is denoted as LiveSet(c,), is the set of probe cells cp such that it is possible that (cg), cp) is an interesting gradient-probe pair, for some descendant cell cg? of cg. The major advantage of live set method is that as generating an interesting gradient- probe cell pairs, we only need to compare the gradient cell with its related probe cells (i.e. live set) but the complete set of probe cells P. Thus the derivation of live set of gradient cell is a key issue of LiveSet-style algorithm. The matching analysis method is used to derive the LiveSet. Let cp=(dpl, dp2, .., dpm) be a probe cell and cg=(dgl, dg2, .., dm) be a gradient cell. The number of solid-mismatches between cp and cg is the number of dimensions in which both values are not * but are not matched, i.e. of different values. The number of *-mismatches between cp  and cg is the number of dimensions in which cp is * but cg is not. It is noticed that the notion of *-mismatches is not symmetric, i.e., if cg has * value on a dimension but cp has a non-* value on the same dimension, this is not considered a *-mismatch. A probe cell cp is matchable with a gradient cell cg if either cg or cp has no solid-mismatch, or they have exact one solid- mismatches but no *-mismatch. Two important properties are used to derive the live set.

Propertyl. Assume cgl and cg2 are two gradient cells and cg2 is a descendant of cgl. Then LiveSet(cp) cLiveSet(cgl).

This property ensures that we can produce the live set of a descendant cell from that of the ancestor cell.

Property2. Let cp  be a probe cell and cg be a gradient cell.

If cp is matchabel with cg then cp E LiveSet(c,) otherwise cp e LiveSet(c,). This property shows the way to derive the live set, that is, using the matching analysis method.

4 The algorithm based on condensed cube  Definition1 (live set) Since the probe cells and the gradient probe cells are likely single tuples, the definition of LiveSet of gradient cell is extended as follows: (1) The gradient cell cg is a non-single tuple. For a probe  cell cp that is a single tuple, we say C ~ E  LiveSet(c,) if there exists cell c p ) ~  ExpandSet(q,) and (c i ,  cp?) is a possibly interesting gradient-probe cell pair for some descendant cells c i  of cg. On the other hand, if the cp is a non-single tuple, we say c p ~  LiveSet(c,) if (cg?, cp) is a possibly interesting gradient-probe cell pair for some descendant cells cg? of cg.

The gradient cell cg is a single tuple. Since cg likely condenses many gradient cells, we define LiveSet(cg)=uLiveSet(c@) where C@E ExpandSet(c,) and llillExpandSet(c,)l and I ExpandSet(c,)l denote the cardinal number of the set ExpandSet.

Definition2 (basic cell) Given a single tuple r, the cell in ExpandSet (r) that is obtained by taking the * value for each dimension of r except the dimensions in SD of r, is called the basic cell of the single tuple r and is denoted as cb. For example, assume that a gradient cell cg=(A=lr B=2, C=3, D=4) is a single tuple and its SD=(BC}, then the basic cell  an ancestor of the other cells in ExpandSet(r) because those cells are obtained by taking the non-* value for the dimensions that is not included in SD from cb.

Definition3 (potential) A gradient cell cg is potential to expand higher dimension (i.e., further partition) if it satisfies the following conditions: (1) cg is not a single tuple; (2) Csig(cg)=true; (3) C@(c,,c,)=true, for some probe cell cp in LiveSet(cg).

Two basic lemmas are, developed for the derivation of the live set.

Lemma 1. Given a gradient cell cg and a probe cell cp that is a single tuple. If cp is not matchable with cg, then for any cell CE ExpandSet(c,), c is not similar to cg? that is a descendant of cg.

Proof: Suppose that cp is not matchable with cg. According to the definition of matchable, cp and cg have at least two solid-mismatches (there are not *-mismatches between q, and cg because cp is a single tuple and has no * dimension value). Without loss of generalization, we assume that cp has different non-* values with cg on dimensions (al,. . .,al) (22). Both cg) and cg have the same non-* values on dimensions (al,. . .,a,) because cg? is a descendant of cg. Thus, cp also has different non-* values with cg) on dimensions (al ,..., ai) (22). Two cases raise here, i.e., {a, ,..., al)nSD#O and (at ,_.., a,}nSD=0, where SD is the single dimensions of the single tuple cp.( I ) Suppose { al,. . .,a,)nSD#0. Without loss of generalization, we suppose (al,. ... a,}nSD=(al, ..., a,)(j2l). From the property (2) of the single tuple, it is known that all of cells in ExpandSet(c,) share the same non-* values on dimensions (a1, ..., a,). Thus, c has different non-* values with cg) on dimensions (al, ..., a,). Suppose j>l ,  that is, c and cg? have at least two different non-* values on dimensions (al,. . .,a,).

According to the definition of similar, c is not similar to cg).

Suppose j=1. Since cp  and cg? have at least two different non-* dimension values, there is at least another dimension ad, wherelldli, in which c, and cg? have different non-* value. Again, according to the Expand principles, the value of dimension ad of any cell CE ExpandSet(c& is either equal to a non-* value %(ad), where cp(%) denotes the value of cp on dimension ad, or equal to *. If equal to a non-* value, c and cg? have different non-* dimension values on dimension ak and a. Then c is not similar to cg?. If equal to * value, c and cg? have different non-* dimension values on a, except the dimension in which c has * but cg) has non- *. Thus, c is also not similar to cg?. ( I1 )Suppose { al, . . . ,al) n S D = 0  (22). According to the values of dimensions (al,, . . ,%), which is computed according to the Expand principles, the cells in Expandset(%) can be divided into three types: (a) the cells in which the values of  Of Cg, Cb=(*, 2,3,*). It iS Clear that CbE ExpandSet(r) and Cb iS     dimensions (al, ..., a,) are all equal to *; (b) the cells in which only one dimension value is non-* and the others are *; (c) the cells in which more than two dimension values are non-*. For the case of (a), suppose c is similar to c,?, there is only a case that c is an ancestor of c,? (Because the values of dimensions (al,...,%) of cell c are * value, whereas the values of dimensions (al, ..., aJ of cell cgl are non-*). Again, according to the definition of ancestor, c and c i  should share the same value or c has * on the remaining dimeusions A-(al ,..., a,], where A denotes the all dimensions of cube. Due to c has non-* dimension value on SD that is contained in A-(al,. . .,ai], c and c,? should share the same non-* value on SD. Then c; should be contained in Expandset($) according to the property(2) of the single tuple. If C,?E ExpandSet(c,), the non-* values of dimensions of c,? are all derived from the corresponding dimension values of cp. Again, because cg is an ancestor of c,?, the non-* values of the corresponding dimensions of cg should be the same to that of c,?. Then the non-* dimension values of cg are also derived from cp. So c,, and cg surely have no two solid-mismatches. It is in contradiction to the supposition: cp is not matchable with c,. So c is not similar to c,?. For the case of (b), suppose the only one different non-* value dimension is a, ( l l j l i )  and the * value dimension is ak (Kkli ,  k#j). Then c and c,? have one different non-* value on dimension aj except one dimension ak in which c has * but cg) has non-* value, so c is not similar to c,?. For the case of (c), c and c,? have more than two different non-* dimension values, so c is not similar to  Lemma 2. Given two gradient cells c,, cg? and c,? is obtained from cg in the depth-first order. We have LiveSet ( c , ? ) ~  LiveSet (cg).

Proof: ( I )Suppose cg) is a non-single tuple. For every cpeLiveSet (c,?), there are two special cases: cp is either a single tuple or a non-single tuple. In the case that cp is a non-single tuple. Since cpELiveSet (c,?), it is known that (cg)?, cp) is a possible gradient-probe cells pair, where cg)? is a descendant of c,?. Since c,? is obtained from cg in the depth-first order, that is, c,? is obtained by partitioning cg on some dimensions, i.e., by taking specific values for some dimensions in which cg has * value. Then c,? is a descendant of cg. Since cg). is a descendant of c,? and c,? is a descendant of cg, c c  is also a descendant of cg according to the definition of descendant. Since (c,?, cp) is a possible gradient-probe cells pair, we also have cPe LiveSet (c,). On the other hand, assume that cp is a single tuple. It is known that there exists cell %?~ExpandSet(c,) and (cc,  c;) is a possible gradient-probe cells pair, where cg)? is a descendant of cg? and c,. Thus, we have $E LiveSet (c,). In a word, for every C ~ E  LiveSet (c,?), we have cPe LiveSet (c,). Therefore, LiveSet (c , ? )~  LiveSet (c,) is held. (1I)Suppose c,? is a single tuple. Then we have LiveSet(c,?)=uLiveSet(cgi?) where C ~ ? E  ExpandSet(c,?) and l l i l l  ExpandSet(c,?)l.

Assume that cb? is the basic cell of cg? and we have Cb?E ExpandSet(c,?). For any cell cgi? in ExpandSet(c,?) that  Cg) .

is not equal to cb?, it is known that cgi? is a descendant of cb?.

Since both cgi? and cb? are non-single tuples, according to the above proof of ( I ) of lemma2, we have LiveSet(cgi?) E LiveSet(cb?). So uLiveSet(c@?) E LiveSet(cb?) where Cgi? f c b ?  and llillExpandSet(c,?)l. Then (uLiveSet(cgi?))u(LiveSet(c<))=LiveSet(cb?), that is, uLiveSet(c@?)=LiveSet(cb?) for every cell C@?E ExpandSet(c,?). Again, c,? is a single tuple. Then c,? is obtained by partitioning cg on the dimensions of SD of c,? in the depth-first order and cg has * value on some dimensions of SD. According to the definition of basic cell, the basic cell of c,?, cb? is obtained by taking more special values (i.e., non-* value) for the dimensions of SD of c,?.

Then cb? is a descendant of c,. Similarly, according to the above proof of ( I ) of lemma2, we also have LiveSet(cb?)cLiveSet(c,). Therefore we have LiveSet(c,?)=uLiveSet(c,?)=LiveSet(c<) c LiveSet (c,) .

From lemma1 and lemma2, we can derive LiveSet(c,?) by pruning the single tuple that is not matchable with cg? or the basic cell of c,?, cb? from LiveSet(c,). Note that there likely exist another kind of cells in the LiveSet(c,), that is, the non-single tuples . For these cells, they can treated as the cells of a general cube since they don?t condense any cells and they can be pruned from LiveSet(c,) by the propery2 in the section3. In summary, for a given potential gradient cell cg and it?s descendant cell c,?. we can derive LiveSet(c,?) from LiveSet(c,) by pruning the cells that are not matchable with c,? or cb? that is a basic cell of c,? from LiveSet(c,).

Based on the above two lemmas, we develop a novel algorithm eLiveSet through the extension of LiveSet algorithm. The framework of eLiveSet algorithm is similar to that of the LiveSet algorithm. The description of the eLiveSet algorithm is given below.

Algorithm: eLiveSet Input: A relation table R, a Csig, a Cph and a Cpd.

Output: The complete set of interesting gradient-probe cell pairs.

Method:    Apply the BU-BST algorithm to computing set of probe cells P from R.

Initialize the potential gradient cell to cell cg= (*,*,. . .,*) and LiveSet(c,)=P.

Use the bottom-up manner, depth-first order BU-BST cube algorithm to find all interesting gradient-probe cell pairs.

If cg is significant, then for every cp E LiveSet(c,) output the interesting gradient-probe cell pairs (e,e?). If cg and cp are single tuples, then ee  ExpandSet(c,) and e?? Expandset($), otherwise e=cg (or e?=$).

If LiveSet(c,) is empty or cg has no potential to grow, terminate this branch and recursively backtrack to process the next cell according to the depth-first order.

If cg has potential to grow, expand it to the next level according to the depth-first order. If a      descendant cell cg? of cg is processed from this expansion, derive LiveSet(c,?) from LiveSet(c,).

It is worth mentioning that in our study, the probe cells in LiveSet(c,) are assumed to be stored in the ascending order according to the measures. Thus, if the measure of one probe cell cp can not satisfy the constraint Cgrad(cg,cp), then all the probe cells following it will not satisfy the gradient constraints function and hence can be pruned from LiveSet(c,). In the case that the LiveSet(c,) is very large set, the hash table method can be used to fast access the Liveset( cg) .

5 Experiments  In this section, we present our experimental results on the performance (in terms of time) of mining constrained cube gradient from a data cube. All experiments are conducted on a PC platform with an Intel PentiumIII 500M CPU, 218M RAM and Windows 2000 OS. Two mining algorithms are compared in our experiments. The first algorithm is eLiveSet that is constructed on a condensed cube that is computed by the BU-BST algorithm. The other is LiveSet that is constructed on a general cube that is computed by BUC algorithm. The performance of mining cube gradient from cube with the two algorithms is compared. All experiments are performed using synthetic (algorithmically generated) datasets of 1M (=1,024) tuples that is uniformly distributed and the number of dimension is set to 9 and the cardinality of all attributes is set to 100. The aggregate function used in the cube computation algorithm is SUM function. The performance of the algorithms with different constraints is shown in Figl. In Figl, the significance constraints C,,,>O and the gradient constraints Cd=m(c,)-m(cp)>O. The value of the non-* value dimension in probe constraint C is set to [1,50]. The number of non-* value dimension is varied from 1 to 9. The non-* value dimensions in c p r b  are fixed on the first k dimensions (1%9), respectively. The larger the number of non-* value dimensions, the smaller number of probe cells that satisfy the cprb .  In Figl, the scalability of the two algorithms over probe cells is shown. The runtime of the two algorithms is less with the increase of the number of non-* value dimensions in C because the number of non- * value dimensions in Cp,,,?: larger and the probe cells satisfied Cph are larger. However, eLiveSet algorithm is faster than LiveSet algorithm. The reason is that many cells are condensed into the condensed cube and the search space of the cells including the probe cells and the gradient cells to be handled reduces dramatically in eLiveSet algorithm.

Similarly, eLiveSet algorithm is also faster than LiveSet with Cslg and Cgd. Due to the limitation of space, the corresponding figures of results are not given.

P  1300.00 P 1100.00 2 2 900.00 2 700.00  500.00 a 2  3 300.00  100.00 1 2 3 4 5 6 7 8 9  The number of non-* va lue dimension of Cprb  Fig. 1. The scalability with probe cells  6 Conclusions  In this paper, we study the problem of mining constrained cube gradient using the condensed cube approach. A new algorithm, eLiveSet, is developed by the extension of the existing efficient algorithm LiveSet. The experiment results show eLiveSet is efficient and scalable.

There are many interesting issues in our future work, for example, instead of computing the cube gradient on a non- materialized cube, as shown in this paper, we can compute the cube gradient on a materialized cube since a materialized cube is always pre-computed in a datawarehouse environment. In addition, further enhancing the performance of eLiveSet algorithm is also our interested topic.

Acknowledgements  This paper is supported by the e-government project of National Science and Technology Ministry of China. (No.

2001BA110B01).

