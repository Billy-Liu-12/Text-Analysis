Fuzzy Data Mining: Effect of Fuzzy Discretization

Abstract  When we generate association rules, continuous at- tributes have to be discretized into intervals while our knowledge representation is rwt always based on such dis- cretization. For example, we iwcally use some linguistic terms (e.g., young, middle age, and old) for dividing our ages into somejiuzy categories. In this paper; we describe the extraction of linguistic association rules and examine the peformance of extracted rules. First we modi& the dejinitions of the two basic measures (i.e., conjidence and support) of association rules for extracting linguistic asso- ciation rules. The main digerence between srandard and linguistic association rules is the discretization of continu- ous attributes. We divide the domain interval of each at- tribute into some j i i z y  regions (i.e., linguistic terms) when we extract linguistic a.~sociarion rules. Next we compare j izzy  discretization with standard non-jidzzy discretization through computer simulations on a pattern classijcation problem with many continuous attributes. The classGcation peformance of extracted rules on unseen test patterns is ex- amined iinder various conditions. Simulation results show that lingiiisiic association rules with rule weighls have high generalizalion ability even when rhe domain of each conlin- uous attribute is homogeneously partitioned.

1. Introduction  When our knowledge extraction task involves numerical data with continuous attributes, each attribute is usually dis- cretized into several intervals r1.21. The discretization into intervals is used in many machine learning techniques such as decision trees [3]. In some situations, human knowledge exactly corresponds to such discretization of continuous at- tribute$. For example, the domain of our ages is divided into two intervals by the threshold age 20 in the following knowledge: ?People under 20 are not allowed to smoke?.

In othcr situations, thc discrctization into intcrvals is not appropriate for describing human knowledge. For example,  0-7695-1 119-8/01 $17.00 0 2001 IEEE  we may have the following knowledge: ?Tallpeople are not comfortable in small cars?. We cannot appropriately repre- sent this knowledge using the discretization of the domain of our height into intervals. This is because the linguistic term ?tall? cannot be appropriately represented by an inter- val. A mathematical framework for representing linguistic terms is fuzzy logic. Fuzzy logic has been recognized as a convenient tool for handling continuous attributes by nile- based systems in a human understandable manner [4]. This recognition is supported by many successful applications of fuzzy control methods [51.

For limction approximation problems with n inpub, we use linguistic rules of the following form:  (1)  where q is a nile index, x = (11, . . . ~ t,>) is an n- dimensional input vector, Aqi is an antecedent linguistic tcrm such as ?small? and ?large?, y is an output vari- able. and B, is a consequent linguistic term. The above- mentioned linguistic knowledge on the comfortableness in small cars can be represented in the form of (1) as ?If 2 is tall then y is low? where t is the height and y is the comfort- ableness. The linguistic rule in (1) can be viewed as an as- sociation rule A, 3 B, where A, = (-4,l ~ . . . , -&). The main difference between our linguistic association rules and the standard formulation of association rules [6] is that the domain of each input (and output) variable is fuzzily divided into linguistic terms in our linguistic association rules. For example, one may divide the domain of our height into two linguistic terms ?tall? and ?not tall? as shown in Fig. 1.

The vertical axis of this figure shows the extent to which a particular value of the height on the horizontal axis is com- patible with each linguistic term. Of course, discretization into linguistic terms depends on the situation. When we talk about the height of professional basketball players, we irn- plicitly assume different discretization from the case of the height of college students.

On the other hand, we use linguistic rules of the follow- ing form for pattern classification problems:  If tl is nql a n d . .  . and I, is A,, then Class C,, (2)  If 21 is -qql a n d . .  . and tn is -Aqn then y is E,,  24 1  mailto:ie.osakafu-u.ac.jp   not tall  X 140 150 160 170 180 190 200   Ileight [cm]  150 I Comfortable  Figure 1. An example of fuzzy discretization.

where x = (21 ~. . . , x , )  is an n-dimensional pattern vec- tor and Cy is a class label. The above-mentioned linguis- tic knowledge on the comfortableness can be represented in the form of (2) as ?If E is lull then Class 2? where z is the height and Class 2 is the class label corresponding to ?not comfortable?. This linguistic nile may be obtained from ex- perimental results where a number of examinees are asked whcthcr they fccl cornfortable or not in a small car. Sup- pose that we have responses in Table 1 from ten examinees on the comfortableness in the small car. From Fig. 1 and Table 1, we can extract two linguistic rules ?If z is not tall then Class 1 (i.e., comfortable)? and ?If z is tall then Class 2 (i.e., not comfortabie)?. These rules are much more intu- itive than interval representation rules such as ?If 2 5 175 then Class 1? and ?If E > 175 then Class 2?.

177 I Comfortable  Table 1. Artificial data for illustration purpose.

8 178 I Not comfortable  I 2 I 158 I Comfortable I   Comfortable Not comfortable  174 Comfortable Not comfortable  185 1 Not comfortable 10 19 1 I Not comfortable  The linguistic rules in (1) and (2) can be viewed as as- sociation rules A, + B ,  and A, * C,, respectively.

The confidence and the support of these association rules can be defined by extending their standard definitions [6] to the case of fuzzy discretization. Through computer sim- ulations, we compare standard interval discretization with fuzzy discretization. We also examine three rule selection criteria (i.e., the confidcnce, the support, and their product) and two rule types (i.e., rules withlwithout rule weights).

2. Function Approximation  Let us assume that we have m input-output pairs (xP, yp),  p = 1,2 , .  . . , m where xp is an n-dimensional input vector (i.e., xp = (zl,l ~. . . , xpn)) and yp is the cor- responding output value. Our data set D consists of these nz input-output pairs (i.e., ID1 = m).  For calculating the confidence and the support of the linguistic association rule A, j R,, we have to calculate the number of input-output pairs that arc compatiblc with A, and By. Such calcu- lation is trivial for standard association rules with interval discretization. For example, when A, is the inequality con- dition t > 175, we can see that five examinees in Table 1 are compatible with this condition.

In our rule extraction task, the compatibility grades of input-output pairs with A, are different from each other.

For example, each examinee in Table 1 has a different com- patibility grade with the linguistic term ?call?. Such a com- patibility grade is mathematically described by a member- ship function in fuzzy logic. Thc mcmbcrship function of the linguistic term ?tall? in Fig. 1 is written as  0, i f r  5 170, ( x  - 170)/10, if 1 T O  < G < 180, (3) 1. if 180 5 .c.

A fuzzy set of examinees compatible with ?tall? in Table 1 is explicitly written as  p =  1 , 2 ;  , lo}  0.0 0.0 0.0 150? 158? lGl ? ? .?  where the denominator shows the height of each examinee and the numerator shows its membership value. Each ele- ment in (4) should not be viewed as a fraction number but a pair of xP and pLtall(.rp). The total number of examinees compatible with ?tall? is calculated from (4) as   I -WWI = CPLlall(4 p = l  = 0.0 + 0.0 + 0.0 + 0.3 + ? . ? +  1.0 = 4.8. (5) In gcncral, a fuzzy sct of input-output pairs compatiblc  with A, is written as  where PA, (.) is the membership function of A,, which is usually defined from the membership function of each lin- guistic term A,i  by the product operation as  PA,(xp) = P L d q 1 ( 2 p l )  x . . .  x P A q , , ( x p n ) .  (7)     The number of input-output pairs compatible with A, (i.e., cardinality of D(A,)) is defined as  A fuzzy set of input-output pairs compatible with both A, and R, is defined as  where pBq (.) is the membership function of R,. The cardi- nality of D(A,) r l  D(  B,) is dcfincd as  m  lD(Aq)nu(oq)l = x P A , ( X p )  x P B , ( Y p ) .  (10) p = l  Now wc can define thc confidencc and thc support of the linguistic association rule A, 3 B,  as follows:  m  rn  3. Pattern Classification  Let us assume that we have m labeled patterns (xp , t p ) , p = 1,2: . . . , m where xp is an n-dimensional pattern vec- tor (i.e., xp = (zpl  . xpn)) and t ,  is the class label of x p .  Our data set D consists of these m labeled patterns. As in the previous section, we can define the confidence and the support of the association rule A, j C,. The differ- ence between A, 3 E?, and A, 3 C, is that R, is a linguistic term while C, is a class label. Since C, is a class label, the compatibility grade of 1, with C, is 0 or 1:  1, if t ,  = C,, PC, ($1 = 0, otherwise.

Thus the cardinality of D(A,) n D(C,) is defined as  The confidence c(A, j C,) and the support s(A, =j Cy,) of the association rule A, j C, are calculated from (1 I)- (12) using ID(A,) n D(Cq)l instead of ID(A,) n D(B,)I.

As an example, let us calculate c(tall  3 Class 2) and s( tall 3 Class 2) from the ten training patterns in Table 1 where Class 2 corresponds to ?not comfortable?. Since the data set in Table 1 includes ten examinees, ID1 = 10.

As shown in the previous section, ID( t u l l )  I is calculated as ID(tall)I = 4.8. From Table 1 and (14), ID(tall) fl D(C1ass 2) I is calculated as ID(tall)nD(Class2)1 = 0.3+0.6+0.8+1.0+1.0 = 3 . i .

(15) Thus the confidence and the support are calculated as  c ( ta l l  3 Class 2) = 3.7/4.8 = 0.77, (16)  s(tu2l j Class 2) = 3.7/10 = 0.37. (17)  In the same manner, the confidence and the support of the linguistic rule ?fall j Class 1 (i.e., comfortable)? are cal- culated as  c( ta l l  3 Class 1) = 1.1/4.8 = 0.23, (18)  .?(tall 3 Class 1 )  = 1.1/10 = 0.11. (19)  Thus we choose the linguistic association nile ?fall 3 Class 2? rather than ?tall Class 1?.

4. Computer Simulations  4.1. Rule Extraction and Pattern Classification  In our computer simulations, we used the wine recog- nition database in the UCI Machine Learning Repository (http://www.ics .uci .edu~mleam/MLSummary.html). The wine data set is a three-class pattern classification problem with 178 patterns and 13 continuous attributes. As aprepro- ccssing proccdurc, wc normalizcd cach attributc value into a real number in the unit interval [O, 11. The domain of each attribute was discretized into some linguistic terms. For ex- ample, five linguistic terms are shown in Fig. 2 (i.e., S: small, MS: medium small, M: medium, ML: medium large, and L: large). Using those linguistic lems, we generated linguistic association rules of the following type with two antecedent conditions:  If xi is )Iql and xj is 11, then Cia% C, with CF,, (20)   http://www.ics   A  0.0 1.0 -  Figure 2. Five linguistic terms.

whcrc GF, is a rulc wcight (i.c., ccrtainty factor).

For generating linguistic association rules of the form in  (20), we examined all the combinations of two antecedent conditions: x 5' combinations in the case of five lin- guistic terms. The consequent class C, was specified for each combination A,, = ( A + ,  A,j ) as  c(A, j C,) = mas{c(A, j Class l ) , c(A, a Class a ) , c(A, j Class 3)). (21)  The confidence c(A, j C,) can be directly used as the nile weight C' F, :  C'F, = c(A, + Cq). (22) As shown by computer simulations, we obtained better re- sults from the following definition:  L'F, = c(A, 3 C,) - C ,  (23)  where F is the average confidence for the other two classes:  In this manner, we generated a number of linguistic as- sociation rules of the form in (20). Let S be the set of the generated linguistic association rules. We used a single- winner-based classification method [7,8] for classifying a new pattern x p  = ( x p l ,  x p " ,  . . . , xpn) by the rule set S.

The winner rule R,, for the new pattern x p  was defined as  ~ A , , ( x ~ ) . C F , , +  = max{p , . t , (xp ) .CF,  : R, E '3). (25)  When multiple linguistic rules with different consequent classes had the same maximum value in (25). the classifi- cation of the new pattern xp was rejected.

4.2. Effect of Fuzzy Partitions  Through computer simulations on the wine data set, we compared fuzzy partitions with interval partitions. First we  extracted linguistic association rules of the form in (20) us- ing all the 178 patterns in the wine data set as training data.

We used the two methods for specifying the rule weight CF, of each rule. We also examined the case of no rule weight. This case was examined by assigning the same rule weight to all rules (i.e., CF, = 1.0 for Vq). Next the same 178 patterns were classified by the extracted linguistic as- sociation rules. In this manner, the classification rate on training data was examined. On the other hand, we used the leaving-one-out (LVI) technique for calculating the classi- fication rate on test data. In the LV1 technique, 177 pat- terns were used for generating linguistic association rules, and the remaining single pattern was used for examining the classification ability of the generated rules. This procedure was itcratcd 178 timcs so that all thc 178 pattcrns werc cho- sen as test data. For comparing fuzzy partitions with inter- val partitions, we examined four fuzzy partitions in Fig. 3.

We also examine the corresponding interval partitions. As shown in Fig. 3, each threshold value in the interval parti- tions was specified by the crossing point of the neighboring membership functions.

Simulation results are summarized in Table 2 N Table 5. Table 2 shows classification rates on training data when fuzzy partitions were used. Table 3 shows classification rates on test data. On the other hand, Table 4 and Table 5 show classification rates when interval partitions were used.

~ ~  0.0 0.0 : i  0.0 -,o olo 1 .o A  . i . i  I !  1 I I !  ! ! ! i 0.0 1.0 0.0 I .0  Figure 3. Four fuzzy and interval partitions.

Table 2. Results on training data (fuzzy).

#of linguistic terms I 2 I 3 I 4 I 5  Direct use of confidence I 90.4 I 94.9 1 96.6 1 94.4 Definition in (23)-(24) I 94.9 I 96.6 I 97.2 I 97.2  No rule weight 1 84.8 I 70.2 I 71.9 I 74.7  From simulation results in Table 2 - Table 5, we can     # of intervals 2 3 4 5 Direct use of confidence 84.8 75.3 74.7 Definition in (23)-(24) 84.8 75.3 74.7  No rule weight 0.0 0.0 1.7  61.2 61.2 0.0  ?aining data (  #of linguistic terms 2 3 4 Direct use of confidence 90.4 93.3 93.3 Definition in (23)-(24) 92.7 95.5 94.9  No rule weight 80.3 68.0 68.5  ?interval).

99.4 99.4  4j51 -  89.9 93.3 69.1  . , .

No rule weight I 0.0 I 0.0 I 1.7 I 0.6 t  # of intervals Direct use of confidence Definition in (23)-(24)  observe the following: (1) The use of nile weights improves the classification per-  formance of extracted rules.

( 2 )  The performance of standard association rules with no nile weights is terribly poor.

(3) While standard association niles have high classifica- tion performance on training data, their generalization ability is poor especially when the interval partitions are fine (i.e., when the number of intervals is large).

(4) The generalization ability of linguistic association niles is better than that of standard association rules.

We examine each of the above observations in detail.

First let 11s consider the effect of rule weights [8]. In the case of fuzzy partitions, nile weights can adjust the classifi- cation boundary because we use the product of the compat- ibility and thc nilc wcight for classifying ncw patterns (scc (25)). For visually illustrating the effect of nile weights on the classification boundary, let u s  consider the four linguis- tic rules in Fig.4. Various classification boundaries can be generated from those four linguistic niles as shown in Fig. 5.

90.4 I  0.0 1 .O X1  Figure 4. Four linguistic rules.

(d) CF,=I.O, CF2d.O CF3~0.2 ,  CF4d.3  Figure 5. Classification boundaries.

Next let us consider the classification with inkrval discretization. In Table 4 and Table 5, classification rates were very low when we used no rule weights.

In this case, the classification of almost all patterns was rejected because they were compatible with many rulcs with diffcrcnt conscqucnt classcs. This sitriation is illustrated in Fig. 6 where the following two niles are depicted: ?If xI is in the interval -4 then Class 1? and ?If x 2  is in the interval B then Class 2?. When these two rules have the same rule weight, we cannot classify any pat- terns in the overlapping area (i.e., the square in the center of Fig. 6). In our computer simulations, many rules overlap with each other in the 13-dimensional pattern space. Thus the classification of almost all patterns was rejected. When we used a different rule weight for each rule, such an unde- sirable situation was resolved.

Another interesting observation in Table 4 and Table 5 is that the same results were obtained from the two differ- ent specifications of rule weights. This i s  because the rule weight of each rule was used only for determining the win- ncr rulc in thc casc of interval discrctization. On thc con- trary, rule weights modify cla$sification boundaries in the     1.0) I  x2 B{  - 1.0 A 0.0 XI  Figure 6. Two overlapping rules.

case of fuzzy discretization as shown in Fig. 5.

Thc poor gcneralization ability of association nilcs in thc  case of interval discretization (i.e., Table 5 )  is due to the following reasons: (1) The discretization was not adjusted. The classification  performance can be improved by appropriately parti- tioning the domain of each continuous attribute. This will be discussed in Section 5.

(2) All the association rules generated from training pat- tcms wcrc uscd for classifying tcst pattcrns. Thc rcla- tion between the number of rules and their generaliza- tion ability will be examined in the next subsection.

While the above two reasons also apply to the case of fuzzy discretization, the generalization ability of linguistic associ- ation rules is much higher than that of standard association rules. This is because the threshold values for discretization are not crisp but fuzzy. The classification boundaries can bc adjusted by thc usc of rulc wcights as wc havc alrcady shown in Fig. 5.

In Fig. 7, we compare the fuzzy discretization with the interval discretization from the viewpoint of the covered re- gion by a single rule. In the case of the fuzzy discretization in Fig. 7 (a), the shaded region can be classified by a sin- gle linguistic rule located in the center of that region. On the other hand, the corresponding standard association nile can classify the much smaller shaded region in Fig. 7 (b).

Thus the classification of much more new patterns may be rejectcd in thc casc of intcrval discrctization than fuzzy dis- cretization especially when the interval partitions are fine.

The deterioration in the generalization ability by the use of fine partitions is not so severe in the case of fuzzy partitions because each linguistic association rule can cover a much larger region (compare Table 3 with Table 5).

43.  Effect of Rule Selection  In our computer simulations in the previous subsections, we used all the extracted rules for classifying new patterns.

In this section, wc examine the relation bctwccn the num- ber of rules and their generalization ability. We first divided  . : : :  S S M M L L  (a) FULLY partition (b) Interval partition  Figure 7. Classifiable region by a single rule.

the extracted niles from training data into three groups ac- cording to their consequent classes. Then the niles in each group were sorted in a descending order of their confidence values. When ~nultiple niles had the same confidence value, they were randomly sorted. For decreasing the effect of such randomization, we averaged simulation results over 20 independent runs. Finally we chose the first iV niles from cach group to classifying tcst data (iV = I ,  2, . . . ~ 30).  As in the previous sections. we used the leaving-one-out (LV 1) technique for evaluating the generalization ability of the ex- tracted niles from training data. In addition to the confi- dence, we also examined two different nile selection crite- ria: the support and the product of the confidence and the support. In such computer simdations. we used (23)-(23) for specifying nile weights and the fiizzy partition by three linguistic terms. For comparison, we also used the interval partition by three intervals.

Simulation results are summarized in Fig. 8 - Fig. 10.

As we have already shown, higher classification rates on test data were obtained from linguistic association niles than standard association niles. We can see from those figiires that thc product of thc confidcncc and thc support is thc bcst nile selection criterion among the examined three criteria.

When we use the confidence as a nile selection criterion, we tend to select association niles with high confidence but low support. On the other hand, we tend to select asswi- ation niles with high support but low confidence when we use the support as a nile selection criterion.

5. Inhomogeneous Partitions  In computer simulations in the previous section, we used homogeneous fuzzy partitions using symmetric triangular membership functions. While such partitions have been of- ten used in many applications of fuzzy rule-based systems, those partitions are not optimal from the viewpoint of the classification pcrformancc of cxtractcd linguistic rulcs. In the previous section, we also used interval partitions ob-     Linguistic  r* Number of tules for each class  Figure 8. Results by the confidence.

100, I  Number of rules for each class  Figure 9. Results by the support.

tained from homogeneous fuzzy partitions. Such interval partitions arc not optimal, either. In this scction, wc dis- cretize the domain of each attribute into some intervals us- ing the entropy measure (for details, see Quinlan [31). The domain of each attribute was discretized independently of the other attributes. Let S be the number of intervals for each attribute. That is, the domain interval [0, 11 was dis- cretized into A' intervals using (A' - 1) threshold values.

The threshold values were selected from (m- 1) candidates, each of which is the mid-point of a pair of neighboring at- tribute values in the given m patterns. All the m - ~ C ~ - l combinations for selccting (A7 - 1) thrcshold valucs from (rn - 1) candidates were examined by calculating the cor- responding entropy for each combination. The discretiza- tion with the minimum entropy was selected for each at- tribute. We performed this discretization as a preprocessing procedure before the rule extraction. For comparison, we specified the corresponding fuzzy partition from the inter- val partition as shown in Fig. 11. The specification of the fuzzy partition in Fig. 11 is based on the following two con- ditions: 1) The sum of neighboring membership functions is always 1, and 2) Crossing points of neighboring mem- bership functions coincide with the threshold values in the interval partition.

I  Yumber of rules for each class  Figure 10. Results by the product.

. .

I !  I I I 1 0.0 1 .o  Figure 11. Inhomogeneous fuzzy partitions.

Using inhomogeneous partitions based on the entropy criterion, we compared fiizzy partition with interval parti- tions in the same manner as Subsection 4.2. Simulation results are summarided in Table 6 - Table 9. From the comparison between the simulation results in this section and those in Subsection 4.2, we can see that the discretiza- tion based on the entropy measure improved the classifica- tion performance of standard association rules on training data (comparc Tablc 4 with tablc 8). Wc can also obscrvc from the comparison between Table 3 and Table 7 that the generalization ability of linguistic association rules does not strongly depend on the choice of a fuzzy partition.

Table 6. Results on training data (fuzzy).

#oflinguistic terms 1 2 I 3 I 4 I 5  Directuseofconfidence I 93.7 I 93.3 I 97.2 1 98.3 Definition in(23)-(24) I 97.8 I 94.9 I 97.2 I 98.3  No rule weight I 68.0 1 50.0 I 45.5 I 28.7  In the same manner as in Subsection 4.3, we performed rule selection using the three criteria: the confidence, the support, and their product. We observed from simulation results that the generalization ability of standard association rulcs was drastically improvcd by the use of thc cntropy- based discretization and the rule selection. In Fig. 12, we     Table 7. Results on test data Ifuuvh ?  Direct use of confidence Definition in (23)-(24)  No nile weight  Table 9. Results on test data fintervall.

93.3 91.6 95.5 94.4 94.9 92.7 96.6 94.4 67.4 50.0 43.8 24.7  I #of linguistic terms I 2 I 3 I 4 I 5 I Direct use of confidence Definition in (23)-(24)  No rule weight  I #of intervals 1 2 1 3 1 4 1 5 1 92.7 76.4 64.0 52.8 92.7 76.4 64.0 52.8 0.0 3.9 3.4 2.8  # of intervals Direct use of confidence Definition in (23)-(24)  No rule weight  Table 8. Results on trainina data (interval).

2 3 4 5  97.8 98.9 99.4 100 97.8 98.9 99.4 100 0.0 3.9 3.9 2.8  demonstrate the relation between the number of standard association rules and their generalization ability for several specifications of the interval discretization where Ii shows the number of intervals. From this figure, we can see that high generalization ability was obtained by appropriately choosing standard association rules.

-e- K=2 -0- K=3 -m- K=4 4 K=5  0 30 Number ofrules for each class  5 0  Figure 12. Results by the product (interval).

6. Concluding Remarks  In this paper, we extended the definitions of the two basic measures (i.e., confidence and support) of association rules to the case of linguistic rules. As we explained, linguis- tic d e s  are much more intuitive than the standard associ- ation rules in some situations. We demonstrated through computer simulations that the generalization ability of lin- guistic rules is less sensitive to the choice of a discretization method and the number of rules than that of standard associ- ation rules. Thus linguistic rules have two advantages over thc standard association rulcs: intuitivc intcrprctability and robust performance. Of course, the usefulness of linguis-  tic rules is problcm-dcpcndcnt. Linguistic rulcs are morc intuitive in some situations, and standard association rules are more appropriate in other situations. We also demon- strated through computer simulations that we can obtain high generalization ability from standard association rules by appropriately specifying the discretization of continuous attributes and the number of rules.

