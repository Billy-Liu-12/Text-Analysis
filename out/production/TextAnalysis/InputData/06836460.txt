

Abstract?Aerospace avionics and operation need another dimension. The ?glass cockpit? came as a response to an impossible perceptive situation, when pilots had to monitor countless gauges (the concept being ?one info-one gauge?). The change came with the renegotiation of the info/surface ratio given to data at the pilot level by using CRTs to integrate data in a user-friendly (dedicated) format. Since then improvements were made on the same paradigm, through very efficient cyclic design. Yet, in a way and to be blunt, little has been done, but just cosmetic changes regarding the distribution and size of screens. One does not want to change a ?winning team?. But the efficiency of the paradigm has faded away with the evolution of the aeronautical environment (traffic increase & permanence of service). The today?s problem lies with ?non- defective aircraft? monitored by ?perfectly trained crews? still falling from the sky. One explanation is, at the crew level, that we have reached a system complexity that, while acceptable in normal conditions, is hardly compatible with human cognitive abilities in degraded conditions.  The today?s mitigation of such risk relies on the enforcement through intensive training to manage extremely rare (off-normal) situations explained by the potential combination of failures of highly complex systems with variable environment & with variable humans. Looking back into the limits and strengths of operators, we may find with some very basic knowledge on human cognitive strategies ways to revisit and review our design principles to give back to pilots the ability to stay in the loop: not through the management of more & more complex systems, but by helping them doing what they do best, manage their own resources to make adequate decisions.

First, basic ground rules regarding human factors must be known and accepted by the designers and planners. Second, design must respect cognitive strategies ?naturally? followed by operators to spare their resources. A form of ?ecological? design will follow, facilitating operators? economical interactions. In my opinion, the probabilistic design for reliability concept is an important step in the right direction.

TABLE OF CONTENTS  1.! INTRODUCTION .............................................. 1!

2.! THE ANNOYING TRUTHS ABOUT HUMAN OPERATORS THAT ENGINEERS NEVER WANTED TO HEAR ABOUT .................................................... 1!

3.! COGNITIVE RESOURCES MANAGEMENT AS A REFERENCE FOR DESIGN ....................................... 3!

4.! CONCEPT DEMONSTRATOR ........................... 5!

5.! CONCLUSION ................................................. 6!

REFERENCES ................................................. 6!

1. INTRODUCTION Since the early sixties, there have been few breakthroughs in the cockpit display, one is the glass cockpit (as a renegotiation of data info/surface ratio at pilot level) the other being the Head Up Display (as a way to fuse conform data to the outside view).

Both came as answers to pilot resources being unable to deal with display demands. Since then and to be blunt, though improvements were made, they were mostly confined to the distribution and size of screens. One does not want to change a ?winning team?. But the efficiency of that display paradigm has faded with the evolution of the aeronautical environment (traffic increase & permanence of service).

Our current situation is that ?perfectly functional aircraft? monitored by ?perfectly trained crews? are falling from the sky. For the crew, a level of system complexity has been reached again, that while acceptable in normal conditions, is hardly compatible with human cognitive abilities in degraded conditions.

To mitigate such risk, companies rely on intensive training enforcement. At great cost then, can crew manage extremely rare (off-normal) situations. These occur through the potential combination of highly complex systems failures with variable environment & with variable human beings.

By reviewing the limits and strengths of operators, we found and explain some very basic knowledge on human cognitive strategies that can help revisit and review our design principles. Our objective is to give back to pilots the ability to stay in the loop: not through the management of more & more complex systems, but by helping them doing what they do best, manage their own resources to make adequate decisions.

As a prerequisite, we will review some basic ground rules regarding human factors that designers and planners cannot ignore. Then we will detail cognitive strategies that designers must acknowledge and that are ?natural? to operators sparing their resources. Such ?ecological? design will facilitate operators? economical interactions thus improving their ability to stay ?ahead of the aircraft? no matter what.

2. THE ANNOYING TRUTHS ABOUT HUMAN OPERATORS THAT ENGINEERS NEVER  WANTED TO HEAR ABOUT I?m not an engineer; I?m a Human Factors Engineer, but not an ?engineer? engineer. So I speak the language, use some of the idioms but deeply inside I?m a physician and I?m fluent in psychology, though I?m not a Psychologist. What I mean is that I understand humans using technology AND I understand the complexity of designing technology. This is about bridging the gap between technology and human users through the revelation of some ?annoying? yet very useful truths.

The plea for the operator centered approach in design  Now why would any engineer try to incorporate into a perfect machine design some unclear and imprecise dimensions from a human operator that would threaten the very mechanical precision of any design?

There are many ways to talk about the position of an operator in a system. The worst ever, to my knowledge, was given to me by a defense contractor that once told me that ?the human operator was the weak link of the system and whenever possible he/she should be replaced by any kind of automation?. Of course he was the weak link in the system he was designing, he just didn?t have enough brain to understand it.

So NO, the human operator isn?t any sort of weak link, in fact the human operator is the reason for the mere EXISTENCE of the system. There is no such thing as a system without a human being. Archeologists know that.

They call them artifacts. Pieces of man made ?things? that no one as heard of, that remain puzzlement and speculations with no proof to whatever their use was.

Figure 1: a tool is an artifact that was given meaning through human use. Without a known use an artifact is  a senseless piece of junk.

The word is out: USE. It transforms any kind of artifact into a TOOL. The use gives the artifact (the technology) a meaning with regards to Human USE. It is because we have needs that we shape artifacts into tools through usage [1] There is no such thing as a system without a Human being because that system only makes sense because there is a human use behind it. Hence the first ground rule:  ?Thou shall not design without the human use in mind?  The plea for the operator?s right to be different  We are all born equal ?in rights?, but for everything else we are all ?different?. So Variability is another key word to understanding Human limits and strengths. First there is inter-variability. Through gender, creed, size, strength culture, etc. we are all different. In figure 2 one can witness common human height variability. But there is more, people of the same height may have different respective body lengths (short legs and long torso VS long legs and short torso) and the same height when standing. Yet if they sit, the difference in torso height will become obvious.

Figure 2: variability of human dimensions, example of height and arms length  All human dimensions (physical or not) are subject to a certain degree of variability within the Human race.

There is no such thing as a constant average Human.

Engineers should not trust their opinion on the end user.

Only the confrontation to reality will enable design progress by testing it against a rough environment and variable human being . This will promote the use of cyclic design (design!try!analyze fault! design!?). Hence the second ground rule:  ?Thou shall never think you know enough about the end user ?  The plea for the operator?s creativity  The last time you opened a paint bucket, you were faced with a dilemma: how do you open it without a specific tool?

You were confronted by a urgent need that had no imminent real solution. So like everyone else you looked around for a semi sharp object yet sturdy enough to make a lever without bending or breaking to pry between that thin opening and force the cover open. What did you select as the perfect ?tool?:  a ? screwdriver. The artifact you selected has a handle for rotating grip enhancement and a thin yet sturdy blade that ends with a flat form that fits screws. By using that artifact for another use, you created a ?new? tool. And it served its purpose well. Yet was it designed for that? Did ?prying open a can of paint? fit the specifications that were made to the designers in charge of that ?tool?? NO, at least not initially. But constant use of screwdrivers as prying tools, chisel, drift punch or else? had them to incorporate those uses as needed specifications (the customer is always right, no?). What you did with this screwdriver is called a       CATACHRESIS [2] [3], a deviation in use. People do that all the time. Even with complex systems, usually when those refuse to do what the operator wants to achieve. They find a way around, a simplification, a short cut, any way to use it that would require less investment (or fatigue) for an acceptable result. Such catachreses are a wealth of information about how to enhance the system you designed.

Always look for them when talking/observing the end users of your technologies. Collect them & analyze them because they reveal the true need that is not satisfied by what you designed. Hence the third ground rule:  ?Thou shall expect & analyze end user creativity ?  The plea for the operator?s limits and resources  As our arms have a certain reach, it is useless to design any handle anywhere further then that length. Consider the same constraint for any human dimension, physical or not. It is inhuman to ask for anyone to remember multiple sets of passwords and logins using each: Min. 11 char, lower & upper cases, numbers, non Alpha-num, changing every 3 months ? the outcome is unavoidable: DEVIANCE. The average memory capacity is unable to tackle such challenge (at an acceptable resource cost). No one will comply. They will use cyclic mnemeotechnics (password01, usually windows will need 12 iteration before accepting a old password again), cheat sheets, excel files protected by a single password, ? anything to avoid the painful process of remembering it all! The cost is too great compared to the understood risk. Of course when you do not know what to protect it?s easier to protect all, thus imposing protection protocol on meaning less data, thus demonstrating the uselessness of the protection! Human beings, their psychology & physiology, are build around the concept of saving energy. The efficiency of the brain for understanding perception (visual, aural, tactile, olfactive, ?) is unmatched by technology, so efficient with so little resources. Let?s not fool ourselves; we never willingly do more than expected.

The principle of ?good enough? for the minimal investment defines our daily objectives. We are governed by that need to spare ourselves to achieve the one indisputable goal we are genetically designed for: perpetuate the species; the rest is entirely optional? Hence the fourth ground rule:  ?Thou shall know & respect end user limits & resources  3. COGNITIVE RESOURCES MANAGEMENT AS A REFERENCE FOR DESIGN  Basics of resource theory  Management of cognitive resources is a key interest in cognitive psychology. The fact that there could be a ?fuel? for cognition that one could evaluate to assess operator?s state and limits has been around for a century [3].

Moray [4] hypothesized that Human beings have a central processor with limited processing capacity. Capacity would be allocated to diverse mental operations that would diminish the available potential and allocation is subjected  to task demand. Kahneman [5] argued that there would be a limited reserve of capacity available for distribution, hence suggesting that performance would depend on the intensity of allocation and that at higher allocations, performance wouldn?t last as long. Management of ?capacity? becomes then a performance issue, with poor management strategies leading to poor performance and good capacity management strategies enabling optimal efficiency with the same initial potential. In 1977, Sperandio [6] witnessed changes in cognitive resource strategies when observing aircraft controllers confronted to increasing number of aircraft to control, thus suggesting that operators never wait for extreme work demands to adapt their work strategy. Their objective is to minimize the impact on their invested resources, keeping them in a more comfortable zone. As of today, there is still no academic consensus on cognitive resources, so let?s assume that there is some form of energy fuelling and limiting our cognitive capacities that while not yet fully described can still be used for understanding cognitive strategies.

Cognitive resources as a fuel consumption paradigm  The tank is limited (there is no way to stock more than the upper limit). It delivers a flow. ?Consumption level? is related to a certain flow. The bigger the flow, the shorter the ability to maintain it at that level, i.e. one can sustain a very straining effort for only a limited time. Regulation is the key to its management. Good managers like to stay at a confortable level of cognitive ?flow? (investment in the task). That comfort zone is known to all of us, it is where we feel ?in control?, with the feeling of having enough resources available to face what is coming. A feeling of workload appears if we sense that the constraints (demands from the situation or lack of time) overflow the amount of resources we think we can mobilize. As an opposite, when the demand from he environment is very low (thus needing very little resources to deal with) we have a feeling of boredom that is also hard to maintain. Either extreme situation induce a sense of discomfort and usually lead the operator to a change of strategy (internal renegotiation of the objective, due date, risk/performance choice, ?etc., of the task at hand) to return to that comfort zone (figure 3).

Figure 3: a representation of cognitive investment and the feeling of comfort (or not) induced by the various  possible levels.

For example, pilots need to be very good at managing their resources, because they navigate a highly dynamic and risky environment and drive highly a complex system. When faced with such a limitation of available resources, they use a number of strategies to maintain their resources at an       acceptable level (as a matter of fact every one does the same with maybe a less effective result?):  ? Routinization of behaviors through training to the level of instinctive responses  ? Delegation of work to available agents (human or not) ? Schematization of information to simplify their  management ? Use of anticipation and planning   We shall review the characteristics of each strategy and its implications for design of systems.

Routinization of behaviors through training to the level of  instinctive response  Training is very effective for reducing the cognitive cost of task. If you remember those first driving lessons? all that had to be done, controlled, checked at the same time, straining to the point of exhaustion. And now, years of practice later, it?s a breeze; you can even chat or tune the radio while expertly controlling the vehicle in total safety.

The more you train, the more you automate your response (to a reflex like behavior).  Experience favors the efficiency of one?s internal library of recognition patterns for detecting significant cues in a given situation (?I?ve been in this situation before, I recognize it?) and remembering the associated solution to deal wit it (?I know what to do, I?ve done it before with a certain success?).

But remember also the constraints of training. First training is expensive and is now a budget that clients of complex systems like aircraft look into for reducing costs; an intuitive HMI means less training & safer usage. Second, training has to be done before being efficient in reducing cognitive load, it?s also time consuming.

Last but not least when dealing with a novel system (supposedly helping to achieve a better performance) any operator has to ?feel? the benefit of investing in such training for a gain of performance. It?s a ?return on training investment? type of assumption. It is often the case for systems with fast learning curves.

Delegation of work to available agents (human or not)  Second very effective way to alleviate one?s resource demands is to share some of the workload with someone else a part that you initially had to do. Delegation isn?t a last minute choice, or it will fail. It has a cost; it implies anticipation, relies on a robust knowledge of the delegation recipient and remains fragile through the adjustment one makes between control and confidence.

Articulation work is the work one invests in when participating to a collective achievement. The weight of the benefit (how much one will regain by not doing the delegated part) must be appreciated with regards to the efficiency of the recipient with the given part. It implies knowledge of the ?others?: culture, education, habits,  efficiency, stress resistance, resilience, ? but also a certain level of shared references that will enable the collective to bypass implicit knowledge. To gain and maintain such knowledge, one must communicate. Articulation work relies on substantial professional communications and is often trained by Crew Resource Management (CRM) in aeronautics. Understanding one another is always the key to successful delegations.

All the above is also valid for an artificial agent. The system needs to be aligned with the potential of the human agent (level of training, detection of cues and compliance to procedures). From all this comes a confidence degree into the efficiency of the delegation through a definite level of control with an accepted level of risk. In fact, delegated work relies on control-confidence adjustment that is directly linked to an accepted level of risk. One can delegate with a lot of confidence and no control on the efficiency of the delegation, thus taking the risk of failure through lack of verification of subtask results. On the other hand, one can delegate with little confidence and invest a lot of resources on regular efficiency verification, thus rendering useless the benefit of delegation for regaining resources.

Delegation needs to be prepared at the initiator?s level but also at collective level. One must be able to choose wisely what should be delegated and not let the overflow of demands automatically dump the last incomings. It takes a certain amount of resources to choose to delegate.

Finally, delegation by being based on the efficiency of articulation work is the first to be discarded when workload increases, communications drop to a minimum and every one (except the system) focus on their own (short term) objective, ignoring all the other agents. Delegation monitoring in high workload situations is therefore fragile.

Schematization of information to simplify its management  We rely on condensed representations to manipulate concepts, ideas, references, ? everything that we need to use to understand and act on the world around us. Schemas are representation dedicated to the object of our plan of action. They are formatted with efficiency and use in mind.

Efficiency, because they are detailed to the just need, with a lot of default data, they are adaptable, inherently incomplete (by lack of details that need to be filled with the specifics of the situation at hand), goal oriented and structured around abstraction-refinement levels [7].

For instance, if we have to deal with a malfunctioning washing machine, the abstraction schema we will need will be specific to the type of suspected problem: fluid, pumps, pipes & tubing if we have a leak; electric blue print if it?s an electrical problem. That abstraction can then be at various refinement level (the amount of detail) and we use the ?just enough? detailed level to spare the resources needed to manipulate that schema.

An HMI should whenever possible follow the structure of our schemas to represent data. If the system?s representation fits our own, the transfer of information it presents is seamless. There is no need for painful computing, transformations or interpretations. The data fits the need.

The information is directly useful in coherence with the representation we rely on to deal with and control it. The loop from perception to action is thus greatly reduced. The more we rely on end user schematization for our HMIs, the more they will feel it as intuitive and easy to use.

If the operator uses spatial cues in the environment, the HMI should help with such type of spatial referencing and offer 2D or 3D maps. If the operators keeps computing time data for tasks to come one should help with time manipulation through timelines or Gantt like representations.

Use of anticipation and planning  Planning is a key resource saver. When you know what possible (potential) things are going to happen and how you?re going to deal with them, the search for a match between ?cues about the situation? and ?solution to it? is much faster. It restricts the whole process to what has been prepared.

There are two ways to anticipate: pro-action and preparation of action.

Pro-action is about doing a task in advance, so you won?t have to do it later when you?re too busy. We all remember the constraint of late homework completion when we were younger. That is typically a poor pro-acting anticipation.

When an operator has enough time to analyze upcoming work load, he/she will look for bottlenecks that can be dealt with, i.e. task not too constrained in time so they can be moved before or after their initial position in the upcoming ordered task plan. Doing something in advance implies follow-up measures that can be also resource consuming, one must remember (i) that the state of the task that was acted on has been changed and (ii) that the subsequent consequences of that action can also be of importance. Pro- action must be helped with some sort of follow-up device that will facilitate its control and monitoring (over a time line for example).

Preparation of action is already much in use with professionals in the aerospace domain. Amalberti [8][9] mentions the preparation in advance of an answer to a specific question as an example of a way to spread the cognitive cost of an interview. One is building a ?routine? for a specific question. Before a flight, pilots ?brief?. They reach into memory for those ?potential future situations? they might encounter (and they have encountered before and know how to deal with), and put those schemas into the light of the future right at hand ?now and here?. They contextualize prototypical schemas with the reality of today, here and now.

After such a review they accept the ?deal? at hand because they know they have the answers for the problems of the probable future, and putting those answers to work will require less effort that to search for them at the last minute.

But there are limits to the efficiency of preparation of action. The first one is due to memory capacity. There is a limit to what can be memorized for a given task. Second, there is a trade-off between preparing (because it needs resources) and the efficiency of that preparation (in terms of saving resources). Lastly, there is a combinatory explosion of possible futures with time and there is no efficiency of preparing too much in advance if the situation prepared for never comes up.

4. CONCEPT DEMONSTRATOR A HMI prototype based on such concepts was developed in the Human Engineering Aerospace Laboratory [10][11].

Anticipation Support for Aeronautical Planning (ASAP)  The ASAP HMI relies on 3 concepts: (i) the use of a time line reference (besides the classical spatial reference like map displays) to synchronize available data. (ii) The possibility to represent a number of abstractions (points of views) on the time line (upcoming tasks, fuel consumption, Vertical display, waypoints, weather, ?) representing pilots schema and synchronized on the time line. (iii) the use of what-if queries to fulfill pilot?s need for optimization of resources (either theirs or the aircraft)    Figure 4: ASAP HMI with on the left a control panel and on the right two abstraction levels separated and  synchronized by a time line. On top the task abstraction with 3 levels of refinements, on the bottom, the vertical  display with waypoints and altitudes.

The figure 4 shows a sketch of the HMI with the time line, the abstraction zones (with their potential for refinement display) and a control zone used for management and ?What-ifs?1. The project was partially funded by an ARTEMIS European project called ASTUTE [12] and its results will feed the reflection in a FP7 project called ACROSS [13].

1 A "what-if" function gives the operator the ability to test hypothesis on future command to see their effect with the possibility to "undo" without consequences.

5. CONCLUSION All that was presented here is based on the will to change from techno-centered design to Human-centered design.

Obviously, there is no such thing as a system without a Human. Our primary concern then should be about the Human in our designs and the way operators are driven by parsimony at psychological and physiological level.

Applying cognitive knowledge in HMI design to facilitate Human strategies for managing their resources is today?s key to Human centered innovation. Designing with the human cognitive limits, strengths and strategies in mind is an Ecological [14] process, it means respecting the nature of human cognition. By doing so, we will not deal with the system?s complexity, but we will improve its emerging part at user level and shrink it to a bearable difficulty level.

All efforts not made at design level will cost dearly at user level.

