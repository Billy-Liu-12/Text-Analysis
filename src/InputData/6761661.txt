HOW TO IMPROVE EFFICIENCY ON ANALYSING BIG DATA ON A LARGE  NUMBER OF MEASUREMENT AND VERIFICATION PROJECTS

ABSTRACT  Big data can be defined as a collection of large and complex data sets which are difficult to process using readily available or traditional data processing applications. These big data sets introduce new challenges such as capture, storage, search, sharing, transfer, analysis, versioning and visualization.

This paper present a design philosophy, techniques and experience, from an information technology perspective, in dealing with large quantities of energy data. It provide a vehicle for the general (non-IT) user to increase his or her efficiency in analysing and visualizing energy data for business decisions.

1.  INTRODUCTION  The inevitable result of dealing with a vast number of energy projects is big data. Big data can be defined as a large and complex collection of data which became difficult to process using readily available or traditional data processing applications [1]. Raw data on its own is nothing but a bunch of unusable numbers, letters, or words and working through thousands of these can be laborious and confusing [2]. Certainly, the volume of data makes it big, however even small amounts of data can be leveraged by some new techniques [3]. In other words, data is not just big in terms of size, but also big in terms of comparative advantage.

This paper focus on the infrastructure and technologies underlying the management of large quantities of Measurement and Verification (M&V) related data. M&V has the objectives to quantify and assess impacts and sustainability of Demand-Side Management (DSM) and energy-efficiency projects [4].

Usually M&V teams receive data in no standardized manner.  The means of data collection may vary from manual techniques such as text files received via email, to automated systems integrated with real-time-metering solutions. Without the correct systems in place, the unavoidable consequence will be unstructured and distributed data which hold challenges as numerous as the opportunities.

Effective data management and the ability to refine data into greater business insights may deliver benefits such as profitable decision-making, enhanced quality of service and new product offerings [5]. Data is changing the game in a lot of industries and those who have the ability to wield it, can perform more efficiently than those who do not. Once the domain only of actuaries and scientists, big data can  now be exploited with new technologies by a wide range of people [3].

This paper briefly discuss a typical M&V data life cycle and its underpinning infrastructure to support further data mining.

2. THE DATA LIFE CYCLE  At the start of the data life cycle, individuals will have primary responsibility for stewardship [6]. But, if data grows rapidly and are to be consolidated and shared frequently a good and reliable infrastructure is essential.

The importance of a solid data infrastructure cannot be more emphasized as scientists are not necessarily good data managers and can more effectively spend their time doing science. Also, it is increasingly ineffective to assign long term information management tasks for example to a rotating staff of students or interns [6].

Figure 1 depicts the typical life cycle of M&V data.

Because of the various data sources and collection techniques of raw M&V data, the data is unstructured and dispersed and must be standardized. The final outputs can be numerous, but most common is to use data from one or more projects to generate reports or to mine valuable information from the data sets by further exploration.

Figure 1: M&V data life cycle   The life cycle stages as depicted in figure 1 will be shortly discussed in the following sections.

2.1 DATA CAPTURE  The first step is to Extract, Transform and Load (ETL) data from the different sources of energy (and other) data for each M&V project to a central database. A web based application allows general (non-IT) users to map data sources, whether it is a locally stored text file or data from a real-time metering solution from a relational database. The application also allows users to add functions to calculate  DB  www  ETL DB  text  Project  Calc  BI  Reports  Analytics      new fields e.g. to calculate demand (kW) values from one or more fields. The initial setup is a fairly once off process, if  the supplier of the data is consistent with data structures and delivery methods. New data is periodically fetched from the specified source(s) and imported to the raw set of data. Where applicable the original documents are also stored. For Eskom M&V reporting the outcome is always 30 minute interval demand (kW) values (and related data such as kVA, temperature, status, etc.) which is stored in a relation database, centrally available to other systems.

2.2 PROJECT INFORMATION  A user will add a new M&V project and capture specific and supportive information such as descriptions, geographic information, demand and/or energy targets, tariffs, etc.

Also, part of the project is setup to fetch the correct data from the centralized collection of raw data (as discussed in section 2.1). The project owner (user) can export the 30 minute interval demand (kW) data for instance to use in a set of tools to engineer a baseline. A baseline is also uploaded to the central database and associated with a specific project.

Figure 2: Visualization of Actual and Baseline data   At this stage a project is associated with monthly actual and baseline data (30min kW). A web based tool allows users to visualize the data (see Figure 2)  in order to confirm data quality or spot irregularities.

2.3 CALCULATIONS  At first all calculations was done on the fly when a report was generated. Calculations varies from the grouping of data into Eskom MegaFlex Time-Of-Use (TOU) periods, computing of targets and impacts, tariff calculations, handling public holidays, etc.  However, when data sets accumulated to a few millions of records, report generation became time consuming. On top of that new report requirements increased complexity of calculations which on its turn worsen generation time and degrade user experience.

With traditional databases (relational databases) it is easier to get the data in than out as these are designed for efficient transactions processing such as inserters, updates or  retrieval of small amounts of data [7]. Because of experiencing this first handed, a complete reconsideration of the infrastructure designs was necessary. This led to the investigation of a move towards a Business Intelligence (BI) system.

Instead of concurrent calculations, calculations are initiated by the user as soon as data (actual and baseline) are imported. The calculation results are stored in a data warehouse which is also centrally available for any client tools to utilize. Data validations exists to ensure calculated data match with source data. It can happen that source data changes, e.g. better quality data was received and imported or there was a change to the tariff being used. In  such a case, calculations for the specific period will rerun.

2.4  REPORT GENERATION  Getting data in has limited value, but when users and applications can utilize the data to support decisions the full value of the data warehouse can be realized [8]. The generation of M&V or related reports is one of the end goals of the infrastructure in place and the final stage in the data life cycle.

Fundamental to a good user experience is quick report generation. Stored calculations in a data warehouse, holds the advantage of rapid report generations. In a recent test case the ability was demonstrated to calculate M&V project summary results for the last month, Year To Date (YTD) and Inception To Date (ITD) for more than 300 projects in just over one minute. Amongst others, the results contained impacts, averages and totals for each Eskom MegaFlex TOU period and for each day type (Weekday, Saturday, Sunday).

However, the value of the data it is not only limited to generating reports. The populated data warehouse is the fundamental core for analytics and the exploration of further information  from it, will be discussed in section 3.

3. SELF SERVICE BI  Business Intelligence (BI) refer to the key technologies that support the activity of transforming data into useful information [9]. The main technologies of BI encompass data warehousing, On-Line Analytical Processing (OLAP) and data mining [10]. The purpose of these systems is to support managers and other levels of decision makers in an organization to make informed decisions.  Also, BI as a term replaced decision support, executive information systems, and management information systems [10].

To implement BI, an organization need to rely heavily on highly skilled professionals to source information from data using enterprise BI tools such as SAS or Business Objects [11]. Also the complexity associated with the implementation of a BI system may require considerable resources. Self-service BI is the new buzzword, which allows users without specialised BI training or analytics experience, to develop data models and calculations      (except the setup of a data warehouse underpinning the BI system). [12] Furthermore, small- to mid-size companies need the ability to yield information without requiring unreasonable investments in hardware, software and people.

In the engineering world, where M&V is no exception, Microsoft Excel is probably the most used tool. However, dealing with millions of rows in Excel can be challenging and time consuming. But, Microsoft released PowerPivot which is a add-in to the 2010 version and a built-in functionality of Excel 2013. As a self-service BI product, PowerPivot allows users to conduct powerful BI in an environment that is already familiar and allows processing and analysing vast amounts of data that come from different and dispersed sources for discovering new knowledge [13].

Figure 3: Subset of a PowerPivot data model with relationships.

In this contribution, we show that Excel PowerPivot tables can be used to answer many questions or needs. Because self-service BI in the form of PowerPivot is used mostly by non tech-savvy staff, the IT department help users to understand what data is available and often also help to prepare the initial data models with its relationships and hierarchies (see Figure 3). The available data from the data warehouse and/or other sources can be defined as a meta- data dictionary at the business layer level.

Figure 4: Tabular view of a data model.

By giving the staff Excel documents with already build data models, helps them to focus on the meaning of the data and not necessarily the technology itself. Another major advantage is that the data in the Excel document is preloaded and disconnected from the life systems, which  gives the user the freedom to add calculations or run big queries without hitting the databases or data warehouse with straining queries. Also, users can design and customize their own reports and create their own views of the data.

Using PowerPivot as a self-service BI solution empower users with a large number of features and advantages. A short discussion of four of these is given in the subsequent sections.

3.1 MANAGE LARGE DATA SETS  As mentioned previously, a standard Microsoft Excel worksheet is not ideal for large amounts of data. With a PowerPivot data model, the user can enjoy fast calculations on millions of rows with about the same experience as a few hundred of rows.

3.2  COMBINE DIFFERENT DATA SOURCES  Data can be combined from multiple sources including relational databases, spread sheets, reports, text files, internet data feeds, etc.  This functionality is really powerful. For example, a user retrieved M&V project data (actual and baseline) from the data warehouse, but needs to correlate it with weather patterns. It is as easy to retrieve the weather data from an Internet data feed and relate it to the energy data timestamps. Also, a nice and important feature is that a user can refresh data to ensure that decisions is based on the most recent data.

3.3 CALCULATIONS  Once the data is retrieved, the user has the freedom to add relationships, hierarchies, calculated columns,  measures or key performance indicators (KPIs) and has the ability to literally aggregate over millions of rows. Data analysis expressions (DAX) is used for calculations, which is a very powerful formula language quite similar to normal Excel expressions. For example use =[BaselineValue] ? [ActualValue] to calculate a new field named [Impact] (see Figure 4).

3.4 VISUALIZATIONS  PowerPivot for Excel allows users to create a variety of visualizations in normal (familiar) spread sheets. For example, data can be presented in pivot tables, pivot charts, combinations of tables and charts or flattened pivot tables (see Figure 5). Another useful feature of Excel 2013 is PowerView, where users can design interactive reports that allows interactions such as drill-in and out.  The discussion of the functionality of these are beyond the scope of this paper. However, by combining these different visualizations, an appealing dashboard view can be built for really easy and fast analytics.

Figure 5: PowerPivot with a PowerChart visualizing a summary  of 25 M&V projects using more than 2 million records.

According to research, organizations must embrace visualization tools as a form of communication, because business success can be hindered by falling into old habits of representing data [14].

4. CONCLUSION  This paper included a discussion on the typical M&V data life cycle and mentioned some of the underlying technologies and importance of a good and solid infrastructure. The heart of the system is a good designed data warehouse which allows instantaneous generations of large reports. However, fixed reports is merely one of the numerous outputs. The true value lies in further exploration of valuable information hidden in the sea of data. Instead of investing in an expensive end-to-end BI solution (which may have steep learning curves and require additional resources) a self-service BI solution was proposed.

Microsoft Excel PowerPivot was presented as a self-service BI solution to handle a vast amount of M&V data from a data warehouse (and other data sources) for discovering new knowledge. The major advantage of this solution is that users work in a familiar environment where the focus is on the value of data instead of the technologies and infrastructures.

5. REFERENCES  [1]  White, T.: Hadoop: The definitive guide, O'Reilly  Media, Inc., 2012.

[2]  Making sense of the Chaos, Koninklijke Philips  Electronics N.V., 2012.

[3]  Boyd, D. and Crawford, K.: ?Critical questions for  big data.? Information, Communication & Society, Vol. 15, No. 5, 2012,  pp. 662-679.

[4]  Den Heijer, W. and Grobler, L.: ?The Measurement  and Verification Guideline for Demand-Side Projects (v5r3)?, 2006.

[5]  Apte, C., Liu, B., Pednault E. P., and Smyth, P.,  ?Business Applications Of Data Mining? Communications of the ACM, Vol. 45, No. 8, 2002, pp. 49-53.

[6]  Lynch, C., ?How do your data grow?,? Nature, Vol.

455, No. 7209, , 2008,  pp. 28-29.

[7]  Jacobs, A., ?The pathologies of big data,?  Communications of the ACM, Vol. 52, No. 8, 2009, pp. 36-44.

[8]  Watson H. J.  and Wixon B. H., ?The Current State  of Business Intelligence,? Computer, Vol. 40, No. 9, 2007, pp. 96-99.

[9]  Moss, L. T. and Arte, S.: Business Intelligence  Roadmap: The Complete Project Lifecycle for Decision-Support Applications, Boston, Addison- Wesley Professional, 2003.

[10]  Thomsen, E.: ?BI?s Promised Land,? Intelligent  Enterprise, Vol. 6, No. 4, 2003, pp. 21-25.

[11]  Pirdue, T.: About.Com. [Accessed 25 May 2013]. 5  Cool Things You Can Do With PowerPivot for Excel http://newtech.about.com/od/microsoft/tp/5-Cool- Things-You-Can-Do-With-PowerPivot-For- Excel.htm.

[12]  Rouse M.: TechTarget. [Accessed 25 May 2013]  SearchBusinessAnalytics.

http://searchbusinessanalytics.techtarget.com/definiti on/self-service-business-intelligence-BI   [13]  Umpetha IT Solutions. [Accessed 26 May 2013] 10  reasons to use PowerPivot for self service Business Inteligence.

http://www.umpetha.co.za/10-reasons-to-use- powerpivot-for-self-service-business-intelligence/   [14]  Frankel F. and Reid, R. ?Distilling meaning from  data? Nature, Vol. 455, September 2008, pp. 30.

6.  AUTHORS  Principal Author: Gerhard de Jager holds a M.Sc. degree  in Computer Sciences from the North- West University.  At present he is a senior software developer and software team leader at the North-West University.

Co-author: Prof. LJ Grobler is full professor at the  School for Mechanical Engineering.  He specialises in energy management and has been certified as an Energy Manager (CEM) by the Association of Energy Engineers in the USA.  He also obtained a certification as an M&V Expert by this association.

Presenter:  The paper is presented by Gerhard de Jager.

