SIBA: A Fast Frequent Item Sets Mining Algorithm  Based on Sampling and Improved Bat Algorithm

Abstract?The adaptability of the classical top-k frequent item  sets mining algorithms is unsatisfied when being applied to some large datasets with both large scales of transactions and items. In this work, an algorithm called SIBA (sampling, improved bat algorithm (BA)) is proposed to solve this problem. SIBA applies BA and improves it by cloud model to search frequent item sets from a large number of items rapidly, and builds the sub-dataset in every iteration to reduce the scanning cost. Based on four open access datasets, it is compared with Apriori, FP-growth, and other heuristic algorithm such as PSO (Particle swarm optimization) and GA (Genetic algorithm). The experimental results show that when being applied to frequent item sets mining, SIBA is faster than Apriori and FP-growth, and meanwhile, it is more robust than PSO and GA.

Keywords?data mining; association rule; bat algorithm; cloud model; sampling

I.  INTRODUCTION Top-k frequent item sets returning the k most frequent item  sets from the transaction dataset, which is always a research hotspot in data mining. The classic algorithms in this realm are Apriori, FP-growth [1-3] and their improvements [4-6].

Despite of the wide application, these methods can?t perform well on those datasets with high number of transactions and items [1], which make some traditional operations infeasible, such as scanning all transactions even once, testing every candidate item set and so on. Therefore, some researches have been done to face them.

Firstly, some heuristic algorithms [8-9] are applied, which simulates the nature system [10], needn?t test every candidate item sets and give good solution in a little cost [11]. Mourad applies quantum swarm evolutionary algorithm to this field [11], Kuo et al propose a mining method based on particle swarm optimization algorithm [12] and Alatas et al present a way to mine frequent item sets by rough particle swarm optimization [13]. All of them have achieved the high performance. However, without sampling or other methods, they can?t reduce the time cost by scanning transactions, and the large searching space reduces the global optimization performance of some classic heuristic algorithms.

Then, some sampling ways are applied [14-18]. Toivonen proposes an algorithm, which builds a candidate of frequent item sets the probabilities according to the sample size [19].

Some progressive sampling way, which keeps increasing the sample size until the stop condition is satisfied, have been studied and proposed [18, 20-21]. However, despite they successfully reduce the cost to scan transactions, the problem brought by the large scale of items is not solved.

Therefore, a new method, SIBA (Sampling, improved bat algorithm), is proposed in this work, which has the following characteristics:  SIBA applies BA (bat algorithm) to this field [22-26].

which is a new heuristic algorithm  Cloud model is applied to improve BA.

SIBA builds sub-dataset by sampling in every  iteration to reduce the transactions scanning cost.

SIBA needs a fixed length of the frequent item set to code them more conveniently. So, it is a fast mining algorithm to mine top-k frequent l item sets.



II. BASIC CONCEPTS  A. Bat Algorithm Bat algorithm simulates the behavior of micro-bats based  on the following idealized rules [22].

1) Any bat flies randomly with velocity 1[ ... ]i idv viV  and pulse frequency min max[ , ]ifreq freq freq  at 1[ ... ]i idp piP , varying the rate of pulse emission min max[ , ]irate rate rate  and the loudness min max[ , ]iL L L .

2) If the food is closer, the irate  will be bigger and the iL will be lower.

The rules of the updating of a bat are shown as follows [22]  min max min: ( )i ifreq freq freq freq  : ( ) ifreqi i i *V V P P  :i i iP P V  where [0,1]i  is a random value according to a uniform distribution. *P  is the current global best solution among all the n bats, which is chosen through the fitness function ( )F iP .

The pseudo code of BA can be summarized as follows [22].

Input: The fitness function F.

Output: The best solution.

Method: 1) Initialize the bat population ( 1,2... )i niP  and iV .

2) Define pulse frequency irate  at iP .

3) Initialize pulse rates irate  and the loudness iL .

4) For 1 iter  (1) Generate new solutions by adjusting frequency, and updating velocities and positions by Eq. (1-3).

(2) If irand rate 1. Select a solution among the best solutions.

2. Generate a position around the selected best one.

(3) End if (4) Generate a new solution by flying randomly.

(5) If & ( ) ( )irand L F Fi *P P  1. Accept the new solutions.

2. Increase irate  and reduce iL .

(6) End if (7) Rank the bats and find the current best *P .

5) End for 6) Return *P  B. 2nd-normal cloud model Let U be a universal set described by precise numbers, and  C be the qualitative concept containing Ex, En and He related to U, wherein, Ex is the most representative sample of a concept, En is the granularity scale of the concept and He is the uncertainty of En [24]. If there is a random number x U , which is a realization of the qualitative concept C and satisfies  2~ ( , )x N Ex y , where 2~ ( , )y N En He . The certainty degree of x on U is   ( )  2( ) x Ex  yx e .       (4)  The distribution of x on U is a 2nd-order normal cloud, which is denoted ( , , )C Ex En He , and x is a cloud drop [25].

Given Ex, En and He, the 2nd-normal cloud generator, denoted CG (Ex, En, He, n) [25], can produce n cloud drops, whose pseudo code is as follow.

Input: Ex, En, He and n.

Output: n cloud drops.

Method: 1) For 1i n  (1) Generate 2~ ( , )iy N En He  and 2~ ( , )i ix N Ex y .

(2) Calculate the certain degree ( )ix .

2) End for 3) Return n cloud drops.

C. Problem definition Let ( , , , )Db tid A V f  be a transaction dataset, where  1 2{ , ...}tid t t  is the universe of transactions, 1 2{ , ...}A a a  is the universe of items, V is the value domain, which actually is {0,1}, and ( , )f t a  is the function from tid A  to V. For  a A  and t tid , ( , )=1f t a  if and only if t contains a.

For A  and t tid , t?s transaction vector under , denoted ?t , is defined as  1 2 ( )[ ( , ), ( , ),..... ( , )]cardf t a f t a f t a ?t , ia .   (9)  For t tid  and A , t contains  to the degree , denoted t , if 1|| || ( )card  ?t .

For A , the support of , denoted ( )Sup , is  defined as  1({ | })( )= ( )  card t tid tSup card tid  .          (10)  For A , let ( )b  be the set of those item sets whose supports are higher than ( )Sup . The top-k frequent l item sets under Db is defined as  , ( ) { | ( ) , ( ) }k lTop Db A b k card l         (11)

III. THE SIBA ALGORITHM  A. The code of item sets For ( , , , )Db tid A V f , let {1,2....}rA  be the set of  indexes of A?s elements. Any l item set can be coded as  1 2[ ... ]li i iI , i ri A .             (12)  Gven ii , if 1ii  or ( )ii card A , it will be modified by ( ) ( , ( ))icard A mod i card A  or ( , ( ))+1imod i card A , where  mod(a, b) is the remainder of a/b. If ii  is not an integer, ii  will be modified to [ ii ].

B. The improved BA based on cloud model Considering the large searching space in top-k frequent l  item sets mining, SIBA uses the 2nd-normal cloud model to describe the position of a bat, which simulates the disorder microscopic movements of a real bat [26] shown in Fig. 1.

The movements of a real batThe movements of a real bat The movements of a bat in classic BAThe movements of a bat in classic BA   Fig. 1. The movements of a real bat and a bat in classic BA  For Db , ( , , )i i i iBatc En HeEp  represents a bat, called bat cloud, where iEp  is the expected position, i iEn freq  and  0.3i iHe freq . For j ip Ep , ( , , )ij j i iC p En He  is a 2nd- normal cloud. The new position updated model are shown in Fig. 2 and the functions are explained in Eq. (14-17), where iD is the set of the drops of iBatc , and j iDd  is actually an item set. Dni is the number of the drops of iBatc , which is a random number according to uniform distribution. In this work,  [0,7]iDn . gbestd  is the historical global best drop of all the bat clouds. F is the fitness function of iBatc .

min max min: ( )i ifreq freq freq freq ,               (14) : ( , ,0.3 , )i i i i iD CG freq freq DnEp  ,                   (15)     : ( )i gbest ifreqi iV V Ep d  ,                               (16) : +i i iEp Ep V .                                                      (17)  The drop in step 1The drop in step 1 The drop in step 2The drop in step 2 The drop in step 3The drop in step 3 The historical global best dropThe historical global best drop   Fig. 2. The position updated model of a bat in SIBA  C. The sub-dataset produced in every iteration Suppose the mining process of SIBA is in the iterth  iteration, for ( , , , )Db tid A V f , to reduce the scale of mining object, the subset of tid , which is itertid , will be sampled to build ( , , , )iter iterDb tid A V f . Any bat cloud drop calculates its fitness through iterDb  but Db . The ( )itercard tid is set by the user depending on the real problem. In this thesis,  ( ) 500itercard tid .

Despite the sampling size of every iteration is chosen, the sub-dataset are updated in every iteration so that the probability of losing the real frequent item sets is relatively low. However, on the other hand, the sampling in every iteration brings the unstable solutions. SIBA proposes a way to evaluate the stability of every solution and to choose gbestd in every iteration.

For any item set I, let ( )iterF I  be the fitness of I  on the iterth iteration. The variance of I  is evaluated by    ( ) ( ( ( ) ( )) )  iter  iter i i  Var F F iter  I I I ,                (18)  where ( )F I  is the mean value from 1( )F I  to ( )iterF I .

The lower ( )iterVar I  means the higher stability of I .

Let iteribestd  be the local best drop of iBatc  on the iterth iteration, and itergbestd  be the global best drop on the iterth iteration. 1 2{ , ... }eiteriter itergbest gbest gbest gbestCd d d d  is the set of the e best global drops chosen from the first iteration to the iterth iteration. The gbestd  on the iterth iteration satisfies  ( ) ( )j jgbest ibest iter gbest iter gbestCd Var Vard d d .             (19)  In this work, e=5.

D. The whole pseudo of SIBA Based on the improved BA, the sampling in every iteration  and the rough fitness, for Db , the whole pseudo of SIBA is shown as follow.

Input: Db , Nbat, Niter, k and l Output: , ( )k lTop Db Method: 1) For 1m k  (1) For 1 batj N 1. Initialize jfreq , jrate , jL  and jBatc .

2. Modify jEp  in jBatc .

(2) End for (3) Build 0Db .

(4) Find *Ep . For Ep , 0 0 *( ) ( )F FEp Ep .

(5) *{ }gbestCd Ep .

(6) For 1 iteriter N  1. Build iterDb .

2. Update the variance of 'gbestCd s  item sets.

3. Choose the gbestd .

4. For 1 batj N  i Update jfreq  and jD .

ii Modify the item sets of jD .

iii Update the fitness of 'sjD  item sets.

iv Update jV  and jEp .

v Modify jEp .

vi If jrand rate  a) 1+[ ... ]j gbest lEp d .

b) Modify jEp .

vii End if viii Choose iterjbestd  from jD .

ix If & ( ) ( )iterj jbest gbestrand L Sup Supd d  a) Accept jEp .

b) Increase jrate  and decrease jL .

x End if 5. End for 6. Update gbestCd .

(7) End for (8) Add gbestd  to , ( )k lTop Db .

2) End for

IV. THE EXPERIMENTS AND DISCUSSIONS Several datasets, shown in Table 1, were used to evaluate  the performance of SIBA algorithm. They were downloaded from http://fimi.ua.ac.be/data/. All of the experiments were conducted in the environment of Microsoft Windows XP using a compatible computer with Inter i7 3.4GHz and 3.49GB RAM, and were coded by the Matlab.

TABLE 1 THE INFORMATION OF DATASETS  Dataset Average length #Items #Trans kosarak 8.1 41270 990002 mushroom 23 120 8124 connect 43 130 67557 retail 10.3 16470 88162  A. The experiment 1 In experiment 1, both Apriori and SIBA were used to mine  the top-3 frequent 3 item sets of the datasets in table 1. In SIBA, the number of iteration was set to 1000, the population size of bat was set to 20, minfreq  , minrate  and minL  were all set to 0, and maxfreq , maxrate  and maxL  were all set to 1. The minimum support thresholds of the Apriori were shown in     table 2. Apriori was used to mine every dataset once and SIBA was used to mine every dataset 10 times. During the experiment, the solutions given by Apriori, the best and worst solutions given by SIBA, and the error counts given by SIBA were recorded. The error count is the times in which the solutions given by SIBA didn?t equal the solutions given by Apriori. The results of experiment 1 are shown in table 2.

Table 2 indicates that SIBA has high performance of accuracy, which can always get the accurate solutions from any dataset in table 1 in 10 times. Meanwhile, all the error rates of SIBA were less than or equal to 10%. For the high speed performance proved in experiment 2, such low error rates can be accepted to mining the large dataset.

B. The experiment 2 In experiment 2, Apriori, FP-growth and SIBA were  respectively used to mine the top-3 frequent 3 item sets from the datasets in table 1. The speeds of them were compared. All algorithms were used to mine the datasets 10 times.

Parameters of SIBA were the same with experiment 1. The results of experiment 2 are shown in table 3.

Table 3 indicates that SIBA is faster than Apriori and FP- growth when it was used to mine ?kosarak?, ?connect? and ?retail?. Especially, when the minimum support threshold was respective 6% on ?kosarak?, 85% on ?connect? and 2% on ?retail?, it can save the time by one order of magnitude. The high speed performance of SIBA can be explained as follows.

1) When the length and dimension of a dataset is very large. Some costs brought by the traversal operation from Apriori and FP-growth such as scanning dataset, building FP- tree and computing the combination of items, make the problem become NP-hard.

TABLE 2 THE RESULTS OF SIBARS AND APRIORI Dataset Min-sup Apriori SIBARS Best Worst Error Count  kosarak 6% 3,6,11 3,6,11 3,6,11  0 1,6,11 1,6,11 1,6,11 1,3,6 1,3,6 1,3,6  mushroom 30% 34,85,86 34,85,86 34,85,86  1 34,85,90 34,85,90 85,86,90 85,86,90 85,86,90 34,86,90  connect 85% 91,109,127 91,109,127 91,109,127  1 75,91,109 75,91,109 75,91,109 75,91,127 75,91,127 75,109,127  retail 2% 39,41,48 39,41,48 39,41,48  1 38,39,48 38,39,48 38,39,48 32,39,48 32,39,48 38,41,48  TABLE 3 THE TIME COST BY APRIORI, FP-GROWTH AND SIBARS Dataset Min-sup The Average Time Consumption (s) Apriori FP-growth SIBARS  kosarak 8% 159.78 106.18 8.15 7% 189.2 137.29 7.78 6% 272.22 201.98 7.86  mushroom 50% 2.72 1.78 7.53 40% 6.78 4.27 7.54 30% 13.60 10.78 7.86  connect 95% 67.74 49.25 8.24 90% 134.02 103.19 7.71 85% 220.73 164.86 8.45  retail 3% 20.68 14.98 7.59  2.5% 25.00 17.30 7.97 2% 40.47 30.01 8.53  TABLE 4 RESULTS OF THE SPEED AND ACCURACY PERFORMANCES? TESTS ON SIBARS, PSO, GA AND THE CLASSIC BA  Database Error count The Average Time Consumption(s) SIBARS PSO GA The classic BA SIBARS PSO GA The classic BA  kosarak 1 4 3 2 8.13 5.12 6.16 6.38 mushroom 0 2 2 1 7.59 6.01 6.03 6.48  connect 1 3 4 3 7.78 5.95 5.43 6.85 retail 2 4 5 2 8.04 5.99 6.43 6.56     2) Sampling and the improved BA make SIBA not have to scan all the transactions and check all the candidate item sets, which saves much time.

However, when the minimum support threshold is 50% or 40% and the object dataset is ?mushroom?, SIBA is slower than other algorithms, which is explained as follows.

1) When the dataset is small, such as ?mushroom?, and the minimum support threshold is high, the problem of mining top-k frequent item sets becomes easy and the advantage of SIBA on NP-hard problem, which is compared to Apriori and FP-growth, is gone.

2) The fixed iteration steps and the fixed population size of SIBA make some unnecessary computations when the problem becomes easy.

So SIBA has better speed performance when applied to the large dataset but not suited to the small one.

C. The experiment 3 In experiment 3, through mining the top-3 frequent 3 item  sets from the datasets in table 1, the accuracy and speed performances of SIBA, PSO, GA and the classic BA were compared. Every algorithm was used to mine every dataset 10 times. The error counts and average consuming time of every algorithm were recorded. PSO, GA and the classic BA took  ( )Sup  to be the fitness function and built the sub-dataset with 500 transactions by sampling before mining.

In this experiment, the population size of every algorithm was set to 20, and the iteration number were set to 1000. For PSO, the inertia weight was set to 1, and both the cognitive and social parameters were set to 0.5. For GA, both the crossover and mutation probabilities were set to 0.5. For both the classic BA and SIBA, minfreq  , minrate  and minL  were all set to 0, and maxfreq , maxrate  and maxL  were all set to 1. The results of experiment 4 are shown in table 4.

Table 4 indicates that despite PSO, GA and the classic BA have higher speeds than SIBA. However, their error rates are higher than SIBA. The analysis are given as follows.

1) Unlike any particle in PSO, gene in GA or bat in the classic BA, every bat in SIBA is a 2nd-normal cloud, and the computation of cloud generator, which generates the bat cloud drops, may consume more time.

2) Before SIBA, the classic BA itself has already possessed better accuracy performance for many non-liner problems than many other heuristic algorithms [22], which is a good foundation.

3) The large number of items results in the large searching space, which probably reduces the search ability of PSO, GA and the classic BA.

4) Through the bat cloud and the stability assessment method, SIBA not only improves the global optimization performance but also avoids the impact of changeable data and noisy data brought and increased by sampling.



V. CONCLUSION SIBA not only saves the time in the top-k frequent item  sets mining job, but also has a high accuracy. Firstly, the application of BA and the sampling in every iteration evidently pick up the speed of top-k frequent item sets mining in large database. Then, the 2nd-normal cloud and solution stability assessment improve the accuracy of BA.

