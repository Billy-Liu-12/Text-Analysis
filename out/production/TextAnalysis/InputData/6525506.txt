Performance and Reliability Effects of Multi-tier Bidding on MapReduce in Auction-based Clouds

Abstract?Hadoop has become a central big data processing framework in todays cloud environments. Ensuring the good performance and cost effectiveness of Hadoop is crucial for the numerous applications that rely on it. In this paper we analyze Hadoop?s performance in a multi-tier market-oriented cloud infrastructure known as Spot Instances. Amazon Spot Instances (SIs) are designed to deliver a cheap but transient alternative to fixed cost On-Demand (ODIs) instances. Recently, AWS introduced SIs in their managed Elastic MapReduce offering. This managed framework lets the users design a multi-tier Hadoop architecture using fine grained controls to define the instance types both in terms of capacity, i.e.

compute/storage/network, but also in terms of costs, i.e. ODI vs SI. The performance effects of such fine grained configurations are not yet well understood. First, we analyze a set of cluster configurations that can lead to important performance effects that can affect both the running time and the cost of such cloud Hadoop clusters. Second, we examine Hadoop?s fault tolerance mechanisms and show the inadequacy of these mechanisms for multi-tier bidding architectures. Third, we discuss directions for making the Hadoop framework more market-aware without losing its focus on extreme scalability.

Keywords-Failures, Hadoop, Auction-based Clouds, Perfor- mance of Systems, Fault Tolerance.



I. INTRODUCTION  Hadoop has become a crucial element of today?s cloud ecosystem [1]. Large companies need to deals with ex- tremely large data [2]?[4] for applications such as log analy- sis [5], [6], intelligent e-commerce and business intelligence applications [7], [8]. Scientists have also found Hadoop capabilities beneficial in terms of large scale text process- ing [9], genome assembly [10], [11] and social network graph analysis [12], [13]. The computer research community also contributed many studies and improvements: adapting Hadoop to opportunistic and heterogeneous environments [14], [15], improving the overall performance of jobs [16], running Hadoop as a back-end of higher level APIs [3], [17] as well as providing detailed analysis of Hadoop failure model [18].

On the other hand running large scale clusters dedicated to Hadoop can be a costly endeavor even using pay-as-you- go cloud resources such as AWS EC2. Spot instances are considered the cheapest purchasing model in the cloud pric-  ing structure in the EC2 infrastructure. These spot instances, provide market-based guarantees concerning the time of launch and termination of instances. Given this uncertainty of the availability of this type of instances AWS Elastic MapReduce(EMR) introduced a multi-tiered option where instances are launched in one of three groups: Master, Core nodes and Task nodes. This fine grained configuration option gives more power to the cluster administrator since it is possible to fine tune the EMR job flows to fit the budget and time requirement of various applications. The main issue that this configuration option introduces is a loss of data locality for tasks that are not part of the core group.

While core nodes are part of the HDFS cluster and also are TaskTracker nodes, task nodes only run TaskTrackers that do not participate in the HDFS cluster. This loss of data locality can go against the ?bringing computation to the data? philosophy that Hadoop has been built on and can affect the runtime and the cost of the Hadoop jobs.

In this paper, we focus on Hadoop?s behavior under a set of multi-tiered market-oriented configurations using the EMR framework. Despite the high usage of Hadoop in the cloud, little research work has been done on analyzing Hadoops performance under multi-tiered market-oriented configurations and we identify a lack of understanding concerning the efficiency of fine grained configurations in the context of Spot Instances on the EC2 platform.

Specifically, in this research work we analyze Hadoop?s behavior under different core nodes to task nodes ratios.

This ratio is important because the number of DataNodes is bounded by the number of core nodes and thus task nodes do not have access to local data as they do not run DataNodes and do not participate in the HDFS data storage. Quantifying the performance effect of the resulting lack of data locality is hard and in this work we analyze its effect to provide best practices for future EMR users.

Second, we analyze Hadoop?s fault tolerance mechanisms in this kind of multi-tier bidding architecture. This is im- portant because the main tradeoff of Spot Instances is cost vs. availability. Since the SIs are prone to being shutdown by the cloud provider, it is central to deal explicitly with failure for sustainable and cost effective usage of this type of  2013 IEEE Seventh International Symposium on Service-Oriented System Engineering  DOI 10.1109/SOSE.2013.13   2013 IEEE Seventh International Symposium on Service-Oriented System Engineering  DOI 10.1109/SOSE.2013.13     infrastructures. We perform a set of experiments to identify Hadoop?s fault tolerance mechanisms that are the most/least beneficial when dealing with virtual machine failures in a market-based environment.

Hadoop?s popularity and ever shrinking research and oper- ating budgets make this research direction directly applicable in cloud environments. The problem of dealing with multi- tier bidding and cluster configurations is complex and we aim to contribute to the understanding of the effects these configurations have on the runtime of Hadoop applications.

This paper is organized as follows. In section 2 we review related works on Spot instances, the Hadoop framework, its fault tolerant mechanisms and current research in multi-tier bidding in cloud infrastructures. In section 3 we describe the experiments carried out to understand the effects of multi- tier bidding on performance and fault tolerance. In section 4 we provide the results of our experimentation. In section 5 we discuss the implications of our findings and propose new directions for adapting Hadoop-like frameworks to market- based and auction-based cloud infrastructures. Finally, we conclude in section 6.



II. BACKGROUND A. Spot Instances Background  Spot Instances (SI) are the cheapest instance types in the cloud pricing structure of the AWS EC2 infrastructure. These SIs expose directly to the user the tradeoff between cost and reliability using a market-based scheduling mechanism. The time of launch and termination of SIs is a function of the user?s bid and the current market conditions.

If an SI is terminated by the cloud provider due to the rise in market prices, the user is not charged for any partial hour. On the other hand, if the SI is terminated by the user, the user is charged for the full hour. A third important characteristic is that the user pays the market price for an SI regardless of the initial bid on those instances. This aims to simulate a second-price sealed auction to encourage true value bids from customers. Amazon also provides a choice between persistent requests that are unbounded requests and as long as they are active, the cloud providers will run the SI every time the market price is below the initial bid. Finally, currently changing the bid on an instance is only allowed at the request creation time and cannot be changed during the lifetime of an instance.

Since the introduction of the SI cost model a set of research works have tried to address some of the challenges related to this opportunistic environments. Early efforts have worked on simulating the availability of single instances un- der different bid conditions [19], [20]. This work presented a decision model and simulator to optimize the runtime and the costs of using spot instances. Further work in [21] addressed high-availability for single business applications using a cost-reward model and SLA guarantees. The studies reported in [22], [23], provide resource allocation strategies  that try to deal with potential unavailability periods using monetary and runtime estimation techniques.

Even if the research efforts have addressed many of the challenges of spot instances, only a few such as [24] and [25] have addressed the applicability of spot instances to HPC environments. While developing strategies is important for single applications if they are to use spot instances, this knowledge is not fully applicable to HPC or data-intensive applications such as big data problems that requires a large number of interconnected components to solve a single problem.

The closest research work related to the current work is [26], which deals directly with the usage of SIs in MapRe- duce applications. In that research work, the authors studied the effect of adding SIs as ?accelerators? to a core group of Hadoop nodes. The findings were preliminary results and the authors pointed to the large variation of performance when using SIs in the case of failure-free runs (good performance) and in the case of out-of-bid failures (worse performance than non-accelerated Hadoop clusters). Further work in [14] developed a version of Hadoop to deal with opportunistic environments such as volunteer computing. Both these work pointed in the direction of multi-tier Hadoop clusters but did not examine the performance effect of multi-tier bidding on a Hadoop cluster.

We consider that this gap in data-intensive market- oriented computing research is important and should not be overlooked due to the potential cost and time savings of infrastructures such as SIs. In this work, we analyze the performance effect of multi-tier bidding on performance both in failure-free and out-of-bid failure scenarios.

B. Hadoop Background  We first briefly describe the Hadoop MapReduce frame- work. Also since the main trade off when using SIs is the possibility of out-of-bid failures, we describe the fault tolerance mechanisms that are used by Hadoop to mitigate node failures.

The Hadoop framework [27], [28] comprises two main types of tasks: Mappers and Reducers. Initially, Mappers read the input data from the HDFS(Hadoop distributed filesystem) in the form of splits. Each Mapper produces intermediary data composed of key-value pairs. These pairs are stored locally on the node running the Map function and are not written to the HDFS storage. This intermediary output is the input of the Reducer tasks. Each Reducer is assigned a separate key range and tries to copy the parts of the Map outputs which have records in that key-range. This phase is called the shuffle phase. Each Reducer then applies its Reduce function to each key-pair and stores its output to the HDFS.

To determine the progress of a MapReduce job during large scale processing each task has a progress score. This progress score is an estimation of completion percentage of     each task and is bounded between 0 and 1. For Reducer tasks this score is increased to 0.33 at the end of the shuffle phase which means that all the Map intermediary outputs have been copied to the Reducers. The Reducer score is increased to 0.66 after the key-pairs received have been sorted. While the Reducer applies the Reduce function to its key-pairs, the score is between 0.66 and 1. For Mappers, the progress score is determined by the fraction of input data read. The progress rate of a task then is the ratio of this progress score over the running time of that task so far.

The Hadoop distributed filesystem, HDFS, is composed of a centralized metadata NameNode and of set of distributed DataNodes. The NameNode handles the coordination of the reads and writes and the filesystem metadata. Each block written to HDFS is written in a pipeline fashion and replicated across a set of DataNodes. A configurable replication factor defines the number of DataNodes that will have this block of data. In the event of a DataNode failure, a Write or Read Time Out occurs, WTO/RTO.

Connection timeouts, CTO, can also occur if a DataNode is no accessible. Mappers can suffer from RTOs while Reducers can suffer from WTOs, and CTOs can affect both kinds of tasks.

Each compute node can also run a TaskTracker, TT, which is a daemon responsible for managing local tasks. By default, TTs are initially configured with a pre-set number of Mapper and Reducer slots that depends on the compute node capacities and by default all the TaskTrackers have the same number of slots. For large datasets the number of tasks that are to be scheduled can be far greater than the number of slots available and a job proceeds in multiple waves. The JobTracker, JT, is a centralized component that manages user?s jobs, communicates regularly with the TaskTrackers and coordinates the scheduling and location of the tasks of the currently submitted Hadoop jobs.

C. Failures in Hadoop  Due to the large number of components involved in large scale Hadoop cluster, individual failure components are considered the norm in Hadoop and are dealt with explicitly by the framework with minimal intervention from the application user. The main components that need to be monitored are the TaskTrackers, the Map and Reduce tasks and the DataNodes. We do not discuss the fault tolerance mechanisms of logically centralized components such as the HDFS NameNode and the JobTracker; readers can find in [29] distributed coordination mechanisms to deal with these components. We use the notation initially proposed in [18], Table I, to describe the fault tolerance mechanisms of Hadoop.

1) Task Tackers: Concerning TaskTrackers failures, The  JobTracker checks every 200s if any TaskTracker did not send heartbeats for the past 600s. If a TaskTracker is marked as dead, all the Map and Reduce tasks that were assigned to  Table I FAILURE TOLERANCE VARIABLES IN HADOOP  Symbol Description  Rj Number of Reduce tasks running for job j at time of check  Mj Number of Map tasks for a job, equal to input data splits  ARj Total number of shuffles attempted by Reducer R  KRj Total number of failed shuffles attempted by Reducer R  DRj Total number of Map outputs successfully copied by Reducer R  SRj Total number of Maps Reducer R failed to shuffle from  FRj (M) Total number of times Reduce task R failed to copy Map M?s output  Nj(M) Total number of notifications received by the JobTracker that Map M?s output is unavailable.

PRj Time from Reducer R?s start until it last made progress  TRj Time since Reducer R last made progress  Qj Maximum running time among completed Maps  Z(Ti) Progress rate of Task Ti  Tset Set of tasks running or completed in Job Jj  it are re-attempted on a different TaskTracker. In addition, any Map task that completed on that TaskTracker is also restarted if the corresponding job is still running and has at least 1 Reducer. This process is depicted in Figure 1.

2) Map Tasks: In addition to detecting dead TaskTrack-  ers, the JobTracker also recomputes lost Map outputs early.

This is can happen even before declaring that a correspond- ing TT is dead, if enough Reducers notify the JobTracker that they are not able to copy a Map output. The JobTracker restarts a Map task if the number of notifications received is bigger than half the number of Reducers running for the current job and at least 3 notifications are received for that Map task. This process is depicted in Figure 2.

3) Reduce Tasks: In the case of malfunctioning Reducers,  it is the responsibility of each parent TaskTracker to restart a Reducer. If a Reducer is repeatedly not able to copy Map outputs, the TaskTracker considers that Reducer faulty. To make this decision three conditions need to be met. First, half of the copy attempts of a Reducer need to fail. This process is depicted in Figure 3. Second, either the total number of Maps Reducer R failed to shuffle from needs to be bigger than 5 or equal to the difference between the total number of Map tasks and the number of successfully copied Map outputs by the Reduce task. Third, either the Reduce task has copied less than half of the total number of Map tasks or the time since the Reducer last made progress is bigger than half the maximum time between the time since the start of the Reduce task or the maximum running time of all the completed Maps. This checks if the progress of the Reducer is not enough or, when compared to other tasks, it     Figure 1. Detection and mitigation strategies for dead TaskTrackers JobTracker  Start Hadoop Job  Check TTi in job Ji  Is TTi dead?

Any TT left?

TTi had running tasks?

Restart tasks  Any completed  Map tasks?

Job has reducers?

Restart completed Map tasks  no  yes  yes no  yes  no  yes  no  yes  no  Figure 2. Detection and mitigation strategies for lost Map outputs JobTracker  Start Hadoop Job  Check Mi in job Jj  Nj(Mi) >  Nj(Mi) > 0.5Rj  Restart M ji  no  yes  no  yes  is has been stalled for the majority of its predicted running time.

4) Speculative execution: Due to the existence of tran-  sient slow tasks in large Hadoop clusters, called stragglers, Hadoop implements a preventive strategy by speculating under-performing tasks. The version that we are using for our tests is the stable Hadoop 0.20, which relies on the progress rate of tasks for making its scheduling decisions.

Figure 3. Detection and mitigation strategies for malfunctioning Reducers JobTracker  Start Hadoop Job  ?TTi : check local Ri?s of Jj  KRj ? 0.5ARj  SRj ? 5 ? SRj = Mj ?DRj  DRj < 0.5Mj ? TRj ? 0.5max(PRj , Qj)  Restart Ri  yes  no  no  yes  no  yes  This native approach uses individual and global tasks statis- tics to determine which task to select for speculation. A task is chosen if its progress rate is slower than the average progress rate of the other tasks in the same job by at least one standard deviation. We describe this process in Figure 4.

Figure 4. Speculation candidate selection procedure (native Hadoop) JobTracker  Start Hadoop Job  Current speculated tasks = 0 ? Job Ji running for more than 60s?

no  Check Task Tcur ? Tset  Z(Tcur) ? avg(Z(Ti)Ti?Tset) ? std(Z(Ti)Ti?Tset)  Add Tcur to candidates  Checked all running tasks Ti?

Select slowest Tslowest task  Select first TTavail with an available slot  Run Tslowest on TTavail  yes  yes  no  no  yes     D. Multi-tier Bidding Background  One of the most fundamental differences between the current large scale scientific clusters and auction-based cloud clusters is that the bidding strategy chosen by the user also define the failure rate of the cluster nodes. This is in contrast with the traditional clusters failure model which only considers hardware, software and human errors to be part of the failure spectrum. This traditional model assumes that failures can affect up to 10 percent of the cluster at one time. Traditionally, users/admins have assumed that failures in large scale cluster can affect only a single node or a subset of the processing nodes at one time.

Contrary to traditional clusters, in the case of spot in- stances it is central to recognize that the bidding strategy that the cluster user/admin employs affects the failure rate and the failure model of the cloud cluster. On one hand, the interaction between the market price fluctuation and the bidding price choice produces a matching expected failure frequency [24]. On the other hand the granularity of the bidding strategy, which is related to how the bid decision spreads the risk over the individual instances, affects the failure model itself [25].

To illustrate this idea we differentiate between uniform and multi-tier bidding using the spot instance market. In the case of uniform bidding the whole cluster behaves as a single ?macro? application with a lifetime based on a single bid. This leads to an ?all-or-nothing? behavior as shown in Figure 5; In the case where the market price is above the user?s bid all the nodes are shutdown. We call this effect bid-price coupling [25].

Out-of-Bid Failure (Market price moves above customer's bid  price)  Spot-based Virtual Cluster using uniform bidding  Figure 5. Example of an out-of-bid failure in a spot-based virtual cluster with uniform bidding leading to a total failure  On the other hand, multi-tier bidding is a logical direction for mitigating this price coupling. One illustration of this strategy is shown in Figure 6. In this Figure we show that nodes are grouped in bid classes, with decreasing bid prices from on-demand price to some minimum bid price.

This strategy does not completely remove the possibility of complete cluster failure but has the potential to provide more flexibility to meet budget and deadline constraints as well as a finer grain of control over which part of the cluster components can be hardened to deal with out-of-bid failures.

These observations apply directly to decoupled program-  Out-of-Bid Failure (Market price moves  above customer's lowest bid price)  Spot-based Virtual Cluster using multi-tier bidding  On-Demand Bid Price  High Bid Price  Medium Bid Price  Low Bid Price  Figure 6. Example of an out-of-bid failure in a spot-based virtual cluster with non-uniform bidding leading to a partial failure  ming models such as Hadoop/MapReduce. While this pro- gramming model considers failures as the norm, we argue that the fault tolerance mechanisms that Hadoop implements are only partially effective in market-oriented clusters such as SI clusters. The lack of information about the nature of the failures that affect the nodes, the conservative thresholds and sometimes conflicting strategies [18] make it hard for Hadoop to detect and differentiate between traditional node failures and market-based failures. Furthermore, Hadoop being a data-intensive programming model, has to keep large amounts of data distributed in the cluster using a distributed filesystem such as HDFS [30]. While HDFS assumes that the user is taking responsibility for determining a suitable replication factor when used in faulty environment, the default factor of 3 copies per block cannot be used directly on spot instances since all the replicas could be randomly distributed on low bid instances and be lost all at once if the market outbids their individual prices.

One current solution provided by the EMR framework on AWS, Elastic MapReduce, proposes a mitigating strategy by providing the user with the option to use a mixture of On- Demand and Spot Instance depending on the role allocated for each node. This includes allocating the centralized and sensitive components on OD instances, such as the NameN- ode, JobTracker and HDFS DataNodes. On the other hand, the TaskTrackers can be assigned to the OD instances as well as to ?accelerators? nodes that run on SIs and do not participate in the HDFS cluster.

One of the potential problems with the multi-tier archi- tecture of EMR when using SIs is the potential loss of data locality for certain tasks. Map tasks usually are scheduled on nodes that have the input splits needed by that Map task. This can improve dramatically the performance of Map tasks since they are able to read their input data directly from the local disks. However, if a Map task is scheduled on a Task node which does not have a DataNode process running locally, then all the reads for this Map task will be done from the network which can slowdown this Map task significantly. In the case of Maps that need to write their outputs, this architecture change should have minimal effect because usually Maps write to the local disk to store their intermediate results.

In the case of the Reduce tasks the shuffle phase almost always happens over the network since they need to collect all the key pairs to which they were assigned, from all the Map tasks that generated these key pairs. On the other hand the write phase is done to the HDFS. Since some Reduce tasks can also be scheduled on Task nodes then their writes will be over the network which can also slow these tasks considerably.

We are interested in quantifying the performance effect of this lack of data locality. Also we are interested in the performance effect of out-of-bid failures in this multi-tier context especially since Hadoop?s default fault tolerance mechanisms might not be optimized for this kind of en- vironments.



III. METHODOLOGY  We divided our evaluation into two sections, shown in Table II. First, we evaluated the effect of multi-tier bidding on the Elastic Map Reduce(EMR) cloud platform [31].

Second, we simulated the multi-tier EMR platform on the Tcloud private HPC cloud [32], a local virtualized testbed.

We evaluated the effect out-of-bid failures on performance and how Hadoop?s fault tolerance mechanisms react to such failures.

For the EMR experiments we used 11 EC2 nodes. One node was running the NameNode and the JobTracker, the rest were divided into core nodes or task nodes. Core nodes run both DataNodes and TaskTrackers while the Task nodes only run TaskTrackers. The master node is of type m1.xlarge which has 4 virtual cores with 2 EC2 Compute Units each, 15GB memory and 1.7 TB of storage. The core and task nodes are of type m1.small which has 1 virtual core with 1 EC2 Compute Unit, 1.7 GB memory and 160 GB of storage.

The network used is the default EC2 network which consist of 1 GigE with 500 to 800 Mbps bandwidths [33] and the all the instances were located in the same region US-East.

For the Tcloud experiments we used 10 virtual machines with 2 virtual cores, 4GB of memory and 20GB of storage.

The network was a dedicated 1 GigE network. The 10 virtual machines run on a 10 physical node cluster with 12 cores, 24GB memory and 2TB of storage for each node. We used the Hpcfy v2.2 virtual cluster management [34] to build the Hadoop clusters out of the virtual machines.

For all the benchmarks we used the Hadoop 0.20 version.

The EMR clusters had 2 Map and 1 Reduce slot per Task- tracker node. The Tcloud virtual clusters were configured with 2 Maps and 2 Reduce slots per TaskTracker.

For the EMR cluster we measured the failure-free multi- tier cluster performance. These experiments are aimed to determine the effect of the data locality loss introduced by the multi-tier setup. We varied the ratio of core nodes to total number of nodes, we use the following notation in the rest of this paper: C/T ratio, from 0.2 to 1 which is effectively modifying the ratio of on-demand vs. total  Table II ENVIRONMENTS USED IN EVALUATION  Environment Scale (VMs per cluster)  Experiments  EC2 production 11 (x4) Measuring Failure-Free Multi- tier cluster Performance  Local testbed 10 (x4) Measuring Failure-Injection effect on Multi-tier cluster Performance  number of instances (SIs + ODIs). We evaluated 4 ratios, 0.2, 0.5, 0.8 and 1. We used 4 benchmarks, DFSIO-Read, DFSIO-Write, Terasort and Wordcount.

For the Tcloud cluster we specifically try to reproduce the multi-tier clusters similar to the EMR when using Spot instances. To achieve this, for each run, we started the virtual machines and used the Hpcfy toolkit to contextualize them into one Hadoop cluster. Then we decommissioned a number of VMs from the HDFS cluster to match the ratios of core to task nodes. For this experiment we used the 0.8 and 0.5 ratios of core nodes to total nodes.

In this experiment we choose to study a set of relatively short Hadoop jobs, emphasizing response time, which have been shown to be popular in current public workloads traces [15], [35], [36]. The latest SQL-like frameworks that run on top of MapReduce such as Hbase [37] and Pig [38] show the need for tools where the user wants an answer quickly, for example when accessing log data for monitoring or for decision making in data-driven business situations. We used the Terasort benchmark to sort a 10GB random dataset. In a failure-free and all-core environment the job takes 210 seconds to complete.

Our goal is to see the efficiency of Hadoop fault tolerance mechanisms in the case of task node failures. We do not aim at analyzing Hadoop over many job types. We consider that Terasort is a suitable benchmark as it contains a set of desirable benchmark features [39]. Parallel sorting includes a set of features that make it a reliable benchmark since it is simply described and can be scaled easily. Additionally, for data-intensive architectures it tests the system?s ability to move data efficiently, which effectively tests the bisection bandwidth of the underlying system. Finally, it provides measurable levels of stability, determinism and load balanc- ing based on the input data and algorithm used which can provide reasonable baselines for other applications.

On the other hand, we argue that the insights of our paper are suitable for longer jobs and other types of jobs. Hadoop?s failure mechanisms, described above, still remain the same at larger scale since it uses a mixture of fixed thresholds and the proportion of IO/network failures.

We simulate the behavior of the spot market where all the task nodes that are out-bid will be shutdown by the cloud provider. We inject a fail-stop failure by killing all the Task nodes, i.e. effectively terminating each Task VM,     at a specified time after the job is started and before the end of the failure-free job runtime. We introduce the Task node failures at 30s intervals for each run. After the run we restart all the VMs and rebuild the whole virtual cluster from scratch.

In the following section we start by describing the results on the EMR platform. We describe the performance effect of the multi-tier bidding architecture. Then we describe the results of the fault injection experiments on the Tcloud platform.



IV. RESULTS  In this section we report the results of the experiments that we performed for the EMR and Tcloud platforms.

First, we discuss the results of the multi-tier bidding on the EMR performance using 4 benchmarks and 4 C/T ratios of core nodes to total nodes in a failure free environment.

Second, we discuss the results of the performance effect of failure injections in a multi-tier bidding architecture using the Tcloud platform.

A. Multi-tier bidding effect on the EMR failure free perfor- mance  Concerning the EMR DFSIO-READ benchmark in Figure 7, we notice that increasing the C/T ratio consistently improves the read performance but the improvement is not linear with the increase of C/T . For example setting the C/T to 0.2 produces a failure-free performance loss of at most 3.07x while the expected loss should be in the 5x range since we are using only 1/5th of the virtual drives available for the HDFS. Also for a C/T ratio of 0.8 we notice that we only experience at most a 1.11x performance loss compared an expected 1.25x since we are using only 4/5th of the available drives.

20 40 60 80 100  0.2  0.4  0.6  0.8   Data Size (Gb)  S pe  ed up  T al  l? co  re /T  m ul  ti? tie  r  0.2 0.5 0.8  Figure 7. Elastic MapReduce DFSIO-Read running times comparison with 4 different Core nodes to Total nodes ratios  Concerning the EMR DFSIO-WRITE benchmark in Fig- ure 8, we notice that increasing the C/T ratio does not always improve the write ratio. In the case of C/T = 0.8,  the write performance is actually improved by at least 1.32x instead of a performance loss of 1.25x. For the other C/T ratios we also notice that in the case of a 0.5 ratio we get a loss of 1.17x instead of 2x and for 0.2 we get a loss of 1.31x instead of 5x.

We also notice for both of these read and write bench- marks are sensitive to the datasize. By increasing the datasize we notice that the performance is slowly decreasing for all three ratios.

20 40 60 80 100  0.2  0.4  0.6  0.8   1.2  1.4  1.6  Data Size (Gb)  S pe  ed up  T al  l? co  re /T  m ul  ti? tie  r  0.2 0.5 0.8  Figure 8. Elastic MapReduce DFSIO-Write running times comparison with 4 different Core nodes to Total nodes ratios  Concerning the EMR Terasort benchmark in Figure 9, we notice that increasing the C/T ratio improves the sort performance. On the other hand we notice that the improve- ment increases with the increase in the data size. Also we notice that setting the C/T to 0.8 gives minimal performance loss compared to the expected 1.25x loss. In the case of C/T = 0.5 the performance loss is at most 1.15x while the expected value is 2x and decreases with larger datasizes. For C/T = 0.2, the performance loss is at most 1.5x compared the expected 5x performance loss.

20 40 60 80 100  0.2  0.4  0.6  0.8   Data Size (Gb)  S pe  ed up  T al  l? co  re /T  m ul  ti? tie  r  0.2 0.5 0.8  Figure 9. Elastic MapReduce Terasort running times comparison with 4 different Core nodes to Total nodes ratios  Concerning the EMR Wordcount benchmark in Figure     10, we notice that increasing the C/T ratio has a less pronounced performance effect for all three ratios. The most striking result is the 0.2 ratio that sees only a 1.047x perfor- mance loss compared to an expected 5x loss compared to cluster using all the available virtual drives. The other C/T ratios 0.5 and 0.8 see minimal losses and even comparable running times to the all core cluster running times.

20 40 60 80 100  0.2  0.4  0.6  0.8   Data Size (Gb)  S pe  ed up  T al  l? co  re /T  m ul  ti? tie  r  0.2 0.5 0.8  Figure 10. Elastic MapReduce Wordcount running times comparison with 4 different Core nodes to Total nodes ratios  In summary we noticed that the C/T ratio in multi- tier Hadoop clusters has less negative effect than expected.

We noticed that for all the workloads increasing the C/T ratio improved the performance except for the write bench- mark. However increasing the C/T ratio is characterized by rapidly diminishing returns and should be taken into account when choosing this ratio.

On the other hand, one of the main attractive features of spot instances is the cost savings that come with paying the cloud market price. In our case we noticed that on average the spot price for m1.small instances was 11x cheaper than the on-demand price. Effectively a smaller C/T ratio decreases the cost of failure free Hadoop jobs and the previous benchmarks showed that smaller C/T ratios have relatively small effect on performance and thus on running time.

The challenge comes in choosing the correct ratio for each application. Related work in [25] discusses possible directions for this problem. We consider this problem outside the scope of this paper.

B. Effect of failure injection on multi-tier MapReduce per- formance  We also evaluated the effect of failure injection on multi- tier MapReduce performance. This experiment differs from previous experiments related to failure analysis in MapRe- duce clusters [18]. In our experiments we are injecting failures in the form of total VM termination. Furthermore, contrary to single node failures simulations, we experiment with the effect of a whole Task node group termination for  each run. This characteristic is used to simulate the spot instances that can go down at once if the market price fluctuation provokes an out-of-bid failure in all the task nodes.

To evaluate the failure mechanisms of Hadoop 0.20 we only analyze the distributed components failures and do not deal with the centralized components, mainly the NameNode and the JobTracker. Also we restrict our study to failures affecting the task nodes that do not contribute to the HDFS cluster. Thus we are left with the 4 mechanisms described in section II-C to deal with the following components: TaskTrackers, Map tasks, Reduce tasks and Speculative tasks. We are interested the effect of multiple failures on these mechanisms to see which one gets triggered and how it affects the application runtime.

We run two set of experiments that differ based on the C/T ratio, 0.5 and 0.8. For each we obtain an average failure free runtime. Then we run the Terasort benchmark while varying the time of failure from the start to near the end time of the failure free job time. We run this experiment with and without speculation to isolate the benefits of each mechanism.

For the cluster with C/T ratio of 0.5 we notice that in Figure 11 the average failure free runtime is 250s. On the other hand we notice that no matter when the failure occurs all the runs get delayed by at least 600s on average.

This is the default timeout of the Hadoop TaskTracker death detection. Also we notice that there is minimal change between runs with speculation enabled and no speculation.

30 60 90 120 150 180 210     Time of out?of?bid failure relative to start of job (s)  Ti m  e (s  )  Average?No speculation Average?Speculation Average?Failure Free  Figure 11. Tcloud MapReduce Terasort running times comparison with a Core nodes to Total nodes ratio of 0.5  For the cluster with C/T ratio of 0.8, Figure 12, we notice that the average failure-free runtime is 280s. We see similar effects in the 0.8 cluster. The runs are delayed by at least 600s for all the failure times and enabling speculation does not bring any additional gains.

This consistent behavior is explained by two elements.

30 60 90 120 150 180 210         Time of out?of?bid failure relative to start of job (s)  Ti m  e (s  )  Average?No speculation Average?Speculation Average?Failure Free  Figure 12. Tcloud MapReduce Terasort running times comparison with a Core nodes to Total nodes ratio of 0.8  First, the long wait for the 600s timeout shows that the main mechanism that gets triggered in our case is the TaskTracker death detection and recovery, Figure 1. The detection of lost Map output, Figure 2, does not get triggered because there are not enough notifications from Reducers to restart the lost Map jobs, this is due to the fact that less than half of the Reducers sent notifications to the JobTracker. Also the detection of faulty Reducers, Figure 3, are the responsibility of the TaskTracker and therefore if the VM is terminated then the TaskTracker is also terminated, which means that there is no recovery of faulty Reducers in our case.

The second point is that the default speculative execution did not get to fully complete. This is because even if the JobTracker selected a task to be rerun on a different node, that node might be down because it is part of the Task node group. The JobTracker does not know that a node is not available to run speculative tasks until the TaskTracker times out. Another inefficiency of the speculative execution is that it caps the number of speculative tasks to 1. Since we have 20% and 50% of the nodes down in an out-of-bid failure then only one of the tasks will be rescheduled, leaving the other ones to wait for their respective TaskTracker to timeout before being retried.



V. DISCUSSION Fine grain multi-tier bidding The multi-tier bidding in the EMR platform constitutes  a starting point for fine grained market-based clusters. Cur- rently the platform allows only for 2 types of nodes, core and task nodes. We think that more classes can give more flexibility to users. This can accomplish a wider spread of risk due to wider bids and possibly wider instance types, without further loss of performance. Also as we stated previously the selection of risk-aware C/T ratios is still a new topic and comes at the intersection of economics, computer science and systems research.

Adaptive Thresholds Hadoop relies on static timeouts and thresholds which are  set at startup time and cannot be tuned at runtime. This does not allow for flexibility when faced with new informa- tion. The conservative nature of this approach is inherited from best practices and assumptions in production clusters.

However, in the case of novel market-based clusters more adaptability is needed. Especially in case there is additional knowledge such as the current market prices. Future work will try to include a feedback to trigger faster responses for detection and recovery using the market prices knowledge.

One can imagine a market-aware JobTracker that would be able to identify dead trackers faster using the bid price and current market price. Also another direction is to increase the information sharing between the bidding platform and the centralized components such as the JobTracker to determine the number of Task nodes that are susceptible to go down and to adapt the number of speculative tasks accordingly.



VI. CONCLUSION In this paper we analyzed Hadoop?s performance in a  multi-tier market-oriented architecture. We examined several aspects of the Hadoop framework and how it reacts to various design decisions. We analyzed the performance effect of the ratios of core to task nodes and the effect of failures in such multi-tier Hadoop clusters. We found that that the core to task nodes ratio in multi-tier Hadoop clusters may be used to decrease the cost of Hadoop jobs with mininal negative performance effects in failure-free situations. However, under out-of-bid failures, the current Hadoop fault tolerance methods are not fully aware of the market and thus can cause unnecessary performance losses due to various static timeouts and fixed thresholds that are market-oblivious. This trade-off between cost, performance and reliability may be reconciled by further studies into optimal bidding strategies in cloud markets and adequate market-aware fault tolerance for the Hadoop framework.

We also believe that our results can provide insights be- yond the Hadoop/MapReduce framework and can contribute to the development of more flexible and tunable large scale computing frameworks that can achieve the larger goals of sustainable extreme scale HPC cloud computing.



VII. ACKNOWLEDGMENTS This research is supported in part by the National Science  Foundation grant CNS 0958854 and educational resource grants from Amazon.com. Views and conclusions contained in this document are those of the authors and should not be interpreted as representing the opinions of the funding agencies or sponsoring companies mentioned.

