Rough Association Rule Mining in Text Documents for Acquiring Web User  Information Needs

Abstract   It is a big challenge to apply data mining techniques for effective Web information gathering because of duplications and ambiguities of data values (e.g., terms). To provide an effective solution to this challenge, this paper first explains the relationship between association rules and rough set based decision rules. It proves that a decision pattern is a kind of closed pattern.   It also presents a novel concept of rough association rules in order to improve the effectiveness of association rule mining. The premise of a rough association rule consists of a set of terms and a frequency distribution of terms. The distinct advantage of rough association rules is that they contain more specific information than normal association rules. It is also feasible to update rough association rules dynamically to produce effective results.

1. Introduction   Web information gathering (WIG) systems endeavour to obtain interesting and useful information from the Web to meet their user information needs.

One of the fundamental issues regarding the effectiveness of WIG is ?overload? [10].  The problem of information overload occurs when a large number of irrelevant Web information are considered to be what users want.

The difficulty for solving the fundamental issue is how to efficiently acquiring knowledge about user information needs. It is easier for users to answer which of Web pages or documents are relevant to a specified topic rather than describe what the specified topic they want. Therefore, the research issue is the  discovery of useful knowledge in user feedback - a training set of text documents.

Data mining has been used in Web text mining, which refers to the process of searching through unstructured data on the Web and deriving meaning from it [6] [8]. One of main purposes of text mining is association discovery [2]. The existing approaches are maximal patterns [7], sequential patterns [18] and closed patterns [19].

Typically, the existing data mining techniques can return numerous discovered patterns (knowledge) from a training set. However, it is a big challenge to use the discovered knowledge efficiently for making decisions due to the noise in the discovered patterns. The concept of closed patterns is forward one more step for dealing with the noise, but there are still many meaningless patterns retained [19] [18]. Another approach for improving the quality of association rule mining is to generate only those patterns that are interesting to users based on some constraints [21] [22].

Rough set based decision tables [16] provide an alternative representation of discovered knowledge.

Different from the association rule mining, decision tables do not attempt to represent all of interesting patterns. They use constraint-based decision rules that isolate premises and conclusions of rules initially. In terms of association mining, however, there is a puzzle for decision tables, that is, we do not understand what nature of patterns is represented in the decision tables.

In this research, we present the concept of decision patterns to interpret this puzzle.

The association discovery approaches only discuss relationship between terms in a broad-spectrum level.

They pay no attention to the duplications of terms, and the labeled information in the training set. The consequential result is that the effectiveness of the  on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006    systems is worse than the traditional information retrieval. Another objective of this paper is to improve the effectiveness of association discovery by presenting the concept of rough association rules, where the premise (precondition) of a rough association rule consists of both a set of terms and a frequency distribution of terms.

The remainder of the paper is structured as follows.

We begin by introducing the concept of association mining in Section 2. In section 3, we present the concept of decision patterns to interpret decision tables in terms of association mining. In Section 4, we present the concept of rough association rules. We also introduce a method for updating rough association rules.  Section 6 evaluates the proposed approach for information gathering. Section 7 discusses related work and the last section contains the conclusions.

2. Associations in Information Tables   Formally, the association discovery from text documents can be described as an information table (D, VD ), where D is a set of documents in which each document is a set of terms (notice: the duplicate terms are removed here); and VD = {t1, t2, ?, tn} is a set of selected terms for all documents in D. Usually D consists of a set of positive documents D+ and a set of negative documents D-.

Definition 1. A set of terms X is referred to as a termset if X ? VD. Let X be a termset, we use [X] to denote the covering set of X, which includes all positive documents d such that X ? d, i.e.,  [X] = {d | d? D+, X ? d}.

Given a termset X, its occurrence frequency is the  number of documents that contain the termset, that is |[X]|; and its support is |[X]|/|D+|. A termset X is called frequent pattern if its support ? min_sup, a minimum support.

Table 1. An information table  Documents Terms Positive d1 t1  t2   yes d2 t3  t4 t6  yes d3 t3  t4 t5  t6    yes d4 t3  t4 t5  t6  yes d5 t1  t2 t6  t7  yes d6 t1  t2 t6  t7 yes d7 t1  t2   no d8 t3  t4   no   The confidence of a frequent pattern is the fraction  of the documents including the pattern that are  positive. Given a frequent pattern X, its confidence is defined as |[X]|/N, where N = {d | d? D, X ? d}. The confidence shows the percentage of the pattern?s occurrence frequency in the positive documents. A frequent pattern is called an interesting pattern if its confidence is great than |D+|/|D|.

Table 1 lists a part of an information table, where VD = {t1, t2, ?, t7}, D = D+ ? D-, D+= {d1, d2 ,?, d6} and D- = {d7, d8}. Let min_sup = 50%, we can obtain the following 10 interesting patterns:   Interesting Pattern Covering Set <t3 t4 t6> {d2, d3, d4} <t3 t4> {d2, d3, d4} <t3 t6> {d2, d3, d4} <t4 t6> {d2, d3, d4} <t3> {d2, d3, d4} <t4> {d2, d3, d4} <t1 t2> {d1, d5, d6} <t1> {d1, d5, d6} <t2> {d1, d5, d6} <t6> {d2, d3, d4, d5, d6}   Not all interesting patterns are meaningful. For  example, pattern <t3 t4> is a noise pattern since it always occurs with larger pattern <t3 t4 t6>.

Definition 2. Given a set of positive documents Y, we define its termset, which satisfies  termset(Y) = {t | t? VD, ?d ?Y => t?d}.

Given an interesting pattern X, we know its covering set [X] which is a subset of positive documents. We also define its closure C(X) = termset([X]). From the above definitions, we have the following theorem about the closure operation.

Theorem 1.  Let X and Y be frequent patterns. We have  (1) C(X) ? X for all patterns X; (2) X ? Y  => C(X) ? C(Y).

Proof:  (1)  For any t ? X, t ? d for all d ? [X]   since X ? d. Also, from Definition 2 we have t?termset([X]), that is X ? C(X).

(2) For any t ? C(X), that is t?d for all d ? [X]. For any d? ? [Y] we know Y ? d? and d? is a positive document based on Definition 1. From X ? Y, we have X ? d?, and hence d? ? [X]. Therefore, t?d? and t ? C(Y).

Definition 3. Let X be a pattern. X is closed if and only if X = C(X).

on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006     For example, there are only three closed patterns in  Table 1: <t3 t4 t6>, <t1 t2> and <t6>.

We can assume that each closed pattern is actually  an association rule, e.g., <t1 t2> means ?<t1 t2> ? (POS = yes)?. This assumption satisfies the definition about traditional association rules in data mining.

3. Decision Patterns   The training set D can also be characterized by a decision table. Table 2 shows a decision table which describes the binary representations of 1000 documents, where Ng is the number of documents that are in the same granule; terms t1, t2, ?, t7 are called condition attributes and Positive is called decision attribute. Compare to Table 1, a granule in Table 2 not only describes a feature of a document, it also shows the number of documents that have the same feature.

Table 2. A binary decision table  Granule t1 t2 t3 t4 t5 t6 t7 Positive Ng g1 1 1 0 0 0 0 0 yes 80 g2 0 0 1 1 0 1 0 yes 140 g3 0 0 1 1 1 1 0 yes 490 g4 1 1 0 0 0 1 1 yes 220 g5 1 1 0 0 0 0 0 no 20 g6 0 0 1 1 0 0 0 no 50   Decision tables provide an alternative way to  represent discovered knowledge in database. For example, each granule in Table 2 can be mapped into a decision rule: either a positive decision rule (the conclusion is ?yes?) or a negative decision rule (the conclusion is ?no?).

Formally, each granule g determines a sequence t1(g), ?,  t7(g), Positive(g). The sequence can determine a decision rule:  t1(g) ^ ?^  t7(g) ? Positive(g) Its strength is defined as Ng/|D|, and its certainty factor is defined as Ng/K, where K = ? =??? )()(, gtgtVtg gii iND .

For example, g1 in Table 2 can be read as the following positive rule:  t1 ^ t2 ? yes Its strength is 8% and certainty factor is 0.8.

Different with the association rule mining, decision tables do not tend to represent all of interesting patterns, instead of; they only keep some sorts of larger patterns. In terms of association mining, however, the puzzle for decision tables is that we do not understand what kinds of interesting patterns used in the decision tables. In the following, we present the concept of decision patterns to interpret this puzzle.

Let X be an interesting pattern. We call it a decision pattern if ?d ? D+ such that X = d.

Theorem 2. Decision patterns are closed patterns.

Proof: Let X be a decision pattern. From the definition of the decision patterns, we know there is a positive document d0 such that X = d0, that is d0 ?[X].

Given a term t ? termset([X]), according to Definition 2 we have  t ? d for all d ? [X], that is,  t ? d0 = X.  Therefore, C(X) = termset([X]) ? X. We also have X ? C(X) from theorem 1, and hence we have    X = C(X).

4. Rough Association Rules   One important factor is missed in both closed patterns and decision patterns: the duplications of terms in a document. This factor is very import in terms of information retrieval.  To consider this factor, we start to initialize this problem by using decision tables as multidimensional databases.

Table 3. A decision table  Granule t1 t2 t3 t4 t5 t6 t7 Positive Ng g1 2 1 0 0 0 0 0 yes 80 g2 0 0 2 1 0 1 0 yes 140 g3 0 0 3 1 1 1 0 yes 40 g4 0 0 1 1 1 1 0 yes 450 g5 1 1 0 0 0 1 1 yes 20 g6 2 1 0 0 0 1 1 yes 200 g7 2 2 0 0 0 0 0 no 20 g8 0 0 1 1 0 0 0 no 50   Table 3 demonstrates the corresponding decision  table (G, AC, AD) if we consider the duplications of terms in Table 2, where the numbers are frequencies of terms in the corresponding documents, the set of granules G = {g1, g2, g3, g4, g5, g6, g7, g8}; the set of condition attributes AC = {t1, t2, t3, t4, t5, t6, t7}, and the set of decision attributes AD = {Positive}.

Each granule is also mapped into a decision rule, e.g., g1 in Table 3 can be read as the following positive rule: (t1, 2) ^ (t2, 1) ? Positive = yes.

Let termset(g) = {t1, ?, tk}, formally every granule g in Table 3 determines a sequence:  (t1, f(t1, g)), ?,  (tk, f(tk, g)), Positive(g).

The sequence can determine a decision rule:  (t1, f(t1, g)) ^ ? ^ (tk, f(tk, g)) ? Positive(g) or in short g(AC )? g(AD ).

Normally, we would obtain more such decision rules than using the binary decision table, and there exists ambiguity when we use them for making  on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006    decisions. For example, give an instance of a piece of information that contains only four terms t3, t4, t5 and t6; but we can find two rules? premises (g3 and g4 in Table 3) match this instance.

To remove such ambiguities, in this section we present the concept of rough association rules in order to compose some decision rules into a single granule if they have the same termset. We also use a weight distribution for the granule to specify the possible semantic meaning in it.

4.1 Definitions   For every attribute t?AC, its domain is denoted as Vt; especially in the above example, Vt is the set of all natural numbers. Also AC determines a binary relation I(AC) on G such that (gi, gj) ? I(AC) if and only if termset(gi) = termset(gj).

It is easy to prove that I(AC) is an equivalence relation, and the family of all equivalence classes of I(AC), that is a partition determined by AC, is denoted by G/AC. The classes in G/AC are referred to AC- granules (or called the set of condition granules). The class which contains gi is called AC-granule induced by gi, and is denoted by AC(gi). We also can obtain AD- granules G/AD (or called the set of decision granules) in parallel.

For example, using Table 3, we can get the set of condition granules, G/AC = {{g1, g7}, {g2}, {g3, g4}, {g5, g6}, {g8}}, and the set of decision granules, G/AD = {Positive = yes, Positive = no} = {{g1, g2, g3, g4, g5, g6}, {g7, g8}}, respectively. In the following we let G/AC = {cg1, cg2, cg3, cg4, cg5} and G/AD = {dg1, dg2}.

We also need to consider the weight distribution of terms for the condition granules in order to consider the factor of the frequencies of terms in documents.

Let cgi be {gi1, gi2, ?, gim}, we can obtain a frequency distribution for terms tj in these documents using the following equation:  ?? =  CAt i  ij j cgt  cgt tf  )( )(  )(          (4.1)  where we use the composition operation (see [18]) to assign a value to condition granules? attributes, which satisfies:  t(cgi) = f(t, gi1) + f(t, gi2) + ... + f(t, gim) for all t?AC.

Table 4 illustrates AC-granules and AD-granules we obtain from Table 3 according to the above definitions, where each condition granule consists of both a termset and a frequency distribution. For example, cg1 = < {t1, t2}, (4/7, 3/7, 0, 0, 0, 0, 0)> or in short  cg1 = {(t1, 4/7), (t2, 3/7)}.

Table 4.  Granules  Condition granule  t1 t2 t3 t4 t5 t6 t7  cg1 4/7 3/7 cg2   1/2 1/4  1/4 cg3   2/5 1/5 1/5 1/5 cg4 1/3 2/9    2/9 2/9 cg5   1/2 1/2  (a) AC-granules   Decision granule Positive dg1 yes dg2 no (b) AD-granules   Using the associations between condition granules  and decision granules, we can rewrite the eight decision rules in Table 4 as follows:  cd1? {(dg1, 80/100), (dg2, 20/100)} cd2 ? {(dg1, 140/140)} cd3 ? {(dg1, 490/490)} cd4 ? {(dg1, 220/220)} cd5 ? {(dg2, 50/50)}  Formally the association can be represented as the following mapping:  ]1,0[)/(2/: ??? CACA GG  where )}(,),,{()( |)(|,|)(|,1,1, ii cgicgiiii wdgwdgcg ??=? , ...  , a set  of AD-granule float pairs, the set of conclusions of premise cgi (i = 1, ?, |G/AC|), which satisfies:  || || ,  , i  jii ji cg  dgcg w  ? =  and  )(),(  =? ?? wicgwdg  for all cgi? G/AC We can also define a support function for the  condition granules based on their frequencies.

? ? ? ? ?  ?= C  i  Acg cgg g  cgg g  i N  N cg  /  )( G  sup  for all cgi? G/AC. It is obvious that sup is a probability function on G/AC. Therefore, the pair (?, sup) is an association set as defined in [13].

We call ?    jii dgcg ,? ? a rough association rule, its strength is sup(cgi) ? wi,j and its certainty factor is wi,j, where 1? j ? |?(cgi)|.  ?    jii dgcg ,? ? can be either a positive decision rule (dgi,j is ?Positive=yes?) or a negative decision rule (dgi,j is ?Positive=no?).

According to the above discussion, we use an alternative way to represent granules into a 2-tier structure. Figure 1 illustrates a 2-tiers structure for the representation of rough associations between condition granules and decision granules. The 1st tier, the left  on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006    hand, is AC-granules and the 2nd tier, the right hand, is AD-granules. The relationship between the two tiers is described as an association set (?, sup). The association set also can determine a probability distribution on the set of decision granules (see [18]):  ? ??? ?= )(),(,/ )()( iCi cgwdgAcg i wcgdg  G supPr .

sup G/AC  ?(cgi)  0.10 cg1 ? (dg1, 0.8) (dg2, 0.2) 0.14 cg2 ? (dg1, 1) 0.49 cg3 ? (dg1, 1) 0.22 cg4 ? (dg1, 1) 0.05 cg5 ? (dg2, 1) Figure 1. A 2-tier structure for rough associations between condition granules and decision granules   Let n be the number of granules in the decision  table, it is obvious |G/AC| ? n. In addition, to obtain all decision rules in a decision table, the traditional method needs to search in the table to determine the conclusions for each condition.  The 2-tier structure does not need to search any more. Therefore, it is better of using the 2-tier structure than using a decision table in time complexity.

4.2 Updating Rough Association Rules   Let Rule+ be the set of positive rough association rules. We use the following weight function to determine the relevance of documents:  weight(term) =? ??? + ?cgftermRuledgcg fcg),(, )( sup Given a testing document, d, we use the following  equation for using rough association rules to determine its relevance:  ? ??  = dtermVterm D  termweightdrel ,  )()(       (4.2)  The consequential result of using Equation (4.2) is that many irrelevant documents may be marked in relevance. To avoid making many mistakes, we now consider how to update the positive rough association rules based on some interesting negative rough association rules.

Give a negative rough association rule ?cg ? (dg2, x)?, where termset(cg) =  {t1, t2 , ?, tm}. We call it an interesting negative rule if  rel(termset(cg)) ? min{rel(d) | d?D+}.

We use the following procedure to update some  positive rough association rules for all interesting negative rules cg in our experiments:  for (i = 1 to |G/AC |)  if (termset(cgi) ? termset(cg)) subtract half sup from cgi; else if  (termset(cgi) ? termset(cg) ??) reshuffe cgi?s term frequency distribution by shifting half weights from all terms? termset(cgi) ? termset(cg) to cgi?s rest terms;  For example, ?cd5 ? (dc2, 1)? is an interesting negative rule (see in Figure 1 and Table 4) and it does not include any condition granules but we have that  termset(cd2) ? termset(cd5) = termset(cd3) ? termset(cd5) =  {t3, t4 }??.

Therefore reshuffle operation can be used to update the frequency distributions of cd2 and cd3. We first take half weight from both t3 and t4. The total weights we can take from cg2 and cg3 are (1/4 + 1/8) = 3/8 and (1/5 + 1/10) = 3/10, respectively. We also distribute the total weight 3/8 to t6 for cg2 and 3/10 to both t5 and t6 for cg3.

Table 5.  Reshuffle operation   Table 5 illustrates the result of reshuffling term  frequency distributions for condition granules cd2 and cd3, where cd5 is the negative rule. To compare with the weights in Table 4, we can find that the weights of t3 and t4 in granules cd2 and cd3 are weakened, respectively because t3 and t4 are terms of cd5.

5. Evaluation  We use Reuters Corpus Volume 1, also known as RCV1, to evaluate the proposed method. We also use the first 20 topics that TREC (Text REtrieval Conference, see http://trec.nist.gov/) developed for filtering track in 2002.

We compare rough association rule mining model with two baseline models: closed pattern based association rule model (see Section 2 for details), and binary decision rule model (see Section 3).  A common basic text processing is used for all models, which includes case folding, stemming, stop words removal and 150 term selection that uses tf*idf (term frequency times inverse document frequency) technique.

In the training phase, the closed pattern based model finds all closed patterns (where the min_sup is  Condition granule  t1 t2 t3 t4  t5 t6 t7  cg1 4/7 3/7 cg2   1/4 1/8   5/8 cg3   1/5 1/10  7/20 7/20 cg4 1/3 2/9     2/9 2/9 cg5   1/2 1/2  on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006    the double of the average frequency of terms, and the size of largest patterns is 5) and calculates their support and confidence values. In the testing phase, the relevance of document d is the sum of the multiplications of support and confidence of all closed patterns that occur in d.

The binary decision rule model, first obtains a set of granules, G, in the training phase. It also uses the following equation to evaluate a weight for each term:  ? ??  = gtermGg gtermset  gstrengthtermweight , |)(|  )()(  Given a testing document, d, we also use Equation (4.2) to determine its relevance in binary decision rule model.

Table 6 is the experimental results. We use two measures in the table: top 25 precision and breakeven points, where a breakeven point is a point in the precision and recall curve with the same x coordinate and y coordinate.

Table 6. Experimental results.

Rough  association rules  Binary decision rules  Closed association rules  Avg. of breakeven points  0.51 0.48 0.49  Avg. of top25 precision  55.00% 50.60% 49.20%   Figure 2 shows the differences of rough  association mining, binary decision rule mining and closed pattern based association rule mining in top 25 precision for the 20 topics. The positive values (the bars above the horizontal axis) mean the rough association mining performed better than others. The negative values (the bars below the horizontal axis) mean others performed better than rough association mining.

-60.00%  -40.00%  -20.00%  0.00%  20.00%  40.00%  60.00%  80.00%  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  Topic  Pr ec is io n  Rough-Binary Rough-Closed   Figure 2. Difference between models    It is no less impressed by the performance of the rough association rule mining since both top 25 precision and breakeven points gain a significant increase. As a result of the experiment we believe that the proposed method is significant since they can improve the effectiveness of the association discovery for Web text mining.

6. Related Work  The key issue regarding the effectiveness of WIG is automatic acquiring of knowledge from text documents for describing user profiles [10] [13]. It is also a fundamental issue in Web personalization [3].

Traditional information retrieval (IR) techniques can be used to provide simple solutions for this problem. We can classify the methods into two categories: single-vector models and multi-vector models. The former models produce one term-weight vector to represent the relevant information for the topic [4] [17]. The latter models produce more than one vector [15] [9]. The main drawback of IR-based models is that it is hard to interpret the meaning of vectors using user acceptable concepts.

Text mining tries to derive meaning from documents. Association mining has been used in Web text mining for such purpose for association discovery, trends discovery, event discovery, and text classification [6] [8] [19].

The association between terms and categories (e.g., a term or a set of terms) can be described as association rules. The trends discovery means the discovery of phrases, a sort of sequence patterns. The event discovery is the identification of stories in continuous news streams [2]. Usually clustering based mining techniques can be used for such a purpose. It was also necessary to combine association rule mining with the existing taxonomies in order to determine useful patterns [12] [5].

To compare with IR-based models, data mining- based Web text mining models do not use term independent assumption [1] [14].  Also, Web mining models try to discover some unexpected useful data [2]. The disadvantage of association rule mining is that the discovered knowledge is very general what makes the performance of text mining systems ineffectively [20].

Decision rule mining [16] [12] [23] can be a possible solution to specify association rules.

However, there exists ambiguities whist we use the decision rules for determining other relevance information for specified topics. Rough association  on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06) 0-7695-2747-7/06 $20.00  ? 2006    rule mining can be used to overcome these disadvantages.

7. Conclusions  This paper, discusses the application of data mining techniques within Web documents to discover what users want. It introduces the concept of decision patterns in order to interpret decision rules in terms of association mining. It has proved that any decision pattern is a closed pattern. It also presents a new concept of rough association rules to improve of the quality of text mining. To compare with the traditional association mining, the rough association rules include more specific information and can be updated dynamically to produce more effective results.

The distinct advantage of rough association rules is that they can take away some uncertainties from discovered knowledge through updating supports and weight distributions of association rules. It also demonstrates that the proposed approach gains a better performance on both precision and recall, and it is a considerable alternative of association rule mining.

This research is significant since it takes one more step further to the development of data mining techniques for Web mining.

