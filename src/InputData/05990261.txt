An Improvement Apriori Arithmetic based on Rough set  Theory

Abstract. Rough set theory and the association rules algorithm are mining methods which are used to find implicit rules model from large amounts of data. As the association rules mining algorithm, Apriori algorithm is gotten a lot of application used for its easy use. However, in practice, it often encountered some problem as low mining efficiency, too many invalid rules acquired and the rules of pattern mining disorder. In this paper, a algorithm called R_Apriori is designed for the problems with decision-making domain. First the conditions of the cores are mined with the rough attribute reduction algorithm, then 1-frequent item sets and the corresponding sample set is found with use mining cores set by the Apriori algorithm. And then multi-stage frequent item sets and the corresponding support and confidence can be obtained by the sample collection intersection operator. According to degree of confidence and support the corresponding strength of the rule is decided. R_Apriori algorithm solves the problems of Apriori algorithm to improve the efficiency of the algorithm and is in promotion on a certain significance.

Keywords: Apriori Arithmetic, Rough Set Theory, Reduction, Frequent Sets, Algorithm Efficiency.

1 PREAMBLE  Data mining is a method for search the knowledge and implicit rules in large amounts of data.. It is both a data processing and a knowledge acquisition technique. The technology evolved from artificial intelligence, so the results of artificial intelligence can be transplanted to the data mining system, such as the traditional statistics, clustering, decision tree, set theory, association rules, rough set theory, artificial neural network, genetic algorithms and evolutionary computing, etc. [1]  Rough Set Theory can find structured relationship of inaccurate data or noisy data, which is based on a equivalence class establishment with the given training data.

As a method to find any modes, association rules is common concerned. The Apriori algorithm based association rule is continue to improve by scholars as it?s more simple. [2-4]  However, Apriori algorithm often faces the difficulties in process of the association rules discovery as follows:  (1) While using of data mining association rules, the mining item sets generally are not limited to so to dig out a wider range of rules. But it will also lead to too many invalid rule excavated and to bury really useful mode under the valid rules;  (2) Time complexity and space complexity of Apriori algorithm is increased as the exponential level with the  increase of the item set, and acquirement of x-frequent item sets is needed to scan all data. So how to improve the efficiency of their time and space is one of the focus of the study by Scholars;  (3) Apriori algorithm can dig out the effectively rule mode. But in some practical problems, it needs to dig out either the rules model or the order of the rules to find its associated intensity level.

In many practical cases, especially in decision-making problems, decision set is clear. Under the premise of the clear decision-making domain, it is necessary to improve Apriori algorithm to address the efficiency of mining algorithms and validity and strength sorted of model rules.

2?THEORY FOUNDATION  Improved algorithm is mainly based on rough set theory and Apriori association rules algorithm, the basic knowledge as follows:   2.1 Rough Set Theory  In 1982, Poland mathematician Pawlak proposed rough set theory. Any information system (or information sheet) in rough set theory called I can be describe with an ordered 4-tuple <U, A, V, f>, where: U = {x 1 ,x2?...?xn} are all limited collection of samples, A=C?D is a set of all finite number of attributes, C is condition attributes set that is the characteristics of the object, D is the decision attribute set that is the type of study object and C ? D = ?. Suppose a is either an attribute, xi is either an object, then f (xi ,a) is value of xi in a attribute, while V is the value range of attribute A.

[4-5]  Pawlak attribute importance reduction algorithm confirms mainly the importance of the attribute based on the information system changes in size classification ability while it removed. Attribute importance is defined as follows:  A given set of information systems CBfVCUIS ??= ),,,,(  And BCa ??? , Define  )( ))(/(})){(/();,(  Ucard BindUcardaBUindUcardCBasig ?=  as the attribute importance of a  to the attribute set B ? Where U is the domain of information systems, C is the  attribute set, V  is the value for the attribute, f  is the information function, ind is undifferentiated relationship attribute set of the information system between, card is the base for the attribute set. General attribute reduction is processed according to the importance of attribute. [5-6]   2.2 Association Rules and Apriori Arithmetic       First introduce the data mining association rules, association rules discovery data mainly to an interesting correlation between item sets like A ==> B such a law can be extended for the A1 ^ A2 ^ ... ^ An ==> B1 ^ B2 ^ ... ^ Bn class rule. Can be understood as in the database to find A1, A2, ..., An also exist, B1, B2, ..., Bn the same time, the establishment of rules.

Concrete realization depends on the degree of interest rules, including those of support and confidence of two terms.

Degree of support (support) S refers to the transaction in the rules the frequency. X ==> Y degree of support for S to  S (X ==> Y) = | T (XUY) | / | T | Where, | T (XUY) | refers to the number of transactions of the data set contains XUY, | T | the total number of that matters.

Degree of support is too low, said the rules are not general.

Degree of confidence (confidence) C, said association rule X ==> Y intensity, defined as:  C (X ==> Y) = | T (XUY) | / | T (X) | Where, | T (XUY) | refers to the data set contains transactions number of XUY, | T (X) |, said data set that contains transactions number of X. The lower the confidence level that the credibility of the rules of difference.

Association rule X ==> Y means that the confidence level of X given Y, that is the conditional probability, that is  C (X ==> Y) = P (Y / X) Data mining association rules is found to have a  user-specified minimum support degree Smin and minimum confidence degree Cmin of association rules. Namely:  X ==> Y is equivalent to (S (==> Y)> Smin) ^ (C (X ==> Y)> Cmin) [7]  3 IMPROVEMENT R_APRIORI ALGORITHM DESCRIPTION  To the the problem with clear decision-making field by mining association rules, improved R_Apriori algorithm can be form by integrating rough set theory with the Apriori algorithm to solve the problem raised in the preamble as follows:  About the problem of the efficiency of Apriori algorithm and the validity of the mining rules on account of the large amount of attributes set, we can first get the nuclear of attribute set by rough set attribute reduction, then the association rule mining to the nuclear data. In certain extent, it can improve the efficiency and effectiveness of mining;  For inefficient Apriori algorithm raised from the needs of scanning all attribute sets to obtain each frequent attribute set, we can solve as follow:  (1) 1-frequency set can be obtained by scanning the set of attributes, which assumed to be L1={X11, X12, ... ,X1M}.

Denote the set of samples of X1P as S(X1P)={t1P , t2P ,?, tmP}, 1<P<M. Obviously the number of the element of S(X1P) is transactions number, namely | S(X1P)| ,and | S(X1P)| /| T |> Smin (Smin is minimum support degree);  (2) 2-frequent set and above 2-frequency set can be obtained just by set operations of 1-frequent set. According to the character of frequent sets: the attribute which is not attribute of low frequency set must not be attribute of high frequency. So the attributes of 2-frequent sets must include the frequency set L1. For any two item sets X1i and X1j in L1 (where 1<i,j<M), S(X1i)? S(X1j) includes all transaction in X1i and X1j. Then its number of elements namely | S(X1i)?  S(X1j) | is the number of its transactions. So if | S(X1i)? S(X1j) | / | T |> Smin, then {X1i, X1j } ?L2, otherwise {X1i, X1j } ?  L2; then intersection operation to all 1-frequency set and to get their support, you can get L2; if {X1i, X1j } ?L2, you can also calculate | S(X1i)? S(X1j) | / | S(X1i)| and | S(X1i)? S(X1j) | / | S(X1j)  | which is the confidence of X1j ==> X1i and X1i ==> X1j. And comparison with the minimum confidence degree Cmin, association rules is determined.

(3) Thus intersection operation to the frequent item sets LK-1 and L1, and then determines the number of elements in the intersection set, all frequent sets LK can be obtained. For the problem of the sort of strength of the rule model, it could be sorted by support degree, confidence degree and order of mining:  Given two rules ri and rj, ri> rj (i.e., ri precedes rj or ri has higher precedence over rj) if one of the following holds good:  (1) the confidence degree of ri is greater than that of rj (2) their confidences degree are the same but support of ri  is greater than that of rj (3) both the confidences and supports of ri and rj are the  same, but ri is generated before rj  The algorithm described as follows:  U = RS(U,C,D,f) //attribute reduction according to decision set L(1) = Apriori(U,1)= {X11, X12, ... ,X1M} //Calculate 1-frequency set S(X1P)={t1P , t2P ,?, tmP}?1<P<M //record of 1-frequency set transaction sets k=2 Do while L(k)=null{ L(k)=null For i=1 to | L(k-1)| For P=1 to  | L(1)| If X1P? E(L(k-1)) then // E(L(k-1)) is( k-1)- frequency set If | S(X1P)? S(E(L(k-1)))| /| T |> Smin then L(k) = L(k) U{ S(X1P)? S(E(L(k-1)))} //Is the elements of k-frequency set If | S(X1P)? S(E(L(k-1)))| /| S(X1P)| > Cmin then //To determine whether S(E(L(k-1))) ==> S(X1P) is the  rule R = R U{ S(E(L(k-1))) ==> S(X1P)} Sort(R, S(E(L(k-1))) ==> S(X1P))// Sort  if | S(X1P)? S(E(L(k-1)))| /| S(E(L(k-1)))| > Cmin then  //To determine whether S(X1P) ==> S(E(L(k-1))) is the rule  R = R U{ S(X1P) ==> S(E(L(k-1)))} Sort(R, S(X1P) ==> S(E(L(k-1))))//sorted  Next Next } Output rules R  4 EXAMPLE  In my research of the dialectical treatment of traditional Chinese medicine by data mining, to determine the TCM symptom diagnostic criteria and Efficacy Evaluation Index      System, above R_Apriori algorithm is used more efficiency and more accurate results obtained. On the clinical data of Elderly pneumonia, three medicine diagnosis symptoms indicators including body temperature(denoted by TW), respiration(denoted by HX) and headache(denoted by TT) is taken to study the relationship with TCM syndrome(denoted by ZYBZ). The example algorithm is as follows:  Basic data(U is denoted patient) is like Table 1.

Table 1 Basic data U TW HX TT ZYBZ 1 L N Y Wind-heat 2 M N Y Wind-cold 3 H N Y Wind-cold 4 L N N Wind-heat 5 M F N Wind-heat 6 H N N Wind-cold 7 M F N Wind-cold 8 H N N Wind-heat  The first step: data reduction using rough set according to decision-making set ZYBZ, the nuclear of symptom index is obtained including body temperature and headache. The second step: define Smin=2 and Cmin=0.5, then 1- frequency set is:  L1={ TW=?L??TW=?M??TW=?H??TT=?Y??TT=?N? ?ZYBZ=? Wind-heat??ZYBZ=?Wind-cold?}  S(TW=? L?)={U1 ,U4 } S(TW=?M?)={U2 ,U5 , U7} S(TW=?H?)={U3 ,U6 , U8} S(TT=?Y?)={U1 ,U2 , U3} S(TT=?N?)={U4 ,U5 , U6 , U7, U8} S(ZYBZ=? Wind-heat?)={U1 ,U4 , U5, U8} S(ZYBZ=?Wind-cold??)={U2 ,U3 , U6 ,U7} 2-frequency set can be gotten by intersection operation of  1-frequency set: S(TW=? L?)?S(ZYBZ=? Wind-heat?)= {U1 ,U4 } S(TW=?H?)? S(TT=?N?)= {U6 , U8} S(TW=?H?)?S(ZYBZ=?Wind-cold??)= {U3 , U6} S(TT=?Y?)?S(ZYBZ=?Wind-cold??) = {U2 , U3} S(TT=?N?)?S(ZYBZ=?Wind-heat?) = {U4,U5 ,U8 } S(TT=?N?)?S(ZYBZ=?Wind-cold??)= {U6 , U7} Then L1={{TW=?L??ZYBZ=?Wind-heat?}?  {TW=?H?,TT=?N?}, {TW=?H?,ZYBZ=?Wind-cold??}, {TT=?Y?,ZYBZ=?Wind-cold??}, {TT=?N?,ZYBZ=? Wind-heat? }, { TT=?N?,ZYBZ=?Wind-cold?? } }  To calculate confidence degree(only for the decision set ZYBZ):  C((TW=? L?) ==> (ZYBZ=?Wind-heat?))=2/4=0.5 C((TW=?H?) ==> (ZYBZ=?Wind-cold?))=2/4=0.5 C((TT=?Y?) ==> (ZYBZ=?Wind-cold?))=2/4=0.5 C((TT=?N?) ==> (ZYBZ=?Wind-heat?))=3/4=0.75 C((TT=?N?) ==> (ZYBZ=?Wind-cold??))=2/4=0.5 Because the support degree of all intersections of  2-Frequency Set and 1-frequency set are not larger than 2, there is no 3-Frequency set.By comparison with confidence degree and support degree, five rules can be sorted as follow:  R={(TT=?N?)==>(ZYBZ=?Wind-heat?),(TW=?L?)==>( ZYBZ=?Wind-heat?),(TW=?H?)==> ZYBZ=?Wind-cold?), (TT=?Y?) ==> (ZYBZ=?Wind-cold?), (TT=?N?) ==> (ZYBZ=?Wind-cold??)}  By comparison with the TCM clinical experiment, the above results is in line with the actual situation.[8-9]  5 RESULTS  In summary, the R_Apriori algorithm improves the efficiency, the mining rules effectiveness and strength level by combining rough set theory and the Apriori algorithm.

R_Apriori algorithm has much practical significance and worthy of promotion.

