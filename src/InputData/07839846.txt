

it iN SMaRt CitiES   Big Data Processing Stacks  Big Data  Sherif Sakr, King Saud bin Abdulaziz University for Health Sciences, Saudi Arabia  The past few years have seen the rise of big data processing stacks enhanced with domain-specific, optimized, and vertically focused features. The author analyzes these processing stacks? capabilities and describes ongoing developments in this domain.

T he radical expansion and integration of computation, networking, digital devices, and data storage has generat- ed large amounts of data that must be  processed, shared, and analyzed. For example, Facebook generates more than 10 petabytes of log data monthly, and Google processes hun- dreds of petabytes per month. Alibaba generates tens of terabytes in daily online trading trans- actions. This collected information is grow- ing, and the explosive increase of global data in all 3Vs (volume, velocity, and variety) has been termed big data. According to IBM, we are cur- rently creating 2.5 quintillion bytes of data ev- ery day (https://www-01.ibm.com/software/data/  bigdata/what-is-big-data.html). IDC predicts that the worldwide volume of data will reach 40 zettabytes by 2020, 85 percent of which will be unstructured data in new types and formats?in- cluding server logs and other machine-generated data, data from sensors, social media data, and many other sources (www.emc.com/about/news/ press/2012/20121211-01.htm). In practice, these conditions represent a new scale of big data that has been attracting a lot of interest from both the research and industrial communities, which hope to create the best means of processing, ana- lyzing, and using this data.

In principle, data is a key resource in the mod- ern world. However, it is not useful in and of itself.

computer.org/ITPro  3 5  Data has utility only if meaning and value can be extracted from it. Therefore, continuous, in- creasing efforts are devoted to producing and analyzing it to extract this value. In principle, big data discovery enables data scientists and other analysts to uncover patterns and correlations by analyzing large volumes of diverse data. Insights gleaned from big data discovery can provide businesses with significant competitive advan- tages, such as more successful marketing cam- paigns, decreased customer churn, and reduced loss from fraud. Therefore, it is crucial that these large, emerging data types be harnessed to provide a more complete picture of what is happening in various application domains. Con- sequently, the increasing demand for large-scale data processing and data analysis applications has triggered the development of novel solutions from industry and academia. In the current era, data represent the new gold, with analytics sys- tems representing the machinery that analyzes, mines, models, and mints it.

For roughly a decade, the Hadoop framework has been the de facto standard of big data tech- nologies; it has been widely used as a popular mechanism for harnessing the power of large computer clusters. However, with the increasing demands and requirements of various big data processing applications (big graphs, big streams, big SQL, big machine learning, and so on), both the research and industrial communities have recognized various limitations in the Hadoop framework.1,2 It has become apparent that Ha- doop?s original design and implementation can- not serve as a one-size-fits-all solution for all big data processing problems. Therefore, the Hadoop big data processing stack has been equipped with various extensions to deal with new demands. In addition, new big data processing stacks, such as Spark and Flink, have been introduced to address various limitations of the Hadoop framework.

Hadoop Stack The Hadoop project was introduced as an open source Java library that supports data-intensive distributed applications and clones the imple- mentation of Google?s MapReduce framework.3 In principle, Hadoop consists of two main com- ponents: the Hadoop Distributed File System (HDFS) and the MapReduce programming model (see Figure 1). Specifically, HDFS provides  the basis for distributed big data storage, which distributes data files into data blocks and stores them in different nodes of the underlying com- puting cluster to enable parallel data process- ing. The MapReduce programming model is a simple but powerful model that enables the easy development of scalable parallel applications to process vast amounts of data on large comput- ing clusters.3 In particular, it isolates the appli- cation developer from the sophisticated details of running a distributed program, including is- sues of data distribution, scheduling, and fault tolerance. In this model, the computation takes a set of key-value pairs as input and produces a set of key-value pairs as output. MapReduce us- ers can express the computation using two func- tions: Map and Reduce. The Map function takes an input pair and produces a set of intermediate key-value pairs. The MapReduce programming model groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function, which re- ceives an intermediate key I with its set of values and merges them together. Typically, an output value of zero or one is produced per the Reduce invocation.

This model?s main advantage is that it allows large computations to be easily parallelized and re-execution to be used as the primary mecha- nism for fault tolerance. Therefore, Hadoop has been widely used in various big data applications, including clickstream analysis, spam filtering, network search, and social recommendations.

Hadoop has been used by several big compa- nies, such as Yahoo and Facebook, and many companies support Hadoop commercial execu- tion environments, including IBM, Oracle, EMC, Cloudera, MapR, and Hortonworks.

Despite the Hadoop framework?s widespread success, it suffers from various limitations. For  Figure 1. Hadoop?s ecosystem. Hadoop allows large computations to be easily parallelized and enables the implementation of a simple and elegant fault-tolerance mechanism, but its design is not adequate for supporting real-time processing of large-scale streaming data.

Tajo  Impala  Hive  Hama  Giraph  MapReduce  HDFS  Mahout    36 IT Pro  January/February 2017  Big Data  example, the oversimplicity of the MapReduce programming model?which relies on a rigid, one-input, two-stage dataf low?requires that users devise inelegant workarounds to perform tasks that have a different dataflow (such as joins or n stages). In addition, many programmers might prefer to use other abstract and declarative languages (in which they are more proficient), such as SQL, to express their tasks, while leav- ing all the execution optimization details to the backend engine. In addition, several studies have reported that Hadoop is the wrong choice for in- teractive queries that have a target response time of a few seconds or milliseconds.4  In particular, Hadoop has proven to be inef- ficient at processing large-scale structured data, and traditional parallel database systems have been doing much better in this domain. There- fore, the Hadoop stack has been enhanced by several components that are designed to tackle these challenges. For example, Hive has been in- troduced to support SQL on Hadoop with famil- iar relational database concepts such as tables, columns, and partitions.5 It supports queries that are expressed in a SQL-like declarative language, the Hive Query Language (HiveQL), which rep- resents a subset of SQL92 and can thus be easily understood by anyone familiar with SQL. These queries automatically compile into Hadoop jobs.

Impala (http://impala.io) is another open source project, built by Cloudera, to provide a massively parallel processing SQL query engine that runs natively in Apache Hadoop. It utilizes the standard components of Hadoop?s infrastructure (such as HDFS, HBase, and Yet Another Resource Ne- gotiator, or YARN) and can read the majority of widely used file formats (Parquet, Avro, and so on). Through Impala, users can query data that is stored in HDFS. The IBM big data processing platform, InfoSphere BigInsights, which is built on the Apache Hadoop framework, has provided a Big SQL engine as its SQL interface. It provides SQL access to data that is stored in InfoSphere BigIn- sights and uses the Hadoop framework for com- plex datasets and direct access for smaller queries.

Apache Tajo (http://tajo.apache.org) is another distributed data warehouse system for Apache Ha- doop that is designed for low latency and scalable ad hoc queries of ETL (extract, transform, load) processes. Tajo can analyze data stored on HDFS, Amazon Simple Storage Service (S3), OpenStack  Swift, and local file systems. It provides an exten- sible query rewrite system that lets users and exter- nal programs query data through SQL.

With the enormous growth in the sizes of graph datasets, the demand on scalable graph processing platforms has been increasing. For instance, Facebook has reported that its social network graph contains more than a billion users (nodes) and more than 140 billion friendship re- lationships (edges). In practice, large-scale graph processing requires huge amounts of computa- tional power to analyze. In particular, graph pro- cessing algorithms are iterative and must traverse the graph in a particular way. In practice, graph algorithms can be implemented as a sequence of Hadoop invocations that passes the entire state of the graph from one step to the next. How- ever, this mechanism is not adequate for graph processing and leads to inefficient performance because of the associated serialization and com- munication overhead.

Apache Giraph has been introduced as an open source project that supports large-scale graph processing and clones the implementa- tion of Google?s Pregel system.6 Giraph provides an implementation for the Bulk Synchronous Parallel (BSP) programming model, which sup- ports a native API specifically for programming graph algorithms using a ?think like a vertex? computing paradigm. BSP is a parallel program- ming model that uses a message-passing inter- face (MPI) to address the scalability challenge of parallelizing jobs across multiple nodes; the com- putation on vertices is represented as a sequence of supersteps with synchronization between the nodes participating at superstep barriers. Each vertex can be active or inactive at each iteration (superstep). Giraph was initially implemented by Yahoo. Facebook has since developed its Graph Search facilities using Giraph.

Giraph runs graph processing jobs as map- only jobs on Hadoop and uses HDFS for data input and output. Like Giraph, Apache Hama (https://hama.apache.org) is another BSP-based implementation project designed to run on top of the Hadoop infrastructure. However, it focus- es on general BSP computations, not just graph processing. For example, it includes algorithms for matrix inversion and linear algebra. Machine learning algorithms represent another type of ap- plication that is iterative in nature. The Apache    computer.org/ITPro  3 7  Mahout project has been designed to build scal- able machine learning libraries using the Ha- doop framework.

Several applications and domains?including financial markets, surveillance systems, manu- facturing, smart cities, and scalable monitoring infrastructure?include a crucial requirement to collect, process, and analyze big streams of data to extract valuable information, discover new in- sights in real time, and detect emerging patterns and outliers. For example, the world?s largest stock exchange, the New York Stock Exchange (NYSE), reported trading more than 800 million shares on a typical day in October 2012. As an- other example, IBM reported that, by the end of 2011, about 30 billion RFID tags were in circula- tion, each of which represented a potential data generator (www.dotgroup.co.uk/wp-content/ uploads/2014/11/Harness-the-Power-of-Big-Data- The-IBM-Big-Data-Platform.pdf). Hadoop?s design is not adequate for supporting real-time process- ing of large-scale streaming data.

Spark Stack The Spark project (http://spark.apache.org) has been introduced as a general-purpose big data processing engine that can be used for many types of data processing scenarios. Spark, writ-  ten in Scala, was originally developed in the AMPLab at the University of California, Berke- ley. It was made open source in 2010 as one of a new generation of dataflow engines following the line of the Hadoop framework. In particular, Hadoop introduced a radical new approach based on distributing data when it is stored and run- ning computation where that data is. However, one of its main limitations is that it requires that the entire output of each map and reduce task be materialized into a local file on the HDFS before it can be consumed by the next stage. This ma- terialization step allows for the implementation of a simple and elegant checkpoint/restart fault- tolerance mechanism, but it dramatically harms system performance. Spark takes the concepts of Hadoop to the next level by loading the data in distributed memory and relying on less expensive shuffles during data processing. Figure 2 illus- trates the main architectural differences between the Spark and Hadoop frameworks.

In Spark, a function represents the fundamen- tal unit of programming; its fundamental data ab- straction is called a resilient distributed dataset (RDD).

These RDDs represent a logical collection of data partitioned across machines that are created by referencing datasets in external storage systems or applying coarse-grained transformations (such  Figure 2. (a) Spark framework vs. (b) Hadoop framework. Spark takes the concepts of Hadoop to the next level by loading the data in distributed memory and relying on less expensive shuffles during data processing.

Spark job  Step Step Step Step  StepStepStepStep  RAM (Memory)  RAM (Memory)  RAM (Memory)  HDFS (Hard disk)  HDFS (Hard disk)  HDFS (Hard disk)  HDFS (Hard disk)  HDFS (Hard disk)  Hadoop job  (a)  (b)    38 IT Pro  January/February 2017  Big Data  Figure 3. Spark?s ecosystem. In addition to its core API, Spark supports several libraries and provides additional functionalities for big data processing.

Spark SQL GraphX Spark streaming MLib Spark R  Spark core engine  Apache YARN Apache Mesos  Other sourcesAmazon S3CassandraHDFS  as filter, map, reduce, or join) to existing RDDs.

An RDD is an in-memory data structure, which gives power to Spark?s functional programming paradigm by enabling user-defined jobs to exploit the loaded data into a cluster?s memory and query it repeatedly. The resilient features of RDDs en- sure that if data in memory gets lost, it can be rec- reated with the available metadata information.

In addition, users can explicitly cache an RDD in memory across machines and reuse it in mul- tiple parallel operations. Specifically, RDDs can be manipulated through operations such as filter, map, and reduce, which take functions in the pro- gramming language and ship them to nodes on the cluster. This simplifies programming com- plexity because the way applications manipulate RDDs is similar to how local data collections are manipulated.

In addition, Spark relies on a lazy execution model for RDDs. In particular, RDD data is not processed and materialized until an action is per- formed. For cluster management, Spark supports both Apache Mesos and Hadoop YARN. Spark can interface with a wide variety of distributed storage implementations, including the HDFS, the Cassandra NoSQL database, and Amazon S3. Spark provides APIs for various programming languages, including Scala, Java, Python, and R. During the 2014 annual Daytona Gray Sort Challenge (http://sortbenchmark.org), which benchmarks the speed of data analysis systems, Spark strongly outperformed Hadoop and was able to sort through 100 terabytes of records in 23 minutes, whereas Hadoop took more than three times as long (approximately 72 minutes) to execute the same task. Currently, Spark has more than 500 contributors from more than 200 orga- nizations, making it the most active project both in the Apache Software Foundation and among big data open source projects in general. Popular distributors of the Hadoop ecosystem (Cloudera, Hortonworks, and MapR, for example) are cur- rently including Spark in their releases.

Figure 3 shows an overview of the stack for the Spark big data process- ing framework. In addition to the Spark core API, various libraries are part of the Spark ecosystem and provide additional functionalities for big data processing. In particular, Spark provides various packages with higher-level libraries, including sup-  port for SQL queries,7 streaming data, machine learning,8 statistical programming, and graph pro- cessing.9 These libraries increase developer pro- ductivity and can be seamlessly combined to create complex workflows. For example, SparkSQL inte- grates relational processing with Spark?s function- al programming API.7 It bridges the gap between the two models by providing a DataFrames API that can execute relational operations on both ex- ternal data sources and Spark?s built-in distributed collections.7 SparkSQL relies on an extensible op- timizer, called Catalyst, that supports adding data sources, optimization rules, and data types for do- mains such as machine learning.

GraphX is a distributed graph engine built on top of the Spark framework.9 GraphX extends Spark?s RDD abstraction to introduce the resilient distributed graph (RDG), which associates records with vertices and edges in a graph and provides a collection of expressive computational primi- tives. The GraphX RDG leverages advances in distributed graph representation and exploits the graph structure to minimize communication and storage overhead. Spark is also equipped with an extension API that adds support for continuous stream processing. In particular, Spark stream- ing relies on the micro-batch processing mecha- nism, which collects all data that arrives within a certain time period and runs a regular batch pro- gram on the collected data. While the batch pro- gram is running, the data for the next mini batch is collected. Therefore, it can be considered as a batch-processing mechanism with a controlled time window for stream processing.

Flink Stack Apache Flink (https://flink.apache.org) is anoth- er distributed in-memory data processing frame- work. It represents a flexible alternative to the Hadoop framework that supports both batch and real-time processing. Instead of Hadoop?s map and reduce abstractions, Flink uses a directed    computer.org/ITPro  3 9  graph approach that leverages in-memory storage to improve runtime execution performance. Flink can run as a completely independent framework or on top of HDFS and YARN. It originated from the Stratosphere research project, which began at the Technical University of Berlin in 2009 before joining Apache?s incubator in 2014.10 Recently, Flink has become a top-level project at the open source Apache Software Foundation.

In principle, Stratosphere uses a richer set of primitives than Hadoop, including ones that al- low the easy specification, automatic optimiza- tion, and efficient execution of joins. It treats user-defined functions (UDFs) as first-class citizens and relies on a query optimizer that au- tomatically parallelizes and optimizes big data processing jobs. Stratosphere offers both pipe- line (interoperator) and data (intraoperator) parallelism. In particular, Stratosphere relies on the parallelization contracts (PACTs) program- ming model,10 which represents a generalization of map/reduce based on a key-value data model and the PACTs concept. A PACT consists of ex- actly one second-order function, called an input contract, and an optional output contract. An input contract takes a first-order function with task- specific user code and one or more datasets as input parameters. The input contract invokes its associated first-order function with independent subsets of its input data in a data-parallel fashion.

Figure 4 shows an overview of the layers for the Flink big data processing framework. The Flink sys- tem is equipped with the Flink Streaming API, an extension of the core Flink API for high-through- put and low-latency datastream processing. The system can connect to and process datastreams  from various data sources (such as Flume or Ze- roMQ) where datastreams can be transformed and modified using high-level functions similar to those provided by the batch-processing API. In addition, the Flink open source community recently devel- oped libraries for machine learning (FlinkML) and graph processing (Flink Gelly).

Table 1 summarizes the features of the Hadoop, Spark, and Flink stacks.

Pipelining Frameworks One of the most common scenarios in big data processing is that users need to be able to break down the barriers between data silos such that they can design computations or analytics that combine different types of data (structured, un- structured, stream, graph, and so on) or jobs. To tackle this challenge, several frameworks have been introduced to build pipelines of big data processing jobs. Table 2 summarizes the features of these various frameworks.

Table 1. Feature summary of Hadoop, Spark, and Flink.

Feature Hadoop Spark Flink  Year of origin 2005 2009 2009  Place of origin MapReduce (google)  Hadoop (Yahoo)  University of California, Berkeley  technical University of Berlin  Programming model Map and reduce function over key-value pairs  Resilient distributed datasets (RDDs)  Parallelization contracts (PaCt)  Data storage Hadoop Distributed File System (HDFS)  HDFS, Cassandra, and others HDFS, amazon Simple Storage Service, and others  Execution engine Yet another Resource Negotiator (YaRN)  YaRN and Mesos Nephele  SQL support Hive, impala, and tajo SparkSQL N/a  graph support N/a graphX gelly  Streaming support N/a Spark streaming Flink streaming  Figure 4. Flink layers. The Flink system is equipped with the Flink streaming API, an extension of the core Flink API for high-throughput and low-latency datastream processing.

Meteor Flink Gelly Flink ML  PACT  Nephele  Apache YARN Amazon EC2  HDFS Amazon S3 Other sources    40 IT Pro  January/February 2017  Big Data  Apache Tez (https://tez.apache.org) is a gen- eralized data processing framework. Tez allows building dataflow-driven processing runtimes by specifying a complex, directed acyclic graph (DAG) of tasks for high-performance batch and interactive data processing applications. In Tez, data processing is represented as a graph in which the vertices of the graph represent data processing, and the edges represent the move- ment of data between the processing elements.

Tez uses an event-based model to communicate between tasks and the system and between vari- ous components. These events are used to pass information such as task failures to the required components, whereby the dataflow moves from output to input. In Tez, the output of a Hadoop job can be directly pipelined to the next Hadoop job without requiring the user to write the inter- mediate results into HDFS. In case of any fail- ure, the tasks from the last checkpoint will be executed.

In general, Tez is designed for frameworks such as Hive and Pig and not for developers to directly write application code for execution. In particu- lar, when using Tez along with Pig and Hive, a single PigLatin or HiveQL script will be con- verted into a single Tez job and not into a DAG of Hadoop jobs. However, execution of a DAG of Hadoop jobs on a Tez can be more efficient than its execution by Hadoop because Tez applies dy- namic performance optimization mechanizers that use real information about the data and the resources required to process it.

Apache MRQL (https://mrql.incubator.apache.

org) is another framework that has been intro- duced as a query processing and optimization framework for distributed and large-scale data analysis; it?s built on top of Apache Hadoop, Spark, Hama, and Flink. MRQL provides a SQL- like query language that can be devaluated in four independent modes: MapReduce mode us- ing Apache Hadoop, Spark mode using Apache  Spark, BSP mode using Apache Hama, and Flink mode using Apache Flink. However, further re- search and development is still required to tackle this important challenge and facilitate the job of users.

Apache Crunch (https://crunch.apache.org) is a Java library that provides implementing pipelines that are composed of many user-de- fined functions and can be executed on top of Hadoop and Spark engines. Apache Crunch is based on Google?s FlumeJava library11 and is efficient for implementing common tasks such as joining data, performing aggregations, and sorting records. Cascading (www.cascading.

org) is another software abstraction layer for the Hadoop framework that is used to create and execute data processing workflows on a Hadoop cluster using any Java virtual machine- based language (Java, JRuby, Clojure, and so on); it hides the underlying complexity of the Hadoop framework.

A fter in-depth research, a McKinsey re-port pointed out that big data has created value in the US healthcare, US retail, EU public sector administration, global personal lo- cation data, and global manufacturing markets.12 The report also shows that big data has played a primary role in economic functions and in im- proving the effectiveness and productivity of the public sector and enterprise, creating valuable benefits for consumers.

For roughly a decade, the Hadoop framework has dominated the big data processing world.

However, with increasing big data processing de- mands and requirements, recently, we have wit- nessed the rise of competing processing stacks with various engines that are vertically focused and optimized for tackling specific problems and application domains. In addition, various pipelining frameworks have been introduced to  Table 2. Features summary of pipelining frameworks.

Feature TEZ MRQL Cascading Crunch  Execution engine Hadoop Hadoop, Spark, Flink, and Hama  Hadoop Hadoop and Spark  Pipeline definition Edges SQL Operators Operators  Pipeline connection No SQL Source and target Branches, joins, and mapto  Programming languages  Java SQL Java virtual machine-based languages  Java and Scala    computer.org/ITPro  41  enable the creation of workflows that can be ex- ecuted over multiple engines. Therefore, these big data processing stacks can evolve from being competitors of or replacements for each other into augmentable or complementary engines that can efficiently deal with complex big data processing requirements. For example, the re- cent Cloudera distributions support Hadoop with Spark over the same ecosystem. I believe that this direction will progressively continue to further support the wider vision of big data pro- cessing technologies.

