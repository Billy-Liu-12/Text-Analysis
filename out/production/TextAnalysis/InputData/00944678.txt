Linguistic Association Rules

Abstract  The class of Apriori algorithms are popular association rule mining techniques. However, these algorithms are computationally expensive.

In this paper we propose a another novel approach to extract association rules. The method represents an itemset information as a cell of a hypercube. The hypercube encodes associations between the items of each transaction. Apart from proposing the main result we also propose Linguistic Association Rules. Linguistic Association Rules encode Fuzzy information and represent summarized rules.

1. Introduction By mining huge data repositories we discover latent, useful and previously unknown information. Recently in this area researchers are extensively focusing on development of faster and new algorithms of data mining.

Since the advent of Agarwal et. a1 [ll?s proposal the association rules have become one of the cornerstones representing the market-basket analysis in the data mining research. We describe a scenario that explains the need for discovering association rules.

Many readers might note that many of today?s online bookstores recommend their customers to buy different types of books in the related areas by tracking their clickstream patterns. And association rules can be used to track these pattems.

In our scenario we assume that the customer has bought a few different books on related to data mining. Next time the person visits that online bookstore, she finds that the web page recommends her books on clustering. Her earlier buying patterns were analyzed along with the buying pattems of other customers by mining association rules. Recommendations like Data Mining+ Clustering these are instances of association rules. As we note that an  association rule encodes statistical information that implies ?Data Mining? is related to ?clustering?.

Similarly, other topics related to data mining can be discovered. And in this fashion any business store can use these new-fangled association rules to generate new profitability opportunities.

The items often co-occur together in a database transaction, and the set of these items is known as an itemset. An association rule denoted by X * Y where X and Yare itemsets. Note that these rules are not logical implications or actual rules. However, their role is to encode information about associations within itemsets. In other words, these rules indicate the strength of coupling of items within the itemsets for a given set of transactions.

From a statistical perspective, it is observed that the association rules locate the groups of itemsets. The co- occurrence of any particular itemset intuitively suggest that there may be some form of association binding or there may exist a relationship that binds these items together. The idea is not to discover what type of relationship does the transaction set encodes, but to figure out the relational association that inherently exists.

Since the original proposal of association rules by Agarwal et a1 [ I ]  the fundamental reasoning to derive these rules has remained the same. Only better performing algorithms have been proposed in recent years [2,3,5,9,101.

In most of the algorithms, the construction of association rules follows two distinct steps.

They are:  Extraction of frequent itemsets -- To find out how many itemsets dominate or influence the data. This phase is an expensive procedure and requires many I/O operations.

Generation of Association Rules -- The rules are generated from the set of extracted frequent Itemsets.

This operation is less expensive and is fairly simple.

Researchers have proposed many various algorithms, and they have improved the computational complexity of  0-7803-7@78-3/0lL$lO.W (C)Uw)l IEEE. Page: 645  mailto:sroychow@us.oracle.com   the algorithms over the years. More specifically a substantial amount of research has gone in developing algorithms that improve upon the computation factor of finding itemsets efficiently.

Computation of association rules requires identifying of a structure in the domain ofL{S}x%, where LIS} is the lattice of a set S a n d  31 is the domain of real numbers.

The substructure is shown in Figure 1. In the Figure 1 {B}, {C}, and {BC} are identified as these sets have higher frequency values that the threshold value of 10.

states only one major result that this method has produced for a given dataset.

2. Association Rules: Earlier Works In this section we will briefly review few algorithms that have been proposed in the literature.

2.1 Apriori Algorithm  Aganval and Srikant [2 ]  proposed the idea of building rules from transactional databases. The idea was to mine all association rules from a given database with respect to a set of minima1 threshold measures. These measures were called support eupp) and confidence (conj). The support measured the frequency of itemsets throughout the database. It is mainly a frequency based measure. For instance, if an itemset AB had a frequency of 20 in 50 transactions then we would compute Supp(AB) as 20150.

Therefore, we note that this is a frequency-based measurement. On the other hand we have Conf of a rule (AB + C) that is defined as:  {AB} :4 {AC} :2  Supp( ABC) Conf (AB 3 C )  = SUP(AB) .

Figure 1. {B}, {C}, and {BC} are identified as itemsets of interests as these sets have higher frequency values than that of the threshold value of 10.

Note that each transaction is a part of the lattice. The frequency of itemset cumulates during the database scan.

This particular observation has lead us to propose this model, where computing of all the itemsets is performed in only one database pass. Another aspect of our model lies in our belief that a typical transaction does not produce high frequency of large itemsets (as we approach towards the top of the lattice). Using this idea an additional control parameter is introduced apart from the traditional parameters like support and confidence. This parameter facilitates the user to specify the maximum cardinality of the itemset that the association rule should focus, and discard other itemsets that have cardinalities larger than desired cardinality. This is a major step as it regulates the number of rules produced, and can aid to manageability of the rules.

On conceptually comparing this work with other well-  known works like Apriori[2] and DIC[lO] and others, we find that we do not spend time to identify the candidate itemsets and thereby avoid overhead computation.

In the following sections we will briefly examine the other algorithms that have been proposed in the literature.

Section 3 explains our algorithm. The following section  These measures are still primarily used in most of the algorithms that extract association rules.

Briefly, apriori uses the downward closure property (see [2] for details), that any subset of frequent itemset is also frequent. During each iteration of this algorithm, a set of candidate itemsets is generated and latter from this candidate set a frequent-set is finally computed. Frequent- set stores the valid itemset. The implementation this algorithm uses a data structure called Hash Set to store the itemsets.

The method?s computational expensiveness is one of the serious drawhacks. Moreover, many association rules are generated which can be overwhelming.

The algorithm is: Begin  L, = {Frequent 1-itemsets}; for (k = 2; L,-, != 0; k*)  C, = Set of New Candidates; for all transactions t E Database  for all k-subsets s of t if (s E C,)s.count+,  Lk = {c E C, I c.count > minsupp}; Set of all frequent itemset = U , L,  End  The other methods in this class of algorithm are namely the SETM method [2],  generalized association rules [4] and quantitative associationruies[6,13].

0-7803-7@78-3/0U$l0.00 (C)2001 IEEE Page: 646    2.2.1 Alternative methods  Sarvare et al[S] proposed the Partition method. In this method the database is horizontally partitioned into non- overlapping partitions. Each partition is locally computed in the memory using tid-apriori method described in [2] by Agarwal and Srikant. Global frequent itemsets should be locally frequent sets in each partition. When compared with apriori, the results were encouraging, as the algorithm was able to reduce CPU and I/O overhead. For details refer to [5]. On the other hand, Dynamic Itemset Count (DIC) [IO] algorithm finds new itemsets using a few passes over the databases using the sampling technique that was originally proposed by Toivonen.[7] significantly improving the results of apriori algorithm.

They also provided with another measure of evaluation called conviction in lieu of confidence. Silverstein et al, [9] are have extended association rules to analysis of dependence rules. This work is taking the market-basket analysis into more generic framework of implication rules.

3. The Proposed Method This is the main section that explains how our algorithm in detail. It also introduces a simple yet a novel approach to our understanding of association rules. In the process we also propose transactional hypercube which is the cornerstone of our mining process.

3.1 Definitions  Definition 1 : An attribList is defined as a set of attributes of a dataset. It is denoted by {Ci I i ?  {1, ..., m } }  , where Ci is i' attribute and m denotes the cardinality of the attrib- List.

The attribList should have at least one attribute. For a table in database the attributes are columns.

Definition 2 An attribute can have binary value. The attribute has value 1, if the attribute exits. Otherwise, the attribute value would be 0.

,Definition 3: A transactionstring is an attriblist, and each attribute has a binary value.

ExamDle 1 : <O, 1, 1, 0, 1> is a transactionstring with c, =o ,c, = 1,  c, = 1 ,c4 = o  ,c, = 1.

We model each transaction of the dataset as transactionstring.

Definition 4: Assume that A is an attribList as defined in the Definition 1. We define a transactional hypercubeM, such that  k  *I M , = x A ,  where k can take value from 1 to k = m  . Note that k encodes the degree of the transactional hypercube M ,  .

Definition 5 :  A cell is defined as occurrence of more than one attribute. A cell encodes an association between two or more attributes.

Examde 2: Following the Example 1, a cell c,C, is composed of C, andC, . Therefore, in our case, a cell represents an itemset. C,C,  Definition 5 :  The cell value is computed by the following binary function f (.) .

1 ci=l cj=l 0 c,=o e,=+ 0 e,=+ c,=o  Note that the above function is a logical AND function.

And we use this function to compute the values in the cell.

ExamDle 3: Following the Example 2, the cell value is c,c, = 0.

Definition 6: Binary operation @ of two transactional hypercubes of same dimension is possible. And the addition is performed on a cell-to-cell basis.

Examde 4: Let M ,  and M,' are two transactional hypercubes to be added to form ML-. We perform cell- wise addition as mentioned below:  (CiCj>" = (CiCj)' + (CiCj) to get the following:  M,-= M,' CB M ,  .

Similarly other operations like subtraction, multiplication, and division are possible. However, for our analysis we do not need those here.

3.2 Transactional Hypecubes  Itemsets are encoded in cell in a transactional hypercube for us. Therefore there is no need for us to find candidate itemsets etc. The type of approach most of the association rule algorithms define and use.

A cell in M, encodes an itemset with two attributes. For example, we had ( C,C, ) in the Example 2. Similarly, to encode the three attributes (C,C,C,) at a time we need a cell in M ,  , ( C,C4C,C, ) four at time in M ,  and so on till  0-7803-7@78-3/0U$l0.00 (C)2001 IEEE. Page: 647    we exhaust our m. Initially this may look strange from a computational point of view as these dimensions keep on increasing, but interestingly we have to many zeros that spare our computational life. With a few more common-  -c2 c3 c4 %  c2  c3  c4  C,  Figure 2. A transactional hypercube of dimension 2. Note that the cell C,C, = C,C, is 0.

sense reduction our computational factor is taken care off quite smoothly.

When a transactional hypercube M ,  is constructed it  creates m 2  cells. Out of these cells the diagonal cells ( CiCi) that encode self-information is useless. Now using the property of associativity of the cells we can safely discard CjCi cells. In a transactional hypercube of dimension higher than 2, it can be easily realized that the  cells actually exist. only a small number of valid  Note that ( y ]  << m' . We use sparse matrices to compute the cells. Only these cells represent a transactional hypercube. Addition of these transactional hypercubes is computationally fast and effective. We are fortuitous because lattice is a unique structure and is embedded in these transactional hypercubes.

(3  3.3 The Algorithm  The algorithm consists of two parts: 1) Itemset computation and 2) Association rule generation.

3.3.1 Iternset Computation  Let the user select a user-defined itemset cardinality level.

This parameter is not the same as the support parameter.

Moreover, this is a new parameter that is introduced in  this algorithm. The parameter allows the user control the extraction of association rule for a given cardinality of the itemset.

All the algorithms defined in the literature continue till no larger candidate itemsets the can be found. For instance, if we have an attribList of 10 items. In this case, if the user wants to have only the set of rules that are generated from itemsets with 6 items it is very much possible to handle such situation. Since computation of itemsets are independent with respect to itemset cardinality, so there is no impact if need not derive the higher order itemset. This significantly improves the computation time. This is a significant change in our approach in the extraction few association rules and flexibility that it renders make this algorithm quite desirable. Furthermore, if the user wants to use all itemset cardinality levels that to is possible.

Algorithm to compute itemsets Begin  U =user defined Itemset level // max of u would be n for (k = 2; k < U; k++)  create MPk & initialize cell value to 0  for all transactions t E Database Begin  Fetch the binary transactionstring of the transaction t for (k = 2; k < U; k++) Begin  Compute M,; MPk = M P t  @ M , ;  End  //Normalize for (k = 2; k < U; k++) Begin  MPk = MPk /I t 1; End accept if (cell E M P t  )  cell.count > minsupp; End .Set of all frequent itemset = U, MPk  Once the itemsets are found, then for each itemset find association rule.

3.4. I Association Rule Generation  This part remains identical to the Apriori algorithm. We use the confidence measure Confof a rule ( CiCjC, * C, ) that is defined as:  0-7803-7078-3/0l/$l0.00 (C)u)ol IEEE Page: 648    We know that an itemset C,C,C,C, can generate  CsCjC, 9 Ci ,  with 3 antecedents and one consequent.

And also we get rules with 2 antecedents and 2 consequents.

Consequently the rules have Confidence and support parameters for each of the rules.

c,cjc, * c,,c,cjcs * c,, cicsc, * c j ,  cicj * c,c,,cic, * c*cj c,cj * c,ci cicj * c,c,  0 1  1 0 1 0 1 1 0 1  4. Computational Result The plots in Figure 3 shows that a relationship between execution time and support value. Note that as the support increases the execution time decrease. However, large support values tend to eliminate most of the itemsets.

Therefore large support values are not very desirable. At low support values much of itemsets are produced. And  1 2 3 4 5 6 7 6 9 1 0  support value ( X 0.1)  Figure 3. Series 1 denotes Apriori algorithm, wherear Series 2 is our proposed method.

the execution time is definitely higher. Our proposed algorithm produces series 2, whereas the series 1 provides the results of the Apriori algorithm. In the future we would like to extend this technique and analyse the storage and temporal complexities. We used all the dimensions in our example in order to be comparable with the Apriori algorithm.

5. Linguistic Association Rule The above algorithm works for binary attributes. In the next section we will use this algorithm in our study of linguistic association rules.

Researchers began to extend binary attributes to quantitative attributes. Srikant et al. proposed an algorithm to find association rules followed by partitioning the attribute domain, by combing the adjacent partitions, and then transforming the problem into a binary problem. Strict partitioning of intervals lead to non -intuitive restrictions. To solve such problems researchers  approached started mapping the intervals to binary attributes by performing a mapping between the intervals and the Boolean value. Later researchers have extended the intervals to fuzzy sets, and generated fuzzy association rules. For details, refer to [15,16,17].

However, we view that itemsets themselves can be arranged in such a way where the attributes in the universe of discourse can be arranged as fuzzy sets, as shown in Figure 4.

Rubber Plastic Wood  Figure 4. The universe of discourse of the transaction is represented by the three fuzzy sets. The Cs represent the attributes that are made up of rubber, plastic or wood.

Fuzzy association rule is defined on quantitative attributes but the linguistic association rule is defined on binary attributes.

From the above figure we can note that C,has a membership of 0.5, where C, has membership value of 0.8. In the reality, C, might be rubber ball, C, might be a automobile type. The degree of rubber content might be differentiating these products. And so are the items are classified in (Rubber, Plastic, Wood}.

The linguistic association rules derived using the above universe of discourse and the association rule given below (for the binary attributes having confidence of 50%) we have:  [ O  2 4 1  + [6 7 8 91:: conf = 0.5 and [<Rubber><l.O><Rubber><0.8><Plastic<l.O>] -> [<Plastic><0.5><Wood><O.8><Wood><l.O> <Wood><l.O>] : : confidence f ac to r  = 50%  This rule linguistic rule can be further summarized using a T* (T-conorm):  [T*(<Rubber><l .O><Rubber><0.8>)& <Plastic4 .O>] + [<Plastic><0.5>&T*(<Wood><O.8><Wood><l.O> <Wood><l.O>)]:: confidence factor = 50%  [T<Rubber><l.O>&<Plastic<l.O>] -> [<Plastic><0.5>& (<Wood><l.O>)]:: confidence factor = 50%  to  Page: 649    Now we may note that for a given user choice level we can summarize information from various itemsets to find more condensed association rules at a higher granular level.

6. Conclusions We have proposed a new method of computing itemsets.

The method employs construction of transactional hypercube. We use the transactional hypercube of different dimensions simultaneously to accommodate the computation of itemsets of different cardinalities.

Therefore, we only make one database pass while we compute all the itemsets. We also introduced another parameter apart from the standard parameters like support and confidence that can allow users to restrict the generation of itemsets larger than certain cardinality.

