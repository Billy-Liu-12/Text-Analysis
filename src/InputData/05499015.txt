1 INTRODUCTION

Abstract: Aiming at value reduction, a kind of RSVR algorithm was presented based on support in  association rules via  Apriori algorithm. A more effective reduction table can be obtained by deleting those rules with  less support according to  least support---minsup. The reduction feasibility of this algorithm was achieved by reducing the  given decision table.

Testing by UCI machine learning database and comparing this algorithm with least value reduction  algorithm indicate    the validity of RSVR algorithm.

Key Words: Association rules; Value reduction; Support; Apriori algorithm; Rough sets  468978-1-4244-5182-1/10/$26.00 c2010 IEEE  Definition 5 Let the data quantity of initial database be  N, the reduced data quantity be M, then the data reduction  ratio is:  (1 )*100%wE M N= ?

The data reduction ratio denotes decrease of  information in database.

3 ALGORITHM DESIGN  Based on Apriori algorithm, if the rule t? s is not  strong, then the extended rule t?p? s is not strong either.

The reduction table is obtained by deleting the rules whose  support is less than the least support minsup appointed by  user.

The description of the algorithm is as follows:  Input: decision table DT, the least support minsup  Output: rules set Rk  Step 1: Attribute reduction for decision table.

Step 2: Set k as 1.

Step 3: Calculate attribute support and rule support of  every attribute in candidate set Ck.

Step 4: Delete the rules from Ck if its rule support is  less than or equal to the least support minsup, transfer the  rule into rule sets Rk if its attribute support is equal to rule  support.

Step 5: Expand Ck into Ck+1. Scan Ck first, combine  every two items in Ck into candidate item with k+1  attributes and insert the candidate item into Ck+1. Then  check every item C in Ck+1, if an item is in k-subset of C  but not in Ck, delete C; if C is antipathic, delete C too.

Finally obtain Ck+1 and set k as k+1.

Step 6: Repeat steps 3 to 5 until Ck is empty.

Step 7: End.

4 EXAMPLE OF ALGORITHM  In the original decision Table 1, U is the concerned  universe, a, b, c, d are condition attributes, e is decision  attribute. The least support is minsup=1.

Reduce the decision table according to the algorithm  presented in the above section:  Attribute reduction. Only attribute a is e-omissible in  the original table, so delete attribute a to form a new    decision table.

Value reduction. Calculate attribute support and rule  support of every attribute to form candidate set C1 with 1  condition attribute. Check every item in the new table, if  rule support of an item is less than or equal to the least  support minsup, delete the item from C1; if attribute support  of an item is equal to rule support, transfer the item to rule  set R as determinate rule.

Form candidate set C2 with 2 condition attributes.

Combine two items that have the same decision attribute in  C1 to form new item with 2 condition attributes by  extending C1. Then deal with them according to the above  method.

Form candidate set C3 with 3 condition attributes.

When candidate set C4 with 4 condition attributes is  empty, stop the algorithm.

At last we get the reduced decision Table 2 yielding  the following rules:  (1)  (d,1)? (e,3)  (2)  (b,2) ?(c,2)? (e,3)  (3)  (b,1) ?(c,1)? (e,3)  (4)  (b,2) ? (c,1) ? (d,2)? (e,2)  (5)  (b,1) ? (c,2) ? (d,2)? (e,1)  Table 1  The original decision table  U a b c d e  1 1 2 1 1 3  2 1 2 2 1 3  3 1 2 1 2 2  4 1 1 1 1 3  5 2 1 2 2 1  6 2 2 2 1 3  7 2 2 2 2 3  8 2 1 2 1 3  9 2 1 1 1 3  10 3 2 2 2 3  11 3 2 1 2 2  12 3 1 1 2 3  13 3 1 1 1 3  14 3 2 2 1 3  15 3 1 2 2 1  16 3 1 2 1 3  Table 2  The reduced decision table    U b c d e Rule support  1 ? ? 1 3 9  2 2 2 ? 3 5  3 1 1 ? 3 4  4 2 1 2 2 2  5 1 2 2 1 2  5 COMPARISON OF ALGORITHMS  The 8 discrete datasets in UCI machine learning  database are used to test this algorithm and the least value  reduction algorithm is used for comparison. Let minsup=2  and minsup=3 in the two algorithms with the reduction  results listed in Table 3 and Table 4, where only rule  reduction ratio and data reduction ratio are listed because  the attribute reduction of the two algorithms is the same.

Generally speaking, the satisfactory values of Ea, Ei,  Ew are Ea>30%, Ei>60%, Ew>85% respectively.

2010 Chinese Control and Decision Conference 469  Table 3  Reduction results when minsup=2  Data sets Number  of  objects  Number  of  attribute  s  RSVR algorithm Least value reduction algorithm  Rule  reduction  ratio  Data  reduction  ratio  Runtime  (s)  Rule  reduction  ratio  Data  reduction  ratio  Runtime  (s)    *monk1 124 7 87.9% 93.7% 0.011 86.3% 92.9% 0.01  monk3 122 7 77% 88.5% 0.019 81.1% 90.6% 0.011  mux6 64 7 71.9% 83.5% 0.084 84.4% 93.3% 0.004  *led7 200 8 93.5% 96.1% 0.677 61.5% 68.6% 0.04  *parity5+5 100 11 81% 89.6% 0.072 64% 81.7% 0.018  iris-disc 100 5 88% 93.4% 0.002 94% 96.4% 0.003  Table 4  Reduction results when minsup=3  Data sets Number  of  objects  Number  of  attributes  RSVR algorithm Least value reduction algorithm  Rule  reduction  ratio  Data  reduction  ratio  Runtime  (s)  Rule  reduction  ratio  Data  reduction  ratio  Runtime  (s)  *monk1 124 7 89.5% 94.6% 0.009 86.3% 92.9% 0.01  *monk3 122 7 82% 91.3% 0.015 81.1% 90.6% 0.01  mux6 64 7 71.9% 83.5% 0.086 84.4% 93.3% 0.004  *led7 200 8 94% 96.4% 0.458 61.5% 68.6% 0.056  *parity5+5 100 11 88% 93.5% 0.062 64% 81.7% 0.018  iris-disc 100 5 89% 94% 0.002 94% 96.4% 0.003  470 2010 Chinese Control and Decision Conference  By comparing the two algorithms, we can see from  Table 3 and Table 4 that:  a) The rule reduction ratio and data reduction ratio of  6 datasets in RSVR algorithm are greater than those in  least value reduction when minsup=3 (denoted by * in    front of dataset). When minsup=2 there are 5 datasets only.

It shows that the reduction ratio increases with  improvement of minsup value. The reduction ratio of  RSVR algorithm must be greater than that of least value  reduction algorithm if the value of minsup is increased.

b) RSVR algorithm reserves all rules being useful for  users. The algorithm mainly focuses on applied system  instead of on reduction ratio.

c) The runtime of RSVR algorithm is commonly  longer than that of least value reduction algorithm,  especially when the quantity of objects and attributes is  large, because RSVR algorithm adopts times-iterative  method and complicated structure database.

6 CONCLUSION  This paper presents an RSVR algorithm based on  support in association rules mining via Apriori algorithm.

The more effective reduction table can be obtained by  deleting those rules with less support according to least  support minsup. The reduction feasibility of this algorithm  was achieved by reducing the given decision table.

Comparing this algorithm with least value reduction  algorithm reveals the characteristics and advantages of  RSVR. Testing by UCI machine learning database showed  the validity and feasibility of this algorithm.

