China Communications ? July 2016 108

Abstract:The rapid development of network technology and its evolution toward heteroge- neous networks has increased the demand to support automatic monitoring and the manage- ment of heterogeneous wireless communica- tion networks. This paper presents a multilevel pattern mining architecture to support auto- matic network management by discovering interesting patterns from telecom network monitoring data. This architecture leverages and combines existing frequent itemset dis- covery over data streams, association rule deduction, frequent sequential pattern mining, and frequent temporal pattern mining tech- niques while also making use of distributed processing platforms to achieve high-volume throughput.

Keywords: automatic network monitoring; sequential pattern mining; episode discovery module

I. INTRODUCTION  Recent advances in telecommunication net- work technologies and their evolution [1], [2], [3] to heterogeneous networks have intensified the need for more efficient network monitor-  Multilevel Pattern Mining Architecture for Automatic Network Monitoring in Heterogeneous Wireless Communication Networks Zhiguo Qu 1,2, John Keeney3, Sebastian Robitzsch2, Faisal Zaman2, Xiaojun Wang2  1 Jiangsu Engineering Center of Network Monitoring, Nanjing University of Information Science and Technology, Nanjing 210044, China 2 School of Electronic Engineering, Dublin City University, Dublin, Ireland 3 Network Management Lab, Athlone, Ireland  CLOUD COMPUTING AND DATA MINING    China Communications ? July 2016109  quirements, frequent temporal pattern mining can discover temporal patterns with greater accuracy. For each pattern discovery approach, parallel processing mechanisms (e.g., Hadoop [24] and Spark [25]) can be deployed to han- dle high volumes of data.

This paper presents an architecture to support the combination of tasks required to discover interesting patterns in high-volume high-dimensionality data. This architecture can efficiently achieve pattern mining for heterogeneous network monitoring data and discover potentially interesting patterns by leveraging and combining existing frequent itemset discovery over data stream, associa- tion rule deduction, frequent sequential pattern mining, and frequent temporal pattern mining techniques within distributed processing plat- forms.



II. CONTEXT OF THE ARCHITECTURE  The presented novel multi-level pattern min- ing architecture forms part of a larger auto- matic network management system [6]. To achieve this goal, a pattern discovery work package strives to extract sequential event pat- terns from network monitoring data, especially in the context of ever-increasing management data volumes, which is the subject of this pa- per.

2.1 Overall architecture  The overall project architecture is shown in Fig. 1. The architecture is made up of five main modules: a dimension reduction module (DRM), an episode discovery module (EDM), an episode classification module (ECM), a pat- tern matching and prediction module (PMM), and a recommender system module (RSM).

1. The DRM [24] greatly reduces the di- mensionality of the data and filters out most noisy events in the data stream to reduce the computation complexity of EDM and PMM.

2. The EDM discovers and identifies epi- sodes or sequential patterns from the dimen- sion-reduced event stream and then exports sequential pattern models into the Pattern  [20], [21], [22], [23].

When dealing with large volumes of data  with high dimensionality, providing admin- istrators with timely response is necessary.

Thus, real-time processing is the primary challenge for this work on pattern mining. To achieve this goal, this work proposes to mine multilevel patterns according to different time efficiencies by incorporating frequent itemset mining, frequent sequential pattern mining, and frequent temporal pattern mining. In simple terms, frequent itemsets are frequent combinations of unique items supported by transactions [7]; moreover, frequent sequential patterns not only comprises all the items but also comprises those in the correct sequence [17]. Furthermore, frequent temporal patterns extend frequent sequential patterns by also considering the time between elements and the sequence of elements [22]. Of these mining tasks, frequent itemset mining requires the least time, memory, and computing resources and is often sufficient for applications with relaxed pattern accuracy requirements or strict time requirements. In addition, frequent item- set mining can often be used as a precursor to other mining tasks while supporting faster analysis and summarization for some use cas- es and contexts. Frequent sequential pattern mining has high time and memory require- ments but achieves better pattern accuracy, thus supporting applications with high accura- cy requirements but with less constrained re- sponse times. With high time and memory re-  Fig.1  Overall system architecture    China Communications ? July 2016 110  accuracy/overhead trade-offs is flexible and benefi cial.

The EDM is made up of fi ve sub-modules: the Data Analyzer, Mining Controller, Data Miner, Combiner, and the Episode Analyzer.

These five sub-modules cooperate with each other to accomplish the task of the EDM by fulfi lling their functions in turn. A fl owchart of the EDM is presented in Fig. 3.

Model Library (PML), thus updating the set of discovered sequential patterns for the via the ECM.

3. The ECM models and classifi es the dis- covered sequential patterns. Feedback from the ECM can help the DRM and the EDM op- timize their parameter settings (such as trans- action length and window size) to improve performance. The identifi ed pattern models are output to the PML for the PMM.

4. The PMM scans the trace stream and matches the partial pattern models (from PML) to predict interesting patterns in the data just before they occur. These predictions will be exported to the RSM.

5. In response to PMM matches of pattern prediction rules, the RSM (retrieves and ranks suggestions to avert, alleviate, or mitigate against the effect of predicted issues on the ba- sis of historic actions, experience of network administrators, or best practice.

2.2 Multilevel pattern structure  As mentioned, we classify patterns, and asso- ciated techniques into discover patterns, into three successive types: frequent itemsets, fre- quent sequential patterns, and frequent tempo- ral patterns. This classifi cation also establishes three different levels of patterns where mining tasks can be cascaded, as shown in Fig. 2.

2.3 Architecture of EDM  The task of the EDM is to constantly mine frequent episodes (pattern candidates) from dimension-reduced data streams. To facilitate distributed processing and to target and min- imize time and memory consumption when mining frequent sequential patterns, the EDM adopts a three-level pattern structure. Frequent closed itemsets are fi rst discovered, then fre- quent closed sequential episodes are mined based on the frequent closed itemsets, and fi nally, frequent closed temporal episodes can be selectively mined. This three-level pattern mining approach aims to reduce the discov- ery of unnecessary frequent patterns thereby minimizing the overhead. Notably, outputting three different types of patterns with different  Fig.2  Multilevel pattern structure  Fig.3  Flowchart of the EDM module  Data Plane  Control Plane  Pattern Model Library  Data Miner  Data Analyser  Dimension Reduced Data-Stream  Episode Classifier  Dimension Reduction  Mining Controller  Combiner  Episode Analyser    China Communications ? July 2016111

III. EDM  1) Data Analyzer: As introduced, the Data Analyzer automatically determines a proper setting of parameters for the EDM module using statistics and feedback from DRM and ECM. Specifically, the Data Analyzer analyzes the dimension-reduced data by collecting and calculating essential characteristics of the data, such as average frequency of each event and the distribution of events. On the basis of the analysis, different data sub-streams of dimension-reduced for different contexts or use cases will be separated, and in some cases, data sub-streams shall be further divided into subgroups for parallel processing. Correspond- ingly, decisions about parameters for these data sub-streams will be automatically and ac- curately estimated and output after integrating feedback from DRM and ECM. Different sub- streams may have different parameter settings, but different time sliding windows within the same sub-stream or sub-group share the same parameter setting. Splitting the data stream into different sub-streams is mainly based on the following considerations: First, different use cases usually have different episodes with different contexts, and the episodes that cross several use cases are usually meaningless.

Thus, reorganizing the data according to dif- ferent use cases or contexts before the process of episode mining is better. Second, different sub-streams of the data that originate from dif- ferent use cases generally have different data characteristics/features; thus, different opti- mized algorithm configurations may exist for mining episodes from different sub-streams.

For example, consider two distinct telecom use cases, one where patterns related to indi- vidual mobile users are sought and another that focuses on patterns related to individual fixed cells in a mobile network. Clearly, when separating the data stream into individual sub- streams, a sensible approach is to partition the data in different ways, one where the data is partitioned by the user regardless of which cell they use and the other where the data are par- titioned by cell regardless of users in the cell.

1. Data Analyzer: It determines appropriate configuration parameters for the data mining tasks (e.g., Min-Sup, window size, transaction length) and also determines if and how the stream could be split into sub-streams for par- allel processing. For episode discovery in the EDM, automatic determination of algorithm settings is one of the significant challenges.

2. Mining Controller: After obtaining con- figuration information from the Data Analyzer, the Mining Controller splits the dimension-re- duced trace stream into sub-streams or sub- groups and then assigns computation resources according to the configuration settings, such as I/O, memory and CPU, to each sub-stream or sub-group of the data. The Mining Control- ler then recursively calls the Data Miner to process the datasets split into different sliding window views over sub-streams to discover frequent episodes based on parameter settings provided by the Data Analyzer.

3. Data Miner: As the core sub-module of the EDM, the Data Miner continuously dis- covers frequent closed itemsets from each slid- ing window view according to its parameter settings. In this sub-module, the NewMoment [12] algorithm implementation can be selected to handle this task.

4. Combiner: After discovering new fre- quent closed itemsets from different sliding windows, the Combiner combines these fre- quent closed itemsets to form an overall set of frequent closed itemsets with a uniform format for each sub-stream. Then, on the basis of these discovered frequent closed itemsets and their corresponding projected databases, both PrefixSpan [17] and HTPM [22] can be selected to find the frequent closed sequential episodes or frequent closed temporal episodes for each sub-stream.

5. Episode Analyzer: This sub-module adopts an association rule mining approach to analyze discovered frequent episodes to filter out most episodes that are not relevant for the given use case. The identified episodes will be outputted and stored into the PML as pattern models.

China Communications ? July 2016 112  Specifically, after receiving FCITs and bit vectors of events and itemsets of each sub- stream and sub-group, the Combiner fi rst com- bines the FCITs of sub-groups into an overall FCIT for each sub-stream. For this specific purpose, the algorithm [26] presents a set of FCIT combination policies and suitable ap- proaches. Then, for each frequent closed item- set, a corresponding projected-database is built by fi ltering out all sub-sequences/transactions  The Data Analyzer comprises seven com- ponents: Statistic Information Collector, Feed- back Parser, Use Case Parser, Statistic Parser, DRM Parser, ECM Parser, and Parameter Setting Controller. A fl owchart of the Data An- alyzer is shown in Fig. 4.

2) Mining Controller: After receiving in- structions from the Data Analyzer, the Mining Controller allocates computation resources to carry out the work specifi ed by the Data Ana- lyzer. Specifi cally, after obtaining the param- eter settings from the Data Analyzer, the Min- ing Controller starts to split the reduced data into sub-streams (if necessary, the sub-streams could be further divided into sub-groups in the Mining Controller) and then assigns computa- tion resources, such as I/O, memory, and CPU, for these sub-streams or even sub-groups according to their corresponding parameter settings. Afterwards, the Mining Controller will recursively call the Data Miner to process the data sub-streams or sub-groups to discover episodes according to the parameters set by the Data Analyzer.

The Mining Controller consists of three components, namely, Stream Splitter, Confi g- uration Controller, and Source Controller. A fl owchart of the Mining Controller is shown in Fig. 5.

Data Miner consists of four components, namely, Event Bit Representation, Itemset Bit Representation, Frequent Closed Itemset Dis- covery, and Occurrence Recorder. A fl owchart of the Data Miner is shown in Fig. 6.

4) Combiner: As mentioned above, the fre- quent episode mining process consists of two phases. The fi rst phase is to discover frequent closed itemsets, and it is handled by the Data Miner. The second phase is to mine frequent closed episodes from the frequent closed item- sets found in the fi rst phase, and it is handled by the Combiner.

Combiner has three components, namely, Frequent Closed Itemset Combiner, the Pro- jected Database Builder, and the Frequent Closed Episode Miner. A flowchart of the Combiner is shown in Fig. 7.

Fig.4  Flowchart of Data Analyzer  Fig.5  Flowchart of Mining Controller    China Communications ? July 2016113  in which the particular itemset is not contained and the events with the types other than those present in the particular itemset. Finally, the Combiner recursively invokes a frequent se- quential or temporal mining algorithm, such as Prefi xSpan [17] or HTPM [22], to discover frequent closed episodes in the projected da- tabase for each frequent closed itemset. Only frequent closed episodes that consist of all the different event types in the corresponding itemset and have suffi ciently large support val- ues in the corresponding projected databases will be outputted.

5) Episode Analyzer: The Episode Analyzer aims to deduce association rules from the dis- covered episodes. Most of the meaningless ep- isodes will be fi ltered out in this sub-module.

Specifically, after receiving frequent closed episodes from the Combiner, the Confidence and Lift of each episode are calculated in terms of the support values of the antecedent and consequent parts of the episode sequence.

(Confi dence denotes the conditional probabil- ity of the consequent actually occurring after the antecedent events occurred, which indi- cates whether useful association rules can be derived from the discovered patterns. Lift rep- resents the degree to which one event or event sequence occurrence predicts another event or event sequence occurrence.) Then, discovered episodes are identifi ed according to the thresh- olds of Confi dence and Lift, and the episodes with values less than the thresholds will be fi ltered out; the default thresholds are 60% for Confidence and 100% for Lift. The frequent closed episodes with suffi cient Confi dence and Lift values are exported into the PML, while a notifi cation will be sent to the ECM for further processing. The Episode Analyzer consists of two components, namely, the Confi dence Cal- culator and the Lift Calculator. A fl owchart of the Episode Analyzer is shown in Fig. 8.



IV. EXPERIMENTAL EVALUATIONS  To prove the feasibility of the EDM module, three classic algorithms, namely, NewMo- ment, Prefi xSpan, and HTPM, are implement-  Fig.6  Flowchart of Data Miner  Fig.7  Flowchart of Combiner    China Communications ? July 2016 114  continuous data stream that contains complex context with unpredictable unbounded vol- umes. A multi-level pattern mining solution  ed in Java. All the experiments are performed on a 2.5 GHz Intel Core i5-3210M PC with 8 GB of main memory and running Micro- soft Windows 7. Each approach was evalu- ated using a simulated dataset called aPG_ Sim, which is generated by the open-source OpenMSC real-time emulator [27]. On the basis of a set of confi gurable parameters and predefined message sequences, correlations, and temporal pattern parameters, OpenMSC generates realistic real-time pattern traces ob- fuscated with a large corpus of random noise events. The PG_Sim dataset consists of a total of 100K events, including 500 + 12 unique types of events, incorporating 500 noise event types {100,?..,599} and 12 information event types. Three candidate sequence patterns are manually pre-defi ned for inclusion.

The performance of the NewMoment, PrefixSpan, and HTPM algorithms is tested by running them on the test dataset PG_Sim.

Results show that all the predefined patterns are found by the NewMoment, PrefixSpan, and HTPM algorithms with different pattern forms. The time taken for the algorithms to complete their search given different Min-Sup values is shown in Figs. 9 and 10. The Min- Sup value is incrementally increased from 0.35 (35%) to 0.8 (80%), while transaction length is fi xed at 20.

Figs. 9 and 10 clearly show signifi cant dif- ferences in the time required for NewMoment, Prefi xSpan, and HTPM to complete. The time requirements for HTPM are much larger than that of Prefi xSpan, while Prefi xSpan is slower than NewMoment. With the decreasing Min- Sup, this difference increases substantially.

Fig. 9 also shows that for relatively simple streams with a small number of event types and patterns, where streams can be adequately split into smaller transaction sets (in this case, 100K events), NewMoment can process the stream online in a near-real-time manner.



