

Abstract?Recent studies have shown that associative classifica- tion is a promising classification method. However, when the min- imum support is too low, associative classification often generates a large set of rules, which results in two main challenges: (1) how to select an appropriate subset of rules to build a classifier; and (2) how to select a best rule for classifying new instances. In this paper, we propose a new associative classification approach called ACMA (Associative Classification based on Mutually Associated pattern). It is distinguished from other associative classification algorithms in two aspects. First, in order to reduce the number of rules, ACMA selects mutually associated patterns to generate rules, and also exploits information entropy of items to reduce research space. Second, ACMA employs a new rule ranking method which considers mutual association between the itemset and the predictive class. Our experiments on six UCI data sets show that ACMA approach is an effective classification technique, and has better average classification accuracy in comparison with CBA.



I. INTRODUCTION  As one of the most essential tasks of data mining, classifica- tion has been extensively studied. Various types of classifica- tion algorithms have been proposed, such as decision trees [1], KNN [2], [3], associative classification [4], [5].

In recent years, many associative classification algorithms have been proposed, such as CBA [4], CMAR [5], CPAR [6], HARMONY [7]. These algorithms show that associative clas- sification achieves higher accuracy than traditional classifi- cation approaches such as C4.5 [1]. These algorithms are slightly different in discovering frequent class association rules (CARs). They use the minimum support threshold (minsup) to control the number of rules generated. However, if minsup is set high, those possible rules that cannot satisfy minsup but with high confidences will not be included in classifier. If minsup is set too low, most algorithms generate a large set of CARs, which results in two main challenges:(1) how to select an appropriate subset of rules to build a classifier; and (2) how to select a suitable rule to classify new instances.

To address the first problem, most associative classification algorithms employ some rule pruning techniques to delete redundant and noisy information. Rule pruning techniques include the pessimistic error rate based pruning method [4], database coverage [4], ?2 testing [5].

To address the second problem, some algorithms like CBA use the best rule to classify new instances. These algorithms rank rules in terms of confidence and support. When several rules have the same confidences and supports, they randomly choose one of the rules, which may degrade accuracy. Other  algorithms like CMAR classify new instances based on multi- ple rules. They divide the rules into groups according to class labels. The difficulty of this method is how to measure the combined effect of each group.

In this paper, we propose a new algorithm namely ACMA (Associative Classification based on Mutually Associated pat- tern). ACMA employs the following techniques to generate a small set of rules: (1) ACMA deletes these items with high information entropy; (2) ACMA finds not only frequent but also mutually associated itemsets to generate rules. We use all- confidence proposed by Omiecinski [8] to measure the degree of mutual association in an itemset.

ACMA ranks the rules generated according to confidence, mutual association between the itemset and the predictive class, the degree of mutual association in an itemset, and support. it uses the best rules to predict the class label of new instances.

The remaining of the paper is arranged as follows. Section II reviews the general ideas of associative classification. Section III describes the rule generation process and discusses how to classify a new data object. The experimental results on classification accuracy and the performance study on efficiency are presented in section IV. We conclude the study in section V.



II. ASSOCIATIVE CLASSIFICATION  In associative classification [4], the training data set T has m distinct attributes A1, A2, . . . , Am and a list of classes c1, c2, . . . , cn. An attribute can be categorical or continuous.

For a categorical attribute, all the possible values are mapped to a set of consecutive positive integers. For a continuous attribute, its value range is discretized into intervals, and the intervals are also mapped to consecutive positive integers.

In general, we call each attribute-value pair an item. An itemset X = ai1 , . . . , aik is a set of values of different attributes. A data object is said to match itemset X = ai1 , . . . , aik , if and only if for (1?j?k), the object has value aij in attribute Aij . The number of data objects in T matching itemset X is called the support of X , denoted as sup(X).

Information entropy of item X, denoted as entropy(X) is defined as follows:  entropy(X) = ? 1 log2 k  k? i=1  p(Ci|X) log2 p(Ci|X) (1)     Algorithm 1 ACMA (D) Input: Training data set D; Minimum Support thresh- old(minsup); Minimum all-confidence threshold(minallconf); Maximum entropy threshold (Maxentropy).

Output: A set of rules R.

1: C=?; R=?; L=?; 2: C1 ? init? pass(T ); 3: ruleSelection(C1, L1, R); 4: for (k = 2;Lk?1 6= ?; k ++) do 5: Ck ? candidateGen(Lk?1); 6: supportCount(Ck); 7: ruleSelection(Ck, Lk, R); 8: end for 9: Sort(R);  10: return R;  where k is the number of classes, and p(Ci|X)is the proba- bility that an object matching X belongs to Ci.

All-confidence [8] of itemset X = ai1 , . . . , aik , denoted as allconf(X), is defined as follows:  allconf(X) = sup(X)  max(sup(ai1), . . . , sup(aik)) (2)  It represents the minimum confidence of all association rules extracted from an itemset. We use all-confidence to measure the degree of mutual association in an itemset.

We define all-confidence of a rule r:X ? ci as follows:  allconf(r) = sup(X ? ci)  max(sup(X), sup(ci)) (3)  It represents the mutual association between itemset X and class ci, which we use to measure the quality of rules.

Given a training data set T , let c is a class label. A class association rule(CAR) is of the form: condset ? c, where condset is an itemset. The support count of the condset (called condsup) is the number of objects in T that match the condset.

The support count of the rule (called rulesup) is the number of objects in T that match the condset and belong to c.

The support of a rule is (rulesup/|T |) ? 100%, where |T | is the size of the dataset, and the confidence of a rule is (rulesup/condsup) ? 100%.



III. ACMA: ASSOCIATIVE CLASSIFICATION BASED ON MUTUALLY ASSOCIATED PATTERN  The algorithm proposed in this paper consists of two phases: (1) rules generation which is based on algorithm Apriori [9] for finding frequent and mutually associated patterns; and (2) a classifier builder, which includes rules pruning and rules sorting. The algorithm ACMA is presented in Algorithm 1 and 2. Subsection III-A discusses rule generation, subsection III-B states the pruning technics, subsection III-C describes rules ranking, an example is given to demonstrate the process of ACMA algorithm in section III-D.

Algorithm 2 ruleSelection(Ck, Lk, R) 1: for all X ? Ck do 2: compute allconf(X); 3: if condsup(X)/|T | ? minsup and allconf(X) ?  minallconf then 4: calculates the confidence of all rules with X as  condition, and selects the rule with the largest confidence:X ? ci;  5: if conf(X ? ci) > sup(ci) then 6: find all general rules of (X ? ci) in R 7: if the confidence of each general rules of (X ?  ci) < the confidence of ( X ? ci) then 8: push X ? ci into R; 9: end if  10: end if 11: push X into Lk; 12: end if 13: end for  A. Rules Generation  Rules generation of ACMA is based on algorithm Apriori.

It scans training data set multiple passes. Line 2 in algorithm 1 represents the first pass of the algorithm. It counts the condsup and rulesup of individual item at the same time.

ACMA will delete such items with high information entropy.

The rationale is that these items carrying little information for classification when its information entropy is approximately 1. Then the Subroutine ruleSelection is executed, also done in each subsequent pass.

For each subsequent pass, the algorithm performs three major operations. First, function candidateGen which is similar to the function apriori-gen in algorithm Apriori [9] is executed.

Second, function supportCount scans the data set and counts the condsup and rulesup of the candidates in Ck. Finally, Subroutine ruleSelection is executed.

In Subroutine ruleSelection, ACMA discovers frequent and mutually associated itemsets which pass minimum support and all-confidence threshold. Once an frequent and mutually asso- ciated itemset has been identified, ACMA algorithm calculates the confidence of all rules with that itemsets as condition. Only the rule with the largest confidence is considered as a class association rule.

B. Rules Pruning  The number of class association rules can be huge. To make the classification effective and also efficient, we need to prune rules to delete redundant and noisy information.

Definition 1: A rule R1 : P ? ci is said a general rule w.r.t. rule R2 : P  ? ? cj , if and only if P ? P ? .

ACMA employs the following methods for rule pruning.

First, using general rule to pruning more specific rule. Given  two rules R1 and R2, where R1 is a general rule w.r.t. R2.

ACMA prunes R2 if the confidence of R1 is greater than that of R2. Thus more specific rules with low confidence should be pruned.

TABLE I AN EXAMPLE DATA SET  Patients Headache Pain Temperature Flu x1 yes yes normal no x2 yes yes high yes x3 yes yes very high yes x4 no no high no x5 no yes normal no x6 no no high no x7 no yes very high yes x8 no no high yes x9 no yes very high no x10 no no high no  Second, selecting only positively correlated rules. For each rule R:P ? ci, we prune it if the confidence of the rule R is lower than the support of class ci.

The pruning strategies is pursued when the class association rule is inserted into the set of rules (line 5-10 in algorithm 2).

C. Rules Ranking  To select the best rule for classifying new instances, most associative classification methods usually rank rules firstly.

Rule ranking plays an important role in associative classifi- cation. CBA and CMAR rank the rules mainly according to confidence and support. When several rules have the same confidences and supports, CBA and CMAR randomly choose one of the rules, which may degrade accuracy. To addressing this issue, ACMA ranks rules according to not only confidence but also mutual association between itemsets and predictive class. A total order on the generated rules is defined as follows:  Definition 2: Given two rules, ri and rj , ri precedes rj if: 1. the confidence of ri is greater than that of rj , or 2. their confidences are the same, but the all-confidence of  ri is greater than that of rj , or 3. the confidence and all-confidence of ri and rj are the  same, but all-confidence of condition of ri is greater than that of rj , or 4. the confidence, all-confidence of ri and rj and all-  confidence of condition of ri and rj are the same, but the support of ri is greater than that of rj , or 5. all above criteria are identical for ri and rj , but ri has  fewer conditions in its left hand side than that of rj .

After rule ranking, the classifier is built. ACMA finds the  first rule matching the new instance to predict its class label.

D. An example  Consider the data set shown in Table I. The data set describes a set of patients in terms of four attributes:headache, pain, temperature, and flu. Suppose that attribute flu is the decision attribute and others are condition attributes. We set minsup to 20%, minAllconf to 50%, and Maxentropy to 0.99.

For the first pass, we will get frequent single items, the result is shown in table II.

Because of entropy(Pain=yes)=1, we delete this item. Ac- cording to Equation 2, all-confidence values of all single item are 1.

TABLE II FREQUENT SINGLE ITEMS  itemset X sup(X) sup(X? sup(X? entropy(X) allconf(X) Flu=no) Flu=yes)  Headache=yes 3 1 2 0.9183 1 Headache=no 7 5 2 0.8631 1  Pain=yes 6 3 3 1.0000 1 Pain=no 4 3 1 0.8113 1  Temperature=normal 2 2 0 0.0000 1 Temperature=high 5 3 2 0.9710 1  Temperature=very high 3 1 2 0.9183 1  TABLE III RULES GENERATED FROM FREQUENT 1-ITEMSETS  rules (X ? Ci) confidence allconf(X allconf(X) ? Ci)  Headache=yes?Flu=yes 67.67% 0.5 1 Headache=no?Flu=no 71.43% 0.71 1  Pain=no?Flu=no 75% 0.5 1 Temprature=normal?Flu=no 100% 0.33 1 Temperature=high?Flu=no 60% 0.5 1  Temperature= Very high?Flu=yes 67.67% 0.5 1  Once an item has been identified as frequent and mutually associated pattern, the ACMA algorithm generates all rules with that item as condition, and we only select the rule with the largest confidence. So ACMA generates rules as shown in table III  Because the confidence of Temprature=normal?Flu=no is 100%, item Temprature=normal is not extended at next pass. Table IV shows frequent 2-itemsets. The all-confidence of itemset Headache=no and Temperature= very high is not greater than the minallconf, so it is not a mutually associated pattern, and we delete it from itemset. The other three itemsets will generate three rules as shown in table V.

Because the confidence of the rule:Pain=no?Flu=no is 75%, and the confidence of the rule:Headache=no and Pain=no?Flu=no is also 75%, itemset Headache=no and Pain=no is specific of itemset Pain=no, we prune the rule:Headache=no and Pain=no?Flu=no. So does the rule:Pain=no and Temperature=high?Flu=no.

In the last pass, we will generate one rule:Headache=no and Pain=no and Temprature=high?Flu=no,confidence:75%,whose confidence  TABLE IV FREQUENT 2-ITEMSETS  itemset X sup(X) sup(X? sup(X? allconf(X) Flu=no) Flu=yes)  Headache=no and Pain=no 4 3 1 0.57  Headache=no and Temperature=high 4 3 1 0.57 Headache=no and  Temperature= very high 2 1 1 0.29 Pain=no and  Temperature=high 4 3 1 0.80     TABLE V RULES GENERATED FROM FREQUENT AND MUTUALLY ASSOCIATED  2-ITEMSETS  rules (X ? Ci) confidence allconf(X allconf(X) ? Ci)  Headache=no and Pain=no?Flu=no 75% 0.5 0.57 Headache=no and Temperature=high?Flu=no 75% 0.5 0.57  Pain=no and Temperature=high?Flu=no 75% 0.5 0.8  TABLE VI CLASSIFIER  rules (X ? Ci) confidence allconf(X allconf(X) sup(X) ? Ci)  Temprature=normal?Flu=no 100% 0.33 1.00 2 Pain=no?Flu=no 75% 0.50 1.00 3  Headache=no and Temperature=high ?Flu=no 75% 0.50 0.57 3  Headache=no?Flu=no 71.43% 0.71 1.00 5 Headache=yes?Flu=yes 67.67% 0.50 1.00 2  Temperature= Very high?Flu=yes 67.67% 0.50 1.00 2 Temperature=high?Flu=no 60% 0.50 1.00 3  is also not greater than that of Pain=no?Flu=no, so it is not inserted to classifier. We sort the generated rules according to Difinition 2. The final classifier is shown in tabel VI.



IV. EXPERIMENTS  To evaluate the accuracy and efficiency of ACMA, we have performed an extensive performance study.

All the experiments are performed on a 3GHz Pentium 4 PC with 1G main memory, running Microsoft Windows XP.

We test 6 data sets from UCI Machine Learning Repository.

The characteristics of 6 data sets are summarized in table

VII. A 10-fold cross validation is performed. The results are given as average of the accuracy, number of rules and runtime obtained for each fold. All reports of the runtime only include the training time.

Table VIII shows the accuracy comparison of algorithm CBA and ACMA on six data sets. CBA experiments are con- ducted using implementation version provided by the authors of [10]. The minimum support threshold of both algorithms is set to 0.05%. The minimum confidence threshold of CBA algorithm is set to 50%, and ACMA do not need to set minimum confidence threshold. The minimum all-confidence, information entropy threshold of ACMA is set to 10%, 0.95,  TABLE VII UCI DATA SET CHARACTERISTICS  dataset ]instances ]attibutes ]classes Balance 625 5 3 Breast 699 9 2 Led7 3200 7 10  lymph 148 18 4 mushroom 8124 23 2  votes 435 17 2  TABLE VIII ACCURACY COMPARISON ON 5 DATA SET  dataset CBA ACMA balance 72.68 74.52 Breast 96.14 96.09 Led7 69.43 71.81  lymph 82.86 87.14 mushroom 99.98 99.99  votes 95.43 94.88 Average 86.09 87.41  TABLE IX IMPACT OF MUTUALLY ASSOCIATED PATTERN ON ACCURACY AND  EFFICIENCY  data set accuracy ]rules runtime w/o allconf ACMA w/o allconf ACMA w/o allconf ACMA  balance 70.97 74.52 407 106 2.23 1.04 Breast 95.65 96.09 1651 357 18.28 3.73 votes 94.19 94.88 15010 343 1350.97 25.82 Led7 71.56 71.81 448 313 15.91 6.5  mushroom 99.95 99.99 4972 389 1338.51 80.76 average 86.46 87.46 4497.60 301.60 545.18 23.57  respectively. The results show that ACMA outperforms CBA in terms of average accuracy.

Table IX shows the impact of mutually associated pattern on accuracy and efficiency. When the mutually associated pattern is employed to generate rules, higher accuracy is achieved, the number of rules are reduced greatly.

ACMA deletes these single items whose information en- tropy is larger than maximum entropy throeshold (Maxen- tropy). We develop two groups of experiments. We set Maxen- tropy to 0.95 in one group of experiments, and 1.0 in the other which means ACMA do not deletes single items according to its information entropy. Table X shows that after deleting these items with high information entropy, the classifier ACMA generates is much smaller and more efficient.

Table XI shows the impact of pruning rules. We can see that after pruning, the number of rules is much smaller, the accuracy is slightly higher and runtime is much faster. The results show that the pruning techniques ACMA employs is effective and efficiency.



V. CONCLUSION  In this paper, we proposed a new associative classifica- tion method, ACMA, i.e., Associative Classification based  TABLE X IMPACT OF PRUNING ACCORDING TO ENTROPY  data set accuracy ]rules runtime 0.95 1.0 0.95 1.0 0.95 1.0  balance 74.52 74.52 106 106 1.04 1.22 breast 96.09 96.23 357 386 3.73 4.41 Votes 94.88 94.42 343 7170 25.82 1703.83 led7 71.81 71.66 313 503 6.5 11.47  mushroom 99.99 99.98 389 4177 80.76 6688.37 average 87.46 87.36 301.60 2468.40 23.57 1681.86     TABLE XI IMPACT OF PRUNING ACCORDING TO CORRELATION  data set accuracy ]rules runtime w/o pru pru w/o pru pru w/o pru pru  balance 74.52 74.52 106 165 1.04 1.02 breast 96.09 96.09 357 508 3.73 3.97 Votes 94.88 94.65 343 6625 25.82 214.6 led7 71.81 71.81 313 356 6.5 6.23  mushroom 99.99 99.99 389 1508 80.76 87.95 average 87.46 87.41 301.60 1832.40 23.57 62.75  on Mutually Associated pattern. ACMA uses all-confidence to measure the degree of mutual association in an itemset and selects these itemsets with high all-confidence value to generate rules. Furthermore, ACMA selects the best rule not only with high confidence value but also with high mutual association to predict new instances. Therefore, ACMA builds an efficient and accurate classifier. Our experiments on 6 databases from UCI machine learning database repository show that ACMA has better average classification accuracy in comparison with CBA.

