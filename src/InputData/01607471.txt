Effective Data Mining by Integrating Genetic Algorithm into the Data Preprocessing Phase

Abstract  Dividing a data set into a training set and a test set is a fun- damental component in the pre-processing phase of data mining (DM). Effectively, the choice of the training set is an important factor in deriving good classification rules.

Traditional approach for association rules mining divides the dataset into training set and test set based on statisti- cal methods. In this paper, we highlight the weaknesses of the existing approach and hence propose a new methodol- ogy that employs genetic algorithm (GA) in the process. In our approach, the original dataset is divided into sample and validation sets. Then, GA is used to find an appropriate split of the sample set into training and test sets. We demon- strate through experiments that using the obtained training set as the input to an association rules mining algorithm generates high accuracy classification rules. The rules are tested on the validation set for accuracy. The results are very satisfactory; they demonstrate the applicability and ef- fectiveness of our approach.

Keywords: pre-processing, data mining, classification, as- sociation, genetic algorithms, clustering, data-splitting.

1 Introduction  Data mining is generally defined as the process of ex- tracting previously unknown knowledge from a given data- base. Classification rules mining and association rules min- ing are two important data mining techniques. The former aims to discover a set of rules in the database to form an accurate classifier; and the latter extracts in the database all rules that satisfy some minimum support and minimum con- fidence constraints [8]. The main target of the association rules mining process is to obtain the best set of rules. We focus on a special subset of association rules whose con- sequent (right-hand-side) is restricted to the classification  class attribute. This subset is often referred to as class as- sociation rules (CARs). Associative classification has high classification accuracy and strong flexibility at handling un- structured data [9]. Also, exponential time complexity of constructing an optimal classifier is a strong factor to resort to heuristics based solutions to save time.

Classification algorithms like Neural Network, SOM, etc, have proven to be very accurate in classifying datasets [13, 14]. The inherent problems with these clas- sifiers is that the rules are implicit, and therefore hard to decipher. It is easier to derive classification rules using rule mining because the rules are explicit. In our research, our aim is to benefit from the advantages of both approaches.

We also want to exploit the power of good classification al- gorithms in deriving good CARs using rule mining.

To produce accurate classification rules using the ARM technique in the mining stage of the DM process, selecting an appropriate training set in the pre-processing stage is an important factor. The traditional methods for data-splitting split the dataset into a training set and a test set to represent the unknown sample population and the unknown valida- tion sets of the real-world. These methods try to simulate the sample population using different sample training and test sets based on different statistical techniques. The de- rived training set(s) is used as the input to the ARM tech- nique to derive the classification rules and the test set(s) is used to test the accuracy of the rules before validating the rules using a validation set. The union of the derived classi- fication rules (using different splits) with good classification accuracy forms the resulting rule set. There are two flaws in using this approach. 1) In any DM process, the role of the test set is to detect over-training from the trained model.

Unfortunately, the traditional data-splitting methods do not ensure that the trained classification model (using the ARM technique) built from their training sets fits their test sets with least noise. 2) It is still unclear how well these rules will perform on an ?unknown? validation set [1].

As opposed to the traditional methods, where the train- ing set is used to simulate the population, in this paper, the entire dataset should represent the population as closely as possible. Different samples can be simulated to retain the generality of the resulting DM model by splitting the dataset into: 1) sample set and 2) validation set. Then each ob- tained sample set should be divided into training and test sets efficiently so that: i) the training set represents the true relationships in the sample set as closely as possible without over-fitting; and ii) the test set detects overtraining from the trained model as closely as possible. As a result, the prob- lem of deriving the best training and test sets from a given sample set is addressed in this paper. The method proposed is based on searching for the split which maximizes the clas- sification accuracy of the trained model (when tested with the splits) and at the same time minimizes the noise per- centage (between the splits). The search process is carried out by GA, which is well known as effective optimization technique.

We first split the original dataset into sample set and val- idation set using statistical strategies [2, 3, 4]. We then ap- ply our GA-based method to the sample set to appropriately split it into training and test sets. Then, the training set is used as the input to an association rules mining model to derive CARs. The accuracy of the CARs (tested) on the unknown validation set is satisfactory. We tested and vali- dated our approach on breast cancer dataset. The reported results are promising in demonstrating the applicability and effectiveness of the proposed method.

The rest of the paper is organized as follows. Section 2 describes the traditional methods for the data-splitting process. Section 3 discusses how GA can be used for this problem. In Section 4, we give the experimental results and compare the results of random sampling with the results from the GA method. Section 5 is summary and conclu- sions.

2 Related Work  In this section, we discuss three currently available meth- ods to address the data-splitting process, namely: sampling, cross validation, and bootstrap.

Simple random sampling refers to the method of select- ing members (or instances) from a dataset (population) such that every possible instance from the sample has an equal chance of being selected to represent the sample popula- tion. For classification models, simple random sampling does not guarantee that every class from the dataset is prop- erly represented in both training and test sets [2]. Stratified random sampling [2] refers to a sampling method in which the dataset is divided into subgroups called stratas so that each instance in the dataset belongs to only one strata. The objective is to form a strata so that the instances of inter-  est from the dataset in each strata are similar. For exam- ple, each strata could contain the instances representing the same class.

A general way to mitigate any bias arising from the par- ticular sample chosen for training or testing is to repeat the whole process, i.e., train and test several iterations with different random samples using any of the sampling meth- ods [16]. The overall error rate in the performance of the DM model is the average of the error rates on different iter- ations.

An alternative data-splitting method that draws the train- ing and test sets from the original dataset is the cross valida- tion method [2, 3]. The advantage of this method is that the greatest possible amount of data is used for training in each iteration, which increases the chance of generating an accu- rate DM model. The disadvantage is that it is impossible to guarantee that each class is properly represented in the test set; so it is unsuitable for building classification models.

In all these methods, samples were drawn without re- placement i.e., an instance, once selected, could not be se- lected again. Sampling with replacement is a statistical method which can use the same instance twice. Bootstrap is an estimation method that uses sampling with replacement to form the training set. Given a dataset D containing K in- stances, generate the training set D  ? by randomly selecting  K ?  instances from D with replacement. The instances not in the training set forms the test set [1, 2, 3]. The 0.632 boot- strap is the most popular estimation technique. The whole bootstrap method is repeated several times with different re- placement samples for better results.

Current methods for the data-splitting task are general statistical methods for data-splitting. These methods are applicable to a wide class of problems. However, these methods are not robust and are based on random processes.

Hence, the splits derived from these methods may not be the best splits for a special class of problems such as the ones considered in this paper. In fact, the choice of TRS is an important factor in deriving good CARs using the ARM technique. Therefore, these methods must be modified to obtain a split of the dataset in a manner which guarantees that the splits cover the classification relationships without over-fitting or developing a more suitable approach that can effectively handle the splitting process.

3 Data Splitting Using Genetic Algorithms  GA?s were introduced by Holland in 1962 [5], originally intended as a general model of adaptive processes, but sub- sequently widely opted as optimizers [6]. Basically, GA can be used for solving problems for which it is possible to construct an objective function (also known as fitness func- tion) to estimate how a given representative (solution) fits the considered environment (problem).

In general, the main motivation for using GA?s in any DM process is that they perform a global search and cope better with interaction than the greedy rule induction algo- rithms often used in DM. In data pre-processing, GAs have been used for handling one of the key problems, namely the attribute selection problem [7].

Using the attribute selection problem as the basic idea, we have explored the possibility of using GA for the in- stances selection problem in the area of data pre-processing.

We discuss how GA can be used for one of the key problems in data preparation, namely to divide a data set into training and test sets. Here, it is also important to emphasize that to the best of our knowledge, this is the first time a problem of this nature is being explored using GA.

The goal is to find the best split of the sample set into training and test set such that: 1) the CARM model (i.e., classification model built from the ARM technique) using the sample set fits the training and test sets with least noise; 2) Classification accuracy of the derived CARs using such a training set on an unknown validation set is high. A binary encoded GA is used to search for the best training and test sets that can be obtained. Hence, each chromosome in the population is a possible split of the dataset. The length of the chromosome is the number of instances in the data set.

For example, if there are 10 instances in the dataset, then ?1001100111? is a possible chromosome, where the genes ?1?s? and ?0?s? represent instances in the training and test sets, respectively.

The goal is to split the sample set so that the classi- fication relationships learned from the sample set fits the training and test sets with minimum noise. Therefore, the first task is to find the classification algorithm that performs the best on the entire sample set. So, prior to running the GA, the classification algorithms namely: Support Vector Method, Decision Trees, Neural Network, Naive Bayes, Nearest Neighbor Method, and Kernel Density are run on the existing sample set. The algorithm that gives the high- est accuracy on the correctly classified instances (verified against the sample set) is chosen. Although the neural net- work is the best classifier in most cases, the time complexity of a neural network is very high. So, using this classifier for the data-split problem would increase the time complexity of the entire task significantly. Hence, this classifier is elim- inated from future discussions of the data-splitting problem.

Table 1. Confusion Matrix for the Classifica- tion Model  Actual Class Predicted Class a b  a Ta Fa b Fb Tb  The predictive accuracy of the classification model is obtained by computing the confidence factor (CF) of the model. This confidence factor, denoted CFSample, is com- puted with a confusion matrix [16] as illustrated in Table 1.

The confidence factor of the training model has a direct im- pact on how well the relationships are learned from TRS.

The higher this factor, the better is the learning. The most accurate classification algorithm (that maximizes CFSam- ple) is chosen. Therefore, the aim is to derive a TRS which represents the complete classification relationships in the sample set with a confidence factor of at least CFSample without over-fitting the test set as well. Let a and b be two classes in the dataset. The labels in each quadrant of the matrix have the following semantics: ? Ta represents true positives of a, i.e., the number of ex- amples (in the tested set) satisfying the model and a.

? Fa represents false positives of a, i.e., the number of ex- amples satisfying the model but not a.

? Fb represents false positives of b, i.e., the number of ex- amples satisfying the model but not b.

? Tb represents true positives of b, i.e., the number of exam- ples satisfying the model and b.

CF is defined (in terms of the above notation) as: CF = Ta+Tb  N ?100, where N denotes the total number of instances in the dataset.

The fitness function uses the classification model learned from the sample set to test whether the model fits the train- ing and test sets evenly. That is, the classification model is tested on the training set to determine how well the relation- ships are learned in the training set. Thus, the confidence factor of the trained model is determined; it is denoted CF- Training. If CFTraining is greater than (or equal) to CF- Sample, the performance on the test set is observed. That is, the classification model is tested on the test set to determine how well the relationships are learned in the test set. Thus, the confidence factor, denoted CFTest, of the test model is determined. If CFTest is greater than (or equal) to CFSam- ple then ErrorFit is calculated. That is, the difference of Er- rorTraining (defined as the number of incorrectly predicted instances by the model on the training set) and ErrorTest (defined as the number of incorrectly predicted instances by the model on the test set) is found. If ErrorFit is less than specified threshold fixed as 5% (chosen arbritrarily), then the fitness value of the chromosome is CFTraining. Other- wise, the chromosome is given a default fitness value. The intuitive idea to choose this default value is: 1) the value has to be greater than zero to continue the GA runs in sub- sequent generations; and 2) since the fitness criteria is not satisfied by the respective chromosome, an arbitrary value is chosen to punish the less fit chromosome. This value is intuitively chosen as a positive integer less than CFTrain- ing. There are multiple ways of choosing this value. For example, one way is to choose this value as CFTraining2 .

An alternative could be any value that is less than CFSam- ple (since CFSample is always less than CFTraining in this case).

The fitness function is defined as: If CFTraining and CFTest ? CFSample and  ErrorF it ? 0.05, then Fitness Function = CFTraining  F itnessFunction = CFTraining/2 Otherwise (1)  There is a possibility that the GA will attempt to capture as many 1?s as possible in a chromosome to maximize the global fitness value in subsequent generations. This could result in a solution where the size of the test set is signifi- cantly smaller than the size of the training set. This could also affect the convergence of the GA. To avoid this prob- lem, a size factor is added to the fitness function. When the size of the test set falls below 30%, the fitness value of the represented chromosome is decremented by 30%.

The value 30% is chosen because the most common split in the literature is performed with 70% of the instances in the training set and 30% instances in the test set.

All these aspects are precisely encoded and implemented into the GA and all the chromosomes (potential solutions) should be awarded or punished according to the criteria stated above during the process of evolution. The outcome of several evolutions modeled by this GA generates the right split of the dataset into training and test sets.

The approach has been implemented as a standard GA written in C, similar to Grefenstette?s GENESIS program; Baker?s SUS selection algorithm [17] is employed; 2-point crossover is maintained at 60% and mutation is very low; and selection is based on proportional fitness. The GA starts with a random population of 100 chromosomes that repre- sent a random splitting of the dataset into training and test sets. In the fitness function, a UNIX shell script is used to call the classification model (using the sample set) from the WEKA data mining tool [16] to test on the training and test sets for over-fitting.

4 Results and Discussions  The original dataset is divided into a sample set and a validation set using different statistical strategies [2, 3, 4].

The GASPER algorithm is used to split the sample set into training and test sets. The results are tested on the valida- tion set. As an alternative, the traditional method, namely the random sampling is also used to obtain the training and test sets from the sample set. As compared to the other two traditional methods, it is suitable for both small and large datasets. In fact, bootstrap is a form of random sam- pling (with replacement) and cross validation is a very rudi- mentary form of random sampling. We tested our approach on 3 data sets. All the tests have been conducted using a  single Processor, Intel(R) Xeon(TM) UNIX machine with CPU power of 2.80GHz and cache size of 512 KB.

The proposed approach has been tested using Breast Cancer Data Set, a real data set obtained from Tom Baker Cancer Center, Calgary, Alberta, Canada. The original dataset consists of 221 records and 16 attributes. Each record represents follow-up data for one breast cancer case.

Breast cancer ?recurred? in some of these patients after the initial occurrence. Hence, each patient is classified as ?re- current? or ?non-recurrent?, depending on his or her sta- tus. With respect to classification of the dataset, there are 2 classes: 1) Recurrent Patients, and 2) Non Recurrent Pa- tients.

The original dataset is divided into a sample dataset and a validation set using the random sampling tech- nique. The sample dataset has 121 instances whereas the validation set has 100 instances. The proposed GA based approach is applied to split the sample dataset into training and test sets. The fitness function uses SVM classification algorithm to train the classification model using TRS. The thresholds in the fitness function are: CFSample = 85% and DefaultV alue = 70%.

Finally, the GA parameters are: PopulationSize = 100, 2-PointCrossOverFraction = 0.6, ReproductionFraction = 0.1, MutationFraction = 0.1%, and the SelectionMethod is Proportional.

Figure 1. Trend of the Best Fitness Values of the GA Method  The Average number of generations it took to find the best solution, Standard deviation (?) and Best fitness mean (?) of the solutions obtained in 30 experiments for the GA method are 78, 0.4024, and 93.05, respectively. Figure 1 shows the plot of the best fitness values obtained at the end     of every experiment for 30 GA experiments. The best so- lution results in 75 instances in the training set and 46 in- stances in the test set. The training set is used as input to the association rules mining algorithm in the WEKA data- mining tool [16] to obtain the possible classification rules.

The obtained rules have been checked and validated by ex- perts at Tom Baker Cancer Center. This increased the con- fidence in the accuracy and effectiveness of the approach.

Some of these rules are given next: 1. t=1 ? sx1=mast =? recurrence = -1 2. cancer1=IDC ? t=1 ? sx1=mast =? recurrence = -1 3. sx1=mast ? rad=R =? recurrence=-1 4. method=lump ? rad=R ? sx1=seg =? recurrence=1 5. n=0 ? sx1=mast =? recurrence=-1 6. method=lump ? n=0 ? sx1=mast =? recurrence=-1  Table 2. Performance Evaluation of CARs on the Validation Set for Dataset1  Correctly Predicted (in%) 75 Incorrectly Predicted (in%) 10  Unknown (in%) 15  Table 3. Results of GA and Random Sampling for Dataset 1  GA Random Sampling  Confidence Factor of Train- ing Model  94.4 92.4  Noise(With the Test Model) 0.01 0.05 Number Training Instances 75 61 Number Test Instances 46 60  The above classification rules can be interpreted as fol- lows. For example, it can be inferred from rule 4 that if the method of diagnosis is lump (that is, the patient found this herself), the treatment is segmentectomy, meaning that she had removal of the tumor only and not the adjacent breast tissue and the treatment of radiotherapy is consistent to the remaining breast tissue in an attempt to kill off any remain- ing cancer cells, then the cancer is likely to recur. The per- formance evaluation of the derived CARs (using TRS from the GA method) from the rule mining algorithm on the vali- dation set is shown in Table 2. The results of the GA method and random sampling are listed in Table 3. It is important to observe that the confidence factor of TRS obtained using the GA method is higher than the confidence factor of TRS derived using random sampling. Further, the training model from the GA method fits the test set with 1% noise whereas the training model from the sampling method fits the test set  (a) the GA Method  (b) Random Sampling Method  Figure 2. Variation of Fitness Value in a Single Run  with 5% noise.

As a result, it is necessary to emphasize that the GA based method is an intelligent and efficient search technique as compared to a random search technique such as sam- pling for the pre-processing problem. Learning in the GA search technique is the best because the solution increases monotonically from a default value to the best solution in a finite number of generations. In random sampling, as the name signifies, the search process is random, so the solution starts with a default value and fluctuates at different splits displaying an erratic behavior. Running the classification algorithm is an off-line process. So once TRS is derived us- ing the GA method, the invaluable trade off is the high ac- curacy of the CARs derived using the ARM technique with     this TRS as input. In applications such as medical diag- nosis, this trade-off in the form of accurate results (CARs) from this method would prove to be invaluable.

5 Conclusions  In this paper, we proposed a novel approach that em- ploys GA for splitting a data set into training and test sets.

We focused specifically on the appropriate split for deriving the best classification rule set from the output of an associa- tion rule-mining model. The data-splitting problem (for the ARM technique) presented in this paper identified the fol- lowing tasks: the entire dataset should represent the popu- lation as closely as possible; different samples are simulated by using the traditional methods to split the dataset into a) sample set and b) validation set; and each such sample set should be divided into training and test sets efficiently so that a) the training set represents the true classification rela- tionships in the sample dataset as much as possible without overfitting; and b) the test set detects overtraining of the trained classification model as much as possible. The ad- vantages are: 1) he primary problem of simulating the sam- ple and the validation sets of the real-world is addressed; and 2) one approach is identified to define the right data- split for efficient ARM, namely, to distribute the classifi- cation relationships between the splits evenly. Using ex- perimental validations, it is verified in this paper that the performance of the CARM model using TRS derived from such a split performs with good accuracy on the validation set.

