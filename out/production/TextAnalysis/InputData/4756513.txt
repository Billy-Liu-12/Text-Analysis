Association Rules Mining Based on an Optimized Probability and Statistics  Estimate Model

Abstract   This paper has analysed the Apriori algorithm performance, and has pointed out performance bottleneck question of the Apriori algorithm. Currently those algorithms to mine association rules only pay attention to one aspect of efficiency or accuracy respectively. There is a paradox between efficiency and accuracy. In order to resolve to this conflict, a novel algorithm based on Probability estimate and least square estimate is proposed to mine the association rules from database with the high correlativity and the high confidence. Probability estimate reduce the times of database scanning so as to increase efficiency; least square estimate is based on rigorous and classical mathematical model so as to enhance accuracy.

Furthermore, we deduce a recurrence formula to resolve K-itemsets issue. Experimental results have demonstrated that our algorithm is not only efficient but also keeps the completion of frequent items.

1. Introduction   Association rule mining is an important embranchment of the artificial intelligence. Association rule in the military field, the most typical example of association rules is that it can be used to examine the failure of the military equipment. During the examination, one failure will often cause many failures. So during the diagnosis process, we need not only the default judge rules of fault in each subsystem but also the rules of transmission of failure information and interaction. Mining the association rules from a great deal of failures, we can get available association rules between failure and cause or get available association rules between one failure and another failure, and then we can provide important reference for eliminating the failure quickly and completely [1].

There is a bottleneck question in classical Apriori algorithm that is frequently repeated scanning the transaction database [2][3]. So we propose an algorithm  based on probability estimate, which has an obvious improvement in efficiency. At the same time, in order to keep the completion of frequent items, that is to say accuracy, we utilize least square method to estimate parameters of probability model.

This paper is organized as follows: Section 2 gives a review of transitional Apriori algorithm, and analyses its performance. Section 3 presents our Probability model, which increases the speed of data mining and decreases computer?s I/O operation. In section 4, we propose an algorithm based on least square method to estimate parameters of probability model. Section 5 gives out the experimental results. And we compare Apriori algorithm with our algorithm. Finally, conclusions and future work will be illustrated in Section 6.

2. Apriori algorithm   Now Apriori algorithm is core technology for other each kind of Boolean Mining association rules algorithm.

But the Apriori algorithm has two bottlenecks. Firstly, the algorithm performs as many passes over the database as the length of the longest itemsets. This incurs high I/O overhead for scanning large disk-resident database many times. For each item in the itemsets Ck to verify it in the frequent itemsets Lk must scans the database one time.

Secondly, it may come into being a large number of itemsets. The itemsets Ck which produced by Lk-1 grows rapidly [4][5].

The most crucial step in the Apriori algorithm is finding frequent itemsets. The Apriori algorithm finds frequent itemsets by scanning the database frequently and it consumed too much time. It also may come into being a large number of item sets by JOIN operation and it consumed too much time too [6]. In order to solve this issue, we present an algorithm based on probability estimate.

3. Our Probability model   2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application  DOI 10.1109/PACIIA.2008.21   2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application  DOI 10.1109/PACIIA.2008.21   2008 IEEE Pacific-Asia Workshop on Computational Intelligence and Industrial Application  DOI 10.1109/PACIIA.2008.21     3.1. 2-itemsets probability algorithm  Association rules mining uses a support-confidence  framework is useful for many applications. However, the support-confidence framework can be misleading in that it may identify a rule A?B as interesting, when in fact, A does not imply B. In this section, we consider an alternative framework for finding interesting relationships between data items based on correlation.

Two events A and B are independent if ( ) ( ) ( )P AB P A P B= , otherwise A and B are  dependent and correlated. This definition can easily be extended to more than two variables. We can know that the correlation between A and B can be measured by computing  , ( )  ( ) ( )A B P ABcorr  P A P B =                                    (1)  If the ,A Bcorr  is less than 1, A and B are negatively correlated, which means that one event rate restrain the occurrence of another.

If ,A Bcorr  is greater than 1, A and B are positively correlated, which means that one event rate promotes another.

If ,A Bcorr  is equal to 1, A and B are independent each other, and there is no correlation between them[7] [8].

Obviously, we can conclude that ( ) min{ ( ), ( )}P AB P A P B?  for correlations between each  A and B.

Especially, in mining association rules, we only  consider the situation of , 1A Bcorr ?  that all the event rate is positive correlation.

Suppose independent probabilities of each attribute item 1 2, , , nK K K  in Database are 1 2, , , nP P P .

Simultaneous happening probability of arbitrary two attributes items iK , jK is ijP . According to discussion above, in positive correlation, we have  ( ) ( ) ( ) min{ ( ), ( )}P B P A P AB P A P B? ?  [9].

Then, we can deduce:  1 1 1(1 ) ( )ij j i j j i j i jP a P a PP a P PP PP= + ? = ? +              (2) We can compute 1a by following steps: (1) Scanning mined database at the first time is in order  to get each probability of 1-itemset.

(2) Scanning mined database at the second time is in  order to get each probability of 2-itemset.

(3) Calculate every 1ia by expressions (2) and then  average to get 1a .

3.2. 3-itemset probability algorithm  There are four categories logistics facility in China,  such as Hub, Central Distribution Center, Regional Distribution Center and Distribution Center.

We only take the situation in positive correlation into account, and we have ( ) ( ) ( ) min{ ( ), ( ), ( )}PCPAB PABC PA PB PC? ? .

Then we can deduce:  1 1  1 1  1 1  (1 )  (1 )  (1 )  ijk ij ij ik  ijk ij ij jk  ijk ik ik jk  P a P a P P  P a P a P P  P a P a P P  ? ? = + ? ? ? ?? = + ?? ?  ??? = + ???  (3)  According to total probability formula  ( ) ( | ) ( )  n  i i i  P X P X Y P Y =  =? , we can get  1 2 3 1 2 3( 1)ijk ijk ijk ijkP k P k P k P k k k? ?? ???= + + + + =           (4) Where 10 1k? ?  20 1k? ?  30 1k? ? .We use  1 2 3 1/ 3k k k= = =  in the calculation.

3.3. Description of this algorithm  1. Scan the database, the algorithm simply scans all of  the transactions in order to count the number of occurrences of each item. It can get frequent 1- itemsets,L1. It consists of the candidate 1-itemsets having minimum support. It can get each support of frequent 1- itemsets, And Then the number of frequent 1-itemsets is m.

2.To find Lk ,a set of candidate k-itemsets is genetated by joining Lk-1 with itself. This set of candidate is denoted Ck.The join, 1 1k kL L? ? ,is performed, where members of Lk-1 are joinable if they have (k-2) items in common, that is, 1 1 1{ | , ,| | 2}k k kL L A B AB L A B k? ? ?= ? ? = ? .The algorithm uses 1 1L L  to generate a candidate set of 2- itemsets, C2 . And scan the database secondly. The set of frequent 2-itemsets, L2 ,is then determined, consisting of those candidate 2-itemsets in C2 having minimum support. And then the number of frequent 2-itemsets is f.

3. From step one and two, the probability of Ki is Pi and the probability of Kj is Pj, when they happen at the same time, the probability of them is Pij. We can get ak  from formula (2). Then 1 n  k k  a a  n == ?  and 2mn C= .

4. The algorithm uses 2 2L L to generate a candidate set of 3-itemsets, C3.

5. The algorithm uses formula (4) to generate all occurrent probability of candidate 3-itemsets. It get candidate frequent 3-itemsets whose occurrent probability is greater than minimum support.

6. Iterate the above process, estimate candidate frequent 4-itemsets and 5-itemsets and so on, and then analogy until get the longest candidate frequent itemsets.

7. From step 6, we can get candidate frequent itemsets, and then use it to scan the database to get the support of each candidate frequent itemsets; and then use the support to get the frequent itemsets; if the support of the candidate frequent itemsets is greater than the minimum or equal to it, we should get this frequent itemsets.

8. From the frequent itemsets, we can get the associate rules.

However, we only average 1ia  to get a , which is not optima estimation. Thus, we propose to use least square method.

4. Least square estimate   We have ( ) ( ) ( ) min{ ( ), ( ), ( )}PC P AB P ABC P A P B PC? ? .Then we can deduce:  2 2 2 2 1  1 1 2 2 2  (1 ) (1 ) ( ( ) )  ( ) ( ) ijk i k ij i k j i j i j  k j i j k j i j k ij k ij i  P aP a PP aP a P a P PP PP  P P PP a aaP P PP aPP PP PP  = + ? = + ? ? +  = ? ? ? + + + (5)  Because, we have already known ijP  after 2-itemset were computed. And ijP  is substituend into ijkP , so we can get ijkP .

This is a non-linear function, so we expand Taylor formula to linearization:  )|(  |       ijkijk ijkijk  i  ijkijk ijkijk  PPda a P  da a P  v  da a P  da a P  PP  ?? ? ?  + ? ?  =  ? ?  + ? ?  +=                   (6)  Where ( )T1 2 nV = v , v , , v , iv  is a correction value, ( )1 2,  Tda da=x , 0|ijkP is an approximation for ijkP ,  and 11 12  1 2n n  k k  k k  ? ? ? ?= ? ? ? ? ? ?  B  ( ) ( ))(),()(,, 12  21 jijkijkjijkjijk ijkijk  ii PPPPaPPPPPPaPPPPa P  a P  kk ?????=?? ?  ? ?? ?  ? ? ?  ? ?  =        (7)  Generally speaking, this function model obeys normal distribution, whose probability density is:  / 2 1/ 2  1 1( ) exp (2 ) | | 2n  f ?  ? ?? = = ?? ? ? ?  T -? ? D ? D  (8)  We introduce a random model: 12 2 0 0? ?  ?= = n n n nn n  D Q P  Where nn  Q  is a factor matrix (inverse of weight matrix),  nn P  is a weight matrix. According to maximum likelihood estimation: min=TV PV .We get each element  covariance by 0?=ij ijD Q , where LLQ = Q ,   2n ? =  ?  TV PV .

Program layout is as follow Figure 1:   Figure 1  Program layout   We generalize an N-itemsets recurrence formula 1 2 1 1 2 1 2  1 2 1 1 1 2 1  1 2 1 2 1 2 1 1 2 2  1 2 3 2 3 2 3 1 2  1 2 1  ( ) ( ) ( ) min( ( ), ( ), , ( )) ( ) ( ) (1 ) ( ) ( ) ( ) ( ) (1 ) ( ) ( )  ( ) ( ) (1 ) ( ) ( ) ( )  n n n n  n n n n n n  n n n n n n  PAA A PA PAA A PA PA PA PAA A a PA a PA PAA A PAA A a PA a PA PAA A  PAAA aPA a PA PAA PAA a  ?  ? ? ?  ? ? ? ? ? ?  ? ? = + ? = + ?  = + ? = 2 1 2 1( ) (1 ) ( ) ( )PA a PA PA+ ?  (9)   5. Experiment   We have already achieved the Apriori algorithm which  is in the environment of VC6.0, and we also program with VC++ to realize our algorithms. Then we compared them     in the same condition. The test condition is: P4 2.0 CPU; 512M DDR; 80G HD and Windows xp-sp2 professional OS. And the test data that we use in the experiment is the failure detection history notes of certain system equipment which used by navy. The number of the total failure detection history notes is 103 638 notes. The minimum support is 2?, the degree of confidence is 0.3. In the experiment, the failure detection history notes are different part of the all notes, and the numbers of them are: 1?104, 2?104, 4?104, 6?104, 8?104, 1.0?105; From figure 2, we can know the comparative result of Apriori algorithm and this algorithm In the execution speed. And from figure 3, we can know the comparative result in the quantity of rule sets.

Figure 2  Compare of implementation time     Figure 3  Compare of rules    From the experimental result, we can easily find that under the same conditions, if the notes in data sets are increasing, the executive time of Apriori algorithm and this algorithm will increase, but the slope of this algorithm is less than the slope of Apriori algorithm, the increaser of this algorithm is less than the increaser of Apriori algorithm. So the diffusibility of this algorithm is better than the diffusibility of Apriori algorithm. With the increase of items and notes, the difference will be more and more obvious. And in the experiment, we have found that it decreases I/O operation of computer and it only needs less memory.

In order to estimate parameter a  in the formula (2) (5) (8), we can use sample mean, however this method is not optimal. Thus, we use least square method to estimate a .

And from the experiment, we can prove that this algorithm is steady and effective. Experimental result show that ours spends a little more time than our algorithm with sample mean estimate, but algorithm results with least square estimate are  more approximate to Apriori algorithm results. Furthermore, the operating speed of algorithm with least square estimate is still fast, and approximate to mean estimate. So we propose our algorithm based on probability model with least square estimate.

6. Conclusions   In this paper, our probability model that is based on probability estimate and least square estimate is proposed to mine the association rules from database. Our algorithm takes both efficiency and accuracy into account and it is proved and validated by experiment.

Sometimes, we need to know which factors have the most influence to an event rate and how much this influence is. Therefore, in the future, we will research principal factors leading up to even rate so as to distinguish correlativity in the association rules data mining.

Acknowledgement   It is a project supported by Wuhan Digital Engineering  Institute.

