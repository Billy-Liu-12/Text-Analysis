<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">25-29 July,  2004 Budapest, Hungary

Abstract- Discovering knowledge from large databases is a challenge in many applications. The implicit meanings of knowledge can be repressed by different knowledge representations. A concept hierarchy is a concise and general form of knowledge representation. Hierarchical concept description can organize relationships of data and express knowledge embedded in databases explicitly. In this paper, we propose a new scheme based on rough sets to cluster and refine the concept hierarchy automatically for a given data set with nominal attributes. The proposed scheme consists of two algorithms: the concept clustering algorithm and the concept refinement algorithm. The experimental results show that the concept hierarchy mined by the proposed scheme contains meaningful concept in comparison with the previous approaches.

The analyses of the algorithms also show that the proposed scheme is efficient and scaleable for large databases. It is also able to be extended to mining meaningful rules from databases.



I. INTRODUCTION Knowledge discovery in databases (KDD) is one of the important research topics in both areas of machine leaming and databases. The objective of KDD is to extract knowledge automatically from huge amounts of data. The knowledge mined from large practical databases can be applied to many applications. For different applications, the different types of knowledge lead to various methodologies in the development of data mining techniques. For example, classification leaming algorithms for decision trees construction [ 151, the Apriori algorithm for mining association rules [ 11, and unsupervised learning algorithms for generating clusters [9].

Regardless of the mining approaches involved, we can imagine the various types of discovered knowledge as the concept in the database based on different viewpoints.

For representing concepts explicitly, a meaningful and clear concept description is required. Concept hierarchies are a concise and general form of concept description widely used. Many knowledge representations are variants of concept hierarchy in different presentations. For transaction databases, as an example, association rules are the set of rules describing the relative relationships in a partial ordering hierarchy of large item sets. Decision tree is another type of concept hierarchy used for generating classification rules and classifying the categories of data. The other type of concept hierarchy, like clusters, is a one-level concept grouping data with high similarity into classes.

Animal Level0 Fig. 1. An animal world of concept hierarchy.

ANY Level 0 (Ho) Fig. 2. The hierarchical concept model A concept hierarchy defines a sequence of mappings from a set of low-level concepts to higher-level, more general concepts on one or a set of attribute domains. Such mappings may organize the set of concept in partial order according to a general-to-specific ordering and form a tree-like structure.

The most general concept can be a universal concept, whereas the most specific concepts correspond to the specific values of attributes in the database, as Fig. 1. Each node in a concept hierarchy represents a concept which helps to  express knowledge and data relationships in a database as a concise, high level term [4j. Formally, we define a hierarchy H on a set of domains D1, . . ., Dk, we have Hl : {Dl x . . . x Dk} a H,.l . . . = HO , where H, denotes the set of concept at the primitive level, H,-l denotes the set of concept at one level higher than those at HI, and Ho represents the most    level higher than those at HI, and Ho represents the most general concept on the top level denoted as ?ANY?, as Fig. 2.

It is reasonable that various concept hierarchies can be constructed fiom the same attributes based on different users? viewpoints or preferences. For constructing concept hierarchies, many methods for numerical or categorical data have been developed. We summarize the related methods 0-7803-8353-2/04/$20.00 0 2004 IEEE 1343 FUZZ-IEEE 2004 into three kinds in the following. The first kind of methods is to specify concept hierarchies by users, experts or data analysts according to a partial or total ordering of attributes explicitly at the schema level. A user or expert can easily define a concept hierarchy by semantics of schema or hisher need. Another kind of methods is that given a set of attributes, the system then tries to generate the partial ordering of attributes automatically so as to construct a meaningful concept hierarchy. Some related researches on generating hierarchies in a single numerical attribute [4] and finding the partial ordering in a set of categorical attributes [8] have been proposed. The third kind of methods is to define a portion of a concept hierarchy by clustering explicit data in databases.

Most of methods of this kind focus on automatic generation of concept by grouping data using clustering techniques.

Without knowledge of data semantics, a concept hierarchy will be generated from the distinct values of attributes at the lowest level of hierarchy; then the more general concept are generated in higher levels. The related researches include Cluster/;! by Michalski and Stepp [ 111, COBWEB by Fisher [3], parallel clustering by Hong and Mao [7], generality- based clustering by Talavera and Bejar [16], etc. It is still a interesting and important problem for finding complete a hierarchical structure from a large database efficiently and meaninally.

The purpose of this paper is to automatically construct a meaningful concept hierarchy in databases. We introduce rough entropy based on rough sets to measure the generality of concept described by the subset of attributes in a database.

We had proved that the values of rough entropy for a set of attributes are increasing monotonously while the attributes are removed one by one from the set of attributes. This property accords with the fact that concept in higher level of hierarchy is more general than concept in lower level. Based on proposed rough entropy, we give a concept clustering algorithm to generate main concept levels and group the most relevant concepts into one. For producing more detailed hierarchy structures, the concept refinement algorithm is designed. To evaluate the performance of the proposed scheme, some experiments and comparisons with the other researches are made. The results show that the proposed method is efficient and the mined concept levels contain meaningful concept knowledge.

The paper is organized as follows: Section 2 introduces rough sets theory and rough entropy. In Section 3, we presented the two proposed algorithms based on rough entropy for generating and refining the concept hierarchy from categorical databases. In Section 4, we describe the experimental results. Conclusions and future work are made in Section 5.

been broadly and successfully applied to knowledge  discovery. The theory provides a powerful foundation to reveal and discover important structures in data and to classify complex objects. An attribute-oriented rough set technique can reduce the computational complexity of learning processes and eliminate the unimportant or irrelevant attributes so that the knowledge can be learned from large databases efficiently.

In an information system, if some categories of objects cannot be distinguished by the available attributes and may be just able to be defined roughly or approximately, the rough set theory is suitable to deals with such information    rough set theory is suitable to deals with such information system. The idea of rough sets is based on establishment of equivalence classes on the given data set U and supports two approximations called lower approximation and upper approximation. The lower approximation of a concept X contains the equivalence classes that are certain to belong to X without ambiguity. The upper approximation of a concept X contains the equivalence classes that cannot be described as not belonging to X. Let S = (U, A )  denote an information system where U means a non-empty finite set of objects with a non-empty finite set of attributes A .  For all a d  and each B d ,  we define a binary indiscernible relation &amp;(B) in the following.

Definition 1: Let x, Y E U ,  we say that objects x and y are indiscemible if the equivalence relation &amp;(B) is satisfied on the set U, RA@) = {(x,y)l X,Y?U, v ? a a  a(x)=aW}.

Definition 2: Let apr = (U, RA) be an approximation space.

The object neU belongs to one and only one equivalence class. Let [x]B denote an equivalence class of R A @ )  and [UJB denote the set of all equivalence classes [x]B for X E U .  We have [ ~ I B  = b I &amp;(B)Y, x, Y E U) and [UB = { [ x ] ~  I X E U f .

Definition 3: Let S = (U, A),  x, E U and B c_ A. The rough entropy of information based on a subset of attributes B is where l[xi]BI denotes the cardinality of the equivalence class of [&amp;]B and n is the number of objects in U.

Theorem 1: Let S = (U, A),  B E A and B? E A .  If [xj]B c [x~]B? , then E(B) I E(B?).

Corollary 1: Let S = (U, A),  B E A and B? d. If B? E B, then E(B) 5 E(B?).

11. ROUGH ENTROPY Theorem 1 states that the rough entropy of equivalence classes decreases monotonously when the granularity of information becomes smaller through finer partitions. Hence, The rough set theory was first proposed by Zhislaw Pawlak in 1982 [121[141. Rough set is used to deal with the identification of common attributes in data sets [13]. It has 25-29 July, 2004 Budapest, Hungary the concept is more general while the value of rough entropy is larger as Corollary 1 states.

111. THE ALGORITHMS A. Notations The symbols used in our proposed algorithms are defined in the following.

U : A database with categorical attributes.

n : The number of objects in U, A : The finite set of categorical attributes in U.

m : The number of attributes in A .

aJ : The j-th attribute of A,  aJ E A, 15 j &lt;  m.

[&amp;]B : The equivalence class of x, satisfying RA(B), x, E U.

I[x,]Bl : The cardinality of [&amp;]B, x, E U.

[ v], : The partition of RA(@ on U, [U], = { [ x , ] B  1 x, E U}.

C, : The k-th concept.

We define the reduced set of attributes in the following.

Assume that B = {al,  a2, . . . , a,} is a set of attributes. Let BJ be a reduced set with m' attributes of B, m' &lt; m. BJ = {bl, b2, . . . , b,,} is a subset of B, where b,, bJ E B and b, f bJ for l&lt;i,j&lt; m'. Let Pn,,(B) denote the set of all possible reduced sets with m' attributes of B. Thus, the number of attributes in B, is lBJ1 = m' and IP,,(B)I = (2,).

B. The Concept Clustering and Refinement Algorithms After the definitions, we describe the algorithms for clustering hierarchical concept with respect to a given database.

Algorithm: The hierarchical concept clustering Input: A database U with the set of attributes A .

Output: A concept hierarchy H for S = (U, A).

Step 1: Initialize the values of m' = m, k = 1 and B = A .

Step 2: Generate the concepts Ck of lowest level with the    Step 2: Generate the concepts Ck of lowest level with the Step 3: Let IBI be the number of attributes in B, we set m' = Step 4: Compute the rough entropy E(BJ) for all B, E P,,(B).

Step 5 :  Generate a higher concept level HI,,, by the following 5.1) Let B' be the reduced set B, with max(E(BJ)}, for B, E Pn7,(B); then we generate new concepts C, in H,, with the partition [Ua.. The CA is the concept that merge the concepts C, in the lower level H,,.+l belonging to equivalence class [ x ] ~ .

5.2) For the others C, in the lower level H,,,'+I, if C, = [x]B', the C, is unchanged and added into H,,.

Step 6: Let B = B', if B # 0, go to Step 3; otherwise, output The time complexity of the proposed algorithm mainly depends on the partitioning of equivalence class [ U]B and the number of reduced sets of attributes of each processing iteration. Assume that the time complexity of partitioning U set of attributes A .  That is, H, = [UA.

IBI - 1 and find f,,,)(B).

two sub-steps: all concept levels H = {Ho, . . . , H,n}.

is T(n). In Step 3 of the algorithm needs O(m) to generate P,t(B). Then, we have to spend O(mT(n)) to compute the values of rough entropy E(BJ for all Bi E P,@) in Step 4. In Step 5, the time for generating a single concept level is less than n. Finally, the number of attributes is decreased when a new concept level is generated. The overall time complexity is O(m2(T(n))). The time complexity of finding partitions is obviously bounded by the sorting problem. Hence, the time complexity of the algorithm is no more than O(m2nlogn). We give an example to help readers to understand the above algorithm.

TABLE I THE EXAMPLE DATA SET Example 1: TABLE I shows a database containing eight objects U = {xl, x2, x3, x4, xs, X6, x7, x8} and five attributes A = {gender, income, insurance, status, car}.  The symbolic categories in the five attributes, respectively, are: gender: {male (m),  female Q), boy (b), girl (g)}, income: {high (h), low ( I ) } ,  insure: {yes (y), no (n ) } ,  status: {single, married, divorce, yet} and car: {yes, no}. The hierarchical result of clustering is shown in Fig. 3. The steps are executed as follows: Input: The dataset as TABLE I shows.

Output: The concept hierarchy H.

Step 1: Initialize the values of m' = m = 5, k = 1 and B = A  = Step 2: Generate the concepts of lowest level H, with the set {gender, income, insure, status, car).

of attributes A .

HS = [V!A = [Uligender, income, innire. slatus. car) = {cl, c2, c3, c4, c5, c6, cl, c8} = { ] A  71x21A, [x31A,[x4lA ,[xSIA 9 b 6 i A 9  b 7 1 A  9 [%]A .

Step 3: We set m' = JBJ - 1 = 4. Find P4(B), which is the set of all possible reduced sets with 4 attributes of B.

Pm*(B) = {Bj I BJ E B, IB,I = m'} = CBI, B2, B3, B4, B5}, where BI = {income, insurance, status, cur}, B2 = {gender, insurance, status, car} , 8 3  = {gender, income, status, car} , B4 = {gender, income, insurance, car}, B5 = {gender, income, insurance, status}.

Step 4: Compute the values of E(BJ) for BJ E P4(B).

E(B1) = E( { income,insurance,status,car}) = 4, E(B2) = E( {gender,insurunce,status,cur}) = 0, FUZZ-IEEE 2004 E(&amp;) = E( {gender,income,statuS,car~) = 0, E(B4) = E( f gender,income,insurance,car}) = 8, E(&amp;) = E( (gender,income,insurunce,stutus}) =0.

Step 5: Generate a higher concept level HS by the following two sub-steps: 5.1) Since E(&amp;) is the maximum of E(BJ for B, E P4(B), 5.2) For the others C, in the lower level Hs, because the C,    5.2) For the others C, in the lower level Hs, because the C, is unchanged and added into H4. We have H4 = (Cs, Step 6: The reduced set of attributes is B4, thus B = B' = B4 = {gender, income, insurance, car} f 0. The next step of the algorithm goes to Step 3 to generate higher concept levels until reduced set of attributes B = 0.

B' = B4. Thus, C9 = C~UC~,  Clo = C~UC~.

c6, c7, CS, c9, ClO).

Xi x2 .% x6 x7 X3 XA X8 Fig. 3. The result of clustering algorithm for TABLE I.

Algorithm: The concept refinement algorithm.

Input: A concept hierarchy H for S = (U, A).

Output: A refined concept hierarchy P for S = (U, A ) .

Step 1 : Let j = 1, B, be the reduced set of attribute used in H,, and Ck is the top concept in Ho. We set k = k + 1.

Step 2: For all the reduced sets of attributes Bef,(B,+,) and B z B,, repeat Step 3 to Step 5 to refine the concept level H,.

Step 3: Find the partition of [U], = { [xIB 1 x E U } .

Step 4: If there is [x]~ existing objects that belong to distinct concepts in H,, go to Step 5 ;  otherwise, go to Step 2.

Step 5: Generate the new concept CL in concept level of H,, CL = [XIB.  The [ x ] B  is the equivalence class satisfying the condition in Step 4.

Step 6: If j &lt; m, j = j + 1, go to Step 2 for refining the higher concept level; otherwise, the algorithm is halt.

Example 2: We continue to explain the concept refinement algorithm following the result of Example 1. The result of refinement is shown in Fig. 4. The steps are explained as the following example.

Input: A concept hierarchy H in Fig. 3.

Output: A refined concept hierarchy P for S = (U,  A) .

Step 1: j = 1, k = 16 and BI  = (income}, HI  = (C14, C ~ Z } .

Step 2: P,(B,+*) = PI(B2) = {{income}, { c a r } } ,  for B~f'1(B2) and B # B1 = (income}; that is B = (car} ,  execute Step 3 to Step 5 to refine the concept level HI.

Step 3: Find partition [Ul{car, = (C7uC12, C13).

Step4:Because there is an object x7 in C7 belonging to distinct concepts C14 and C12 in HI at the same time, the next step goes to Step 5 .

Step 5 :  Generate the new concept CI6 in concept level of Hl, c,, = c7vc,2.

Step 6:j=1 &lt; m = 5 , j  = j  + 1 = 2, the algorithm repeats Step 2 for refining the hgher concept level H, untilj = 5 .

It is easily to know that the time complexity of the concept refinement algorithm is still the same to the concept clustering algorithm. While the detailed concept we need is only the part of whole hierarchy, our scheme can help to find the useful concept levels without wasting time to generate all detailed concept hierarchy.

x i  xi x5 x; x, x3 x4 x* Fig. 4. The result after refinement algorithm for TABLE I.

1v. EXPERIMENTAL RESULTS To evaluate the performance of the proposed scheme, we make some experiments and comparisons using several test data sets. The experiments are done by using Pentium I11 800MHz CPU with 128MI3 RAM. Test datasets are selected from UCI Machine Learning Repository [2] summarized in TABLE 11.

25-29 July, 2004 Budapest, Hungary TABLE 11.

SUMMARY OF SELECTED DATASETS TABLE 111.

PREDICTIVE ACCURACIES AND EXAMINED NODES Attributes Instances Classes BCW 9 683 2 CLEVE 13 296 2 CRX 15 653 2 GLASS 9 2 14 7 HORSE 12 326 2 PIMA 8 768 2    Data No refinement With refinement a sets acc(%) nodes acc(%) nodes 0.5 78.77 1.00 73.79 1.00 0.6 78.77 1.00 73.79 1.00 0.7 78.77 1.00 74.52 1.27 BCW 0.8 92.39 2.40 90.48 2.02 0.9 92.09 4.85 92.97 4.66 1.0 91.51 5.75 91.51 5.75 For observing the hierarchy structure generated by the proposed concept hierarchy generating algorithm, all numeric features were discretized by the fimctions in MLC++ [lo] to fit the requirements of the proposed scheme and performed the tasks of test. In order to evaluate the effectiveness of the concept level, we perform learning and prediction processes.

The decision attribute is used only at prediction, but hidden during the learning process. After the leaming process, we use the following measurement function and a threshold a to label the concept in the concept hierarchy to predict the test data. The function is where X denotes the concept of decision attribute, B is the reduced set of attributes at that concept level. We select the nodes with p.,? (x) &gt;a as the concept we predict. TABLE 111 shows predictive accuracies and number of nodes examined in prediction for original hierarchy and refined hierarchy.

The experiments are done with IO-fold cross validation.

Considering the best accuracies on each dataset among the methods, the method with refinement is better than without refinement. We compare the results of the generated concept hierarchy with refinement with GCF [16] and COBWEB [3] methods. TABLE 1V shows the comparison including the average accuracy and number of nodes examined to make the predictions with the standard deviations. The average number of nodes examined can be viewed as a measure of the complexity of the hierarchy. Results show that none of the systems is the best for every domain, thus suggesting that each one may be better suited for a particular type of problem.

Considering only the best accuracies for three systems on each data set, our algorithm needs to examine fewer nodes on average to attain the same accuracy or the better accuracy.

COBWEB uses an objective function that implicitly weights features and it is intended to produce predictive clusters.

GCF model uses similarity metrics which are sensitive to the choice of features and may need some information about feature weights to be more robust. This may the reason for COBWEB performing much better than GCF and our approach in some domains. Experimental results confirm that our system traverses fewer nodes than previous work for making good prediction in average.



V. CONCLUSIONS 0.5 52.70 1.00 45.27 1.00 0.6 71.28 2.00 71.28 1.67 0 7 73.99 4.54 73.99 4.27 76.69 6.78 76.69 6.78 0.9 73.31 7.05 73.31 7.05 1.0 69.59 7.32 69.59 7.32 CLEVE o:8 0.5 54.52 1.40 48.70 1.26 0.6 68.61 1.86 68.45 1.68 75.96 2.70 0.7 75.96 2.73 0.8 81.32 9.19 81.32 9.28 0.9 80.09 9.53 80.09 9.65 1.0 76.26 10.12 76.26 10.20 0.5 56.07 4.05 56.07 3.53 0.6 54.67 4.42 54.67 3.95 0.7 55.61 4.54 54.67 4.16 0.8 55.61 4.75 54.67 4.45 0.9 55.14 4.69 54.21 4.59 1.0 54.21 4.63 54.21 4.64 0.5 62.27 1.00 62.27 1.00 0.6 62.27 1.00 72.39 3.68 ~ 0 7- 77.91 4.91 77.91 4.86    ~ 0 7- 77.91 4.91 77.91 4.86 76.99 6.71 76.99 6.70 0.9 73.31 6.86 73.31 6.86 1.0 68.71 7.20 68.71 7.20 HORSE 0:8 0.5 62.50 4.61 65.89 4.49 0.6 67.84 4.76 67.84 4.76 0.7 67.32 4.81 67.32 4.81 ?IMA 0.8 67.32 4.82 67.32 4.82 0.9 66.80 4.78 66.80 4.78 1.0 66.80 4.78 66.80 4.78 Knowledge discovery from a large database is an important research topic of late years. The concept hierarchy is an explicit representation of knowledge and can be widely used in many applications. In this paper, we present a new scheme for generating concept hierarchies based on rough set and rough entropy. The scheme contains the concept clustering algorithm and the concept refinement algorithm. We can fast construct a meaningful concept hierarchy by the concept clustering algorithm first, and then refine the concept level by the concept refinement algorithm as users? needed. The experimental results demonstrate that the proposed scheme is efficient and the generated concept hierarchy is effective. In FUZZ-IEEE 2004 the future, we are going to extend the research to cope with the data with missing values.

