Implicit Groups of Web Pages as Constrained Top N Concepts

Abstract  This paper presents an effective depth-first mining algo- rithm for finding relatively smaller therefore more implicit groups of Web pages as formal concepts. The algorithm is based on a dynamic ordering method depending on each search node and some search tree expansion rules. More- over it is designed so as to find top N implicit concepts subject to the size restriction and some space constraints reflecting user?s interests.

1. Introduction  A huge collection of text data involved in pages over Web has been considered as an information source of knowledge. Therefore, various approaches for searching, clustering and mining pages and their structures have been intensively investigated in these decades. Among such stud- ies, this paper is mainly concerned with a problem of min- ing implicit page groups represented as formal concepts (concepts in short) in Formal Concept Analysis (FCA) [2, 3] from the data in the form of page-term relationship. In other words, our target page group is a relatively smaller set X of pages that has an intentional definition that ?X is a set of pages that have every term in a feature term set A?. Then a formal concept is a pair of X , called the extent of concept, and its term set A, called the intent.

Such an implicit concept will be useful in discovering ?Crossover Group of Pages? for instance. Suppose we have several concepts with their extents of many numbers of pages so that they are visible by applying standard effective mining engines as [6, 5, 4, 7] for instances. These pages are not necessarily connected by links, as we consider here a page-term relationship only. Suppose furthermore those groups are extensionally far away. There may be no over- lapping. Even for such a case, there exists a possibility for two minor groups, each from each major group, to share a common important feature terms. From a viewpoint of  FCA, the union of minor groups appears as a part of concept defined from the common terms. When the concept is minor with relatively smaller extent, the concept is worth examin- ing to check if some invisible interconnection among the parent major groups occurs via the minor ones. Those im- plicit concepts are also hard to be found by clustering [1].

To detect implicit extents with smaller size, we are forced to have many number of smaller clusters. It is actually unprac- tical for users to check them all. Without category labels to pages, or almost equivalently without using prior clustering, we show in Section 3 that our algorithm succeeds in finding several interesting implicit concepts beyond several distinct categories.

As is well known, each intent of concept just corresponds to a closed itemset of an association rule [15]. Many nice algorithms [6, 5, 4, 7] for finding frequent closed itemsets have been developed successfully. However, since our tar- gets are non-frequent, we cannot apply them at least di- rectly.

A similar problem about potentially implicit page groups has been already conceived as ?implicitly defined commu- nities? [8]. The implicitly defined communities have too specific interests and are generally difficult to be identified via Web portals or centers in the bipartite graph (bigraph).

Consequently the number of such communities is large.

The situation will be worse when we consider a bigraph representing page-term relationships with a higher density.

Therefore, we are required to have more effective miner for detecting implicit concepts under some constraints. In this sense, ours is an instance of Constrained Mining [16].

For this purpose, we present a revised version of the top N algorithm [17]. Both of them try to enlarge extents as long as their intents are longer patterns to some extent. In other words, since too much smaller extents are out of our concerns, we maximize the extent size under the constraint about the corresponding intent?s size. The algorithms are basically a depth-first and branch-and-bound search method [9] with a pruning rule to cut off candidate concepts when- ever their over-estimated evaluation values are less than the   DOI 10.1109/WIIAT.2008.325    DOI 10.1109/WIIAT.2008.325     tentative top N values already detected.

In this paper, to cope with large scale data and to reflect  user?s interests, we firstly improve the ability to enumer- ate possible solution concepts based on a dynamic order- ing technique, and then to introduce additional space con- straints. A similar ordering strategy is also used in [10, 14] to find longer itemsets using a set enumeration tree. How- ever, in that case, no special expansion rule to avoid dupli- cation is needed, while ours needs an expansion rule to skip duplications. Another important technique to improve the efficiency of pattern miners is a preprocessing method for concise representation [5] of dataset. However, our top N algorithm accesses only a part of whole data by the branch- and-bound pruning. For this reason, we here do a direct depth-first search without applying prior data analysis. A miner that searches for longer patterns is also proposed in [11]. It is based on some bias to avoid hopeless search for longer patterns, while we introduce some space constraints under which ours keeps the ability to enumerate every solu- tion satisfying the constraints.

We introduce the constraints of three kinds. The first one defines a starting extent that must include positive example pages. The second one requires for an extent not to cover any negative example pages. The positive and negative ex- amples are also used in [18, 12] to dicover Web communi- ties, given an Web bigraph consisting of centers and fans, where the communities are found by enlarging initial page groups guided by best-first search heuristics. Our top-N method is also considered as an enlargement process. How- ever, it is complete in the sense that it finds every solution page group under the constraints.

Although we allow to use negative examples, users seem not to be aware of target pages or concepts and their coun- terparts as well. For this reason, we introduce the third con- straint in addition to positive and negative examples. The third one is for realizing searches with an upper bound con- cept whose intent is just a set of terms given by user.

In a word, the constrained search can respond within 10 seconds for 10,000 pages with 1200 terms, given an ad- equate set of constraints. Thus, the algorithm can run in an interactive mining environment for analyzing search re- sults and for realizing implicit page groups connecting ma- jor groups. This will motive us to search Web from a differ- ent point of view represented by implicit concepts.

2. Problem Specification  We suppose two sets: O and F representing a set of pages and a set of their feature terms, respectively. Then, the set of terms possessed by every page in X ? O is de- noted as ?X . Conversely, ?A is a set of pages with every term in A ? F . The actual construction of ? and ? from Web pages is described in Section 3. When common terms  of Z appear as terms shared by W (?Z ? ?W ), we here say that Z implies W and write as Z ? W . Then, an ex- tent is defined as a set X such that X = {x | X ? x}.

That is, the extent is closed under (object) implication, and is called closure (or closed set). Similarly, an intent A of terms is similarly defined using (attribute) implication [2].

The only fact remarked here is that ?X and ?A are an in- tent and an extent for any set X ? O and A ? F , respec- tively. A formal concept is defined as a pair of extent X and its corresponding intent ?X . So we identify the con- cept with its extent (or its intent). We suppose in addition a pair of monotone evaluation functions evalO and evalF such that evalO(X1) ? evalO(X2) whenever X1 ? X2 and evalF (A1) ? evalF (A2) if A1 ? A2. Their most simple forms are set sizes which we assume simply in this paper. Another forms of eval can be found in [17] includ- ing rank information. Now, the problem of finding implicit concepts is described as follows:  Objective: Enumerate every solution extent X with top N evaluation value evalO(X), where they must be sub- ject to the followings:  Length Constraint (required): Given ? > 0, evalF (?X) ? ? for excluding larger X .

Space Constraint (option): X must satisfy (POS): S+ ? X for an example page set S+. (NEG): S? ?X = ? for a negative page set S?. (SUB): X ? ?K for a relevant term set K.

(POS) is requiring I = {z | S+ ? z} ? X . Hence S+ defines the starting extent I in the bottom up search. (SUB) assigns an upper bound closure ?K, and is equivalent to K ? ?X meaning thatX must have every term inK which users show their interests. By (POS) and (SUB), a sublattice with I and ?K as the least and the greatest closure, respec- tively, is formed. When (POS) is not presented, S+ is just the bottom extent of whole concept lattice. Similarly, we treat other constraint types in the same manner when they are not explicitly presented.

It is straightforward to show that every constraint is anti- monotonic w.r.t. set inclusion. That is, suppose we have extents X1, X2 such that X1 ? X2. Suppose in addition that X1 violates some type of constraint. Then X2 also vio- lates the same one. So, we can easily install the constraints to depth-first bottom up searches, from the bottom extent to larger extents. Although we are allowed to restrict the search space by the constraints, it is a key to have an ef- fective enumeration method of concepts when the optional constraints are not presented explicitly or when the data in the form of page-term relationship scales up. For this rea- son, we introduce a dynamic ordering of candidates and a tree expansion rule customized to it.

Candidate page: LetX be a present extent consistent with the given constraints. Then, a page x /? X is called a candidate at X if the enlarged extent, cl(X,x) = {z | X,x? z}, still satisfies the constraints.

Some candidate z at X cannot be a candidate at cl(X,x) if {w | X,x, z ? w} violates the constraints. Thus the sequence of candidate sets is monotonically decreasing as we add new candidates to the closure extents.

Dynamic candidate ordering: For a present extent X and its candidate x, x is a branch to form the next extent.

We arrange candidates x in the increasing order of the sizes of term sets ?(X ? {x}). The ordering is locally fixed at each X . So we denote it as ?X  When the candidate x is actually chosen at X , another y s.t. X,x ? y is included together with x into the next closure. As x has smaller term set atX , it has more chances to imply such additional y. This helps us to form larger next closure.

Now, based on the ordering strategy, we expand our search tree. The root node is {z | (S+ ? z}. The procedure expands tree nodes in the depth first manner by selecting a candidate at each node according to the dynamic ordering.

The sequence of chosen candidates c1, ..., ck represents the path from the root to the extent {z | S+, c1, ..., ck ? z}.

Thus a path with S+ is just a generator [6] of the extent.

Unlike set enumeration tree, some control to avoid dupli- cated generations of the same extents is needed, as there exist several generators for the same extents. For this rea- son, we classify candidates into two types, One is called a right candidate used for expansion. The other is called a left candidate used for checking the duplication. Suppose we have a series of extents Xk = {z | S+, c1, ..., ck ? z}, where ck is a chosen candidate at Xk?1 to form Xk. That is, Xk = {z | S+,Xk?1, ck ? z}. Then a candidate r at Xk is called a left candidate, given a chosen candidate ck+1 at Xk to form Xk+1, if r ? {c1, ? ? ? , ck} or r ?Xk+1 ck+1.

Inverse Implication Pruning: For a present extent X and  its right candidate r, if X, r ? ? holds for some left candidate ? at X , we need not take the branch by r.

Branch-and-Bound Pruning: For a present X and a right candidate r, we skip the branch by r whenever the evaluation value of (Xr = {w | X, r ? w})) ? {right candidate at Xr} by evalO is less than the min- imum of the current top N values. When the number of values stored is less than N , this rule is void.

The algorithm repeats the tree expansion on a path in a depth-first manner, using the above pruning rules, and goes back to its parent node to try another right candidate at the parent node.

3. Experimental Results  We present here our experimental results. We have tried to extract Top-N clusters from a dataset called BankSearch. The dataset has been released as a bench- mark for Web document clustering [19]. It consists of Web documents (HTML sources) in 11 categories, ?Commer- cial Banks?, ?Building Societies?, ?Insurance Agencies?, ?Java?, ?C/C++?, ?Visual Basic?, ?Astronomy?, ?Biology?, ?Soccer?, ?Motor Sport? and ?Sport?. The total number of documents is 11, 000 (1, 000 for each category).

As a preprocess, we have first converted each HTML source into a plain text by removing HTML tags. From the text documents, adjectives and adverbs in WordNet [21] and a set of stopwords have been eliminated. After Stem- ming Process with Porter stemmer [20], we have selected 1223-words as feature terms by removing terms that are too frequent and too infrequent. It should be emphasized here that the category information never appears in the doc- uments as features explicitly.

Our system has been implemented in JAVA and run on a PC with Dual-Core AMD Opteron processor 2222 SE and 16GB main memory.

Extracted Clusters:  Given an Web page in the dataset, http://www.vbsquare.com/files/association/, we have tried to find Top-3 concepts under ? = 50. As an example, a concept ? { http://www.vbsquare.com/files/association/, http://www.vbsquare.com/registry/tip471.html,  http://www.vb-helper.com/links.htm, ...

http://www.vbsquare.com/databases/dbclass/,  http://www.vbsquare.com/databases/learndb/,  http://www.vbsquare.com/mouse/context/ }, { API, component, resource, . . . tips, VB, graphic } ? con- sisting of 35-pages has been extracted. All of the pages are related to Visual Basic (VB) resource links, tutorials and stories on VB. They belong to the same category assigned in [19]. It should be noted here that our method never uses the information about the categories explicitly. Our clusters are extracted based on only terms appearing in Web pages.

Thus, without the category information, our method can extract clusters which are consistent with the known cate- gories.

Given two Web pages, http://www.citibank.com/uk/portal/  consumer/helpdesk/tc/tc1.htm and http://vbtechniques.com/useragreement.asp, and two terms, claim and Internet, as positive examples and relevant terms, respectively, we have tried to find Top-1          0.0150.020.0250.030.035 1e+06  1e+07  1e+08  1e+09  C om  pu ta  tio n  T im  e (s  ec .)  N um  be r  of F  re qu  en t C  lo se  d Ite  m se  ts  Minimum Support  Computation Time Num of Freq. Closed Itemsets  Figure 1. Computation Time by LCM and Num- ber of Frequent Closed Itemsets  concepts under ? = 50, then obtained a concept ? { http://www.citibank.com/uk/portal/  consumer/helpdesk/tc/tc1.htm,  http://vbtechniques.com/useragreement.asp,  http://www.hrbs.co.uk/cashisatandcapply.htm, ...

http://www.hrbs.co.uk/panthertandconline.htm,  http://www.hrbs.co.uk/rewardsixtandcapply.htm,  http://www.lloyds.com/un/en/  termsandconditions/category/article/ }, { claim, Internet, accept, law, condition, . . .

reason, right, term, transfer } ? consisting of 22-pages. These pages are concerned with contracts and terms of agreement. Furthermore, since they belong to different categories, ?Commercial Banks?, ?Visual Basic?, ?Building Society? and ?Insurance Agency?, we consider that it is a concrete example of crossover concepts actually obtained with our method.

Thus, our Top-N method has an ability to flexibly ex- tract various concepts reflecting our interests represented as positive example and relevant terms.

Problem Difficulty - Finding Formal Concepts by Closed Itemset Miners:  As has been mentioned previously, formal concepts can be obtained by any closed itemset miner, e.g. LCM [4]. Such a system is, however, not always helpful for finding our Top- N formal concepts satisfying some constraints. More con- cretely speaking, in order to find our Top-N formal con- cepts, a closed itemset miner must first enumerate frequent closed itemsets including our targets and then chooses the  0.1        C om  pu ta  tio n  T im  e (s  ec .)  Delta  with Examples, Terms and Dynamic Ordering with Examples and Terms  with Examples and Dynamic Ordering with Examples  with Dynamic Ordering  Figure 2. Comp. Time with Positive Exam- ples, Relevant Terms and Dynamic Ordering  targets from them. However, the miner often enumerates a huge number of frequent closed itemsets, taking long com- putation time.

Figure 1 shows the computation time by LCM and the number of frequent closed itemsets under various minimum support thresholds (minsup) for the BankSearch dataset.

The figure tells us that for lower minsup values, ex- tracting Top-N concepts with LCM would be impractical from the viewpoint of its computation time and output size.

For example, the setting of minsup = 0.015 forces us to extract all concepts consisting of at least 165 documents.

Therefore, any smaller concepts (say, below a hundred) can never be obtained with the help of closed itemset miners in practice. More concretely speaking, the extent of each concept just presented above consists of 35-pages and 22- pages, respectively. In order to obtain the former concept with a minsup-based closed itemset miner like LCM, there- fore, we have to set minsup = 3511000 = 0.003. For the lat- ter, minsup = 3511000 = 0.002. Needless to say, our targets are out of range for which such a miner can compute. Thus, our Top-N method can extract targets actually intractable for minsup-based itemset miners. This is a remarkable ad- vantage of our Top-N method.

Effectiveness of Positive Examples, Relevant Terms and Dynamic Ordering:  Since positive examples and relevant terms restrict the search space, our computational cost can be reduced. In ad- dition, our dynamic ordering on candidate expansions also achieves improvement in computation time. For the same positive examples and relevant terms, their effectiveness is verified in Figure 2. In the figure, we can easily observe that     they are quite effective in improving our computational ef- ficiency. We can enjoy significant improvement with them.

Although the positive examples can solely provide a great reduction of computation time, the relevant terms bring us further drastic improvement. Particularly, for lower ?- values, the ratio of computation time with only examples to those with both examples and relevant terms is above 100.

It is highly expected that the larger our dataset becomes, the greater difference we will observe. Thus, our method would be promising even for large-scale datasets.

4 Concluding Remarks  In this paper, we showed that the top-N algorithm suc- ceeds in finding less frequent (crossover) concepts under some space constraints. In order to have more effective method under more vague constraints, we are planning to define the notion of crossover concepts more directly and to design more efficient and accurate procedure under the help of clustering of pages allowing outliers [1].

