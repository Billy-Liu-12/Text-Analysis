Privacy Preserving Support Vector Machine using Non-linear Kernels on Hadoop Mahout

Abstract?Four main challenges (volume, velocity, variety, veracity) have confronted computation algorithm designers in big data mining. Homomorphic cryptosystem with secured multi- party computation of matrix operations has been shown to yield high privacy preserving while data miners perform information retrieval from big data. This research concerns with the compu- tation complexity of the big data with specific focus on computa- tional load reduction while preserving data privacy. We propose a Teo-Han-Lee (THL) algorithm with various matrix operations to reduce the cryptographic cost significantly by cutting off at least one-third or more total computational operations . In THL, a pre- generated random key technique that we propose to apply here can decrease the computational time in which the random keys can be retrieved from memory without being generated on the fly. We further develop a collusion-resistant secure sum product protocol (CRSSPP) which is integrated in THL algorithm over arbitrary partitioned data. Experimental results demonstrated that THL-CRSSPP algorithm is more efficient than Vaidya et al SVM method [2] (state-of-the-art SVM method) and hence would be more applicable to the cloud-based big data mining. The THL- CRSSPP algorithm can also be integrated into Hadoop Mahout with a minimal effort.

keywords: Support Vector Machine, Classification, Security, and Privacy

I. INTRODUCTION  Data mining has three key tasks: regression, information retrieval, and clustering on data to yield a data driven model.

Specifically, it has to discover patterns in data by analyzing large quantities of data. In data mining, the clustering or classification takes a number of input variables and allocates them to classes to produce patterns based on training data sets. The patterns are generally meaningful to make a useful prediction on new data. In conventional data mining, data are presented in the centralized or federated form. The sensitive and critical data from hospitals, government agencies and so on are needed to be protected to avoid privacy violation and abuse for malicious purposes. To prevent the misuse of data, some laws and regulations, such as the United States of America (U.S.) healthcare laws and the 1996 administrative provision in HIPAA [1] , have been proposed to prohibit companies or groups from sharing user?s private and sensitive data without appropriate anonymization. In this paper, we regard the sensitive data as private data to be protected.

In the modern day, large amount of data have been collected by a central repository from various asynchronous sources. These combined data are shared to other parties to perform one or more of the three key data mining tasks of regression, information retrieval and anomaly detection.

Clustering of rare pattern has been gathered from different parties who participate in pattern discovery activities. Many privacy-preserving data classification algorithms are proposed to perform distributed knowledge discovery without disclosing any individual party?s private data. There are two aspects of measuring privacy preserving algorithms. First is the compu- tational efficiency. The second is the privacy and data security level of measure. With one measure being given (i.e. regarded as exogenous, which is the case of this paper), the objective is to improve computational efficiency at no expense of privacy level of the data. In the arbitrary partitioned data, any party can hold a different set of data distributed in either horizontally, or vertically and even both of them. In this paper, we consider three parties where data are arbitrarily split between them.

Motivating Example: The Unites States of America (the U.S.) and Australia collect different personal details of the potential terrorists like their dates of birth and religions. The United States is not allowed to retrieve Australian?s details due to privacy protection laws in Australia. However, the United States and Australia can share their data to evaluate if a person is a potential terrorist via privacy-preserving data classification algorithms.

In machine learning, Support Vector Machine (SVM) is one of the most popular classification algorithms, which which has been shown to yield classification accuracy than other machine learning algorithms in many real-world applications. Like other conventional classifiers, Support Vector Machine (SVM) as- sumes data miners have a full access to the correct set of data.

In this paper, we propose a Privacy-Preserving Support Vector Machine (PPSVM) method with various matrix operations over arbitrary partitioned data. The PPSVM method protects data privacy and can acquire a similar accuracy of classifier as running in conventional Support Vector Machine(SVM). Com- paring to Vaidya et al. method [2] over arbitrary partitioned data, our proposed PPSVM method can significantly improve run time on large data sets. To circumvent colluding problems, we propose a protocol, the collusion-resistant secure sum product protocol (CRSSPP), to further enhance the privacy.

In Section II, SVM Overview is presented. We discuss the related work of PPSVM in section III. The Proposed PPSVM method with THL algorithm and the Vaidya et al.

method [2] are presented in Section IV. Experimental results and conclusion are discussed in Section V and Section VI respectively.

DOI 10.1109/CSE.2013.200    DOI 10.1109/CSE.2013.200

II. SVM OVERVIEW  A basic classification of linear SVM finds a linear boundary by taking a set of input data and predicts two possible classes from the input. In a non-linear (kernel) classification, SVM uses kernel functions to separate the input data into two possible classes by mapping input data into high dimensional feature spaces. To get a good overview of SVM classification is starting with the linear SVM as depicted in Figure 1.

Figure 1: The separating hyperplane that maximises the margin to separates the classes.

y = w.x+ b. (1)  In the linear SVM classification[3], [4], SVM finds a sep- arating hyperplane that maximizes a margin by a straight line in Eq.(1). The margin is the distance between the hyperplane and closest data points (i.e., support vectors). A classifier line accepts any value, x, that are positive (square) or negative (round) values as depicted in Figure??. Instead of using shapes, we make the target classes be +1 and -1 on Eq.(1).

M =  2 [w] =   2.

? w.w  . (2)  Given the classifier line, the margin, M , is computed based on Eq.(2). To optimally separating points by SVM, we need a decision boundary that classifies the input data well in which making w . w as small as possible.

Considering a case where SVM makes a mistake by putting a data point just on the wrong side of the line, we need to consider this constraint using the minimisation technique on Eq.(3). In the technique, R is a number of misclassified data points, and each ? (slack variable) is the distance to the correct boundary line for the missing point. A parameter, ?, is a trade- off weight, which a small ? prizes a large margin over a few errors and otherwise. Hence, each data point, x, is targeted 1 if x.w >= 1? ?, or -1 if x.w <= 1? ?  L(w, ?) = w.w + ?.

R? i=1  ?. (3)  L(w) = max R? i=1  ?i ? 1  R? i=1  R? j=1  ?i?jtitjxixj .

subject to the constraints 0 < ?i < ? and  R? i=1  ?ixk = 0.

(4)  We can use the Lagrangre Multiplier to transform ? which is treated as a parameter instead of a constant and to find a minimiser by looking at the derivatives of the function with respect to each of the parameters independently by setting them to zero using Eq.(3). Instead of the minimisation technique, we can formulate the problem into a maximisation problem to find its dual by Karush-Kuhn-Tucker (KTT) to construct the dual function using Eq.(4).

In the non-linear (Kernel) SVM, we use the kernel trick to compute a gram matrix, G, by dot product of the original vectors, which is incurred 0(n) in computational cost. Some good kernel functions of SVM [3] that are widely used as follows,  K(x, y) = (1 + x.y)s. (5)  K(x, y) = tanh(kx.y ? ?). (6)  K(x, y) = exp( ?1(x? y)2  2?2 . (7)

III. RELATED WORK  Privacy preserving data mining protocols use different approaches to protect individual data while performing data mining. These protocols use different protection techniques like Perturbation (Randomization) method, secure multi-party computation (SMC) and K-anonymity method. In the per- turbation method, one can add noises to data before data mining process taking place. Reconstruction techniques [5], [6] mitigate the noises and thus data mining results can be recovered. SMC with cryptographic approach is found in decision tree [7], association rule mining [8], [9], clustering [10] and classification [11]. Our proposed PPSVM method uses SMC with cryptographic approach and the perturbation method to protect data privacy. In the K-anonymity method [12], it removes some values of data in a way such that the given data cannot be distinguished from at least (k-1) other data.

In SMC, Yao?s protocol [13] computes a result from participating party without revealing and learning each in- dividual participating party data. Since then, many privacy- preserving protocols have been inspired and proposed based on Yao?s protocol especially in the data-mining field. Researchers proposed various privacy-preserving data mining protocols     Fig. 1. Privacy-preserving data mining through partitioning data.

targeted at different machine learning techniques, like decision trees, Bayesian networks, and support vector machine (SVM) on vertical and horizontal partitioned data, but as yet a few protocols targeted on arbitrary partitioned data are found to be less efficient and less practical on large data sets. This motivates us to find a more efficient and practical approach of PPSVM on arbitrary partitioned data without privacy violation.

Many Privacy Preserving Support Vector Machine (PPSVM) techniques focus on data that are partitioned either vertically or horizontally [14], [15], [16], [17], [18]. So far we know a few of PPSVM methods like Vaidya et al. [2] that targeted on arbitrary partitioned data. Their method is found to be less efficient and has incurred prohibitive computation and communication costs. In contrast, our proposed PPSVM method provides a more efficient approach without privacy violation.



IV. PRIVACY-PRESERVING SVM CLASSIFICATION  In the arbitrary environment, data can be randomly split on either horizontally or on vertically, and even both of them (Figure 1). Both horizontal and vertical partitioned data are considered as special cases of arbitrary partitioned data.

To protect data privacy, the kernel matrix (i.e., K) securely computes the gram matrix (i.e., G) without disclosing the data of one party to others. The Gram matrix, Gij = xi.xj , is computed by the dot product of the data vectors either by Polynomial kernel function, K(x, y) = (1 + x.y)s or RBF kernel function, K(x, y) = exp(?1(x?y)   2?2 ), without privacy violation.

We propose an efficient and secure PPSVM method on arbitrary partitioned data. To securely compute the gram matrix (i.e., G) on arbitrary partitioned data, Vaidya et al. method[2] use a modified version of SSP[19] based on Paillier homo- morphic encryption[20]. Our PPSVM method computes the gram matrix (i.e., G) with different proposed matrix operations based on the perturbation method and SSP [19] based on Paillier homomorphic encryption [20], which is proven secure and efficient. There are other several homomorphic encryption systems existed such as Goldwasser-Micali cryptosystem and Naccahe-Stern cryptosystem. They all are semantically-secure public-key encryptions with an additional property that exists an encryption E(A ?B) such that  E(A) ? E(B) = E(A ?B)  Fig. 2. Collusion-resistant secure product protocol.

where ? is either addition or multiplication (in some Abelian group).

THL algorithm significantly improves run time over Vaidya et al. method [2] on large data sets without privacy violation.

The performance will be evaluated and demonstrated in the experiment section. THL algorithm uses different matrix oper- ations like Matrix Multiplication with Upper/Lower involve- ment, Strassen Multiplication [21], Strassen-Coppersmith- Winogard Multiplication [22], and Strassen-Bodrato Multipli- cation [23].

In the following sections, we discuss THL algorithm in details. In Section IV-A, we propose a collusion-resistant secure sum product protocol to circumvent the colluding problems. THL algorithm is presented in Section IV-B while Section IV-C shows the protocol analysis.

A. Collusion-Resistant Secure Sum Product Protocol (CRSSPP)  Yao [13] introduced the general concept of Secure Multi- Party Computation (SMC). The basic idea of SMC is that each participating party knows the computation results only and learns nothing else from other parties. For instance, four parties collaborate to compute the sum without disclosing their data to other parties. The private values of each individual party, P1, P2, P3, P4, are 3, 6, -8 and 2 respectively. At the end of a sum computation, each party learns a sum value that is 3 (i.e., 3 + 6 + (-8) + 2) and learns nothing about the private values of other parties. Murat and Chris [9] proposed a method by introducing a master site to solve the colluding problem in Yao [13]. However, Murat and Chris [9] face the colluding problem if the party i? 1 and the party i+ 1 collude to determine the exact value of party i by comparing the value they receive and send. We could not find the implementation details of PPSVM to circumvent the colluding problem targeted on arbitrary partitioned data in Vaiday et al[2].

We propose the Collusion-Resistant Secure Sum Product Protocol, CRSSPP for short, is depicted in Figure 2 based on Paillier homomorphic encryption. The CRSSPP helps to circumvent the colluding problems as occurred in Yao [13] and Murat and Chris [9]. We use some convenient notations     P =  ? ??  a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 a16  ? ??  Fig. 3. Square matrix, 4 x 4, shows four features in data indicated by a number of columns.

PPT =  ? ??  a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 a16  ? ?? .

? ??  a1 a5 a9 a13 a2 a6 a10 a14 a3 a7 a11 a15 a4 a8 a12 a16  ? ??  Fig. 4. Dot product of matrix, P and its transpose, PT .

to illustrate how the CRSSPP works in Figure 2 . The notations use as follows: Init as an Initiator, ea as an encrypted method using Partya?s public key, and da as a decrypted method using Partya?s private key. The collusion problem can be circumvented by having a pair of keys generated on each party instead of using the public key generated from the initiator. For instance, Partya uses its own generated public key to encrypt its data and sends the encrypted data with the public key, PKpartya , to Partyb. After receiving the message from Partya, Partyb encrypts its data together with Partya ?s encrypted data using the public key, PKpartyb . Hence it is no longer possible for any participating party to reveal other parties inputs. In round 2, all parties run the CRSSP together to compute the dot product securely. In this way, the CRSSPP helps to protect the private data from the colluding problems. The detailed analysis of the protocol is provided in Section IV-C.

B. The Proposed THL Method  We use a matrix placeholder, Ph, to store data owned by each party which the Ph is square matrix as depicted in Figure 3. The gram matrix, G, computes the dot product of the placeholder, Ph, and its transpose, PhT , as depicted in Figure 4. We place null values into the placeholder, Ph, to make it a square matrix to run THL algorithm if data points of the placeholder, Ph, can not construct in the square matrix form.

Let say we have k numbers of participating parties, P1, P2, ..., Pk, where P1 is an initiator that acts to collect the datasets from all parties and to compute the gram matrix.

The initiator here contributes no data to any computation.

The initiator, P1, generates a pair of keys which are public and private keys based on Paillier Cryptosystem [19]. Each party uses the public key of the initiator, P1, to encrypt its own data. To avoid privacy breach, we use the perturbation (randomization) technique to distort actual data of the parties, P2, P3, , Pk. For instance, party Pk generates a row vector with random values, Vi..n ? Zm to create a random placeholder, Phr. After receiving Phnew and Rk from Pk, the initiator, P1 computes the gram matrix, G, by subtracting the Ph  ? new  from the Rk which is G = P ? new ?Rk using Eq.( 8). THL  algorithm is depicted on Algorithm 1.

Paillier cryptosystem [20] provides support only on  positive integer values. To allow non-positive values as input data, we assume the limit of the input value range in (?m/2,m/2) where m = p ? q and p and q are two large prime numbers randomly generated in the system. For non-positive values, v, we encrypt it to get a message, v +m [24]. After the computation, we decrypt it to obtain a result, s and later subtract the value of s from m to get the output, s?m if s > m/2. We assume the range of each number in (?m/2v,m/2v) to compute the summation of v numbers.

((Ph+Phr)?Phr).((Ph+Phr)?Phr)T = Phnew +Rk (8)  Algorithm 1 PPSVM on arbitrarily partitioned data using the THL algorithm.

Require: (a - e)  a) An Initiator, I, compute the gram matrix and create the classifier.

b) A total of n points across m parties.

c) I creates a matrix data placeholder, ph and a matrix random placeholder, phr .

d) Data Matrix A, Abc represent the value of the cth feature of the bth point.

e) Let say, k parties participate in the algorithm.

I creates a new semantically secure homomorphic encryption system keypair {pk, sk}.

I sends the public key pk, the placeholder, ph with a size of n ? n to all of the parties.

for i = 2...k do for j = 1...n do  partyi encrypts phij = Epk(Aij) end for  end for  partyk generates a random row vector, r? = [ri, ri+1, ..., rn]lim 1?n where ri ? Zm for i = 1...n do  for j = 1...n do Set phij = phij . Epk(rj) Set phrij = rj  end for end for  partyk computes phnew and Rk based on Eq.(8).

1) Matrix Multiplication with Upper/Lower Involvement: We compute the gram matrix, G, by the dot product of matrix, A and its transpose, AT . Both of them are square matrices that have a size of n? n.

(AAT )ij = n?  i,j=1  aija T ji (9)     Fig. 5. Integration of THL with CRSSPP into Hadoop Mahout.

On Eq.(9), we know that the result of (AAT )ij is symmet- ric which it exhibits (AAT )ij = (AAT )ji. Thus, we reduce at least one-third operations of n3 in multiplications and of n2(n+ 1) linear operations (addition/subtraction).

2) Strassen Multiplication: Strassen?s method [21] requires square matrices in order to compute dot product of matrices.

It reduces a total number of multiplication operations while it increases linear operations. For instance, 2?2 matrix requires a total of n2(n? 1) = 4 additions and 8 multiplications in the naive dot product whereas it requires 18 additions and 7 multiplications in Strassen?s method [21]. The complexity of this method is asymptotically in O(n2.807) [21].

T (2n) = 7T (2n?1) + 18(4n?1) (10)  The total operations cost of Strassen?s method [21] in Eq.(10) is derived from Master?s theorem [25].

3) Strassen-Coppersmith-Winogard Multiplication: Strassen-Coppersmith-Wingard method [22] enhances Strassen?s method [21] by reducing linear operations from 18 to 15 and keeping a same number of multiplication operations. The complexity of this method is asymptotically in O(n2.374)[22]. Hence it is one of highly regarded practical algorithms for computation over a finite field.

4) Strassen-Bodrato Multiplication: Strassen-Bodrato method [23] is a Strassen-like method to compute dot product of matrices. Bodrato [23] proposes a new sequence technique to reduce the total operation cost for squaring and more complex products. However, this method has same numbers of multiplication and linear operations as in the Strassen- Coppersmith-Wingard?s method [22]. In Bodarto?s paper[23], we could not find any complexity indication. So we assume that Bodrato?s method has a similar complexity of O(n2.374) as in Strassen-Coppersmith-Wingard?s method [22].

C. Protocol Analysis  Privacy Measurement: Let say we have k numbers of participating parties, P1, P2, ..., Pk, where P1 is an initiator.

The initiator, P1, learns nothing from the placeholder, Phnew(Eq. 8). A random placeholder, Phr(Eq. 8), has a row vector with random values, Vi..n ? Zm. The initiator, P1, is computationally hard to probe and infer the original values of the parties, P2, P3, ..., Pk. For instance, a party?s data values [0, 1, 1, 0, 1] is added to a random row vector values, [2, 3, 2, 4, 3], to generate new values, [2, 4, 3, 4, 4]. Thus, we prove to apply the same random row vector values onto the row vector, Phr that is sufficient to protect data privacy and to make, Phnew, as a non-deterministic matrix. Moreover, we can make Phnew even harder to probe and infer from its original values by generating a different random value of the placeholder, Phr(NP hard). In the CRSSPP, the computation and communication costs are O(n). However It needs (n3) times on both of the encryption and decryption. Vaidya et al.

method[2] shows not much details of the colluding problems over arbitrary partitioned data .

THL Algorithm: The THL algorithm uses the perturbation and SMC approches to compute the gram matrix, G. The perturbation technique has been discussed in the privacy measurement section. In SMC, we use the SSP[19] based on Paiiler?s cryptosystem[20] which is proven efficient and secure. The Vaidya et al. method[2] did not include the colluding test in their experiment. We use a similar test case to make our experiment results comparable to their method.

Even it is unlikely possible to find honest and trustworthy relationships, the proposed CRSSPP and HL algorithm can protect the data privacy issue in semi-honest parties. The asymptotic complexity and communication cost of the Vaidya et al. method are O(n3) and O(n3) respectively. Their method needs to perform (n3 + n2 + n) times on encryption and (n2) on decryption. In contrast, the asymptotic complexity of THL algorithm is less than or equal to O(n2.807) which is based on different matrix operations. The communication cost of THL algorithm is O(n). In THL algorithm, we need a total number of (4n2 + n? 1) on encryption and (n2) on decryption.

The Vaidya et al. method [2] only published data mining results. In contrast, THL algorithm can publish each individual record data to compute data mining results. Our approach is not tight-coupled with any specific data mining algorithm and data mining tasks can be unknown at the time of data publishing. In some data publishing scenarios, it is a crucial step to publish each individual record data that relates to existing record data in a real life. For instance, pharmaceutical researchers may use the actual patient records to examine and discover previously unknown side-effects of the tested drug [26]. Hence it is hard to deploy the data mining result in the real world if we do not publish a patient record related to a real life patient.

Matrix Multiplication: The complexities of these matrix op- erations are less than or equal to O(n2.807) on the gram matrix, G, computation asymptotically. Theoretically speaking, the Strassen-Coppersmith-Winogard Multiplication [22] and the Strassen-Bodrato Multiplication [23] are expected to give the best performance as their asymptotic run times in O(22.374).

From experimental results, the Matrix Multiplication with Up- per/Lower involvement outperformed other matrix operations     TABLE I. SAMPLE CODES OF HADOOP MAHOUT CLASSIFICATION SERVER AND CLIENT  Classification SVM server (the initiator, P1): ZooKeepr zk = new ZooKeeper(< host >, < port no >, null); Ops modelHandler = new Ops(); ...

...

...

TServerSocket socket = new TServerSocket(< port no >); Classifier.Processor processor = new Classifier.Processor(modelHandler); TServer server = new ThreadPoolServer(new TThreadPoolServer.Args(socket).processor(processor)); server.server();  Classification SVM clients (P2, P3, ...Pk): ZooKeepr zk = new ZooKeeper(< host >, < port no >, null); List servers = zk.getChildren(Server.ZK CURRENT SERV ER, false, null); String hostname = servers.get(i); Connection conn = new Connection(< host >, < port no >); ...

...

...

conn.close();  by reducing at least one-third computational time.

Features Search: We propose a method to allow the initiator, P1, to request features from parties involved in the mining activity. The initiator deserialises a proposed XML schema (Listing 1) into a binary format and send it to requested parties, P2, P3, ..., Pk. Each requested party serializes the receiving schema into a Java object. Based on instance names and features requested as in the Listing 1, each party provides features using THL algorithm 1 with the CRSSPP protocol (Figure 2). In the end of computation, the initiator learns nothing of the involved parties data.

Listing 1. The Proposed XML Schema <?xml version="1.0" encoding="utf-8"?> <dataset>  <instance_name> [Dataset_Name|Datase_ID]  </instance_name> <!--more than one feature id/feature name requested is separated by comma--> <features_requested>  [Feature_IDs|Feature_names] </features_requested>  </dataset>  Hadoop Mahout Integration: THL algorithm can be inte- grated into the Hadoop Mahout Machine Learning Library with a minimal effort as depicted in Figure 5. To avoid the collu- sion problem, the CRSSPP works seamlessly with the THL algorithm in the Hadoop Mahout. In the Hadoop environment, the arbitrary partitioned data can be retrieved from Relational Database Management System (RDBS), NoSQL and log files.

We can use different open-source tools to read/write analytical processed data from heterogeneous sources like Sqoop for RDMS, MagooDB Connector for NoSQL and Flume for data log files. In Hadoop Mahout, classification SVM server can act as the initiator, P1, using ZooKeeper instead of Java RMI.

The classification SVM server gathers data from classification clients, P2, P3, ..., Pk and computes the gram matrix, G, using THL algorithm as depicted in Table I.

0 2000 4000 6000 8000 10000 12000 14000  0.5   1.5   2.5 x 10   Number of points  T im  e( m  s)  Methods comparisons      Method1 Method2 Method3 Method4 Method5  Fig. 6. Run time for different methods.



V. EXPERIMENTS  In this section, we evaluated the proposed THL algorithm with various matrix operations and presented their run times against Vaidya et al. method [18]. We used the tic-tac-toe dataset which is included in the SMO package.1 The tic-tac-toe dataset was also experimented in the state-of-the-art work [18].

It contains 958 data objects with 27 features. The number of points in this paper are represented features ? rows of tic-tac- toc records in which the data points are split arbitrarily among different parties. The experiments were done with three parties which each party has CPU 2.00GHz intel core 2 processor with 2G memory. We used 1024 bits as a security parameter in Paillier homomorphic encryption. The results for different methods are recorded in Fig. 6. In Fig. 7, it reports the run times where pre-random numbers (i.e., keys) were generated before the algorithms are actually executed. We note that the various operations under the THL algorithm and state-of-the- art Vaidya et al.method are labeled as follows:  ? Method 1: Vaidya et al. method  Matrix Operation: Matrix Multiplication (M1)  ? Method 2: The THL algorithm  Matrix Operation: Proposed Upper/Lower Multiplication (M2)  Matrix Operation: Strassen Multiplication (M3)  Matrix Operaton: Strassen-Winogard Multiplication (M4)  Matrix Operation: Strassen-Bodrato Multiplication (M5)  1http://www.datalab.uci.edu/people/xge/svm/     0 2000 4000 6000 8000 10000 12000 14000  0.5   1.5   2.5 x 10   Number of points  T im  e( m  s)  Methods comparisons with pre?generated numbers      Method1 Method2 Method3 Method4 Method5  Fig. 7. Run time for different methods with pre-generated numbers.

TABLE II. RUN TIME FOR DIFFERENT METHODS.

Points M1(ms) M2(ms) M3(ms) M4(ms) M5(ms) 540 370,471 64,073 64,326 64,426 64,195  1350 2,789,284 227,171 237,855 237,984 235,523 6750 52,872,464 5,649,606 5,943,309 5,806,669 5,863,947  13500 220,777,286 25,855,619 34,105,912 33,965,783 34,013,433  Figs. 6 and 7 show that the proposed method signifi- cantly improves the efficiency, comparing with state-of-the- art Method 1. Among the proposed method of the THL algorithm, Method 2 slightly works faster than other three matrix operations, while the other three operations come near in run time.

TABLE III. RUN TIME FOR DIFFERENT METHODS WITH PRE-GENERATED NUMBERS.

Points M1(ms) M2(ms) M3(ms) M4(ms) M5(ms) 540 370,471 63,951 64,290 64,440 64,106  1350 2,789,284 226,565 232,488 235,326 233,794 6750 52,872,464 5,576,520 5,898,840 5,711,527 5,742,218  13500 220,777,286 25,500,580 33,844,312 33,587,155 33,592,112  TABLE IV. RUN TIME FOR DIFFERENT METHODS ON CAR?S CATEGORICAL DATASET .

Points Method2(ms) Method3(ms) Method4(ms) Method5(ms) 10368 20,274,977 23,754,412 24,652,132 24,100,271 20736 513,886,542 521,612,751 524,162,351 524,741,421  TABLE V. COMPARE MINING RESULTS OF SVM-SMO [27] ON DATA WITH/WITHOUT PRIVACY-PRESERVING .

Method Dataset Non-privacy-preserving Privacy-preserving SVM-SMO [27] Tic-tac-toe 1 w1 w1 SVM-SMO [27] Car 2 w2 w2  From Table II, we can see that for the data point 13,500, the run time is around 61hours in the Vaidya et al. method (Method 1) while THL algorithm only requires 7.5 hours.

Each point took 1.29320ms in Vaidya et al. method (Method 1) compared to 0.15256ms in THL algorithm (method 2). It significantly improves the run time and increases practicabil- ities of THL algorithm in the cloud. Pre-generated number technique can further reduce computational cost, especially when data points becomes very large, observed from Tables II and III. THL algorithm can be applied to categorical datasets by transforming categorical values into binary values. Taking  an example of categorical values for ?small?, ?medium?, ?big?, we can convert them into ?small? 01?, ?medium? 10? and big ? 11? respectively. Table IV shows the experimental results of THL algorithm run on categorical car dataset. 2.

We showed that the proposed PPSVM method can mim- imise data noises and circumvent the colluding problems. We computed data mining results of gram matrices, Gs, produced by proposed PPSVM method and conventional SVM, using SVM-SMO algorithm [27]. Both of the results are nearly identical presented in table V

VI. CONCLUSIONS  In this paper, we propose a highly efficient privacy- preserving Support Vector Machine (PPSVM) to separate objects or attributes into different classes over arbitrary parti- tioned data. We evaluate to show that our proposed method is more run time efficient and applicable on large data sets while preserving truthfulness of values at the record level without privacy violation. Furthermore, the pre-generated random key technique integrated into THL algorithm can be further to improve computational efficiency where the random keys can be retrieved from memory without being generated on the fly.

THL algorithm is not tightly-coupled to any machine learning technique, we will continue to investigate and apply THL algorithm in other classification techniques like neural network and decision tree in the Hadoop Mahout.

