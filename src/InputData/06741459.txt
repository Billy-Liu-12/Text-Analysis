A Paravirtualized File System for Accelerating   File I/O

Abstract? With several new virtualization technologies, a virtual machine has gradually achieved higher performance.

However, I/O-intensive workloads still suffer from performance deg-radation due to CPU mode switching and duplicated I/O stacks. In this paper, we propose a framework for improving file I/O performance in a virtualized environment, which consists of a paravirtualized file system, a shared queue, and an I/O-dedicated thread. The key idea is to make file I/O requests handled without mode switching and to bypass a guest I/O stack. We implemented a prototype and measured its performance. The results show that our framework gives 1.2-1.6 times better performance than virtio, and most of vmexits are eliminated.

Keywords? I/O virtualization; paravirtualization; file system; vmexit; I/O stack

I.  INTRODUCTION With the development of high-performance virtualization  technologies, virtualization is now being widely used in both desktop and server system. The advance of virtualization technology can be categorized into two types: software-based approaches and hardware-based approaches. The hardware- based approaches take advantages of devices which are aware of virtualization. The software-based approaches do not require special features of hardware but need a change of guest kernel.

Nowadays, in terms of I/O device virtualization, both hardware-based approaches and software-based approaches have been actively studied, whereas hardware-based ap- proaches are dominant in processor and memory virtualization.

SR-IOV [1] and IOMMU [2] are well-known hardware- based techniques of I/O virtualization. Providing the interface through which a virtual machine can directly access physical devices, these techniques give high performance close to that of non-virtualized systems. However, there are few devices adopting the new techniques currently, because they require high product cost. Besides, data servers and cloud service centers are unwilling to adopt hardware-based I/O virtual- ization because it can weaken key characteristics of virtualization, such as portability and flexibility. In contrast, software-based approaches make it easy to encapsulate virtual machines, i.e., live migration or VM snapshot. They also allow virtual devices with no physical counterpart to be assigned to guests, e.g., a disk image file as a block device. But software-  based approaches practically showed worse performance than hardware-based ones.

In this paper, we concentrate on two major causes of performance degradation in software-based I/O virtualization.

The first is that a hypervisor should interfere in guest I/O process because a virtual machine has no privilege to access physical I/O devices. The second problem is the nesting of I/O stacks. A host and many guests have their own OS, so also have their own I/O stack. To resolve these problems, we propose a new framework for accelerating file I/O. It consists of 3 components: a paravirtualized file system, a shared queue, and an I/O-dedicated thread. With a paravirtualized file system, we can avoid the execution of a redundant I/O stack on the guest side. An I/O-dedicated thread and a shared queue help virtual machines to reduce the occurrence of vmexits.

To give details of our approach, this paper is organized as follows. In Section II, we overview related work. Section III describes our framework and its components. The experimental results of our prototype is presented in Section IV, and Section V discusses some issues relating to our approach such as compatibility and safety. Finally, we conclude this paper in Section VI.



II. REALATED WORK One of the prevalent device driver frameworks for  virtualization is virtio [3], which has been included in mainline of Linux kernel since version 2.6.24. In most cases, full emulation of I/O devices is complicated and inefficient, so virtio relatively gives better performance than typical device emulations. And if I/O requests are generated frequently, virtio coalesces the requests for increasing I/O throughput. At the point of processing the requests, however, it is required to switch a CPU mode from a guest to a host, i.e., vmexit, which causes significant overheads. Especially, the faster a device operates, the more vmexits affect the performance degradation.

It implies virtio cannot control the occurrence of vmexits well, whereas our proposed approach remarkably decreases the occurrence of vmexits, which is explained in Section IV.

Some previous studies suggested structures using dedicated CPU cores for accelerating performance of virtual machines.

SplitX [4] suggested a new virtualization model that a hypervisor ran on dedicated CPU cores. With this model, they decreased a direct cost and an indirect cost caused by vmexits. This research was supported by the IT R&D program of MKE/KEIT  (10041244, SmartTV 2.0 Software Platform) and Next-Generation Information Computing Development Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (2010-0020730).

Shared Queue  File System  I/O Scheduler  Device Driver  Block Device  I/O-dedicated Thread  Paravirtualized File System  I/O scheduler  Device DriverGuest  Guest Kernel  VM emulator  Host Kernel  Host  H/W  ApplicationGuest User  Block I/O workflow Fig. 1. Structure of the proposed I/O framework and block I/O workflow  On the contrary, our approach uses dedicated threads to process block I/O requests. VPE [5] and ELVIS [6] also proposed dedicated cores for handling I/O requests. Their studies demonstrated that using dedicated cores for I/O was an effective way to reduce vmexits in a virtualized environment.

While these two studies aimed to improve the performance of general I/O devices and modified the device driver to be aware of virtualization, we only focus on a block I/O and a file system layer. Our approach not only effectively reduces vmexits but also bypasses a redundant block I/O stack on the guest side, so we can achieve better performance enhancement in file I/O.

VirtFS [7] previously proposed the concept of a virtualization- aware file system. Their used a client-server model like a network file system, and utilized virtio. Both VirtFS and our approach have the same spirit of optimizing I/O procedure using virtualization awareness in the file system level, but we further take account of the overheads caused by vmexits.



III. ARCHITECTURE DESIGN In a virtualized environment, minimizing the occurrence of  vmexits is one of the best ways to increase system performance [8]. To do this, we suggest an I/O framework which enables virtual machines to interact with a hypervisor without a vmexit.

Another virtue of the design is eliminating redundant I/O stacks. In a hosted virtual machine environment, there are more than one operating system; one is for the host and the others are for guests. Thus, there also exists several block I/O stacks.

Each block I/O stack includes an I/O scheduler and a device driver. The structure of our framework consists of a virtualization-aware file system, a shared queue, and an I/O- dedicated thread. The overall system structure and block I/O workflow are illustrated in Fig. 1.

A. Paravirtualized File System A major role of the paravirtualized file system is to bypass  guest I/O stack. When an application requests file I/O, a paravirtualized file system passes the I/O request to the host instead of the block layer of guest kernel, and waits for completion. After the request is processed in the host, the file system takes the result and acts the same as if it was handled in the guest?s block layer. A file system is the first step of handling a file I/O requested from user applications. If I/O requests are passed to the host at this point, the rest of the guest I/O stack can be bypassed. Compared to the I/O process in a non-virtualized system, duplicated I/O stacks are one major cause of degrading performance in a virtual machine.

Especially, in case the virtual machine utilizes image files as block devices, merging and sorting operations of an I/O scheduler may give negative effects to the I/O process in the host. It is because the consecutive disk blocks from a viewpoint of the guest are no longer consecutive in the host. It depends on the file system type of both the host and the guest as well as the format of the disk image file. Consequently, eliminating a redundant I/O stack on the guest side can improve file I/O performance.

B. Shared Queue Between Host and Guest The shared queue is used to interact between the guest and  the host. Originally an I/O operation triggers a vmexit because  it attempts to access physical block devices but the virtual machine has no privilege to access them. When the shared queue is used, however, I/O operations are substituted with memory operations which can be directly executed in a virtual machine. In this context, the memory address should be appropriately translated among guest virtual addresses, guest physical addresses, and host virtual addresses.

After block I/O requests are enqueued from the para- virtualized file system, the I/O-dedicated thread handles them and updates the completion state in the queue. Finally, the file system dequeues the requests marked with completion and continues the remaining I/O process. For sharing a queue, both the VM emulator and the virtual machine should know the memory address of the queue. In a typical hosted virtualization environment, it is not difficult to share memory addresses because the emulator and its virtual machine have the same address space.

C. I/O-dedicated Thread With our framework, the kernel of guest OS delegates  block I/O process to the I/O-dedicated thread. Through the shared queue, I/O requests are sent to the host without trigger- ing a vmexit. But there remains one more problem; notifying the host of new requests. For this, virtio intentionally raises a vmexit [3]. To avoid incurring vmexits, we introduces an I/O- dedicated thread, which monitors the queue whether new requests are arrived or not. To check it as fast as possible, the thread uses polling mechanism. As a polling thread occupies one CPU core all the time, it has a drawback in terms of the CPU usage, but it can improve I/O performance when I/O- intensive workloads are running on the system. In addition, block I/O operations are asynchronous, so the individual I/O- dedicated thread is able to handle several I/O requests from many VMs concurrently.



IV. EVALUATION In this section, we will evaluate the proposed framework.

To verify performance improvement of our approach, we implemented a prototype and measured the I/O performance against virtio. We also analyzed the overheads of vmexits in both our framework and virtio.

128 512 1024 2048 4096 16384 32768  Th ro  ug hp  ut (k  B/ s)  File size (kB)  Random Read  host virtio ours       128 512 1024 2048 4096 16384 32768  Th ro  ug hp  ut (k  B/ s)  File size (kB)  Sequential Write  host virtio ours       128 512 1024 2048 4096 16384 32768  Th ro  ug hp  ut (k  B/ s)  File size (kB)  Sequential Read  host virtio ours       128 512 1024 2048 4096 16384 32768  Th ro  ug hp  ut (k  B/ s)  File size (kB)  Backward Read  host virtio ours   Fig. 2. File I/O performance of IOzone benchmark  A. Prototype Implementation We used FUSE to handily utilize a file system and  implemented our scheme based on fuse-ext2. Fuse-ext2 is an implementation of ext2 file system based on FUSE. The overall mechanism of the modified fuse-ext2 is the same as that of original fuse-ext2, except the modified one sends I/O requests to the shared queue rather than block layer of guest kernel at the end of the internal process. A file system implemented with FUSE, of course, has partial differences with the kernel-level file system, but it does not seems to be crucial in verifying performance improvement of our approach. We used Native Linux KVM tool for emulating virtual machines, and added a shared queue and an I/O thread to the emulator. Non-blocking concurrent queue algorithm is applied to the shared queue for consistency and scalability, because it is shared among the I/O thread and many virtual machines.

B. Experimental Setup Our test machine is equipped with Intel i5 3570 (4 cores,  3.4 GHz) and 4 GB memory. Samsung 470 Series SSD is used for experimental disk and the other programs including OS are installed on other disks. We used KVM [9] as a hypervisor and Native Linux KVM tool for emulating virtual machines. We assigned 1 virtual CPU, i.e., vcpu, and 2 GB main memory to the guest machine. The OS of both the host and the guest are Ubuntu 12.04 with 3.9.0 kernel.

For fair experiments, we create three 16 GB partitions on 64 GB SSD. The first partition is used to test the file I/O of host system, and the second and the third partitions are respectively used for virtio and our framework in the virtual machine. In addition, to avoid the effect of page cache, all file I/O operations were executed with O_DIRECT option.

C. Performance Analysis We measured file I/O performance in a host, a guest using  virtio, and a guest using our framework. To compare our framework with the others, all environments are based on fuse- ext2; the host and the guest with virtio utilize unmodified fuse- ext2, and guest with our approach used modified one mentioned above. We used IOzone benchmark to evaluate file read/write in various patterns, such as sequential read/write, random read/write, and backward read. Fig. 2 shows the results of them. Ours gives better throughput than virtio in all cases, and ours gives very similar performance to host within the margin of error. The performance of ours sometimes exceeds that of host, because I/O-dedicated thread exclusively performs I/O process in our framework. It can work to advantage of our scheme in terms of cache hit ratio and context switching overheads. In the case of sequential read/ write, our framework achieves 40%-60% better performance than virtio. Due to the direct I/O, which makes block I/O bypass page cache, the result of write operations is similar to that of read operations.

In terms of random read/write and backward read, ours improves by 20%-25%. Random write also shows similar results to random read, so we leave it out of this paper. A common block device has the characteristics that non- sequential I/O is slower than sequential I/O, so random and backward I/O are less affected by the overheads of vmexits. As a result, the performance of non-sequential operations is less improved.

D. Occurrence of vmexits One advantage of our approach is that file I/O does not  trigger a vmexit. To verify this, we further analyzed the pattern of vmexit occurrences in following cases: (1) in idle state, (2) when I/O-intensive workload is running on a guest using virtio,      Fig. 3. Detailed analysis of vmexit  (3) when the workload is running on our framework, (4) the workload is running on our framework and with IRQ affinity setting. We used perf tools [10] to count detailed events of vmexits and the results are illustrated in Fig. 3. In idle state, there are almost no vmexits, and the overheads caused by vmexits is just 0.0055%. In case of virtio, the overheads are about 36ms per every second. In our framework, on the other hands, most of I/O instruction, MSR write, and NMI exception are disappeared. However, compared to the idle state, vmexits wasted about 13.9ms, which are mainly caused by external interrupts. It is due to hardware interrupts; when a physical core running on the virtual machine receives hardware interrupts, it switches to host mode for handling them. In case of our framework, I/O requests are processed by the I/O- dedicated thread instead of the vcpu, so the physical core stays in the guest mode most of the time. To prevent this problem, we set IRQ affinity to deliver the interrupts of block I/O to the I/O-dedicated thread. As a result, the number of external interrupt is reduced by 87%, and finally our framework saves about 34ms per second against virtio. The remaining external interrupts are caused by local timer, rescheduling, and others.

These interrupts can also be removed with IRQ affinity setting but it seems to be inordinate configuration.



V. DISCUSSION As we previously mentioned, the paravirtualized file system  is needed in our proposed framework, and it means the guest OS should be modified. So our framework cannot be applied to proprietary OSs, which are restricted from modification, e.g., Microsoft Windows. On the contrary, Linux allow to add a new file system as a module, so the paravirtualized file system is utilized without re-compiling the kernel. Additionally, if the file system of our framework is implemented based on a pre- existing file system, they can be fully compatible with each other. For example, if the paravirtualized file system is based on ext4, our framework can manipulate data written under ext2, ext3 or ext4. It is because the modifications of our approach are altering the destination of block I/O requests, not impacting on the intrinsic mechanism of the file system.

We need to address security issues of our approach because it delegates the partial role of the guest to the host. But originally a virtual machine cannot directly execute I/O operations, except using a SR-IOV device. In other cases,  block I/O requests from a virtual machine should be handled by the host, in one form or another. Our approach also hands the block I/O requests over to the host. Through the paravirtualized file system, only the point of delivering I/O requests is put forward, and it would not cause an additional sacrifice of safety or isolation.

Another consideration is request coalescing. Some previous studies for improving I/O performance introduced batching several I/O requests into one bundle [3], [6], [11]. In the case of virtio, a vmexit will be triggered in order to handle each I/O request, and it incurs a considerable delay. Coalescing can improve overall I/O throughput, but it may make latency of each request longer. The proposed approach in this paper, in contrast, doesn?t involve a vmexit during the I/O process so the delay is minimized, by sacrificing one core to poll the shared queue. Thus, our approach can achieve the performance close to non-virtualized system without coalescing I/O requests, and also does not sacrifice the latency of file I/O.



VI. CONCLUSION In a virtualized system, I/O-intensive workloads suffer from  the two major causes of performance degradation compared with non-virtualized environment. First, access to physical I/O devices leads to vmexits, and second, there are redundant I/O stacks because of the nested OS. In this paper, we proposed a new framework to improve file I/O performance by removing the two causes mentioned above. To deliver I/O requests without a vmexit, we used a shared queue and an I/O-dedicated thread. The I/O-dedicated thread uses a polling mechanism to promptly handle requests after arrived in the shared queue. Our framework also adopted a paravirtualized file system, which makes I/O requests bypass the redundant I/O stack of the guest, so we achieved further enhancement of file I/O. The polling mechanism makes the I/O thread occupy one CPU core all the time, but it will put little effect on the system because nowadays the number of cores on a single CPU has been increased gradually. So our proposed approach can be persua- sive if it provides reasonable performance improvement. To verify this, we implemented a prototype and evaluated our approach. The result showed that our proposed framework improved the performance by 20%-60% against virtio, and most of vmexits are eliminated. Moreover, if hardware itself get better performance, the frequency of the vmexit will be increased, so our approach will achieve further improvement.

Currently, we are investigating how to apply our approach to the kernel-level file system, ext4. It helps us to verify our approach more practically and accurately. In the future, we plan to study how to efficiently handle the requests generated by numerous VMs.

