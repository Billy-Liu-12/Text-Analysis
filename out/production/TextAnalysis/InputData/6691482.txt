Application of detection and recognition algorithms to persistent wide area surveillance

Abstract?The persistent airborne surveillance of large  geographical areas is now a viable proposition. As well as providing cues to moving objects, it presents new oppor- tunities for understanding the behaviours and motivations of people, both individually and collectively. Exploitation of these huge collections of imagery (a facet of the Big Data challenge) requires more effective tools to derive and abstract useful information to cue the analyst. This paper describes a new system which brings together a number of techniques: moving target detection; tracking; recognition and photogrammetry, to address wide area surveillance prob- lems. We provide a first report on the demands this places on component parts and interfaces. Significantly, we adopt international interoperability standards, particularly with regard to video metadata, to constrain the solution space. We also describe new performance improvements to the video moving target indication and photogrammetry algorithms as well as analysing for the first time the performance of our integrated target model matching capability in our automated system.



I. INTRODUCTION  Video imagery collected from a loitering airborne plat- form provides a powerful means of enhancing the tactical ground picture and improving situational awareness in general. An important reason for this is its capacity to capture movement and in doing so convey scene dynamics over time. The potential offered by Wide Area Surveil- lance (WAS) video sensors to Defence is the persistent surveillance of large geographical areas. This facilitates the modelling of behaviours and pattern of life, and hence the detection of interesting activities. However, the huge amount of imagery collected presents problems for imagery analysts who up until now have generally been able to monitor individual video feeds with minimal use of software tools (although video moving target indication tools are now becoming more widespread).

This paper consolidates disparate pieces of detection and recognition research to address the types of problems that will present themselves with the development of these wide area, persistent surveillance sensors. In particular, four discrete software systems; the Defence Science Tech- nology Organisation?s (DSTO) Analysts? Detection Sup- port System (ADSS) [1] and developmental registration, model matching and photogrammetric applications have been pipelined to solve a non-trivial intelligence problem.

The analyst, though still ?in the loop?, is now exempted  from having to continuously assess the interest value of every moving target (object) in the scene for the duration of the surveillance. This paper also reports significant improvements to several of the component algorithms, particularly to Video Moving Target Indication (VMTI) [2] implementation and photogrammetry.



II. OVERVIEW  This paper builds on previous work, in which re- searchers introduced a multi-INT framework [1], [3], demonstrated its application to the detection of moving targets in video [2], and described how systems might be interconnected in a metadata standards compliant way [4] to promote interoperability and phased, federated exploitation. More recently interactive photogrammetric techniques were applied to this metadata-rich video for de- tailed content analysis [5]. Subsequently, researchers have also reported on a model-matching vehicle recognition al- gorithm [6] for classifying well-defined, rigid objects such as vehicles. Potential applications for the latter include hand-over, vehicle tracking, traffic monitoring etc.

Important aspects of this work are that we leverage motion imagery interoperability standards, and ADSS, and that we present a new end-to-end video exploitation system where algorithms in each subsystem add value for the analyst. This system is then exercised in our quasi real-world scenario and an analysis of the system?s performance is presented.

A. Motion Imagery Standards  Video metadata is considered vital for effective ex- ploitation of ISR (Intelligence, Surveillance and Recon- naissance) motion imagery because it provides geotempo- ral context [4]. That is, the metadata allows the analyst to identity exactly where in the world and when in time depicted events occurred. Until recently, the lack of accurate or complete absence of video geolocation limited post mission fusion or association of the video with other sources of information. Digital metadata for video is now in widespread use. It is increasingly compliant with NATO (North Atlantic Treaty Organisation) STANAG (Standard- isation Agreement) 4609, and Motion Imagery Standards Board (MISB) standard 0601 [3] and has become the  978-1-4799-2126-3/13/$31.00 ?2013 Crown    accepted local data set within Defence for the transmission of metadata elements within motion imagery.

As well as being used to describe mission, platform, sensor and collection configuration, the metadata can store image content descriptors which, when populated and interrogated offer the potential to cue analysts, reduce bandwidth requirements - useful for realtime transmission over a Tactical Common Data Link (TCDL), and to facilitate staged (or collaborative) exploitation. Metadata is particularly useful for recording events that are of little or no immediate interest but which may become interesting at some later point.

If we consider the complexity dimensions of Big- Data type analytical problems: data variety, velocity and volume, it is clear that video metadata is useful to cate- gorise variety, manage velocity by standardising systems interfaces and decrease volume by abstracting important information from potentially sparse video data. Video metadata provides a means of accessing key video imagery descriptors, following which other forms of analysis (by human or machine) can be instigated [3].

In this work the metadata is used to provide esti- mates of time and geopositioning, ground sample distance (GSD) and viewing geometry. We also use standards compliant metadata to bound registration computation.

GSD is used for target filtering and model selection, and viewing geometry for constraining the vehicle recognition searches. Target attributes may be stored in the relevant VMTI metadata fields to facilitate subsequent analytics and forensics [5].

B. Analysts? Detection Support System (ADSS)  The techniques described in this paper address differ- ent aspects of the detection, recognition and identification problem for moving objects in video. Ingestion of video and associated metadata and detection of moving targets was managed by ADSS.

ADSS was originally conceived to assist imagery analysts in the detection of relatively small objects in large volumes of imagery [1]. Early work focused on the analysis of wide area synthetic aperture radar (SAR) imagery with the system growing over time to support a collection of Defence and open imagery standards, data processing algorithms, hardware interfaces, compute par- allelisation tools, user interfaces and software engineering infrastructure.

Contemporary Defence requirements mean that ADSS is now positioned as a scalable GEOspatial INTelligence (GEOINT) data processing framework which processes imagery and geospatial data.

C. Scenario  A scenario was developed to demonstrate the per- formance of our pipelined subsystems (see Figure 1) in an operational setting. End-to-end exploitation of a contiguous video collect was important to explore our subsystems? interfacing requirements and robustness, and the performance of the algorithms chosen. Our experience with pipelined image processing in the past reinforces the need for this type of exploration; algorithmic corner cases or nuances of the data which have negligible immediate  effect may cause error, so it is necessary for testing to occur end-to-end.

The scenario was an airborne surveillance activity with appropriately choreographed ground movement. The input imagery was collected from an airborne platform (the Defence Experimentation Airborne Platform (DEAP)) pictured in Figure 4, mounting a L3 Wescam MX20HD turret designed for ground-surveillance. The turret contains three, approximately co-boresighted sensors. One is an electro-optical (EO) visible waveband sensor with wide field of view (FoV) optics. This is supplemented with an EO visible waveband sensor with narrow field of view optics, and a thermal infrared (IR) video sensor, also narrow field of view. The turret and surveillance aircraft contain accurate orientation and positioning sensors and an onboard surveillance management system (SMS).

The SMS multiplexes KLV (Key Length Value) video metadata with an MPEG Transport Stream (MPEGTS) encoded from each video channel. In an operational sce- nario a downsampled MPEGTS might be transmitted to ground based portable video playback devices. Although processing of imagery from our scenario was performed offline, with appropriate development effort the techniques we use are suitable for realtime analyst cueing.

The ground activity was conducted with the support of the Australian Defence Force and involved the surveillance of seven vehicles: a station wagon, a Land Rover, a Uni- mog light truck, a Mack heavy truck, an M113 armoured personnel carrier, a Bushmaster protected mobility vehicle and a (stationary) Leopard tank.

A subset of the vehicles were observed circling through the local road network by the wide field of view camera.

VMTI and target tracking algorithms were applied to motion imagery (MI). When a vehicle carried out an elementary activity of interest, such as failing to stop at a checkpoint, the higher resolution camera was cued to recognise the type of target. The two sensors are notionally co-boresighted, however, to map from pixel locations in the wide FoV image to the narrow FoV image, alignment as well as scaling must be considered. Once cued, the target is extracted (or segmented) and an image chip was produced. A vehicle model matcher then takes grey level image chips as its inputs. Classification performance is discussed in Section III-G5. Vehicles were considered to be of interest when entering a particular location.

We provide some example technical analysis using interactive photogrammetry to characterise the tank. It is worth noting that after analysis the analyst has the option to review the positional history of the vehicle by automatically backtracking through the moving target associated KLV metadata rather than reprocessing the video itself.



III. COMPONENT MODULES  The functional components of the processing chain are outlined below. Please see Figure 1 for a depiction of the systems, their interconnection and associated workflow.

A. Video Moving Target Indication (VMTI)  The VMTI algorithm pipeline is founded on a back- ground modelling technique where the background model    Fig. 1. The system pipeline indicating subsystem interconnection with image product examples.

is computed by maintaining a sliding temporal window of recent frames and recording the distribution of corre- sponding registered pixels [2]. The inter-frame registration necessary to remove camera motion and compute the back- ground model is achieved using the Kanade, Lucas and Tomasi (KLT) feature tracker. Movement at a particular pixel location in a frame is based on the illumination intensity value with the corresponding distribution of pixel values in the background model.

The legacy system [2], [7] was implemented in ANSI C; however, live processing of 1080p 30fps video (and rapid off-line processing of large datasets) necessitated a faster implementation. The majority of the required speedup was achieved by implementing paralleliseable KLT and background generation algorithmic components with Nvidia?s CUDA language and processing the data on Nvidia Tesla accelerators. Other speedup improvements we are reporting for the first time are that KLT feature point sorting was replaced by a faster CUDA thrust sort [8] and the RANSAC algorithm used in KLT point selection was replaced by a locally optimised RANSAC [9]. Other detection performance improvements were achieved by computing the background median over every frame pixel in the temporal window rather than a previous method which relied on computing the median over the least varying third of pixels.

B. Probabilistic Multi Hypothesis Tracker (PMHT)  The PMHT is a point tracking method [10] derived by the application of Expectation-Maximisation (EM) to multi-target tracking [11]. The strength of the algorithm is that it performs measurement to target data associa- tion with linear computation complexity, whereas other approaches enumerate the set of permutations of measure- ments and targets, which is infeasible for more than a few targets.

In this problem, PMHT has been used to associate together a sequence of detections from a particular target  and reject false alarms, where each input measurement represents the centre of gravity of a target segmentation, the derivation of which was described in previous work [11].

PMHT is an iterative algorithm that alternates between data association and state estimation. In the association step, an assignment weight is calculated for each track and measurement. The weight for track m and measurement r at time t is defined as  wmtr = ?tmp(ztr|xtm)  ?Mp=0?tpp(ztr|xtp) , (1)  where ?tm is the prior probability of track m forming any given measurement at time t, and p(ztr|xtm) is the measurement probability density function (PDF) for track m, which, in our case, we assume to be a known linear Gaussian function. There are M tracks, with ?t0 and xt0 being the prior and state of the clutter (noise) model, respectively. The target states above are obtained from the previous EM iteration or by predicting the previous temporal state on the first iteration. For the VMTI data tested thus far, a uniform clutter PDF has been sufficient, and the assignment prior for all targets has been assumed to be constant, i.e. ?tm = ?? for m > 0. The value of this prior can be adjusted as a tuning parameter.

In the estimation step, the target state estimates are independently updated using a bank of parallel Kalman filters. The input to each Kalman filter is a synthetic measurement formed by taking the weighted mean of the input data. The measurement for track m and its covariance are given by  z?tm = (?rwmrt)?1?rwmtrztr, (2)  R?tm = (?rwmtr)?1R, (3)  where R is the covariance of the target measurement PDF, p(ztr|xtm), which is a tuning parameter and reflects the variation in object location due to noise effects.

The algorithm must automatically detect new targets,    Fig. 2. Pixel detections in EON image (red) compared to the EOW detections projected to the EON coordinate frame (blue).

and terminate tracks when targets are no longer present.

The basic approach is to over-model the data and then reject insignificant tracks. Numerous methods exists for measuring track confidence [11]. In our implementation track confidence is defined as the time filtered assignment weight sum  Ctm = ?C(t?1)m + (1 ? ??rwmtr). (4)  C. Sensor Cueing  Contemporary surveillance requirements mean that it is desirable to monitor a large geographical area; subject to the resolution constraints imposed by the need to resolve the objects of particular interest. When a target is considered interesting in the large surveilled area a high resolution image of it is typically obtained by cueing a narrow field of view sensor. In the example dataset we use for this work our video surveillance sensors are co- boresighted with only a small alignment error, see Figure 4. The workflow and systems we present in this paper are generalisable and will be exploited on a very high resolution fixed axis sensor and 1080p pan tilt zoom camera pairing in future work.

Registration is the process of geometrically aligning two or more images so that they can be compared pixel- by-pixel [12]. Popular methods involve finding an optimal transformation that maximises the correlation between pixel intensities in two images (area-based), while other methods find key feature points in both images and then determine the optimal transformation between correspond- ing features (feature-based). In this work, reliable results were obtained using an area-based method provided by Elastix, software based on the Insight Segmentation and Registration Toolkit suite of image processing routines [13]. Elastix was used to calculate the transformation between the wide FoV image and the narrow FoV image.

This meant the pixel space location of any target in the narrow field of view image can be determined from its location in the wide field of view image. In general, if the geoposition of pixels in one of the images is known then registration of other imagery against it means geoposition can be correlated.

Figure 2 compares the location of the detections ob- tained from the EON (electro-optic sensor narrow FoV) video (red points) to the EOW (electro-optic sensor wide FoV) points once they have been projected onto the EON image (blue points). Note the better object delineation of the projected EOW detections over the EON detections.

This is an artifact of the original VMTI algorithm design.

D. Vehicle Extraction  Our vehicle recognition algorithm requires an image chip which tightly bounds the target vehicle. Optimally, the vehicle should also not be occluded. We found that VMTI algorithm did not provide sufficiently precise cues or the segmentation quality our recognition algorithm required. To bridge this disparity we employ an interme- diate vehicle extractor which, when cued by the VMTI, segments the vehicle and fits a tight bounding box (Figure 3).

First, the vehicle is segmented (extracted) using piece- wise constant active contours [14]. These are applied to the greyscale intensity image and are based on curve evolution, the Mumford-Shah functional for segmentation [15] and level sets.

Consider, ? ? R2 as the image domain, and I : ? ? R  2 as a greyscale image. Then find an optimum contour C? that segments the given image I , and two constants c1 and c2 that approximate the image intensities outside and inside the contour C by minimising the energy function [14]  ?(c1, c2, I, C) = ?1 ?  outside(C)  |I(x) ? c1|2dx +  ?2  ? inside(C)  |I(x) ? c2|2dx + v|C|,  where ?1, ?2 and v are positive constants. The func- tions (C) and outside(C) represent the regions inside and outside the contour C, respectively. Hence, the optimum contour C? is given by  C? := arg min C  [min c1,c2  ?(c1, c2, I, C)]. (5)  This approach is insensitive to contour C initialisation but the constants c1 and c2 will not be accurate if the inten- sities outside and inside the contour C? are homogeneous.

Segmentation using this approach with inhomogeneous and homogeneous segments is shown in Figure 3(b) and 3(e), respectively.

To overcome the effect of homogeneous intensities on segmentation, we combine segments produced using an intensity image and texture-image by multiplying corre- sponding picture elements. Thus the optimum contour, C?, is given by  C? = C?I ? C?T , (6) where, C?I and C?T are optimum contours for intensity and texture images, respectively. The texture image, T , is derived using the greyscale image, I , and is given by,  Ti,j = ? ? ?k  p k log2pk , (7)  where, p k  is the kth bin of the histogram of intensity values within a window centered around the pixel I(i, j).

By combining intensity-based and texture-based active contour segmentations we achieve a close-fitting bounding box, enclosing the target vehicle, as shown in Figure 3(c) and 3(f). By fitting an ellipse, we can also approximate the pixel space length of the target vehicle.

(a) (b) (c)  (d) (e) (f)  Fig. 3. Input image, intensity-based segmentation, and fitted bounding- box as determined by intensity and texture based active contour segmen- tation on two targets.

E. Target Characterisation and Filtering  The object segmentations (or delineations) are now of sufficient quality to allow the shape of the vehicle and the underlying appearance to be characterised more precisely.

The target width (see Figure 4) available in the video metadata means GSD can be determined and then used to derive actual lengths. For convenience, lengths of the major and minor axes of the fitted ellipse are used as estimates of the vehicle length and width, respectively.

However, an alternative would be to compute the eigen- values of the principal axes from the object segmentation using singular value decomposition.

F. Model Selection  The search for an appropriate match is constrained within intervals of elevation and azimuth angles to max- imise the retrieval speed and efficiency. The orientation, ?, of the vehicle in the scene is given by the direction of motion of the vehicle as provided by the VMTI tracker. However, estimating the elevation angle of the sensor requires some effort. Video metadata supplies the platform?s absolute yaw, pitch and roll angles along with sensor?s yaw, pitch and roll angles relative to airborne platform. To solve for the sensor?s absolute pitch with reference to a target object involved two 3D transforma- tions, transforming into a North-East-Down (NED) frame  Fig. 4. Platform, sensor and imaging footprint geometry.

and then computing from a NED frame. For our work, we require an approximate elevation angle of the sensor relative to the target object, hence a more direct approach was implemented. Considering A = [Alat, Along, Aalt] as a vector consisting of latitude, longitude and altitude of the airborne platform?s position, F = [Flat, Flong, Fhgt] as a vector consisting of latitude, longitude and height above sea-level of the point at the centre of the frame and the location on the ground in the direction of the sensor denoted by G = [Glat, Glong, Fhgt], a vector consisting of latitude , longitude and height above mean sea-level. The three dimensional vectors, A, F and G are transformed into Earth-Centred Earth-Fixed (ECEF) co- ordinate space. Each point in ECEF co-ordinate system is a three dimensional vector denoting the location of that point, in metres, with respect to the Earth?s centre (0, 0, 0).

The x-axis points to the Equator at a longitude of 0?, the y-axis points to the Equator at an angle of 90? longitude and the z-axis points towards the North Pole. The sensor elevation angle, e, is calculated by computing the angle between vectors ?FA and ?FG using the cosine rule.

G. Target Recognition  This recognition and pose estimation approach is based on maximising correlation of features in the image and projections of the 3D models.

1) Construction of 3D Models: Each model is repre- sented by numerous 2D images of a 3D model taken at varying camera elevations (ranging from 0? to 90?) and orientations (ranging from 0? to 180?). These renderings were captured using the Blender [16] software package.

The process was largely automated by running custom 3DNP scripts [17].

The 3D models were sourced in a number of ways; commercial source [18]?[20], in-house model construction and scanning using the NextEngine 3D Scanner [21] and interactive model construction using VideoTrace [22].

2) Descriptors: Our intention is that the recognition framework be flexible, particularly with regard to the fea- tures used for matching. In this work the feature set used relies on a Histogram of Oriented Gradients (HoG) [23] to characterise object appearance, and the Edge Orientation Histogram (EOH) [24] to characterise object structure.

The image chip (be it part of a video frame or a model rendering) is divided into a 162 raster, and the descriptors computed for each cell. Within each cell, the features describe the local object appearance and structure, and collectively they capture their distribution over the whole object. The division of the chip into a raster of fixed dimension renders the feature set scale invariant. For this stage to be effective, precision is required during the previous target extraction stage.

3) Similarity Function: Consider Se,? as a 2D snap- shot of a 3D model captured at an elevation angle (pitch), e, and an azimuth (yaw), ?. Let T be the test image and f(.) be a function that produces normalised n-dimensional feature vectors. We propose to use either the Euclidean distance or Chi-square histogram distance between Se,? and T , in this case the former.

Fig. 6. A dendrogram depicting class separation  a) Euclidean: Euclidean distance between Se,? and T is given by,  dE(Se,?, T ) = 1?  ???? n? i=1  (fi(Se,?) ? fi(T ))2 (8)  where, f(.) produces a L2-normalised vector and ?x ? f(.), x ? 0.

4) Optimisation Strategy: The optimisation was per- formed using the robust and efficient downhill simplex [25] algorithm. Like most optimisation techniques it re- quires a reasonable starting position. Here, the sensor ele- vation, e, is extracted from the metadata, and an estimate of the vehicle azimuth is provided by the ADSS VMTI tracker.

5) Results: The dendrogram in Figure 6 shows the similarity between Classes in terms of their proximity in feature space (using the Euclidean distance metric). This similarity appears to map well to a human perception of vehicle similarity. For example, all Tanks (TA) are considered to be more similar to M113s (M3) than Freight Liners (FL), and all Land Rovers (LR) are considered to be more similar to Trucks (TR) and Vans (VA) than Tanks (TA) and/or Cars (CA). The dendrogram also shows an ability to generalise i.e. individual vehicles of the same class tend to cluster. Clustering on function (rather than structure) is more problematic, but the results are generally good. However, at a lower level, some inability to distinguish between certain types of Car (CA) and SUV (SU) suggests that those classes should be merged, or that individual car and SUV models should be labelled and used in a series of identification tasks. On a similar note, to make an effective analysts? tool, the functional class of vehicle may need to be split into several physical sub- classes. Consider Surface to Air Missile (SAM) launchers which are diverse in structure. A launch vehicle, when the launcher is not elevated, may resemble a truck; some SAM launchers have caterpillar tracks like tanks; other SAM launchers resemble armoured cars. If tasked to locate SAM launchers, a multiple vehicle identification or recognition strategy might be reasonable.

Performance was tested on a set of six hundred and forty samples of target vehicles1: one hundred and twenty five Cars (CA); one hundred and twenty five Heavy Trucks (HT); two hundred and sixty two Bushmasters (BM); twenty six Land Rovers (LR); sixty two Light Trucks (LT); thirty five M113s (M3) and five Tanks (TA). Each vehicle class is represented at various azimuths but within a small  1SUV (SU), Van (VA), SAM (SM), Freightliner (FL) and Utilities (UT) were not available for ground activity.

Fig. 7. Model classes: Tank (TA); M113 (M3); SAM (SM); Light Truck (LT); Heavy Truck (HT); Freightliner (FL); Utilities (UT); Land Rover (LR); SUV (SU) and Car (CA).

Fig. 8. Confusion matrix (Matrix is colour coded to represent the percentage of the particular class? test samples that are associated with the corresponding database class, with white representing 100% and black representing 0%)  range of elevation angles (60o to 70o, approximately).

The retrieval results are captured in a confusion matrix (e.g. Figure 8). The y-axis corresponds to the vehicle classes held in the database, the ordering being consistent with the taxonomy revealed by the dendrogram (Figure 6), and the x-axis represents the samples under test. Each element in the matrix is colour coded to represent the percentage of the particular class? test samples that are associated with the corresponding database class.

TABLE I. CLASSIFICATION PERFORMANCE  CA HT LT BM LR M3 TA  Samples 125 125 62 262 26 35 5 Correct 91 15 22 91 4 10 5 % Correct 72.8 12 35.5 34.7 15 28.6 100  Observations:  1. All samples of the Tank (TA) and the majority of samples of the Car (CA) have been classified correctly.

2. Approximately, 1/3rd of the samples of the Bush Master (BM), M113 (M3) and Light Truck (LT) have been classified correctly. The remaining 2/3rd (approx.) of the samples have been classified as their nearest neighbours.

The confusion seems to be prevalent between the actual class and its nearest neighbours. Usage of better discrim- inating features might alleviate this confusion.

(o) Car (p) Land Rover (q) Light Truck (r) Heavy Truck (s) M-113 (t) Bushmaster (u) Tank  Fig. 5. Trials vehicles: from the ground (upper), from the air (middle), and corresponding 3D model.

3. Almost all samples of the Heavy Truck (HT) have been misclassified, with Light Truck (LT) being first ranked and Freightliner (FL) being second. Semantically, in a scale-invariant space, a Heavy Truck (HT) could be placed in between a Freightliner (FL) and a Light Truck (LT). Another consideration is that the 3D model of the Heavy Truck (HT) was constructed from a sequence of video frames using Video Trace [22]. This is an interactive process in which the user extracts salient surfaces to include in the model. The fact that our model of the Heavy Truck (HT) (Figure 5) is not as complete as the other models may have contributed to the misclassifications.

4. The majority of samples of the Land Rover (LR) have been misclassified, with Bushmaster (BM) being first ranked and the M113 (M3) being second. Camouflaged small vehicles at low resolution are proving to be quite hard to classify correctly. Camouflage, for obvious rea- sons, is quite effective in dominating the structural detail leading to degraded performance.

H. Technical Analysis  Finally, the analyst can invoke an interactive mensura- tion tool for analysing the vehicle or its surroundings in more detail. In the case of a vehicle, this might enable any residual ambiguity of its class to be resolved, or it might enable unusual features to be measured and understood.

However, precision of such measurements is dependent upon the resolution of the imagery, accuracy of position and attitude of the sensor.

Distance (in geographical units) can be recovered from an image if the camera parameters and the lengths of some reference features in the image are known [26]. In our experiments, the camera parameters were recovered from the equations of dominant lines in the image which were identified by the user. The projection is then transformed from an unscaled Euclidean to metric using either the known height of an object in the scene [27] or, in this case, the width of the sensor foot-print is extracted from the metadata [5]. This approach has in the past been used mainly for measuring ground features [5], but the errors resulting from measuring slightly elevated features will be minor in comparison to the other sources of error mentioned previously.

Results on measuring lengths on a tank are shown in Figure 9 and given in Table II. The GSD is estimated at about 17cm per pixel. Error does exist in these measured  lengths and is probably influenced by the accuracy of the GSD.

TABLE II. COMPARISON OF LENGTH MEASUREMENTS (MM).

1 2 3 4 5 6 7  Truth 3830 3300 6900 950 3850 1850 1800 Measured 3490 2921 6112 906 3616 1820 1659 % Error 8.9 11.5 11.4 4.6 6.1 1.6 7.8  Fig. 9. Measured distances in Table II overlaid on EON video.



IV. FINDINGS  This piece of work has pipelined a suite of algorithms for the first time, unifying VMTI and target recognition capabilities. The latter has up until now been used in isolation. The VMTI algorithm is now relatively mature and has been applied extensively, however the recognition algorithm is still firmly at the development stage.

The performance of the VMTI algorithm has been improved recently by the modifications outlined in Section III-A. Increased compute capability offered by commodity hardware platforms means that the algorithms can be parameterised more effectively. For instance, more frames can be stacked to compute the background model, and the background model itself can be computed using rigorous statistics. Despite this, the VMTI will always produce some false tracks due to 3D structure and environmental movement in the scene.

The impact of these false alarms can me mitigated by subsequent processing. In some instances erroneous tracks are only an issue if they can trigger an event flag. Regard- less, the target extraction algorithm has proven to generate good quality segmentations which lend themselves well to the extraction of shape statistics (in real world coordinates) and subsequent filtering.

The recognition framework has been applied to im- agery collected in a controlled laboratory environment [6], as well as the airborne imagery we have presented.

The results are promising, showing good discrimination between vehicle classes although performance is degraded by vehicle camouflage and low resolution imagery. The framework is scalable in terms of feature sets and the amount of input imagery; and computationally paral- lelisable as model matching operations are independent.

However, the recognition software does require attention in several areas. Firstly, results show that a certain amount of confusion will exist between vehicle classes, particu- larly camouflaged military ones. A confidence measure is required to facilitate decision making, automatic or otherwise. Secondly, the requirement for a tight bounding box surrounding the potential vehicle places significant demands on the preceding modules. A means needs to be found to remove the impact of clutter surrounding the vehicle from degrading recognition performance, the so- called ?target delineation? problem. We also need to make more effective use of ground-sample distance information when carrying out the model selection and recognition.

Although we demonstrate a system addressing a small subset of possible targets, the potential exists for the set of model candidates to be significantly larger. Further investigation of the associated impact on classification and execution speed is required.



V. CONCLUSION  The work described in this paper is the first stage in moving from fundamental interactive video cueing aids to behaviour characterisation and anomaly detection tools that wide area, persistent surveillance will demand. We have demonstrated the value of ADSS in configuring a so- lution to a particular problem from component algorithms.

The results show definite promise and there is now a clear way forward for algorithm and systems development.

In future we intend to exercise our framework and algorithms in an airborne trial with a very high resolution wide area motion imagery sensor. This will facilitate activity monitoring over a large area and allow us to carry out complex, repeated ground behavior activities for subsequent analysis.

