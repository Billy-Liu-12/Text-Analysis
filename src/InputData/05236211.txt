An Improved Apriori Algorithm for Association Rules of Mining

Abstract  Apriori --the classical association rules mining algorithm is a way to find out certain potential, regular knowledge from the massive ones. But there are two more serious defects in the data mining process. The first needs many times to scan the business database and the second will inevitably produce a large number of irrelevant candidate sets which seriously occupy the system resources. An improved method is introduced on the basic of the defects above. The improved algorithm only scans the database once, at the same time the discrete data and statistics related are completed, and the final one is to prune the candidate item sets according to the minimum supporting degree and the character of the frequent item sets. After analysis, the improved algorithm reduces the system resources occupied and improves the efficiency and quality.

1. Introduction   Data Mining is a method that withdraws some kind of information knowledge which can?t be discovered easily but contains a certain regularity from the massive primary data. It is one of the most promising cross-disciplinaries of information technology. In 1994, R. Agrawal and R. Srikan put forward the original Apriori algorithm of Boolean association rules for mining frequent itemsets [1,3], which  adopted the layer-by-layer searching iterative method, that is k- itemsets coming from the exploration (k +1) itemsets.

The defects are obvious. One needs to scan the database many times, and the other is the candidate data sets scale generated is large. A lot of research and exploration have been done by the scholars and experts at home and abroad. Literature [5] proposed that only needs scan the database one time, using frequency 1 itemsets to redevelop business database to improve the  algorithm efficiency; Literature [4] put forward the dynamic frequent itemsets mining algorithm based on undirected itemset graph, which can improve the searching speed of frequent itemsets and save the disk memory, etc.; Literature [6] proposed a method to search for the frequent itemsets using Boolean matrix's row vector directly. An improved algorithm is got mainly based on the two defects of the Apriori algorithm in this article. Through scanning the database once, it counts each candidate itemsets, and finally concentrates to prune the candidate itemsets that can?t meet the minimum supporting degree, so as to improve the efficiency of algorithms effectively.

2. Basic Concept   Suppose that I={I1,I2,?,Im} is a set of itemsets, D is a set of business database. And every business T is a muster of items, and that T I. Every business has and only has one business id Tid, then every business in D can be expressed as {Tid,T}.

Definition 1. If there are k items in the item muster X, the muster X is called k-item muster, then X I.

Definition 2. If X I,Y I,X?Y= ,X=>Y are called association rules, then X is called the presupposition of the association rules, Y is called the result of the association rules, => is called the operation of the association rules.

? ? ?  Definition 3. Supporting Degree: If the rule A=>B in the database D is reasonable, then the frequency of A B in the database is called supporting degree.

Support (A=>B) = P(A B)       (1) Definition 4. Degree of Confidence: If the rule  A=>B in the database D is reasonable, then the ratio of the frequency of AUB to the frequency of A in the database is called degree of confidence.

Confidence (A=>B) = P (B|A)    (2) P (B|A) = Support (A=>B) / Support (A)  (3)  _____________________________    The mining of association rules is to mine out the freq  degr  then  2. If a frequent k-1 itemsets can produce freq  ion rules can be anal  viously the  the frequent item  3. Apriori Algorithm Theory  .1 Apriori Algorithm Theory ithm is to find out  all  -  uent itemsets according to the least supporting degree and the least confidence degree set by the users.

Definition 5. Frequent Itemsets: If the supporting (1) ee of a k-itemset?min_sup( the least supporting  degree ), the k-itemset is called frequent k-itemsets, then all the musters of the frequent k-itemsets is marked as Lk.

Character 1. If a k-itemset is not a frequent one, the muster containing the k-itemset is not a frequent  one. If a  k-itemset is a frequent one, then all the proper subsets (except null sets) of the k-itemset must be frequent ones.

Character uent k itemsets, the number of itemsets in the  frequent k-1 itemsets is no less than k.

The problems of mining associat  yzed as the two following sub problems.

1)  Find out all the frequent itemsets, ob  supporting degree of   them all? min_sup.

2)  Find out the association rules from sets, then make further the confidence degree of  the rules?min_conf(the least confidence degree).

The basic idea of Apriori algor  frequent 1- itemsets through scanning business database D. Then these itemsets compose L1, then L1 produces the frequent 2-itemsets L2, then L2 to L3, and so on until no new frequent itemsets produced. To get the achievement from Lk, to Lk-1, we first connect Lk-1 with its own to produce the candidate itemset k- itemsets marked  with Ck, then count the frequency degree of the data itemsets , remaining the data itemsets with over the least supporting degree. The process of forming the Lk connection is the following : carry out Lk-1         Lk-1,take out l1,l2,??l k-1,L k-1 [j] standing for the j-th item; if the first k-2 in any two of them are the same, then carry on linking to form k- candidate itemset l1 [1] l2[2]?l1 [k-1] l2 [k-1],joining to Ck; whether the itemset Ck can compose frequent k- itemsets Lk, it needs to contrast the data items to the result got from the database scan. If there are candidate k-itemset which is not more than the least supporting degree in the candidate itemsets, then cancel the itemset to get the frequent k-itemsets Lk. The original Apriori algorithm needs to scan the database to get a Lk, which occupies the system resources severely and reduces the computer functions.

3.2 Apriori Algorithm Input  Database D with the format (Tid, itemset), then Tid is a business id and itemset is the itemset corresponding to the business.

(2) The least supporting degree of users: min_sup.

Output: All the frequent itemsets (1)L1=find_frequent_1-itemsets (D); //find out all the frequent 1 itemsets (2)for (k=2;Lk-1??;k++){ (3)Ck=apriori_gen (Lk-1, min_sup); (4)for each transaction t D do{//scan D to count number (5)Ct=subset (Ck, t);//take out the candidate musters contained in business t (6)for each candidate c Ct (7)c.count++; (8) } (9) Lk={c Ct ;c.count ? min_sup}} (10) return L= kLk; //to get the union of Lk.

Apriori_gen takes frequent (k-1) itemsets Lk-1 as the candidate muster of the independent variables to produce a function. This function returns the super musters containing all the frequent k-itemsets, carried on by the two steps including join and prune.

The 1st step: join Procedure apriori_gen (Lk-1:frequent(k-1)-itemsets; min_sup) (1)for each itemset l1 Lk-1 (2)for each itemset l2 Lk-1 (3)if((l1[1]=l2[1])(l1[2]=l2[2]) ? (l1[k-2]=l2[k 2]) (l1[k-1]?l2[k-1])then{ (4)c=l1 l2;//join, produce candidate itemsets (5) if has_infrequent_subset (c,Lk-1) then (6) delete c;//prune, cancel the useless candidate itemsets (7)else add c to Ck; (8)} (9) return Ck; The 2nd step: prune Procedure_infrequent_subset(c: candidate k-itemset; Lk-1: frequent(k-1)-itemsets);//using the former knowledge (1) for each(k-1)-subset of (2) if s Lk-1 then (3) return true; (4) return false;    4. Apriori Algorithm Improvements     4.1 The Ideology of Apriori Algorithm Improvement  Output: Export all the frequent itemsets.

(1) scan the database (2) for (i=1; i? k; i++)  In order to enhance the production efficiency of the frequent itemsets, this article discusses two sides according to two defects of the Apriori algorithm.

Firstly, separate every acquired data according the discretization of the data items and count the data while scan the database. All the statistics of the frequent items in the business musters can be got by scanning the database once. Secondly, prune the acquired frequent itemsets using the two basic characters of the Apriori algorithm. The pruning needs two steps to complete:  (3) {get every business Tid and ItemSet {I 1 I 2 I 3?? I k } muster;// ItemSet muster can the whole, also the a part of it  (4) after the business discretization to get all the proper subsets ? ?; (5) for (j=1; j<= Cki ; j++) //seek all the candidate itemsets according to the combinatorial items, and count the data while in scanning (6) {C i [j].count++;}} Prune the acquired candidate itemsets.

(7) for (i=1;i<k+1;i++)// cancel each candidate itemset which <min_sup (1) When the frequency of Lk[j].count< min_sup in  each kind of frequent itemset L k[j] , delete the itemset. (8) {for (j=0; j< Cki ; j++) (2) When the number of candidate k-1 in C k [j] of Itemset candidate k- itemsets<k, then frequent k itemsets can not be reasonable, so delete the C k [j] in candidate k-itemset.

(9) {if (C i [j].count < min_sup) (10) delete(C i [j]); } (11) if (C k-1 .count < k) //If the total number of candidate k-1<k,then Ck-1i <k,so cancel C k according to Character  4.2 The Description of Apriori Algorithm  Improvement (12) delete C k} 5. Case Analysis Input:  (1)Database D with the format (Tid, itemset), then Tid is a business id and itemset is the itemset corresponding to the business.

As shown in Table 1: business itemsets composed  of Tid{100~110}, database itemsets composed of ItemSet{I1,I2,I3,I4,I5}, suppose min_sup=2. (2) The least supporting degree of users: min_sup; the  least confidence: degree min_conf.

The 1st step: Scan the database, carrying on the discretization of the business data itemsets, while count the supporting degree of each candidate itemset  according to the results of the discrimination. As shown in Table 2:  Table 1. Business Database ItemSets Tid ItemSet Tid ItemSet 100 I1,I5 106 I3,I4 101 I2,I3,I4 107 I2,I3,I4,I5 102 I2,I4 108 I3 103 I1,I2 109 I4 104 I2,I4 110 I1,I3 105 I2,I5    Table 2. The Statistics of Candidate Frequency ItemSets Candidate 1 Itemset I1 I2 I3 I4 I5  1-COUNT 3 6 5 6 3 Candidate 2 Itemset I1,I5 I2,I3 I2,I4 I3,I4 I1,I2 I2,I5 I3,I5 I4,I5 I1,I3  2-COUNT 1 2 4 3 1 2 1 1 1 Candidate 3 Itemset I2,I3,I4 I3,I4,I5 I2,I4,I5  3-COUNT 2 1 1 Candidate 4 Itemset I2,I3,I4,I5  4-COUNT 1   The 2nd step: Based on the each candidate itemset  got from Table 2 and according to the ratio of the candidate itemsets to the least supporting degree,  cancel the candidate itemsets which can?t meet the least supporting degree. As shown in Table 3:    Table 3. Prune the Candidate Frequency ItemSets Candidate 1 Itemset I1 I2 I3 I4 I5  1-COUNT 3 6 5 6 4 Candidate 2 Itemset I1,I5 I2,I3 I2,I4 I3,I4 I1,I2 I2,I5 I3,I5 I4,I5 I1,I3  2-COUNT 1 2 4 3 1 2 1 1 1 Candidate 3 Itemset I2,I3,I4 I3,I4,I5 I2,I4,I5  3-COUNT 2 1 1 Candidate 4 Itemset I2,I3,I4,I    4-COUNT 1  The 3rd step: According to the pruned results in Table 3, get a further prune based on Character 2, until  all the frequent itemsets meet the qualifications. As shown in Table 4:  Table 4. Frequency ItemSets Frequent 1 Itemsets I1 I2 I3 I4 I5 1-COUNT 3 6 5 6 4 Frequent 2 Itemsets I2,I5 I2,I3 I2,I4 I3,I4 2-COUNT 2 2 4 3 Frequent 3 Itemsets I2,I3,I4 3-COUNT 2 Frequent 4 Itemsets 4-COUNT   Analyzing from Table 4, the biggest frequent item  sets mined from the business muster is Frequent 3 Itemset.

Frequent 1 Itemsets:{I1}:3;{I2}:6;{I3}:5;{I4}:6; {I5}:4; Frequent 2 Itemsets: {I1,I2}:2;{I2,I3}:2;{I2,I4}:4; {I3,I4}:3;  Frequent 3 Itemsets: {I2,I3,I4}:2;   5.2 Confidence degree Analysis According to Table 4, the biggest frequent  itemsets compose of {Tid, ItemSet} is {I2,I3,I4}, supposing min_conf=60%.

{I2,I3}=>I4   100% {I3,I4}=>I2        66.67% {I2,I4}=>I3   50% According to the results of confidence degree  analysis, the confidence degree ?min_conf(60%), so there is the association rule {I2,I3}=>I4,{I3,I4}=>I2 in the frequent 3 itemsets. Furthermore the confidence degree of {I2,I3}=>I4 is 100%. So far, we can conclude that {I2,I3}=>4 is strong association rules, that is to say that if the combinatorial item {I2,I3} is found in a business, then the item I4 has been a high rate of emerging.

Whether the association rules got through Apriori algorithm for mining can truly reflect the relationship between data still needs to be evaluated. If the association rules confidence level is quite high, then it can not tell that there is a relationship as the one mentioned above in the real life, but only tell the pure and absolute characteristics of the data relationship reflected by the association rules at a certain moment.

So the efficiency level of the association rules got needs further analysis and verification, especially those whose confidence levels are 100%.

6. Conclusion   Improved algorithm in process of seeking the maximal frequent itemsets, it only needs to scan the database to complete the separation and statistics of the entire database. After the scanning and counting the entire database, the system gets the candidate itemsets of all the business itemset {C1 C 2?? Ck}. Then prune all the candidate itemsets according to the least supporting degree and the confident degree (credibility), making sure of all the frequent itemsets {L 1 L 2?? Li,(i=1,2,??k)} from the 1st and the k- th .

Compares with the original algorithm, the improved algorithm reduces the times of scanning the database greatly, and doesn?t need to the operation of joining  and pruning. After scanning the database, complete to make sure all the frequent itemsets {L 1 L 2?? Li,(i=1,2,??k)} according to the least supporting degree and the characters of the frequent itemsets, and makes sure the biggest frequent itemsets by the pruned frequent itemsets table. Make sure of the strong association rules according to the biggest frequent itemsets and the confidence degree between data. The efficiency and quality of data mining are promoted by the improved algorithm, and the occupying of the system resources is also reduced greatly.

Acknowledgement the National Natural Science Foundation of China  (No.60873247); the Natural Science Foundation of Shandong Province of China (No.Y2006G20); High- tech self-innovation project of Shandong Province (No.

2008ZZ28)  7. References  [1] Han Jiawei,Kamber Miceline. Data mining concepts and technique [M].BeiJing: Machinery Industry Press,2001:149- 179.

[2] Wen Lei,Li Minqiang.A new association rules mining algorithms based on directed itemsets graph [C].China: Proceeding of 9th the Int?l Conf RSFDgrc,2003:660-663.

[3] Dzeroski S, RaedtLD. Multi-relational data mining: The current frontiers[C].Preceding ECML/PKDD, ACM Press, 2002.

[4] Kong Fang, Qan Xuezhong. Research of Improved Apriori algorithm in mining association rules. [J]. Computer Engineering & Design,2008,29(16) :4220-4222.

[5] Gong Jianmei,Song Shilin,Li ShiSong. Improved algorithm based on Apriori algorithm [J]. Computer Engineering & Design,2008,29(11):2814-2815,2820.

[6] Liu Yi?an,Yang Bin. Research of Improved Apriori algorithm in mining association rules [J].Journal of Computer Application,2007,27(2):418-420.

