LPMiner: An Algorithm for Finding Frequent Itemsets  Using Length-Decreasing Support Constraint*

Abstract  Over the years, a variety of algorithms forfinding fre- quent itemsets in very large transaction databases have been developed. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of the problem. In gen- eral, itemsets that contain only a few items will tend to be interesting ifthey have a high support, whereas long item- sets can still be interesting even if their support is relatively small. Ideally, we desire to have an algorithm thatjnds all the frequent itemsets whose support decreases as a func- tion of their length. In this paper we present an algorithm called LPMinel; thatfinds all itemsets that satisfy a length- decreasing support constraint. Our experimental evalua- tion shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for$nding itemsets at a constant support constraint, and that its runtime increases gradually as the average length of the transactions (and the discovered itemsets) increases.

1 Introduction Data mining research during the last eight years has led to the development of a variety of algorithms for find- ing frequent itemsets in very large transaction databases [2, 1, 4, 81. These itemsets can be used to find association rules or extract prevalent patterns that exist in the transac- tions, and have been effectively used in many different do- mains and applications.

The key feature in most of these algorithms is that they  'This work was supported by NSF CCR-99725 19, EIA-9986042, ACI- 9982274, by Army Research Office contract DA/DAAG55-98-1-0441, by the DOE ASCI program, and by Army High Performance Computing Re- search Center contract number DAAH04-95-C-0008. Access to computing facilities was provided by the Minnesota Supercomputing Institute.

0-7695-1 119-8/01 $17.00 0 2001 IEEE  control the inherently exponential complexity of the prob- lem by finding only the itemsets that occur in a sufficiently large fraction of the transactions, called the support. A lim- itation of this paradigm for generating frequent itemsets is that it uses a constant value of support, irrespective of the length of the discovered itemsets. In general, itemsets that contain only a few items will tend to be interesting if they have a high support, whereas long itemsets can still be in- teresting even if their support is relatively small. Unfortu- nately, if constant-support-based frequent itemset discovery algorithms are used to find some of the longer but infrequent itemsets, they will end up generating an exponentially large number of short itemsets. Maximal frequent itemset discov- ery algorithms [8] can potentially be used to find some of these longer itemsets, but these algorithms can still gener- ate a very large number of short infrequent itemsets if these itemsets are maximal. Ideally, we desire to have an algo- rithm that finds all the frequent itemsets whose support de- creases as a function of their length. Developing such an al- gorithm is particularly challenging because the downward closure property of the constant support constraint cannot be used to prune short infrequent itemsets.

In this paper we present another property, called small- est valid extension ( S V E ) ,  that can be used to prune the search space of potential itemsets in the case where the sup- port decreases as a function of the itemset length. Using this property, we developed an algorithm called LPMiner, that finds all itemsets that satisfy a length-decreasing sup- port constraint. LPMiner uses the recently proposed FP- tree [4] data structure to compactly store the database trans- actions in main memory, and the SVE property to prune certain portions of the conditional FP-trees, that are being generated during itemset discovery. Our experimental eval- uation shows that LPMiner is up to two orders of magnitude faster than the FP-growth algorithm for finding itemsets at a constant support constraint, and that its runtime increases   mailto:cs.umn.edu   gradually as the average length of the transactions (and the discovered itemsets) increases.

The rest of this paper is organized as follows. Section 2 provides some background information and related research work. Section 3 describes the FP-growth algorithm [4], on which LPMiner is based. In Section 4, we describe how the length-decreasing support constraint can be exploited to prune the search space of frequent itemsets. The experimen- tal results of our algorithm are shown in Section 5, followed by the conclusion in Section 6.

2 Background and related works The problem of finding frequent itemsets is formally de- fined as follows: Given a set of transactions T, each con- taining a set of items from the set I ,  and a support o, we want to find all subsets of items that occur in at least o(TI transactions. These subsets are calledfrequent itemsets.

Over the years a number of algorithms have been de- veloped for finding all frequent itemsets. The first compu- tationally efficient algorithm for finding itemsets in large databases was Apriori [2], which finds frequent itemsets of length 1 based on previously generated ( 1  - 1)-length fre- quent itemsets. The key idea of Apriori is to use the down- ward closure property of the support constraint to prune the space of frequent itemsets. The FP-growth algorithm [4] finds frequent itemsets by using a data structure called FP- tree that can compactly store in memory the transactions of the original database, thus eliminating the need to access the disks more than twice. Another efficient way to repre- sent transaction database is to use vertical tid-list database format. The vertical database format associates each item with all the transactions that include the item. Eclat in [7] uses this data format to find all frequent itemsets.

Even though to our knowledge no work has been pub- lished for finding frequent itemsets in which the support decreases as a function of the length of the itemset, there has been some work in developing itemset discovery algo- rithms that use multiple support constraints. Liu et al. [5] presented an algorithm in which each item has its own min- imum item support (or MIS). The minimum support of an itemset is the lowest MIS among those items in the item- set. By sorting items in ascending order of their MIS val- ues, the minimum support of the itemset never decreases as the length of itemset grows, making the support of itemsets downward closed. Thus an Apriori-based algorithm can be applied. Wang et al. [6] allow a set of more general support constraints. In particular, they associate a support constraint for each one of the itemsets. By introducing a new function called Pminsup that has ?Apriori-like? property, they pro- posed an Apriori-based algorithm for finding the frequent itemsets. It is possible to represent a length-decreasing sup- port constraint by using the formulation in [6]. However, the ?pushed? minimum support of each itemset is forced to  be equal to the support value corresponding to the longest itemset. Thus, it cannot prune the search space. Finally, Co- hen et al. [3] adopt a different approach in that they do not use any support constraint. Instead, they search for similar itemsets using probabilistic algorithms, that do not guaran- tee that all frequent itemsets can be found.

3 FP-growth algorithm In this section, we describe how the FP-growth algorithm works because our approach is based on this algorithm. The description here is based on [4].

The key idea behind FP-growth is to use a data struc- ture called FP-tree to obtain a compact representation of the original transactions so that they can fit into the main mem- ory. As a result, any subsequent operations that are required to find the frequent itemsets can be performed quickly, with- out having to access the disks. The FP-growth algorithm achieves that by performing just two passes over the trans- actions. Figure 1 shows how the FP-tree generation algo- rithm works given an input transaction database that has five transactions with a total of six different items. First, it scans the transaction database to count how many times each item occurs in the database to get an ?Item Support Ta- ble? (step (a)). The ?Item Support Table? has a set of (item- name, support) pairs. For example, item A occurs twice in the database, namely in a transaction with tid 1 and another one with tid 5; therefore its support is 2/5 = 40%. In step (b), those items in the Item Support Table are sorted accord- ing to their support. The result is stored in item-name field of Node-Link header table NL. Notice that item F is not included in NL because the support of item F is less than the minimum support constraint 40%. In step (c), items in each transaction in the input transaction database are sorted in the same order as items in the Node-Link header table NL. While transaction tid 5 is sorted, item F is discarded because the item is infrequent and has no need of consider- ation. In step (d), the FP-tree is generated by inserting those sorted transactions one by one. The initial FP-tree has only its root. When the first transaction is inserted, nodes that represent item B, C, E, A, and D are generated, forming a path from the root in this order. The count of each node is set to 1 because each node represents only one transaction (tid 1 )  so far. Next, when the second transaction is inserted, a node representing item B is nor generated. Instead, the node already generated is reused. In this case, because the root node has a child that represents item B, the count of the node is incremented by one. As for item E, since there is no child representing item E under the current node, a new node with item-name E is generated as a child of the current node. Similar processes are repeated until all the sorted transactions are inserted into the FP-tree.

Once an FP-tree is generated from the input transaction database, the algorithm mines frequent itemsets from the     sort items in each transaction  Item Support Table  Figure 1. Flow of FP-tree generation.

Conditional Pattern Base of conditional pattern D item count  E----:-- -*::!! _ _ _ _ _ - - - - -  _ _ _ _  Node-Link NL Conditional FP-tree of conditional pattern D (Single path FP-tree)  Figure 2. Conditional FP-tree.

FP-tree. The algorithm generates itemsets from shorter to longer ones adding items one-by-one to those itemsets al- ready generated. It divides mining the FP-tree into mining smaller FP-trees, each of which is based on an item on the Node-Link header table in Figure 1. Let us choose item D as an example. For item D, we generate a new transac- tion database called conditional pattern base. Each trans- action in the conditional pattern base consists of items on the paths from parent nodes whose child nodes have item- name D to the root node. The conditional pattern base for item D is shown in Figure 2. Each transaction in the con- ditional pattern base also has its count of occurrence corre- sponding to the count of the node with item-name D in the original FP-tree. Note that item D itself is a frequent item- set consisting of one item. Let us call this frequent itemset ?D? a conditionalpattern. A conditional pattern base is a set of transactions each of which includes the conditional  pattern. What we do next is to forget the original FP-tree in Figure 1 for a while and then focus on the conditional pattern base we got just now to generate frequent itemsets that include this conditional pattern ?D?. For this purpose, we generate a smaller FP-tree than the original one, based on the conditional pattern ?D?. This new FP-tree, called conditional FP-tree, is generated from the conditional pat- tern base using the FP-tree generation algorithm again. If the conditional FP-tree is not a single path tree, we divide mining this conditional FP-tree to mining even smaller con- ditional FP-trees recursively. This is repeated until we ob- tain a conditional FP-tree with only a single path. During those recursively repeated processes, all selected items are added to the conditional pattern. Once we obtain a single path conditional FP-tree like the one in Figure 2, we gen- erate all possible combinations of items along the path and combine each of these sets of items to the conditional pat- tern. For example, from those three nodes in the conditional FP-tree in Figure 2, we have 23 = 8 combinations of item B, C, and E: ? ? (no item), ?B?, ?C?, ?E?, ?BC? , ?CE?, ?EB?, and ?BCE?. Then we obtain frequent itemsets based on conditional pattern base ?D?: ?D?, ?DB?, ?DC?, ?DE?, ?DBC?, ?DCE?, ?DEB?, and ?DBCE?.

4 LPMiner algorithm  LPMiner is an itemset discovery algorithm, based on the FP-growth algorithm, which finds all the itemsets that sat- isfy a particular length-decreasing support constraint f ( 1 ) ; where 1 is the length of the itemset. More precisely, f ( 1 ) Satisfies f ( l a )  2 f ( l b )  for any Za,lb such that I!, < l b .

The idea of introducing this kind of support constraint is     that by using a support that decreases with the length of the itemset, we may be able to find long itemsets, that may be of interest, without generating an exponentially large num- ber of shorter itemsets. Figure 3 shows a typical length- decreasing support constraint. In this example, the support constraint decreases linearly to the minimum value and then stays the same for itemsets of longer length. Our problem is restated as finding those itemsets located above the curve determined by length-decreasing support constraint f ( 1 ) .

support( %) I  support(%) I  length of itemset Figure 4. Smallest valid extension (SVE).

1 10 length of itemset  Figure 3. An example of typical length-decreasing support constraint.

A simple way of finding such itemsets is to use any of the traditional constant-support frequent itemset discovery algorithms, in which the support was set to minl>o f ( Z ) , and then discard the itemsets that do not satisfy the length- decreasing support constraint. This approach, however, does not reduce the number of infrequent itemsets being discovered, and as our experiments will show, requires a large amount of time.

As discussed in the introduction, finding the complete set of itemsets that satisfy a length-decreasing support func- tion is particularly challenging because we cannot use the downward closure property of the constant support frequent itemsets. This property states that in order for an itemset of length Z to be frequent, all of its subsets have to be frequent as well. As a result, once we find that an itemset of length I is infrequent, we know that any longer itemsets that include this particular itemset cannot be frequent, and thus elim- inate such itemsets from further consideration. However, because in our problem the support of an itemset decreases as its length increases, an itemset can be frequent even if its subsets are infrequent.

A key property, regarding the itemset whose support decreases as a function of their length, is the following.

Given a particular itemset I with a support of ( T I ,  such that (TI < f(lIl), then  TI) = min({llf(Z) = ( T I } )  is the minimum length that an itemset I' such that I' 3 I must have before it can potentially become frequent. Figure 4  illustrates this relation graphically. The length of I' is noth- ing more than the point at which a line parallel to the x-axis at y = (TI intersects the support curve; here, we essentially assume the best case in which I' exists and it is supported by the same set of transactions as its subset I .  We will refer to this property as the smallest valid extension property or W E  for short.

LPMiner uses this property as much as it can to prune the conditional FP-trees, that are generated during the itemset discovery phase. In particular, it uses three different prun- ing methods that, when combined, substantially reduce the search space and the overall runtime. These methods are described in the rest of this section.

4.1 Transaction pruning, TP  The first pruning scheme implemented in LPMiner uses the smallest valid extension property to eliminate entire candi- date transactions of a conditional pattern base. Recall from Section 3 that, during frequent itemset generation, the FP- growth algorithm builds a separate FP-tree for all the trans- actions that contain the conditional pattern currently under consideration. Let CP be that conditional pattern, lCPl be its length, and (T(CP) be its support. If C P  is infre- quent, we know from the W E  property that in order for this conditional pattern to grow to something indeed frequent, it must have a length of at least f - ' (o (CP)) .  Using this requirement, before building the FP-tree corresponding to this conditional pattern, we can eliminate any transactions whose length is shorter than f-l(c~(CP)) - ICPI, as these transactions cannot contribute to a valid frequent itemset in which C P  is part of it. We will refer to this as the trunsac- tion pruning method and denote it by TP.

We evaluated the complexity of this method in compari- son with the complexity of inserting a transaction to a con- ditional pattern base. There are three parameters we have to know to prune a transaction: the length of each transac- tion being inserted, f - ' ( o (CP) ) ,  and (CPI. The length of each transaction is calculated in a constant time because we can count each item when the transaction is actually being generated. As ~ - ' ( ( T ( C P ) )  and lCPl are common values     for all transactions in a conditional pattern base, these val- ues need to be calculated only once for the conditional pat- tern base. It takes a constant time added to the original FP- growth algorithm to calculate JCPI. As for f-'(a(CP)), evaluating f-' takes O(log( 111)) to execute binary search on the support table determined by f(l) .  Let cpb be the con- ditional pattern base and rn = CtranEcpb Itranl. The com- plexity per inserting a transaction is O(log(lIl)/rn). Under an assumption that all items in I are contained in cpb, this value is nothing more than O(1). Thus, the complexity of this method is just a constant time per inserting a transac- tion.

4.2 Node pruning, NP The second pruning method focuses on pruning certain nodes of a conditional FP-tree, on which the next condi- tional pattern base is about to be generated. Let us consider a node w of the FP-tree. Let I(v) be the item stored at this node, a(l(w)) be the support of the item in the conditional pattern base, and h(v) be the height of the longest path from the root through v to a leaf node. From the SVE property we know that the node w will contribute to a valid frequent itemset only if  (1) h(v)  + ICPl 2 f-'(.(w>> where lCPl is the length of conditional pattern of the cur- rent conditional FP-tree. The reason that equation (1) is correct is because, among the transactions that go through node w, the longest itemset that I(v) can participate in has a length of h(v).  Now, if the support of I(v) is small such that it requires an itemset whose length f - l ( a ( I ( v ) ) )  is greater than h(v) + JCPI, then that itemset cannot be supported by any of the transactions that go through node U. Thus, if equation (1) does not hold, node v can be pruned from the FP-tree. Once node w is pruned, then a(I(w)) will de- crease as well as the height of the nodes through U ,  possibly allowing further pruning. We will refer to this as the node pruning method, or NP for short.

A key observation to make is that both the TP and NP methods can be used together as each one of them prunes portions of the FP-tree that the other one does not. In par- ticular, the NP methods can prune a node in a path that is longer than f-'(a(CP)) - lCPl, because the item of that node has lower support than CP. On the other hand, TP reduces the frequency of some itemsets in the FP-tree by removing entire short transactions. For example, consider two transactions; (A, B, C, D) and (A, B). Let's assume that f-l(a(CP)) - lCPl = 4, and each one of the items A,B,C,D has a support equal to that of CP. In that case, the NP will not remove any nodes, whereas TP will eliminate the second transaction.

In order to perform the node pruning, we need to com- pute the height of each node and then traverse each node v  to see if it violates equation (1). If it does, then the node v can be pruned. The height of all the nodes whose longest path goes through v must be decremented by one, and the support of I(v) needs to be decremented to take account of the removal of U. Every time we make such changes in the tree, nodes that could not have been pruned before may now become eligible for pruning. In particular, all the rest of the nodes that have the same item I(v) needs to be rechecked, as well as all the nodes whose height was decre- mented upon the removal of w. Our initial experiments with such an implementation showed that the cost of perform- ing the pruning was often quite higher than the saving we achieved when used in conjunction with the TP scheme. For this reason we implemented an approximate but fast version of this scheme that achieves a comparable degree of prun- ing.

Our approximate NP algorithm initially sorts the trans- actions of the conditional pattern base in decreasing trans- action length, then traverses each transaction in that order, and tries to insert them in the FP-tree. Let t be one such transaction and l ( t )  be its length. When t is inserted into the FP-tree it may share a prefix with some transactions al- ready in the FP-tree. However, as soon as the insertion o f t results in a new node being created, we check to see if we can prune it using equation ( I ) .  In particular, if w is that newly created node, then h(v)  = l ( t ) ,  because the trans- actions are inserted into the FP-tree in decreasing length.

Thus v can be pruned if  If that can be done, the new node is eliminated and the inser- tion of t continues to the next item. Now if one of the next items inserts a new node U ,  then that one may be pruned us- ing equation (2). In equation (2), we use the original length of the transaction l ( t ) ,  not the length after the removal of the item previously pruned. The reason is that Z(t) is the cor- rect upper bound of h(u), because one of the transactions inserted later may have a length of at most l ( t ) ,  the same as the length of the current transaction, and can modify its height.

The above approach is approximate because (i) the elim- ination of a node affects only the nodes that can be elimi- nated in the subsequent transactions, not the nodes already in the tree; (ii) we use pessimistic bounds on the height of a node (as discussed in the previous paragraph). This approx- imate approach, however, does not increase the complexity of generating the conditional FP-tree, beyond the sorting of the transactions in the conditional pattern base. Since the length of the transaction falls within a small range, they can be sorted in linear time using bucket sort.

4.3 Path pruning, PP Once the tree becomes a single path, the original FP- growth algorithm generates all possible combinations of items along the path and concatenates each of those com- binations with its conditional pattern. If the path contains k items, there exist a total of 2k such combinations. How- ever, using the SVE property we can limit the number of combinations that we may need to consider.

Let {il, i2,. . . , ik} be the k items such that a(i j )  2 a(ij+l). One way of generating all possible 2k combina- tions is to grow them incrementally as follows. First, we create two sets, one that contains i l ,  and the other that does not. Next, for each of these sets, we generate two new sets such that, in each pair of them, one contains i2 and the other does not, leading to four different sets. By continuing this process a total of k times, we will obtain all possible 2k combinations of items. This approach essentially builds a binary tree with IC levels of edges, in which the nodes cor- respond to the possible combinations. One such binary tree for k = 4 is shown in Figure 5.

To see how the SVE property can be used to prune cer- tain subgraphs of this tree (and hence combinations to be explored), consider a particular internal node v of that tree.

Let h(v) be the height of the node (root has a height of zero), and let P(v) be the number of edges that were one on the path from the root to w. In other words, P(v) is the number of items that have been included so far in the set.

Using the SVE property we can stop expanding the tree un- der node v if and only if  P(v) + (k - + lCPl < f - l ( 4 4 L ( v , ) )  .

Essentially, the above formula states that, based on the fre- quency of the current item, the set must have a sufficiently large number of items before it can be frequent. If the num- ber of items that were already inserted in the set (/3(v)) plus the number of items that are left for possible insertion (k- h(v))  is not sufficiently large, then no frequent itemsets can be generated from this branch of the tree, and hence it can be pruned. We will refer to this method as path pruning or PP for short.

The complexity of PP per one binary tree is k log 111 be- cause we need to evaluate f-' for k items. On the other hand, the original FP-growth algorithm has the complexity of O(2') for one binary tree. The former is much smaller for large k. For small k ,  this analysis tells that PP may cost more than the saving. Our experimental result, however, suggests that the effect of pruning is bigger than the cost.

ID1 IT1  A  IOOK lOOK 3 to35 3 to30  5 Experimental results We experimentally evaluated the various search space prun- ing methods of LPMiner using a variety of datasets gener- ated by the synthetic transaction generator that is provided  IT1 /2  i l l ' ILI N  Figure 5. Binary tree when k = 4.

IT1 /2 loo00  by the IBM Quest group and was used in evaluating the Apriori algorithm [2]. All of our experiments were per- formed on Intel-based Linux workstations with Pentium 111 at 600MHz and 1GB of main memory. All the reported run- times are in seconds.

We used two classes of datasets DS1 and DS2. Both of them contained lOOK transactions. For each of the two classes we generated different problem instances in which we varied the average size of the transactions from 3 items to 35 items for DS1, obtaining a total of 33 dif- ferent datasets, DS1.3, . . ., DS1.35, and from 3 items to 30 items for DS2, obtaining DS2.3, . . ., DS2.30. For each problem instance in both of DS 1 .z and DS2.2, we set the average size of the maximal long itemset to be 2/2, so as z increases, the dataset contains longer frequent itemsets. The difference between DS1.z and DS2.z is that each problem instance DS 1 .x consists of 1 K items, whereas each problem instance DS2.2 consists of 5K items. The characteristics of these datasets are summarized in Table 1.

Table 1. Parameters for datasets used in our tests (JDI: Number of transactions, IT/: Average size of the transactions, 11): Average size of the maximal po- tentially long itemsets, IL(: Number of maximal po- tentially large itemsets, N: Number of items).

I Darameter I DS1 1 DS2 1  In all of our experiments, we used minimum support con- straint that decreases linearly with the length of the frequent itemsets. In particular, for each of the DS1.x datasets, the initial value of support was set to 0.5 and it was decreased linearly down to 0.01 for itemsets up to length 2. For the rest of the itemsets, the support was kept fixed at 0.01. The left graph of Figure 6 shows the shape of the support curve for DS1.20. In the case of the DS2 class of datasets, we used a similar approach to generate the constraint, however in- stead of using 0.01 as the minimum support, we used 0.005.

The right graph of Figure 6 shows the shape of the support curve for DS2.20.

Length of Patterns  Figure 6. Support curve for DS1.20 and DS2.20.

5.1 Results  Tables 2 and 3 show the experimental results that we ob- tained for the DSl and DS2 datasets, respectively. Each row of the tables shows the results obtained for a differ- ent DS1.z or DS2.z dataset, specified on the first column.

The remaining columns show the amount of time required by different itemset discovery algorithms. The column la- beled ?FP-growth? shows the amount of time taken by the original FP-growth algorithm using a constant support con- straint that corresponds to the smallest support of the sup- port curve, 0.01 for DSl, and 0.005 for DS2. The columns under the heading ?LPMiner? show the amount of time required by the proposed itemset discovery algorithm that uses the decreasing support curve to prune the search space.

A total of seven different variations of the LPMiner algo- rithm are presented that use different combinations of the pruning methods described in Section 4. For example, the column label ?NP? corresponds to the scheme that uses only node pruning (Section 4.2), whereas the column la- beled ?NP+TP+PP? corresponds to the scheme that uses all the three different schemes described in Section 4. Note that values with a ?-? correspond to experiments that were aborted because they were taking too long time.

A number of interesting observations can be made from the results in these tables. First, either one of the LPMiner methods performs better than the FP-growth algorithm. In particular, the LPMiner that uses all three pruning methods does the best, requiring substantially smaller time than the FP-growth algorithm. For DSI, it is about 2.2 times faster for DSl.10, 8.2 times faster for DS1.20, 33.4 times faster for DS 1.30, and 1 15 times faster for DS 1.35. Similar trends can be observed for DS2, in which the performance of LP- Miner is 4.2 times faster for DS2.10, 21.0 times faster for DS2.20, and 55.6 times faster for DS2.27.

Second, the performance gap between FP-growth and LPMiner increases as the length of the discovered frequent itemset increases (recall that, for both DS1.z and DS2.2, the length of the frequent itemsets increases with z). This is due to the fact that the overall itemset space that LPMiner can prune becomes larger, leading to improved relative per-  formance.

Third, comparing the different pruning methods in isola-  tion, we can see that NP and TF? lead to the largest runtime reduction and PP achieves the smallest reduction. This is not surprising as PP can only prune itemsets during the late stages of itemset generation.

Finally, the runtime with three pruning methods in- creases gradually as the average length of the transactions (and the discovered itemsets) increases, whereas the run- time of the original FP-growth algorithm increases expo- nentially.

6 Conclusion In this paper we presented an algorithm that can efficiently find all frequent itemsets that satisfy a length-decreasing support constraint. The key insight that enabled us to achieve high performance was the smallest valid extension property of the length decreasing support curve.

So far, we have dealt with a common length-decreasing support for all the items. However, the proposed algorithm can be easily extended to allow different length-decreasing support constraint to be specified for each item or itemset.

