<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Mining  Frequent Closed Itemsets for Large Data

Abstract Mining frequent closed itemsets is one effective method to analyse frequent pattern, and further, to generate asso? ciation rules. Several algorithms were proposed to gener? ate frequent closed itemsets, including CLOSE, A-CLOSE, CLOSET, CHARM and CLOSET + etc. However it's still hard for these algorithms to deal with dense and very large data.

In this paper, we analyze the search space of frequent closed itemsets and propose a new decomposition algo? rithm for mining frequent closed itemsets called PFC. PFC can dynamically generate non-overlapping partitions of the search space and mine frequent closed itemsets in each par? tition. Furthermore, each partition is independent and only shares the same source data with other partitions. So it is possible to implement PFC with multi-threads or paral? lel methods, and prune efficiently the search space of fre? quent closed itemsets. In this study, P FC is implemented in Java. We compare PFC with an author's C++ version of CLOSET + on some large VCI repository datasets and on the worst case. The preliminary experimental results demonstrate good performance of PFC for dealing with dense and very large data.

1 Introduction Association rule mining [1] is an important mining task that consists in searching all frequent item sets among trans? actions composed of data attributes. Many algorithms are proposed for discovering association rules from database.

But these algorithms usually suffer from the complexity of mining large or dense databases. We know that it generates a very large number of frequent itemsets and rules to mine association rules. Some research works [10] have focused on developing efficient algorithms based on closed itemsets in order to prune the search space or estimate the complex? ity of the mining task.

Closed itemsets theory relies on concept lattice theory mainly known as Formal Concept Analysis (FCA) [6].

Concept lattice structure has shown to be an effective tool for data analysis and knowledge discovery [9], and more especially for finding frequent closed item sets for association rules [10, 11, 12, 13]. It can generate formal concepts from the data to reveal the relations between objects and attributes. The concept lattice can be graphi? cally represented as conceptual hierarchies, allowing the analysis of complex structures. A formal concept (or concept) consists of an extent (transactions) and an intent (attributes). There is a dual relationship between intents and extents. In the context of association rule mining, all frequent itemsets are uniquely determined by the set of frequent closed itemsets. The problem of finding frequent itemsets from data for association rules can be reduced to finding frequent closed itemsets with closed itemset lattice.

And it's possible to prune the number of rules produced without information loss using closed itemset lattice [10].

For example, considering a database that contains three transactions 01 (al' a2, ... ,a50), O2 (al' a2, ... , aSO), 03 (al' a2, ... , alOo), and the minimal support threshold is 2, a frequent closed itemsets mining will generate  only two frequent closed itemsets: (al' a2, ... , a5o) and (al' a2, ... , aso). So this work focuses on determining frequent closed itemsets.

In recent years, extensive studies have proposed fast algorithms for mining frequent closed itemsets, such as CLOSE, A-close [10], CLOSET [11], CHARM[13] and    CLOSE, A-close [10], CLOSET [11], CHARM[13] and CLOSET + [12], free-sets [3] etc. The recent work of Wang and colleagues [12] shows CLOSET+ is one of the best al? gorithms for mining frequent closed itemsets. CLOSET + have good performance for sparse data. However, when the data is dense or the size of items is large, it has different performance even if the minimal support threshold is high.

All these algorithms are based on previous designs of al? gorithms to generate frequent itemsets, like Apriori [2], FP? growth [7], etc ... They aren't linked to previous reported algorithms that generate formal concepts [8]. In their ex? perimental work, Kuznetsov and Obiedkov [8] have demon? strated the potential of Ganter's algorithm (NextClosure), and in a recent work Fu and Mephu [5] have shown that Ganter's algorithm is one of the best when dealing with dense datasets. Fu and Mephu have extended this algorithm in order to deal with large data. By using a "divide-and? conquer" strategy, they proposed a new algorithm called ScalingN extClosure [4] that creates different partitions of the search space and allows to deal with huge and dense data.

In the present work we design a new algorithm to generate frequent closed itemsets, that is based on Scal? ingNextClosure. We propose a new algorithm : PFC (par? allel Partitioning Frequent Closed itemsets) for mining fre? quent closed itemsets. This algorithm is a kind of decom? position algorithm of concept lattice. We use it to dynam? ically partition the search space into some non-overlapping and independent search sub-spaces. For each partition, we mine all frequent closed itemsets. There is no relation be? tween each partition. The partitions only share the same source data. We can deal with any partition independently.

So we can apply this algorithm for parallel, distributed and network computing.

We compare our algorithm PFC to the C++ executable code of CLOSET + provided by its author, and discuss the results.

The rest of the paper is organized as follows. We present the notions of frequent closed itemsets in section 2. The search space of closed item sets is analyzed in section 3.

Section 4 presents an algorithm of mining frequent closed itemsets. Some experimental results are demonstrated in section 5.

2 Frequent closed itemsets Given a specified minimal support threshold (min? support) and minimum confidence, the problem of min? ing frequent closed itemsets is decomposed into two sub? problems: (1) Finding the frequent closed itemsets which have sup? port above the predetermined min-support. The whole per? formance is mainly determined by this step. It is a hard problem to generate all frequent closed itemsets for very large data. So we need more effective method to mine fre? quent closed itemsets.

(2) Deriving all rules on frequent closed itemsets, based on each frequent closed itemset, which have more than pre? determined minimum confidence.

Definition 2.1 Data context or formal context is defined by a triple (O,I, R), where 0 and I are two sets, and R is  a relation between 0 and I. The elements of 0 are called o?jects or transactions, while the elements of I are called items or attributes. A subset of I is called itemset.

Definition 2.2 Given a subset A &lt;:;;; 0 from a data context (O,I, R), we define an operator that produces the set A' q{their common attributes for set A &lt;:;;; 0: A':= {b E I I (a,b) E Rfor alla E A} Dually, we define 8' for subset q{ attributes 8 &lt;:;;; I, 8' : 8':= {a E O  I (a,b) E Rfor allb E 8} These two operators are called the Galois connection for (O,I, R). According to the definition, (A')' and (8')'    for (O,I, R). According to the definition, (A')' and (8')' are noted as A" and 8" respectively, we have: 8 &lt;:;;; I :::} 8" &lt;:;;; I and A &lt;:;;; 0 :::} A" &lt;:;;; 0 Thus we define two closure operators as 8 -+ 8" for set I and A -+ A" for set 0.

Definition 2.3 A formal concept (?{ the data context (O,I, R) is a pair (A, 8), where A &lt;:;;; 0, 8 &lt;:;;; I, A = 8' and 8 = A'. A is called extent, 8 is called intent.

For every set 8 &lt;:;;; I, according to the definitions 2.2 and 2.3, (8',8") is always a concept.

Definition 2.4 An itemset C &lt;:;;; I is a closed itemset iff C" =C.

Thus, a closed itemset is the intent of a formal concept.

Definition 2.5 Let (0, I, R) be a data context, X and Y be closed itemsets, where X C I, Y C I. An association rule is an implication q{the form X:::} Y, where X n Y = 0. X is called antecedent while Y is called consequence q{ the rule.

Each rule has two measurements, support and confi? dence. Support indicates how frequently the pattern occurs, while confidence is a measure q{ the rule ',I' strength. The support q{ an itemset X, denoted as support (X), is the number of the extent of X. The support of the rule X :::} Y, denoted as support( X :::} Y), is support(XUY).

The confidence of the rule, denoted as conf(X :::} Y), is support( X UY) / support(X).

Proposition 2.1 Let an itemset C &lt;:;;; I be a closed itemset, the support ofC is: support(C) = IIC'II Definition 2.6 1{ C1 and C2 are closed itemsets, C1 &lt;:;;; C2, then we say that there is a hierarchical order between C 1 and C2. In this case, C1 is called super-closed itemset, C2 is called sub-closed itemset, and we denote C1 :::; C2? All closed itemsets with the hierarchical order of closed itemsets form q{ a complete lattice called closed itemset lattice.

IIli Figure 1. An example of the search space of closed itemsets.

The first partition #1 #2 The second partition /18 (== a4a3a2al ---) IIli Figure 2. An example of partitioning the search space of closed itemsets.

3 Decomposition of the search space of closed itemsets Closed itemset lattice is an ordered set. The ordered re? lation and the duality are two properties for closed itemset lattice. We consider that the ordered relation and the duality should be studied for mining frequent closed itemsets.

The ordered relation among the closed itemsets can help us to prune the number of generating frequent closed item? sets. The following property allows us to adopt the strategy: generate super-closed itemsets C  if support (C) &gt; min-support then generate its sub-closed item sets else don't generate any sub-closed itemset end if Proposition 3.1 Let C1 and C2 be closed itemsets, where C1 :::; C2. We have : support(Cd :::: support(C2) Proof: According to the definitions of the formal concept and the closed item set, (C ? , Cd and (C?, C2) are two con? cepts. If C1 :::; C2, then we have C? :::: C?, that is to say support(Cd :::: support(C2) by proposition 1.

In order to analyze the search space of closed itemsets, we show an example : a data context which contains four attributes a4, a3, a2, al. The figure 1 shows the search space of non-empty closed itemsets.

In this example, each node is an item set, the label of    In this example, each node is an item set, the label of node indicates the order of the generating itemsets. If we suppose itemset a3a2 (the label is #6) is not frequent, we can stop to generate its sub-closed itemsets. That is to say we needn't generate a3a2al, a4a3a2 and a4a3a2al. So the search space of closed item sets can be pruned.

On the other hand, we try to partition the search space of closed itemsets. Here (in figure 2) two partitions are given only to illustrate the main idea. The first partition is for all closed itemsets which contain only a3, a2, al. The second partition is for all closed itemsets which contain a4.

It is obvious to get the following property from this ex? ample: Proposition 3.2 Let ai E I be an attribute ql the data con? text (0, I, R), Ci be a closed itemset which contains ai. We have support(ai) :::: support(Ci) When the extent of an attribute is smaller than min? support, this attribute can be omitted. For example, if support ( a4) &lt; min-support, we can omit all itemsets in the second partition.

1 x x x x 1 x x x 2 x x x x x 2 x x x x 3 x x x x x x 3 x x x x x 4 x x x x 4 x x x x 5 x x x x x 5 x x x x 6 x x x x x x 6 x x x x x 7 x x x x 7 x x x 8 x x x x 8 x x x x Data Context Frequent ordered context Figure 3. An example of data context and frequent ordered context (min-support is 2).

In order to decompose the search space of closed item? sets, we need to answer these questions: (1) How can we decompose the search space? (2) Are there closed item sets in each partition? (3) What relations are there among parti? tions?

Analyzing all closed itemsets of a context and their search space, we define the frequent ordered context, and analyze its property in order to answer the questions.

Definition 3.1 A data context is called frequent ordered context, if 1) the attributes, of which extent is smaller than min-support, are omitted, 2) we order the attributes (?f con? text by the number qf objects that verifies each attribute from the smallest to the biggest one, and 3) the attributes with the same objects are merged as one attribute.

Thus in a frequent ordered context, all attributes are fre? quent. The smallest attribute is in the first column, and the biggest one is in the last column. An example is shown in figure 3. This definition make sure that the partition of fre? quent closed item sets is not empty.

Here we define folding search sub-space in order to de? compose the search space.

Definition 3.2 Let the attribute set I qf a data context (O, I, R) be {am, ... , ai, ... , ad. Given an attribute ai E I, let F3S (ai) = { all subsets of the set {ai, ai-I, ... , ad which contain a;}, F3S(ai) is called folding search sub-space (F3S) qf ai? In fact, a closed itemset is a subset of I of the data con? text ( 0, I, R), so all subsets of I are elements of the search space. This search space can be considered as the folds of certain subsets of I. All subsets of {am, ... , ai, ... , a1} = U {all F3S(ai), 1 ::; i ::; m}.

Proposition 3.3 Given a data context (0, I, R), Vai E I, (fthe number qfthe extent (?f ai is n, then the size qtP3S( ai) is the minimum (?f2n - 1 and 2i-1.

Proof: Vai E I, there are 2i-1 subsets of {ai, ai-I, ... , ad that include ai. On the other hand, if the number of objects that verify attribute ai is n, we have 2n - 1 object subsets corresponding to ai. So there are at most 2n - 1 closed itemsets that include attribute ai.

Thus the size of F3S (ai) is the minimum of 2n - 1 and 2i-1.

2i-1.

This proposition demonstrates the ordered context can prune the search space of closed itemsets.

Proposition 3.4 For a frequent ordered context, there are frequent closed itemsets in the folding search sub-space of an attribute.

Proof: For a frequent ordered context (O, I, R), Vai E I and aj E I( i &lt; j), we have {a;}' ? {aj r, so {a;}" is in F3S ( ai), otherwise :3aj(i &lt; j), {a;}' &lt;:;;; {ajr.

This property of frequent ordered context allows certain search sub-spaces of an attribute to be formed a partition, and the partition is not empty.

We can conclude our analysis in order to anwer the three above-mentioned questions. We need to order the data con? text into frequent ordered context. And then we can divide the search space into sub-spaces that include some F3S of attributes. Such partitions are not empty. In order to bal? ance the partitions, according to propositions 3.3 and 3.4, the attributes that are bigger can be put together to form the partition. For example, a3, a2, a1 can be in one partition, the other partition contains only F3S( a4). The concrete par? titioning method and the relations of partitions will be pre? sented in the next setion.

4 A decomposition algorithm: PFC We propose a new algorithm, called PFC, which has two steps : determining the partitions (see algorithm 1) and gen? erating all frequent closed itemsets in each partition (see al? gorithm 2).

4.1 Determining the partitions First, the frequent ordered context should be generated from a data context according to the min-support (MS). In our algorithm, we don't preprocess the data, we only need binary context with the name of the items and transactions.

We don't generate a data file for frequent ordered context, the frequent ordered context is only generated and stored in main memory. CHARM and CLOSET + also preprocessed the data to improve their performance.

Then we determine the partitions. Each partition con? tains certain attributes. Let the arribute set of the fre? quent ordered context I = {am, ... , ai, ... , ad. We choose some attributes (including am and aI) of frequent ordered context to form an order set P. If the num? ber of the elements of P is T, we have a 1 = apl &lt; ap2 &lt; ... &lt; apk &lt; &lt; apT = am. We denote u (1 ::; k ::; T - 1) [ai,ai] = F3S(ai) (1::; i::; m) [apk' apk+l[ (0 ::; k ::; T - 1) and [am,am] are the partitions. All these partitions cover the search space of fre? quent closed itemsets. We can mine frequent closed item? sets in each partition. So the search space of frequent closed itemsets is U [apk,apHl[ U [am, am] O&lt;k&lt;T-I In the first step of the algorithm (see algorithm 1), we can decide the number of partitions by a parameter P P(O ::; P P &lt; 1) according to the size of data and our needs. P P is used to approximately balance the size of each partition.

When P P = 0, we get only one partition that contains all the search space. From the algorithm 1, we get the following proposition easily.

Proposition 4.1 The smaller the value of P P is, the more the number of partitions is.

All elements of listPartitions correspond to all Pk (listPartitions[O] --+ PT, listPartitions[l] --+ PT-I, ... , listPartitions[T - 1] --+ PI ). We use all elements of listPartitions to form the partitions [apk' apk+l [ and [am, am], where 0 ::; k ::; T - 1. Here Pk means the posi? tion of an attribute of the frequent ordered context, and we use it to represent the attribute apk of the frequent ordered context.

context.

4.2 Generating frequent closed itemsets For each generated partition, we mine the frequent closed itemsets with algorithm 2. The main method is based on the following propositions.

Algorithm 1 Determining the partitions 1: input a parameter PP (0::; PP &lt; 1) 2: input a parameter M S (min-support) 3: input data context 4: generate the frequent ordered context 5: create a list listPartitions to store the position of each par? tition 6: m = cardinal of the attribute set of the frequent ordered con- text 7: min:= m 8: k:= 0 9: while (min &gt; 0) do 10: add min to listPartitions 11: min:= (int) ((double)min * PP) 12: k := k + 1 13: end while 14: T:= k liT is the number of the partitions 15: output listPartitions Proposition 4.2 Each partition is independent.

Proof: When we find the frequent closed itemsets of a par? tition, we only need to know the first subset and the last sub? set of this partition. And then we build next closed itemsets from the first subset until the last subset, the computing is closed in this partition. So each partition is independent. In other words, we can find all closed item sets of one partition independently.

Proposition 4.3 For a data context (O, I, R). Given A c I, I ={ am, ... , ai, ... , aI}, A --+ A" is the closure opera? tor. All closed itemsets form a closure system. The smallest one of all closures that is larger than A, called next clo? sure (next closed itemset), with respect to the lexicograph? ical order is A EB ai , where EB is defined by A EB ai = (A n (am,am-I, ... ,ai+d U {ad)", where A ? I and ai E I, ai being the smallest element of I with A &lt; A EB ai by the order: am &gt; ... &gt; ai &gt; ... &gt; al.

In other words, for ai E I\A, from the smallest element to larger one of I\A, we calculate A EB ai, until we find the first time A &lt; A EB ai, then A EB ai is the next closed itemset. The proof can be seen in [6] .

Proposition 4.4 Let A be an itemset, A = {bk, ... , bi, ... , bI}, bi E I, bk &gt; ... &gt; bi &gt; ... &gt; bI &gt; al . Let B = (A n (am' am-I, ... , ai+d U {ad). When ai &lt; bI or ai E A, support(B) &lt; support(A).

Proof: When ai &lt; bI or ai E A, A c B, so support(B) &lt; support(A).

This proposition gives us a method to prune the non? frequent closed item sets when we find the next closed item? set. From one item set, we can find its next closed itemset.

But if the support of a closed itemset :::; min-support, its next closed itemset can be omitted using the proposition 7.

Definition 4.1 Given a set A c I, A = {bko ... , bi, ... , bl}, bi E I. The next candidate of frequent closed item? set is the set Al?Iai = (An (am' am-I, ... , ai+d U {ad )" , where ai &gt; bl and ai ? A, ai being the smallest element of I with A &lt; A l?I ai by the order: am &gt; ... &gt; ai &gt; ... &gt; al? So for each partition, we compute the next closed itemset from {apK } to {apk+l} (see algorithm 2). There is no re? lation between each partition. The partitions only share the same source data. We can deal with any partition indepen? dently.

Algorithm 2 Generating all frequent closed itemsets in each partition 1: input frequent ordered context 2: input the index (1 ::::: index::::: T) of partition 3: get Pk and Pk+l from listPartitions with index    3: get Pk and Pk+l from listPartitions with index 4: A f- {apk} Ilbegin of partition 5: END f- apk+1 Ilend of partition 6: EXI:= false 7: A f- A" 8: while (!EXIT) do 9: ifllA'l1 &lt;= MSthen 10: if IIA'II = MS then 11: output frequent closed itemsets 12: end if 13: A f- next candidate of frequent closed itemset 14: else 15: output frequent closed itemsets 16: A f- generate next frequent closed itemsets 17: end if 18: if END E A when searching the next closure then 19: EXIT := true 20: end if 21: end while 4.3 An example With our approach, we can dynamically generate parti? tions according to the size and density of the data, and our need. In figure 4, we demonstrate how to generate different partitions for the same data.

Using the data context of Figure 3, we show below an example (see Figure 4) of PFC algorithm. First, we need to generate the frequent ordered context. Given the min-support M S = 2, a2 and ag are merged as a2. As support (a5) &lt; 2, a5 is omitted. The attribute set of the frequent ordered context is: as, a6, a7, a4, a3, a2, al? And then, we can give a value to the parameter to deter? mine the partitions, for example, when P P = 0.5 we get 3 partitions: [aI, a7[, [a7, as[ and [as, as]. When PP = 0.6, there are 4 partitions to generate: [aI, a4[, [a4, a6 [, [a6, as [ and [as, as]. When PP = 0.8,6 partitions are generated: [aI, a3[, [a3, a4[, [a4, a7[, [a7, a6[, [a6, as[ and [as, as] .

When P P = 0.0 there is only one partition: [aI, as].

At the end, using algorithm 2, we find separately all fre? quent closed item sets in each partition. For the example of 3 partitions: [aI, a7[, [a7, as[ and [as, as], the frequent closed itemsets of each partition can be generated by algo? rithm 2 (in Figure 4, Ci is the label of frequent closed item? sets. We get 4 frequent closed itemsets in [aI, a7[: {ad with 8 objects, {a2' al} with 5 objects, {a3, al} with 5 ob? jects, {a3, a2, ad with 2 objects, {a4' ad with 4 objects, {a4' a3, adwith 3 objects.

5 Experimental results We have implemented PFC algorithm in Java. The experiments are performed on a PIII900 computer with 512Mo RAM and Windows XP system. We have tested our algorithm on some datasets of the UCI repository and the worst case. The worst case means the case when the sizes of (') and I of data context (('), I, R) are equal to n, and each attribute is verified by n - 1 different objects, each object possesses n - 1 different attributes.

In order to supply the performance references of our al? gorithm, CLOSET + is tested on the same data. This isn't a real comparison, because 1) PFC is implemented in Java, but CLOSET + in C++. C++ can improve performance than Java. 2)CLOSET + need preprocessed data. But PFC needs certain time to generate frequent ordered context. If the size of data is large, it will increase time-space cost. However, the comparison with CLOSET + algorithm shows that our algorithm is better for large and dense data.

Table 1 shows some examples of our experimental re? sults on real data. The results demonstrate CLOSET+ out? perform PFC in most case. But if the size of item set is large and min-support is low, PFC is faster than CLOSET +. From table 1, the results of audiology data, lung-cancer data and soybean-large data show that PFC is faster than CLOSET +.

For the worst case, PFC is faster than CLOSET +. In    For the worst case, PFC is faster than CLOSET +. In table 2, the data name shows the size of data, for exam? ple, worst16 means the worst case data with 16 objects and items. It's hard to generate frequent closed itemsets for the worst case, so we only test for high min-support. Symbol " I" means the algorithm needs very long time to gener? ate frequent closed itemsets, so we didn't get the result.

Here we demonstrate the number of partitions and maxi? mum time for one partition for PFC.

In summary, PFC is efficient for large dense data, and is faster than CLOSET + when the number of items is much higher than the number of objects.

PP=O.O PP=O.S PP=0.6 PP=0.8 Total partitions: I Total partitions: 3 Total partitions: 4 Total partitions: 6 #Partition 1 [al ,  as1 #Partition l [al, a7[ #Partition I [al , a4[ #Partition 1 [ al, a3 [ CO: al [8J CO: al [81 CO: al [81 CO: al [8J CI : a2, al [SJ Cl : a2, al [Sl CI : a2, al [SJ Cl : a2, al [Sl C2 : a3, al [Sl C2 : a3, al [SJ C2 : a3, al [SJ #Partition2 [ a3, a4[ C3 : a3, a2, al [2J C3 : a3, a2, al [21 C3 : a3, a2, al [21 CO: a3, al [SJ C4 : a4, al [4J C4 : a4, al [41 #Partition2 [a4, a6[ Cl : a3, a2, al [21 CS : a4, a3, al [31 CS : a4, a3, al [3J CO: a4, al [4J #Partition3 [ a4, a7 [ C6 : a7, al [4J #Partition2 [ a7, as[ Cl : a4, a3, al [31 CO: a4, al [4J C7 : a7, a2, al [3J CO: a7, al [41 C2 : a7, al [4J Cl : a4, a3, al [31 C8 : a6, a4, al [31 CI : a7, a2, al [3J C3 : a7, a2, al [3J #Partition4 [ a7, a6 [ C9 : a6, a4, a2, al [2J C2 : a6, a4, al [31 #Partition3 [ a6, as[ CO: a7, al [4J CIO: a6, a4, a3, al [2J C3 : a6, a4, a2, al [21 CO: a6, a4, al [3J Cl : a7, a2, al [31 C11 : as, a7, al [31 C4 : a6, a4, a3, al [2J C I : a6, a4, a2, al [2J #PartitionS [ a6, as [ C12: as, a7, a2, al [2J #Partition3 [as, as1 C2 : a6, a4, a3, al [21 CO: a6, a4, al [3J C13 : as, a7, a3, al [21 CO: as, a7, al [31 #Partition4 [as, asJ Cl : a6, a4, a2, al [21 CI : as, a7, a2, al [2J CO: as, a7, al [3J C2 : a6, a4, a3, al [21  C2 : as, a7, a3, al [21 Cl : as, a7, a2, al [21 #Partition6 [as, asJ C2 : as, a7, a3, al [2J CO: as, a7, atf31 Cl : as, a7, a2, al [21 C2 : as, a7, a3, al [2J Figure 4. An example of the different partitions for the same data (The min-support is 2. The number in brackets [ ] after each frequent closed itemset means the its support).

6 Conclusion In this paper we analyze the search space of frequent closed itemsets and propose a new lattice-based algorithm PFC for mining frequent closed itemsets from data. PFC can generate non-overlapping partitions of the search space and mine frequent closed item sets in each partition. There is no relation between each partition in this algorithm. The partitions only share the same source data. We can deal with any partition independently. So we can apply this algorithm for parallel, distributed, or network computing. PFC shows good performance when dealing with very large data. Pre? liminary experimental results show that PFC outperforms CLOSET + for dense data. It is an efficient algorithm to mine frequent closed itemsets for large and dense data.

The ongoing research is studying the problem of balance of all partitions. And we will implement this algorithm on a parallel or distributed platform.

Acknowledgements We are grateful to anonymous reviewers for helpful com? ments. This research benefits from the support of IUT de Lens and the region NordlPas de calais.

