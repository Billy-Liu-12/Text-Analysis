Multi-label Classification based on Association Rules with Application to Scene  Classification

Abstract   In this paper, a multi-label classification based on association rules is proposed. To deal with multiple class labels problem which is hard to settle by existing methods, this algorithm decomposes multi-label data to mine single-label rules, then combines labels with the same attributes to generate multi-label rules. It extracts partial dataset features to build the initial classifier through assembling, and conducts classification prediction by assembling the classifiers. Thus, the computational complexity caused by the high dimensional attributes decreases while the performance and efficiency increases. Then, the multi-label classification algorithm based on association rules which achieve good performance in an application to scene classification.

Keywords?association rules; multi-label; ensemble; classification 1. Introduction   Generally speaking, association classification involves two steps. The first step is to find all Rule Consequent as class association rules (CARs) [1] of class label. The second step is to choose rules with high priority from the CARs to cover training set. The priority evaluation of rules usually depend on the confidence, support, rule length or common quality standard of classification rules.

Because of the high interpretability, classification accuracy, in subsequent years, lots of new associate classification algorithms have arisen. The most representative is Classification based on Multiple  class-Association Rules (CMAR) [2]proposed by W. Li in 2001 and a classification method named Classification based on predictive association rules (CPAR) [3] proposed by Yin.

However, all of these traditional associate classification algorithms can only process single class label problem, but in real-world applications multi-label problems are universal. Because a real-world object may be associated with a number of classes. For instance, an image may belong to several different classes simultaneously, while an image of snow-mountain can belong to snow as well as mountain.

Until now, few researches have been done on multi-label problems home and abroad. Comit? expand binary decision tree to process multi-label data[4]. Li Hong proposed a more effective algorithm of multi-label decision tree SCC_SP[5]; they adopt an evaluation method of class-label set similarity by introducing identity and consistency of sets, and compute the class-label set similarity of multi-label data by this way, then regard it as the evaluation index of performance of attribute classification.

Elisseeff and Weston used to classify multi-label data[6], and achieve good performance in experiment proceeding on gene data set of yeast. Adaboost.MH[7] is a classical assembled algorithm of all these method based on multi-label classification, because of introducing assembling method. The performance and stability of decision tree have been enhanced. Zhi-hua Zhou et al proposed the ML-KNN[8], the multi-label learning algorithm. Classification algorithms based on association rules have good interpretability and high classification accuracy, but few of these can be used on   DOI 10.1109/ICYCS.2008.524     multi-label learning. Fadi A. Thabtah proposed (MMAC) [9] in 2004. But without any experiment proceeding on multi-label data set in their paper, so we cannot confirm the performance of MMAC upon multi-label learning. In this paper, we proposed an associate classification algorithm based on multi-label learning, by analyzing the difficulty of using associate classification algorithm to proceed multi-label learning, and compare it with other multi-label learning algorithms.

2. Basic theories  2.1. Similarity definition  (1) Similarity between two label sets Set two label set: iY , jY , and we define ( , )i jsim Y Y as  the similarity of iY , jY ,then  ( , ) ( , )  ( , ) i j  i j i j  consistent Y Y sim Y Y  occurrence Y Y =  (1.)  ( , )i jconsistent Y Y  represents the sum of number of label both existing and number of label neither existing in two label sets; ( , )i joccurrence Y Y  is the number of all different labels existing in candidate label set. For example, if candidate label set c={c1,c2,c3}, and  is ( 1, 2), ( 2)jc c s c= = , then ( , ) 2i jconsistent S S = , occurrence is 3.

(2) The similarity of label sets Based on the calculation of similarity between two  label sets, the similarity of label sets, i.e., the set of label set, is obtained by Formula (2). Set S as a set of label set {Y1,Y2,?, Yn } , n is the number of records, then   ( , ) ( ) i ji j  n  similarity Y Y SIM Y  C <=  ?  ( 2.) Performing this Formula on an internal node of  Decision Tree, we can get the similarity of node?s label sets.

2.2. Evaluation measures  The evaluation criterion of performance based on multi-label is different from the one based on single class label, general evaluation criterion of single class label include: accuracy, precision, recall and so on. But the evaluation criterion in field of classification based on multi label is more complicated.

Definition 1: ( , )i if x y  denote the probability that instance ix  may belong to class iy ; Definition 2: ( , )f i irank x y  denote the rank index of  iy  in class label set ix ; With regard to multi class test set  1 1 2 2{( , ),( , ),..., ( , )}p pD x Y x Y x Y= , the main evaluation criterions are: (1) Hamming Loss  Denote the proportion of wrong predictions of class label from all predictions. For instance, a label not belonging to the instance is predicted or a label belonging to the instance is not predicted. The performance is perfect when hlossS(h) = 0; the smaller the value of hlossS(h), the better the performance.

It is defined in Formula (3).

1 1( ) | ( ) | p  i i i  h lo s s h h x Y p Q=  = ??  (3.) Actually this formula is also suitable for performance  evaluation of a multi-class single-label. Because for each instance, |Yi|=1 is possible, where the data set is single class label data set, and when the classifier wrongly predict class label, Hamming loss hlossS(h)=2/Q (2) One-Error  It denotes the percentage which predicts class label with highest priority for instance not belonging to the original class label of instance. The performance is perfect when one-error S(f) =0; the smaller the value of one-error S(f), the better the performance.

It is defined in Formula (4).

1( ) arg max ( , ) p  s y Y i i i  oe f f x y Y p ?=  ? ?= ?? ??  (4.) While this index is used to evaluate performance of  single-label classifier, it actually denotes the error of single-label classifier, ( )soe f =1-accuracy.

(3) Coverage  It denotes the percentage of the class-label which belongs to test set but not predicted by classifier in all predicted class-labels. It is loosely related to precision at the level of perfect recall. The smaller the value of coverage S(f), the better the performance.

It is defined in Formula (5).

1cov( ) m ax ( , ) 1 y Y  p  f i i  f rank x y p ?=  ? ?= ?? ?? ?? (5.)  (4) Average Precision It denotes the percentage of the predicted class-label  whose priority rank is higher than original class-label belonging to instance and which also belongs to the original class-label set of predicted instance in all predicted class-label. It is originally used in information retrieval (IR) systems to evaluate the document ranking     performance for query retrieval. The performance is perfect when avgp(f) = 1; the larger the value of avgp(f), the better the performance.

It is defined in Formula (6).

1 1( ) | | |  |{ | ( , ) ( , ), } | ( , )  i  p  s i i  f i f i i  y Y f i  avgprec f p Y  y rank x y rank x y y Y rank x y  =  ?  =  ? ? ?? ?  ?  ?   (6.)    3. Problem analysis  Generally, existing associate classification algorithms consist of two steps: rule mining and building classifier. In the step of rule mining, input train set first; find out frequent item set through algorithm to build classifier; then filter rules by a certain ways, keeping rules with best classification performance and sending to the classifier. There are two main difficulties for existing associate classification algorithm proceeding multi -label learning according to steps above: how  to generate multi-label rules and how to resolve the high dimensional attributes caused by the computational complexity.

3.1. Multi-label rules  Traditional associate classification algorithm can only product rules containing single class-label, if simply regard multi class label as single class label, for instance, turn multi class label(c1,c2,c3) to a new single class label will lead to produce plenty of rare categories, and there will be few rules higher than support threshold to classify these data. It is just the method Kazawa adopted, he put label into a similarity-induced vector space by the way of processing mass assumable data-scarcity(q topics will product 2 class),during this procedures, original vectors resemble labels are placed close. But this algorithm cost huge with low running efficiency.

3.2. High Dimensional Data  Generally, data based on multi class label belong to high dimensional data[10], can extract hundreds of effective feature, such as image data. Mining rules by general associate classification algorithm will cause low efficiency, and produce mass rules, about 2..n(n>100) rules will be produced. Due to the space possessed by data will turn to more sparse as the dimension increased, produced association rules can hardly proceed classification prediction correctly on data, thereby curse of dimensionality emerged.

4. Algorithms  4.1. Propose Algorithms  To deal with the difficulties above, firstly we exploit steps of: resolving data class label->mining single class label rules->assembling single class label rules to produce multi class label rules; then, during the rule filtering step, using training data traversing rule set to find out rules with the highest similarity to class label of training data; after that, by the method based on assembling, we extract part of feature attribute from data set to build K subsets, train K subsets to produce K base classifiers, and assemble base classifier to predict test sets by voting. By this way, we introduced a new algorithm to settle association rules classification problem with application to scene classification.

4.2 Description of Algorithms  The multi-label classification based on association rules include 3 parts: (1) Part 1. Mining effective multi-class label rules  Firstly, we extract part of features of data set K times, form K portion of data set. Then we input the data set, Input: training data set D_train1 to D_trainK Output: rule set Rule1 to RuleK 1) Divide data set to two parts: feature and class label 2) According to features, divide multi-class label data into single-label data with only single item in class label part.

Example: di?{A1,A2,A3,(L1,L2,L3 ? di1?{A1,A2,A3,(L1)}; di2?{A1,A2,A3,(L2)};di3?{A1, A2, A3, (L3)}. Find out all frequent models above set Supp.

3) Find out all rules above set Conf by frequent model.

4)Combination rules with same feature and different class label to one rule, unchanged the features and merge the class labels.

(2) Part 2. Build base classifier by filtering rules.

(Chose rule set based on database coverage) Input: Rule set Rulei, training data set D Output: Base classifier Ci 1) Order the rules in rule set as level descending order 2) Compare rules: R1>R2(R1 higher than R2 in rank) ? If: conf (R1) > conf (R2) ? elself: conf (R1) = conf(R2) but sup(R1) > sup(R2); ? else: conf (R1) = conf(R2) and sup(R1) = sup(R2) but R1 below R2 in left feature value     3) Confidence of multi class label (k denotes class labels, k>1) rule.

K   ( ) ( )j ji i j  c o n f R c o n f R =  = ?  (7.) 4) Set cover count of each rules in rule set to 0.

5) While Training data set not empty do { ? for Let each test set traverse rules list by ordering the data. do { ? while Number of rules not empty. do { ? for Check each rule R in rule set by the data of rank descending order. do{ ? if The class label of test data match the class label of rule completely. Then the data is covered by the rule, number of rules+1.

? else if Have not found any rule matching the data class label completely, then Find out the rules with highest ( )SIM Y .

? else if Found ( ) ( )i jSIM Y SIM Y= , then chose rules with high priority.

? else if No rule can cover this data, jump to next data}} ? if Cover count of rule is 0,then Delete this rule from rule set; Assemble the rest rules to build classifier Ci (3) Part 3. (Method of combination) Merge these K classifiers C1, C2, C3?Ck to form assembling classifier, conduct classification prediction.

Input: base classifier C1 to Ck, check data set T.

Output: set of classifier C(x), probability P[(x/Li)] and predicted class label sets examine dataset T as outputs 1) D stands for the primitive training dataset, K is the number of the base classifier, T stands for testing dataset, L(L1,L2,L3?Lk) stand for rule set.

2) for i=1:k do ? Construct training set Di by D; ? Construct base classifier Ci by Di; ? end for 3) for every test record X belongs to T do  ))(,),(),(()( 21 * xCxCxCVotexC k=  (8.)  K xClxPOutputs i )()(  *  ==   (9.)  ? if ( ) 0.5iP x l ? ,then add il  to class label set of X; ? else? il  does not belong to class label set of X; ? end for The ultimate output of it?s algorithm is the combination of predicted set class label of test data set T and probability ( )iP x l that each data may belong to  different class label. Pre label and Outputs in output are all m*n matrix (m denote the number of class of test data set, n denotes the number of instance of test data set).

5. Experiment  We compare our algorithm to 4 general multi-label algorithm: ML-KNN?Adaboost.MH?MIMLSVM? Rank-Svm , and use the five index introduced above in section 2 to analyze the experiment result, by this way ,we can test the performance and stability of these algorithms, and can determine their efficiency by comparing the computing time. The four general multi-label algorithms above are conducted on Matlab R2006, and our algorithm conducted by Java and Matlab R2006 on eclipse3.2. All the experiments are conducted on T2600 with 2G memory, Windows 2000.

The multi-label Natural scene data set comes from the Corel image collection (download the data set and description about it in: http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/ann ex/miml-image-data.htm).

Table1 The image data set (d: desert, m: mountains, s: sea, su: sunset, t: trees)  label # images label # images d 340 m+su 19 m 268 m+t 106 s 341 s+su 172  su 216 s+t 14 t 378 su+t 28  d+m 19 d+m+su 1 d+s 5 d+su+t 3  d+su 21 m+s+t 6 d+t 20 m+su+t 1 m+s 38 s+su+t 4                 Figure1. Part of source images.

There are 2000 images in Natural scene data set, each  image may belong to one or more class of desert, mountains, sea, sunset, trees. Take figure 1 for example, image (a) only belongs to class desert, and  b  d c  a     image (b) only belongs to class mountains, they are all single-label data; but image (c) belongs to class mountains and trees simultaneously, and image (d) belongs to class sea and sunset, so (c) and (d) are multi-label data.

Table 1 described the distributions of images for each class, by this way; we note that about 22% of images belong to multi-label data. For instance, only one data belong to three classes (desert+mountains+sunset) simultaneously.

In order to investigate the influence caused by extracting different feature vector, two methods were used in experiments extracting feature vector of these images. The first one is SBN[11], which separate each image to 9 parts, each part has 15 features; in the second one, we firstly turn these images from RGB space to CIE Luv space[12], and extract feature vector by computing Euclidean distance. Then, we use 7?7 matrix to separate each image to 49 parts, so each image has been turned to a vector of 49?3?2=294 dimensions.

We use cross validation in experiment, the final result is typical value and covariance results of 8 experiments.

From Table 2, we can see our algorithm?s experiment result ranked first in 4 tables out of 4.

From Table 3, we can see our algorithm?s experiment result ranked first in 3 tables out of 4, the best in performance of four algorithms.

Table2 typical value and covariance of 5 index  Evaluation Criterion  our algorithm  ML-KNN Adaboost.

MH  MIML-svm  Hamming Loss  0.170?0.00  0.205?0.00  0.231?0.01  0.196?.0103  One-error 0.304?0.03  0.401?0.03  0.451?0.04  0.368?0.032  Coverage 1.043?0.07  1.135?0.09  1.258?0.13  1.115?0.122  Average Precision  0.765?0.01  0.741?0.02  0.708?0.03  0.756?0.022  Table 3 typical value and covariance of 5 index  Evaluation Criterion  our algorithm  ML-KNN Adaboost.

MH  MIML-svm  Hamming Loss  0.171?0.01  0.169?0.01  0.193?0.01  0.253?0.055  One-error 0.275?0.03  0.300?0.04  0.375?0.04  0.491?0.135  Coverage 0.904?0.10  0.939?0.10  1.102?0.11  1.382?0.381  Average Precision  0.814?0.02  0.803?0.02  0.755?0.02  0.682?0.093  6. Summary  In this paper, we examined the problem of multi-label learning by analyzing the utility of assemble theory in associated classification algorithm and the  method of generating multi-label association rule, and proposed an algorithm which generates a set of association rule classifier with different attribute sets by extracting feature of attribute set . This novel algorithm uses assembling to vote: assemble the outputs of multiple association rules classifier by creating the classifier through a random attribute subset.

The experiment results indicate that the multi-label classification algorithm based on association rules which achieve good performance in an application to scene classification.

Acknowledgement  This work was supported by the National Science Fund for Distinguished Young Scholars Nos.

60425310.

