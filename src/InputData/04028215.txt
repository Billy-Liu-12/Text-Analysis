

Abstract: With the rapid evolution of Internet and the broad  application of computer technology, the number of electronic texts increases quickly. Classifying these texts effectively becomes more and more important. This paper proposes a novel approach for automatic Chinese text categorization. By introducing fuzzy sets concept, it improves the existing categorization approaches based on association rules. The mined fuzzy association rules are used as a classifier to classify Chinese texts. Experimental results show that the new approach is feasible and the categorization effectiveness is enhanced.

Keywords: Association rules; fuzzy sets; Chinese text categorization  1. Introduction  With the rapid evolution of Internet and the broad application of computer technology, the number of electronic texts increases quickly. How to manage and utilize these resources becomes an imminent issue.

Automatic text categorization technique is one solution.

Automatic text categorization can be defined as assigning class labels to new texts based on the knowledge obtained in a categorization system at the training stage [1]. Several classifiers have been devised and used abroad, such as decision trees [2], neural networks [3], k-nearest neighbors [4], support vector machines [5] and association rules [1, 6].

Categorization based on association rules needs short time to train and classify, and the classifier built can be understood easily [1]. [1, 6] have applied association rules to classify texts, and experiments show that they are effective. But they ignored different words? contribution to text categorization. Words which exist in all categories do little contribution to categorization, while the words having large proportion in one category but small proportion in others do great contribution. In previous approaches, if a text contains all the words appearing in a rule, the rule matches the text or the rule satisfies the text. Because of ignoring the word frequency, the number of rules satisfying  the same text increases. This paper offers a novel approach which introduces fuzzy sets concept in, so the classification rules not only contain words, but also the corresponding frequencies. The degree of a word appearing in texts is restricted in the classifier, so texts can be classified more accurately than previous approaches.

2.  Background knowledge  2.1. Association rule  Definition1. Let I= {i1,?,im} be a set of items, and D be a set of transactions, where each transaction T?I. An association rule is an implication of the form ?X? Y?, where X?I, Y?I, and X?Y?? . The support of the rule X?Y is the percentage of transactions in D that contain both X and Y, and the confidence of the rule is the percentage of transactions in D containing X that also contain Y. The problem of discovering association rules from D consists of generating the rules having higher support and confidence than given thresholds. These rules are called strong rules [7].

2.2. Fuzzy association rule  Fuzzy method is proposed to cope with quantitative association rule mining [8]. For quantitative attributes, the ordinary treatment is transforming them into binary ones.

There are two traditional methods. The first is to partition the attribute domain into discrete intervals. Each element maps to its own interval. However, some related intervals may be missed because this partition excludes some potential elements near the sharp boundaries. The second is to divide the domain into overlapping regions. The boundaries of intervals overlap one another, so each element locating near the boundaries maps to two intervals, which surely overemphasizes the importance of these elements. The literature [9] proposed a soft method. The attribute domain is divided into several fuzzy sets, and an      element belongs to a set with a membership value. Fuzzy sets provide a smooth transition between members and non-members of a set, so fewer boundary elements are excluded. Moreover, because the membership degree of an element outside an interval is smaller than that in the interval, it is not overemphasized. Usually fuzzy sets are described by linguistic terms, which make them understood more easily.

Let D= {T1,?, Tn}be a set of transactions and Tj is the jth tuple. Let I={i1,?,im}be a set of items, where each attribute ik associates with a set of fuzzy sets },,{ 1 likik ff L , so the set I can be expressed by {y1,?,ym}, where yk is a fuzzy set associated with one attribute of I. The membership degree of Tj in the fuzzy set yk is denoted by Tj(yk).

Definition2. Let X={y1,?,yp}and Y={yp+1,?,yp+q} be fuzzy subsets of I, and X?Y=? . In the fuzzy association rule X?Y, the support of X is defined as:  n  yT XSup  n  j  p  i ij? ?  = == 1 1 )(  )(         (1)  The support of X?Y is defined as:  n  yT Sup  n  j  qp  i ij? ?  = == 1 1 )(  ?  (2)  The confidence of X?Y is defined as:  )( XSup SupConf =                  (3)  3.  Related work  The literature [6] used the approach presented in [10] to build classifier and it brought a new method to predict classes forward. However, the process of building classifier is complex. When one rule is added into the classifier, it is necessary to calculate the current total number of errors and to determine the default-class over again. The literature [1] proposed a method to select rules according to their ranks.

It is easier to build a classifier, but it also has some limitations. It predicts classes for new texts only by confidence, and it does not consider the frequency of words.

This paper improves the approach proposed in [1], combining the class prediction method in [6], and then designs a text classifier based on fuzzy association rules. In this section, we introduce the categorization approach in [1], and in the next section, the improved version will be introduced.

The process of building classifier in [1] is generalized  as follows.

Step1: Data preprocessing. In this step, data cleaning is  required. The useless words for building classifier can be weeded out according to a given list of stop words and TF/IDF values. For mining association rules, texts should be transformed into transactions. A text corresponds to a transaction, which consists of the cleansed words and the class label attached to the text.

Step2: Rule mining. The Apriori algorithm is used to mine association rules. It is needed to divide the training text set into subsets by category first, and then association rules are mined on each subset. When all rules are generated, each rule?s confidence can be calculated based on the whole rule set. Deleting the rule whose confidence is lower than the minimum confidence threshold, the remainder rules are strong rules.

Algorithm1: Mining association rules.

Input: A set of texts of the form di: {c, w1,?, wn},  where c is the class label attached to the text, and wi is a feature word of the text; The minimum support threshold min-sup; The minimum confidence threshold min-conf.

Output: A set of association rules of the form w1 ? ? ? wm?c.

Method: (1) generate the candidate 1-itemsets C1; (2) generate the frequent 1-itemsets F1; (3) for (i=2; Fi-1?? ;i++) (4)   { combine two items of Fi-1 to generate Ci; (5)   delete the item whose (i-1)-itemset?Fi-1 from Ci; (6)   delete the text containing no item in Fi-1; (7)   calculate the support of each item in Ci; (8)   according to min-sup, generate Fi; } (9) for each item M in Fi?i>1? (10)  generate the rule M?c.

Step3: Rule pruning. In the mining phase, too many rules are mined. Such a large number of rules would decrease the classification effectiveness and efficiency, so pruning is required. The criterion of pruning is that removing the rule containing more words but having lower confidence, and removing the rule which can not classify any training text correctly.

Definition3. Being given two rules R1 and R2, R1 has higher rank than R2 if:  (1) R1 has higher confidence than R2; (2) If the confidences are equal, the support of R1 must  exceed R2; (3) Both confidences and supports are equal, but R1 has  less attributes in antecedent than R2.

Algorithm2: Pruning association rules.

Input: The training text set D; The rule set S found in  the mining phase.

Output: The classifier ARC.

Method:  (1) sort S according to definition 3; (2) for each rule R1 in S (3) for each rule R2 having higher rank than R1 in S (4) if C(R1)=C(R2) and Conf(R1)<Conf(R2) and  Word(R1) ? Word(R2) (5) remove R1; (6) for each rule R of the reminder rules (7) {for each text d in D (8)   if R classifies d correctly (9)    save R and remove d; (10)   remove the rule classifying no text in D correctly. }  In line (4), C (R1) represents R1?s category, and Conf (R1) represents R1?s confidence. Word (R1) ? Word (R2) means that the words appearing in the antecedent of R1 include all the words in the antecedent of R2.

Table 1. The form of rules in the classifier       Step4: Classifying new texts. The rules survived in the pruning phase constitute the actual classifier. The form of rules in the classifier is shown in Table 1. Given a text without class label, the classification process is matching all rules in the classifier with the text. Usually, there are many rules matching the text. If these rules have the same consequent, no doubt the text falls into the category corresponding to the consequent. But it is more general that the consequents are different. In this case, we should estimate which class is more suitable. Here the average confidence is used to predict classes for new texts.

Algorithm3: Text classification.

Input: A new text d; The classifier ARC (the rules in  ARC have been sorted according to definition 3); The confidence threshold? .

Output: Class assigned to the new text.

Method:  (1) initialize the rule set S as? ; (2) for each rule R in ARC (3)  if R satisfies d (4)   if R is the first rule (5)    save R in S, and save its confidence in  Maxconf; // the first rule has the highest confidence (6)   else if R.conf > Maxconf -? (7)     save R in S; (8)  divide S into subsets by category;  (9)  calculate the average confidence on each subset; (10)  assign the class label corresponding to the subset  which has the highest value to d.

4.  Algorithm improvement  4.1. Data preprocessing  The paper [1] classified English texts, while we deal with Chinese texts in this paper. For Chinese texts, the first thing is segmenting them into words. Then the method of Information Gain [11] is used to select feature words from training texts. For building a fuzzy classifier, the weights of feature words are used to describe the frequency of words appearing in texts. We use the method of Entropy-weighting [11] to calculate weights. After all the weights are calculated, the operation of transforming texts into transactions is completed.

4.2. Fuzzification  The word-items in transactions are expressed by <word, weight> pairs, where weight is real-value. If rules were mined on these precise values, the obtained rules would be too special, and have no meaning [9]. In section 2.2, three methods are introduced to deal with quantitative attributes. We adopt fuzzy sets to define weight, and use natural language to describe the fuzzy sets. In our experiments, we partition the weight domain into three fuzzy sets and define three linguistic terms: f, m, and n to describe them separately. The partition is shown in Figure 1, which is drawn based on the distribution of weights. From Figure 1, we can calculate the membership values of each word in the three fuzzy sets, and transform original crisp transactions into fuzzy ones.

4.3. Fuzzy association rules generation  We improve the mining algorithm in [1] to mine fuzzy association rules. In the improved algorithm, the form of the input text is di:{c,<w1,l1>,?,<wn, ln>}, where li is a fuzzy set, and the form of the output rule is <w1,l1> ? ? ? <wm, lm>?c. In the mining process, we should store not only words in candidate item-sets Ci and frequent  <agriculture>, <farm produce> ?  Agriculture <computer>, <input> ?  Computer  <art>, <main body>, <create> ?  Art      item-sets Fi, but also their corresponding fuzzy sets. The membership values need not to be reserved. Take an example to explain: the membership degree of the word w1 in the fuzzy set f is 0.6, in the set m is 0.4, and in the last set n is 0. Suppose min-sup is 0.2, <w1, f>and<w1, m> should be stored in F1, but the values 0.6 and 0.4 need not to be stored.

4.4. Rule pruning  In the pruning algorithm in [1], if a text is classified by a rule correctly, the text is deleted and the rules covering the same text are removed, but some significant rules may be lost. To improve this algorithm, we set a cover-count variable [12] for each text, which is used to record how many rules cover the text correctly. The initialization is 0.

When a rule classifies a text correctly, add 1 to the variable value of this text. When the value equals to the given threshold, the text is deleted. That allows more selected rules. When classifying a new text, it may have more rules to consult. If a classifier contained too few rules, there would be many new texts matching with no rule. However, if the number of rules in a classifier were too large, the classification error rate would increase, and the efficiency would decrease. By experiments, it is found that when the cover-count threshold is 4, the classification performance is optimal. In our approach, the default-class [10] is defined, which will be used in the next phase. We choose the category which contains the largest number of training texts covered by no rule correctly to be the default-class.

The fuzzy classifier we build consists of many rules.

Only four rules are selected to show in Table 2. Compared with Table 1, we can find that the rules in the fuzzy classifier contain the frequency of words. We use the fuzzy sets corresponding to weights to represent the frequency. If the weight of a word locates between two fuzzy sets, in other words, if a word has membership degrees in two fuzzy sets simultaneously, the case that several rules contain same words but different fuzzy sets in antecedents would occur. The first two rules in Table 2 show this case.

Table2. The form of rules in the fuzzy classifier        4.5.  Class Prediction  The paper [1] applied average confidence to predict  classes for new texts, but there is a limitation. Suppose rules R1 and R2 both satisfy the new text d, where R1: <w1,f>, <w2,m>?C1, conf=92?,sup=5? and R2: <w1,f>, <w3,n> ?C2 ,conf=90?, sup = 30?. According to their confidences, C1 should be assigned to d, because the confidence of R1 is higher than R2. But in fact, assigning C2 to d is more reasonable. Although R2 has appreciably lower confidence, its support is far higher than R1. We adopt the method proposed in [6]. It used ? (min (preofrule) *sup * conf) to predict class, where min (preofrule) is the minimum time that the words in the antecedent of a rule appear in the new text, sup and conf are the rule?s support and confidence separately. We calculate the sum of min (preofrule) *sup * conf on each rule subset to replace the average confidence. Because multi factors are taken into account, the limitation existing in the average confidence method clears away. The categorization method in [1] does not consider the case that there is no rule matching the new text, which generally occurs. We use the default-class defined in rule pruning phase to deal with this case. If there is no rule satisfying the new text, we assign the default-class to it.

Classifying texts by fuzzy association rules is matching words and their frequency in rules with these in texts. If a word does not appear in the classifier, the word is useless, because it does not have any influence on categorization. Therefore, we can remove the words that do not appear in the classifier from texts to improve categorization efficiency.

5.  Experimental results and performance analysis  Experimental data comes from Chinese text database of Fudan University. The texts in the database belong to 20 categories, but the numbers of texts in each category are not equal. If training texts are too few, they are not sufficient to train a classifier. Thus, we ignore the categories which contain less than 100 texts, and only consider nine categories with larger number. Table 3 shows the numbers of texts of each category in both training set and testing set.

We use precision, recall, macro-average F1 and total correct rate on the whole testing set [13] to evaluate the performance of our approach. In experiments, four approaches are compared. In these approaches, we do the same preprocessing operation on the training texts, and in the rule mining process, the support and confidence thresholds are uniform, which are 1% and 30% separately.

The experimental results are shown in Table 4.

In Table 4, Method1 and Method2 represent the categorization method in [1] and [6] separately. Method3 is an improvement on Method1. It adopts the class prediction  <consumption, f>,<requirement, m>?Economy <consumption, m>,<requirement, n>?Economy  <agriculture, f>, <peasant, m>?Agriculture <art, f>,<main body, n>,<create, n>?Art      Table 3. Text database Category The number of  training set The number of testing set  Agriculture 1021 1022 Environment 1217 1218  Computer 1357 1358  Politics 1024 1026  Space 640 642  Art 740 742 History 466 468  Economy 1600 1601  Sports 1253 1254  Total 9318 9331   method of Method2, but does not use fuzzy sets concept.

Method4 is our approach, which uses fuzzy sets based on Method3.

Table 4 indicates that because Method3 derives merits from Method1 and Method2, its performance exceeds both of them. On the basis of Method3, Method4 introduces fuzzy sets in, and obtains better results.

6.  Conclusion  This paper introduces a new approach for text categorization, which is an improvement on the existing association rules categorization approaches. It first proposes to introduce fuzzy sets concept in text categorization.

Experiments show that it is feasible and the performance of our approach is improved further. In class prediction process, new texts should match with fuzzy rules, so it is needed to calculate weights of the words in new texts.

Therefore, this method is suitable for the case of classifying many texts once. Compared with traditional approaches, our method increases the spending on weights calculation and fuzzification. Taking classification effectiveness improvement as the result, the cost is worthy.

Acknowledgements  This work is partially sponsored by the National natural Science Foundation Project of China (60473045), the Research Plan of Hebei Province (05213573), the Research Plan of Education Office of Hebei Province (2004406).

