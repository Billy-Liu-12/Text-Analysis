Mining Weighted Closed Itemsets Directly for Association Rules Generation Under  Weighted Support Framework

Abstract-Closed itemset mmmg avoids many duplicate  itemsets generation, which derives the whole set of frequent itemsets exactly but is orders of magnitude smaller than the  latter. But generally traditional methods assume every two  items have same significance in database, which is  unreasonable in many real applications. This paper addresses the issues of mining concise association rules with different  significance, which can lead to reasonable but concise result.

We find that weighted itemset search space is enumerable  through exploiting the weighted support-significant framework.

By adopting specific technique, duplicate search space can be  pruned early with little cost. All the weighted closed itemsets  are derived directly while enumerating them without many  duplicate candidates generation. Then concise association rules  based on weights can be generated. As illustrated in  experiments, the proposed method leads to good results and  achieves good performance.

Keywords-concise association rule; weighted closed itemset; support-significant; algorithm  1. INTRODUCTION  Association rule is an important knowledge representation in data mining area. It tells us significant relations among itemsets present in large number of transactions. Extensive studies have been devoted into association rules mining. But traditional association rule definition arise two problems as follow. First, traditional methods assume items in transaction database have same significance, thus the mining process is flooded in the combinatorial explosion of insignificant relationships. The mining result may comprise duplicate information and consume lots of space. Second, those methods ignore the difference between two items, which may lead to incorrect result biased with users' expectation in real applications. We inspect the problems in two aspects as follow. First, we prune the duplicate information and just derive concise knowledge representation. Additionally, concise representation can derive all the duplicate parts if we will.

Second, we differentiate items' importance in the mining process, which make the mining results more rationally.

In real applications, one item may be different greatly from the other for the semantics in context. It is impossible to express all the differences absolutely between them. An alternative is to assign different weights to items. In the    mining process, we derive the result selectively referring to the weights on items, which may make more reasonable results. Closed itemset is an alternative to achieve concise results, which is a concise representation for frequent itemset.

It losslessly derives the knowledge, but is orders of magnitude smaller than the whole set of frequent itemsets.

Based on general frequent itemset mining approaches [1 ]-[5], many proposed closed itemset mining approaches maintain the closed itemsets in main memory and filter those non? closed, which is a costly process and may include much redundant work. But traditional closed itemsets may not hold when being assigned different weights. Additionally, many proposed traditional closed itemset mining approaches maintain the closed itemsets in main memory and filter those non-closed, which is a costly process and may include much redundant work.

In this paper, weighted closed itemset is re-defined through exploiting weighted support-significant framework.

We find that it is possible to define weighted closed itemset when the "downward closure property" holds. The weighted closed itemsets can be enumerated gradually by adding items into itemset. Through exploiting the depth-first mining strategy and exactly itemset checking approach, subset checking is simplified, and duplicate closed itemsets can be identified early. Thus the method avoids much redundant checking work. Additionally, based on tree structure concise association rules in weighted closed itemsets representation can be generated. Through specific filtering methods, we can generate weighted closed itemsets directly without the need of keeping the whole set of closed itemsets and concise association rules in the main memory.

Experimental studies are conducted to show that the method in this paper is reasonable and achieves good results.



II. DEFINITIOIN AND TERMINOLOGY  Let J={iJ, i2, ... , im} be a set of items and TDB={TJ, T2, ... , Tn} be a transaction database, where Ti(l:9:9l) is a transaction which contains a set of items in 1. A set of items is called an itemset or a pattern. A transaction T is said to contain itemset X if and only if Xr;;;,T. The number of transactions in TDB that contain X is called the support count of X, denoted as count(X), and the support of X is denoted as    support(X), which equals to count(X)/ITDBI, where ITDBI is the total number of transactions in TDB.

Given the form X=> Y, the confidence of X is denoted as conjidence(X=>y), which equals to support(X U Y)/support(X).

Definition 1. Weighted item. Given a transaction database TDB, and the item sets I={i], i2, ... , in} which appear in TDB. We attach a value Wm to each im representing its significance. Such an item is called weighted item. And its weight is denoted weightCim) or weightC{im}), which equals Wm.

The items' weights may be defined repectively in different area to balance the significance between items in transaction database.

Definition 2. Weighted item set. Given a itemset X, for each item in it has a weight, itemset X may take different significance beyond the other. We denote the significance of  X weight(X), which varies according to the itemsetX For the huge amount of itemsets, here itemset's weight  cann't be defined respectively. It can only be calculated from weights of items in it reasonably in specific application.

Definition 3. Given an itemset X, from above definition, we define the weighted support of X as ws(X).

Definition 4. Given a transaction database TDB and a minimum support threshold ?], an itemset X is a frequent weighted closed itemset if both of the following conditions are true:  1) WS(X)2::?l; 2) 'lfY:::JX such that ws(Y)<ws(X).

The problem of weighted closed itemsets mining is to  find the complete set of frequent weighted closed itemsets in a given transaction database with respect to the given support threshold ?.

Definition 5. Given a transaction database TDB and a minimum support threshold ?l and a minimum conjidence threshold ?2' an rule X=>Y is a weighted concise associateion rule based on closed itemset if both of the following conditions are true:  1) X, Y(Y:::JX) are frequent weighted closed itemsets; 2) ws(y)/ws(x)2::?2.

The problem of weighted concise associateion rule  mining is to find the complete set of weighted concise associateion rule based on closed itemsets in a given transaction database with respect to the given support threshold ?l and confidence threshold ?2.



III. WEIGHTED CLOSED ITEMSET MODEL-ITEMSET SEARCH SPACE ARRANGEMENT  In definition 4, frequent weighted closed itemset is defined. In order to mine the complete set of weighted closed itemset and rules, we should decide how to calculate the weights of itemsets reasonably when being enumerated.

Different definition of itemset's weight may lead to different result, but properties of traditional closed itemset may not hold. And enumerating itemset will be difficult, or even the total itemset search space will be viewed, which is impossible. The core of the problem is how to enumerating   itemsets and pruning search space efficiently while keeping proper weights for closed itemsets.

Among those properties of the support -significant itemset, "anti-monotone" is an important one. If the property of itemset hold on weight itemset, itemset will become less frequent when they are enumerating by adding items. So the process can be stopped when a infrequent itemset is found.

Otherwise, we don't know when the enumerating process should be stopped. The total search space will be viewed.

Recently a weighted support-significant framework [10] is proposed which keeps "downward closure property". We adopt the method of calculating weighted support of an itemset. With little adjustment, we prove that weighted closed itemset keeps the anti-monotone properties, and weighted closed itemset can be enumerated iteratively in depth-first manner by adding items.

Definition 6. Given a transaction database TDB and each transaction Ti in it is attached a weight twi. Then the weighted support of itemset X is defined as follow:  (1)  For any TiE TDB, we can define ws(TD as we will reasonably. Generally weighted support of itemset X is defined as follow:  (2)  Lemma 1 (downward closure property). For an itemset X, ::JY:::JX, ws(Y)?ws(X).

Lemma 2. For an itemsetX, ::JY:::JX, ws(Y)=ws(X), thenX can't be a weighted closed itemset.

Proof From definition 4, we can have the lemma.

Lemma 3. For two itemsets X, Y(Y:::JX) , ws(Y)=ws(X),  then 'If Z (Y:::JZ:::JX), Z isn't a weighted closed itemset.

Proof Let TS(X)={TiITiE TDB, Ti:::JX} be the set of  transactions containing itemsetX, then we can have that:  T,eTDB, T,eTS(X)  For Y:::JZ:::JX, we have that TS(X);;J.TS(Z};;;;J.TS(y), then  ?>lli ? ?)Wi ? ?)Wi T,eTS(X) T,eTS(Z) T,eTS(Y)  From equation (3), we can have  (3)  (4)    T,ETDB, T,ETDB, T,ETDB,  Then ws(X)?ws(Z)?ws(Y), and ws(Y)=ws(X), so ws(Y)=ws(Z) =ws(X), from definition 4, we have the lemma.

Lemma 4. For itemsetsX, Y(Y:::JX), ws(Y)=ws(X), then \;f Z (Z:::JX, Yct.Z), Z can't be a closed itemset.

Proof For Y:::JX, TS(X)-;;;).TS(y), and ws(Y)=ws(X) in lemma 4, then TS(X)=TS(Y).

From the property of set, we can have TS(XuZ)=TS(YuZ)  Additionally, Z:::JX, Y ct.Z, we have TS(Z)=TS(YuZ) and YuZ:::J Z  From the inference in lemma 3, we have that ws(YuZ)= ws(Z)  then we have the lemma.

From above lemmas, we can explore the weighted closed  itemsets search space in specific manner, and can prune much search space early.



IV. DIRECTL Y WEIGHTED CLSOED ITEMSET MINING FRAMEWORK  In this section we exploit the weighted closed itemset properties, and simplify the mining process throught just exactly itemset checking approach while pruning search space when necessary.

A. Conditional Weighted Closed Set  To ease the itemsets enumerating process, we assume that there is a partial order on itemset I, denoted as -<.

Without loss of generality, we assume in the remainder of the paper -< is the ascending supports order of items in 1.

Definition 6. Let i],i2, ... ,ik (k 2:: 1) be items in 1. The conditional weighted closed set {i],i2, ... ,id is denoted as CWCS(i],i2, ... ,ik), while following conditions are satisfied:  1) ik-<. .. -<i2-<ij ; 2) \;fXE CWCS(i],i2, ... ,ik) such that :JTcTDB,  X U  {i],i2, ... ,idcT ;  3) Let im be the largest item in X, then im-<ik ; 4) CWCSO is the conditional weighted closed set of 0.

The conditional weighted closed set can be interpreted  differently. But it is meaningful only during the mmmg process, and it changes as the mining goes on.

E. Transferring Weighted Transactions  For weighted transaction and weighted itemset have different definitions, transactions containing certain items are projected from transaction database with their weights. At the same, we present itemset in linked data structure.

First, we initialize CWCSO. We eliminate all the items whose weighted support is less than q in all transactions and insert itemsets left into CWCSO. If there is a same itemset, then eliminate it and update weight of the other, otherwise initialize its weight. Then for every frequent item im in a CWCS(i],i2, ... ,ik), we do as follows: For every itemset t containing im in CWCS(i],i2, ... ,ik), we collect all the frequent   item in in t where in-<im, as a new itemset to be inserted into CWCS(i],i2, ... ,ik,im). Then we set CWCS(i],i2, ... ,ik) to 0.

With the mining process recursively going on, all the conditional weighted closed sets are initialized.

C. Exactly Same Weighted Itemset Checking  For two itemsets Sj in CWCS(i],i2, ... ,ik,im) and S2 in CWCS(i ],i2,? .. ,ibiJ(im -<in), if S2 equals Sj U hm} and their local weights are same, itemset Sj U hm} can be a weighted closed itemset, then we remove itemset Sj from CWCS(i],i2, ... ,ik,im). We say that we do WeightedChecking(CWCS(i ],i2, ... ,ik,im), CWCS(i ],i2, ... ,ik,iJ).

During the depth-first mining process, the larger items are always dealt with first. For an item in in CWCS(i],i2, ... ,ik), we assume all CWCS(i],i2, ... ,ik,im)(in-<im-<ik) have been dealt with, then for every im (in-<im-<ik), we do WeightedChecking(CWCS(i],i2, ... ,ibiJ, CWCS(i],i2,??? ,ibim))? After all items in CWCS(i],i2, ... ,ik) have been dealt with, we do as follows: for every item ie in CWCS(i],i2, ... ,ik,in), ie is inserted into every element of CWCS(i],i2, ... ,ibin), then elements of CWCS(i],i2, ... ,ik,in) are removed to CWCS(i ],i2, ... ,ik).

D. General Weighted Closed Itemset Enumerating Framework  From above analysis, we present the general enumerating and exactly weightness checking framework. For the given database and minimum support threshold q, all the weighted closed itemsets can be get from CWCSO by calling procedure WeightnessEnumerateAndCheck() as in Figure 1.

The exactly checking and projecting methods in the algorithm are described in previous subsection. Following theorem ensures the framework can mine all the weighted closed itemsets correctly.

Procedure WeightnessEnumerateAndCheck (a ) (Here a represents iJ,i2,??? ,ik (ik-< . . .  -<i2-<ij? l. recalculate all support(i) with the new occurrence  frequencies in CWCS(a); 2. if(all support(i)<1;J f 3. set CWCS(a) ?{0}; 4. return; 5.

6. for item i?max to min contained in CWCS(a) do { 7. if(support(i)::> .; )  { 8. project item set t III CWCS(a) to CWCS(a, i); 9. EnumerateAndCheck (a, i); 10. for j?i+ 1 to max WeightnessEnumerateAndCheck  (CWCS(a, i), CWCS(a, j?; 1l. }} . . . .

10. lf(there IS an Item I III CWCS(a ) and support(a )?  support(i? 12. set CWCS(a )?RS(a)?0; ll.else set CWCS(a) ?0; RS(a)?{0=> fi}}; 12. forCitem i?max to min contained in CWCS(a)) { 13. for(every item set s in CWCS(a,i? set  CWCS(a)?CWCS(a) U {s U {i}}; 14. for(every rule r(x=>y) in RS(a, i? set 15. RS (a)? RS (a)U {xU {i}=>y};  }}  Figure 1. Pseudo code for enumerating and exactly checking method    Theorem 1. After WeightnessEnurnerateAndCheck 0 is executed, CWCSO camprises the exactly whale set af weighted clased itemsets.

Proof First, all the frequent weighted clased itemsets can be generated. For any weighted clased itemset {i],i2, ... ,im}, the mltIahzatIan process inherits all the frequency mfarmatIan abaut {i],i2, ... ,im} in ariginal database carrectly.

When WeightnessEnurnerateAndCheck (i],i2, ... ,im) is executed, line 3 or 13 ensure that the weighted clased itemset {i],i2, ... ,im} be generated an upper recursive level. Thus we have that any weighted clased itemset {i],i2, ... ,im} can be generated.

Secand, any itemset in CWCS() is clased after the mining process. For a nan-clased itemset {i],i2, ... ,im}(im-<' .. -<i2-<i1), with lemma 1, there must be an itemset Y(Y:::J {i],i2, ... ,im}) that ws(Y)=ws({i],i2' ... ,im}). From lemma 2, we have that there is an item i[E(Y-{i],i2, ... ,im}) that ws({i],i2' ... ,im,if})=ws({i],i2, ... ,im}). Then aur procedure ensures {i],i2, ... ,im} can be eliminated in three circumstances: (1). i1-<if, and ws({if,i],i2, ... ,im})=ws({i],i2, ... ,im}). In procedure EnurnerateAndCheckO, we can see that when WeightedChecking(C WCS(i 1)' CWCS(ir)) is dane, itemset {i2, ... ,im} is remaved from CWCS(il). So. itemset {i],i2, ... ,im} can't be generated. (2). if-<im, and ws({i],i2, ... ,im, ir} )=ws( {i ],i2, ... ,im}). When EnurnerateAndCheck(CWCS(i],i2, ... ,im)) is executed, line 12 ensures that clased itemset {i],i2, ... ,im} can't be generated in the upper recursive level. (3). ::3 nCin+l-<if-<in, l::Sn<m) that ws( {I ],12, ... ,lm,lr} )=ws( {i ],i2, ... ,im}). When EnurnerateAndCheck(CWCS(i],i2, ... ,in)) is executed, procedure. . .  WeightedChecking(CWCS(i],i2, ... ,in+l)' CWCS(i],12, ... ,In,Ir)) ensures that itemset {in+2,i2, ... ,im} is remaved from CWCS(i],i2, ... ,in+l)' so. it can't be generated.

E. Weightness-Projection: Mining Weighted Closed Iternsets Directly  In this sectian, several aptimizatian methads are proposed to. derive the weighted clased itemset directly. To.

explOlt the enumerating and exactly itemset checking framework, a weightness-projectian methad alang the tree links methad is propased, which make the mining process aVOld keepmg the whale set af clased itemsets in main memary with little cast.

Weightness Projection Property: When an item in is cancerned, we project larger items than in thraugh adjusting its descendant linked branches. After all the descendant linked branches are cambined, anly the items with the same weight af item in are linked to. its suffix link.

Su?x Growth Optimization: In the mining process, the larger Items are always cansidered first in depth-first manner.

Far i?ems i3-<i2?il: if i?e caunt af i3 in CWCS(i1) is equal to.

that m CWCS(Ij, 12), Item i3 need nat to. be cansidered in CWCS(il). And item i3 needn't to. be prajected an larger items in suffix links.

Fram abave twa main aptimizatians, we can just tell whether an weighted itemset I is clased thraugh respecting the suffix af the minimum item in 1. Additianal calculatian and space are nat needed.

ITIDI items I ws I litem I weight I support I w? I 100 b,d,t;c 1.05 a 0.4 0.425 0.2770 200 f,a 0.85 b 0.8 0.5 0.4122 300 b,a 0.6 c I 0.425 0.3784 400 d,f 1.2 d 1.1 0.5 0.5405 500 b,d,a,c 0.6 e 1.2 0.125 0.1554 600 d,e 1.15 f 1.3 0.5 0.5743 700 f,c 1.15 800 b 0.8  Figure 2. Transaction database TDB and weight settings  Example 1. Given the transactian database TDB with the item weight settings, the minimum weighted support threshald ? =0.2, the created glabal FP-tree like structure is shawn in Figure 3. Item e is remaved for its support is belaw ? The ascending suppart order an I is f-<d-<b-<c-<a-<e.

Figure 3. data structure created for CWCSO  As the mining process gaes an , we can derive fallawing weighted clased itemsets:  {a}, {b}, {c}, {d}, {f}, {bcd}:0.2230, {cf}:0.2973, {df}:0.3041.

For example, item f is assigned to. a larger weight in abave result, cancise itemsets {{ f}, {cf}, {df}} and rules cantain more likely item f than traditianal anes .

. From abave we can see that the mining process shaws different behaviaur in weight settings againt traditianal ane.

Thase clased itemsets and rules with higher weights are selected first, which is more reasanable in real applicatians than . traditianal anes. Additianally the number af target canClse Itemsets and rules generated decreases greatly. And they can be more camprehensible and easily processed.



V. EXPERIMENTAL RESULTS  . In this sectian, We canduct experiments an public simulated data sets. The target platfarm is a Lenava PC equipped with 2.6G clack rate CPU and 1024M main memary. The aperatian system is WindawsXP Prafessianal.

The propased algarithm is implemented in C++. Tested datasets include mushraam, chess, pumsb, kasarak, Tl0I4DlOOK, which can be dawnlaad fram UCI Repasitary [9] ar Frequent Itemset Mining Implementatian Repasitary.

Here we just demanstrate the results an mushraam when mining general clased itemsets and weighted clased itemsets with same suppart threshald under specific weights setting.

Mushraam dataset cantains all kinds af mushraam infarmatian with different praperties. When attaching hight    weight to specific properties, the according properties are more likely mined which decide whether this kind of mushroom is poisonous. And the number of all weighted closed itemsets does increase against that of closed itemsets.

Figure 4 shows the test results.

Figure 4. Size of results generated comparison  Figure 5. Runtime comparison  Figure 6. Memory usage  Additionally, by adopting proper technique, the proposed method derives all the weighted closed itemsets with little cost, and doesn't need to keep all the generated closed itemsets in main memory to filter closed itemsets.



VI. CONCLUSION  In this paper, we proposed a new technique for mining weighted concise association rules and closed itemsets in   depth-first manner, which leads to more reasonable results.

Based on an effective framework, several pruning methods were adopted. Those methods can prune search space early but with little cost. Additionally two steps are integrated and share same divide-and-conquer process and data structure.

Thus much more search space can be pruned. The performance results show that the algorithm has good time and space scalability.

