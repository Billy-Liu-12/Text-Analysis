

Fast Algorithm for Mining Multilevel Association Rules N.Rajkumar M.R.Karthik S.N.Sivanandam  Research scholar Department of Electrical Head, Depilrtment of Computer Dept. of Computer Science Engg. and Electronics Engg. Science Engg.

PSG College of Technology PSG College of Technology Coimbatore - 641004 Coimbatore - 641004 Coimbatore - 641004  India India India nrk29@rediffmail.com KarthikvZk@yahou.co.uk  PSG College of Technology  Ahsrruct-In this paper. we present two algoiithms AprioriNewMulti and AprioriNewSingle, for Data Mining multilevel and single level association rules in large database respectively. The database consists of following fields, Transaction ID and items purchased i i i  the transaction. The algorithms introduce a new concept called multi minimum support i.e. minimum support will vary for different length of the itemset. Unlike other algorithms AprioriNewMulti does not depend on number of levels in concept hierarchy i.e. it doesn?t scan the database for each level of abstraction for finding association rules. Scale up experiments show that ?both of these algorithms have scale linear with the number of customer transaction.

Keywords: Data Mining, Association rules, Minimum support  1. INTRODUCTION Datu Mining refers to extracting or ?Mining? knowledge from large amounts of data. Today?s Industrial scenario is having manifold of data which is data rich and information poor .The information and knowledge gained can be used for applications ranging from business management, production control ,and market analysis, to engineering design and science exploration. Data Mining can be viewed as a result of natural evolution of information technology L2l.

Associution ride mining finds interesting association among a large set of data items. With massive amounts of data continuously being collected and stored. Many industries are becoming interested in mining association rules from their databases. The discovery of interesting association relationships among huge amounts of business transaction records can help in many busine ision making process, such as catalogue design, cross eting, and loss leader analysis[2].

A typical example of association rule mining is market based analysis. This process analyzes customer buying habits by finding association between the different items that customers place in their ?shopping baskets?. The discovery of such associations can help retailers develop marketing strategics by gaining insight into which items of frequently  0-7803.765 I-X/O3/$17.00 0 2003 IEEE  purchased together by customers. For instance, if customers buying milk, how likely are they to also buy bread on the same trip to super market? Such information can lead to increased sales by helping retailers do selective marketing and plan their self space. For example, placing milk and bread with in close proximity may fufrher encourage the sales of these items together within visit to the store.

2. PROBLEM STA?EMENT We are given a database of customer transactions. Each transaction consists of following fields: Transaction ID  and items purchased in that transaction. We don?t consider quantities of items brought in a transaction. The items are represented by an item ID. An itemset is a non erirpty set of items, in which items?are arranged in ascending &der with respect to its ID.

In order to explore the frequent k-itemseL we use the prior knowledge of the (k-I) itemsets, (k-2) itemset up to I itemset. To improve the prior knowledge, an important property called the Apriori Property presented below, is used to reduce the search space.

I n  order to use the Apriori Property, all nonempty subsets of a frequent itemset must also be frequent [41, this property is- based on the following observation. By definition. if an itemset I does not satisfy the minimum support threshold.

minimum support then I is not frequent. i.e. P (I) <minimum support. If an item A is added to the itemset I, then the resultink itemset (i.e. I U A) cannot occur more frequently than I. Therefore, I U A i s  not frequent either, That is, P ( I  U A)< minimum support.

This property belongs to a special category of properties called anti-monotone i n  the sense that if a set cannot pass a test, ali of its supersetS%iH:fail the same test as well. I t  i s called anti-moootone bec*e the property is monotonic in the context of failing a-test:  Multi Minimum Support, the concept is used to find valuable sequence i n  higher length of itemsets. I n  the existing algorithms the minimum support is constant for all length of sequence; this will not be suitable for most of the transactional data because the higher length itemsets will have very low support when compared to I-itemsets  mailto:nrk29@rediffmail.com mailto:KarthikvZk@yahou.co.uk   Web Technology and Dafa Mining /689  T2000 T3000 T4000 T5000  3. RELATED WORK  The problem of discovering "what items are brought together in  a transaction "over basket data was introduced.

This problem of finding what items are bought together is concerned with finding intra transaction patterns, whereas the problem of finding sequential patterns is concerned with inter-transaction patterns. A pattern in the first problem consists of an unordered set of items whereas pattern in laner case is an ordered list set of items 111.

Discovering rules from data has been a topic of active research in AI in [IO], the rule discovery programs have been categorized into those that find quantitative rules and those find qualitative laws. The purpose of quantitative rule discovery programs is to automate the discovery of numeric laws of the type, commonly found In scientific data such as boyle's law PV=C. The problem is sated as follows[l I1:Given m variables x,,xz,x 3.. . .xm and k groups of observational data di,dz,d3 ... d, where each d: is set of m values.- one for each variable, find a formula f(xI,x2,x3 .... x,) that best fits the data and symbolically reveals the relationship among variables because too many formulas might fit the given data, the domain knowledge is generally used to provide the bias towards the formulas that are appropriate for the domain. Examples of some well- known systems in  this category include ABACUS [IZ], Bacon 181, and COPER [9].

/ IlJ3, I5 12,14,16 11.15, I6 Ii ,12 ,15,16  Litemet: Any itemset having support satisfying the minimum threshold value is termed as litemset.

Level Pass Threshold: In multilevel association &le mining, a value that determines whether itemsets of sub levels are to be processed o r  not. This usually set between minimum supports of the k-level and (k-1)-level for k-level, i.e. the itemsets of (k-1)-levels are processed only if its support satisfies the level pass threshold of k-level. Through out this paper we set.the level pass threshold of k-level equal to the minimum support of the (k-1)-level.

5. ALGORITHM APRIORINEWMULTI  We solve the problem of finding all multilevel interesting patterns in three phases.

Mapping Phase I-itemset Phase Main Phase  The following section describes the above phases in detail.

TID I Items in the transaction mnnn I 1.4 16 1  Business database reflect the uncontrolled real 5.1 Mapping Phase world, where many different causes overlap and many !

patterns are likely, to co.exist [91 , Rules in such data are likely to have uncertainty, The qualitative rule discovery programs are targeted at such business and they  Only used for mining multilevel association rules. Here each item's concept hierarchy path is mapped into its address table 5.1.1. Each item has unique , '  generally use little or no domain knowledge. There has been . .

leve l  : 0  leve l  : 1  considerable work in discovering classification: Given examples that belong to one of the pre-specified classes, discover rules for classifying them. Classic work in this area includes [SI.

4. TERMINOLOGY  Supporr Consrrainrs: These constraints concern the number of transactions that support a rule. The support for a rule is defined to be the fraction of transactions that satisfy the  Support should not be confused with confidence. While confidence is a measure of the rule's strength, support corresponds to statistical significance.

Besides statistical significance, another motivation for support constraints comes from the Fact that we are usually interested only in rules with support above some minimum threshold for business reasons. If the support is not large enough, it means that the rule is not worth consideration or.

that i t  is simply jess preferred (may be considered later).

Q, 5.1.1: Shows item in concepr tree.

union of items in the consequent and antecedent of the rule. . .

. .

Table 5.1.1: Mapping ofaddress to irems.

TENCON 2003 / 690  Level \ Iremset  Table 5.1.?. Minimum support of each item as a function of level and length of itemset.

5.2 Idemset  Phase  Here we scan the database and find the frequency of all 1- itemset and stored in a concept hierarchy tree (created physically).Thls phase is pruned to get the frequent 1- itemsets. This will be used as a reference for the rest of the process.

Fig 5.2.1 Gives support of each and every item in the database (Fig 5.l.l)suppon is prefixed by's'.

5.3 Main Phose  This phase is split into 4 more modules, repeated for every transaction.

5.3.1 Primaryprune module  The transaction is split into I-itemsets each of which is tested for the minimum support. If it fails, it is pruned from the transaction; else it retains its position in the transaction.

Now a new transaction is formed consisting of only 1- Lilemsets.

5.3.2 Split Module . ' The prunqd transaction is mapped to contiguous integers.

The number of possible k-subsets for the mapped transaction is derived from the database of subsets. The database of sub sets is created preliminarily to save time during mining.

These possible k-subsets with the primed transaction are given to the next module.

Fig 5.3.2: Possible subsets of a 4-itemset in a tree  5.3.3 $econdary prune Module  The possible k-subsets of the pruned transaction are tested for the existence of non litemsets (of length < k). I f  a k- subset is not a litemset then all superset of this subset is removed from the tree. In this example, (1:3) is not a litemset so all its superset are removed from the tree.

Fig 5.3.3: Pruned tree off is  5.3.2  5.3.4 Tree construction module  The itemsets from the secondary prune step are stored in concept tree. The address of the each item in an item set is used to construct the tree. This enables to find thesupport of the itemsets at each level. The support of a node is the sum of the support of its child node i.e. support of k-level node = sum of the support of its (k-1)-level child nodes. Though non-litemsets do not form nodes their support is added to parent node.

Fig 5.3.4: shows 2-Litemsets arranged in concept tree with its support at each level. support is prefxed by 's'.

Web Technology and Data Mining /691  Itemset  Level 2 has no litemset  Fig 5.3.5: shows 3-Liremsers arranged in concept tree with irs support at each level support is prefixed by's'.

6. ALGORITHM APRIORINEWSINGLE  1 2 3 Minimum suppori  corresponding level, but AprioriNewMultUSingle will scan the database as if there is only one level.

If a database contains I O  levels and large itemset.of length IO, then it will scan thedatabase lOO(lO*lO) times. But AprioriNewMultilSingle will scan the database only 10 times.

8. GENERATION OF SYNTHETIC DATA  To evaluate the performance of the algorithms over a large range of data characteristics, we generate synthetic customer transactions. The synthetic data .takes the following  arameters.

Average number items per Transaction  Maximum length of the frequent itemset Number of Levels in Concept Hierarchy  Fig 7.0.1: Terminology used in synthetic data generation.

9. EXPERIMENTAL RESULTS  To assess the performance our algorithms on a large itemset generations by using an IBM PC powered by Pentium 111 750 MHZ, 64MB RAM, 5400 RPM HDD. We compare our algorithm with existing Apriori algorithm; performances are shown in the following graphs. The default values are D=100000, N=1000, L10 ,  Mz0.5, F=7, T 2 0 .

5 4 3     0 50000 100000 150000 200000  4- Aprion ~~~~~~ ,. ...... ~-  ~ -. .~. .. - - .  ~ ~ . .  ~ Fig 9.1: Varinrion ofexecurion time with D.

I I  5000 10000 15000 20000 0  --c ApriociNewMulti  e- Apriori  t+No.of items  Fig 9.2: Variation of execution time with N    TENCON 2003 /692  0 5 ? 10 - 15 20 25 j i i  i  Fig 9.3: Viriarion ofexecution.rime with L , .  . . .

. .

..,! ! . . / < ,  ?   ._ .

0 0.1 0.2 0.3 0.4 0.5 0.6  1 M- Mlnlnmum Support t ApiofiewMuhi ( .  ! -a-Apriori .~ ~ ~~ ~ ~ .

pie 9.4: Variation ofexecution rime with M.

10. PERFORMANCE EVALUATION It is seen from the above results that AprioriNewMulti/Single performed better than Apriori, due to following reasons.

Time wasted in candidate generation is saved in AprioriNewMultilSingle.

If number of 2-Litemset is very large then generating candidate sets is time consuming.

Among these candidate sets only few will be useful.

AprioriNewMulti scans the database much less often than Apriori.

Performance of AprioriNewMultiISingle will increases with increase in the number of levels in a database.

11. CONCLUSION  We extend the scope of the study of mining from single level to multiple level association rules from a large transaction databases. Mining multiple-level association rules may lead to progressive mining of refinedJywledge from the data and have interesting applications for knowledge discovery in transaction-based, as well as other business .or engineering databases. With the help of multi minimum support, concept, finding interesting frequent itemset in .  h ighemength  of itemsets made easy.

AprioriNewMulti scans the database less, this made it to  perform well over other existing algorithms in duration of the process.

