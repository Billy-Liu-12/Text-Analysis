

Abstract?Exploration techniques of video knowledge have been proposed for years to help people discover the details about videos. However, existing systems still yield limited information for users. In this paper, we present a video knowledge browsing system, which can establish the framework of a video based on its summarized contents and expand them by using online correlated media. Thus, users can not only browse key points of a video efficiently but also focus on what they are interested in. In order to construct the fundamental system, we make use of our previous proposed approaches to transforming a video into a graph. After the relational graph is built up, the social network analysis is then performed to explore online relevant resources. We also apply the Markov clustering algorithm to enhance the results of the network analysis. The experiments demonstrate that our system can achieve better performance than the traditional systems.



I. INTRODUCTION Video content exploration aims to extract hidden knowledge  from videos. As the amount of video information has become larger, mining effective knowledge is getting more important.

In this paper, we attempt to discover video knowledge based on video summarization techniques [1]-[5]. Traditional methods usually focus on extraction of specific events in a video by employing object recognition approaches or classifiers [6].

These extracted events are then indexed and rearranged in the user interface, so that people could browse them efficiently. In some literature [7], spatial and temporal similarities between frames are also adopted to detect events. Nevertheless, in most cases, these systems do not tend to emphasize the concept relations among the indexed contents, and the explored knowledge from a video is often constrained to itself.

Consequently, finding a way to enrich existing contents is important.

To alleviate these problems, we propose a framework based on the four kinds of entities, "who," "what," "where," and "when," to summarize video contents. Once we extract these entities, we can weave each entity together according to their interrelations and establish a relational graph. At the same time, we could extend the contents of these entities by linking semantic relevant media to them. In this paper, we endeavor to provide: 1) a systematic approach of summarizing and augmenting video contents according to the concept entities; 2) the enhanced social network analysis based on the Markov clustering algorithm (MCL) by transforming a graph between the line-graph and vertex-graph domain; 3) a content enrichment method via community-knowledge to improve the existing information.

The remainder of this paper is organized as follows: Section  II reviews the related techniques. Section III elaborates the details of our proposed methods. We exhibit the performance of our system and the experimental results in Section IV. Section V summarizes the conclusions of this paper.



II. RELATED WORK In video knowledge mining, one of the conventional ways is  to utilize a classifier to determine whether a shot contains events or not [6]. Low-level visual data are usually extracted from frames and taken as features for classifiers. However, its disadvantage is that relations between shots are often ignored.

To handle this problem, some authors have proposed a more enhanced approach based on correlations of shots. The co-occurrence patterns can be discovered by applying association rules or predefined rules [8].

The essence of summarization is similar to video mining techniques; its purpose is to highlight the most important shots among a video for users. Its algorithm is usually designed on the basis of maximum frame discrepancy strategies [1], [2]. Lu et al. [2] make use of dynamic programming to calculate optimal skimming length on the graph structure, and the shot-relations are modeled by spatial-temporal linkages.

Recently, more and more research tends to exploit attention models as the criteria to choose salient frames [3], [5]. For example, Ma et al. [5] combine a series of visual attentive indicators and then convert the results into numerical curves to estimate the significance of shots. There are also some approaches, which focus on extracting caption and transcript information in videos because text can help people grasp high-level concepts in video summaries [4] .



III. PROPOSED METHOD The proposed system architecture is shown in Fig. 1, where  the input is a video, and the outputs are the summarized knowledge and relevant media. It consists of the two phases: one is to summarize the input into a relational graph with the structure of the four entities; the other is to explore media, which are correlated to the graph on the web.

A. Summarizing Contents into a Relational Graph At this stage, we mainly follow our previous work in [9] to  achieve the aim. First of all, we apply the approach mentioned in [9] to detect shot boundaries. The middle frame of a shot is selected to represent the shot. Meanwhile, we follow the steps proposed in [10] to extract image features. Each frame is divided into 6 ? 6 blocks. A block is denoted as a 23-dimensional feature vector, which is calculated from the  Bo-Wei Chen Department of Electrical Engineering  National Cheng Kung University Tainan, Taiwan, R.O.C.

Email: chenbw@icwang.ee.ncku.edu.tw  Video Knowledge Augmentation Based on Summarized Contents and Online Media  Jhing-Fa Wang and Jia-Ching Wang Department of Electrical Engineering  National Cheng Kung University Tainan, Taiwan, R.O.C.

Email: wangjf@csie.ncku.edu.tw; wjc@icwang.ee.ncku.edu.tw     color, texture, location, and motion vector of the block. After extracting the features, we cluster all the blocks of the frames using K-means algorithm and assign the label of the centroid to each block. These blocks are denoted as V = {v1,?, v|V||}, where vi is a block, |?| means the size operator, and i = 1,?,|V|. The speech transcripts corresponding to the videos are collected. All the words except nouns are removed from the transcripts. We represent these keywords as W = {w1,?, w|W|}, where wj is a keyword, and j = 1,?,|W|. After analyzing visual and textual data, we then employ an annotator [10] to label each shot according to the co-occurrences between vi and wj. Therefore, a shot can be linked to a proper word, and we could utilize WordNet (wordnet.princeton.edu) to classify those shots into the four entities ("who," "what," "where," and "when") by querying their hypernyms.

In order to organize the shots into a relational graph, we have to calculate vertex weights and build up relational edges. Let G = <U,E> denote a relational graph, U represent its vertex set, and E denote its undirected edge set. We make use of a fusion function that combines the visual and textual data to calculate vertex weights. The visual data is converted from the results of salient maps [5] by analyzing color, intensity, and orientation contrast in each frame of a video. The textual data could be obtained by gathering statistical information using the traditional TFIDF function. Once the weights of all vertices are calculated, we may estimate their relational degree according to the expanded terms of each annotation, which are queried and derived from WordNet and ConceptNet (conceptnet.media.mit.edu).

When the structure of the story contents is built up, we extract important vertices and edges from the graph. Mining meaningful vertices in the relational graph is important to present video knowledge. In our system, we conduct graph entropy algorithm [11], which allows us to explore the vertices whose removal has significant impact on the graph. By iteratively testing the removal of each path and vertex, we could select the important components.

B. Content Enrichment Based on the Social Network Analysis  1) Constructing the Social Network After building the video summary, we make use of the  keywords with higher occurrences in the transcripts to query Youtube website (it feeds ranked and related videos back to users). We may begin to build up a network by crawling the search results and their relevant linked-lists. Each video represents a node, and the edge represents the feedback  hyperlink between videos. To avoid confusion, we denote the social network we discovered by ? = <U?,E?>, the video set by U?, and connections between videos by E?.

2) Converting to the Line Graph and Exploring Clusters Based on the MCL  In the next step, our system takes and converts the network  into a line graph, ( ) ,L U E? ?? ?? = . In contrast to the structure of ?, the node in the line graph now represents the edge, whereas the edge represents the node. Fig. 2 illustrates an example.

Fig. 2.  Simple illustration of converting a vertex graph (the left) into a line graph (the right). We use the punctuation mark ":" to represent that the vertices are in the line-graph domain.

In the line-graph domain, we observe that given any two connected vertices in L(?), we may describe their dependency by their ranks when they are transformed back into the vertex-graph domain. Consider a subgraph with two adjacent vertices in L(?) illustrated in the right part of Fig. 2. Let "p:q" and "q:r" represent these two vertices. We use the punctuation mark ":" to represent that these vertices are in the line-graph domain. Assume the ranks of q in the linked-list of p and r are rankp,q and rankr,q, respectively. The rank pertinence of the edge between "p:q" and "q:r" can be modeled by    ( ) ( )(( ( ) ( ) ( ) ))   Rank , , ,   , , ,  Pertinence , exp tanh 1  tanh 1 tanh 1 tanh 1  p q r q p q  p q r q r q  rank rank rank  rank rank rank  = ? ? +  ? ? ? + ?   (1)  where tanh(?) is the hyperbolic tangent function. When both rankp,q and rankr,q equal one, the rank pertinence reaches the maximum, which infers two edges in E? have the highest association. After iteratively calculating the rank pertinence value of each edge, we then proceed to employ the MCL to filter those edges, whose pertinence values are less important.

The MCL is a kind of graph-based clustering algorithm. The key notion behind the MCL is that if a graph possesses a clustered structure, random walks between nodes lying in the   Fig. 1.  Proposed system architecture.

same cluster are more likely than those between nodes that are located in different clusters [12].

3) Converting Back to the Vertex Graph and Applying the MCL Again  As long as the MCL process finishes, we take the clustering results and convert the line graph back to the vertex graph.

Since the edges represent content-relations, we may exploit visual similarity and text information between the videos to estimate their relational degrees. For this purpose, we define the content pertinence, which is given as follows:   ( )  ( ) ( ) ( ) ( )( )( ) Content  2 21 1  Pertinence ,  exp 1- 1  sift vsm  sift sift vsm vsm? ?  =  ? + ? + ? (2)   where sift (for visual) is the number of matching coordinates decided by running the scale-invariant feature transform (SIFT) between the representative frames of two videos, and vsm (for literal) is the cosine-similarity score between the literal information of two videos using the vector space model (VSM).

The higher the content pertinence score is, the more similar the two videos are.

After the MCL process is complete, each cluster could be referred to as a group of similar videos. To find out the videos relevant to our summary among the results, we can adopt (2) as the criterion to compare with the summary. The cluster with the higher content pertinence is kept as the complement to the entities of the summary.



IV. EXPERIMENTAL RESULTS  A. Evaluation of Summarized Video Knowledge We choose ten types of videos as our test data, and each  video is summarized into a relational graph. Table I lists the information of these videos.  In order to observe the influence on humans, we use informativeness and interrelation [5] to evaluate the results. The criterion, informativeness, measures whether the result can bring users as much information as the original video does or not. Interrelation estimates the mutual relation between the linked shots.

Table I  TEST VIDEO DATA  No. Subject Length No. Subject Length I-1 Dam 30 minutes IV-1 World War II 50 minutes I-2 Bridge 45 minutes IV-2 World War II 50 minutes II-1 Sea 51 minutes V-1 Egyptian 54 minutes II-2 Sea 51 minutes V-2 Mummy 52 minutes III-1 Caribou 57 minutes III-2 Bear 47 minutes   To compare our proposed method with the traditional system, we select our mining results as the baseline (by removing annotations, edges and the structure information). In our experiments, we invite ten users, and each of them is asked to give a score to informativeness. After browsing the video results, they can watch the original video and write down their decisions. We follow the same procedure to assess interrelation between shots in the graph. Users can give scores ranging from 1 (worst) to 5 (best) to the criterion.

Table II lists the result of the video summaries. The average  score of the videos using our system is 82.3. Compared with the baseline, the informativeness difference between the two methods reaches 10.6% on average. Such results may imply that our summary has better interpretive ability to summarize stories than the traditional one. Besides, in the assessment of the interrelation test, our average score is above the mean value, which also shows the efficacy of the proposed method. In summary, judging from these evaluations, we may see that the traditional (baseline) yielded limited information about the story, but our system has better organizing ability.

Table II  EVALUATION RESULTS  No. Informativeness (%) Interrelation Proposed Baseline Proposed  I-1 84.6 72.2 4.1 I-2 82.9 70.3 4.5 II-1 83.3 76.2 4.3 II-2 84.0 75.9 3.9 III-1 83.5 68.0 3.1 III-2 80.5 65.0 3.0 IV-1 75.8 67.3 4.1 IV-2 76.8 71.2 3.9 V-1 85.4 74.7 2.3 V-2 85.9 76.0 2.9  Aver. 82.3 71.7 3.6 Drop 17.7 28.3 1.4    B. Evaluation of Augmented Contents In order to construct a social network for the test, we use the  test videos in Table I and retrieve their augmented videos from Youtube. All the related videos in the linked-list of each returned video are also processed. To compare our algorithm with other methods, we conduct the following metrics: 1) betweenness [13]; 2) hyperlink induced topic search (Hits) [14]; 3) node degree; 4) page rank [15]. The precision score is then computed to benchmark the performance. The results are shown in Fig. 3.

Fig. 3.  Evaluation results of the network analysis using different metrics.

The mean of the precision scores is adopted to measure the  precision over the relevant video set corresponding to each entity. As shown in the chart, our mean precision reaches 80.20%, which is higher than the other four results. This indicates that our proposed method can find out knowledge, which is more relevant to the queries. Accordingly, the users can obtain what they are interested in by using our feedbacks.

Fig. 4 demonstrates some examples of the augmented content with regard to video I-1. The augmented contents associating with each entity are also displayed in this figure. We use a representative frame to denote a relevant video from Youtube.

For clarity, we list the first two videos only. The topic of video I-1 is about the Hoover dam. As shown in those representative frames, we can see that the augmented videos have strong correlations with video I-1. Besides, after checking the descriptions of these correlated videos, we found that their VSM scores are also higher than the scores of the other videos.

This implies the selected ones can provide another literal data for the users. One advantage of our proposed system is that as long as the users select shots of interest in the summarized graph, the system can offer the other clues, which are correlated with the selection. Thus, they can simply click on the shots and browse a series of contents to obtain complete information. On the contrary, the traditional approach cannot provide users with such results.



V. CONCLUSION In this paper, we present a video knowledge exploration  system that can offer augmented and summarized contents to users. In order to discover significant shots among summarized contents, we incorporate visual and textual weights into the graph mining algorithm. Moreover, these weights are also integrated into the social network analysis to calculate content pertinence values, which can determine association degree to the summary. While searching for online videos, we utilize the Markov clustering algorithm to refine the output of the network analysis. Thus, the resulting videos can be appended to each corresponding entities, serving as the augmented contents. Our proposed method not only structuralizes video knowledge but also discovers semantically correlated videos on the web. The assessment of the summarized contents has demonstrated the performance of our system. Compared with the conventional approach, we improve informativeness by 10.6% averagely. In the evaluation of the social network analysis, we have proved the effectiveness of our approach. Averagely, the mean precision between the summary and augmented content is 80.20%, which is higher that the other baselines. These experiments also indicate the organizing ability of our system and prove the necessity of augmented knowledge.

