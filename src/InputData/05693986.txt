Strati?ed Sampling for Data Mining on the Deep Web

AbstractIn recent years, one mode of data dissemination has  become extremely popular, which is the deep web. Like any other  data source, data mining on the deep web can produce important  insights or summary of results. However, data mining on the deep  web is challenging because the databases cannot be accessed directly,  and therefore, data mining must be performed based on sampling of  the datasets. The samples, in turn, can only be obtained by querying  the deep web databases with speci?c inputs.

In this paper, we target two related data mining problems, which  are association mining and differential rule mining. We develop  strati?ed sampling methods to perform these mining tasks on a deep  web source. Our contributions include a novel greedy strati?cation  approach, which processes the query space of a deep web data  source recursively, and considers both the estimation error and  the sampling costs. We have also developed an optimized sample  allocation method that integrates estimation error and sampling  costs. Our experiment results show that our algorithms effectively  and consistently reduce sampling costs, compared with a strati?ed  sampling method that only considers estimation error. In addition,  compared with simple random sampling, our algorithm has higher  sampling accuracy and lower sampling costs.



I. INTRODUCTION  In recent years, one mode of data dissemination has become  extremely popular, which is the deep web. Deep web is the term  coined to describe the contents that are stored in the databases  and can be retrieved over the internet by querying through HTML  forms, which are also called query interfaces. Deep web data  sources are playing an important role in the modern society,  impacting practically every Internet user. An early study on the  deep web, conducted in year 2000, estimated that the public  information in the deep web is 500 times larger than the surface  web, with 7,500 Terabytes of data, across 200,000 deep web  sites [5].

Deep web data sources have a unique characteristic, which is  that data can only be accessed through the query interface they  support. These interfaces are based on input attribute(s), and a    user query involves specifying value(s) for these attributes. In  response to such a query, dynamically generated HTML pages  returned as the output, comprising output attributes.

The deep web has received much attention lately [7], [9].

A number of recent efforts have also been building deep web  querying systems[7], [9], trying to provide mediator-like support.

However, one issue that has not been considered much is mining  data available in one or more deep web data sources. Clearly, like  any other data sources, data mining on the deep web can produce  important insights or summary of results. For example, there may  be interest in analyzing data available from two different travel  web-sites, and summarizing the cases for which one can offer  lower fares than the other.

Data mining on the deep web is challenging because the  databases cannot be accessed directly. Thus, data mining must  be performed based on sampling of the datasets. The samples, in  turn, can only be obtained by querying the deep web databases  with speci?c inputs. Though sampling for ef?cient data mining  has been widely studied [23], [21], [11], [6], these methods  are not directly applicable, as samples can only be obtained by  issuing queries with speci?c inputs on the deep web. At the same  time, there have been recent efforts on sampling the deep web [3],  [13], [15]. However, none of these have been in the context of  mining the deep web. Web mining is another widely studied  topic [20], but refers to analysis of the surface web. The deep  web involves a distinct set of challenges.

In this paper, we target two related data mining problems,  and develop sampling methods to ef?ciently mine deep web data  sources. The ?rst is frequent itemset mining [2], [26], one of the  most widely studied problems in the data mining community. The  second is a somewhat similar problem, differential rule mining.

The goal in differential rule mining is to obtain a summary of  the comparison of values of attributes between two data sources.

A differential rule is of the form X ? D1(t) > D2(t), where X  is a frequent itemset composed of identical attributes, and t is a  differential attribute in the two data sources. This rule indicates  that given the occurrence of the itemset X , the value of attribute  t from the data source D1 is signi?cantly larger than that from  the data source D2. Here, identical attributes are attributes whose  values are same in these two data sources, whereas differential  attributes are the attributes whose values are different.

Sampling for frequent itemset mining has been studied by    several researchers [23], [21], [11], [6], but the current work  has been in the context of relational databases or streaming  data, where data records are visible and samples are easy to  obtain. An intuitive method for the deep web will be to identify  rules from a pilot random sample and further obtain samples to  verify these rules. This method seems similar to the algorithm  proposed by Toivonen [23]. However, since the data distribution  in the backend database is unknown and the data records are not  directly accessible, the sampling procedure is quite challenging.

For example, to verify a discovered association rule A = a ? Y ,  more data records containing A = a are required. If A is an  output attribute from the deep web, obtaining such data records  is quite hard, as the deep web data source will only accept values  of input attributes in the queries. Randomly issuing more queries   1550-4786/10 $26.00  2010 IEEE  DOI 10.1109/ICDM.2010.17   may yield desired data records, but it may need a large and  unknown number of queries to collect a certain number of desired  data records. Acquiring data from the deep web is especially time  consuming, since deep web queries are executed over a wide area  network. A recent study from a deep web integration system  shows that nearly 80% of the execution time is spent on data  delivery between the server and the clients [24]. Thus, to reduce  the computation cost and the network connection time, sampling  methods must be ef?cient and effective.

In this paper, we introduce the strati?ed sampling method to  support association rule mining and differential rule mining on  the deep web. Our approach includes novel methods for both  strati?cation and sample allocation. As stated earlier, we ?rst  pick a pilot random sample from the deep web for identifying  interesting rules. Then, the data distribution and relation between  input attributes and output attributes are learnt from the pilot  random sample. After that, strati?cation and sample allocation is  conducted on the deep web.

Traditional strati?ed sampling methods [12], [22], [8] aim  at minimizing variance of estimation, which is an indication  of estimation accuracy. However, in order to conduct strati?ed  sampling in the context of deep web, ef?ciency or sampling cost,  i.e., the number of distinct queries that need to be issued, is  also an important consideration. We de?ne an integrated cost that    takes into account both the variance of estimation and sampling  cost. We also allow users to specify different weights to these  two factors.

Strati?cation, which aims at dividing the entire population into  sub-populations, is conducted on the query space of deep web  data sources based on the knowledge obtained from the pilot  random sample. The query space is recursively strati?ed in order  to minimize the integrated cost in a greedy way. At each step,  the input attribute that reduces the integrated cost maximally is  chosen and the query space is divided. For sample allocation, i.e.,  for deciding how many data records should be sampled from each  stratum, we propose an algorithm which optimally minimizes the  integrated cost.

Our experimental results using two real datasets show the  following. First, compared with simple random sampling on  the deep web, our algorithm has a higher sampling accuracy  and a lower sampling cost. Second, our algorithm ef?ciently  and consistently reduces sampling cost by trading off a certain  fraction of estimation error, compared to approaches based on  adapting the traditional strati?ed sampling methods for the deep  web.

The rest of the paper is organized as follows. Section II gives  a brief overview of association rule mining and differential rule  mining. Section III formulates our sampling problem, and then  gives background on strati?ed sampling and the well-known  Neyman allocation method. Section IV describes our proposed  methods for strati?cation and sample allocation. Section V reports  the results from our detailed evaluation study. We compare our  work with related research efforts in Section VI and conclude in  Section VII.



II. BACKGROUND: ASSOCIATION RULE MINING AND  DIFFERENTIAL RULE MINING  In this section, we describe the background knowledge about  association rule mining and differential rule mining problems.

The goal of association rule mining algorithms is to discover  the co-occurrence patterns for items. The key step is ?nding  frequent itemsets X whose support is larger than a prede?ned  threshold. Based on frequent itemset mining, association rule  mining discovers rules in the form of X ? Y where X ? Y  is a frequent itemset and the con?dence, support(X?Y )support(X) , is larger  than another speci?ed threshold.

Somewhat related to association mining, differential rule min-    ing is proposed to discover the patterns in the differences between  the values for the same entity provided by different deep web data  sources. Summarizing such a difference is clearly important for  the users of these data sources. For example, users will bene?t  from a summary information stating that one website routinely  offers rooms at hotels of certain type, and/or in cities within a  certain geographical region, at a lower price. This can be viewed  as a variation of contrast set mining [4], [16], with emphasis on  differential values of numerical attributes.

More speci?cally, in differential rule mining, the goal is  generating a high-level summary of different values of target  attributes, by comparing values for the same data entities across  different sources, under different combinations of conditions. We  consider two types of attributes when comparing data objects.

Identical attributes are the attributes whose values are the same  when considering the same data object obtained from different  data sources. These attributes help identify the same data objects.

Differential attributes or target attributes are the attributes whose  values could be different for the same data object. For example,  in ?nding differential rules across travel-related deep web data  sources, hotel name and location are identical attributes, whereas  price is a differential or target attribute.

A differential rule is of the form X ? D1(t) > D2(t),  and shows the difference in the values of the target attribute  t, across the data sources D1 and D2, under the condition X ,  which is captured by the values of identical attributes. In order  to identify differential rules, there are two main steps: 1) Frequent  itmesets mining which discovers the frequent itemsets composed  of identical attributes 2) Having identi?ed the frequent itemsets,  a hypothesis test, paired Z-test, is conducted to ?nd signi?cant  differential pattern of the values for each target attribute. The  patterns of values for the target attribute is considered to be  signi?cantly differential if the mean of the difference between its  values in the two data sources is signi?cantly larger or smaller  than 0.

Besides analysis of travel-related websites, differential rule  mining has several other applications. For example, it can be used  for comparing the values of attributes for the same entity under  different circumstances, i.e. sales amount of the same product in  different years, or comparing the values of different attributes for  the same entity, i.e. incomes of husband and wife in the same  household.



III. BASIC FORMULATION  In this section, we formulate the sampling problem we are  addressing in this paper, and then describe the basic idea of  applying strati?ed sampling [17] in data mining problems, as well  as one of the well known sampling allocation method, Neyman  Allocation [12].

A. Problem Formulation  For mining any deep web source, the sampling process will  involve two steps:  A pilot sample is randomly sampled from the deep web, and  interesting rules are identi?ed from these samples. Such a  random sample can be obtained using existing methods, such  as those from the recent work of Dasgupta et al. [13], [15],  [14].

Additional data records are sampled under the space of iden-  ti?ed rules for verifying them, based on the data distribution  revealed by pilot sample.

While applying association rule mining and differential rule  mining on the deep web, an item is represented as A = a,  where A is an attribute that is assigned the value a. Both input  and output attributes can be involved. Identi?ed association rules  are of the form X ? Y , and identi?ed differential rules are of  the form X ? D1(t) > D2(t), where X is a frequent itemset.

To verify an identi?ed rule, which could be an association rule  or a differential rule, additional data records are required under  the space of X . If X only contains input attributes, the task  of sampling is easy. Queries with values of items in X can be  randomly selected and submitted to the query interface. However,  if X involves output attributes, the sampling problem becomes  more complicated, because of the limited interfaces supported by  deep web data sources. These interfaces only accept queries with  values of input attributes, and do not have mechanism to ?nd  records with speci?c values of output attributes. Our goal is to  achieve high estimation accuracy with a low sampling cost, and  random sampling is not a satisfactory solution.

To simplify the presentation of our approach, we focus on the  rule whose left hand side only contains a single output attribute,  i.e. X is {A = a}, where A is an output attribute. However, our  approach is general, and can consider multiple different output  and/or input attributes in the rule. Our method tries to learn  the relation between output attributes and input attributes, and    conduct strati?ed sampling in the query subspaces composed of  input attributes. If there are multiple different output and/or input  attributes in the rule, which can be considered as a composite  output attribute, our method is still able to learn the relation  between the composite output attribute and input attributes except  those included in the composite output attribute, and then conduct  strati?ed sampling as before.

While considering association rule mining, identi?ed associa-  tion rules are in the form of A = a ? Y , with supports(A=a?Y )supports(A=a) ,  which is the probability of Y given A = a in the pilot sample,  being larger than a threshold. Our goal is to obtain more data  records under the space of A = a to more accurately estimate the  value of support(A=a?Y )support(A=a) . For this, we need to develop sampling  methods that can achieve high accuracy of estimation, with small  sampling cost, or while acquiring as few samples as possible.

If our goal is differential rule mining, the problem is quite  similar. We focus on the differential rules in the form of A =  a ? D1(t) > D2(t), where A is an output attribute of the deep  web data source, and an identical attribute for our formulation. t  is a differential attribute, whose value we are comparing across  the two data sources, D1 and D2. The differential rule indicates  the behavior of differential attribute t given A = a. Thus, our  goal is to design sampling plans for accurately estimating the  mean value for D1(t)?D2(t), given A = a.

B. Strati?ed Sampling and Neyman Allocation  Before presenting our approach, we show how the strati?ed  sampling is applied in data mining problems. Unlike simple  random sampling, which draws a sample from the population  in entirety, strati?ed sampling [17] picks separate samples from  H groups, which are also called strata or sub-populations. When  data varies considerably across sub-populations, and the variance  within each sub-population is small, it is advantageous to sample  each sub-population (stratum) independently.

There are two considerations in reducing the variance in  strati?ed sampling: strati?cation, which is the process of dividing  the entire population into sub-populations, and sample allocation,  which determines the size of sample drawn from the ith stratum.

A well known method for sample allocation is the Neyman  Allocation [12]. Neyman allocation is based on the observation  that the variance of the strati?ed estimation is minimized when  sample size for the ith stratum is proportional to the size of the  stratum and to the variance of the target values in the ith stratum.

Target values have different meanings for different data mining  problems.

Now, let us consider the application of strati?ed sampling and  Neyman allocation to differential rule mining and association  mining. For differential rule mining in the form of A = a ?

D1(t) > D2(t), the goal is to estimate the mean value for  D1(t) ? D2(t) given A = a over the entire population. Let n  denote the number of data records sampled across the strata, and  ni denote the number of data records drawn from the ith stratum.

The size of population containing tuples under the space of A = a  is denoted by Ni for the ith stratum. Now, let the expression y  denote D1(t) ? D2(t) given A = a and yi denote the sample  average of the ith stratum. Then, using strati?ed sampling, the  mean value of y is estimated as  ys =  H?

i=1  Ni  N  yi (1)  It turns out that ys is an unbiased estimation , which implies  that E(ys) = E(y). If ?2i is the variance of the ith stratum, the  variance for ys is:  ?2s =  H?

i=1  N2i  N2  ni?

i (2)  In the stratum whose variance is unknown, ?2i is estimated by   sample variance, i.e,  S2i =  ?

y2ij  ni  ? (  ?

yij  ni  )2    Here, yij is a sampled record from the ith stratum.

These expressions are also applied to association rule mining  in the form of A = a ? Y , with the goal being to estimate the  con?dence supports(A=a?Y )supports(A=a) over the entire population. We use  the expression y to denote whether Y is contained in the data  record under the space of A = a: if the data record under space  of A = a contains Y , let y = 1; otherwise, y = 0. The con?dence  for the association rule is calculated by estimating the mean value  of y based on Expression 1, and the variance of estimation for  the con?dence over the entire population is still calculated by  Expression 2.

Formally, with Neyman allocation, the sample allocation can  be stated as:  ni =  n?

j=1Nj?j  Ni?i (3)  where ?i are calculated as described above.

While Neyman Allocation ef?ciently reduces the variance of  estimation, it does not consider the sampling cost in the deep  web. Speci?cally, sampling cost is de?ned as the number of  queries submitted to deep web data sources in order to obtain  a ?xed number of data records under the particular space. We  assume that for each query submitted to the deep web, a single  data record is obtained. In the process of sampling data records  corresponding to A = a over the deep web, the probability of  A = a in each stratum is not considered in Neyman allocation (or  other existing methods). On a deep web source, it is quite possible  that a large sample might have to be drawn from a stratum where  the probability of a data element satisfying A = a is small. This  will result in a high sampling cost.

The following example shows the inef?ciency of Neyman  Allocation in the case of differential rule mining on the deep  web. For a rule A = a ? D1(t) > D2(t), we assume there are  three strata. The columns one through four of the Table I show  the stratum ID, population size (Ni), ?2i , which is the variance  of D1(t)?D2(t), and ?i, which enotes the probability of A = a  being true in any given record in this stratum.

TABLE I  EXAMPLE OF APPLICATION OF NEYMAN ALLOCATION FOR DEEP WEB  MINING  ID Size Variance Probability of A = a Sample size    1 800 10000 0.9 100  2 800 40000 0.8 200  3 800 90000 0.1 300  By assuming that the total sample size is 600, the sample size  ni according to Neyman Allocation for each stratum is shown in  the ?fth column of Table I. According to the Expression 2, the  resulting estimation is 66.67, and the estimated sampling cost is  3361.1. In this example, the sampling cost is very high, because  a large sample is allocated to the stratum 3, which happens to  have a low probability of A = a. If we could allocate a smaller  sample to the stratum 3 and a larger sample to the strata 1 and  2, the sampling costs would be decreased, though possibly with  some increase in the sampling variance.

C. Our Approach  As we stated in the previous section, randomly constructing  and submitting queries can not ensure that the output results  contain data records within the space of A = a. Thus, ef?ciently  acquiring samples under the space of output attribute A = a  and accurately estimating the parameters (con?dence or mean  value) of target attributes under the space of A = a both become  challenging problems.

Since sampling on the deep web can only be performed by  constructing and submitting queries, strati?cation needs to be  performed on the query space, i.e., involving input attributes  only. In order to ef?ciently acquire samples under the space of  a speci?c value of an output attribute (i.e. A = a), the relation  between input attributes and output attributes is important. In  other words, we need to know which input queries are more  likely to yield output web pages containing tuples under the  space of A = a. Furthermore, in order to accurately estimate  the parameters (con?dence or mean value) of the target attributes  under the space of A = a, similar values of target attributes under  A = a are required to be in the same group. For this purpose,  we are proposing a greedy strati?cation method. Our method  considers both ef?ciency and accuracy, so that input queries that  are more likely to yield output web pages containing tuples under  the space of A = a are identi?ed, and similar values of target  attributes under A = a are grouped in the same sub-population.

In our approach, the relation of input attribute and output attribute  as well as the data distribution of target attributes are learnt from  the pilot sample.

The second component of our approach is a novel sample    allocation method. To ef?ciently acquire samples under the space  of output attribute A = a, large sample should be drawn from  the query subspace that is more likely to yield output web pages  containing tuples under the space of A = a. On the other side, to  accurately estimate the parameters (con?dence or mean value) of  target attributes under the space of A = a, a large sample should  be drawn from the query subspace that has a larger variance of  the target value under the space of A = a. We have developed an  optimized sample allocation algorithm which integrates the two  aspects (ef?ciency and accuracy).



IV. MAIN TECHNICAL APPROACH  In this section, we introduce our strati?ed sampling approach in  the context of association rule mining and differential rule mining  on deep web data sources. As we stated earlier, our approach  comprises of a strati?cation method and a sample allocation  method. These are described in the next two subsections. Towards  the end of this section, we describe how differential rule mining  can be performed using these two methods.

A. A Greedy Strati?cation Method  Strati?cation refers to partitioning the population being sam-  pled. Traditional strati?cation methods such as Recursive strat-  i?ed sampling [22], [8] aim at grouping similar values into the   Algorithm 1 Strati?cation(N,Q, PS, s, Lf )  1: Cost(N) = ?s  SampCost(N) + ?v  ?2(N)  2: if Cost(N) < ? then  3: ? integrated cost is smaller than ?

4: Lf = Lf ?N  5: else  6: ? partition the query space of N  7: splitCost ? maxV alue  8: M ? null  9: for all IA ? PS do  10: CH ?strata generated by IA  11: Cost(CH) = ?s  SampCost(CH) + ?v  ?2(CH)  12: if Cost(CH) < splitCost then  13: splitCost=Cost(CH)  14: M=IA  15: end if  16: end for  17: DM ? domain of M  18: ASM ? sample allocation    19: for all mi ? DM do  20: Strati?cation(CHi, Q ? {M = mi}, PS ?M,asi, Lf )  21: end for  22: end if  same subpopulation, in order to maximally reduce variance of  estimation. As we have stated previously, two related issues make  the sampling for a deep web data source different. The ?rst  is because of the limited access to the database, i.e., samples  can only be obtained by querying through a simple interface.

Thus, obtaining additional samples meeting A = a is non-trivial.

Second, ef?ciency of sampling is an important consideration,  since each deep web query is slowed down due to the data  delivery delay between the server and the client.

We now describe our greedy strati?cation method which par-  titions the population of the deep web recursively. Strati?cation  is performed on the query space composed of input attributes  in the query interface. In other words, the population of a deep  web source can be considered to correspond to the entire query  space, and the subpopulations correspond to query subspaces.

More precisely, a subpopulation comprises of the data records  that can be obtained by submitting queries from the corresponding  subspace.

To establish this correspondence, the relationship between  input attributes and output attributes is learnt from the pilot  sample. In this fashion, the subpopulations or strata that are more  likely to contain tuples where A = a is true are identi?ed. We  de?ne sampling cost to be the estimated number of data records  that will need to be drawn from the deep web, in order to obtain  a speci?ed number of records where A = a is true. Let ?i  represents the probability of ?nding a data record with A = a  in the stratum i. This value can be estimated based on the pilot  sample. Thus, to obtain ni data records where A = a is true in  ith stratum, the estimated number of data records drawn from  ith stratum is ni?i . Overall, in order to obtain our desired n data  records where A = a is true from the entire population, the  estimated sampling cost will be:  SampCost =  ?

i  ni  ?i  (4)    While focusing on the sample cost and using the strata that  are more likely to contain tuples where A = a is clearly  important, variance of estimation is another important issue, since  it corresponds to the error of estimation. Thus, we de?ne the  integrated cost by taking into account the variance of estimation  ?2s and the sampling cost SampCost:  Cost = ?s  SampCost + ?v  ?2s (5)  where ?s and ?v are weights for sampling cost and estimate  variance, respectively, such that their sum is 1. Users could set  the two weights based on the importance they attach to these two  factors. For example, if response to a query is needed quickly,  sampling cost should have a higher weightage.

The relation between input attributes and output attributes, as  well as data distribution, are modeled by a tree built on the  query space recursively. Each node in the tree represents a query  subspace, and is associated with a query that comprises speci?c  values for a subset of input attributes, the sample size, as well  as the potential splitting attributes. The sample size shows how  many data records would be drawn from the subpopulation of the  node.

Algorithm 1 shows the process of splitting a node N , which as  we stated above, is associated with the query Q, the sample size  s, and a list of potential splitting attributes, PS. The inputs of the  algorithm also include set Lf , which represents leaf nodes of the  tree. At the beginning, the entire query space is represented by  the root node. The corresponding query of the root node is null,  the sample size n is the size of the sample that is to be drawn  from the entire population, and the potential splitting attributes  list is the complete set of input attributes of this data source. The  initial set of leaf nodes is empty.

The main goal of the algorithm is to split the query tree in a  greedy way. For each potential splitting attribute, the integrated  cost after splitting is computed according to the Expression 5. The  input attribute which brings the most reduction in the integrated  cost is selected. More speci?cally, ?rst, the integrated cost for  the subpopulation of the node N is calculated. If this value is  smaller than a prede?ned threshold ?, node N is set to be a  leaf node (Lines 2-5). Otherwise, node N will be partitioned by  considering the set of potential splitting attributes PS (Lines 6-  21). For each input attribute IA ? PS with domain DIA, there  would be |DIA| strata under the space of node N . The integrated  cost on |DIA| strata is computed. Let M ? PS with domain DM    denote the input attribute with the minimum integrated cost after  the split, and let the set of allocated sample sizes, computed by  the sample allocation method explained later, be ASM . Then,  |DM | children are generated for the node M . For each child  CHi, i = 1, . . . , |DM |, the associated query is QN ?{M = mi},  sample size is asi ? ASM , and potential splitting attributes is  PSN ? M . The process of splitting is then recursively applied  to children of node N .

In the process of calculating costs during the strati?cation  process, we need to perform sample allocation, i.e., divide the  parent nodes sample size among the potential children nodes.

This is required for calculating the integrated cost for the potential  split. This is based on our sample allocation method, which we   describe next in Section IV-B. Furthermore, for calculating the  integrated cost, the variance of target value ?2i and probability of  output attribute A = a, ?i, for each stratum is computed based  on the pilot sample.

Initially, the strati?cation process on the query space begins by  calling Stratification(R, null, F IA, n, null), where R is the  root node. The process of strati?cation would stop if there is no  leaf node with integrated cost larger than a prede?ned threshold  ?. Each leaf node in the tree is a ?nal stratum for sampling, and  the associated sample size denotes the number of data records  drawn from the subpopulation of the stratum.

B. An Optimized Sample Allocation Method  Now, we introduce our optimized algorithm for sample allo-  cation which integrates variance reduction and sampling cost.

As introduced in section IV-A, integrated cost is de?ned by  taking into account of variance of estimation and sampling cost.

The goal of our sample allocation algorithm is to minimize the  integrated cost by choosing the sample size, ni, for each stratum.

In our algorithm, we adjust the value of SampCost and ?2s so  that their in?uences on the integrated cost are in the same unit:  SampCost? =  SampCost  SmpCost(r)  where SmpCost(r) = n?r is the expected sampling cost of the  entire population, and ?r denotes the probability of A = a being  true for the entire population.

??2 =  ?2s    ?2r  where ?2r =  ?2  n denotes the variance of estimation of the target  value on the entire population.

The key constraint on the values of the sample sizes for each  strata is that their sum should be equal to the total sample size.

A vector ?n = {n1, n2, . . . , nH} is used to represent the sample  sizes, where the ith element, ni, is the sample size for the ith  stratum.

By including sampling cost and variance of estimation into the  integrated cost, our sample size determination task leads to the  following optimization problem:  Minimize Cost(?n) =  ?

i(?s  ni  ?i  + ?v  N2i  niN2?

i )  subject to  ?

i ni = n  (6)  where Ni denotes the population size of data records under the  space of A = a in ith stratum. Note that this value may not be  known if A is an output attribute. However, the total number  of records in the ith stratum is typically known, and can be  denoted as DNi. Then Ni can be estimated by ?iDNi, and the  population size of A = a on the entire population is estimated  by N =  ?

i ?i DNi.

For ?nding the minimum of integrated cost, we utilize La-  grange multipliers, a well know optimization method. Lagrange  multipliers aims at ?nding the extrema of a function object  to constraints. Using this approach, a new variable ? called a  Lagrange multiplier is introduced and de?ned by:  ?(?n, ?) = Cost(?n) + ? (    ?

i  ni ? n)  If ?n is a minimum solution for the original constrained problem,  then there exists a ? such that (?n, ?) is a stationary point for the  Lagrange function. Stationary points are those points where the  partial derivatives of ?(?n, ?) are zero:  ??n,??(?n, ?) = 0 (7)  In our problem, by conducting partial derivatives on Formula 7,  a group of equations are yielded as follows:  ?s  ?i  ? ?v N   i  n2iN2?

i + ? = 0 i = 1, ..H?

i ni = n  (8)  where the solution ?n leads to the minimum value of integrated  cost in Formula 5.

However, it is dif?cult to solve the group of equations directly.

Thus, we use numerical analysis to approximate the real solu-  tion. Newtons method is utilized for ?nding successively better  approximations to the zeroes (or roots) of a real-valued function.

Given an equation f(x) = 0 with f ?(x) which denotes the deriva-  tive of function f(x), and x0, a ?rst guess of the root. Newtons  method iteratively provides, xt+1, a better approximation of the  root, based on xt, the previous approximation of root according  to the following formula:  xt+1 = xt ? f(xt)  f ?(xt)  The iteration is repeated until a suf?ciently accurate value is  reached, i.g. |f(xt)| is smaller than a prede?ned threshold.

In our problem of Formula 8, there are H + 1 equations  F (?xt) = {f1(?xt), ..., fH+1(?xt)} on H + 1 variables ?x =  {n1, .., nH , ?}, where the equations in F (?xt) are as follows:  fi(?xt) =  ?s  ?i  ? ?v N     i  n2iN2?

i + ? i = 1, .., H  fH+1(?xt) =  ?

i ni ? n  The Newtons method is also applied iteratively via the system  of linear equations:  JF (?xt)(?xt+1 ? ?xt) = ?F (?xt) (9)  where JF (?xt) is the (H + 1) (H + 1) Jacobian matrix of the  equation system F (?xt) by assigning variables ?x with values of  vector ?xt. The entry JF (i, j) is calculated by  d(fi(?x))  dxj  , where  fi(?x) is the ith equation of F (?xt) and xj is the jth element of  ?x.

From the Expression 9, a better approximation ?xt+1 is obtained  based on previous approximation ?xt. The iterative procedure  would be stopped if ?i|fi(?xt)| is smaller than a prede?ned  threshold, and then the sample size ni is allocated for each  stratum so that the integrated cost is minimized. In reality, we  are required to pick an integral number of records from each  stratum during the sampling step. Thus, we round down each ni  to its nearest integer, ?ni + 0.5?.

In the example shown in Table I, suppose we set both weights  ?v and ?s to be 0.5. Further, assume the variance of the entire  population, ?2r , to be 80000. The probability of A = a over the  entire population ?r is 0.242. By using the proposed optimized  sample allocation method, the sample sizes for the three strata   are 162, 299, and 139, respectively. In this case, the variance  of estimation according to the Expression 2 is 93.66, and the  estimated cost is 1943.7. We can see that, compared with Neyman  allocation, the sampling cost is decreased by 42.1%, but results  some increase in variance. Overall, this example shows that we  can achieve lower sampling cost by trading off some accuracy.

C. Overall Sampling Process  Algorithm 2 DiffRuleSampling(DW1, DW2, F IA, t, St)  1: PS ? a pilot sample from DW1, DW2    2: DR ? identi?ed rules from PS  3: OA ? output attributes of DW1, DW2  4: for all R : X ? DW1(t) > DW2(t) ? DR do  5: if X ?OA = null then  6: Acquire St data records from the space of X  7: else  8: R ? root node  9: Lf ? null  10: Strati?cation(R,null, F IA, St, Lf )  11: for all N ? Lf do  12: s ? sample size of N  13: Draw s data records from the subpopulation of N  14: end for  15: end if  16: Update the mean value of DW1(t)?DW2(t)  17: end for  Algorithm 2 shows the overall sampling process for differential  rule mining on two deep web data sources, DW1 and DW2,  and with differential attribute t. The inputs of the algorithm  also contain the full set of input attributes FIA as well as  the sample size St. The algorithm starts with a pilot sample  PS, from which the differential rules are identi?ed. For the rule  R : X ? DW1(t) > DW2(t) only containing input attributes,  St data records are directly drawn from the space of X (Lines  5-6). However, for the rule R : X ? DW1(t) > DW2(t)  containing output attributes, query spaces of DW1 and DW2  are strati?ed and sample is recursively allocated to each stratum  with corresponding query subspace (Line 10). For each leaf node  of the tree built by strati?cation, a sample is drawn according to  its sample size (Line 13). The mean value of DW1(t)?DW2(t)  is updated by the further sample (Line 16). The sampling process  for association rule mining is very similar and not shown here.



V. EVALUATION STUDY  We evaluate our sampling methods for association mining and  differential rule mining on the deep web using two datasets,  described below.

US Census data set: This is a 9-attribute real-life data set  obtained from the 2008 US Census on the income of US  households. This data set contains 40,000 data records with 7  categorical attributes about the race, age, and education level  of the husband and wife of each household and 2 numerical  attributes about the incomes of husband and wife.

Yahoo! data set: The Yahoo! data set, which consists of the data  crawled from a subset of a real-world hidden database at  http://autos.yahoo.com/. Particularly, we download the data on  used cars located within 50 miles of a zipcode address. This  yields 30,000 data records. The data consists of 7-attribute with  6 categorical attributes about the age, mileage, brand, etc, of the  cars and one numerical attribute, which is the price of the car.

Variance of Estimation is estimated for the target value (i.e.

mean value in differential rule mining, and con?dence in associ-  ation rule mining). In our experiments, the variance of estimation  is calculated according to the Expression 2. Since variance of  estimation reveals the variation of the estimated value from the  true value, smaller variance suggests better estimation.

Sampling Cost is estimated by the number of queries submitted  to data sources in order to acquire a certain number of data  records containing target output attributes, i.e. A = a. Larger  sample size implies higher sampling costs in a deep web setting,  where the queries are executed over the internet.

Estimate accuracy is estimated by Absolute Error Rate (AER).

Small AER value indicates higher accuracy. For an estimator on  variable Y with true value y and estimated value y, the AER of  the estimator is calculated by AER(y) = |y?yy |.

A. Association Rule Mining  In this section, we present the results of our method for  association rule mining. Using our overall approach, we have  created four different versions, which correspond to four different  sets of weights assigned to variance of estimation and sampling  costs. 1) Full Var only considers variance of estimation by setting  the weight ?v = 1.0 and ?s = 0.0, 2) Var7 is created by setting  the weight ?v = 0.7 and ?s = 0.3, 3) Var5 is created by weights  ?v = 0.5 and ?s = 0.5, and 4) Var3 is the version with the the  weights ?v = 0.3 and ?s = 0.7. In addition, we also compare  these approaches with a simple random sampling method, which  is denoted by Random.

We focus on the queries in the form of A = a ? B = b,  where A and B are output categorical attributes. Other categorical  attributes in the data set are considered as input attributes.

Our goal is to estimate Supp(A=a,B=b)Supp(A=a) . In our experiments, 50  association rules are randomly selected from the datasets. Each  of the 50 association rules are re-processed 100 times using 100  different (pilot sample, sample iterations) combinations. Thus, the  result is the average result for 5000 executions.

In all charts reported in this section, the X-axis is k, which  denotes the size of sample under the space of a target rule drawn  from deep web. The sample size for each point on X-axis is  k x, where x is a ?xed value for our experiment, and depends  upon the dataset. At each time, queries are issued to obtain kx  data records under the space of a target rule. Overall, all our  experiments show the variance of estimation, sampling costs and  sampling accuracy with varying sample size.

Figure 1 shows the result from our strati?ed sample methods  on the US census data set. The size of pilot sample is 2000, from  which all of the 50 initial rules are derived. In this experiment,  the ?xed value x is set to be 300, which means the smallest  sample size at k = 1 is 300, and the largest sample size at k =  10 is 3000. Figure 1 a) shows the variance of estimation for  the ?ve sampling procedures. Figure 1 b) shows the sampling  cost for the sampling procedures. In order to better illustrate the  experiment result, in each execution of sampling, the variance of   6DPSOLQJ?9DULDQFH  ?  ???  ???  ???  ???  ?  ???  ???  ???  ? ? ? ? ? ? ? ? ? ??  .

9D  UL  DQ  FH  ?R  I?  (V  WL  PD  WL  RQ  )XOOB9DU    9DU?  9DU?  9DU?  5DQG  (a) Variance of Es-  timation  6DPSOLQJ?&RVW  ?  ???  ???  ???  ???  ?  ???  ???  ? ? ? ? ? ? ? ? ? ??  .

6D  PS  OL  QJ  ?&  RV  W )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (b) Sampling Cost  6DPSOLQJ?$FFXUDF\  ?  ????  ????  ????  ????  ???  ????  ????  ? ? ? ? ? ? ? ? ? ??  .

$(     )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (c) Sampling Accuracy  Fig. 1. Evaluation of Sampling Methods for Association Rule Mining on US Census Dataset  estimation and sampling cost for the sampling procedures var7,  var5, var3, and rand are normalized by the corresponding values  of Full Var. Thus, in our experiment, the values of sampling cost  and variance of estimation for sampling procedure Full Var are  all 1. Furthermore, Figure 1 c) shows the accuracy for the ?ve  sampling procedures.

From Figure 1 a) and b), we can see that, as expected, com-  pared with sampling procedures Var7, Var5 and Var3, Full Var  has the lowest estimation variance and the highest sampling  cost. From sampling procedures Var7, Var5, and Var3, we can  see a pattern that the variance of estimation increases, and the  sampling cost decreases consistently with the decrease of the  weight for variance of estimation. At the largest sample size of  k = 10, the estimation variance of sampling procedure Var3 is  increased by 27% and the sampling cost is decreased by 40%,  compared with sampling procedure Full Var. The experiment  shows that our method decreases the sampling cost ef?ciently  by trading off a percent of variance of estimation. Similar to  variance of estimation, the sampling accuracy of these procedures  also decreases with the decrease of the weight on variance of  estimation. For the largest sample size at k = 10, we can see  that the AER of sampling procedure Var3 is increased by 20%  compared with sampling procedure Full Var. However, for many  users, increase of the AER will be acceptable, since the sampling  cost is decreased by 40%. By setting the weights for sampling  variance and sampling cost, users would be able to control the  trade-off between the variance of estimation, sampling cost, and  estimation accuracy.

In addition, compared with sampling procedure of Full Var,  Var7, Var5, and Var3, sampling procedure Random, has higher  estimation of variance, sampling cost and lower estimation accu-  racy. Thus, our approach clearly results in more effective methods  than using simple random sampling for data mining on the deep  web.

Figure 2 shows the experiment result of our proposed strati?ed    sampling methods on the Yahoo! data set. The size of pilot sample  on this data set is 2,000, and the ?xed value x for sample size  is 200. The results are similar to those from the US census  dataset. We can still see the pattern of the variance of estimation  increasing with the decrease of its weight. Besides, the sampling  accuracy is also similar to the variance of estimation. However,  although the variance estimation of sampling procedure Random  is 60% larger than sampling procedure Full Var, the sampling  cost of Random is 2% smaller than Full Var. This is because  Full Var does not consider sampling cost. It is possible that  Full Var assigns a large sample to a stratum with low ?, which  denotes the probability of containing data records under the space  of A = a, resulting the larger sampling cost than that of simple  random sampling. Sampling procedures Var7, Var5, Var3 consider  sampling cost as well, and have smaller variance estimation and  sampling cost, compared with Random. Furthermore, Random has  smaller sampling accuracy than Full Var, Var7 and Var5, but has  larger sampling accuracy than Var3. This is because Var3 assigns  much more importance to the sampling cost, and loses accuracy  to a large extent.

To summarize, our results shows that our proposed strati?ed  sampling are clearly more effective than simple random sampling  on the deep web. Moreover, our approach allows users to trade-  off variance of estimation and sampling accuracy to some extent,  while achieving a large reduction in sampling costs.

B. Differential Rule Mining  In this section, we present results from experiments based  on differential rule mining. Particularly, we look at the rules  of the form A = a ? D1(t) ? D2(t), where A is an output  categorical attribute and t is an output numerical attribute, while  other categorical attributes in the data set are considered as input  attributes.

In this experiment, we also evaluate our proposed method with  different weights assigned to variance of estimation and sampling  cost. Five sampling procedures, Full Var, Var7, Var5,Var3 and  Random, have same meanings with those in the experiments of  association rule mining. Similarly, 50 rules are randomly selected  from the datasets, and each of the 50 differential rules are re-  processed 100 times using 100 different (pilot sample, sample  iterations) combinations. Thus, the result is the average result for  5000 runs.

First, we evaluated the performance of these procedures on    the US census data set. The size of pilot sample is 2000, and all  50 rules are derived from this pilot sample. In this experiment,  the ?xed value x for the sample size is set to be 300. The  attribute income is considered as a differential attribute, and the  difference of income of husband and wife is studied in this  experiment. Figure 3 shows the performance of the 5 sampling   6DPSOLQJ?9DULDQFH  ?  ???  ???  ???  ???  ?  ???  ???  ???  ???  ? ? ? ? ? ? ? ? ? ??  .

9D  UL  DQ  FH  ?R  I?  (V  WL  PD  WL  RQ  )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (a) Variance of Es-  timation  6DPSOLQJ?&RVW  ?  ???  ???    ???  ???  ?  ???  ? ? ? ? ? ? ? ? ? ??  .

6D  PS  OL  QJ  ?&  RV  W )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (b) Sampling Cost  6DPSOLQJ?$FFXUDF\  ?  ????  ????  ????  ????  ???  ? ? ? ? ? ? ? ? ? ??  .

$(   )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (c) Sampling Accuracy  Fig. 2. Evaluation of Sampling Methods for Association Rule Mining on the Yahoo! Dataset  procedures on the problem of differential rule mining on the US  census data set. The results are also similar to the experiment  results for association rule mining: there is a consistent trade  off between the estimation variance and sampling cost by setting  their weights. Our proposed methods have better performance  than simple random sampling method.

We also evaluated the performance of our methods on the  Yahoo! dataset. The size of pilot sampling is 2000, and the  ?xed value x for the sample size is 200. The attribute price is  considered as the target attribute. Figure 4 shows the performance  of the 5 sampling procedures on the problem of differential rule  mining on the Yahoo! dataset. The results are very similar to  those from the previous experiments.



VI. RELATED WORK  We now compare our work with the existing work on sampling  for association rule mining, sampling for database aggregation  queries, and sampling for the deep web.

Sampling for Association Rule Mining: Sampling for frequent  itemset mining and association rule mining has been studied by  several researchers [23], [21], [11], [6]. Toivonen [23] proposed  a random sampling method to identify the association rules,  which are then further veri?ed on the entire database. Progressive  sampling [21], which is based on equivalence classes, involves  determining the required sample size for association rule mining.

FAST [11], a two-phase sampling algorithm, has been proposed  to select representative transactions, with the goal of reducing  computation cost in association rule mining.A randomized count-  ing algorithm [6] has been developed based on the Markov  chain Monte Carlo method for counting the number of frequent  itemsets.

Our work is different from these sampling methods, since  we consider the problem of association rule mining on the  deep web. Because the data records are hidden under limited  query interfaces in these systems, sampling involves very distinct  challenges.

Sampling for Aggregation Queries: Sampling algorithms have  also been studied in the context of aggregation queries on large  data bases [18], [1], [19], [25]. Approximate Pre-Aggregation  (APA) [18] was proposed to estimate aggregation queries over  categorical data utilizing precomputed statistics about the dataset.

Wu et al. [25] proposed a Bayesian method for guessing the  extreme values in a dataset based on the learned query shape  pattern and characteristics from previous workloads.

More closely to our work, Afrati et al. [1] proposed an  adaptive sampling algorithm for answering aggregation queries  on hierarchical structures. They focused on adaptively adjusting  the sample size assigned to each group based on the estimation  error in each group. Joshi et al.[19] considered the problem of    estimating the result of an aggregate query with a very low  selectivity. A principled Bayesian framework was constructed to  learn the information obtained from pilot sampling for allocating  samples to strata.

Our methods are clearly distinct for these approaches. First,  strata are built dynamically in our algorithm and the relations  between input and output attributes are learned for sampling on  output attributes. Second, the estimation accuracy and sampling  cost are optimized in our sample allocation method.

Hidden Web Sampling: There is recent research work [3],  [13], [15] on sampling from deep web, which is hidden under  simple interfaces. Dasgupta et al.[13], [15] proposed HDSampler,  a random walk scheme over the query space provided by the  interface, to select a simple random sample from hidden database.

Bar-Yossef et al.[3] proposed algorithms for sampling suggestions  using the public suggestion interface. Our algorithm is different  from their work, since our goal is sampling in the context of  particular data mining tasks. We focus on achieving high accuracy  with a low sampling cost for a speci?c task, instead of simple  random sampling.



VII. CONCLUSIONS  In this paper, we have proposed strati?cation based sampling  methods for data mining on the deep web, particularly con-  sidering association rule mining and differential rule mining.

Components of our approach include: 1) A tree model to capture  the relation between input attributes and output attributes of the  deep web data source, 2) A recursive strati?cation method to  maximally reduce an integrated cost metric that combines esti-  mation variance and sampling cost, and 3) An optimized sample  allocation method that takes into account both the estimation error  and the sampling costs.

Our experiments show that compared with simple random  sampling, our methods have higher sampling accuracy and lower  sampling cost. Moreover, our approach allows user to reduce  sampling costs by trading-off a fraction of estimation error.

6DPSOLQJ?9DULDQFH  ?  ???  ???  ???  ???    ?  ???  ???  ? ? ? ? ? ? ? ? ? ??  .

(V  WL  PD  WL  RQ  ?R  I?  9D  UL  DQ  FH  )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (a) Variance of Es-  timation  6DPSOLQJ?&RVW  ?  ???  ???  ???  ???  ?  ???  ? ? ? ? ? ? ? ? ? ??  .

6D  PS  OL  QJ  ?&  RV  W )XOOB9DU  9DU?  9DU?    9DU?  5DQG  (b) Sampling Cost  6DPSOLQJ?$FFXUDF\  ?  ???  ???  ???  ???  ???  ? ? ? ? ? ? ? ? ? ??  .

$(   )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (c) Sampling Accuracy  Fig. 3. Evaluation of Sampling Methods for Differential Rule Mining on the US Census Dataset  6DPSOLQJ?9DULDQFH  ?  ???  ?  ???  ?  ???  ?  ???  ?  ???  ? ? ? ? ? ? ? ? ? ??  .

9D  UL  DQ  FH  ?R  I?  (V  WL    PD  WL  RQ  )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (a) Variance of Es-  timation  6DPSOLQJ?&RVW  ?  ???  ???  ???  ???  ?  ???  ? ? ? ? ? ? ? ? ? ??  .

6D  PS  OL  QJ  ?&  RV  W  )XOOB9DU  9DU?  9DU?  9DU?  5DQG  (b) Sampling Cost  6DPSOLQJ?$FFXUDF\  ?  ????  ????  ????  ????  ????  ????  ????    ????  ? ? ? ? ? ? ? ? ? ??  .

