<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Dependence  Among Terms in Vector Space Model

Abstract The vector space model is a mathematical-based model that represents terms, documents and queries by vectors and provides a ranking. In this model, the subspace of interest is formed by a set of pairwaise orthogonal term vectors, indicating that terms are mutually independent.

However, this is a simplification that doesn't correspond to the reality. Based on this scenery, we present, in this work, an extension to the vector space model to take into account the correlation between terms. In the proposed model, term vectors are rotated in space geometrically reflecting the dependence semantics among terms. We rotate terms based on a data mining technique called association rules. The retrieval effectiveness of the proposed model is evaluated and the results shows that our model improves in average precision, relative to the standard vector space model, for all collections evaluated, leading to a gain up to 31%.

1. Introduction In information retrieval (IR), the vector space model is the most popular [15,16], however, it presents disadvantages [5]. The documents are represented by keywords extracted from the documents, and the relationship among them is not considered. This is a simplification of the model that doesn't correspond to the reality.

Based on this scenery, the main objective of this work is incorporate information of correlation among terms in the collection to the vector space model to improve it retrieval effectiveness. The proposed solution alters the representation of term vectors in the vector space model.

In this model, orthogonal vectors represent terms since it is not known a priori any correlation among the terms.

The algorithm proposed in this work has as main foundation the rotation of those vectors in the space, so that its representations reflect the dependence among the terms.

In the set of resultant vectors, term vectors are not necessarily orthogonal among themselves, the proximity between the vectors is related with the degree of dependence between the respective terms. The closer the term vectors, greater dependence observed among themselves.

The rotation of term vectors is based on techniques that result in information on the relationship among terms of the collection. We presented the technique association rules of data mining to obtain that information.

The remaining of this paper is organized as follow. In the next section we discuss some related work. Section three describes foundations of vector space model. In section four we present the association rules in the information retrieval context. The proposed model is described in section five. The experimental results are discussed in section six. Finally, we present some conclusions and future works.

2. Related Work Several approaches for the incorporation of correlation among terms had already been presented in the literature.

Query expansion in the vector space model is suggested in several proposals, among them, [6,10,11,18]. In [18], Voorhees examined the usefulness of lexical query    Voorhees examined the usefulness of lexical query expansion in the collection TREC. Mandala et al. [10] analyzed the characteristics of different thesaurus types and proposed a method to combine them and to expand queries.

In [4], Becker and Kuropka expose a model of IR for the comparison of documents that represents topics, terms and documents as vectors. The basis of the space is formed by a set of orthogonal topic vectors, where term vectors are represented. The angle between the term vectors and the weight of the term is calculated using information about the collection, as for instance, a list of radicals of the terms collection.

An extension to the Vector space model was suggested by Possas et al. in [12,13,14] considering the correlation among the terms, obtained using association rules. In [14], a new model is presented, denominated set-based model, for computing term weights, based on set theory, and for ranking documents. For computing those weights, the theory of the association rules is used. The proposal Proceedings of the International Database Engineering and Applications Symposium (IDEAS?04) 1098-8068/04 $20.00 ? 2004 IEEE presented by the authors in [13] is similar with [14].

Then, in [12], an extension to the set-based model is proposed using information about proximity among the terms of the query in the documents.

The generalized vector space model (GVSM) is another extension of the vector space model, that contemplates the correlation among terms [19,20]. In GVSM, the terms can be non-orthogonal and are represented by smaller components denoted minterms.

The minterms are vectors, with binary weights, that indicate all co-occurrence possibilities of terms in documents. The basis for GVSM is formed by a set of 2t (t is the number of distinct terms in the collection) minterms vectors. The term vectors are linear combinations of minterms, reflecting co-occurrence proceeding from minterms.

Our work differs from the above related works in the following aspects. In none of the cited works, the term vectors are rotated in the space to reflect their correlation as we made in this work.  Moreover, the association rules are used to determine the proximity between term vectors, differing from the cited models.

3. Vector Space Model The Vector space model was, initially, proposed by Gerard Salton [15,16]. In this model, all relevant objects for a information retrieval system are represented as vectors: terms, documents and queries.

Each term ki is represented as a t-dimensional vector, where t is the number of distinct terms in the collection.

In the vector space model, the vector ki represents the term ki. The set  {k1, k2,..., kt} forms the canonical basis for space ?t where k1=(1,0,0,...,0), k2=(0,1,0,...,0), ?, kt=(0,0,0,..,1). The term vectors are pairwaise orthogonal and, in consequence, the corresponding terms are considered independent.

Document and query vectors are represented using the set K of term vectors. These vectors are built as linear combinations of the term vectors. The vector dj  associated with the document dj  is defined as dj=?  t i 1 wi,j ki or dj=(w1,j , w2,j ,...,  wt,j).Similarly, the vector for query q is defined as q = ?  t i 1 wi,q ki or q = (w1,q ,w2,q ,...,  wt,q).

wi,q ki or q = (w1,q ,w2,q ,...,  wt,q).

In the equalities above, wi,j and wi,q are weights of term i in document dj and in query q, respectively. The most efficient definition of term weights for the information retrieval is named tf-idf [3]. This strategy considers the number of times an index term occurs in a document and the number of documents of the collection an index term occurs.

The vector space model evaluates the degree of similarity of the document dj in relation to the query q as the correlation between the vectors dj and q. Usually, that correlation is quantified by the cosine of the angle among those two vectors. That is, sim(dj, q) = dj x q   =          ?ti=1wi,j . wi,q |dj|x|q|    ?? ti=1 wi,j2 ? ? ti=1 wi,q2 After the computation of the similarity degrees, it is possible to order a list of documents (ranking) and their respective degrees of relevance to the query.

4. Association Rules in Information Retrieval In the area of data mining, the association rules serve, typically, to represent discovered frequent patterns in the data [1,2,8]. The main function of the rules is to characterize the data, representing regularities.

One of the purposes of this work is to use the data mining in IR. In general, the literature regarding the data mining works with items and transactions. However, the algorithms used for the discovery of association rules can be adapted also to work with terms and documents, identifying the co-occurrence among terms.

In IR context, X and Y are terms or sets of terms.

Consider the following example, which defines an association rule in IR. The information that documents whose theme is tourism also discuss about hotels is represented in the association rule (1) below: tourism ? hotel [support = 2% , confidence = 80%]  (1) The support and the confidence of a rule are two measures that reflect, respectively, the usefulness and the certainty of the discovered rules. The support is a percentage in relation to all the collection of analyzed documents. In the example above, in 2% of the collection, the words ?tourism? and ?hotel? appear simultaneously in the same document. The confidence is a percentage in relation to an attribute. A confidence of 80% reveals that 80% of the documents that discuss tourism also talk about hotels.

4.1. Basic Concepts Let J = {k1,k2,...km} be the set of distinct terms in a collection of documents D. Each document dj of the database is a set of terms such that dj ? J. An association rule is an implication like A?B, where A ? J, B ? J, and A ? B = ?. The rule A?B is valid in a set of documents D with support s, if s is the percentage of documents in D which contains A and B at the same time. The rule A?B has confidence c in the set of documents D if c is the  percentage of documents in D having A that also contains Proceedings of the International Database Engineering and Applications Symposium (IDEAS?04) 1098-8068/04 $20.00 ? 2004 IEEE B. Rules that satisfy a minimum support (min_sup) and a minimum confidence (min_conf) are denominated strong.

A set of terms is referred as termset. A termset that contains k terms is a k-termset.

Association rules are discovered in large databases in two steps: 1 ? Find all the sets of terms (termsets) that satisfy the minimum support. These termsets are denominated frequent termsets; 2 ? Generate strong association rules of the frequent termsets.

Apriori is an algorithm for mining frequent termsets for association rules [2,3,12]. Apriori uses an iterative    for association rules [2,3,12]. Apriori uses an iterative approach known as search in levels, where k-termsets are used to explore (k+1)-termsets. First, the set of 1-termsets frequent is found. This set is denoted L1. L1 is used to find L2, the set of frequent 2-termsets, which is used to find L3, and so on, until no more frequent k-termset can be found.

Once generated the frequent termsets of the transactions in the database D, the confidence of these termsets are computed and the strong association rules are generated.

In the next section, we present how association rules modify the vector space model.

5. Vector Space Model Modified by Association Rules The mais foundation of the algorithm proposed in this work is the rotation of term vectors in the space, so that their representations contemplates, geometrically, the semantics of correlation of terms adopted. We used the association rules as a tool for the generation of information about the dependence among the terms.

The association rules are of the form ki o kj  and cij is the confidence index of the rule, that indicates the degree of dependence of the term ki in relation to the term kj.

That index is used, in this work, to compute the new angle between the term vectors ki and kj. The confidence was chosen as parameter to determine the proximity of the term vectors, because it reflects the certainty of the association rule. The term vectors are approximated, using the association rules created for the respective terms as it follows.

Definition 5.1 (Rotation of basis vectors): Let ki and kj be term vectors, cij the confidence index of the association rule ki o kj. The new angle Tij between ki and kj is given by  Tij = 90 (1 ? cij), where 90 is the original angle between the vectors ki and kj. In this case, the rotation occur only in the vector ki.

The reason for it is related to the semantics of the association rule and the confidence. The index cij of the association rule ki o kj determines that, in c% times the term ki appears, the term kj also appears. Therefore, the rotation is made in the vector corresponding to the term of the antecedent of the association rule.

The vector ki approximates the vector kj, and the new vector is denominated ki?, in that the r th element of the vector ki?, named ar, is defined as: ar = sin(Tij) ? r = i ar = cos(Tij) ? r = j ar = 0            ? r z i and r z j  In case a term kp presents two or more associated terms, a normalization is made in the new vector kp?.

The vector space basis K is formed by the term vectors set {k1, k2,..., kt}. After the rotation of the term vectors, the new basis for the vector space, denominated K?, is obtained from K, substituting the vectors ki for ki?, so K? = {k1?, k2?,..., kt?}. The set K? continues forming the basis of the vector space ?t because their vectors are linearly independent.

The document and query vectors, dj and q, are represented in the new basis K? as linear combination of terms vectors ki?. Document and query vectors are denoted dj? and q? and defined as dj?=?  t i 1 wij ki? and q?=?  t i 1    i 1 wiq ki?.

So, document and query vectors, dj? and q?, contemplate, now, the dependence semantics among the terms, implicit in basis K?.

The same function in the computing of the similarity is used in the vector space model modified by dependence among the terms.  Therefore, we have, sim(dj, q) =  dj? x q? =   ?ti,s=1wi,j ki?. ws,q ks? |dj| x |q|   ?? ti=1 wi,j2 ? ? ts=1 ws,q2  The normalization of the similarity, or the factors in the denominator of the formula, is made using the original norm of the documents. That strategy was adopted because otherwise, if the normalization uses the document vectors dj?, the norm of all the documents should be recalculated, elevating the computational costs of calculation the similarity. Besides, that simplification doesn't change the results significantly.

In the computation of the similarity between the query and the documents, the main consequence in term vectors rotation is the automatic query expansion. The query is expanded with terms related to their original terms.

Besides, documents which have query terms and associated query terms occupy a position in the ranking above the documents that only have query terms.

Proceedings of the International Database Engineering and Applications Symposium (IDEAS?04) 1098-8068/04 $20.00 ? 2004 IEEE Table 1. Characteristics of the reference collections Reference collections Number of distinct terms Number of documents Average number of terms per document Number of queries Average  number of terms per query Average relevante documents per query CFC 2105 1239 12,2 64 4,0 39 CACM 8716 1602 46,6 50 12,7 13 CISI 9728 1460 53,6 50 9,4 50 TREC-3 1749555 741855 301,1 50 18,58 106,38 5.1. Algorithm The implementation of the presented model is divided in two phases. The first one is the generation of the information on dependence among the terms, which means the construction of vectors ki?. This task is all accomplished in pre-processing phase. The second phase is the development of the proposed model.

The search algorithm used in the implementation of the vector space model modified by dependence among the terms, described in Figure 1, is similar to the original model. The function value(ki , i) returns the value stored in the position i in the vector of the term ki.

(1) Create and initialize a structure of accumulators (A) (2) For each query term ki, add to the query all the terms associated.

associated.

(3) For each term ki of  the modified query do: (4)   For each pair [dj?, fij] in the term inverted list do: (5)      aux = wij * wiq  * (value(ki , i)) (6)      For each term kj associated to term ki do: (7)          aux=aux+ (wij*wiq *value(ki , i)*value(ki , j)) (8)       if  Aj ? A then (9)           Aj = aux (10) else (11)          Aj = Aj + aux (12)     A = A + {Aj} (13)   End For (14) End For (15) Divide each accumulator Aj to the document norm dj (16) Order the list of accumulators Aj and return the documents dj? retrieved.

Figure 1. Search algorithm for the Vector space model modified by dependence among the terms.

In the step (2), there is a difference in relation to the original algorithm. Once determined the identifiers of query terms, the associated terms to each term of the query are added to the list of query terms. This step of the algorithm defines the automatic expansion of the query with the terms related to the query terms.

The steps (5) to (8) are equal to the sum?  t si 1, wi,jki ws,qks of the equation of the internal product among the vectors dj? and q?. The step (5) corresponds to the sum for i = s. And the loop of step (6) corresponds to the other cases, when i z s. These steps are necessary because the term vectors are non-orthogonal.

When analyzing the algorithm, we clearly notice, that the proposed model is an extension to the original vector space model. That is justified because, if none association among the terms exists, the described  algorithm is equivalent to the original algorithm.

6. Experiments To evaluate the efficiency of the vector space model modified by dependence among the terms, the experiments were made with four reference collections denominated CACM [7], Cystic Fribosis (CFC) [17], CISI and Third Text Retrieval Conference (TREC-3) [9].

The collection characteristics are shown in the Table 1.

The evaluation of the IR system proposed here is related with the effectiveness of the retrieval, in other words, how much precise is the answer set returned by the system for a given query. We used the average recall versus precision curves to compare the effectiveness of the vector space model modified by dependence among terms with the one of the classic vector space model.

In the computing of the association rules, some parameters can be adjusted during the process of generation of association rules. Min_sup and min_conf are, respectively, support and confidence thresholds. We accomplished experiments and observed that min_sup should contain a low value (up to 5%) because, in general, the frequency of terms in collections is low.

Besides, in case min_sup is low, association rules, involving terms whose frequency is small in the collection of documents, are discarded. On the other hand, min_conf should contain a higher value (above 40%), because this parameter determines the approach among the vectors. In case min_conf contains a low value, term vectors that have very low co-occurrence are approximated. This harms the effectiveness of the Proceedings of the International Database Engineering and Applications Symposium (IDEAS?04) 1098-8068/04 $20.00 ? 2004 IEEE    1098-8068/04 $20.00 ? 2004 IEEE retrieval, because the system will make the expansion of the query with terms not related to query terms.

As we can see in Figures 2 to 5, the proposed model yields better precision than Vector Space Model, regardless of the collection and of the recall level.

Recall x Precision CACM 0% 20% 40% 60% 80% 0% 20% 40% 60% 80% 100% VSM MVSM Figure 2. Recall-Precision for CACM Recall x Precision CISI 0% 20% 40% 60% 80% 0% 20% 40% 60% 80% 100% VSM MVSM Figure 3. Recall-Precision for CISI Recall x Precision CFC 0% 20% 40% 60% 80% 0% 20% 40% 60% 80% 100% VSM  MVSM Figure 4. Recall-Precision for CFC Table 2 presents a summary of the results obtained, in that the averages of precision are exhibited for the two models in all collections and the gains obtained of the model proposed in relation to the original.

Recall x Precision TREC-3 0% 20% 40% 60% 80% 0% 20% 40% 60% 80% 100% VSM MVSM Figure 5. Recall-Precision for TREC-3 Table 2. Average Precision Curves and gain provided by the vector space model modified by association rules.

Average Precision (%) Collection Classic Modified Gain (%) CACM 30,03 32,08 6,83 CISI 17,64 20,09 13,89 CFC 10,05 13,24 31,74 TREC-3 12,09 14,04 16,13 The results presented for the vector space model modified by association rules are the best ones, considering the analysis of the parameters values described. Then, for maximum min_sup from 4% to 5%, and for min_conf alternating among 45% to 70%, the variation of the results is minimum in relation to the one presented. When defining the minimum confidence with a value up to 70%, few rules are generated and, consequently, the results approach more of the presented    consequently, the results approach more of the presented for the classic vector space model. The several possibilities of values of the parameters were tested, however the collections behave in a similar way in its alteration.

The experiments showed that the proposed model improves the average precision of the answer set for all collections. Besides, the obtained medium precision was not harmed by the recall increase happened when expanding the queries.

7. Conclusions In this paper, we presented an extension to the vector space model to contemplate the dependence among the terms of the collection. In the proposed model, the dependence among the terms is represented geometrically in the vector space.

The proposed model bases on the rotation of the term vectors, in agreement with the dependence among the terms. This rotation is made based on techniques that generate information on the correlation among terms of Proceedings of the International Database Engineering and Applications Symposium (IDEAS?04) 1098-8068/04 $20.00 ? 2004 IEEE the collection. In this work, we used the association rules, however, other techniques can be used.

The generation of association rules is a known technique of data mining, which allows finding frequent patterns in large databases. In the context of this paper, it is used to find sets of terms that appear simultaneously in the collection of documents. This information is  useful to modify the term vectors, so that they reflect the semantics of co-occurrence defined for the association rules.

The extension to the vector space model we presented contemplates the dependence among terms in a clear, flexible and new way. It is clear because the dependence incorporation among the terms is made step by step and the vector space basis reflects the semantics defined for the adopted technique. The proposed model is flexible because it allows the correlation incorporation among the terms of collection obtained in several ways.

Finally, the proposal is new because in the literature there is not an extension to the vector space model which modifies the vector space basis how it was done in this work.

We evaluated the effectiveness of the model proposed with four reference collections. There was an increase in the retrieval model effectiveness in comparison with the classic vector space model for all of the reference collections used.

As future works, the effectiveness of the proposed model will be compared to the effectiveness of the generalized vector space model. Besides, we will research other methods of obtaining correlation among the terms of a collection of documents. These methods will be incorporated in a geometric way to the model proposed in this paper. We also intended to evaluate the model proposed for larger collections formed by Web documents.

