Effective Pruning Strategies for Sequential Pattern Mining

Abstract? In this paper, we systematically explore the search space of frequent sequence mining and present two novel pruning strategies, SEP (Sequence Extension Pruning) and IEP (Item Extension Pruning), which can be used in all Apriori-like sequence mining algorithms or lattice-theoretic approaches. With a little more memory overhead, proposed pruning strategies can prune invalidated search space and decrease the total cost of frequency counting effectively. For effectiveness testing reason, we optimize SPAM [2] and present the improved algorithm, SPAMSEPIEP, which uses SEP and IEP to prune the search space by sharing the frequent 2- sequences lists. A set of comprehensive performance experi- ments study shows that SPAMSEPIEP outperforms SPAM by a factor of 10 on small datasets and better than 30% to 50% on reasonably large dataset.



I. INTRODUCTION  Frequent sequence mining is a task of discovering frequent patterns shared across time among a large database objects [1]. It has attracted considerable attention from database practitioners and researches because of its broad applications in many areas such as analysis of sales data, discovering of Web access patterns, extraction of Motifs from DNA sequence, etc.

In the last decade, a number of algorithms have been proposed to deal with the problem of mining sequential patterns in sequence databases. Most of them are Apriori-like algorithms which utilize a bottom-up candidate generation- and-test method. Unlike frequent itemset mining, the order of items in a sequence is very important for mining sequential patterns. Due to this difference, the task of discovering all frequent sequences in large database is quite challenging. For example, with n different items there are O(nk) potentially frequent sequences of length k. Can we find some effective pruning strategies that help algorithms to generate as fewer candidates as possible? This is the motivation of this paper.

In this paper, we systematically explore the search space of frequent sequence mining and present two novel prun- ing strategies, SEP (Sequence Extension Pruning) and IEP (Item Extension Pruning), which can be used in all Apriori-like sequence mining algorithms or lattice-theoretic approaches. With a little more memory overhead, proposed strategies can prune invalidated search space effectively. For effectiveness testing reason, we optimize SPAM [2] by pro- posed pruning strategies and present the improved algorithm, SPAMSEPIEP, which adopts a two steps mining process.

In the first step, SPAMSEPIEP generates all frequent 2- sequences. In the second step, SPAMSEPIEP uses SEP  and IEP to prune the search space of SPAM by sharing the frequent 2-sequences lists. A comprehensive performance study shows that SPAMSEPIEP outperforms SPAM by a factor of 10 on small datasets and better than 30% to 50% for reasonably large dataset.

The rest of the paper is organized as follows. Section 2 introduces the basic concepts related to the sequence mining problem. Section 3 discusses the related works. The proposed pruning strategies are discussed in section 4. A comprehensive experimental study is presented in Section 5.

Finally, conclusions can found in section 6.



II. PROBLEM STATEMENT  Let I = {i1, i2, . . . , im} be a set of m distinct items comprising the alphabet. An itemset e = {i1, i2, . . . , ik} is a non-empty unordered collection of items. Without loss of generality, we assume that items of an itemset are sorted in lexicographic order and denoted as (i1, i2, . . . , ik). A sequence s is an ordered list of itemsets, denoted as (e1 ? e2 ? . . .? en), where ei is an itemset.

The number of instances of items in a sequence is called the length of sequence. Let |ei| refer to the number of items in itemset ei, a sequence with length l is called l-sequence, where l =  ? |ei| and 1 ? i ? n. For example, (a? ab?  a) is a 4-sequence.

The size of a sequence, denoted as size(s), is the number  of events that it contains. For example, for s = (e1 ? e2 ? . . .? em), size of s is m.

A sequence s1 = (a1 ? a2 ? . . . ? am) is said to be contained in another sequence s2 = (b1 ? b2 ? . . .? bn) if and only if ?i1, i2, . . . , im, such that 1 ? i1 < i2 < . . . < im ? n, and a1 ? b1, a2 ? b2, . . ., am ? bm. If s1 is contained in s2, s1 is a subsequence of s2. This relationship is denoted by s1 ? s1. For example, the sequence (a? c) is a subsequence of (ab? cd).

The database D for sequence mining consists of a collec- tion of input-sequences. Each input-sequence has a unique identifier called sequence-id (sid) and each itemset also have an unique identifier called itemset-id (eid). Given a sequence database D, the support of a sequence s, denoted as ?(s,D), is the fraction of sequences in D that contain s. Given a user-specified threshold min sup, we say that a sequence s is frequent if support(s) is greater than or equal to min sup.

The problem of sequence mining is to find all the frequent sequences in the database.

2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.22   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.22   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.22   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.22   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.22     As an example, consider the database shown in Fig.1 which has five items (a to e) and five input-sequences. The figure also shows all frequent sequences with a min sup of 40%. In this example database D has 22 frequent sequences.

Fig. 1. Frequent sequence mining example.



III. RELATED WORKS  The sequential pattern mining problem was first proposed in [1] and three mining algorithms, AprioriSome, AprioriAll and DynamicSome were presented. All of these algorithms adopted a similar process: litemsets, transformation and sequence stage. AprioriAll performs better than the other two as the literature reported. In subsequent work, GSP [6] algorithm was proposed by the same authors. GSP is a multi-phase iterative algorithm and requires multiple passes of database scanning. Independently, by introducing complex data structure, several more mining algorithms were proposed, such as PLWAP [3], SPADE [8], SPAM [2], SE- QUEST [7], PrefixSpan [4], etc. PLWAP is based on the con- cept of WAPTree and has a desirable scalability. SPADE uses a vertical id-list structure and a lattice-theoretic approach to decompose the original search space into smaller oness.

SPAM employs vertical bitmap for frequency counting and depth-first tree traversal strategy for candidate generation.

SEQUEST generates candidate sequences efficiently based upon a DMA-Strips structure. Among these algorithms, SPAM is to our best of knowledge the fastest sequential patterns mining algorithm.

Several search space pruning strategies were proposed in algorithms discussed above. For example, in SPADE [8], before generating the id-list for a k-sequence, all its (k? 1) subsequences must be frequent. Based on Apriori principle, full pruning [7], s-step pruning and i-step pruning [2] are used to trim invalid computing or searching. Unfortunately, unlike our pruning strategies, these strategies are based on their own algorithm and data structure and can not be shared by other algorithms.

A. The SPAM algorithm  In this paper, we will take SPAM as an example for demonstrating the proposed pruning strategies and com- paring performance. This section describes SPAM in more details.

1) SPAM algorithm SPAM algorithm is based on the lexicographic sequence tree and can make either depth or width first traversal. The pseudo-code of its depth first traversal version is shown in  Fig.2. Taking current frequent sub-sequent node n = (s1 ? . . .? sk), s-step extension candidate items list Sn and i-step extension candidate items list In as input parameters, SPAM generates a new sub-sequence by either s-step or i-step. And this process recursively goes on until no more extension can be performed 2) s-step and i-step pruning  Fig. 2. Pseudo-code of SPAM algorithm.

s-step pruning: Let a and b are two s-step candidate items for sequence s, and candidates (s ? a) is frequent while (s ? b) is infrequent. By the Apriori principle, both (s ? a ? b) and (s ? ab) must not be frequent, thus both of them can be pruned.

i-step pruning: Let a and b are two i-step candidate items for sequence (s ? i1i2 . . . in), and (s ? i1i2 . . . ina) is frequent while (s ? i1i2 . . . inb) is not. By the Apriori principle, (s? i1i2 . . . inab) must not be frequent and can be pruned.



IV. PROPOSED PRUNING STRATEGIES A. The lexicographic sequence tree  The lexicographic subset tree T is presented originally by Rymon [5] and adopted to describe the itemset lattice in most of well-known frequent itemset mining algorithms such as MAFIA, CHARM. This approach is extended to describe the framework of sequence lattice in SPADE[8] and SPAM [2]. Let s1 and s2 are two sequences and s1 is a subsequence of s2, then s2 is a descendant node of s1. By this way, all sequences can be arranged in a lexicographic sequence tree whose root is null. Each lower level k in tree contains all of k-sequences which are ordered lexicographically. Each node is recursively generated from its parent node by using one s-extension step or i-extension step.

B. Candidate item sets for extensions  A sequence may be generated in many ways by s- extension or i-extension. For example, sequence (a ? ab) can either be generated from (a ? a) or (a ? b), and the two according paths are ? ? (a) ? (a ? a) ? (a ? ab) and ? ? (a) ? (a ? b) ? (a ? ab). In order to void sequence duplicated generation, each node n in the tree can be associated with two sets, denoted as CSn and CIn. CSn and CIn are the set of candidate items that can be used to generate next level of sequences of n by s-extension, i- extension respectively. For example, let the CIn of sequence (a ? b) be ?, then (a ? ab) can only be generated from (a? a).

In the Apriori-like sequence mining algorithms or lattice- theoretic approaches, if the candidate sets CSn or CIn may be reduced, i.e. the search space is reduced, thus the performance of these algorithms can be effectively improved.

C. Proposed pruning strategies  LEMMA 1: Given a frequent sequence s and its s- extension candidate set CSn. For each pair of items a, b in CSn, if (a ? b) is infrequent, then b must not be an s-extension item for sequence (s? a).

Proof: Suppose b is an s-extension item for sequence (s ? a), i.e. (s ? a ? . . . ? b) is a frequent sequence.

According to Apriori Principle, each sub-sequence of (s ? a ? . . . ? b) must be frequent. Then (a ? b) is frequent.

This conflict with that (a? b) is infrequent. Thus, b is not an s-extension item for sequence (s? a) and can be pruned.

LEMMA 2: Given a frequent sequence s = (s? ? is1is2 . . . isn) and its i-extension candidate set CIn. For each pair of items a, b in CIn, if (ab) is an infrequent sequence, then b must not be an i-extension item for sequence (s? ? is1is2 . . . isna).

Proof: Suppose b is an i-extension item for sequence (s? ? is1is2 . . . isna), i.e. (s  ? ? is1is2 . . . isna . . . b) is a frequent sequence. According to Apriori Principle, each sub- sequence of (s? ? is1is2 . . . isna . . . b) must be frequent.

Then (ab) is frequent, too. This conflict with that (ab) is infrequent. Thus, b is not a i-extension item for sequence (s? ? is1is2 . . . isna)and can be pruned.

Suppose there are two sequence lists, denoted as S? list, I ? list, which contains all frequent 2-sequences with size of 2 and size of 1 respectively. By sharing S ? list and I ? list, we have pruning strategies SEP and IEP , which are as follows.

Sequence extension Pruning (abbr. SEP ): Given a fre- quent sequence s and its according s-extension candidate set CSn, and s1 is a not null subset of CSn. Let SEP (s1) =? {b|a ? s1 and (a? b) ? S ? list}, according to Lemma  1, the new s-extension candidate set of sequence (s ? s1) is C?Sn = CSn  ? SEP (s1).

Item extension Pruning (abbr. IEP ): Given a frequent sequence s = (s? ? is1is2 . . . isn) and its according i- extension candidate item set CIn, and i1 is a not null subset of CIn. Let IEP (i1) =  ? {b|a ? i1 and (ab) ? I ? list},  according to Lemma 2, the new i-extension candidate set for sequence (s? ? is1is2 . . . isni1) is C  ? In = ISn  ? IEP (i1).

Fig. 3. Pseudo-code of SEP and IEP pruning algorithms.

D. Example Considering the database D and minimal support threshold  in Fig. 1, the S ? list of database is {a ? b, a ? c, a ? d, a ? e, b ? c, b ? d, b ? e, c ? d, c ? e, d ? e} and the I ? list is {bc}. Suppose we are at node (a ? b) in the tree, the node?s CSn is {c, d, e} and CIn = {c}. Without pruning, the possible s-extended sequences are (a? b? a), (a? b? b), (a? b? c), (a? b? d) and (a? b? e).

Because (b ? a) and (b ? b) are not in S-list, sequence (a ? b ? a) and (a ? b ? b) must be infrequent. Item a and b should be trimmed off from node?s CSn. Hence, we do not have to perform these s-extensions. Similarly, item d and e should be trimmed off from node?s CIn.

E. SPAMSEPIEP: Using SEP and IEP to optimize SPAM  In order to test effectiveness of SEP and IEP , we optimize SPAM by using proposed pruning strategies and present the improved algorithm, SPAMSEPIEP, shown in Fig.4. The mining process is divided into two steps. In the first step, SPAMSEPIEP generates frequent 2-sequences and stores them in S ? list and I ? list respectively. In the second step, the algorithm recursively traverses the search space in a depth-first manner. At each node, SEP and IEP are used to trim node?s s-extension candidate set CSn and i-extension candidate set CIn. The rest part of algorithms is as same as that in SPAM.

Fig. 4. Pseudo-code of SPAMSEPIEP.



V. EXPERIMENTAL EVALUATION  To test the performance improvement of SEP and IEP strategies, an extensive set of experiments were performed upon an Intel Pentium 4 CPU 1.7GHz PC with 512MB main memory, running Microsoft Windows 2003 server. Source code of SPAM was downloaded from the (http://himalaya- tools.sourceforge.net/) and modified with proposed strate- gies. Same as SPAM, all synthetic datasets are generated by using the IBM AssocGen program [1].

A. Performance comparison with SPAM  Firstly, a set of experiments were performed for study- ing the performance of the SPAMSEPIEP by compare it with SPAM. The generated datasets include one small database (Fig.5(a)), two medium-sized ones (Fig.5(b) and (c)) and a large one (Fig.5(d)). The experiments show that SPAMSEPIEP outperforms SPAM by a factor of about 10 on small dataset. Although, there is no distinct magnitude difference between SPAMSEPIEP and SPAM on reason- ably large dataset, the running time of SPAMSEPIEP is decreased by 30% to 50% than that of SPAM.

In small sequence databases, frequent 2-sequences are only a small portion in all 2-sequences. As SPAMSEPIEP traverses the lexicographic sequence tree, a mass of infre- quent extensions are pruned. On the other side, in large database, a majority of 2-sequences are frequent ones. When SPAMSEPIEP traverses the lexicographic sequence tree, only few branches are pruned. This is the primary reason why SEP and IEP strategies perform effectively in small database while not so effectively in large ones.

Secondly, we study the scale-up performance of  Fig. 5. Performance comparison: Varying support for synthetic datasets.

SPAMSEPIEP as several parameters in dataset generation were varied and min sup is kept fixed. For each test, only one parameter was varied and others were kept fixed. The pa- rameters that we varied were number of customers, average transactions per customer, average items per transaction and average length of maximal pattern. The results are shown in Fig.6. It can be easily observed that: (1) the proposed prune strategies can effectively improve the performance of SPAM.

(2) The trend of curves of SPAMSEPIEP is same as that of SPAM, so the proposed prune strategies are independent  of SPAM and can be used for other algorithms.

Fig. 6. Performance comparison: Varying datasets generation parameters.



VI. CONCLUSION  In this paper, by systematically exploring the search space of frequent sequence mining, we propose two novel pruning strategies, SEP and IEP , which can be used in all Apriori-like sequence mining algorithms or lattice- theoretic approaches. With a little more memory overhead, proposed pruning strategies can prune invalid search space.

These strategies are independent of underlying enumerat- ing methods and data structures of sequence mining al- gorithms, thus can be shared among different algorithms.

For effectiveness testing reason, we optimize SPAM, one of the fastest algorithms for sequential pattern mining, by using proposed pruning strategies and present the improved algorithm, SPAMSEPIEP. The experimental results show that SPAMSEPIEP outperforms SPAM on every dataset.

