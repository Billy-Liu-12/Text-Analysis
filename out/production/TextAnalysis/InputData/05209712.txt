Web Document Clustering Research Based on  Granular Computing

Abstract-- In this paper, a method of web document clustering based on granular computing (WDCGrc) is presented. The method computes the weight value of the words in documents by adopting the TF-IDF principle. Meanwhile, combinative ways defining documents threshold and average weight value are adopted to reduce dimensions and extract the keywords in each document. The paper establishes the transformation between the keywords in documents and the binary granules, and adopts the algorithm of association rules based on granular computing to obtain frequent itemsets between documents. Bring in the set theory thought, numbers of the same word between documents as the document similarity and the clustering result is obtained. The experiment shows that the method is practical and feasible, with good quality of clustering.

Keywords- Granularcomputing; Clustering; Association rules; Web documents  I INTRODUCTION Along with the popularization and the application of the  Internet around the world, the web data mining technology has already become a hot research topic, having broad prospect in the application. The web documents have mass of information data and numerous and diverse contents and structures. Moreover, it is in changes unceasingly. So we need to carry out the clustering to these documents in order to effectively and fully use these rich documents resources to speed up the retrieval speed and to increase the inquiry precision.

Document clustering divides document set into a number of bunches, requiring the similarities as many as possible if documents are within the same bunch but it is the other way around if they are in different bunches. A method of Web document clustering has two categories based on respectively probability and distance. The former is on the basis of Bayesian probability, using Probability distribution to describe clustering result but with poor precision and efficiency; while category based on distance is mainly K-means and nearest neighbor and so on. This method is direct-viewing, but when clustering data has higher dimensions, its quality and performance apparently declines.

Granular computing [1] was first put forward by San Jose State, Professor of American in 1997, symbolizing arising of applied research areas involving a  multi-disciplinary. Granular computing is a new conception and computing paradigm of information processing, covering all researches related to theories, methods, and technologies of granular [2]. It is a very important tool for studying fuzzy, imprecise, incomplete and mass of information processing.

T.Y.Lin presents expressing granular computing by using neighborhood [3], defining binary neighborhood system: Supposes a binary system to relate the object collection which R, R ? V?U, V is an object set, U is the data set, to ? p?V, NP={u|pRu}, called the mapping: P->NP or {NP} is a binary neighborhood system. It is an expansion based on Rough set division theory. The neighborhood system has the widespread application in the data mining. In 1998, T.Y.Lin emphatically elaborated the expression of the binary neighborhood system [4]. In this article, the author expresses the binary relations with the table form which is called the expansion of information table. Thus, information table processing will expand for the processing of binary relations granular structure.

II WEB DOCUMENT CLUSTERING BASED ON GRANULAR COMPUTING (WDCGRC)  A.  Web document preprocessing In order to enable our model to have the versatility, we  cannot directly use web pages on the Internet but must preprocess the model and the step is as follows:  Download web pages on the Internet and classify them.

Transform web page files into text files. Meanwhile,  remove TAG in web page files.

Extract documents content and neglect documents  structure and organization.

Delete non-using words, remove high frequency words,  process etyma of substantive words, extract stems, eliminate redundant words and establish word list.

B.  Document after preprocessing generates weight value TF-IDF principle [5], a weight value formula, in this article which is simple on the surface but difficult to realize, having the important application in the information extraction.

Here two steps are used: We use the improved TF-IDF  2009 Second International Symposium on Electronic Commerce and Security  DOI 10.1109/ISECS.2009.16         principle [6]to compute the weight value: wij=(coefij)?[logN-logdfj+1]              (1)  in which: 1       if   tfij=1  coefij=       1.5      if   1<tfij?5       (2) 2       if   5<tfij?10  2.5      if   tfij>10 N is total number of documents, dfj indicates document frequency that j happens at least once; tfij means the frequency that the jth lemma appears in the ith document and transforms lemma into weight value.

According to the vector space model, we can express documents as the form of eigenvector V(d) = (<t1,w1>,<t2,w2>,<t3,w3>,?,<tn,wn>), ti is the ith lemma, wi is weight value corresponding with the ith lemma.

C.  Reducing dimension of attributes The dimension of web document has reduced by means of preprocessing. However, we have to continue reducing dimension to make it appropriate for our model and to lower complexity of computing. When extracting key words in documents, most literatures adopt the method of defining document threshold value [6], and then reduce dimension.

But only adopting this method, which requires reducing dimension of documents in a small scope, will result in smaller document threshold value interval, causing the document weight value centralized, little difference among weight values and unremarkable clustering result when clustering documents.

Our steps are as follows:  Set a big document threshold value interval and allow passing certain numbers of lemma.

On the basis of the first step, we adopt the following formula to compute average value of key words.

1 n i  i w  n ?  = = ?  ?3? n represents the total number of key words we need, wi is weight value of lemma i, and u expresses average weight value of key words we need. If weight value of a lemma is smaller than average value of this lemma, delete the lemma and retain those which have bigger weight value than average value. Thus, we can successfully reduce document dimension, ensuring that not only is number of lemma of each document limited in a small scope, but also it will be possible to not affect clustering result. After the massive experimental confirmation, it will obtain a good clustering result if key words in each document are within 10-25 lemmas.

D. Modeling of document data 1)  Establishing information system After above 3 steps, we can combine all the web documents and take it as an object set, and all the lemmas of documents after reducing dimension as an attribute set. In this way, an information system is formed S=?D,T?, in which D={d1,d2,d3,?,dn} ? T={t1,t2,t3,?,tm}. This is a  two-dimensional relational table, which is like TABLE 1: TABLE 1?CORRESPONDING WEIGHT VALUE OF LEMMAS IN DOCUMENTS  t1 t2 ?? tm d1 w11 w12 ?? w1m d2 w21 w22 ?? w2m ?? ?? ?? ?? ?? dn wn1 wn2 ?? wnm  In TABLE 1, wij is corresponding weight value of lemmas in documents. If the lemma is not in this document, make wij be 0.

2)  Establishing granular After above preprocessing, each lemma in web documents has obtained a weight value. The value at this time is equal to the support level of the lemma in documents.

Set a weight and threshold value. If it is equal or greater than the set value, make the corresponding weight value be 1, or 0. In this way, we can transform TABLE 1 to TABLE 2: TABLE 2?TRANSFORMATION FROM TABLE OF WEIGHT VALUE TO TABLE  OF BINARY GRANULAR t1 t2 ?? tm  d1 0 1 ?? 0  d2 0 1 ?? 1  ?? ?? ?? ?? ?? dn 1 0 ?? 1  Therefore, we can use CString 0, 1 to express every document, like using 01?0 to express d1. The length of sequences of CString 0, 1 is sum of numbers of lemmas.

And we can call each document a granular, the form of which is sequences of CString 0, 1. In this way, we establish granular.

3)  Generating frequent itemstes  Set support level threshold value s%, the step of binary documents granular computing frequent itemsets is: the operation di AND dj of binary granular di and dj is denoted by di?dj, and if the ratio between numbers of 1 in binary number and binary radix n in di?dj is equal or greater than s%, then we can say the association rules can be extracted, and di?dj can be expanded to intersection form of limited granular: d1?d2???dn.

Above combination may be regarded as an association rule. If the support level of the association rule is equal of greater than that of threshold value, then the combinations of granular are frequent itemsets.

Suppose the support level weight value is s%, and s is an exact figure. As long as we carry out the AND operation for two documents, association rules among web documents can be found out. And CString 0, 1 resulted from it can be carried on the AND operation with each other granular. Seeking the association rules among documents is determined by numbers of 1 in CString which has carried on AND operation among granular. If numbers of 1 is equal or greater than s, then association rules among documents can be extracted and such a combination of granular is called frequent itemsets.

As can be seen from Table 2, dimensions between         documents granular and words in fact form a matrix expressed by binary number. With words dimensions increasing, numbers of character 0 in each document granular will become more and more. So in order to accelerate speed of running programs, sparse matrix data structure is adopted to express each granular during programming in this paper.

The algorithm is as follows? G1={d1,d2,d3,?,dn|di.count>=s%,1<=i>=n}  //initialization of documents granular; Gpruning_Index(C1,i);// number each document, and  storage it in set C; for(k=2;GK?1?? ;k++)  { if (Gpruning_ Index (Ck,a b)) { da b={da AND db  da?Gk-1,db?Gk-1 && (a,  b?Ck-1, a?b)}; Gk= {da b| da b.count>=s}  //a, b is the subscript of new granular; } } ? =?KGK;  bool Gpruning_Index(Ck,a b) // Ck is global variable; { if(a b ?Ck) { Ck=Ck+{a b}; return true; } else return false; }  di.coun is ratio between numbers of 1 of di and binary radix, that is support level;  in Function Gpruning_Index(Ck, a b), a, b is subscript of new generated granular, and Ck is a set of subscript. This Function is preventing generating repeated granular. For example, there is granular d0, d1, d2 (Assume combinations of them are all equal or greater than support level) which generates respectively binomial frequent itemset d01, d02, d12, and trinomial frequent itemset d0102, d0112, d0212. And these trinomial frequent itemsets are all repeated granular. When the subscript is simplified, three granulars all can be expressed by d012. Function Gpruning_Index (Ck, a b) is preventing generating repeated granular so we can only make the comparison serial number of granular. If the serial number of potential generated granular is the same as that of existed granular, quit directly and carry out the next loop but not the operation AND, which will accelerate the running speed of programs.

4)  Clustering According to 3), we can obtain frequent itemsets among documents granular and junctions within frequent itemsets are very similar. In order to develop this property, we have a train of thought about documents granular clustering as follows: Each frequent itemset can be expressed by a set in which elements are junctions of this frequent itemset. All of these frequent itemsets form lots of sets, making up set storehouse. There are three situations about the relationship  among sets in the set storehouse. They are: ? One set is a subset of another; ? Set intersection includes many sets intersect; ? When sets separate, these sets have no  relationship with other sets; For these three situations, our steps are:  a) First, eliminate inclusion between sets, like set A is a subset of set B, delete A from set storehouse.

b) Second, as regard to the situation of set intersection, we adopt the way of computing support level of junctions to sets. If the support level of one junction to one set is the biggest, put the junction in the set.

c) After step a) and step b), we have eliminated sets which are obtained by intersection of a subset and a set, so sets in the situation of c) are all dependent, separating. In this paper, we carry out the combination with similarity among sets, obtaining clustering result.

In view of process 2) and 3), the definition of discriminant function of junctions to sets and similarity among sets are as follows: Suppose documents carry out operation AND, (at this time, documents have been expressed by binary granular):  di1?di2?di3???din is recorded?  ij  j n  j  d =  = ? ? in which, dij  expresses documents.

Definition one: contribution rate of documents granular to sets: Suppose Ci is a set, document granular d?Ci, then the definition of the contribution rate of d to Ci is:  || || 1  || || 1 || || 1  1 1  || || ( , )  || || || ||  Ci  ip  p i Ci Ci  ip ip  p p  d d d C  d d d d  ?  = ? ?  = =  ? ? =  ? + ?  ?  ? ? ,   in which, |Ci|| is numbers of document granular of set Ci?  and || ||   p Ci  ip  p  d =  = ? expresses numbers of 1 after operation AND  between d and Ci. The bigger the value is, the bigger ?  (d, Ci) is, and the bigger the contribution rate of d to Ci is.

|| || 1   | || Ci  ip  p  d d ?  =  ? ? expresses numbers of different values of corresponding binary digit between d and Ci. This formula excludes the situation that corresponding digit are  all 0 between document d and || ||   p Ci  ip  p  d =  = ?  binary CString  when we don?t know whether corresponding words are equal. Contribution rate of documents is actually cohesion of sets or intra-class.

In definition one, using set Cj to express document  (d?dip) (4)         granular, and we can obtain similarity function Sim (Cj, Ci) between set Cj and set Ci. That is:  Definition two: Similarity functions among sets Sim (Cj, Ci) =  || || || ||  1 1 || || || |||| || || ||  1 1 1 1  || ||  || || || ||  Cj Ci  jq ip  q p Cj CjCi Ci  jq ip jq ip  q p q p  d d  d d d d  = =  = = = =  ?  ? + ?  ? ?  ? ? ? ?   The bigger the value of similarity Sim is which means the more numbers of 1 obtained by AND operation between Cj and Ci is, the more similar they are. Function Sim is in fact the cohesion of intra-class.

Assume frequent itemsets obtained by 3) are as follows: ItemsG= {G1={d11,d12,..d1m1},G2={d21,d22,?,d2m2},?, Gn={dn1,dn2,..,dnmn}}?  Consider G as a set storehouse, expressed by C, and consider G1, G2, ?, Gn are sets in this set storehouse, expressed by C1,C2,..,Cn.

Algorithm of eliminating inclusion and intersection of set is as follows:  while(Cs ? ? ? Csl? ? ) (Csi?C) // Result of intersection of sets in set storehouse is not empty;  { //Eliminate the relationship of inclusion?  for each Ci?C for each  Cj?C  (i<j) if Ci ? Cj //If set C includes set Ci?delete Ci; {  delete(Ci,C);break; }  // Eliminate the relationship of intersection? for each  di  //di is any one document granular? {  di?Cp1?Cp2???Cpv  (Cpi?C); // Find out which sets di belongs to after scanning  set storehouse ? if max( ?  (di,Cpj)) //The contribution rate is the biggest of document  granular d to some certain set Cpj; { for each Cq?{Cp1,Cp2,?,Cpv } delete(di,Cq); (q?pj)  // Delete di from other sets? }  } } Combines sets which have been separated, and the  algorithm is as follows: We can get a series of separation sets from above steps,  which can be expressed: C= {C1= {}, C2= {},?, Cm={}}; //C is set storehouse and C1?C2???Cm= ?? While (m>k) { // k is numbers of waiting for clustering;  for each ?  (Ci,Cj) (Ci?C,Cj?C&&(i?j))  if (max(Sim (Ci, Cj))) // Combine the most two similar sets in set  storehouse? { Ci=Ci+Cj; // Merge set Cj into set Ci? delete(C,Cj); // Delete set Cj from set storehouse;  m=m-1; //Sets will be one less in set storehouse;  } } There exists a special situation in this algorithm that is when we eliminate the relationship of sets intersection in step b) and c), lots of documents have the same biggest contribution rate for lots of set. At this time, function estimate () is adopted in the program to judge it. And its principle is: which set has the most elements, which has the most documents granular when judging many sets, and then put document granular di into the set, delete di from other sets. When we put di into the set which has more junctions, it is regarded that the cohesion of set intra-class is bigger. As regard to step c), we can also adopt this discriminant function when combining two sets which have the biggest similarity in set storehouse. When there is the same biggest similarity between one set and other many sets in set storehouse, we can also use it to combine them.

This method brings in set theory, making full use of the property that there is a great similarity among junctions in frequent itemsets. Contribution function ?  (d, Ci) and similarity function Sim (Ci, Cj) all take into account the relationship between words in documents. Taking the number of the same word among documents as the measure of similarity of documents, the more same word the documents have, the more similar the sets are, and the easier the program.

III  EXPERIMENTAL RESULTS AND ANALYSIS First two groups of the test data set in this experiment are from standard test document set Reuters-21578, and latter two groups are from 3363 websites of business, entertainment, health, Iraq, Mideast, news, arts, finance, science, culture, education at Yahoo. Test data set is presented in TABLE 3:  TABLE 3: TEST DATA SET Data Set  D- Number  classes  Data1 480 Ship, Money-supply, Interest, Jobs  Data2 716 Acq, Trade, Crude, Money-fx, sugar  Data3 1235 Business, entertainment, health , iraq, mideast  Data4 2128 news, arts ,finance, health, science, culture, entertainment, education  Here D-Number is the number of documents.

F-Measure is used to evaluate clustering result. And the  result of total average F-Measure value and average value of execution time for testing data set for 50 times is presented in TABLE 4:  TABLE 4: F-MEASURE VALUE AND AVERAGE VALUE OF EXECUTION TIME  (5)         FOR TESTING 50 TIMES Data Set  K-means SOM WDCGrc F-measure E-Time F-measure E-Time F-measure E-Time  Data1 0.732 33 0.777 1301 0.762 701  Data2 0.690 49 0.735 2012 0.731 1061  Data3 0.653 82 0.696 3313 0.703 1502  Data4 0.598 113 0.642 4014 0.695 1678 Here E-Time is the executive time of the algorithms.

For Data1?Data2, F-Measure values are very similar computing by algorithm SOM and WDCGrc, but for Data3 ?Data4 which has bigger data set, F-Measure values are lower computing by arithmetic SOM, comparing with it by WDCGrc ; The value is low by K-means but with the fastest executive time. And the executive time of WDCGrc is between K-means and SOM.

IV  CONCLUSION AND FUTURE WORK In this paper, a method of web document clustering  based on granular computing is presented?WDCGrc. The method computes the weight value of the words in documents by adopting the TF-IDF principle and denotes each document as a space vector using the vector space model. Meanwhile, combinative ways defining documents threshold and average weight value are adopted to reduce dimensions, denoting the key words which are considered to be important for the document. In this way, the malpractice that most web documents excavation gives key words depending on the domain experts can be avoided, which has certain versatility.

This method also provides the transformation between the keywords in documents and the binary granules, and adopts the algorithm of association rules based on granular computing to obtain frequent itemsets between documents.

Bring in the set theory thought, numbers of the same word between documents as the document similarity and the clustering result is obtained. The documents fully take into account the relationship among words in documents for the function ?  (d, Ci) of the set and the function Sim (Ci, Cj) between the set and the set combination. Taking the number of the same word among documents as the measure of similarity of documents, the more same word the documents have, the more similar the sets are, and the easier the program. The experiment shows that the method is practical and feasible, with good quality of clustering.

In this paper, we establish the transformation between the keywords in documents and the binary granules. It may still lose some information, although the design is very strict.

And whether threshold value is good or bad will influence the clustering result, so our future work is if we can dynamically obtain the best threshold value.

