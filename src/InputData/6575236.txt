Terms Extraction from Unstructured Data Silos

Abstract -The major challenge that the big data era brings to the services computing landscape is debris of unstructured data. The high-dimensional data is in heterogeneous formats, schemaless, and requires multiple storage APIs is some cases. This situation has made it almost impractical to apply existing data mining techniques which are designed for schema-based data sources in a knowledge discovery in database (KDD) process. In this paper, a tool called TouchR is proposed which algorithmically relies on the Hidden Markov Model (HMM) to extract terms from data silos; specifically, distributed NoSQL databases? which we model as network graph. Our use case graph consists of storage nodes such as CouchDB, Neo4J, DynamoDB etc. The evaluation of TouchR shows high accuracy for terms extraction and organization.

Keywords: Unstructured data mining, big data, Hidden Markov Model (HMM), terms extraction, NoSQL.

1 Introduction There is a growing amount of high-dimensional data in the modern data-driven economy; a trend that stakeholders describe as ?big data? [1]. Primarily, user generated content has taken a whole new dimension where all forms of data is being archived as digital assets. To this effect, enterprises (e.g., Health, Security, Geospatial, etc) are re-aligning their services in such a way that paper-based requirements are all being digitized. There are benefits that come along with big data. Importantly, data and information accessibility is made easier and highly available for third parties at minimal or no cost. However, there is a major challenge that we face today with the enormous amount of data that we have. The challenge is that, the data is unstructured [15] since 1) it comes in heterogeneous formats (e.g., file documents, video, image, etc), 2) it is schemaless, 3) it is from multiple sources, and 4) requires different APIs to store the data.

The fact that the data is unstructured has made it difficult to perform data mining activities. Without data mining, we cannot deduce any meaning out of the high-dimensional data at our disposal. The existing data mining techniques have been designed to work with schema-oriented databases for structured data styles [2]. However, these existing techniques are no longer relevant to the modern challenges of information extraction. To enhance the data  mining process, scientist in both the academia and industry are beginning to explore the best methodologies that can aid in the unstructured data mining process. Thus, we have witnessed some methodologies such as: information retrieval algorithms based on templates [3], association rules [4-5], topic tracking and topic maps [6], term crawling (terms) [7], document clustering [8], document summarization [9], Helmholtz Search Principle [10], re- usable dictionaries [11], and so on. On the whole, most of the methodologies are based on Natural Language Processing (NLP) which is inherited from Artificial Intelligence and Linguistics [12-14].

While it appears that the issue of unstructured data mining is receiving recognition, most of the methodologies are based on single data sources; which means the ability to apply the techniques to distributed data in silos is hampered. This latter requirement is actually what most enterprises are aiming for in order to support data extraction from multiple sources. Our major goal in this research is to build a data mining tool that can extract terms from multiple sources. But, before we delve deeper into our goal, we need to understand the modern style of data storage. In an attempt to accommodate the growing high- dimensional data, NoSQL databases have been proposed and this style of database has received high enterprise adoption [20]. NoSQL databases support schemaless, semi- structured, and structured data storage while some accommodate file attachments (e.g., CouchDB). Recently, the commodity cloud providers have also deployed wide- array of NoSQL databases which either support file storage (e.g., MEGA, Dropbox, and Amazon S3, and so on) or any other form of unstructured data. Further, there is also a growing trend in deploying databases which is graph- based. The graph-based storage is changing how queries are run and it has eliminated most of the penalties of relational queries and MapReduce especially with long query durations and computational complexities. Already, we have seen the impact of graphed-based services and systems such as Facebook Open Graph [16], Google Knowledge Graph [17], Bing one-ups Knowledge Graph [18], and Twitter Interest Graph [19].

Our focus is on how terms extraction activities can be performed for multiple NoSQL sources. We built a tool called TouchR that relies on a graph methodology to perform the data extraction. This eliminates the latency cost of creating joins and MapReduce queries. Further, we       employ the Hidden Markov Model (HMM) [26, 29] algorithm to enhance the search pattern and provide user search adaptability. The employment of the HMM transforms the exponential complexity of the search space to linear complexity with high accuracy for generating terms. Currently, TouchR supports terms extraction from network of NoSQL data silos from the following providers: Bigdata, CouchDB, MongoDB, Neo4J, Cloudata, and DynamoDB.

The remaining sections of the paper are structured as follows. The next section expounds on some existing works on terms mining. Section 3 discusses the architecture of TouchR and the evaluation of the deployed architecture is carried out in Section 4. The paper concludes in Section 5 with our contribution and future direction.

2 Terms Mining For brevity, we provide Table I which gives an overview of the methodologies that have been proposed by researchers for unstructured data mining after we surveyed the existing  methodologies, tools, and applications. Our aim in this paper is to focus on terms extraction. Terms extraction aims at establishing a network of associative relationships between terms [6], [24]. The main benefit of term mining is that, it optimizes the search space vector thereby reducing processing time [21-22]. Terms can also be employed to convert concurrent keywords into a vector space [23].

Further, terms are dominant to a software process when it comes to Online Analytic Processing (OLAP) applications [7]. When studied extensively, terms can be applicable to traceability, readability of bugs, maintainability, and so on.

Since terms extraction is a daunting task, researchers only focused on applying it to single data sources or merged databases; however, we seek to apply the terms mining methodology to network of distributed NoSQL by adopting the Hidden Markov Model algorithm. Another challenge is the fact that most tools and methodologies available for terms mining are community-based; so they are not applicable across domains. We seek to overcome this limitation by providing a thesaurus for general usage.

Hence, enterprises can employ TouchR by populating the thesaurus with their specific artifacts.

TABLE I.  SUMMARIZED METHODOLOGIES OF UNSTRUCTURED DATA MINING   Information Extraction  Association Rules Topics Terms  Document Clustering  Document Summarization  Re-Usable Dictionaries  Goal Data Retrieval, KDD Process  Trend discovery in  text, document or file  Topic Recommendation  Establish associative  relationships between terms  Organize documents into sub-groups with closer identity  Noise Filtering in Large  Documents  For Knowledge  Collaboration and Sharing  Data Representation  Long or short text, semantics and ontologies,  keywords  Keywords, Long or short  sentences  Keywords, Lexical chaining Keywords  Data, Documents  Terms, Topics, Documents  Words, Sentences  Natural Language Processing  Feature extraction  Keyword extraction Lexical chaining  Lemmas, Parts-of- Speech  Feature extraction  Lexical chaining  Feature extraction  Output Documents  (Structured or semi-structured)  Visualization, Summary  report Topic linkages Visualization, crawler depth Visualization  Visualization, Summery  report  Summary report  Techniques  Community data mining,  Tagging, Normalization  of data, Tokenizing,  Entity detection  Ontologies, Semantics, Linguistics  Publish/Subscribe , Topics-  Associations- Occurrences  Linguistic Preprocessing,  Term Generation  K-means clustering,  Hierarchical clustering  Document structure analysis,  Document classification  Annotations, Tagging  Search Space Document, Files, Database Document,  Files, Database Document, Files,  Databases Topics vector  space Documents, Databases  Databases, Sets of documents  Databases, Sets of  documents  Evaluation metrics  Similarity and relatedness of Documents,  sentences and keywords  Similarity, Mapping, Co- occurrence of  features, correlations  Associations, Similarity, Sensitivity, Specificity  Frequency of terms,  Frequency variations  Sensitivity, Specificity, Balanced Iterative  Reducing and Clustering (BIRCH)  Transition and preceding  matrix, Audit trail  Word extensibility  Challenges and Future  Directions  Lack of research on  Data Pedigree  Applies varying approaches for different data  Identification of Topics of interest  can be challenging  Community based so  adoption can be challenging  Can be resource intensive  therefore needs research on  parallelization  Needs research on Data Pedigree  Lack of research on dictionary  adaptation to new words     3 TouchR Architecture/Implementation The TouchR framework as illustrated in Fig. 1 is deployed based on our vision which seeks to aid terms extraction in distributed NoSQL data silos. The implementation is done in the Erlang programming language [27] and the user interface is browser-based. We found the Erlang environment as stable and well suited for scalability and fault-tolerance in a previous study [25]. In the next section, we discuss the architectural overview of the framework.

Figure 1. Architectural design of TouchR  3.1 Architectural Flow TouchR is a standalone application that has Erlang backend and an HTML5 interface to enhance user interaction.

Currently, the user interface relies on a visualization tool called JavaScript InfoVis Toolkit [28]. The user interface (TouchR App) allows the user to specify the query terms? which are the actual artifacts that the user is interested in extracting from the data debris.

Once a query term is selected, the selection is forwarded to the semantic engine which contains two components; the thesaurus and the dictionary. As already posited, topic extraction focuses on the actual keywords being searched while terms require the extraction of other features (keyword dependencies). For instance, in an organization, topic extraction can be done by searching for the keyword ?contract?, where contract may refer to all files and information regarding contractual issues. In that case, the search will return all results containing the exact topic ?contract?. However, terms extraction for the same  keyword ?contract? will require other dependencies such as agreements, technical description documents, non- disclosure agreements, and so on. In effect, terms are specified by the user because sometimes, the expected result may not even contain the actual keyword being searched for because that keyword may not literally exist in the data sources while a synonym may exist and be returned. Hence, the thesaurus allows the user to specify the dependent terms and synonyms. For enterprise users of TouchR, we encourage the system administrators to pre- populate the thesaurus with possible terms and their dependencies before the actual mining process commences by novice users. However, the terms and their dependencies can be specified and stored in the thesaurus at the time of performing the terms extraction task. Typically, the data being sent across the system components is JSON so an example of a specified query term can appear as:  {"R Lomotey": { "project": " iOS?, ?unit2?: ?Admin Dept?, ?unit3?: ?HR Dept", } }  The thesaurus will store this information using ?R Lomotey? as the main artifact while the project and units are dependency keywords that will also be extracted as part of the terms mining process. The dictionary is implemented to provide the TouchR framework with adaptability features. Once a terms-extraction task is completed successfully, we allow the users to specify their satisfaction rate from (0 ? 5). The terms extraction is rated very poor (0) to excellent (5). Apart from the human rating, we analyze the accuracy, precision, and recall of every search at the application level. This will be discussed later in the paper. But, highly rated terms extraction tasks by the users which correspond to the systems accuracy and precision are moved into the dictionary. The user does not interact with the dictionary but only the system. So, in the future when users specify a query term such as ?R Lomotey? who is an employee but the only dependent or synonymous keywords that are specified is project and unit2 or even completely different units, the system will add to the query result unit3 or all the previous results that are returned when other users perform the query tasks and report high satisfaction. The dictionary actually stores human rated values from (3 ? 5).

At the moment, the thesaurus and dictionary are implemented using DETS which is disk storage in Erlang.

When the semantic definition tasks are completed, the query term and its dependency keywords (either specified by the user or from the dictionary) are forward to the transactor component. The transactor is the main engine that determines the number of available NoSQL databases, their different MapReduce functions, their REST API formats, and their returned results formats. The challenge we have at the moment is that, the transactor is not adaptable. So, prior to the terms extraction process, the query nature of the expected NoSQL databases has to be specified. For example, the transactor does not support     cloud storage such as MEGA or Dropbox so if these frameworks are specified as the NoSQL databases, then there will be no terms extraction. However, to use these storages, their REST APIs has to be pre-defined in the transactor; and this activity requires expertise in Erlang programming as well as API usage.

At the moment, TouchR supports the following NoSQL databases: Bigdata, CouchDB, MongoDB, Neo4J, Cloudata, DynamoDB. The list of the supported NoSQL is heterogeneous in the sense that some are the non-graph NoSQL storages and a few (e.g., Neo4J) are graph databases. So, the query engine performs the terms extraction tasks on the NoSQL using the key/value pair format which is the property of these NoSQL storages.

However, the graph databases rely on both properties and indexes to access the data on the distributed nodes. The transactor takes every query term as a key and every dependent term is also treated as a key. Hence, the transactor generates MapReduce queries with the terms to extract the data from the NoSQL sources. We consider the details of MapReduce queries of all the NoSQL being studied here outside the scope of this paper for brevity. We encourage the reader to consult the documentations on the websites of each of the products.

Further, there are some instances where file reading may be required (e.g., attachments in CouchDB). This case is simplified by just downloading the file and reading its content and treating it just as an actual data. All the returned data from the queries are sent back to the query response engine on the client side. The query response engine then displays the result on the user interface. So far, we have described the architectural flow composition, terms and their dependency composition, the query generation and query response. The next question is: how is the NoSQL traversed in a proprietary graphed network?

3.2 The Hidden Markov Model (HMM) The answer to the above question is seen in our adoption of the Hidden Markov Model (HMM). Previously, Scheffer et al. [26] attempted to use the HMM to solve textual mining tasks. From Fig. 1, we model the NoSQL databases as the nodes and the links that connect them are treated as the relationships. In graph databases, the concept is about nodes and their relationships. However, in the non-graph NoSQL databases, such relationships don?t exist across products so the relationship we portray in Fig. 1 only means that the terms we are interested in extracting exist on those data sources. For example, the node represented as N can contain the artifact ?contract? but the dependent terms may be scattered on the nodes denoted as D. This means there is a relationship between the N node and the D nodes.

By observing the NoSQL storages, it is obvious we can model the terms extraction as a state transition flow; which makes the Hidden Markov Model a perfect fit. In this case, we found the tutorial presented by Mukherjee and Mitra [29] particular useful though the authors focused on a  different application domain (Bioinformatics). We adopt the methodology the authors presented and this forms the basis and the highlight of the formal model shown in this section. All the mathematical representations are from [29] and we recommend the authors? work for further reading on understanding the HMM. The question is, how do we transition from the initial state through the other nodes until we get to the final state and return the result?

The start state can be specified by the user as any of the NoSQL nodes. Given a term A on a particular node, the conditional probability that the term or its dependency keywords B exist(s) on other nodes can be expressed as:  | , , However, the condition of the existence of terms can be independent and random which follows that:  | , Considering a sequence of reproducible terms and their dependencies, let us have a sequence of random variables A0, A1, ?, An having values in a set S = s1, s2, ?, sn. Then the sequence is Markov Chain if ?n ?1, and j0, j1, ?, jn ? S, we have:  | , ? ,  | The next step is to define an Observable Random Process (ORP) which defines a finite state space of available NoSQL nodes N = n1, ?, nK, such that the two states S and N can contain the same or different number of tuple arguments. The condition that there will be an outcome E from the data silos is then defined as:  ,, where ej(k) ? 0 and ? 1  The third property is the Conditional Independence (CI).

The CI assumes that the outcomes are conditionally independent for the NoSQL state sequence. For a sequence of states j0, j1, ?, jn, the probability of the sequence o0, o1, ?, on can be expressed as: , ? ,  | , ? , , We then define a joint probability based on the terms and states o0, o1, ?, on and j0, j1, ?, jn as:  P(B0=o0, ?, Bn=on, A0=j0, ?, An=jn; T, E, ?(0))  = P(B0, ?, Bn | A0, ?, An, E) ? P(a0, ?, An, T, ?(0)) 0     | The total sum of all possible NoSQL nodes in the entire graph network results into:     , ? , ; , , 0                                     ? 0 0 | At this point, the HMM model is exponential so we have to improve on the algorithm in order to make the model linear. This is achieved by splitting the above equation into forward/backward recursion. From the last equation above, we can re-write a new equation as:  , , ? , ; The forward variable ? is defined formally as:  0 0, 1 1, ? ,  | Consequently, the backward variable is defined as:  1 1, ? ,  | From the two variables, the recursion relations can be written as:  | | and  | Since the two recursive equations have a computational complexity of O(n2), the model in a given state i at a specified time t given a sequence of emitted terms o and a model ? can be formalized as: | , This final equation reduces the exponential complexity of traversing all the nodes in search of the terms to be extracted and the dependency keywords into a linear complexity.

4 Evaluation We deployed TouchR on our Windows powered system with the following specifications: Windows 7 System 32, 3.20 3.12 GHz, 8GB RAM, 1TB HDD. We considered training data sets which are digital archived files from our industry partners. The data is populated into our NoSQL databases which are deployed on an Amazon EC2 instance.

The thesaurus is pre-populated with approximately 18000 employee data, contractual jargons, synonyms, and their dependent keywords that have any associativity. Deploying 1 million datasets on each of our NoSQL, we report the validity test of TouchR in Table II. In this initial experiment, we performed the terms extraction from each database as a single data source. Secondly, we connect all the databases on the different Amazon EC2 instances and  we tried to extract the terms based on different number of available records as shown in Table III.

TABLE II.  INDIVIDUAL NOSQL WITH 1 MILLION RECORDS  CouchDB+ %  Neo4J* %  DynamoDB %  Cloudata %  True Positive 100.00 100.00 100.00 100.00  False Positive 0.00 0.00 0.00 0.00 True Negative 100.00 100.00 100.00 100.00 False Negative 2.00 0.00 0.00 0.00 Precision 100.00 100.00 100.00 100.00 Recall (Sensitivity)  98.04 100.00 100.00 100.00  Specificity 100.00 100.00 100.00 100.00 Accuracy 99.00 100.00 100.00 100.00 Type I Error 0.00 0.00 0.00 0.00 Type II Error 2.00 0.00 0.00 0.00 Search Time 240.77s 188.20s 193.76s 201.21s  + Similar results for MongoDB, * Similar results for Bigdata  TABLE III.  GRAPHED NOSQLWITH HMM ALGORITHM  1  Million Records  5 Million Records  8 Million Records  12 Million Records  True Positive 100.00% 100.00% 100.00% 100.00%  False Positive 0.00% 0.00% 0.00% 0.00% True Negative 100.00% 100.00% 100.00% 100.00%  False Negative 2.00% 2.50% 4.08% 6.44%  Precision 100.00% 100.00% 100.00% 100.00% Recall (Sensitivity)  98.04% 97.56% 96.08% 93.95%  Specificity 100.00% 100.00% 100.00% 100.00%  Accuracy 99.00% 98.77% 98.00% 96.88%  Type I Error 0.00% 0.00% 0.00% 0.00%  Type II Error 2.00% 2.04% 2.30% 2.70% Search Time 481.23s 768.98s 989.45s 1105.80s  From Table II, we did not achieve 0% false negative index in CouchDB and MongoDB because of semantic issues in some file attachments. False negative means the result which is returned is not what the user wants. This is a very practical case where two similar terms semantically mean two different things. This behavior has also replicated in the result of the network of NoSQL as shown in Table III.

Thus, the more we have the presence of these semantic issues, the more false negative results we get. A convenient way to overcome this problem is for the user to refine the query search term and possible dependent keywords.

In both Table II and III, we report the time required to achieve the terms extraction task plus the output of the search result to the user. With 1 million records, the time is recorded for the individual NoSQL storages. Then, with the inter-connected NoSQL nodes, we measured the time to achieve the terms extraction and dependencies. As reported, the search duration is optimal enough at this stage of the work.

5 Conclusions The importance of big data today is the high availability of information for easy access. We have enormous user     generated content that means a lot to the enterprise landscape for services and product delivery. However, it is challenging to make sense out of the huge data because the data is unstructured and comes from multiple sources.

While NoSQL databases have been proposed to house the unstructured data, each NoSQL is a silo and requires its own API or specific style of query with no standardized schema.

In this work, we designed and implemented a tool called TouchR that focuses on terms extraction from various NoSQL databases. The tool is built algorithmically on the Hidden Markov Model and we treat the distributed NoSQL storages as a network graph. Currently, we have tested TouchR with NoSQL data sources such as CouchDB, MongoDB, Neo4J, BigData, DynamoDB, and Cloudata.

The result of the evaluation of the tool is promising regarding the accuracy of the search terms with minimal search duration. The near future extension on this work is to focus more on the dictionary adaptation to new terms.

Also, there is the need for us to improve on reading textual contents in attachments since currently, our results show that it is a source of error.

