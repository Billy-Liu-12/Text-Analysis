Clustering Research Using Dynamic Modeling Based on Granular Computi:

Abstract-Clustering techniques is a discovery process in data mining, especially used in characterizing customer groups based on purchasing patterns, categorizing Web documents, and  so on. Many of the traditional clustering algorithms falter when the dimensionality of the feature space becomes high, relativing to the size of the document space, So it is important to  precondition for modeling .Secondly, we are usually disappointed to their clustering speed when having very large complex data  sets, and another defect is that they always fit some static model, so if the user doesn't select appropriate static-model parameters,  these algorithms can break down. In this paper*, we introduce a new clustering algorithm to improve the speed of clustering, this  clustering technique, which is based on granular computing and hypergraph partition, and it is capable of automatically discovering document similarities or associations, and this approach considers the internal characteristics of the clusters  themselves, thus it doesn't depend on a static model. Finally, we conduct several experiments on real Web data searched by ordinary search engine and received the satisfied results..

Index terms-Association rule discovery, Clustering research,  Dynamic model, Frequent item sets, Granular computing, Hyper-graph partition algorithm, Web documents  I . INTRODUCTION  The World Wide Web is a vast resources of information and services that continues to grow rapidly. Powerful search  *This work is supported in part by ChongQing Municipal Education Commission ,the contract No. is 020502  engines have been developed to aid in locating unfamiliar documents by category, contents, or keywords. Unfortunately, queries often return inconsistent results, with document retrieved that meet the search keywords but are of no interest to the users.

Clustering is a discovery process in data mining. It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. These discovered clusters can help explain the characteristics of the underlying data distribution and serve as the foundation for other data mining and analysis techniques, but how to select the clustering model and the correctness of clustering results will affect the quality of knowledge discovery.

Granular computing is geared toward representing and processing basic chunks of information-information granules.

It can be studied in many fields, such as image patition decision tree, Rough set theory, and so on.

Traditional clustering algorithms either define a distance or similarity among documents, or use probabilistic techniques such as Bayesian classification. Many of these algorithms, however, break down as the very large complex data sets such as the documents on the World Wide Web. There are a number of problems among these traditional algorithms. First, they need to define many parameters to give the threshold of distance and connection respectively; second, distance-based schemes generally require to choose the initial value for document clusters, and the choice is at random. In a very large     complex data sets , the randomly chosen means to receive an unsatisfied effect.

In order to overcome the above defects , in this paper, we introduce the granular computing based on binary system and give a data model. From the model , we extract the frequent item sets, then these frequent item sets are mapped into hyperedges in a hypergraph, and at first, a coarsening process is used to create a small hypergraph, and a good bisection of the small hypergraph is not worse than the bisection directly obtained from the original hypergraph, the clustering speed is improved simultaneously. At last, a multi-level hypergraph partitioning algorithm is used to partition the coarsening hypergraph into k parts, two criterions: fitness and connectivity are defined so as to prune the bad clusters and bad vertices and a synthesize criterion is defined to select the good partition level.

In section 2 we describe how to use the binary granular computing to construct the hypergraph model. In section 3 we introduce clustering method of hypergraph, and give two criterions and a synthesize criterion to filter out of the bad clusters. Finally we conduct several experiments on real Web data and present ideas for future research.



II. MODELING FOR DATA SETS  A. Introducing Granular Computing Information granules are collections of entities as the  name itself stipulates, the entities are arranged together due to their similarity, functional adjacency, or alike. The process of forming information granules is referred as information granules, there are several essential factors that drive all pursuits of information granulation. These factors include a need to split the problem into a sequence of more manageable and smaller subtasks and a need to comprehend the problem and provide with a better insight into its essence rather than get buried in all unnecessary details.

Let S=(U,A) be an information system[l], where U={ul,u2,.....,un} is a set of all objects; A is a set of- attributes .BcA is a subset of attributes, it can divide U into data blocks, these data blocks are defined as granules, so we divided U according to attributes, and we get the quotient sets U/IND(A), Each quotient set is a granule. Then we can mark the number for each element in every granule according to its  location in set U. In each granule, all of the numbers can be described as a binary, so the granule can be defined as a binary.

For example, if the granule includes uj, the corresponding place (i.e. the number i ) in the binary can be put 1, otherwise, if the granule doesn't include ui, then the corresponding place in the binary can be put 0. Obviously, the length of this binary is equal to the radix ofU.

The association rule discovery methods are often used in the field of data mining[1], so we can find the association rule using granular computing according to its principle. Given the granule X and Y, if Xrn Y (they will do Boolean AND caculation), the number of 1 in their intersected result 2 s%(a given threshold), then the association rule (X,Y) may be adopted. For convenience , XrnY may be marked (X,Y).

Obviously, the format XrnY may be expanded to intersection ofmany finite granules:  Xl1nX2n . ..nXnn . ..nY1nY2n ...nyn The combination above can be seen as an association rule,  and the calculation of intersection between two granules is boolean AND calculation. So, if we use the method above, the speed of data mining on many searched WEB pages will be improved greatly.

B. Modeling The Data Based on Binary Granular Computing  At first, we can use the ordinary search engines to receive documents for a certain field , the retrieval documents will then be stored into the database. Firstly, we summary some relative keywords in this field, and calculate the number of different keywords in the documents respectively, as figure 1 below. Then we can give some definitions for our model.

Definition 1 let S=(U,A) be a information system, where U={business, capital, fund, finance ......, invest} is a set of all keywords in a certain field; A={docl,doc2, ...,docn} is a set of all retrieved documents of this field, BcA is a subset of documents, it can divide U into different equivalence classes, then we get the quotient sets U/IND(A).

Definition 2 as shown in figure 1, if the fraction of the number of a keyword and the number of all words in document .?n% (a given threshold, that is the percent of the number of a keyword to the number of all words in document ), the corresponding place in the binary will be put l,obviously, the length of the binary constructed by each granule is equal to the radix of the set U.

business capital fund finance ... invest  Docl 20 35 10 40 ...... 0  Doc2 30 45 0 20 ...... 0  Doc3 0 25 0 35 ...... 27  ...... ...... ...... ...... ...... ...... ......

Docn 16 12 30 0 ....50  Fig 1. a Typical Document--Feature Set  For example, in figure 1, the binary constructed by the granule Doc, is described to 1101 ...... 0.

Definition 3 Let us define the support s, it is equal to the number of 1 in the value gotten after two granules had a Boolean AND calculation. Such value can also have such calculation with other granules or values received through this method. The task of discovering an association rule is to find all rules XnY, in which s is greater than a given minimum support threshold (this threshold may have to be determined in a specific manner).

Definition 4 Let us define the frequent item sets, the frequent item sets are computed by an association rule algorithm, such algorithm only find frequent item sets that have support greater than a specified threshold. the frequent item sets capture the relationships among documents of size greater than or equal to 2. Note that the frequent item sets can capture relationship among larger sets of data points. .

As these definitions shown above, the frequent item sets discovery is composed of four steps. The first step is to receive the documents and keywords by an ordinary search engine. The second step is to discover the granules of this information system. The third step is to generate association rules. The last step is to discover all frequent sets by an association rule algorithm such as Aprior.

A hypergraph H=(V,E) consists of a set of vertices (V) and a set of hyperedges(E). A hypergraph is an extension of a graph in the sense that each hypergraph can connect more than two vertices. In our model, the set of vertices V corresponds to the set of documents being clustered, and each hyperedge eE E corresponds to a set of related documents. A key problem in modeling data items as a hypergraph is to determine which related items can be grouped as hyperedges  and determine the weights of each hyperedge. In our paper, hyperedges represent the frequent item sets found by association rule discovery algorithm, and the weights of hyperedges represent the support of each frequent item set. So a mathematics model is received and can be used for hierarchical clustering based hypergraph.



III. MULTILEVEL HYPERGRAPH BISECTION  The flow chat of the hierarchical clustering algorithm based on hypergraph is described as the figure 2. The basic idea of this algorithm is explained below: at first, the most important problem is modeling data items as a hypergraph, the set of vertices of a hypergraph represents the group of related items which will be used to cluster, the hyperedges with the weights represent the similarity among items; Secondly, a multi-level hypergraph partitioning algorithm may be used to partition hypergraph and produce many sub-clusters; Thirdly, we eliminate bad clusters using the cluster fitness criterion, including fitness and connectivity; Then, the result of clustering can have a optimized treatment, at last, we may achieve the best clusters.

A. Description ofa Clustering Algorithm The clustering algorithm is divided into two steps. The  first step is to cluster all items as a whole, then partition it into many sub-clusters. The second step is to select the optimized sub-clusters. We have a simple description for the two steps below.

coarsening Partition  Sparse graph Sparse graph Sparse graph Sub-clusters  Fig2. the flow chat ofthe three-phase algorithm based on hypergraph  Step 1 At present, the bisection for hypergraph has been solved very well, especially using the multi-level hypergraph partitioning algorithm, which can partition very large hypergraphs in minutes on personal computers. For this task, we use HMETIS[4], its' basic partitioning processes are composed of four steps. (1) initialization clustering all related items as a hypergraph (2) coarsening: Some vertices can be merged together to form single vertices ,we have developed an algorithms for coarsening. If a granular's binary value is similar to another granular's and they are on the same hyperedge, they can merge together. (3)bisection: one way to achieve this is to use the partitioning algorithm which can partition the hypergraph into two parts ,so that the sum of each hyperedge's weight in every part cut by the partitioning is minimized. Note that minimizing the sum of each hyperedge's weight in every part partitioned can minimize the separation among data items in the same frequent item sets (4) iteration: each of these two parts can be further bisected recursively, until each partition is highly connected and the overall hypergraph has been partitioned into k parts.[4] Step 2 Comparing the received sub-clusters by using two criterions-fitness and connectivity: when the sum of fitness and the sum of connectivity of each sub-cluster in the same partitioned hierarchy are up to maximum, at this time all sub-clusters in the hierarchy are optimization. Then we compare the sub-clusters with their fitness and connectivity, eliminate bad clusters and bad data items which have worse similarity with others among the sub-clusters.[6]  B .Optimizing The Cluster Process As we have seen, we can receive a general hierarchy (i.e.,  a tree of clusters ) by using a hierarchical clustering algorithm  based on hypergraph. The root cluster contains all data items, there are different sets of sub-clusters achieved by partition on each level, there is a best level which includes the optimizing sub-clusters in this tree of clusters. How to choose the best level and its sub-clusters is very important. In our paper, we use two criterions-fitness and connectivity to filter out the best level. Obviously , the best clusters should have two characteristics: (1) It must be far away among sub-clusters, in other words, each sub-cluster should have good fitness (2) A sub-cluster's internal closeness among items should be strong, in other words ,there should be stronger connectivity within a sub-cluster. The following defmitions are effective principles .

The fitness:  er_cWeight(e)fitness(C)= ZeC Weight(e) (1) enCI>O  Let e be a set of vertices representing a hyperedge and C be a set of vertices representing a partition. The fitness function measures the ratio of weights of edges that are within the partition and weights of edges involving any vertex of this partition. Thus it can be seen that if the fitness is stronger, the independence of this sub-cluster is better. So the fitness of a level is the sum of each sub-cluster's fitness which is included in this level. Let N be the number of the sub-clusters on this layer, the sum is defined as sumf  N  sumf(N)= E fitness(Ci).

