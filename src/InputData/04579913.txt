COLLECTIVE INTELLIGENCE IN DISTRIBUTED SYSTEMS AND SEMANTIC  DATA VISUALIZATION

Abstract   In the era of monumental development in all spheres of technology, the repository of huge amounts of raw data is but a necessity. Also with increasing availability of storage space, this factor is not a big problem. Hence instead of tackling issues related to reducing the amount of data to be stored, there is bigger need to implement the creation of a knowledge base and hence create semantic data visualization.

The work discussed presents an approach to integrate expert knowledge all along the data mining process in a coherent and uniform manner. A collective intelligence system plays a central role in this approach. The work primarily aims at converting the presence of large amounts of raw source data into a knowledge base and collective framework.

Heterogeneous web sources have been taken into consideration, which act as containers for the raw data source. Data visualization is the second major functionality.

Taking into consideration the patterns mined, the onus is on the developer to present the mined patterns in a format that is most appealing to the end-user. Creation of such data visualizations requires various dimensions of the pattern to be understood. The estimated end product creates visualization from the knowledge base created from heterogeneous web sources.

KEY WORDS   Collective Intelligence, Apriori, Clusters, Semantic  Visualization, Ontology   1 INTRODUCTION Collective intelligence is a form of intelligence that  emerges from the collaboration and competition of many individuals. Knowledge that can be derived from a domain of various voting systems has the capacity for many individual unique perspectives to combine and converge. This convergence can be based on an assumption that the voting systems in use are uninformed. These systems are random to an extent. Knowledge can be obtained by filtering patterns from a decision based process which results in a well informed consensus. Ontology is then framed from this existing knowledge framework and is used as a structure in artificial    intelligence functions. Semantic data visualization and collective intelligence form the heart of the project.

A Collective Intelligence Information System consists of: Instances: Instances are the basic, "ground level" components of the system. Strictly speaking, the system need not include any instances, but one of the general purposes of ontology is to provide a means of classifying individuals, even if those individuals are not explicitly part of the ontology.

Classes: Ontology varies on whether classes can contain other classes, whether a class can belong to itself, or whether there is a universal class.

Properties: Objects in the ontology can be described by assigning values to their properties. Each property has at least a name and a value, and is used to store information that is specific to the object it is attached to.

Relations: An important use of attributes is to describe the relationships between objects in the ontology. Typically a relation is a property whose value is another object in the ontology.

Events: Events in a system play an important role in describing the nature, timing and effects of interaction between various objects.

Knowledge discovery from data is a process that aims to extract potentially useful knowledge hidden in large data sets.

However, it remains a crucial issue to elicit relevant models according domain expert knowledge. The KEOPS approach is based on a method which refers to expert knowledge all along a knowledge extraction process. Furthermore, KEOPS uses a part-way interestingness measure between objective and subjective measure in order to evaluate models relevance according to expert knowledge. The work has been proposed by Laurent Brisson and Martine Collard [4]. It is more accurate to state that the approach has parameters that must be tuned. When surveying the literature on this topic, it was noted that while there are many techniques for automatically tuning parameters, many of these techniques themselves have parameters, possibly resulting in an infinite regression. An additional problem of parameter-laden algorithms is that they make it difficult to reproduce published experimental results and to understand the contribution of a proposed algorithm.

These perils of parameter laden algorithms have been proposed by Eamonn Keogh [1].  These perils have been dealt with accordingly in the proposed system of work.

DOI 10.1109/ICETET.2008.183     The proposed work aims at creating a knowledge base across heterogeneous web sources. The base hence created will perform the functions of a Recommendation System.

Given a system of heterogeneous web sources the work first creates a knowledge base by mapping various datasets distributed geographically and reduces the sets to gain relevant patterns. Based on the association rules gathered from patterns a recommendation system is created. Finally the knowledge gathered from patterns is presented to the end user after implementation of the data visualization module. A method used to conserve bandwidth is the Compression Based Dissimilarity model. The dissimilarity model is described as follows.

Given two strings, x and y, the Compression based Dissimilarity Model is defined as follows  CDM(x, y) =    C (xy) / (C(x) + C(y)) When two strings x and y are not related the CDM value is close to 1 and smaller than 1 by a larger value when x and y are related. But it is to be noted that CDM(x, x) is not zero.

2. THE COLLECTIVE INTELLIGENCE SYSTEM 2.1 Implementation Issues: For highly complex data with mutual relationships, the derived patterns tend to be complex. Thus implementation challenges are as follows: 1. The patterns described by the existing knowledge extraction algorithms are still too abstract for general understanding. A pattern that is misinterpreted is of great danger. For example, many algorithms do not distinguish between causality and co- occurrence. For an application that aims at finding the reason for a certain type of disease, there is a great difference between finding the origin of the disease and finding just an additional symptom. Developing systems which derive understandable patterns and making already derived patterns understandable is a challenge.

2. As stated, current algorithms focus on a limited set of standard patterns. Deriving these patterns often does not yield a direct and complete solution to many problems.

Furthermore, with an increasing complexity of the analyzed data, it is likely that the derived patterns will increase in complexity as well. Thus, the challenge lies in finding richer patterns.

3. The final challenge arises when working with future patterns due to the increased number of valid patterns.

Therefore, the number of potentially valid patterns will be too large to be handled by a human user, without a system organizing the results. Hence future systems deal with providing a platform for pattern exploration where users can browse for knowledge on a personalized basis.

4. Storing Frequent Item-sets using tries is another major issue. In existing algorithms frequent item-sets that are not  needed by the algorithm can be written to the disk, and memory can be freed. Frequent items of size y? only are needed for the candidate generation of (y?+?1)?item-sets, and later they are not used. Hence there is need for memory conservation.

To conclude, systems should generate a large variety of well understandable patterns. Due to variations in the parameterizations, the number of possibly meaningful and useful patterns will dramatically increase and thus, an important aspect is managing and visualizing these patterns.

Figure 2.1: Class Diagram for Implementation Issues  2.2 Overview of the proposed Collective Intelligence Architecture  In the knowledge extraction and data visualization framework several sub modules like, implementation of distributed web sources, pattern mining in the compressed knowledge base, implementation of compression algorithms, and creation of data visualization exist.

2.2.1 Implementation of distributed web sources:  Distributed web sources provide the user an advantage of having to distribute data geographically. Apart from data, even processes are distributed geographically thereby, making the execution process faster and easier from the resource requirement point of view. Hadoop distributed file system (HDFS) clusters are used to create heterogeneous web sources 2.2.2 Pattern mining in the compressed knowledge base: Form the compressed knowledge base that contains various related item sets from the given dataset, various patterns have to be extracted and irrelevant patterns have to be pruned. Various queries with and without parameters have be executed to implement this module. Parameter-light algorithms tend to produce results of higher accuracy.

2.2.3 Implementation of compression algorithms: In order to conserve the bandwidth and relax certain other constraints the compression is implemented using run- length encoding. From here the dissimilarity and similarity measures are calculated.

2.2.4 Implementation of trie-based apriori approach: Once the trie data structure is constructed as per requirements the dataset and knowledge base are processed. Frequent items sets are scanned to generate various patterns. A recommendation system is created based on the Pearson and Euclidean mathematical models.

2.2.5 Creation of data visualization: The result set is parsed and an intermediate representation in the form of graphs and charts is created. The intermediate result is translated to a visualization structure. The visualization structure is classified based on recommendations created.

3 System Architecture  HDFS has master/slave architecture. An HDFS cluster consists of a single Namenode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of Datanodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files.

Internally, a file is split into one or more blocks and these blocks are stored in a set of Datanodes. The Namenode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to Datanodes. The Datanodes are responsible for serving read and write requests from the file system?s clients. The Datanodes also perform block creation, deletion, and replication upon instruction from the Namenode.

Hence is a brief description of system architecture. The same architecture will be deployed later with respect to an application which will bring out practical and commercial purposes involved in implementing the work proposed. The architecture helps in distributing processes across a group of systems.

NAME NODE METADATA  CLIENT  REPLICATION  BLOCK  DATA NODES  BLACK OPERATIONS  READ  METADATA OPERATIONS  WRITERACK 1 RACK 2  CLIENT  NAME NODE METADATA  CLIENT  REPLICATION  BLOCK  DATA NODES  BLACK OPERATIONS  READ  METADATA OPERATIONS  WRITERACK 1 RACK 2  CLIENT   Figure 3.1:  System architecture  4 Real Time Analysis and Application e-Business (electronic business), is the conduct of  business on the Internet, not only buying and selling but also servicing customers and collaborating with business partners.

Today, major corporations are rethinking their businesses in terms of the Internet and its new culture and capabilities.

Companies are using the Web to buy parts and supplies from other companies, to collaborate on sales promotions, and to do joint research. Exploiting the convenience, availability, and world-wide reach of the Internet, is the trend of recent times.

The following are the challenges or limitations in the e- Business domain presently: Complex parameters: During various analysis operations like business process intelligence or customer relationship management queries include with them complex parameters that are hard to parse and many times they tend to produce malicious results because of their false and complex nature.

Integration into complete solution: Using various techniques like knowledge extraction and analytical processing of multi- dimensional data, intermediate results should be deciphered into complete solutions in order to complete the analytical process.

The above challenges can be overcome by using the following solutions.

Text categorization: The aim of this stage is to extract content and using data build topic hierarchy. Once, a hierarchy is built it becomes easier to track customer operations and hence analyze the customer behavior. Analysis operations performed are Extraction of key words and phrases, Transformation of documents and query log records into vectors, Creation of clusters hierarchically, labeling each cluster with significant words, phrases, normalization Catalog creation and service discovery: This stage aims to achieve  Attribute extraction and Data formulation. Once a document structure is recognized attribute extraction and data formulation become easier.

Business Process Intelligence: The goal of this stage is to improve the quality of business services and processes. The recommendation system by providing apt and complete recommendations improves the quality of the system.

4.1 Application Oriented Architecture Architecture for the recommendation system based on the collective intelligence system is described as follows.

The recommendation system is created to test the real-time application and feasibility of collective intelligence system.

4.1.1 Proposed Methodology  The proposed methodology operates on a distributed file system environment based on the SOAP (Service oriented architecture protocol) ontology. The triangular ontology deals with service registry, service provision and remote method invocation. Hadoop clusters are used for this purpose.

4.1.2 User Layer Customer/Buyer Domain: The customer/buyer  domain consists of various customers and buyers who represent the clients of the architecture. A customer or buyer is any remote device seeking to analyze various data sets based on certain queries. The customer seeks access to a remote service on the service domain that analyses datasets. These remote services have direct access to a remote database.

Hence the customers represented in this domain have indirect access to a remote database with service domain activity as an intermediary stage.

4.1.3 Collective Intelligence Layer  Application Server Proxy: The application server proxy performs the function of a routing table. The customer in the user layer searches for various services that are available on the distributed file system. While doing so, the first component a customer?s service query will approach is the application server proxy. The application server proxy contains a table or list of various services available on the distributed file system. Hence all the analysis services have to first register themselves with the application proxy server in order for the customer to access these services. When a service wanted by the customer is available on the service domain, the proxy deploys a remote server where this service is available to the customer.

Service Domain: The service domain mainly consists of one primary service, namely the recommendation system.

The recommendation system used here deals with recommending items for users given a rated and uninformed data set. That is, the data set that serves as the base or input will contain a list of item sets that are ranked by users using the item set. The final output will be an intelligent recommendation of unused item sets for a particular user. The item sets can be any product like books, movies, gadgets, services, techniques etc. based on the real time dataset provided as input.

Semantic Data Visualizer: The output of the analysis service machines needs to be presented in a graphical, understandable and user-friendly manner to the end user who is the customer or buyer. The semantic data visualizer involves a semantic index that is used in tandem with the analytical output. The semantic parameters involved vary with each analysis. Graphs and charts are created as intermediate results (IR). The IR is then used to map the output onto predetermined visualization structures.

4.1.4 Data Layer  The data layer is the core and heart of the entire architecture. The layer has one main component: the remote data set. The remote data set is a set of item sets that are linked to each other by way of ratings. The item sets consist of customer identities and various other e-Business customer  fields like products purchased, time logs, costs incurred, billing information and other logs. The analysis service provider servers have independent direct access to this remote data set.

5 Algorithms The collective intelligence system hence proposed uses a platter of assorted algorithms and following are the algorithms that are being put into use in the collective intelligence system.

5.1 Compression based dissimilarity measure      5.2 Frequent Pattern mining   Function dist = cdm(a,b) Save a.txta?ascii Zip(?a.zip?, ?a.txt?); A_file = dir(?a.zip?); Save b.txt b ?ascii Zip(?b.zip?, ?b.txt?); B_file = dir(?b.zip?); A_n_b = [a; b]; Save a_n_b.txt a_n_b ?ascii Zip(?a_n_b.zip?, ?a_n_b.txt?); A_n_b_file = dir (?a_n_b.zip?); Dist = a_n_b_file.bytes / (a_file.bytes + b_file.bytes); Return Dist End  1. Select random sample, S, from transaction data set (D) 2. Split the random sample (S) into subset (or partitions), so each subset size is in accordance with computer memory.

3. Read one subset.

4. Store all the candidate 1-itemsets in the subset.

5. Count the number of instances of each 1-itemsets to compose the candidate set called C1.

6. Delete least frequent candidate 1-items to generate a large item set list called L1.

7. Join L1 with itself to create candidate 2-itemsets C2. C2 consists of all combinations of 2-itemsets and check for the property that states ?all nonempty subsets of frequent itemsets must also be frequent?.

Figure 4.1: e-Business Architecture                          5.3 Mapping & reducing algorithm                    APPLICATION SERVER PROXY   Service ServiceID Remote Address     SEMANTIC DATA VISUALIZER     Analyzer  Charts  Graphs  Visualization Structures     The  Recommendation System  Service Domain     I N T E R N E T  User Layer Collective Intelligence Layer Data Layer  8. Recursively perform steps 4 to 6 with the addition of an item-set every pass, until some L k becomes empty which means that the join operation (step 6) is not possible.

9. Combine all frequent L n-item-sets into one list called local frequent item-sets (LL p) for the given partition.

<where n=1, 2 ? K> 10. Recursively perform steps 2 to 8 for each partition.

11. Combine all LL p to generate global candidate item- sets (LG).

12. Scan the whole transactional database to count the number of occurrences of each candidate K-item-set in LG.

13. Delete least frequent candidate K-items to generate global frequent item-set in the data set (all subsets).

Function map (string key, string value): key: document name value: document contents  For each word w in value: Emit-intermediate (w, ?1?)  Function reduce (string key, iterator values): key: a word values: a list of counts result=0  For each v in values: result+= parseint (v) emit (string (result))      6 Results and graphs The following are the two sections in which a few results have been inferred from the graphs plotted.

6.1Construction and destruction time for stored filtered transactions:  Primarily no difference is observed when data sets are too small, as seen in Fig 6.1. However, the sorted lists tend to slow down as data set grows. RB-trees are always faster than tries, but the difference is not significant. In support count, each filtered transaction has to be visited to determine the contained candidates. If transactions are stored in a sorted list, then going through the elements is a very fast operation.

In the case of RB-trees and trie, this operation is slower, since a tree has to be traversed. Hence a sorted list is used despite the slowness during large data sets, due to easy traversal options.

Fig 6.1: Time Vs Threshold  6.2 Writing frequent item-sets to disk: The fastest results are obtained if frequent item-sets  are not stored in memory but written to disk in the candidate generation process, as seen in Fig 6.2. In cases of determining small and medium sized candidates most edges lead to leaves, hence removing other edges does not accelerate the algorithm too much. Hence, the memory needs can be significantly reduced if frequent item-sets are not stored in memory.

Consequently if frequent item-sets are needed to determine valid association rules and memory consumption is an important factor, then it is useful to write frequent item-sets to disk and read them back when the association rule discovery phase starts.

7 Conclusion and future implementation The proposed collective intelligence system is hence implemented and its applications are tested using the real-time recommendation system in an e-business background. The      Fig 6.2: Time Vs Threshold  recommendations hence created based on collective intelligence are accurate to a greater extent. The collective intelligence system can be extended in lines of semantic web searching to provide personalized results and optimized link retrievals. Furthermore, a mechanism or structure can be created to improve on the performance of sorted lists in case of large data sets to decrease the time complexity of creation of the filtered transactions from the data set.

