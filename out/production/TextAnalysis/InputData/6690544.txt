Strategic alignment of

Abstract?Big Data is an increasingly significant topic for  management and IT departments. In the beginning, Big Data applications were large on premise installations. Today, cloud- services are used increasingly to implement Big Data applications. This can be done on different ways supporting different strategic enterprise goals. Therefore, we develop a framework that enumerates the alternatives for implementing Big Data applications using cloud-services and identify the strategic goals supported by these alternatives. The created framework clarifies the options for Big Data initiatives using cloud-computing and thus improves the strategic alignment of Big Data applications  Keywords?Big Data, Enterprise Architecture, Alignment

I.  INTRODUCTION Data-driven decision making improves the performance of  firms [1]. Therefore, Big Data [2] has become an crucial topic in management [3]. The importance of this theme is supported by an empirical study (worldwide online survey with over 1300 IT managers) from ZDNet "70% will use data analytics by 2013" [4] (ZDnet 2012). Big Data is one of the most disruptive information technological developments [5]. In [6] Zikopoulos et al. characterize Big Data with the three properties - Volume, Velocity and Variety. Big Data applications are data-intensive applications, with a large volume of data, a high velocity of processing and a data variability of the existing IT solutions.

Big Data enables handling and analyzing more types of unstructured (e.g. user statements in social media) and semi- structured data as before [3]. An example scenario for Big Data is the provisioning of real-time information to mobile users.

Based on a stream of position information created by the mobile user?s smartphone, the mobile receives information selected from a variety of sources and provided nearly in real- time.

Big Data can be seen as a further development of business intelligence (BI) to the three "V" [7] as shown in Fig. 1. In comparison to BI it is now possible to analyze large quantities of data from different data sources and with different structure processed nearly in real-time. The business impact of Big Data is shown in various examples of real business cases [8].

Significant cost cuts could be achieved by decreasing the  estimated and actual arrival time of aircrafts. Furthermore, retailers such as SEARS can increase sales through faster data analysis and thereby better personalized promotions. Business processes in the enterprise, at the link to the customers and suppliers can be improved by using Big Data [9]. An improvement of business processes is possible because of reducing process costs and time, as well as the increase of process quality, based e.g. by a better data quality for decision making [9].

Relational DBMS?&  BI Data?Warehouse  Volume  Variety  Velocity    Fig. 1. Big Data as an extension of traditional business intelligence based on [7]  Big Data can be best understood as a transition instead of a certain new technology [10] or algorithm. This transition is enabled by advancements of a set of overlapping technologies.

First, the management of structured data has become much more efficient. Today, it is possible to distribute relational databases world-wide [11]. Second, new approaches to handle the batch processing of large volumes of data such as Hadoop [12] are highly successful. Third, stream processing of data enables decision making in real-time or near real-time.

In the beginning, Big Data initiatives had been constrained to very large enterprises due to a high required effort. This has changed significantly due to the rise of cloud-computing. Now also small and medium-sized are able to implement Big Data data initiatives. Cloud-services have nearly no up-front costs they are scalable to fulfill huge peaks in demand.

If cloud-services are used to implement Big Data applications, different approaches are possible. These alternatives differ in the set of strategic goals supported.

Therefore, the contribution of our paper is to develop an  2013 17th IEEE International Enterprise Distributed Object Computing Conference Workshops  DOI 10.1109/EDOCW.2013.22     architectural framework enumerating the alternatives for creating Big Data applications using cloud-services. Different alternatives are discussed and compared. The created framework clarifies the options for Big Data initiative using cloud-computing and thus improves the strategic alignment of business process management initiatives.

First we discuss how Big Data influences enterprise architecture and enumerate the sources of data processed with Big Data. Then we shortly introduce the basic building blocks of Big Data implementations. In the following section, the options for implementing Big Data using cloud-services are enumerated. Based on this architectural framework for Big Data is developed. It differentiates the possible options when implementing Big Data in the cloud. Using this framework, possible ways for aligning Big Data with enterprise strategy are analyzed. Related Work is discussed in the following section.

Finally, we give a conclusion and outlook.



II. BIG DATA Big Data induces two phenomenons into enterprise  architecture. First, the advancement of Big Data establishes long information pipelines across formerly separated business units. Information formerly separated into silos becomes interconnected. A typical Big Data pipeline is denoted in Fig.

2. It starts with data acquisition and/or extraction. This may be done either by extracting data from existing data sources such as databases, log files or using acquisition services such as the tag data read in by smartphone users. The data are then transformed and standardized to improve their quality. Data with an acceptable level of quality is stored is available for processing. Processing may be done with a multitude of algorithms such as MapReduce [13]. Typically the processed is aggregated and then analyzed. Finally the data are visualized and used for human decisions.

Acquisiton / Extraction Storage Processing Analysis Visualization  Trans? formation   Fig. 2. Big Data pipeline  The second phenomenon induced into enterprise architecture is the increase in data and the growth in morphological complexity as shown in Fig. 3. Enterprise information systems such Enterprise Resource Planning (ERP) systems started with data from business transactions. These data describe completed transactions, but not data created before and afterwards, such as the goods put in the cart but removed before the purchase. These data are semantically homogeneous. That means their meaning is standardized and defined. Customer relationship management systems take into account also customer interactions before and after the transactions. These interactions do not constitute a business transaction and increase the amount of data by so-called pre- and post-transactional data. These data, however, are still structured ones. Web logs and sensor data further increase the amount of data. Such data may not be fully structured, it introduces non-structured elements by changing formats etc.

Nevertheless web logs and sensor data are still semantically homogenous. Their meaning can be derived easily. Social software [14] provides a large amount of unstructured data and  without defined semantics. Thus social software is a powerful driver for Big Data in Enterprises.

Am ou  nt of  Da ta  Transactional Pre? and post?transactional  Structured? Non?structured  Semantically?homogeneous Semantically?  hetero? geneous  Social Media  Web?Logs Sensor?Data  Customer interactions  ERP  Morphology    Fig. 3. Increase of processed data in enterprise information systems  The huge amount of data created by social software is created by the three concepts contained in social software: social production, weak ties and collective decisions, as shown in Fig. 4. Social production and weak ties are used in social media. Social networks are based on weak ties and collective decisions. The use of social software within enterprises, especially the use of social media and social networks is called Enterprise 2.0 [15]. It strives for optimizing the internal business processes by the use of social software.

Social Software  Social Production Weak Ties  Collective? Decisions  Social media Social networks  Enterprise?2.0?    Fig. 4. Social Software and Enterprise 2.0  Social production, the creation of weak ties and collective decisions, create enormous amounts of data that do not fit into predefined schema and that does not come in predefined batches. Social production [16] is the ? at least partial ? bottom-up organization of production enabling the integration     of customer-initiated features. Therefore, pre-defined and centrally defined schemata for data cannot be enforced. Social software replaces the Taylorism [17] and Fordism [18] - oriented production of goods and provisioning of services by a co-creation oriented one. Goods are produced and services are provisioned together with the customer by collecting suggestions, evaluations and comments of the customer. These contributions of the customer may arrive in a multitude of formats and types. Weak ties [19] facilitate the exploitation of new ideas and knowledge by creating a connection between persons across organizational structures. Although weak ties can be abstracted as a graph, the variety of relationship types may be extremely complex. Social software also supports collective decision approaches often subsumed as wisdom of the crowds [20]. There are not only voting mechanisms but also blogs etc. They created vast amounts of unstructured data.

For example unstructured data from social media (e.g. opinions about products) can be processed with internal, structured data (e.g. purchase and return history) to implement a preventive product returns management to cut down handling costs.



III. INTEGRATING BIG DATA INTO ENTERPRISE ARCHITECTURE  Big Data is integrated into existing enterprise architectures in three different ways as shown in Fig. 5. The first approach is to use Big Data as an extension to an existing business intelligence application. The Business Intelligence application consists of a data processing layer, an information processing layer and a presentation layer. The Big Data application provides information created from data that are too huge and/or too unstructured to be processed by the legacy business intelligence system. The second approach is to present the information created by the Big Data application (with own data processing and information generation) using an own presentation layer. The third approach is to support other applications such as web-shops (with standardized interfaces using data processing and information generation). E.g. the Big Data application provides product suggestions for increasing cross-and upselling. This approach can be used by different applications in the enterprise to generate cost savings by cutting down maintenance, support and total unit costs of transactions.

Legacy Business?Intelligence  ?  Presentation Application  Information? Generation  Data? Processing  Big?Data  Presentation  Information? Generation  Data? Processing  ?  ?  ?    Fig. 5. Big Data integration  The first Big Data applications had been created as on- premise applications. Tools such as Hadoop [12] or MapReduce [13] had been installed on local hardware, configured and parametrized. However, performing such an installation requires a large group of specialized experts and a high effort in hard- and software. To show the complexity of Big Data implementations, Hadoop [12] an open-source framework providing a technology stack for implementing Big Data shall be introduced.

HDFS  HBASE MapReduce  Hive /?Pig    Fig. 6. Hadoop Stack after [21]  Hadoop [12] is organized into nodes. There are master nodes, data nodes and worker nodes. Master nodes contain to answer request from client applications. The TaskTracker receives tasks from the JobTracker. NameNodes contain a directory tree of all files in the Hadoop Distributed File System. Data Nodes store data in the HDFS and are responsible for data replication. Client applications interact with DataNodes identified by the NameNode. Worker nodes include a DataNode and a TaskTracker.

The Hadoop Distributed File System, HDFS [13] is a core part of Hadoop. A fault tolerant approach allows HDFS to run on commodity hardware. HDFS is optimized for data stream access instead of random access to data. Therefore, it optimizes speed instead of latency. To facilitate replication, it uses a write once approach. HDFS enables the locality of computation. The program is moved to the data instead of the data to the program. HDFS is able to manage 25 petabytes of data [22].

MapReduce [13] is a distributed processing model pursuing a divide-and-conquer approach. Processing data with MapReduce is done in three main phases. In the first one, data is loaded as part of an Extract, Transform, Load Sequence.

Then data is retrieved from storage, processed and returned to the storage.  The second phase, is done in two steps, map and reduce. Finally, the results are extracted from the storage.

Amazon's elastic MapReduce is a cloud-based, easy scalable implementation of Hadoop.

HBase [21] is a column-oriented database to make HDFS more useable. Contrary to HDFS Hive uses indexes to provide a fast random access to data. Hive is the data warehousing technology of Hadoop. Hive [21] is used to store structured data and therefore uses a table-based abstraction. It offers the possibility to perform ad-hoc queries on data. Pig [21] is a language for describing and running Hadoop Map Reduce.  It is a data-flow-oriented language.



IV. IMPLEMENTING BIG DATA IN THE CLOUD Cloud-computing [23] provides a number of benefits for  implementing Big Data. It provides services that are easily accessible, with low up-front costs and offer a high degree of scalability. Cloud-computing [24] started by providing single services. Although the use of single cloud-services provides benefits for enterprises and organizations, even larger benefits can be reaped from using cloud-environments.

Over time, single services were enriched by resources and evolved into cloud-environments. Cloud-environments are bundles of services and resources [23]. Examples are OpenStack [25], Microsoft Azure [26] or Google Apps [27]. In the beginning, they had been static. That means the set of service and resources had been fixed and could not be change.

However, it became quickly obvious, that it necessary to add service and resources and to be able to configure them. Many business processes are subject to permanent changes in and outside the organization (e.g. business processes in financial sectors with BASEL III [28] or Solvency II. requirements  [29] ).

To cope with such requirements, the former static cloud- environments evolved into extensible and configurable ones [30]. Extensible cloud-environments provide integration mechanisms for services and import mechanisms for resources.

With integration mechanisms, external services can be made available within a cloud-environment. Integrating a service means to make him transparently accessible within the cloud- environment. Thus, an integrated service can be invoked like an internal service, however it is managed outside cloud- environment. That means, the resources needed for providing the integrated service are outside the cloud-environment.

By this means already existing application functionality can be reused as a cloud-service. External resources can be imported into the cloud-environment. Import mechanisms enable moving resources into the cloud-environment.

Configurable cloud-environments provide configuration mechanisms enable the adaptation of services and resources according to individual requirements. In this way standardized but configurable services become useful in more application scenarios. Dynamic cloud-environments [23] provide both extension and configuration mechanisms for services and resources.

Using dynamic cloud-environments it is possible to move a Big Data pipeline completely into the cloud. External data sources can be either integrated as a service or imported as a resource. The configuration mechanisms can be used to adapt the cloud-service to the requirements of the Big Data pipeline

V. ARCHITECTURAL FRAMEWORK FOR CLOUD-BASED BIG DATA  The framework for cloud-based Big Data uses two dimensions for differentiating architectural approaches. The first dimension differentiates the service model used for implementing the Big Data Pipeline. The NIST cloud definition framework [24] differentiates three service models: Software as a Service (SaaS), Platform as a Service (PaaS) and  Infrastructure as a Service. There is also the possibility to not use cloud services at all. This is called a monolithic big data pipeline.

A. Service Model 1) Decision as a Service In addition to the service-models known from the NIST  cloud-computing definition, a fourth service model shall be introduced, as shown in Fig. 7. It is called Decisions as a Service (DaaS). Decision as a Service. Using the Decision as a Service means to move the decision processes totally into the cloud. That means DaaS cloud-services can be connected directly to create a Big data pipeline.

DaaS  SaaS  PaaS  IaaS  SaaS  PaaS  IaaS  PaaS  IaaS IaaS  Monolithic pipeline Monolithic  pipeline Monolithic pipeline  Monolithic pipeline  Se rv ic e? M od  el   Fig. 7. Big Data Pipeline Service Model  One of the earliest examples for decision as a Service is Microsoft Tag [31]. Individual Quick Response Codes [32] (QR-Code) can be placed on products, newspapers etc. Using an app for mobile phones is provided. By following the link encoded in the QR-code, the uses may obtain information. At the same time, data and location of the activation is registered and can be analyzed. All services are provided as a cloud- service. For example, Whole Foods Market Inc. uses Microsoft-Tag to give their customers information about their products and track at the same time, where their customers are [33]. By this means, the effectiveness of marketing campaigns can be measured with much more detail.

2) SaaS-enabled Pipeline SaaS-enabled Big Data pipelines use virtualized  applications. Applications provided as SaaS contain functionality for implementing decision processes or implement parts of a decision process. By this means, applications can be used without caring about their provisioning. A SaaS-enabled Big Data pipeline is more difficult to scale as a DaaS-pipeline because the monolithic pipeline creates a central bottleneck.

3) PaaS-enabled Pipeline PaaS enabled Big Data pipeline use virtualized software  application components such as databases, message queue, etc.

The virtualized part is smaller than in the SaaS-enabled business process management system, but bigger than in the IaaS-enabled system. In PaaS-enabled Big Data pipelines, the scalability is again reduced because also the application functionality is part of the private Big Data system.

4) IaaS-enabled Pipeline An IaaS-enabled Big Data pipeline uses virtualized  computing resources and storage. By this means, it is possible to add quickly resources in order to react to a suddenly increased demand. To enable scaling, the monolithic pipeline has to implement its own scaling mechanisms. However, these scaling mechanisms do not achieve the scaling of cloud-based resources because there is no resource pool for providing additional capacities.

B. Deploment Model The second dimension is the deployment model of the used  to implement the Big Data pipeline. The NIST cloud definition framework [24] differentiates four deployment models: private, public, hybrid and community. Private clouds are provided on premise only by the enterprise itself. Public cloud-services are provided by an independent service provider. A hybrid cloud is a cloud composed of services provided both by private and public cloud-services. A community cloud is created provided by a community of cloud-users sharing their resources.

The Big Data pipeline can be implemented as a public, private, hybrid or community cloud. A private Big Data pipeline is provided completely on premise. It provides a significantly better protection of intellectual property because the process definitions are no longer stored in the cloud- environment. A public Big Data pipeline uses only cloud- services by an independent cloud-service vendor. Another alternative is, that the cloud-services used for implementing the Big Data pipeline are provided by a community cloud.

A hybrid is created by splitting the Big Data pipeline between the cloud and on premise implementations. As shown in Fig. 8, some computationally intensive steps of the data analysis (e.g. for cluster analysis) can move to the cloud and other data risky steps (e.g. data store; data visualizations) can be done in the enterprise.

Acquisiton / Extraction Storage Processing  Analysis  Visualization Trans?  formation  Pr iv at e? cl ou  d Pu  bl ic ?c lo ud    Fig. 8. Hybrid big data pipeline

VI. STRATEGIC ALIGNMENT OF BIG DATA PIPELINES Now, the two dimensions shall be combined and compared  according to strategic goals. The results are shown in Fig. 9. In the DaaS pipeline, the highest degree of scalability and agility is achieved. For all parts of the Big Data system, virtualized resources are used. Using DaaS creates the smallest effort and needs only a short time to implement and use. Thus, by freeing the companies from the hassles of resource management, DaaS helps the most to strengthen the core competencies of enterprises. On the contrary, an IaaS Big Data pipeline with non-virtualized resources requires the highest effort. A DaaS pipeline can be used by online retailers for credit scoring of their customers. Furthermore generally peaks (e.g. online purchases on Black Friday) can be compensated much better in  a DaaS However, the advantages of DaaS are accompanied also by a number of drawbacks. The use of DaaS easily creates a data lock-in. Data processed in the business processes may be difficult to export. The same effect may be observed on the process description, called process-lock-in. It may be difficult to extract the process descriptions, due to storing all process- relevant data with the DaaS pipeline.

Using public cloud-services strengthens the core- competencies of an enterprise. However autonomy and intellectual property protection is weaker. Information from decision requests can possibly be used by different other enterprises (and maybe competitors). On the contrary a private Big Data pipeline offers much more autonomy than DaaS. For enterprise with a set of high risky data (e.g. pharmaceutical enterprises) a private pipeline could be better. On the other hand, economies of scale and learning effects of scoring different customers can not be realized if every small retailer implements such a private solution.

DaaS  SaaS  PaaS  IaaS  HybridPublic  Deployment Model  Se rv ic e? M od  el  Private  Scalability  Strengthening?Core?Competencies  Effort?(tim e?&  ?m oney)  Data?Lock?in  Process?Lock?in  Intellectual?Property?Protection  Autonomy  Agility    Fig. 9. Strategic Goals depending on service and deployment model

VII. RELATED WORK Up to now, the architecture of Big Data has been  considered primarily from a technical point of view such as [34] and [35][36]. However, these approaches do not discuss the relationship of Big Data to enterprise architecture and strategy. A further example are the technical and performance considerations for moving the Big Data approach Hadoop into the cloud in  [37]. New technologies such as the globally distributed spanner database attract a large amount of attention [38]. Approaches to develop an architecture for Big Data can also be found in [39]. Basic considerations addressing Big Data architecture are made in [40]. A service-oriented architecture for business intelligence systems is proposed in [41]. In [42] a framework using an Service-Oriented-Architectures is proposed to for real-time environments. An architecture adapted to requirements of the telecommunications industry is introduced in [43]. A delphi study on business intelligence and service-oriented architecture is presented in [44]. A further service-oriented architecture for business intelligence is presented in [45]. Different ways to create multitenant     architectures to support Big Data in the cloud are investigated in [46].

Several research approaches related to the framework created in this paper can be found in a number of papers. First approaches for moving Big Data into the cloud have been analyzed in [47]. Cloud-specific problems of data management are discussed in [48]. General technical considerations are found in [35] and [21]. The Big Data management system ASTERIX is introduced in [49]. A value chain for Big Data is conceptualized in [39]. First basic considerations on system architecture for Big Data are made in [50]. The relationship of cloud computing and Big Data in general is discussed in [51].

In [33] the term Decision as a Service is introduced for the separation of decisions logic from the application logic.

However, no considerations about moving the decision service into the cloud are made. The decision process is completely put into one service. Our approach however differentiates into two dimensions. It allows to use separate services for every step of the process and to decide for each service either to use a cloud- service or to provide it locally.

A purely managerial view without relation to enterprise architecture is used in research about information logistics such as [52]. A business oriented approach for structuring the business analytics lifecycle [53]. It consists of capturing information, analyzing information, aggregation and integration of information the use of gained inside to guide further strategies and the dissemination of information and insights. Only little research is done on the organizational impact of Big Data  [54]  and especially the influence on the role of IT departments in organizations.



VIII. CONCLUSION AND OUTLOOK Big Data extends the scope of data processing from  transactional data to non-transactional and unstructured data.

New chains of information are created with enterprises providing an unprecedented velocity, volume and variety of information. This creates important opportunities for enterprises and their IT-departments.

The benefits of Big Data mirror well-known advantages of cloud-computing [24]. New data sources can be integrated quickly and with an extremely low effort. Data and information bases are available on demand and on a low price (e.g. Amazon AWS [55]). The data supply is scalable: additional information can be obtained easily.

Big Data is becoming a crucial element of enterprise architecture. Due to advances in cloud-computing Big Data can be realized using cloud-services in a couple of ways fulfilling different strategic goals.

Therefore, we developed a framework that identifies the alternatives for implementing Big Data applications using cloud-services and compare them according their support for strategic goals such as scalability, data-lock-in etc. By this means, the framework developed in this paper allows IT- departments to align their Big Data initiatives with enterprise strategy.

The use of cloud computing to establish Big Data in the enterprise can be very profitable. Particularly for small and  medium enterprises (SME) this approach can reduce the cost of implement Big Data and make it even possible. SME generally do not have much capital to pay for investments related to a Big Data implementation (e.g. costs for hardware resources and specialists). But a transfer of secret business data to the cloud can be risky. Because of this a hybrid solution of use internal infrastructure and cloud services can be advantageous. In case of a decision as a service approach criteria like lock-in, effort, scalability and autonomy must be discussed for each business case. For example for standardized credit rating decisions (e.g.

customer credit rating for online retailers) a DaaS can be realize better the requirements (e.g. costs, scalability) as an private Big Data solution. In contrast,  the analyze of more critical data and operational risks a private solution (e.g. for analyzing the current production (risks) in a pharmaceutical enterprise) can be better.

The alignment of Big Data with enterprise strategy also improves the standing of the IT departments. For a long time, IT departments has been regarded as a cost-driver and collection of risks [56]. IT-departments have been compared with the production of utilities such as electricity [56].

Therefore, a multitude of outsourcing approaches has been developed. On the other hand, in  [57] Brynjolfsson et. al show that the utility model is not applicable to complex IT resources.

Complex IT resources may provide strategic advantage.

Aligning Big Data with enterprise strategy opens a window of opportunity for IT departments to become an enabler for strategic initiatives.

Not in all cases Big Data can enable better decisions based on a better quality and some moral problems arise with the use and should be considered [58]. Future research must focus on industry and country specific criteria for the use of Big Data and decision as a service. This research can be done empirical by the use of instruments of the quantitative and qualitative studies. Furthermore, a specific decision model for implementation of these approaches is needed.

