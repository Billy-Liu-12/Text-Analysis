<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings

Abstract: This paper points out the bottleneck of classid Apriori's algorithm, presents an improved assofiation rule mining algorithm. The new algorithm is based on reducing the times of Scanning candidate sets and using hash tree to store candidate itemsets. According to the running result of the algorithm, the processing time of mining is decreased and the efticiency of algorithm has been improved.

Keywords: frequent itemset 1. Introduction Data mining; association rule; Apriori algorithm, Mining association rule is one of main contents of data mining research at present, and emphasizes particularly is finding the relation of different items in the database. It was fmt proposed by R. Agrawal et al[l].

The basic concept is described in Table 1: set of items with n different itemsets in DB, each transaction Tin DB is a set of item (i.e.itemsets), so TE I.

Definition 1: Let I=[il, i2, . _ _  , in) be a set of items, then D=( O i d ,  T&gt;I T c  I] is a transaction database, where Tid is an identifier which be associated with each transaction.

Definition 2: Let Xc I, YE I, and XflY= 0, we called X a Y  as association rule.

Delinition 3: Suppose c is the percentage of transaction in D containing A.that also contain B, then the rule X-Y bas confidence c in D. If s is percentage of transactions in D that contain AUB, then the rule X a Y has support s in D.

Definition 4: Hypothesis XG I, minsup is the smallest support. If the frequency of X is greater than or equal to minsup, then X is called as frequent itemset, otherwise X is called non-frequent itemset.

Classical Apriori algorithm: (1)Cl = [candidate I-itemsets); (2) L1 = [ c E  C1 Ic.count?minsup)); (3) (4) Ck=apriori-gen(Lk.i); (5 ) (6) Ct=subset(Ck,t); (7) for all candidates c E Ct do (8) c.counttc; (9) end (10) Lk={c ECk Ic.count?minsup} (12) end for (k=2; Lk- I#@; k++) do begin for all transactions t E D do begin (13)Answe~ ULk; Our p"pose is searching for association rules with the smallest support degree and the smallest confidence degree.

The classical algorithms can mine those rules by scanning database repeatedly. Its efficiency is very low.

The association rule mining is a two-step process: 0 Find all the frequent itemsets in the transaction  database. If support of itemset X, support (X) 2 minsup, then X is a frequent itemset. Otherwise, X is 2. Association Rules Description not a &amp;pent itemset.

0 Generate strong association rules from frequent itemsets. For every frequent itemset A, if BcA, B#0, and support (A)/support (B) 5 minconf, then we    and support (A)/support (B) 5 minconf, then we Given a transaction database DB, I=[il, i2, i l . .  .in] is a 0-78034403-2/04/$20.00 &amp;?OM IEEE 1 577 26-29 August 2004 have association rule B a  (A-B).

The second step is relatively easy, and its algorithm generation can be found in the reference [21 The present focuses in research are to find highly efficient algorithms [3] in the first step. In this paper, we discuss how to mine association rules quickly.

3. The algorithm analysis and improve In Apriori algorithm, from Ck to Lk have two steps 0 Pruning itemsets according to 4 . 1 0 Pruning itemsets according to missupport The question of Apriori algorithm is that the set of candidate itemsets Cr is generated from Lk.,. Every itemset in Cr is tested whether its all k-l subsets constitute a large k-litemset or not, if one of k-litemsets is not in 4 . 1 itemsets, the super itemset of this k-l itemset can be deleted. That is, every time a k itemset is constituted, Apriori must scan all 4 . 1  itemsets.

Analyzing the process of mining rules, we find out a way to reduce the times of scanning frequency itemsets Cr.

The theory can be described as below: Theorem 1: Support XET, YET, and XEY. If Y is a frequent itemset, then X must be a frequent itemset. That is (VX)(VY)((YcT A XEY A Y.count 2 mincount)+(X.count 2 mincount ) ) Where X.count and Y.count stands for X's count and Y's count in the database, and mincount is the minimum suppolt degree count.

Proof: Let T=(tl,t2 ,._., t,) be records of D. According to the definition of the minimum support degree, we get the formulation mincount=minsup*IDI. Since Y is a frequent itemset, there are at least mincount records included Y.

Since XrY, there are at least mincount records included X from the transitivity of the inclusive relationship. So, X.counter 2 mincount. That is, X must be a frequent set.

Theorem 2 Given XcT, YcT, and XrY. If X is a non-frequent itemset, then Y must be a non-frequent itemset. That is (VX)(VY)((YcT A XcY A X.count &lt; mincount)+(Y.count c mincount ) ) PrwE This theorem can be proved using disproof.

Suppose Y is a frequent itemset, that is, Y.count aincount.

Because of XcY, X must be a frequent itemset by theoreml, that is X.count hnincount. This is contradictory with

X.count &lt; mincount. Therefore, if X were a non-frequent itemset, Y must be non-frequent itemsets.[4] According to the above theory, when set of candidate itemsets Cr is generated, we scan the Lk.1 itemset only one time and test whether any LC, item X is subset of item Y in Ck or not, if true, count the number of Y itself. That is, in process: Lr., itemset we statistic the k-lsubset number of any item Y in Cr, then reduce the item in Cr according to the counting result. If the subset number of Y in Lk., is less than K. Y can be deleted from Cr. For example, let L.,=( AB, AC, BC, AE), then Ck=(ABC, ABE, ACE], the k=3, we can contribute large itemsets L, by scan Lz one time. The first item AB is subset of ABC and ABE, so record number in array is 1 l,l,O], item AC is subset of ABC and ACE, the m y  becomes {2,l,l),  finally, after scan BC and AE, the array is (3,2,2). Only item ABC has three subsets, so it remains in C3, and ABC, ACE can be deleted.

Support ILk.,I is the itemset number in Lk.,. and p = lCkl express the itemset number in Ck, every item in Ck has m subsets(m=l..ICkI). As we know, f" Ct to pruning Cr lckl m W j=1 classical Apriori algorithm need z ( I  Lx - I I) times    classical Apriori algorithm need z ( I  Lx - I I) times account, but new algorithm only need p*ILk.,I times.

Another bottleneck of Apriori algorithm: Every item in Cr need to be tested whether it can append in Lk, this process is another bottleneck of Apriori algorithm. This processing of counting the candidate itemsets spend most of the processing time. We can count candidate itemsets number quickly by contributing a hash tree data structure.[4] We use the hash tree to store Cr, every node of tree includes a itemset or a hash table (called inner node). Every hash table points to another node. The depth of root node is defined 1. A node's depth is d+l; which is in d layer.

Itemsets are stored in those leaf nodes.

Suppose n k-itemsets and average size of maner is t, matching one matter and a candidate itemset need time O(t*k). If this matter matches all candidate itemsets, need time O(n*t*k). After contributing a hash tree, the matching time only need O(n*k), because of the tree height is no more than k.

4. Experiment results We use anonymity user visiting log records of www.microsoft.com to be testing data source. In this anonymity web database, we cfioose records of user accessing website in one week.

Data snippet: A,1287,1,"International AutoRoute","lauto~ute" A.1288.1 ,"library","/library" c,"10164",10164 V,1123,1

V.lO09,l minSupport: 0.3 mincofidence : 0.7 Result of association rules mining (part 1): 26-29 August 2004 Association Rules: : ((CS610) =&gt; (CS611) 10.7727272727272727) : ((CS611) =&gt; (CS610) 10.8429752066115702) : ((CS610} =&gt; (CS612) I0.7727272727272727) : ((CS612) =&gt; (CS610) 10.8360655737704918) Time: 2281 ...........

0 Result of association rules mining (part 2): minsupport: 0.04 minCofidence : 0.15 Association Rules: (1001)=&gt;{ 1018) 4.8057227% 35.317905% (1008)=&gt;( 1009) 7.6640887% 23.135843% (1008)=&gt;(1018) 5.9154415% 17.857143% Time: 351722 .

..............

Choose confidence 50%. 70?16, and analyze curve of ppessing time with the support change. It is obvious that,  run time is reducing gradually when the support increases.

It show new algorithm is constringency and stability.

(Fignre 1, Figure 2) confidante ( I O W ) Figure 1. Time change curve I confid.lu. (50%) Figure 2. Time change curve 2 Figure3, Figure4 shows mining of the database with association rule and frequency itemset records by the new algorithm with different minimum support degree, the vertical axle shows the number of reducing records in same database. The unit is one. The horizontal axle shows 4 different suooort demees (in oercentaee).

s u o L t ( ~ m &gt; Figure 3. Reduced association rule records ~    ~ 1 1 2 3 4  5 ? P P O l t  &lt;*lo&gt; Figure 4. Reduced frequency itemset records In order to compare mining process of the database, we used the identical the minimum confidence degree (15%) in the FigureS, Figure5 shows that the new algorithm is more effect than Apriori in mining databases, the horizontal axle shows support degree and the vertical axle shows the time of minine (the unit is second).

5.

- -I Figure 5. Compares Apriori with new algorithm Conclusiuns In this paper, we analyze the Apriori algorithm of mining association rule according to the propetty of frequent items and transaction database, set forth 2 theorems of reducing the database and then put forward the new algorithm. In our test, the new algorithm is superior to the classical algorithm. According to the experiment?s results, the efficiency of improved algorithm is better and it has abroad applications in data mining.

