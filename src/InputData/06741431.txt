Sentence Completion Task using Web-scale Data

Abstract? We propose a method to automatically answer SAT-style sentence completion questions using web-scale data.

Web-scale da-ta have been used in many language studies and have been found to be a very useful resource for improving accuracy in sentence completion task. Our method employs assorted N-gram probability information for each candidate word. We also proposed back-off strategy was used to remove zero probabilities. We found that the accuracy of our proposed method  improved by 52-87% over the current state-of-the-art  Keywords?N-gram;sentence completion;Web-cale Data; Lexical Disambiguation

I.  INTRODUCTION Recently, standardized examinations such as the Test of  English as a Foreign Language (TOEFL), Graduate Record Exam (GRE) and SAT Reasoning Test, have been used for assessing in natural language processing (NLP) tasks [1], [2].

Microsoft Research published sentence completion challenge (SCC)  data for the problem of sentence-level semantic coherence [3]. The challenge was designed as a benchmark for semantic models and consists of SAT-style sentence completion problems. Given 1,040 sentences, each of which is missing a word, the task is to select the correct word from among the candidates provided for each sentence. Previous methods have used Ensemble learning with both local lexical and global coherence, such as latent semantic allocation. [4].

Web-scale data have been successfully used in many research areas, such as lexical disambiguation, machine translation and grammar error correction (Bergsma, Lin, & Goebel, 2009). Most NLP systems resolve ambiguities with the help of a large corpus of text:  (1) The system tried to decide {among, between} the two confusable words.

The larger the corpus, the more accurate the disambiguation [5]. Many systems incorporate the web count into their selection process. For the above example (1), a typical web- based system would query a search engine with the sequences ?decide among the? and ?decide between the? and select the candidate that returns the most hits. This approach would fail when more context is required for disambiguation. Bergsma, 2009 suggested  using contexts of various lengths and positions, and this method was successfully applied in lexical disambiguation. Web-scale data were used with its count information specified as features [6]. The results of previous research show that web-scale data is a very useful resource for  language research. This paper shows that using a probability feature rather than count feature can significantly improve accuracy on SCC. Additionally, rather than using a single sequence of n-gram probability, applying context of various lengths and positions from 2-gram to 5-gram improved accuracy. Moreover, rather than calculating one word?s probability given n words such as     |               , our model calculates the probability of m words given an n word sequence such as  m = 3, n = 1 m = 2, n = 2 m = 1, n = 3     |               .

The proposed back-off strategy was used to avoid cases in  which every distractor has the same probability of 0.



II. DEFINE THE PROBLEM A sentence completion task presents a sentence with one  blank which is to be filled in. Five possible words are given as options for each blank.

Stem: We took no _____ to hide it.

Distractors:  a) fault  b) instructions  c) permission  d) pains  e) fidelity  Figure 1: sample MSR sentence completion challenge data  All possible answers except the one true solution (pains) result in a nonsensical sentence. The system selects the answer by calculating coherence. Among 1,040 questions, we explore how many questions the system can correctly answer.



III. DATA SET We used the Google Web1T N-gram Count corpus1 which  contains 1 trillion words of running text and the counts for all 1 billion five-word sequences that appear at least 40 times. After words that appear fewer than 200 times are discarded, there are   1 Available from the LDC as LDC2006T13.

13 million unique words The MS SCC data were used for test data 2 . The test data includes 1,040 questions, and each equation has five distractors, with the correct answer labeled.



IV. METHOD We constructed a probabilistic language model using the  Google Web1T N-gram Count corpus. Zero probability   2 http://research.microsoft.com/en-us/projects/scc/.

problems are inevitable when considering high order N-gram probability. Thus, various N-gram smoothing techniques were studied, such as Good Turing, interpolation, and back-off [7], [8], [9]. However, retaining every count sequence is impossible in web-scale data because the quantity of data will be too large to handle. The Google N-gram count corpus also cannot be used to build previous smoothing methods because of the frequency cut-offs, which imply that N-grams appearing less than 40 times were removed; most smoothing methods require low-order word counts. The previous method has low performance (at best, an accuracy of 52%) for the sentence completion challenge (Table 2). Only half of the questions in the SCC data could be correctly answered by the previous method which implies that it is a challenging task. The task is to select the highest probability sentence among the five sentences, as shown in Figure 1.

S1) We took no fault to hide it.                  P(S1)  S2) We took no instructions to hide it.       P(S2)  S3) We took no permission to hide it.        P(S3)  S4) We took no pains to hide it.                 P(S4)  S5) We took no fidelity to hide it.              P(S5)  The previous method used the count feature to distinguish the sensible sentence among the non-sensical sentences. We applied the probability information using web-scale count information as in equation (1)      (1)  For smoothing, a proposed backoff model is used as in equation (2).

(2) where  { ?            denotes distractors .(e.g.,   = {fault,  instructions, permission, pains, fidelity}). Stem denotes the prearranged words in the question sentence (e.g., Stem = We took no _____ to hide it.).   denotes the answers the system selects. If the N- gram probabilities are zero for every distractor, we approximate them by backing off to the (N-1)-grams. We describe the notation of PN in more detail in (Table 1). Words immediately preceding and following the blank are already given in the sentence completion, so we can consider both directions from the blank and various ranges. In some sentences, words following the blank are good features for identifying the question position:  I've never _____ a word about it yet to mortal man.

Table 1: Proposed back off N-gram 5-GRAM    )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )  4-GRAM+    )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )+ )  3-GRAM    )+ )+ )+ )+ )+ ) 2-GRAM  )+         )      In other sentences, the preceding words are more useful features:  His grandfather was a royal _____, and he himself has been to Eton and Oxford.

However, we cannot predict which direction will be the crucial feature in selecting the answer. Applying only forward N-gram and applying only backward N-gram have the same accuracy (Table 3). Therefore, our proposed method for the using N-gram model considers both forward and backward N- gram probabilities and has a much reduced data sparseness problem. The 5-gram to 2-gram probability information is used to develop the proposed N-gram model. Detailed algorithms for the proposed n-gram, back-off, and sentence completion tasks are presented in Figure 2. The Ngram algorithm (Figure 2) is exactly the same as in the Table 1.



V. EXPERIMENT AND RESULT We used the evaluation tool provided in the MS SCC set.

The previously established best result of 52% was produced by a combination of the LSA total similarity and N-gram models (Zweig et al., 2012). Note that the result of  (1), (2), and the combination (1)+(2) are from Zweig?s paper. The accuracy of our proposed method is 87.4 %, which is significantly improved.

We used the unsupervised approach that does not require training data for the sentence completion task. To compare the accuracy with Bergsma?s unsupervised approach, we implemented the SUMLM method proposed for disambiguation problem (Bergsma, Lin, & Goebel, 2009). For example, from Figure 1, the following 5-gram patterns are extracted:  We took no _____ to hide it  <s> We took no Di  We took no Di hide  took no Di hide it  no Di hide it .

Di hide it . </s>  Similarly, there are four 4-gram patterns, three 3-gram patterns and two 2-gram patterns spanning the target. The words that replace the target word are called pattern fillers; F ={f1,f2?,f|F|}, F = {fault, instructions, permission,  pains, fidelity}. With |F| fillers, there are 14|F| filled patterns with a relevant N-gram count. A score for each filler is calculated by summing the log-counts of all the context patterns  produced by using that filler. Add-one smoothing is applied in the case of a 0 count. The SUMLM achieves 60% accuracy, which is higher than the current state-of-the-art. From the results of the proposed method (87.4%) and four-gram probability with Laplace smoothing (85.9%), we found that using the probability feature offers considerably better accuracy than using the count feature. The 5-gram count will always be a more critical feature than the 2-gram count. However, if we just consider count information, the bi-gram count is normally larger than the five-gram count, which leads to mis-predictions.

When applying the probability, the high order probability is normally larger than low-order probability.

To explore the effect of using forward and backward n- gram, we conducted additional experiments (Table 3). Laplace  Algorithm : Ngram (S, inx) Require: S: sent Require: inx : target index  1: for k in range(5,1,-1): 2:     for j in range(1, k+1): 3:          for i in range(k-j): 4:              d1 = S [inx-j+1:inx +k-j+1] 5:              d2 = S [inx+1+i:inx+k-j+1] 6:              d3 = S [inx+j-k: inx+j] 7:              d4 = S [inx+j-k:inx-i]  8:              M.append(                  )  9:              M.append(                  ) 10:return M   Algorithm : backoff (W) Require: W(5,40): matrix (Return values from Ngram algorithm on 5 sentences)  1:  fivegram(5,1) = W.T[0:20,:].T  (20,1) 2:  fourgram(5,1)  = W.T[20:32,:].T    (12,1) 3:  trigram(5,1)  = W.T[32:38,:].T    (6,1) 4:  bigram(5,1)  = = W.T[38:40,:].T    (2,1) 5:  if not sum(fivegram(5,1)) is 0: 6:      return fivegram(5,1) 7:  else if not sum(fourgram(5,1)) is 0: 8:      return fourgram(5,1) 9:  else if not sum(trigram) is 0: 10:    return trigram(5,1) 11: else if not sum(bigram(5,1)) is 0:  12     return bigram(5,1) 13: else: 14:     return Uniform Distribution(5,1)   Algorithm : SentComp (Q, inx) Require: Q: List of sentences Require: inx: Target Index  1: for sent in Q: 2:     M = Ngram (sent, inx ) 3:     W.append(M) 4: Result = backoff (W) 5: return argmax(Result)  Figure 2: Detailed algorithms of Ngram, backoff and sentence completion. denotes every element of the matrix is 1, Variable (n,m) denotes the size is n by m. T denotes matrix transformation. __ Underlined syntax is the same as the python syntax.

smoothing the N-gram model [10] avoids zero probability in equation (3).

?    (3)  The accuracy is the same for forwards and backwards  in every n-gram. An interesting result was that the accuracy on 5- gram count is very low (3.8 %). Because all N-grams with counts lower than 40 were discarded, most of the 5-grams counts were discarded. Thus, most five-gram have 0 probability. The 4-gram model with Laplace smoothing has very high accuracy, which indicates that the web-scale data are very useful for improving the accuracy of the SCC task.

Our proposed method has several advantages particularly well suited to SCC. First, data size is very important in the statistical approach, and the web-scale Google 1T n-gram corpus is enormous. Moreover, probability is more informative than the count feature. To treat the high order n-gram as more important than the low order n-gram, we consider various cases of high order n-gram sequence probability (table 1), then only back-off when all the distractors have 0 probability.



VI. CONCLUSION We found that using the web-scale count is the crucial  feature for sentence completion (52?60%). The accuracy is improved when using probability rather than the count feature using web-scale data (60?85.9%). We applied both backward and forward n-gram sequences probability with various ranges (85.8?  87.4%). In the future, to address long dependency features, we will explore the dependency N-gram features to further improve accuracy.

