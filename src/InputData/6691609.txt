Elver: Recommending Facebook Pages in Cold Start Situation Without Content Features

Abstract?Recommender systems are vital to the success of online retailers and content providers. One particular challenge in recommender systems is the ?cold start? problem. The word ?cold? refers to the items that are not yet rated by any user or the users who have not yet rated any items. We propose ELVER to recommend and optimize page-interest targeting on Facebook. Existing techniques for cold recommendation mostly rely on content features in the event of lacking user ratings. Since it is very hard to construct universally meaningful features for the millions of Facebook pages, ELVER makes minimal assumption of content features. ELVER employs iterative matrix completion technology and nonnegative factorization procedure to work with meagre content inklings. Experiments on Facebook data shows the effectiveness of ELVER at different levels of sparsity.

Index Terms?Recommender system, Facebook, Sparse matrix, Social media, Behavioral targeting

I. INTRODUCTION  Recommender systems automate the process of suggesting interesting items to users based on their explicit and implicit preferences. The user preferences are perceived from their and similar users? activity history [1]. For example, Netflix uses recommender systems (instead of choosing at random) to suggest movies that a user might be interested in watching.

Three components are widely assumed and thought to be indispensable for recommender systems. They are users, items, and users? ratings for the items. Assuming the aforementioned three components, two well-established approaches are widely used in constructing a recommender system: collaborative filtering and content-based filtering.

Collaborative filtering makes recommendation by consider- ing inter-user similarities [2][3]. Collaborative filtering works well at web scale. However, one common difficulty in col- laborative filtering systems is the ?cold start? problem. The word ?cold? refers to the items that are not yet rated by any user or the users who have not yet rated any items [4]. ?Cold start? refers to the problem of recommendation involving cold items or users, which is common at early stage of the system.

?Cold start? problems are well studied in the past decade [2][1][4]. Despite differences in algorithms and models, most existing approaches choose to rely on content-based filtering techniques to overcome cold starts [5][6].

Content-based filtering represents a user by the descriptive contents of the items rated by the user. For example, in the case of movie recommendations, a piece of descriptive content could be the movie?s leading actor or director. A major benefit  of using content-based filtering in recommender systems is its effectiveness in handling so-called ?cold items?. Since content- based filtering relies on content features and not the rating, the lack of user ratings will not affect the system. However, purely content-based filtering could potentially destroy the diversity among the recommended items. For example, consider a user watches 5 James Bond movies. Then content-based filtering would recommend the rest 18 James Bond movies and other action movies. As a result, the recommender system fails to give diverse recommendations for the user to explore different genres.

In addition to the diversity problem, a basic underlying assumption in handling cold items with content-based filtering is that any well-rated item, as well as a cold item, must be well represented in the same high-dimensional feature space. Feature extraction from item contents has never been a problem with items such as movies because each valid movie would have dozens of actors/actresses, which are plenty to make a good, discriminative feature vector.

A cold start situation becomes more challenging when a good feature space is unavailable. Because their content features are easy to extract, movies and music are the nice cases of the cold start problem. On Facebook, people express their interests and preferences via entities beyond music and movies. For example, on Facebook people can form groups based on very broad range of concepts. It is not surprising that there are group pages for celebrities like Justin Bieber1.

But many more group pages are about far less solid concepts such as Kids Who Hid in Dep?t Store Clothing Racks while their Mom Was Shopping2. Since Facebook has so many group pages with unprecedented volume (more than 10 million by business owners alone) and variety (virtually unclassifiable as illustrated above), designing a recommender system that suggests interesting group pages is very challenging.

We are particularly interested in the cold start scenario for recommending Facebook group pages since it bears significant commercial interests. Suppose that a small local retailing business, called Crown Jewel, decides to join Facebook and to start its own page in order to engage more potential customers.

Facebook allows Crown Jewel to target its ads to users based on what pages they like (i.e. behavioral targeting). It is  1https://www.facebook.com/JustinBieber 2https://www.facebook.com/pages/kids-who-hid-in-dept-store-clothing-  racks-while-their-mom-was-shopping/10150138929940447      beneficial for Crown Jewel to consult a recommender system to find out which pages to target. Admittedly, recommending Facebook pages would not be an entirely new challenge if the ad placer is Walmart or Target because they are large brands and already have many fans. Existing collaborative filtering techniques would work well in such a scenario. But, to help Crown Jewel with its goal, a recommender system has to solve the cold start problem. Existing methods for the cold start problem, to the best of our knowledge, are all targeted for and tested on movie benchmark data and therefore make heavy use of movie?s rich content features.

A. Our Contribution  We propose ELVER algorithm for recommending cold items from large, sparse user-item matrices. ELVER differs from existing cold recommendation techniques in two aspects. First, ELVER recommends and optimizes page-interest targeting on Facebook instead of movies or songs. The large scale of a social network like Facebook influences ELVER?s design. We find that similarity-based user stratification can both improve the speed of ELVER as well as its accuracy. Second, ELVER makes minimal assumption of content features. Existing tech- niques for cold recommendation mostly rely on content fea- tures in the event of lacking user ratings. Traditional items (e.g., movies or music) have rich, organized content features like actors, directors, awards, etc. Since Pages on Facebook ideologically vary so much from each other, it is very hard to construct universally meaningful features for the millions of Facebook pages. ELVER uses iterative matrix completion technology and nonnegative factorization procedures to work with meagre content inklings and avoid heavy requirement on rich content features.

We conduct experiments on real Facebook pages (22.6K) with activities from real users (27.4M). To illustrate the effectiveness of ELVER in handling cold pages, three different levels of sparsity (99.9%, 99.99%, and 100.00%) in the user activities are tested.



II. RELATED WORK  Related work to our paper can be roughly divided into two categories: notable existing solutions to the cold start problem, and non-negative matrix factorization (NNMF) algorithms.

A. Cold Start Recommendations  [6] represents a class of classical methods for dealing with cold-start problems. Specifically dealing with movie recom- mendations, [6] assumes that the content of each movie is represented by the participating actors in the movie. Other later works such as [2] and [1] follow a similar path, although each work motivates very different models for the underlying user preference variable(s). A key reason for their success is the fact that actors are expressive and discriminative features for the movie contents. As a consequence, a major difficulty in applying such techniques to the problem of Facebook page cold recommendation is that the available contents from each item are either not descriptive enough or not universal enough  to make the modeling work. It can be very difficult to extract features that are both descriptive and universal for Facebook pages.

B. Non-Negative Matrix Factorization  Non-Negative Matrix Factorization (NNMF) [7] is a family of techniques to factorize a matrix V into two matrices W and H . Given a nonnegative matrix V ? Rm?n and an integer k (0 < k < min{m,n}), the NNMF goal is to find nonnegative matrices W ? Rm?k and H ? Rk?n such that the objective  f(W,H) =  ||V ?W ?H||2F (1)  is minimized [7]. Most NNMF algorithms solve Equation 1 by iterative steps, which require initialization of W and H . Efforts have been made in seeding W and H with some meaningful values [7].



III. PROBLEM STATEMENT  Suppose that V is a n-by-m user-page matrix and V (i, j) ? {0, 1, 2, 3, . . .} for 1 ? i ? n and 1 ? j ? m. V (i, j) is determined by the number of comments left on the j- th page by the i-th user. Recall that our motivation is to perform recommendations for a cold page that has just entered the Facebook ecosystem. Without loss of generality, assume that the cold page of concern is in the 1st column, which is referred to as V (:, 1). In addition to V , our formulation also includes content inkling. Content inkling is different from traditional content feature by its general lack of discriminative or descriptive nature. Only three pieces of content inkling are assumed for each page: 1) page likes received from fans, 2) the category assigned to the page by Facebook, and 3) the age group, with which the page is most popular.

Our goal can be expressed as recommending indices j for 1 < j ? m such that, for all suggested j, V (:, j) is targetable for V (:, 1) to acquire fans.



IV. THE ELVER ALGORITHM  ELVER works with three matrices: the user-page matrix V , the content-page matrix H , and the user-content matrix W . ELVER consists of 4 basic algorithmic steps: 1) encoding content inkling, 2) initializing the user-page matrix V into V? , 3) user stratification, and 4) a stochastic Expectation Maximization algorithm that iteratively updates V? and H .

Eventually, recommendations for a cold Facebook page are made from the converged V? .

A. Encoding Content-page Matrix From Content Inkling  The three pieces of inkling assumed available for a Face- book page are inconvenient to work with in numeric matrix operations. Further encoding is therefore needed in H , which is our notation for the content-page matrix.

First, the number of page likes from fans, H(1, :), is already in integer form. But the actual numbers can vary so greatly from page to page that they cannot be directly used. For exam- ple, pages for business like Crown Jewel in our example may have a few hundred likes while global pages such as Coca-Cola     have over 50 million likes. Simply normalizing and putting such vastly different numbers in H will effectively set most entries to be 0. We use log(number of page likes) instead.

Second, the category H(2, :), to which the page belongs, is a categorical variable. Distinct categories can be mapped to discrete integer values. There are two choices to be made here.

One is which categories should get greater/less integer values.

The other is whether similar categories should be assigned to closer integer values. Third, the age group, H(3, :), should also be encoded into real numbers, since different age groups can be compared and ordered like integers.

Three additional statistically derived [8] (from the first three inklings above) inklings are also be part of H: H(4, :) = H(1, :)?H(2, :), H(5, :) = H(1, :)?H(3, :), and H(6, :) = H(2, :)?H(3, :). Note that H in this paper is always 6-by-m in dimension.

B. Initializing User-page Matrix  The original user-page matrix V is incomplete from two aspects. First, its cold column V (:, 1) contains few or no known entries. This aspect of incompleteness defines the cold start problem: it is our goal to provide estimates for the values in V (:, 1). The second aspect is the distinction between a zero entry and an unknown entry. It is well known that in the Netflix problem, some users do not rate certain movies not because they are uninterested in giving a rating after watching the movie (i.e. a zero entry), but because they are never aware of the movies? existence (i.e. an unknown entry). The same situation exists with users and pages on Facebook.

The initialization of V is the process of finding the most likely matrix V? , based on nonzero entries in V [9]. For doing so, ELVER uses matrix completion technologies [10] to a version of V? such that V? is in accordance with V and is low-rank. The exact optimization goal is given below:  minimize ||V? ||? subject to  ? ? (i,j)??0  |V? (i, j)? V (i, j)|2 < ?,  where ?0 = {(i, j)|1 ? i ? n, 1 ? j ? m} \{(i, 1)|V (i, 1) = 0}.

(2)  In Equation 2, || ? ||? denotes matrix nuclear norm.

Completing matrix under minimal nuclear norm constraint  is a member of the family of convex cone problems. Fast, first-order algorithms for solving such convex cone problems can be constructed from general templates [10].

C. User Stratification in V?  User stratification is the process of dividing users into indi- vidual strata, on which independent analysis and estimation are carried out. We say that S = {s1, s2, . . . , s|S|} is a possible stratification of the users if ?|S|i=1si = {1, 2, . . . , n} and si ? sj = ? for i ?= j. Since each user is represented as a unique row in V? , stratification can be done by clustering the n rows of V? into |S| non-overlapping clusters. We use K-means to find the clusters. In the experiments, we vary |S|, the number  Algorithm 1: User stratification and Stochastic Expectation-Maximization (EM) algorithm in ELVER  1 {V?1, V?2, . . . , V?|S|} ? kmeans(V? , |S|, ?cosine distance?) 2 for V?i ? {V?1, V?2, . . . , V?|S|} do 3 W (0) ? V?i ?H? ? (H ?H?)?1; H(0) ? H 4 for j ? {1, 2, . . . , itrmax} do 5 V?i  (j) ?W (j?1) ?H(j?1) 6 W (j), H(j) ? nnmf(V?i(j)) using H(j?1) as  initial guess for H(j)  7 end 8 Approximately, V?i  (?) ? V?i(j) 9 end  10 return V? (?) ? [ V?1  (?) V?2  (?) . . . V?  (?) |S|  ]?  of clusters, and evaluate their performance with n. Cosine distance is used to measure the distance between V? (u, :) and V? (v, :).

Suppose that non-empty clusters are discovered as user  strata, we can represent V? as [ V?1 V?2 . . . V?|S|  ]? .

D. Stochastic EM algorithm After encoding H , initializing V? , and stratifying the V? into  V?i?s, ELVER employs a stochastic Expectation-Maximization (EM) algorithm to search for V? (?), the converged user-page matrix. Algorithm 1 describes the stochastic EM algorithm.

In Figure 1, we visualize the convergence of V? over 500 iterations. The error rate at the ith iteration of the jth stratum is measured between V?j  (i?1) and V?j  (i) . The error rate between  V? (?) and V is not shown in Figure 1 but in Figure 2.

The matrix multiplication in line 6 of Algorithm 1 is  interpreted as the E(xpectation) step. The non-negative fac- torization in line 7 of Algorithm 1 is interpreted as the M(aximization) step. The factorization algorithm is not exact simply because V?i  (j) is not guaranteed to be a product of  two nonnegative matrices W (j) and H(j). By minimizing the objective function in Equation 1, the factorization maximizes the ?fit? of W (j) and H(j).

EM algorithm is designed for fitting incomplete data to the models. Similarly, the iterations in Algorithm 1 try to recover missing values in V? and H .



V. EVALUATION METRICS A. Mean Absolute Error  ?Normalized Mean Absolute Error? (NMAE) is used in [1] as a performance metric for recommender systems. NMAE is a metric that uses ?Mean Absolute Error? (MAE), which is also a standard evaluation metric [11]. MAE can be calculated through user averaging:  MAEu =  n  n? i=1  ? ? wi m  ?n k=1 wk  m? j=1  |V? (i, j)? V (i, j)| ? ? ,  (3)     (a) (b)  Fig. 1. Stratum-wise convergence of the V? user-page matrix over 500 iter- ations. (a) Convergence forV?1  (1?500) at 99.99% sparsity; (b) Convergence  forV?2 (1?500)  at 99.99% sparsity.

where wi is a weights assigned to each user. Another important component in NMAE is MAEb, the MAEu score when pre- dictions on user preferences are made by a baseline algorithm.

Then NMAE is defined as NMAE = MAEu/MAEb.

Both MAEu and NMAE are built around users because, in a traditional recommender system setting, users are the only party that the system needs to please. Recommender systems have always been user-centric. But the situation presented in our running example of Crown Jewel is very different. In our case, Facebook wants to recommend the right pages for Crown Jewel so that targeting them can convert the most users to fans of Crown Jewel given the same budget. This becomes an entirely different problem and therefore needs a better-suited metric in addition to MAE-based ones.

B. Using Kendall ? -statistics  The system?s goal is to recommend an ordered list of pages for Crown Jewel to target. Crown Jewel would be interested in getting the pages to target in the right order, based on which the campaign manager can go down the list as far as he/she likes or his/her funds allow.

The computation of the new performance metric consists of two steps. First, it needs to assign a rank to each page column V? (:, i) for 2 ? i ? m. We assign the rank based on V? (:, i)?s distance from V? (:, 1). Five different ways of measuring inter- column distance are applied and compared. They are L1-norm, L2-norm, LInf-norm, Mean Square Error, and Frobenius Norm.

The second step is to compare obtained rankings Ri for 1 ? i ? 5 (one for each distance measure) with the optimal ranking order Ro. Ro is determined apriori and experimentally. We use Kendall?s tau(? ) coefficient [12] to measure how similar Ri is to Ro. The coefficient range is ?1 ? ? ? 1, where 1 implies perfect agreement between rankings.



VI. EXPERIMENTS  A. Dataset  In the experiments, the data collected over 2012 is used [13]. Table I shows more details. Simple rules are applied to filter out potential spam users: we remove a user from the experiments if he/she is active on over 100 pages or is only active on 1 page.

TABLE I DATASETS SUMMARY STATISTICS  Statistic Available Used in paper Unique pages 32K+ 22,576 All user activities across all pages 10B+ 226M Unique users 740M+ 27.4M  B. Masking Entries in V (:, 1)  In order to evaluate the ELVER algorithm, we place a non- sparse column at V (:, 1), then masking most of its entries.

This scheme can simulate a cold page at V (:, 1) and can still reliably evaluate how well ELVER performs since we know the hidden values in V (:, 1). Three different levels of sparsities in V (:, 1) are tested in our experiments: 99.9%, 99.99%, and 100.00%. These numbers are chosen because traditional ?cold start? settings often choose between 95% 99.9% sparse. We emphasize that the 100.00% sparsity simply indicates that V (: , 1) is an empty column.

In this section, we use the notation E(x,y) and B(x,y) respectively to denote the results when ELVER and Random Bot baseline algorithm are applied to V with x-level sparsity in V (:, 1) and y user strata.

C. Random Bot Baseline Method  A Random Bot algorithm [1] is used as the baseline ap- proach in our experiments. The algorithm injects user activity values, generated from random variables, in V (:, 1). There are |S| Random Bots, X1, X2, . . . , X|S|, for the |S| user strata.

Vi(:, 1) is filled by values drawn from Xi ? N (??i, s2i ), where  ??i =  n  ni? j=1  xj , s i =   n? 1 ni? j=1  (xj ? ??i)2,  and x1, . . . , xni are existing activity values in the ith stratum of V .

TABLE II KENDALL ? -STATISTIC BASED ACCURACY OF RECOVERING USER-PAGE MATRIX V? FROM 99.9% SPARSITY IN V (:, 1) UNDER VARIOUS NORMS  AND USING DIFFERENT NUMBERS OF USER STRATA.

User Strata  L1- norm  L2- norm  LInf- norm  Mean Square Error  Frobenius Norm  1 0.543 0.543 0.543 0.543 0.543 2 0.608 0.608 0.608 0.556 0.608 3 0.386 0.346 0.330 0.346 0.346 5 0.438 0.438 0.438 0.438 0.438 8 0.333 0.307 0.229 0.307 0.307 10 0.281 0.111 0.111 0.111 0.111 20 0.124 0.111 0.111 0.085 0.111 30 0.163 0.085 0.085 0.085 0.085  D. Quantifying Results with Mean Absolute Error  Figure 2 shows how accurately the ELVER algorithm can recover the entries in V (:, 1). On one hand, the error rates are consistent with the sparsity levels as the overall error rates decrease as sparsity decreases, which basically shows that the ELVER algorithm can effectively make use of the sparse ratings in V (:, 1) when they are present. On the other     (a) (b) (c)  Fig. 2. Recovering accuracy of ELVER algorithm, compared to random baseline prediction (MAEb) under 3 different levels of sparsity of V (:, 1): 99.9%, 99.99%, and 100.00%. Three mean absolute error rates are used: MAEr , MAEu, and NMAE.

hand, the error rates vary less monotonically with the number of user strata chosen. As shown in Figure 2, 1 stratum, 2 strata, or 3 strata seem to be good choices that can find balance between capturing inter-user similarity/inter-stratum distinction and avoiding sparsity in small stratum.

E. Quantifying Results with Kendall ? -statistics  Table III provides the Kendall statistics for three config- urations E(100.00, 2), E(99.99, 1), and baseline results for 99.99% sparsity. Both configurations of the ELVER algorithm have significantly higher ? values than the baseline, implying that our rankings are closer in agreement compared to original true rank Ro. Figure 3 presents results of ? -statistics with the ELVER algorithm applied to more configurations of different numbers of user strata, distance measure, and data sparsity.

? -statistics metric can really tell apart the performance in random bot baseline and ELVER. Stratification is also effective in improving ? -statistics for ELVER.

TABLE III KENDALL ? RANK CORRELATION TABLE SHOWING DETAILED  STATISTICS FOR RANKINGS BY E(100.00,2), E(99.99,1) AND B(99.99,2) UNDER THE L1-NORM MEASURE.

Measure E(100.00,2) E(99.99,1) B(99.99,2) Kendall ? -statistic 0.2679 0.3595 -0.0075 2-sided p-value 0.1297 0.02803 0.9396 S, Kendall Score 41 59 -3 Var (S) 697 697 697 S/? , Denominator 153 153 153  (a) (b) (c)  Fig. 3. ? -statistic-based accuracy of recovering user-page matrix V? under 3 different levels of sparsity of V (:, 1): 99.9%,99.99%, and 100.00%. Re- sults are shown in L1-norm, L2-norm, LInf-norm, Mean Square Error, and Frobenius Norm.



VII. CONCLUSION AND FUTURE WORK  Most existing techniques for cold recommendation mostly rely on content features in the event of lacking user ratings.

ELVER makes minimal assumption of content features: It is very hard to construct universally meaningful features for the millions of Facebook pages. ELVER uses iterative matrix completion technology and nonnegative factorization procedure to work with meagre content inklings and avoid the heavy requirement on rich content features. Our experiments on Facebook data have confirmed the effectiveness of ELVER at three different levels of sparsity.

In the future, we plan to improve ELVER?s cold recommen- dation by incorporating the network information among social users, which is largely ignored by existing studies.

