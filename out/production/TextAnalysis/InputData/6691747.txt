MapReduce Implementation of Variational Bayesian Probabilistic Matrix Factorization Algorithm

Abstract?We introduce in this paper a scalable implementation of Variational Bayesian Matrix Factorization method for collaborative filtering using the MapReduce framework. Variational Bayesian methods have the advantage of providing good approximate analytical solutions for the posterior distribution. Due to the independence assumption about the parameters in the posterior distribution, variational methods are also likely to be able to parallelize efficiently. Though Variational Bayesian Matrix Factorization method has shown to produce more accurate results in collaborative filtering, its scaling properties have not studied so far. We ran our MapReduce implementation on the CiteULike data set and show that our parallelization scheme achieves approximately linear scaling. We also compare its performance with the MapReduce implementation of a popular matrix factorization algorithm, ALSWR, from the open source machine learning library Mahout.

Index Terms?Variational Bayesian Matrix Factorization, Probabilistic Matrix Factorization, MapReduce, Distributed Computing, Collaborative Filtering, Recommendation Systems

I. INTRODUCTION  Matrix factorization is a very popular technique used for Col- laborative Filtering to generate personalized recommendations in E- commerce. Its basic idea is to approximate the user-item transaction matrix X as a product of two low-rank matrices U and V  X ? UV T (1) If the X matrix has dimensions M ? N then U and V will have dimensions M ? K and N ? K respectively. Here K << M,N represents a set of hidden features characterizing the factors that drive consumer preferences. In E-commerce scenarios, such as major on- line retailers, typically M and N would be of the order of Millions.

Also the matrix X would be highly sparse with only less than 10% filled by known values. This makes the above matrix factorization task highly non-trivial from the point of view of both computation time due to scale and accuracy due to over fitting.

In the last decade, several machine learning methods have been developed for improving the accuracy of sparse matrix factorization.

Well known examples are Weighted Non- negative Matrix Factoriza- tion (WNMF) [1], Alternate Least Square with Weighted Regular- ization (ALSWR) [2] and Variational Bayesian Matrix Factorization (VBMF) [3] to name a few. Hernndez-Lobato and Ghahramani have shown that VBMF, compared to other matrix factorization techniques, is more robust to over fitting, and generates more accurate predictions at the long tail in E-Commerce [4]. VBMF can also be used as a building block in most complex probabilistic models [5]. Though the variational Bayesian methods have been used in many contexts, use of variational approximation for scaling the machine learning models for modern day massive data sets have not been studied in detail.

Nallapati et. al. have studied the speed and scalability of parallelized variational EM for LDA [6]. Zhai et. al. have implemented variational inference based LDA model using MapReduce [7]. In this paper the  authors argue that variational methods are very flexible for MapRe- duce type of parallelization because under variational approximation the posterior distribution factorizes and hence one can compute the sufficient statistics for different parameters independently. To our knowledge there are no work reported on the MapReduce paral- lelization of VBMF. Implementation of VBMF using MapReduce parallelization scheme will make it useful for applying in commercial scenarios and also in understanding the scalability of variational machine learning methods in general.

In this paper we describe the MapReduce implementation of Vari- ational Bayesian Matrix Factorization method, which is our original contribution. We apply our implementation on the CiteULike data set to study the scaling properties. We also compare its performance with the MapReduce implementation of ALSWR in the open source machine learning library Mahout1.

Rest of the paper is organized as follows. In section II we will describe very briefly the VBMF method. Details of MapReduce implementation is given in section III. In section IV we describe the details of Hadoop infrastructure used for this study and approaches used for tuning the performance. In section V we present the results on the use of our MapReduce implementation of VBMF on CiteULike data set. Summary and conclusions from this work are presented in section VI.



II. VARIATIONAL BAYESIAN PROBABILISTIC MATRIX FACTORIZATION  Variational approximation is a relatively new approach to estimate the posterior distribution in a Bayesian inference problem [8],[9].

Here the idea is to propose an approximate posterior distribution which has a factorized form and is parameterized by a set of variational parameters. One then minimizes the Kullback-Leibler di- vergence between the variational distribution and the target posterior by using the variational parameters to arrive at a good approximate solution for the posterior distribution. Raiko et. al. [3] were the first to use variational method for matrix factorization type of problems.

Here we describe their method briefly, for more details readers may read the paper of Raiko et. al.

We asume the following likelihood function for X , conditional on (U, V ), given by the product of Normal distribution for each of its elements  P (X|U,V) = M? i=1  N? j=1  N ( xi,j |ui ? vTj , ?2x  ) (2)  where ui and vj denotes the i-th and j-th rows of matrices U and V respectively. And ?2x is the noise parameter in observations in X  1Apache Mahout, Open Source Machine Learning Library, http://mahout.apache.org/       and is treated as a hyper-parameter. Also we assume the following factorized prior distributions for U and V respectively  P (U) =  M? i=1  K? k=1  N (uik|u?pik, u?pik) (3)  P (V) =  N? j=1  K? k=1  N ( vjk|v?pjk, v?pjk  ) (4)  Here u?pik and v? p jk are the prior variances for the elements of U and  V respectively. The mean prior values u?pik and v? p jk are usually taken  to be zero.

Using Bayes rule, the posterior distribution for U and V is given  by  P (U,V|X) = P (X|U,V)P (U)P (V) P (X)  (5)  To compute the posterior distribution on the L.H.S of equation (5), Raiko et. al. used the following approximate variational posterior distribution Q (U,V)  Q (U,V) = [ M? i=1  K? k=1  N (uik|u?ik, u?ik) ]  ? [ N? j=1  K? k=1  N (vjk|v?jk, v?jk) ]  (6)  Here u?ik, v?jk, u?ik and v?jk are variational parameters whose values are found by minimizing the Kullback-Leibler divergence between Q (U,V) and P (U,V|X) given by  KL [Q||P ] = ?  Q (U,V) ln Q (U,V)  P (U,V|X)dUdV (7)  The prior variances u?pik and v? p jk , and variance parameter ?  x of the  observations are also found by minimizing the KL divergence.

In Raiko et. al. [3], this minimization for finding the values of u?ik,  v?jk, u?ik, v?jk, u?pik and v? p jk was cast in the form of a set of coupled  equations which needs to be solved iteratively as given below.

1) Computation of the variances u?ik and v?jk according to the  equations  u? (t)  ik = [   u? p (t?1) ik  + ?  j|xij?O  (v? (t?1)  jk ) 2 + v?  (t?1) jk  ? 2(t?1) x  ]?1 (8)  v? (t)  jk = [   v? p (t?1) jk  + ?  i|xij?O  (u? (t?1)  ik ) 2 + u?  (t?1) ik  ? 2(t?1) x  ]?1 (9)  here the summation condition i, j | xij? O implies summation over all values for which the (i, j)th entries in the X matrix contains observables (not missing values).

2) Computation of the means u?ik and v?jk according to the equations  u? (t)  ik = u? (t?1)  ik ? ? ( d2C  (t?1) KL  du?2ik  )?? ( dC  (t?1) KL  du?ik  ) (10)  v? (t)  jk = v? (t?1)  jk ? ? ( d2C  (t?1) KL  dv?2jk  )?? ( dC  (t?1) KL  dv?jk  ) (11)  Here CKL is the cost function (KL divergence defined in equation (7) ). A closed form expression for CKL can be found by substituting equations 5 and 6 in in equation 7 and carrying out the integration over all the variables. The resulting  expression can be trivially written as a sum of 3 cost functions for convenience as follows.

CKL = C X KL + C  U KL + C  V KL (12)  The individual cost functions are given by  CXKL =  2?2x  ? i,j|xij?O  [( xij ?  K? k=1  u?ikv?jk  )2 +  K? k=1  ( u?ikv?  jk + u?  ikv?jk + u?ikv?jk  ) + ?2xln  ( 2??2x  )] (13)  CUKL =   ? i,k  [( u?2ik + u?ik u?pik  ) ? ln u?ik  u?pik ? 1  ] (14)  CVKL =   ? j,k  [( v?2jk + v?jk v?pjk  ) ? ln v?jk  v?pjk ? 1  ] (15)  Expression for the first derivative of CKL is given by  dCKL du?ik  = ?  j|xij?O  ( xij ?  ?K l=1  u?ilv?jl ) v?jk ? u?ikv?jk  ?2x  ? u?ik u?pik  (16)  dCKL dv?jk  = ?  i|xij?O  ( xij ?  ?K l=1  u?ilv?jl ) u?ik ? u?ikv?jk  ?2x  ? v?jk v?pjk  (17)  The second derivative of CKL is given by the inverse of variance  d2CKL du?2ik  = u??1ik (18)  d2CKL dv?2jk  = v??1jk (19)  3) Computation of the prior variances u?pik and v? p jk using the  equations  u? p (t) ik = u?  (t?1) ik +  ( u?  (t?1) ik  )2 (20)  v? p (t) jk = v?  (t?1) jk +  ( v?  (t?1) jk  )2 (21)  4) Computation of observation variance ?2x  ?2x =  | X | ?  i,j|xij?O  [( xij ?  K? k=1  u?ikv?jk  )2 +  K? k=1  ( u?ikv?  jk + u?  ikv?jk + u?ikv?jk  )] (22)  where | X | is the number of observations (non-missing values) in X .

5) Computation of the new cost function CKL using equations (12 - 15)  6) If CKL converges sufficiently stop the iteration or else continue with the iteration with step 1  These iteration steps can be schematically represented as in Figure 1     Fig. 1. Update Scheme of VBMF Equations  Fig. 2. Block Structure used for X Matrix for Parallelization

III. MAPREDUCE IMPLEMENTATION OF VARIATIONAL BAYESIAN PROBABILISTIC MATRIX FACTORIZATION  In this section we present our MapReduce implementation of the Variational Bayesian Matrix Factorization method described in section II. First we describe how we have partitioned the various matrices to store them distributed in the Hadoop framework. Then we describe how we have implemented the computation of the different parameters at each iteration step (8 - 22) using MapReduce steps.

From equations (2 - 6) it is clear that we have the following matrices used in the computation.

1) Observation matrix X of size M ?N 2) Matrices U? , U? , U?p each of size M ?K 3) Matrices V? , V? , V? p each of size N ?K  In the rest of the paper, for convenience, we refer to the matrices U? , U? and U?p as the set of U matrices and V? , V? and V? p as the set of V matrices. Since the observation variance ?2x is same for all elements of X , we are not using a matrix representation for it.

To efficiently distribute the observation matrix X and the set of U and V matrices, we use the following blocks structure for these matrices as shown in Figures 2 and 3. The scheme we used for splitting is explained below in detail.

The X matrix of size M ? N is splits into blocks XB each of  Fig. 3. Block Structure used for U and V Matrices for Parallelization  size MB ?NB . Each block is indexed using I and J where I goes from 1 to M? = M  MB and J goes from 1 to N? = N  NB . The set of  U matrices and set of V matrices are also split into blocks but in a slightly different way. Since usually K << M,N , for these matrices all the blocks have width K, which implies the entire row is in one block. We use index I to represent blocks of U matrices and iB to represent rows within a block. Similarly J to blocks of V matrices and jB to represent rows with in a block. Since every element of a row is in a single block, we can use the row vectors ?ui and ?vi to represent the rows of U and V matrices respectively. The height of each block (number of rows it contains) would be MB for the set of U matrices and NB for the set of V matrices respectively. The value of MB and NB are chosen according to M,N and configuration of the Hadoop cluster such that a balance is obtained w.r.t distribution of the data and network latency. This is discussed more in section (IV).

Since the values of M,N,MB , NB , M? , N? are constants these can be stored in the Hadoop global cache and supplied to all the computing nodes. Similarly values of ?2x and cost functions CXKL, C  U KL, C  V KL  after each iteration can be stored in global cache and supplied to the computing nodes for the next iteration.

For Distributing the X Matrix: ? Map 1:  Inputs: 1) i, j,Xij (from input file)  Method: 1) I = i  MB + 1  2) iB = i? (I ? 1)MB 3) J = j  NB + 1  4) jB = j ? (J ? 1)NB 5) Key = [?X?, I, J ] 6) V alue = [iB , jB , Xij ] 7) Emit(Key, V alue)  ? Reduce 1: None The above MapReduce operation will give output HDFS files  containing (Key, Value) pairs where the block index (I, J) associated with every element (i, j) of the original matrix X will be part of the ?Key?. And the row and column number within each block, (iB , jB), and the element Xij will be part of the ?Value?. This will ensure that Hadoop will store all the elements of a particular block (I, J) together.

Similarly splitting of the set of U matrices and the set of V matrices can be done by the following MapReduce operations. The method is shown only for U? and V? matrices, and is similar for U? , U?p  and V? , V? p matrices.

For Distributing the U? Matrix: ? Map 2A:  Inputs: 1) i, ??ui (from input file containing initial values)  Method: 1) I = i  MB + 1  2) iB = i? (I ? 1)MB 3) Key =  [ ?U??, I  ] 4) V alue =  [ iB , ??ui  ] 5) Emit(Key, V alue)  ? Reduce 2A: None In this case, in the output HDFS files, the block index I will  be part of the ?Key?. And the row number within each block, iB , and entire i-th row of U? , ??ui, will be part of the ?Value?. The MapReduce operations for splitting U? , U?p matrices will be similar and we denote them by (Map 2B, Reduce 2B) and (Map 2C, Reduce 2C) respectively.

For Distributing the V? Matrix: ? Map 3A:  Inputs: 1) j, ??vj (from input file containing initial values)  Method: 1) J = j  NB + 1  2) jB = j ? (J ? 1)NB 3) Key =  [ ?V? ?, J  ] 4) V alue =  [ jB , ??vj  ] 5) Emit(Key, V alue)  ? Reduce 3A: None In this case also, in the output HDFS files, the block index J  will be part of the ?Key?. And the row number within each block, jB , and entire j-th row of V? , ??vj , will be part of the ?Value?. The MapReduce operations for splitting V? , V? p matrices will be similar and we denote them by (Map 3B, Reduce 3B) and (Map 3C, Reduce 3C) respectively.

Theses MapReduce operations can be used to convert X matrix values and initial values for U and V matrices from flat files to HDFS files. To write the update equations (8-22) in MapReduce form, we will rewrite them in a different form so that Map and Reduce operations can be easily defined.

Computation of Variance of U and V : Equation (8) for updating the elements of U? matrix can be rewritten as  ??ui = [(  ??upi )?1  + ??u?i ]?1  ??u?i = ? J  ??uJi  ??uJi = ?  j|j?J&xij?O  ??v2j + ??vj  ?2x  (23)  where the summation in the equation for ??uJi is only over j values in the block J satisfying the condition xij ? O. From here onwards we have suppressed the iteration number index t for simplicity.

Computation of ??ui can now be implemented as a 2 step process where first we compute ??u?i using the first MapReduce operation and then compute ??ui using ??u?i and ??u  p i through a second MapReduce  operation.

? Map 4A:  Inputs:  Fig. 4. MapReduce scheme to update elements of U?  1) (Key, V alue) pairs for X, V? (t?1), V? (t?1) from HDFS files  2) ?2(t?1)x Method:  1) Extract values of I, J, iB , jB , ??vj and ??vj 2) i = iB + (I ? 1)MB 3) Compute ??uJi (using equation 23) 4) Key =  [ ?U??, i  ] 5) V alue = ??uJi 6) Emit(Key, V alue)  ? Reduce 4A: Inputs: 1) (Key, V alue) pairs from Map 4A  Method: 1) ??u?i =  ? V alue  2) Key = [ ?U??, i  ] 3) V alue = ??u?i 4) Emit(Key, V alue)  ? Map 4B: Inputs: 1) (Key, V alue) pairs from Reduce 4A 2) (Key, V alue) pairs for U?p(t?1) from HDFS files  Method: 1) Sum =  ( ??upi  )?1 + ??u?i  2) I = i MB  + 1 3) iB = i? (I ? 1)MB 4) Key =  [ ?U??, I  ] 5) V alue =  [ iB ,  Sum  ] 6) Emit(Key, V alue)  ? Reduce Operation 4B: None The above update scheme for ??ui using MapReduce is shown in Figure 4. We can write the equations for updating the value of ??vj in a similar 2 step form. We denote them by (Map 5A, Reduce 5A) and (Map 5B, Reduce 5B) respectively.

Computation of Mean of U and V : Using equation (10) and (16) expression for ??ui can be re-written as  ??ui = ??ui + ? ( ??ui )? ( ??ui  ??upi  ) + ??u?i  ??u?i = ? J  ??uJi  ??uJi = ?  j|j?J&xij?O ??  ( ??ui )? ((xij ? ??ui ? ??vTj ) ??vj ? ??ui ? ??vj  ??vx  )  (24)  Here ??ui ? ??vj implies element wise multiplication of two vectors.

We can compute ??ui using a 2 step MapReduce operation. Using the first MapReduce step compute ??u?i and then using ??u  ? i and values  of ??ui, ??ui, ??upi from previous iteration step to compute ??ui through a second MapReduce step.

? Map 6A: Inputs:  1) (Key, V alue) pairs for X, U? (t?1), V? (t?1), V? (t?1) from HDFS files  2) ?2(t?1)x Method:  1) Extract values of I, J, iB , jB , ??ui, ??vj and ??vj 2) i = iB + (I ? 1)MB 3) Compute ??uJi (using equation 24) 4) Key =  [ ?U??, i  ] 5) V alue = ??uJi 6) Emit(Key, V alue)  ? Reduce 6A: Inputs: 1) (Key, V alue) pairs from Map 6A  Method: 1) ??u?i =  ? V alue  2) Key = [ ?U??, i  ] 3) V alue = ??u?i 4) Emit(Key, V alue)  ? Map 6B: Inputs: 1) (Key, V alue) pairs from Reduce Operation 6A 2) (Key, V alue) pairs for U? (t?1), U? (t?1), U?p(t?1) from  HDFS files Method:  1)  Sum = ??ui + ? ( ??ui )? ( ??ui  ??upi  ) + ??u?i  2) I = i MB  + 1 3) iB = i? (I ? 1)MB 4) Key =  [ ?U??, I  ] 5) V alue = [iB , Sum] 6) Emit(Key, V alue)  ? Reduce Operation 6B:None The above update scheme for ??ui using MapReduce is shown in Figure 5. Equations for updating ??vj will have similar form. We denote these steps by (Map 7A, Reduce 7A) and (Map 7B, Reduce 7B).

It is straightforward to write the equations (20 and 21) for updating value of ??upi and ??v  p j in a MapReduce form. These will be single step  MapReduce operations and we denote them by (Map 8, Reduce 8) for the update of ??upi and (Map 9, Reduce 9) for the update of ??v  p j .

Fig. 5. MapReduce scheme for update of elements of U?  Finally we write the equations for computing the cost function CKL (12 - 15) in MapReduce form. To compute CXKL, we rewrite equation (13) as  CXKL = ? I,J  CIJ  CIJ =  2?2x  ? i,j|i?I,j?J,xij?O  [( xij ?  K? k=1  u?ikv?jk  )2  +  K? k=1  ( u?ikv?  jk + u?  ikv?jk + u?ikv?jk  ) + ?2xln  ( 2??2x  )] (25)  This can be implemented in MapReduce as follows: ? Map 10A:  Inputs: 1) (Key, V alue) pairs for  X, U? (t?1), U? (t?1), V? (t?1), V? (t?1) from HDFS files 2) ?2(t?1)x  Method: 1) Compute CIJ (using equation 25) 2) Key = [?CX?] 3) V alue = CIJ  4) Emit(Key, V alue) ? Reduce 10A:  Inputs: 1) (Key, V alue) pairs from Map 10A  Method: 1) CXKL =  ? V alue  2) Key = [ ?CXKL?  ] 3) V alue = CXKL 4) Emit(Key, V alue)  To compute CUKL, we rewrite equation (14) as  CUKL = ? I  CI  CI =   ? i,k|i?I  [( u?2ik + u?ik u?pik  ) ? ln u?ik  u?pik ? 1  ] (26) This can be implemented in MapReduce as follows:     ? Map 10B: Inputs:  1) (Key, V alue) pairs for U? (t?1), U? (t?1), U?p(t?1) from HDFS files  Method:  1) Compute CI (using equation 26) 2) Key = [?CU?] 3) V alue = CI  4) Emit(Key, V alue)  ? Reduce 10B: Inputs:  1) (Key, V alue) pairs from Map 10B  Method:  1) CUKL = ?  CI  2) Key = [ ?CUKL?  ] 3) V alue = CUKL 4) Emit(Key, V alue)  The MapReduce steps for computing CVKL can be written in a similar way. We denote this by (Map 10C, Reduce 10C). Also since the expression for computation of ?2x, except for the overall scaling, is similar to that of CXKL we do not need a separate computation step for this.

All the MapReduce operations are summarized in Table I.



IV. HADOOP CLUSTER CONFIGURATION AND PERFORMANCE OPTIMIZATION  A. Infrastructure Setup  We have conducted our experiments on a 10 node Hadoop cluster.

Each node is either a standard Dell or HP desktop running Ubuntu OS with dual core CPU of 2.70 GHz and 3 - 4 GB memory. The machines are networked using an exclusive LAN and a standard Ethernet switch.

B. Data Distribution Strategy  In our MapReduce implementation we are sending blocks from matrix X , the set of U matrices and the set of V matrices to a common reducer for computation. As described in section III, the X matrix is divided into M??N? blocks each of size MB?NB . The set of U matrices are divided into M? blocks each of size MB?K and the set of V matrices are divided into N? blocks each of size NB?K. The blocks required for the computation are send to a single reducer. In our case in these scenarios a block I of a U matrix is send to reducers corresponding to Blocks (I, J) where 1 ? J ? N? . Similarly a block J of a V matrix is send to reducers corresponding to Blocks (I, J) where 1 ? I ? M? . Let us consider a general scenario where P type of U matrices and Q type of V matrices are required to perform the computation. Let us also take the block size of X matrix to be MB ?NB = L. Let ST be the total communication cost, SX be the cost of sending X matrix to reducer, SU be the cost of sending the set of U matrices to the reducer and SV be the set cost of sending  V matrices to the reducer.

ST = SX + SU + SV  SX = No. of blocks? No. of reducers to send a single block ? Block size = M? ? N? ? 1?MB ?NB  SU = No. of U matrices? No. of blocks ? No. of reducers to send a single block? Block size = P ? M? ? N? ?MB ?K  SV = No. of V matrices? No. of blocks ? No. of reducers to send a single block? Block size = Q? N? ? M? ?NB ?K  (27)  Adding the expressions for SX , SU and SV , expression for ST becomes  ST = M?N?MBNB + PM?N?MBK +QN?M?NBK (28)  Replacing M?MB = M , N?NB = N and NB = LMB ,  ST = MN(1 + PMBK  L +  QK  MB ) (29)  Minimizing the Total Cost w.r.t MB by taking derivative and equating to zero, one obtains the optimal block size M?B  M?B = ?  (LQ/P ) (30)  In our case P = Q = 3 therefore M?B = N ? B =  ? L. This implies  that though in general M ?= N , and usually M > N , for reducing communication costs the best way is to split the matrices with MB = NB . After some experimentations, we found that M?B = N  ? B = 7204  is the optimum value of block size in our case.

C. Performance Optimization Strategy  We have assigned 2 Maps and one Reduce task to each node. Each Map and Reduce task is assigned 1 GB memory. With initial testing we found that most of the time is spent in shuffling stage of the reducer. This is due to the fact that there were multiple key value pairs to transmit from mapper to reducer. To minimize this, we have compressed the multiple key-values to a single key value. This step decreased the shuffling time by a large factor. In our settings we also increased the in memory (io.sort.mb) available for sorting to 300 MB.

The computation in a block is roughly proportional to the number of elements in the blocks. In our case the data matrix is highly sparse, which introduces significant variations to the block execution time.

In Hadoop we do not have any mechanism to schedule the reducers.

We found that sometime the last phases of reducers are having bigger blocks to execute, which stalls the further job execution . Bigger blocks also favors more shuffling of data. To avoid this, we choose the rows and columns of each block randomly from the full matrix, instead of going in a sequential manner. This minimized the variation in the storage size of blocks considerably.



V. EXPERIMENTAL EVALUATIONS  For testing the performance of our MapReduce implementation we have evaluated several public domain data sets used for studying recommendation systems. Data sets such as that from Movie Lens is not very useful since they are too small for studying scaling properties. We did not get any data sets large than 10 GB from the public domain. Finally we chose the data set obtained from     TABLE I SUMMARY OF MAPREDUCE OPERATIONS  No Input Map Reduce Output 1 Observed values of X Map 1 Reduce 1 X  2 Initial values for U? Map 2A Reduce 2A U?(t=0)  3 Initial values for U? Map 2B Reduce 2B U?(t=0)  4 Initial values for U?p Map 2C Reduce 2C U?p(t=0)  5 Initial values for V? Map 3A Reduce 3A V? (t=0)  6 Initial values for V? Map 3B Reduce 3B V? (t=0)  7 Initial values for V? p Map 3C Reduce 3C V? p(t=0)  8 X, V? (t?1), V? (t?1), ?2(t?1)x Map 4A Reduce 4A U??  9 U??, U?p(t?1) Map 4B Reduce 4B U?(t)  10 X, U?(t?1), U?(t?1), ?2(t?1)x Map 5A Reduce 5A V? ?  11 V? ?, V? p(t?1) Map 5B Reduce 5B V? (t)  12 X, U?(t?1), V? (t?1), V? (t?1), ?2(t?1)x Map 6A Reduce 6A U??  13 U??, U?(t?1), U?(t?1), U?p(t?1) Map 6B Reduce 6B U?(t)  14 X, V? (t?1), U?(t?1), U?(t?1), ?2(t?1)x Map 7A Reduce 7A V? ?  15 V? ?, V? (t?1), V? (t?1), V? p(t?1) Map 7B Reduce 7B V? (t)  16 U?(t?1), U?(t?1) Map 8 Reduce 8 U?p(t)  17 V? (t?1), V? (t?1) Map 9 Reduce 9 V? p(t)  18 X, U?(t?1), U?(t?1), V? (t?1), V? (t?1), ?2(t?1)x Map 10A Reduce 10A C X(t) KL  19 U?(t?1), U?(t?1), U?p(t?1) Map 10B Reduce 10B CU(t)KL 20 V? (t?1), V? (t?1), V? p(t?1) Map 10C Reduce 10C CV (t)KL  CiteULike website 2. CiteULike is a journal paper citation site where users can post journal articles they have read and liked. CiteULike releases a ?who bookmarked what? data which contains 52,689 users, 1,793,954 items and 2,119,200 user-item pairs from May 2007 till May 2013. This data set is used by several researchers for research related to recommendation systems, e.g. [11]. CiteULike data set is of size approximately 1.5 GB. We removed users who have posted less than 5 articles and also articles that are posted by less than 5 users from the data set. The resulting data set contains 13407 users and 39713 articles. The data is highly sparse, only about 0.1% of entries are filled. Since this data set contains implicit ratings, we randomly replaced 10% of missing values with 0 to create a training set. Replacing the entire missing values by 0 will create bias, and without any zero one cannot learn the preferences. Replacing the missing values by 0 through sampling is one of the methods used in practice for dealing with implicit data sets [12]. The resulting final training set is of size 615 MB. This size of training data can be easily fit into the RAM of any standard desktop machines. However since we have 6 dense matrices (U? , U? , U?p, V? , V? , V? p) and we are storing their values for the current and previous iterations in memory, the total memory requirement to run the VBMF code is very high. Therefore for studying the scaling properties of VBMF on our 10 node Hadoop cluster, this is a big enough data set.

For measuring the accuracy of predictions we have computed the RMSE and Recall values (on Top 10 predictions). For studying the scaling performance we ran the MapReduce implementation on the Hadoop cluster with 2, 4, 6, 8, and10 nodes. For comparing the scaling performance, we have also used ALSWR algorithm in Mahout on the same training data using the same Hadoop cluster  2The CiteULike data can be downloaded from the website of CiteULike (http://www.citeulike.org)  TABLE II COMPARISON OF ACCURACY MEASURES IN SERIAL AND MAPREDUCE  IMPLEMENTATIONS  Implementation RMSE Recall % Serial (Java) 0.801 22.7 MapReduce (Java) 0.809 22.3  configurations. Experimentally we found that VBMF converged in about 100 iterations and ALSWR in 50 iterations. Therefore for measuring the performance we ran VBMF for 100 iterations and ALSWR for 50 iterations and measured the total time taken to complete the run. We used Average Time per Iteration as a measure to compare the scaling performance of both the algorithms. To compute this, we subtracted the time taken to initialize the matrices from the total time taken, using the information from the log files. Then divided the result by the number of iterations to obtain Average Time per Iteration.

First we compared the accuracy metrics of our MapReduce im- plementation with a tested serial implementation to make sure that the MapReduce implementation has no inherent bugs. The serial implementation was run on a Dell PowerEdge 2900 server with 500 GB RAM. The results of this comparison is given in Table II Since the RMSE and Recall values are very close between Serial and MapReduce implementations, we can conclude that there are no serious bugs in the MapReduce implementation. The small difference in values could be attributed to difference in the random initial conditions and the random number generator in the two machines.

Results for the Average Time Per Iteration for different number of nodes are given in Table III. Figure 6 shows the average time per iteration as a function of the number of nodes in the Hadoop     TABLE III COMPARISON OF SCALING OF AVERAGE TIME PER ITERATION BETWEEN  VBMF AND ALSWR  Algorithm Number of Average Time per Nodes Iteration (Sec)  2 775.3 VBMF 4 600.6  6 475.1 8 401.5  10 348.1 2 123.5 4 123.1  ALSWR 6 124.0 8 123.1  10 119.7  Fig. 6. Average Time Per Iteration Vs Number of Nodes  cluster. One can see that there is an approximate linear decrease in the time per iteration for VBMF. A perfect linear scaling is absent perhaps because time taken for data shuffling between nodes is very significant in our computations. As the number of nodes increases this becomes more significant and therefore can cause deviation from linear scaling. In the case of ALSWR, average time per iteration follows almost a flat curve instead of a linear trend. And the average time per iteration is much lower for ALSWR compared to VBMF.

Absence of linear scaling seen in ALSWR is quite surprising. Since ALSWR is not a probabilistic algorithm, it uses only two matrices U and V in the iteration process [2]. In VBMF we are considering multiple matrices (six in number) and due to that the number of jobs are more. Most of the jobs (except related to cost) are independent and use data from previous iteration, resulting in the approximate linear scaling. In ALSWR, for each iteration there are only two matrices to be solved (U and V ). The computation of those matrices are dependent on each other. Due to this only one job is running at a time. In our experiments with ALSWR we found that there are relatively less MapReduce job tasks (two-three) in each iteration. As a result the scaling factor is considerably limited. However we need to carry out studies with larger data sets to confirm this.



VI. CONCLUSION AND SUMMARY  In this work we introduced the parallelization of Variational Bayesian Matrix Factorization method using the MapReduce frame- work. Compared to other matrix factorization methods, presence of  several different variational parameters makes this problem more difficult. However the inherent factorization of variational parameters helps in parallelizing the update equations using MapReduce. We evaluated the performance of MapReduce implementation on CiteU- Like dataset and found near linear scaling. We also compared the scaling performance to that of ALSWR algorithm in Mahout machine learning library. Surprisingly we did not find a linear scaling for ALSWR. We need to study both the algorithms on much bigger data sets to really explain the observed difference in the scaling behavior.

This will be undertaken in future. To understand the scalability of Variational Bayesian methods in general, we are also planning to study the MapReduce parallelization of other machine learning algorithms using the variational approximation.

