Privacy-Preserving Frequent Itemset Mining in

Abstract-Cloud computing has ushered a new interest in a paradigm called Datamining-as-a-service. This paradigm is  aimed at organizations that lack the technical expertise or the  computational resources enabling them to outsource their data  mining tasks to a third party service provider. One of the main  issues in this regard is the confidentiality of the valuable data at  the server which the data owner considers as private. In this  work, we study the problem of privacy preserving frequent  itemset mining in outsourced transaction databases. We propose  a novel hybrid method to achieve k-support anonymity based on  statistical observations on the datasets. Our comprehensive  experiments on real as well as synthetic datasets show that our  techniques are effective and provide moderate privacy.

Keywords-Data mining, Frequent itemsets, Outsourcing

I. INTRODUCTION  Privacy,  While data mining in the big data context have become quite common in research and commercial environments, one should not forget the importance of preserving individual privacy. Frequent itemset generation and association rule mining on transactional databases is one of the most popular applicable data mining algorithms. The goal of frequent itemset mining is to identify strong rules in databases using various measures of interest. In most cases, frequent itemset mining is used to generate association rules for finding patterns in large-scale transaction data. For example, the rule {milk , bread} ? {butter} found in the sales data of a supermarket would point out that if a person buys milk and bread, he is likely to buy butter too. Such knowledge can be used as the basis for decisions about marketing activities.

Association rules are also used in the areas of intrusion detection, Web usage Mining, bio-informatics etc.

A transaction is a set of items such as items purchased in a retail store. A rule is of the form AB ? C, where A, B and C are items in the store. Rules are given importance when they occur in many transactions (support), and where transactions that contain the left hand side are likely to contain the right hand side (confidence). The problem of association rule mining is to extract all such rules. The key component is to   Ravi Mukkamala Department of Computer Science  Old Dominion University Norfolk, V A, USA mukka@cs.odu.edu  find sets of items that occur frequently from which the rules may be derived. Given frequent itemsets, generating association rules is straight forward. The Apriori algorithm [I] is a seminal algorithm for finding frequent item sets.

When a data owner finds it convenient to offload the data mining task to an outside source, we refer to it as outsourcing.

The general scheme of outsourcing frequent itemset mining is given in Fig. 1.

Fig. 1. A general scheme for Outsourcing Frequent Itemset Mining  Here, the client/data owner owns the transaction data to be mined. Since privacy could be compromised at the data miner site, it encrypts the transaction data and gives it to the data miner. The data miner mines the encrypted transaction data for frequent itemsets using any of the existing algorithms and gives them to the data owner. The data owner then decrypts the frequent item sets and gets back the original frequent itemsets.

To preserve privacy in outsourcing frequent itemset mining, the data owner must preprocess the data so that the third-party data miner may not be able to infer or extract more information than it has provided to the data owner. Its knowledge of frequent itemsets and their support from its past experience should not also enable this extraction leading to privacy violation. For example, the data miner may be able to easily identify the top frequent items as they are generally unique in that domain.

To circumvent such attacks, there are several group-based approaches which basically hide the individuality of an item behind a group of items. There are many variations to this     approach including k-anonymity [10], t-closeness [6], 1- diversity [7], etc. Apart from conventional databases, these approaches are also employed in other types of data such as graphs and social networks. Most of the existing work use the approach of k-support anonymity, a variant of k-anonymity, which means that for a given item with its support, we have k- 1 other items with the same support.

In this paper, we study the problem of outsourcing Privacy Preserving Frequent Itemset Mining (PPFIM) in outsourced transaction databases. PPFIM require the mining results to be correct and complete, the privacy in both raw data and mining results be effectively protected, and the overhead of  encryption and decryption be reasonable.

Existing works in PPFIM use the approach of k-support anonymity along with simple substitution encryption to provide privacy at the data miner's site. Fake items or transaction are added to facilitate k-support anonymity resulting in spurious tuples returned from the data-miner. We propose a novel hybrid method to achieve k-support anonymity by removing and also adding some instances of items based on the statistical observation that it is the median which minimizes the sum of mean absolute difference. We also provide a way to deal with outliers which are common in the item support distribution of transactional databases, so that it results in lesser spurious tuples from the data miner. Our comprehensive experiments on real as well as synthetic datasets show that our techniques are effective and provide moderate privacy.

The paper is organized as follows. In section 2, we briefly summarize the related work. Section 3 describes the proposed approach for frequent itemset mining. Section 4 provides details of the experimental runs and their results. Finally, section 5 has some concluding remarks.



II. RELATED WORK  To the best of our knowledge, the work of Wong and Lim [12] is the first encryption solution to PPFIM. Here the data is encrypted using substitution cipher and random fake items are added to the transformed database. The data owner using a one-to-n mapping encrypts the original transaction database to be sent to the third-party miner. The data miner performs the datamining task on the encrypted database and hands over the association rules to the data owner. The data owner then extracts the original rules from the encrypted rules by decryption. The flaws in this methodology have been identified in [8] by proposing a frequency analysis based method for breaking the encryption scheme. The main flaw in the former work was that the fake items added in the encrypted database were independent of other items.

Since FIM is subjected to top frequency attack, Tai et al [11] proposed a novel algorithm based on k-support anonymity.

Because of k-support anonymity, an experienced attacker cannot succeed in re-identifying sensitive items even with the  precise support information. For storage efficiency, rikant and Agarwal [9] convert FIM to Generalized FIM which discovers all frequent itemsets across levels of a given taxonomy.

Although the work provides a very intelligent way of achieving privacy through k-support anonymity, the complexity at the data-miner to create the taxonomy tree is high as the value of k increases. Also the number of tuples generated by the third party data miner can be quite high.

Gianotti et al [5] adopt a frequency-based attack model where the adversary knows the exact set of items along with the item support. Even in this work, the approach of k-support anonymity named differently by the authors as k-privacy is undertaken. The work considers adding fake transactions containing of real items for achieving k-privacy. This is done by means of the Frugal/Rob Frugal Scheme which generates a noise table of real items. The information about the fake transactions is then held in a compact data structure called the synopsis which is used during the decryption phase to retrieve back the true itemsets. Although the Frugal and Rob Frugal Scheme employed by the authors achieves decryption of a frequent itemset in 0 (m) complexity, what they fail to address is the amount of spurious tuples generated. This is because of the simple fact that all the support of all the items in the group is equalized to that item in the group which has the highest support. This is one of the main drawbacks of this method.

Moreover, in real life transaction databases, the item support table follows a power-law distribution. In other words, there are very few items with large supports and many a heavy right-skewed tail of items with very low support. The few items with large support are what we term as outliers. Another serious drawback of this method is that in the presence of outliers in the item support table, the number of spurious generated by the third party data-miner increases exponentially as we encrypt the transaction databases for higher values of k. This is because of the simple fact that as k increases, given the items support table in descending order, the distance between the first item and the second item increases. Considering the way the fake transactions are added in order to achieve the goal of adding fake transactions of different types, if there is sudden dip of support in between in the item support table, the number of spurious tuples generated by the data miner increases by the power set of the fake transaction.



III. PROPOSED APPROACH  This section describes the proposed hybrid method, hybrid in the sense that we add some items and also remove some items, along with an illustrative toy example. It also provides the necessary proofs supporting the proposed method.

The existing methods provide privacy using the approach of k-support anonymity where a given item has k-I other items with the same support. The pseudo-taxonomy scheme doesn't disturb the real items supports and adds dummy fake items. On the other hand, the Frugal scheme clusters items into groups of     k. Given a group of items sorted in descending order of their supports, it equalizes the support of all items to the first item's support which is the maximum in the group. Our method is closer to the Frugal scheme in the way we encode the original transaction database. In our approach, given a group of items sorted in descending order of their supports, we observe that the total number of changes made to the original transaction database is minimal if we equalize all the items support to the median of the group.

Theorem 1. The mean absolute deviation is minimized when measured from the median of the distribution.

Proof We prove the theorem by induction. For n = 1 and n = 2, the result is trivial. Let us assume that the mean absolute deviation is minimized when measured from the median of the distribution for n Xi'S and consider the case of n + 2 points. Let XI be the maximum andxn+2 be the minimum of the points.

Then I XI - ml + I Xn +2 - ml is minimized for any value of m in the range XI :S m :SXn+2. So it suffices to minimize the sum of I Xi - ml for the remaining n points. By the induction hypothesis, this occurs at their median, which is the same as the median of n + 2 points.

Our proposed scheme is based on the following two important observations:  ? By theorem 1, we see that the optimal support in terms of the changes made to the item supports is the median.

? Another point is that of outliers in the item support table.

According to [4], in real life transaction databases, the item support table follows a power-law distribution. In other words, there are very few items with large supports and many a heavy right-skewed tail of items with very low support. Thus outliers are a few in number.

We now provide the notations used in the proposed scheme.

Table 1. Notation  Notation Explanation T DB a transactional database  k k -support anonymity parameter min_sup the minimum support count  threshold E - T DB an encrypted transactional  database a ,j=l..k real items in a group of k s) ,j = l..k support of real items in a group  ork items d lnax maximum absolute difference in  Items removed 1Relnoved set of all items removed Gp,P= Group of k items l....nlk  medianGn median support of a group Gn  A. Encryption Algorithm Using this algorithm, the items in the transaction data are  modified so that privacy is preserved. We now present our encryption algorithm (Algorithm 1).

After generating the item support table and sorting them in descending order (line 1-2), we handle the outliers in the 1ST by adding dummy fake items (line 3-10). For each outlier, we add dummy fake items with support equal to the support of the outlier and add them into the 1ST. To inject them in to the database in such a manner such that we have fake transactions of different length, we divide the fake transaction into smaller transactions of increasing sizes and add them to the encrypted database required number of times. For example if the fake  transaction length is 7, then we divide into smaller transaction of sizes 1, 2 and 4.

After this for each group, we calculate the differences of each items support with the median item's support (line 11 to 14). The proposed scheme involves removing instances of items to achieve minimal changes to the original transaction database. Thus, when the data owner queries the data-miner for frequent itemsets with minimal support threshold, in order to maintain the completeness of frequent itemsets, it has to query with modified support = min_sup - dlnan where dlnax is the maximum absolute value difference among the set of removed items. Setting d lnax to be low would result in lesser number of spurious tuples. If the absolute negative difference in a group exceeds d lnan we equalize the support of each item such that the absolute negative difference is lesser than or equal to dlnax (line 15).

For items whose differences are positive, we generate the synopsis as in the Frugal Scheme and add fake transactions consisting of real items. For items whose difference is negative, we remove instances of those items from TDB. This is done as given in Line 26. This is because we want to reduce the effect of an item's removal from a transaction to the overall number of frequent item sets returned by the data miner. We then replace each item with its corresponding one? to-one mapping (line 29-30).

B. Decryption Algorithm We now present the decryption algorithm (Algorithm 2).

We first decrypt the frequent itemset using the one-to-one mapping (line 1-2). If the itemset contains a dummy fake item, we drop the itemset (line 3). If it does not contain even one item from the set 1relnoved, it results in two cases. First, if the support of itemset is less than min_sup, again we drop the item set. Second, if the support is greater than min sup, we use the synopsis to find whether it is a true itemset or not as explained in the Frugal Scheme in chapter 2(line 6-11). If the itemset contains at least one item from the set 1relnoved, then instead of doing a local mining on the transactions from which we removed instances of items, for each marked transaction, we increment the support of the itemset if it is a proper subset of the marked transaction. This is again followed by a check whether the new support is greater than min _sup or not (line 12-19).

Algorithm 1. Encryption Algorithm Require: TDB, k, min_sup.

Ensure: E - TDB, S, lRemoved.

1: Generate 1ST and sort it in descending order.

2: Detect outliers (if any).

3: for each of the outliers do 4: Add the required # of dummy fake items into 1ST with supports equal to the outliers support.

5: Construct a fake transaction consisting of the dummy items.

6: end for 7: for each fake transaction do 8: Divide the fake transaction of increasing sizes starting from 1, 2 ...

9: Add each of them as separate transaction to E-TDB, corresponding outlier support number of times.

10: end for II : Cluster the items into groups of k.

12: for each group Gp do 13: for each item aj in group Gp do 14: difference(a;) = medianGp - Sj.

IS: if I difference(a I) I > d max then 16: Find 1 :s 1:S k such that lSI - sfl :s dmax.

17: Calculate difference(a;) = Sf - Sj.

18: end if 19: end for 20: end for 21: Sort the differences in descending order.

22: for all items do 23: if di > 0 then 24: Add fake transactions and generate synopsis.

25: end if 26: if d; < 0 then 27: Remove di instances of item ai from those transactions in TDB  28: end if 29: end for  which have item ai and has least cardinality.

30: Generate a one-to-one mapping function M(.).

31: Replace each item ai with M(a;).

32: return E - TDB and synopsis.

end  C. Illustration We explain the proposed scheme with a help of an  illustrative example. Suppose we have a hypothetical transaction database given in Table II. The item support table sorted in descending order for the given transaction database is given Table III.

Suppose the data owner wants 3-support anonymity i.e., k=3. Our encryption algorithm first detects al as the outlier. It then injects dummy fake items.fs andh in to the 1ST as given in the Table II. Let d max = 1. It then constructs fake transaction  with items {is,/9 } and adds the transaction to the database 10 times. This is followed by clustering items into groups of 3 and finding the difference for each item with the median support of the group. This is shown in Table II. We thus need to add one instance of a7, remove one instance of as and one instance a2. The algorithm first adds transaction a7 and creates synopsis consisting of the triple < a7, 1, 0>.

Algorithm 2. Decryption Algorithm Require: Frequentltemsets ,synopsis, min sup 1 d Ensure: Trueltemsets  - , remove'  for each itemset do 2: Decrypt the itemset using the mapping M(.).

3 :  if itemset contains a dummy item then 4: Drop the itemset.

5: Goto to line 1.

6: end if 7: if itemset contains at least one element from lremoved then 8: 9: 10: 1 1: 12:  if itemsetsupport < min _sup then Drop the itemset.

else  true itemset or not.

13: end if 14: else  Goto to line 1.

Use synopsis to find whether it is a  IS: For each marked transaction in TDB increment the support of the itemset by 1 if it is subset  ' Of the  marked transaction.

16: 17: 18: 19: 20: 2 1: 22: end if 23: end for end  if support 2: min _sup then  else  end if  Add the itemset to True Itemsets.

Drop the itemset.

Goto to line 1.

It then finds a transaction which contains a2 and has the least cardinality. We have transaction 1, 2, 5, 7, 8 satisfying this criterion. In case of ties we break it arbitrarily and thus remove a2 from transaction I and mark that transaction.

Similarly we remove as from transaction 4. Finally we encrypt the database with a one-to-one mapping and send it to the dataminer.

Suppose the actual min_sup is 4, the modified support = 3.

Thus we query the data miner with minimal support 3 .  The data miner returns the following itemsets: (al)(1 0), (is)( 10), (!S, h)(IO), (f9)(10), {a4' a 1)( 4), (a4)(4), {a2' a 1)(4), (a3,ad(4), (a2)(4),{a3)(4).

The decryption algorithm first gives (ad (1 0) as the frequent itemset. Since Is and .l9 are fake items, it simply neglects the next three itemsets. Since {a4' at}, {a4} are neither     in the removed itemlist nor in the synopsis, the algorithm outputs it as frequent itemset. For the itemset {a2' at}, since a2 belongs to the removed itemslist, it checks whether it is subset of a marked transaction or not. Since it is a subset of the transaction I, it increments its support and adds it to the true frequent itemset list. For {a3' a t} and {a3}, the decryption scheme outputs it as a frequent itemset and since a2 is in the removed itemlist, it outputs the itemset raJ as frequent itemset with support 5.

D. Privacy In our problem, we consider the following attack model  which is the same as in [5]. The third party data miner who can be an adversary may possess some background information by which he can employ some attacks into the encrypted transaction E - TDB. For the privacy analysis, we refer to the items in the original transaction database TDB as plain items and the items in E - TDB as cipher items.

We consider a conservative model and assume that the adversary knows exactly the set of plain items in TDB and their true supports. The adversary has access to the encrypted transaction database E - TDB and thus has the knowledge about cipher items in the encrypted database and their supports. However, we assume that the adversary has no knowledge about the encryption scheme, plaintext transactions, frequency of item sets and the distribution of transaction lengths in the original transaction database. We assume that the third party data-miner is semi-honest meaning although he doesn't know the details of the encryption algorithm, he can use his past experience to make conclusions on the encrypted transactions.

Theorem 2. For a given cipher item, the set a/candidate plain text items is of size at least k.

Proof: The proof of this theorem is straightforward. Clearly, the encryption scheme equalizes the support of all items in a group of size k following which each item is encrypted using the mapping generated. Thus privacy is achieved through k? support anonymity where each item has k-I other item with the same support. Moreover, notice that the privacy achieved is stronger as compared to the Frugal scheme. This is because in the Frugal scheme, as we only add items, the adversary can infer that fake transactions are added based on the observation that for every cipher item there always exist a plain item whose frequency is no larger than the frequency of the cipher item. He can thus prune the candidate set of plain items to the size of k. This is not the case in our proposed scheme as there is no way the attacker can come to know that some items are added while some are removed. Even if he guesses that, the candidate set of plain items for a given cipher text is always of size k. But when it comes to the privacy of the outliers, it is low as compared to the Frugal scheme because the attacker may guess the fake items added from the way the dummy fake transactions are added. One way to circumvent this problem is to randomly add the fake items to the original transactions but as the value of k increases this again would result in massive spurious tuples as the fake items  would then correlate with real items. This behavior justifies the fact in literature that there is always a tradeoff between privacy and accuracy.

Table II. Hypothetical Transaction Database  tid Real items  I al,a2,a4  2 al,a2,a3  3 al,a3,a4  4 at,a4,as  5 al,a2,as  6 at,a3,a7  7 al,a2,as  8 al,a2,a6  9 al,a4,a6  10 at, a 3  Table III. Item Support T bl a e  Items Support Difference a t 10 0 Is 10 0 19 10 0 a2 5 -I a3 4 0 a4 4 0 as 3 -I a6 2 0 a7 I I  E. Accuracy We now show that the decryption scheme returns true  itemsets of the original database from the frequent itemsets returned by the data-miner of the encrypted database.

Theorem 3: The true frequent itemsets returned by the decryption method are correct and complete.

Proof The proof of this theorem is straightforward. Since the encryption scheme, removes some instances of items, the completeness of the frequent itemsets returned by the data? miner is maintained if the owner queries the dataminer with support = min_sup - dmax. This would result in spurious itemsets which the decryption scheme filters. In the beginning for outliers, as the encryption scheme adds fake transactions containing dummy fake items, the true frequent itemsets would never contain any itemset generated by these fake transactions. Thus the decryption scheme drops the itemsets which contain atleast one fake item. Next, if the the itemset does not contain any item from the list of removed items and its support is lesser than the minimum support threshold, then there is no way its support would increase. If its support > min _sup, then it is possible that the itemset is part of a fake     transaction containing real items. If it is part of a fake transaction containing real items then the decryption scheme uses the synopsis to find its true support and then detects whether it is a true itemset or not, otherwise the itemset is a true frequent itemset. If the itemset contains atleast one item from the list of removed items, then its present support is less by how many ever times the itemset appears in the effected transactions. Thus the decryption scheme, for each effected transactions increments the support of the itemset if it is subset of the effected transaction. After this, if its support > min _sup then it adds to true itemsets otherwise the itemset is dropped.

F. Complexity  The time complexity of the encryption scheme is O(n log n). The decryption algorithm has a complexity of max ( Oem), O( no. effectedtransactions).



IV. RESULTS AND ANALYSIS  The proposed method was implemented on two data sets: a real-life store data set and a synthetic dataset. We summarize results from both the datasets.

A. Real-Life Store Dataset A real-life Retail Dataset provided by an anonymous  Belgian retail store [3] and is publically available in the FIMI Repository (http://fimi.ua.ac.be/datal). The Retail dataset contains 88162 transactions with 16470 items and the average length of the transaction is 7.5. The longest transaction length is 76. As for the support distribution of the items, beyond the 3000 support level, there are only 10 items and the most frequent item occurs in 50675 transactions. All the programs were written in Java using Eclipse IDE. All experiments are performed on an Intel Core i3 Processor with a 2.13 GHz CPU and 4GB RAM over 64-bit Windows 8 platform. The Apriori Algorithm implemented by Christian Borgelt [12] was used to mine the encrypted database for frequent itemsets.

Fig. 2 gives the encryption timings for different values of k. As expected, the execution time increases as k increases.

This is because as the value of k increases, we need to add more fake transactions consisting of dummy items, add more fake transactions consisting of real items and also remove many instances of items. The sudden increase from k=60 to k=80 is attributed to the way we add fake transactions containing dummy fake items. Recall that when we split the fake transaction corresponding to an outlier into increasing sizes, the remainder is added to the previous transaction. Thus as k increases, the no of split transactions increases and thus we need to add more of them.

Table IV gives the number of frequent itemsets returned by the apriori algorithm for encrypted transaction databases with different values of k and also for different minimal support thresholds. The frequent item sets are obtained by querying the data miner with support = min_sup - dmax? As expected, we observe that for each k, as the minimal support threshold  increases, the number of frequent itemsets decreases. Also notice that the frequent itemsets are same for minimal support threshold 0.2 and 0.3. This is because there is no frequent itemset between the two thresholds. Moreover, for a given threshold, the number of frequent itemsets increases as the k increases, the reason for which is the increase in the number of fake transactions added to the encrypted database as the value of k increases to achieve k-support anonymity. In Table 4, we provide the decryption execution timings for different values of k and minimal support threshold. The decryption execution time follows the same behavior as the frequent itemsets. Note that, the execution timings depend upon the characteristic of the frequent itemsets obtained i.e whether the frequent itemset contains a fake item, or an item from the removed item list or part of the synopsis items. All the decrypted true itemsets were correct and complete.

MinSup 0.1  0.2  0.3  0.4  0.5  0.6  Table IV. Decryption time for real-life data set  k=20 k=40 k=60 k=80 k=100 61 164 375 16939 23686  53 153 363 317 21629  53 153 363 317 21629  50 151 356 307 21768  30 90 302 277 11671  0 0 0 0 0  B. Synthetic dataset The proposed method was also run on a synthetically  generated dataset by the IBM data generator. The dataset contained 870 different items with lOOk transactions where the average transaction length is 10 while the maximum transaction length is 29. As for the item support distribution the most frequent item occurs in 14828 transactions and beyond the 7000 mark there are only two items. All the programs were written in Java using Eclipse IDE. All experiments are performed on an Intel Core i3 Processor with a 2.13 GHz CPU and 4GB RAM over 64-bit Windows 8     platform. We used the same Apriori Algorithm as before to mine the encrypted database for frequent item sets.

Fig. 4 gives the encryption timings for different values of k.

As expected, the execution time increases as k increases. The sudden increase in the execution time from k=60 to k=80 can be attributed to the same reason as given in the case of real dataset. Note that this particular feature is that of the algorithm and not of the dataset.

Table 5 gives the number of frequent itemsets returned by the Apriori algorithm for encrypted transaction databases with different values of k and also for different minimal support thresholds.

The frequent itemsets are obtained by querying the data miner with support = min sup - dmax. Notice the sudden dip in the frequent itemsets returned by the apriori algorithm as k increases from 40 to 60. This is because of the way we add fake transaction consisting of real items and dummy fake items. Recall if the fake transaction length consisting of real items is beyond lman we split the fake transaction into smaller sizes. This has an impact over the number of frequent itemsets returned. This behavior is also seen in the decryption timings.

Also, 10% relative support, the number frequent itemsets do not change. This is because there is no frequent itemset between 10 and 12% relative support and all of them except two single items having support greater than 14000 are spurious itemsets. The execution timings depend up on the characteristic of the frequent itemsets obtained i.e whether the frequent itemset contains a fake item, or an item from the removed item list or part of the synopsis items.

We found that all the decrypted true item sets were correct and complete.

Figure 2. Encryption time for synthetic data set

V. CONCLUSION  We have proposed a novel hybrid approach towards privacy preserving frequent item set mining in outsourced transaction databases. In order to provide privacy through k-support  anonymity, the method adds some items and also removes some items. The basis for such an approach is that the median minimizes the sum of mean absolute difference. In our proposed method, we also reduce the effect of outliers generating massive spurious frequent itemsets. We have experimented our proposed method on one real and one synthetic dataset. Our comprehensive experiments demonstrate that our techniques are effective, scalable and protect privacy.

