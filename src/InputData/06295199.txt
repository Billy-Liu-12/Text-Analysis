An Improved Association Rule Algorithm Based on  Itemset Matrix and Cluster Matrix

Abstract?Through the analysis of the method of association rules, an improved algorithm in association rules based on Itemset Matrix(ISM) and Cluster Matrix(CMa) is put forward.

The algorithm can get the new frequent itemsets just through scanning the updated data once again, when the database and the minimum support degree are changed. Studies and analysis of the algorithm show that it just need to scan the database once, so it has the virtues in high-speed producing frequent k-itemsets and less time cost. And it improves the efficiency of the association mining, can fulfill the request of shortening the time of mining.

Keywords?Cluster Matrixes(CMa), association vector, Itemset Matrixes(ISM), association rules

I.  INTRODUCTION Data mining technology is a kind of new technology which  is overlapping by several subjects. It is produced and developed along with accumulation of the data sets, as well as the demand of information and acknowledge by the competition of the market, and it is gradually becoming hot by people.[1] Association rules is an active and challenge research direction in the field of data mining. At present, some of  association rules algorithms are more popular and used more often, such as Apriori, FP-Growth, FP-Array and so on. But they all have some drawbacks themselves, such as producing lots of candidates, repeating to scan database and low mining efficiency. For improving the mining efficiency, many researchers have combined some related technology to improve the algorithms. This paper proposes an idea that joins the concept of the matrix and the clustering analysis into the association rules algorithms to improve the mining efficiency.

It involves with two matrixes?Itemset Matrix(ISM) and Cluster Matrix(CMa). In order to be convenient to describe later, the improving algorithm is named ISMCM algorithm.



II. THE DATA DESCRIPTION AND  DATA PARTITIONING OF ALGORITHMS  A. the Data Description of Association Rules Let },,,{ 21 nIIII ?=  be a set of items. Let D, the task-  relevant data, be a set of database transactions where each transaction T is a set of items such that IT ? . Each transaction is associated with an identifier, called TID. [2]  Definition 1: The vector of each item Ij is defined as    ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  =  nj  j  j  j  d  d d  D ?  ?     ? ?  = ij  ij ij TI  TI d  ,1 ,0 .

So the support of Ij is  ? =  = n  i ijj dIcount   )(sup_                                 (1)  Definition 2: The vector of 2-itemset },{ 21 II  is defined as  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  ?  ? ?  =?=  njni  ji  ji  jiij  dd  dd dd  DDD ?   11 ,  So the support of 2-itemset },{ 21 II  is  ? =  ?= n  k kjkiji ddIIcount   )(),(sup_                   (2)  Definition 3: The vector of k-itemset },,,{ 21 kIII ?  is defined as  kkkk DDDDDDDD ????=???= ? )( 1212112 ??? .

So the support of k-itemset },,,{ 21 kIII ?  is  ? =  ? ????= n  q qkkqqqk ddddIIIcount  )1(2121 ))((),,,(sup_ ?? (3)  Definition 4: The matrix of D is defined as  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  ==  pnpp  n  n  n  ddd  ddd ddd  DDDD  ? ????  ? ?  ?     21 ),,,( .

Apriori property 1: All nonempty subsets of a frequent itemset must also be frequent.[2]  B. The Cluster Partitioning of Data Set In order to mine conveniently by association rules and  improve the efficiency of algorithms, this paper proposes  to scan database just once, and based on item?s numbers, the   Computer Science & Education (ICCSE 2012) July 14-17, 2012. Melbourne, Australia   ThP6.7        database transactions are clustered to produce a series of Cluster Matrixes(CMa) which replace original database. So, all of computing will execute in these CMa.

In order to explain clearly, this paper quotes an example.

Table I is a transactional database which have nine transactions, |D|=9, and five items, I1, I2, I3, I4, I5.

TABLE I.  TRANSACTIONAL DATA  TID List of items T100 I1,I2,I5 T200 I2,I4 T300 I2,I3 T400 I1,I2,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I1,I2,I3,I5 T900 I1,I2,I3    So D are expressed by CMa:  ? ? ? ? ? ?    ?  ? ? ? ? ? ?  ?  ?  =   )2(CMa ? ? ?    ?  ? ? ?  ?  ? =   )3(CMa [ ]10111)4( =CMa .



III. THINKING AND REALIZATION OF ISMCM ALGORITHMS  A. Thinking of Algorithms 1) Initial mining  First, scan database and cluster the data records into classes based on items? number. Then count the frequency of itemsets in the database to produce Itemset vector ISM(k). This vector includes three terms: itemset, occurrence frequency of this itemset (Vec_count) and associating count (Cen_count).

According to Vec_count and Cen_count, calculate support count (sup_count), if they are bigger than minimum support threshold (min_sup), these itemsets are directly judged to be frequent. Filter out these itemsets to produce new CMa, then calculate CMa.

Algorithms steps:  ? Find frequent 1-itemset F1: Let k be the item?s number of a transactional data record. Based on k, D are clustered CMa(k), nkm ???2 , m is the minimum item?s number, and n is maximum. The value of each CMa(k) uses binary system: 0 or 1, where 0 means that the item is absent, and 1 means that it is present. Count the support of each item by equation (1). According to the results, all of item whose sup_count is the bigger than min_sup will be joined in F1, get F1.

? Find candidate k-itemset Hk: A set of candidate k- itemsets is generated by joining Fk-1 with itself. This set of candidates is denoted Hk.[3] Then the itemsets that are directly judged to be frequent itemsets are removed from Hk. When k=2, for H2, get the support count by equation(2). Otherwise to the third step.

? Compare the itemsets in F2 with the last two items of those in Hk(k>2). If not in F2, the itemset is not frequent, so remove it. Because the length of itemsets in Hk is k, only consider CMa(l)( kl ? ). The occurrence frequency of itemset(Vec_count) has been recorded in itemset vector ISM(l), so only count the support of CMa(l)( kl > ). The equation of support of candidate itemset in Hk is:  ? ? += =  ???+  = n  ki  l  j ijkijijk  k  dddIIIcountVec  IIIcountSup  1 1   )()(_  )(_  ??  ? (4)  )(_ 21 kIIIcountVec ?  is the count in ISM(k), dijk is the element of the j row and the k column in CMa(i). If  )(_ 21 kIIIcountSup ?  is bigger than min_sup this candidate k-itemset is included in Fk.

)(_)(_ 2121 kk IIIcountSupIIIcountCen ?? =       (5)  )(_ 21 kIIIcountCen ?  is the associating count about ISM(k) and Fk.

? Repeat the second and third step, until ?=kH .

2) Update association rules in changing min_sup For the database transaction, set initial minimum support as  min_supand set updating minimum support as min_sup?. So the update may be two conditions:  ? supmin_sup'min_ > , this condition is very easy, only verify original sup_count and cen_count of frequent itemsets. Not satisfy, remove it.

? supmin_sup'min_ < , this is a little complicate but we don?t need scan database again. First, according to ISM we compare associating count with min_sup?, get new frequent itemsets and join in original Fk. Second, according to Fk we prune CMa, reduce the number of Candidate itemsets and produce the new CMa?. Last, operate it by mining algorithms above.

3) Update association rules in updating database When the database transactions are updated, there is no  need to scan all the databases, and just need to scan the new data records. Though the algorithms are proposed by references [4-6] only scan database once too, they all must rescan database when databases are updated. Cluster data records by item number, compare ISM. If it has existed in ISM, only alter Vec_count and Cen_count; If not, add it in ISM and count.

Mark these itemsets that are greater than or equal to min_sup be frequent itemsets, update ISM. Remove those itemset vector whose Vec_count and Cen_count all are 0. Prune CMa by updating ISM and Fk. Find updating all frequent itemsets by mining algorithms above.

B. Example Application of Algorithms We will illustrate the realization of algorithms with table I  below.

1) Initial mining Set the minimum support as 2.

ThP6.7        ? Scan database transactionand get CMa and ISM.

Example, in CMa(2), (I1,I3) appear twice. So its itemset vector is (I1,I3,2,2). Comparing Vec_count with min_sup, find I1I3 and I2I3 are frequent itemsets. Under pruning get CMa and ISM:    ? ? ?    ?  ? ? ?  ?  ? =   )2(     II II II  ISM , [ ]01010)2( =CMa ,    ? ? ?    ?  ? ? ?  ?  ? =   )3(     III III III  ISM ,  ? ? ?    ?  ? ? ?  ?  ? =   )3(CMa ,  [ ]11)4( 5321 IIIIISM = , [ ]10111)4( =CMa .

? Find F1. According to Apriori property 1, I1,I2 and I3  have been frequent itemsets. According to equation (4),  ? ? = =  ?=+=    43424 22)(sup_  i i ii ddIcount  .

I4 is joined in ISM(1), and alter the associational count of I4 in ISM(1) by equation (5). Same argument, sup_count(I5)>2. So, find F1={I1,I2,I3,I4,I5}.

? Find Fk( 2?k ). Joining F1 with itself generate candidate 2-itemsets H2={I1I2,I1I4,I1I5,I2I4,I2I5,I3I4,I3I5,I4I5}.

According to equation (4),  ? ? = =  ?=?+?+=    2414231321 24)()(0)(sup_  i i iiii ddddIIcount  .

So, I1I2 is frequent itemset and joined in ISM(2). And alter the associational count of I1I2 in ISM(1) by equation (5). Same argument, find F2={I1I2,I1I3,I1I5,I2- I3,I2I4,I2I5}, and alter ISM(2). So, F3={I1I2I3,I1I2I5}.

Because ?=4H , algorithms stop. ISM(1), ISM(2) and ISM(3) is :  ? ? ? ? ? ?    ?  ? ? ? ? ? ?  ?  ?  =   )1(       I I I I I  ISM  ,  ? ? ? ? ? ? ? ?    ?  ? ? ? ? ? ? ? ?  ?  ?  =   )2(        II II II II II II  ISM  ,  ? ? ?    ?  ? ? ?  ?  ? =   )3(     III III III  ISM .

Algorithms divide the support count into two parts: the occurrence count of same itemset and the associational count of itemset in different data records. We can reduce the number of candidate itemsets by occurrence count and associational count. Only scan database once, save I/O time greatly. The transactional itemsets are pruned by ISM and CMa so reduce the finding time of candidate and frequent itemsets.

2) Changing min_sup There are two conditions of Changing min_sup.:  ? Largening min_sup.

Change min_sup to be 3. There is no need to rescan and mine, only verify associational count of original frequent itemsets in ISM. So, new frequent itemsets F={I1,I2,I3,I1I2,I1I3,I2I3}. Remove those itemsets that don?t satisfy the condition for min_sup being 3 and occurrence times being 0 in ISM(k). Changed ISM(1) and ISM(2):    ? ? ?    ?  ? ? ?  ?  ? =   )1(     I I I  ISM ,  ? ? ? ?    ?  ? ? ? ?  ?  ?  =   )2(      II II II II  ISM .

? Lessening min_sup.

Change min_sup to be 2 again.  Compare associational times between ISM(2) and ISM(3), and find frequent itemsets F={I1,I2,I3,I4,I5,I1I2,I1I3,I1I5,I2I3,I2I4,I2I5,I1I2I3,I1I2I5}.

Find H2={I1I4,I3I5} by pruning CMa(3). By counting I1I4 and I3I5 all don?t meet the condition. Change ISM(1) and ISM(2) again.

3) Updating database transaction Set min_sup still as 2. Database update transaction {I1I4}.

Don?t need to rescan database, only check I1I4 to exist or not in ISM(2). If not, join in ISM(2). According to algorithms, the support of I1I4 is 2, and it is frequent itemset, so joined it in F.

ISM(1) and ISM(2) are changed to:  ? ? ? ? ? ?    ?  ? ? ? ? ? ?  ?  ?  =   )1(       I I I I I  ISM ,  ? ? ? ? ? ? ? ? ?    ?  ? ? ? ? ? ? ? ? ?  ?  ?  =   )2(         II II II II II II II  ISM  .

It can be seen that algorithms only need scan database once at initial mining. In spite of the changing of min_sup changed or the update of database, there is no need  to rescan database.

It greatly save I/O time and improve the mining efficiency. We can directly find some changed frequent itemsets by comparing occurrence times and associational times in ISM. And by pruning for CMa, count the support of candidate itemsets, find new frequent itemsets. Don?t need to generate candidate step by step and frequent itemsets so reduce the generating time of it.



IV. ALGORITHMS ANALYSIS AND SIMULATION EXPERIMENT According to the step of algorithms, we operate the  simulation experiment. Experimental environment is PC Pentium  3.06G/512M(OS is WindowsXP), Matlab7.1. The experimental DataSets use KDD Cup 1999 network DataSets.

We compare ISMCM with Apriori for time. When min_sup is 0.5,0.4,0.3,0.2,0.1 respectively, the generating times of ISMCM and Apriori illustrate with figure 1:   ThP6.7         Figure 1.  Comparing of ISMCM and Apriori.

The experiment demonstrates ISMCM generate faster than Apriori at same condition. And the mining performance of frequent itemsets  is improved greatly. For changed min_sup and updated database, ISMCM all can deal with. So, ISMCM is effective.



V. CONCLUSION In the process of association rules mining, it is the key of  research how to find frequent itemsets effectively. This paper proposes an association rules algorithms based on ISM and CMa. Algorithms only scans database once, greatly reduces the times of scaning database and improves efficiently. Using ISM  to prune, reduce the comparing time of candidate itemsets and records. Using CMa to cluster for data records, shorten the getting times of frequent k-itemsets. When database are changed, only scan the changed part and find frequent itemsets at once. Don?t need to rescan all the databases. So, algorithms improve the efficiency of association rules mining.

