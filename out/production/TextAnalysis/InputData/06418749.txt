Proceedings ofIC-NIDC2012

Abstract: Topic detection is to develop automatic methods to identify topically related documents within a stream of data; many approaches have been developed to classify documents with predefined knowledge. This paper presents a new approach for topic detection and tracking based on credible association rule (CAR). This paper considers topic detection without any prior knowledge of category structure or possible categories.

Topic features are selected primarily based on CAR.

Results on the test set show a marginal improvement by using CAR and its maximal cliques mining algorithm.

The CAR maximal cliques mining algorithm is now applied on real topic detection and tracking system which gives us a lot of experience in adjusting and refining the algorithm. This algorithm also presents many useful interface extensions for other modules of the system to use.

Keywords: Topic detection; Feature selection; Credible association rule; Maximal cliques mining; Quasi maximal cliques mining  1 Introduction  Topic detection is a process to extract the major topics embedded implicitly in a collection of documents. Much work has been done on topic detection. Hsin-Hsi Chen and Lun-Wei Ku [1] presents algorithms for Chinese and English-Chinese topic detection, name entities, other nouns and verbs are used as cue patterns to relate news stories representing the same topic. A two- threshold scheme and a least-recently-used removal strategy models are also taken into consideration.

Experiment results were shown to outperform other models. Nocolas Stokes B.Sc [2] takes account of more complex semantic word associations besides the traditional approaches of 'bag of words' models. Kristie Seymore and Ronald Rosenfeld [3] develop a language model adaptation scheme and a topic specific language model. The results show a small improvement in word rate by using this adaptation. Loulwah Alsumait, Daniel Barbara and Carlotta Domeniconi [4] present Online Topic Model (OLDA) to capture the thematic patterns and identify emerging topics automatically. A solution based on the empirical bayes method is proposed to update the current model basing on the information inferred from the new stream of data. Their approaches also provide an efficient mean to track the topics over time and detect emerging topics in real time. Ramesh Nallapati [5] uses a semantic language model for topic    detection and tracking. A unigram language model is built for each semantic class in a news story. Results on the test set show a marginal improvement over the unigram performance but are not very encouraging on the whole, K. Rajaraman and Ah-Hwee [6] Tan use self- organizing neural networks to topic detection, tracking and trend analysis.

Most of the topic detection approaches described above all has one explicit prerequisite that all the topics (categories) are predefined. In real situations, any prior knowledge of category structure or possible categories may not be known and embedded implicitly in the large scale data set. Given these circumstances, a new approach of clustering keywords to topic detection is proposed by Christian Wartena and Rogier Brussee [7].

The keywords are extracted and clustered based on different similarity measures using the induced k- bisecting clustering algorithm. Experiments show that clusters of keywords correlate strongly with the Wikipedia categories of the articles. A distance measure based on the Jensen-Shannon divergence of probability distributions is used and experiments find that the distance measure outperforms the cosine similarity. In particular, anewly proposed term distribution taking co- occurrence of terms into account gives best results.

This paper proposes a new approach to topic detection without any prior knowing of possible topics. While sharing the methods of clustering keywords (i.e. topic features), this paper tries to seek a new way to select topic features. A lot of feature selection approaches have been introduced and designed. Kenji Kira and Larry A. Rendell present [8] a new feature selection algorithm called RELIEF, which uses statistical method and avoids heuristic search. It is inspired by instance- based learning. RELIEF assigns a relevance weight to each feature and selects those features whose average weight is above the given threshold. Yiming Yang [9] and Jan o. Pedersen compared the feature selection methods in statistical learning of text categorization evaluated document frequency (DF), information gain (IG), mutual information (MI), a x2-test (CHI) and term strength (TS) and found IG and CHI most effective in their experiment. Forman [10] introduced an empirical comparison of twelve feature selection methods, such as IG, PR, DFReq, etc.

Since the best feature selection contains the least number of features that contribute to accuracy, while a barely satisfactory selection might either contribute a    Proceedings ofIC-NIDC2012  low accuracy or have a high dimensionality, this paper first uses the TF-IDF evaluation parameters to select a small amount of features as feature pre-selection set for topic features selection and then applies the CAR and maximal cliques mining algorithm [11] to cluster features into topics. Topics are typically represented by a cluster of words which contribute to topic accuracy.

Section2 introduces an overview of the design. Section 3 describes the implementation of our feature selection algorithm for text classification and its application on topic detection and tracking. Section 4 describes some side-effects of the algorithm and presents several refinements of the algorithm that the paper have found useful. Section 5 has performance of measurements of our implementation for a variety of tasks. Section 6 describes how the algorithm is used at our own currently running system. The paper also discusses some lessons the paper learned in designing and implementing the algorithm. Section 6 also presents our conclusions and future outlook.

2 Design overview  Figure 1 shows the major process of the input data, and the final output is a set of category features that the paper wants.

Figure 1 Algorithm architecture  The algorithm starts by reading input data files. Each input file consists of an amount of lines with each line representing a document. In the Word Segmentation phase, the segmentation algorithm is based on statistics.

The paper has a prepared dictionary and uses the backward maximum backward match algorithm. To reduce the complexity, the Aho-Corasick automation algorithm is used to match words. Aho-Corasick automation algorithm is a known algorithm for multi pattern string match algorithm. The paper found it preferablly useful while matching a word from the dictionary. The dictionary is constructed using a trie tree data structure. A fail pointer is used when one word fails to match a pattern word in the Trie Tree and when this situation occurs, the fail pointer points to another pattern word in the trie tree where the word might match.

The segmented documents are stored in memory using a special data structure to reduce 10 operations. Storing the segmented documents into database or disk files is also acceptable.

In Text Filtering phase, redundant words and litter words are just filtered since it might produce noise for the feature selection and lessen accuracy, such as stop words, meaningless words, punctuation marks, pronouns, conjunctions, odd characters, full-width characters and so on. In Building Inverted Indexing phase, the paper build (key, value) pairs for each word with key refers to the id of the word and the value stores information of the word such as word frequency, etc. In feature Pre-selection phase, the paper sorts the word according to term frequency and select top N (e.g. N = 500) words as set of category features. In Building Adjacent Matrix phase, a sparse matrix recording words correlations is constructed. In Mining Maximal Cliques phase, the maximal cliques mining function [11] accepts the sparse matrix constructed in the previous step and produces a number of word sets. The words in each word set have a closer correlation. Each word set represents a category and the words in the word set are called category features. Till this process, the paper finally gets the category features the paper wants. And these category features can be used in the Topic Detection and Tracking process, in which phase, the paper computes similar score of the text features with each category feature and distribute the text to the category when the score is higher than the category score. As you see, the paper is actually using a bag of words (i.e. features) to represent a topic and here you can just take a topic as a category. Topic detection is usually done by mining maximal cliques while topic tracking by computing similarity scores.

3 Implementation and execution  In implementing an algorithm for our needs, it has been guided by the design described. The paper alluded to some key observations earlier and now lays out our implementations in more details.

3.1 Inverted indexing  The Inverted Indexing is used for further computation and matrix building. The paper finds it preferable useful while gathering word information and more importantly, it saves a lot of 10 operations. The paper scans the files only once and further information gathering can be acquired with a relatively less computation.

3.2 The matrix  According to the CAR and the maximal cliques mining algorithm, the paper uses the adjacent matrix to produce 2-item credible sets matrix and produce all the credible association relationships using the thought of maximum clique [11].the adjacent matrix is built in different ways, the paper lays out our ways in details.

Before that, the paper gives some definitions. The matrix the paper building may call matrix A and the element in the matrix is called aij with i referring to its row index and j referring to its column index. i and j are all started from zero. In the following three ways, When    Proceedings ofIC-NIDC2012  Similarity Computation  i = }, the value of au is the number of documents which include the word whose word id is i.

4 Refinements  computation between the text features and topic features.

Then the paper compares the similarity score to the topic threshold and if the score is over the threshold, the paper judging the text belongs to this topic.

The implementation described in the previous section requires a number of refinements to achieve high performance, availability and reliability. This section describes portions of the implementations in more detail in order to highlight these refinements.

4.1 Inverted document frequency  In order to solve the problem of eliminating noise words, the paper also tests to use the inverted document frequency (IDF) in the Building inverted index phase.

Since some meaningless words appear to have a higher term frequency while contributing less to the result, the paper sorts the words according to TF-IDF instead of TF to discard those noise words. The paper found it preferred useful and the performance strongly improved.

4.2 Quasi maximal cliques mining  While mining maximal cliques [11] phase is completed, the paper acquires a word set representing a specific category and the words in the word set are taken as category features. In our assumption, a category should be obviously different from other categories. But in our experiment, the paper finds that some categories still have a closer connection between each other and these categories should belong to a particular category apparently. In order to get an optimal result, the paper tried to have a merge operation after the maximal clique [11] mining process, called quasi maximal clique mining. Since technically manipulate quasi maximal clique mining process from the very beginning might be unreachable and the paper might lose some results, the paper decides to design quasi maximal clique mining as the following process:  1) The initial condition is the maximal cliques, which the paper gets in the maximal cliques [11] mining phase.

2) The maximal cliques are put into a queue. Each time the paper pops one clique from the queue and computes the similarity score of the clique with other cliques, the computation formula is defined as follows:  sij=2*(Si ns} )/(Si us}). s, refers to the number of words in clique i and Si n s} refers to the intersection set  of Si and s}' while Si us} refers to the union set of Si and s} . A threshold called BETA is predefined as judge criteria.

Topic featu resText Representation  I)When i 7:-} , the value of au is the number of documents which include the two words whose word ids are i and}.

2) When i 7:-} , the value of au is the sum of the minimum occurrences of word i and word}, where word i and word} are in the same document.

3) When i 7:- } , the value of au is computed this way: word i and word} occur in the same document, word i and word} are separated by d words, if d < DISTANCE threshold, the paper plus the co-occurrences of word i and word } by one, and au is the sum of the total cooccurrence of word i and word}.

According to the CAR and the maximal cliques mining algorithm [11], the paper takes the 2-item credible set matrix matrix B for short. Each element in matrix B may be called bij , with i referring to its row index and} referring to its column index. i and} are all started from zero. For each bij in matrix B, bu=au/ (au+ajj -au). According to the CAR and the maximal cliques mining algorithm [11], the input of the maximal cliques mining function is a 0-1 matrix. the paper call matrix C and cij represents each element in matrix C with i referring to its row index and} referring to its column index. i and} are all started from zero. For each cij in matrix C2 , cij = 1 when bij >CONFIDENCE threshold, otherwise cij=O.

3.3 Topic Detection and Tracking  Topic detection and tracking are a practical application of what the paper talks above. Here, a topic can be taken as a category and the paper uses the maximum cliques mining algorithm [11] to detect new topics. The tracking process can be illustrated in Figure 2.

Figure 2 Topic Tracking Process  The topic tracking process starts by accepting input files and represent each text with a vector of words (i.e. text features), then the paper applies similarity  3) If the similarity score is over BETA, clique i and clique} is eliminated from the queue and a new clique is put into the queue.

4) If the similarity score is below BETA, clique i and clique are not supposed to be a new clique, in this     Proceedings ofIC-NIDC2012  situation, clique i doesn't match clique j. if one clique mismatches any other cliques in the queue, the paper puts the clique into the result set.

5) The process continues until the queue is empty.

When this happens, the cliques that the paper gets in the result set are the final results the paper wants, called quasi maximal cliques. The program ends.

5 Experiment results  In this section, the paper measures the performance of our algorithm in several tasks. The tasks described below are representative of a large subset of the real program applications. The data set the paper chooses is 3 months (20 11-03~2011-05) data record from our campus BBS. The algorithm is running on ordinary X86 machines with 2G memory.

5.1 Feature pre-selection  Table I shows the statistics for feature pre-selection process. The paper selects the top N words as the features set for further feature classification. The initial selection criteria are only TF. From the table It can obviously be seen that without IDF factor, only 77 words out of the selected 100 words are useful and while N = 200, the meaningless words increased from 23 to 41. In addition, from the table the paper can also see that if IDF factor is taken into consideration, there is a decent improvement of useful words. The Table also illustrates that a subject word dictionary contributes a lot to selecting meaningful words. Actually, the paper found that the building of an optimal subject word dictionary is very labor-consuming and requires a lot of manual work. Finally, it can be seen from the table that a stop word dictionary contributes less to the final results, whereas it is found that the stop word dictionary is preferable useful in some special occasions.

Table I feature pre-selection statistics  Experiment N(size of features set)  100 150 200 without idf 77 122 159 with idf 85 123 173  with a subject word dictionary 91 136 184  with a stop word dictionary 91 136 187  5.2 Matrix building  Table II shows statistics for matrix building and cliques accuracies. Matrix type 1~3 are corresponding to the three methods of building matrix the paper have discussed in Section 3.and matrix type 4~6 are corresponding to the refinement methods of building matrix the paper have discussed in Section 4. From Table II, It can be concluded that the methods of refinement seem to reduce the performance. It can be concluded that the definition discussed in Section 3 is more appropriate.

Table II matrix building and cliques accuracy statistics  The number of Topic accuracycliques get  Experiment Quasi Maximal QuasiMaximal maximal cliques maximal topic topiccliques accuracy accuracy  Matrix 493 124 64% 72%type 1 Matrix 502 119 58% 71%type2 Matrix 583 151 73% 78%type 3 Matrix 223 109 43% 62%type 4 Matrix 219 III 39% 55%type 5 Matrix 156 98 46% 53%type 6  In addition, from the table It can also be concluded that quasi maximal cliques [11] mining seems to have a better topic accuracy compared with maximal cliques mining, which is a matter of course , whereas quasi maximal cliques mining actually occupies a lot of execution time.

5.3 Quasi maximal cliques mining  Table III presents statistics for topic accuracy. From the table, it can be seen that with topic threshold separately calculated , the paper gets about 7%~8% improvement to the final topic accuracy, whereas it causes a lot more operation in the building inverted indexing phase , which can be tolerated.

Table III topic accuracy statistics  Experiment Topic accuracy  without topic 73%Maximal cliques threshold  with threshold 81%  Quasi maximal without topic 78% cliques threshold  with threshold 85%  6 Conclusions and future outlook  The paper introduces a practical approach on feature selection based on credible association rule mining and applies the maximal cliques mining algorithm in a real campus public opinion detection and management system. The system has been running for more than a year so that we have abundant data to analyze. Given the unusual API of our algorithm, an interesting question would be how difficult it has been for other modules of our system to adapt to use it and how to make the best use of it. The paper is currently trying to open more external access interface to other modules, so that they can select the information they prefer. Besides, we are in the process of implementing a distributed    Proceedings ofIC-NIDC2012  version of our algorithm, given that the social network data scales and the data process efficiency needs improvement. Meanwhile, we are currently implementing several additional features tailored for other module of our system.

Finally, we have got a substantial amount of flexibility from designing our own topic detection algorithm. Our control over the algorithm's implementation means that we can remove bottlenecks and inefficiencies for other modules which depend on the algorithm as they arise.

Acknowledgements The authors thank Wenyi Yang, Xi Wang for their assistance in the building of dictionaries and in the testing of the algorithm and also thank their feedbacks on improving the performance of the algorithm. The research was supported by the Chinese III program Advanced Intelligence and Network Service under grant No. B08004 and a key project of the Ministry of Science and Technology of China under grant No.20llZX03002-005-0l and Beijing Natural Science Foundation under grant No. 4123102.

