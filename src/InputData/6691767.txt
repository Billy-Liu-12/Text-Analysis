Parallel SECONDO: Practical and Efficient Mobility Data Processing in the Cloud

Abstract?This paper presents a hybrid parallel processing system, named Parallel SECONDO. It combines the Hadoop framework and a set of single-computer SECONDO databases, in order to introduce the mobility data procedures into the parallel processing community, and vice versa.

The system keeps the front-end and the executable language of SECONDO to allow the users to state their parallel queries like common sequential queries. Besides, a set of auxiliary scripts is provided so as to make it easier to manage the system no matter how large the underlying cluster is, and keep the Hadoop platform as a transparent level of the system. Further, a parallel data model is also proposed in this paper to encapsulate all available SECONDO data types and operators. Thereby, it is able to transform any SECONDO sequential query to its corresponding parallel expression. For instance, all example queries in the moving objects database benchmark BerlinMOD are transformed, and two of them are demonstrated in this paper. In the last evaluations, this paper illustrates that Parallel SECONDO is not only a practical but also an efficient system.

For queries involving large amounts of data, it performs both linear speed-up and scale-up.



I. INTRODUCTION  In recent years, the MapReduce paradigm [6] and its open-  source implementation Hadoop attracted a lot of attention  from the database community, since it provides a simple,  flexible and efficient approach to process massive amounts  of data on clusters consisting of hundreds or even thousands  of low-end computers. However, the MapReduce paradigm  is low-level and rigid, creating a barrier to the end-users  for their customized requirements, since they have to learn  many details about the MapReduce programming instead  of using understandable declarative languages like SQL.

Besides, in the past decades, there developed abundant  database techniques for the single-computer environment.

For instance, our group has studied the moving objects  database for more than ten years, and proposed tens of data  types [10] to represent and a large number of algorithms  [15] to efficiently process moving objects data, in the open-  source database system named SECONDO. Obviously it is  very difficult and also unnecessary to implement them again  in the parallel platform.

Regarding the above issues, this paper presents Parallel  SECONDO, a hybrid parallel processing system built upon  Hadoop and a set of SECONDO databases. It relies on the  Hadoop framework to assign and schedule tasks running on  a computer cluster in parallel, while the tasks? embedded  procedures are then processed by SECONDO for the sake  of efficiency. The new system keeps the front-end and the  executable language of SECONDO, so that the user can use  Parallel SECONDO still like on a single computer.

In addition, Parallel SECONDO inherits the capability from  SECONDO of processing special data types, like spatial and  moving objects. This is not directly supported in neither  MapReduce or Hadoop as they are primarily studied for  processing massive amounts of unstructured business data  [4]. To the best of our knowledge, Parallel SECONDO is the  first practical and efficient parallel system for the mobility  data processing.

The rest of this paper is organized as follows. The  second section introduces the related work around Parallel  SECONDO. The infrastructure of the system, and also the  data model for representing parallel queries are explained in  the third section. Further, several example parallel queries  are demonstrated in the third and the fourth sections, in order  to illustrate the practicality of the system. At last, the system  is evaluated with the parallel BerlinMOD benchmark on our  small cluster in the fifth section, while a short conclusion and  the extended work about Parallel SECONDO are introduced  at the last section.



II. RELATED WORK  Hadoop forces the users to implement ad-hoc programs  for their queries, since it does not provide any declarative  language. These pieces of custom code are hard to maintain  and reuse, creating barriers for co-workers. Regarding this  issue, the project Hive [17] provides a language named  HiveQL. A HiveQL statement is similar as a SQL query,  and is converted into MapReduce jobs by Hive?s Driver  component. Besides, it views the HDFS as a data warehouse  and offers a meta-store to keep schemas of involved data, in  order to optimize the generated jobs. However, Hive assumes  all the tables in the system are independently distributed on  the cluster, therefore queries involving multiple tables like  join always need to push most procedures into the Reduce  stage and cause additional overhead. For this question,  HadoopDB [4] proposed its SMS (SQL to MapReduce to      SQL) component, which re-scans the query plan generated  by Hive twice, and recreates SQL statements so as to  process the queries efficiently in their integrated PostgreSQL  database.

Inspired by the work like HadoopDB, Parallel SECONDO  combines the Hadoop framework with a set of SECONDO  databases. SECONDO[13, 14] is a ?generic? single-computer  database system, aiming to be filled with the implementa-  tions of various DBMS data models, like relational, object-  oriented, temporal or XML models. It represents different  data types like moving objects, spatial data etc, and pro-  cesses them efficiently[15]. Roughly speaking, SECONDO  includes three level modules: kernel, optimizer and user  interface. The optimizer produces optimal executive plans  according to users? SQL-like queries. The user interface  provides the front-end to this system, containing both the  text and the graphical interface. The latter provides a visual  way to represent the different types of data in a vivid  way. These two modules are kept unchanged in Parallel  SECONDO, in order to make the parallel system to be  compatible with the single-computer interface. The kernel  can be extended by various algebras based on the concept of  second-order signature [8], each contains type constructors  and operators for different data models. All functions in  Parallel SECONDO are implemented as two algebras in the  kernel module. Further implementation details of SECONDO  are described in [12].

Till now, as far as we know, there is yet no approach  proposed to systematically process special data types in  parallel, especially for moving objects data. Spatial-Hadoop  [3] studies the generic parallel processing for spatial data  by extending the Hadoop framework. However, it supports  only a limited number of spatial data types, indices and  operations. New data types and operators can only be  extended as MapReduce programs, making barriers for the  other researchers. There are also several studies made for  processing particular spatial operations. BRACE [18] uses  MapReduce to simulate agents? behaviors in parallel. It  abstracts behavioral simulations in the state-effect program-  ming pattern, which can be processed as iterated parallel  spatial join operations, with two consecutive MapReduce  jobs at each time.



III. PARALLEL SECONDO  The basic infrastructure of Parallel SECONDO is shown  in Figure 1. It is composed by a Hadoop framework and a  number of Data Servers (DS). Data Servers are the basic  processing units in Parallel SECONDO, each contains a  compact SECONDO called Mini-SECONDO and its database.

Besides, each DS has a node of PSFS (Parallel SECONDO  File System), which is a distributed file system particularly  developed for Parallel SECONDO. One of the Data Servers  is set to be the master Data Server (mDS), and its database  is called the master database. The other Data Servers are set  Figure 1: Framework of Parallel Secondo  to be the slave Data Servers (sDSs), and their databases are  called the slave databases. It is possible to set several Data  Servers on the same computer in order to take the full usage  of its computing resources, since nowadays it is common  that even a low-end PC also contains one or more processors  with multiple cores, several hard disks and a large amount  of memory. On every cluster node, one of its Data Servers  is set to be the Main Server (MS), in which a Hadoop node  is set. The computer that contains the mDS is also set to be  the master node in Hadoop.

In Parallel SECONDO, parallel queries are only submitted  to the master database. They are stated in SECONDO exe-  cutable language and later transformed to Hadoop jobs. The  details about stating parallel queries will be further intro-  duced later. In the runtime, Hadoop jobs are executed in a  sequence of Map-Reduce stages. Within each stage, multiple  tasks run in parallel on all involved sDSs, being applied and  scheduled by the Hadoop framework, in order to achieve  a balanced workload assignment on the cluster. Besides,  each task contains a set of SECONDO sequential queries  that are processed by the corresponding Mini-SECONDO  independently so as to achieve an efficient performance. In  general, Hadoop is kept as a transparent level to the end-user,  who can use the master database to process both sequential  and parallel queries at the same time.

Two main issues are considered in this paper for achieving  the purposes above. First, when managing a cluster consist-  ing of tens or even hundreds of computers, even routine work  like installing, updating, starting and stopping the system     becomes overburdened with trivial details, and sets barriers  for end-users. Therefore, Parallel SECONDO provides a set  of auxiliary scripts to help the users to handle such problems  easily and only from the master node. Second, a parallel data  model is provided to facilitate representing the distributed  data and the parallel queries, enabling the users to easily  convert a sequential query to its parallel expression, without  learning the details about the Hadoop programming and the  cluster construction.

A. Auxiliary Scripts  The auxiliary scripts are provided to help the user to  manage the whole system on the cluster from the master  node only. Roughly they are divided into four kinds, and  are introduced in the following, respectively.

First, ps-cluster-format helps the user to initialize Parallel  SECONDO, including deploying all Data Servers and in-  stalling the Hadoop framework. It runs according to a config-  uration file, which has the same format as the configuration  file for SECONDO. An example configuration file is also  provided, based on which the customized configuration only  needs few changes according to the user?s own environment.

Further, a graphical editor is provided to help the user  prepare the configuration file and check whether all needed  elements are available in the current cluster, in order to  improve the robustness of the system. In contrast, the ps-  cluster-uninstall performs the opposite function, removing  the whole system from the cluster safely.

Second, SECONDO is designed as an extensible database  platform, new data models are extended as algebras. Each  algebra contains a set of special data types and related oper-  ators. In order to keep the extensibility in Parallel SECONDO  as well, the ps-secondo-buildMini is implemented. It extracts  the necessary components from the SECONDO in the master  node, and distributes them to all Data Servers as Mini-  SECONDO. Thereby, all Data Servers can support the new  features immediately.

Third, during the runtime, each Mini-SECONDO database  is remotely visited through a daemon named SECONDO  Monitor. All involved monitors should be started before  processing any query. In order to start and stop all SECONDO  monitors in the cluster, the scripts ps-start-AllMonitors  and ps-stop-AllMonitors are provided. Besides, a script ps-  cluster-queryMonitorStatus is prepared to list all running  monitors on the cluster.

Fourth, Parallel SECONDO inherits both the text and the  graphical interface for SECONDO. The ps-startTTYCS is  prepared to start the text interface to any running monitor in  the cluster. The graphical interface can be started as usual,  connecting to Parallel SECONDO by setting the IP address  and port number of the monitor in the mDS.

Typically, installing and accessing the Parallel SECONDO  only needs the following commands:  $ ps-cluster-format  $ ps-secondo-buildMini -c  $ start-all.sh  $ ps-start-AllMonitors  $ ps-startTTYCS -s 1  Here one first initializes the system by using the ps-cluster-  format script. Afterwards, the script start-all.sh starts up the  Hadoop framework, which is provided by the Hadoop itself.

Next, all the monitors are started up, and the text interface  of the master database is opened. The ps-startTTYCS asks  a parameter ?s? for indicating one Data Server set on the  computer. It is 1 as the Main Server on the master node  contains the master database. All above commands are  invoked on the master node only, hence the user can use  Parallel SECONDO like on a single computer.

B. Parallel Data Model  SECONDO supports two levels of query languages, SQL-  like and executable. At the current stage, the parallel queries  are formulated in the executable language, which allows  the user to precisely describe the query plan consisting  of database objects and operators, in order to achieve an  efficient performance. The parallel model is provided to  represent the distributed data and Map/Reduce operations  in the parallel queries.

The ???? Data Type: A distributed data object is repre-  sented as an ???? object in the master database. The ????  is a wrap data type, being able to encapsulate all available  SECONDO data types. After a SECONDO object has been  partitioned, its data are kept on sDSs, while the partition  scheme is kept in the master database as an ???? object.

Basically, ???? represents a PS-Matrix structure of the  distributed data, as shown in Figure 2. A SECONDO object  ?, like a relation, is divided into a PS-Matrix through  two distribution functions, ?????? and ?????????. The ?????? divides the object into ? parts, each part is viewed as one row of the matrix, and must be kept on one DS. It  is possible that ? is larger than the cluster scale 	 , i.e., the  number of the sDSs, and to let one DS contain multiple rows  of the matrix. Afterwards, the ????????? further divides rows into ? columns, and makes up a ??? PS-Matrix at  last.

There exist two kinds of ???? objects, according to dif-  ferent ways of storing the distributed data.

? Distributed Local Files (DLF): Each piece of data of  the ? ? ? PS-Matrix is exported from the SECONDO  database as a disk file, kept in some PSFS nodes, called  sub-file. SECONDO can read a set of sub-files from  the other computers in the cluster and import them to  its own database. Therefore, sub-files are prepared to  exchange data among the slave databases during the  parallel procedures. A PS-Matrix containing sub-files is  represented as a DLF ???? . At present, this kind of ????  is only prepared for relations that can be exported as  sub-files. However, it is widely used in various parallel     Figure 2: PS-Matrix  Name Signature  F lo  w spread ??????(T) ?????(T) collect ????(T) ???????(T)  H ad  o o p hadoopMap ???? x fun(mapQuery) x  ? ?????  hadoopReduce ???? x fun(reduceQuery) ????? hadoopReduce2 ???? x ???? x fun(reduceQuery) ?????  para ????(T) ?T  Table I: Parallel Operators  queries since most SECONDO data types can be stored  in relations as attributes.

? Distributed Local Objects (DLO): A DLO ???? stands  for an ??? PS-Matrix, where each row is saved in one  slave database as a standard SECONDO object, named  sub-object. All sub-objects belonging to the same ????  also have the same name in all slave databases. DLO  ???? can wrap all available SECONDO data types.

However, since the sub-objects cannot be transferred  among the Data Servers, DLO is not as flexible as DLF,  it is mainly prepared for un-exportable data, like an  index structure.

The ???? objects are prepared for large-sized data, like  relations of hundreds megabytes or even gigabytes. Besides,  there also exist some small-sized data, like an integer,  string or rectangle. They are used as global parameters for  the parallel queries, and should be broadcasted to every  sDS. Regarding this issue, Parallel SECONDO indicates such  kind of objects as DELIVERABLE data. They are stored  only in the master database as standard objects, and are  duplicated into every slave database by embedding their  values inside queries during the runtime, in order to simplify  the representation of the parallel queries.

The Parallel Operators: Along with the ???? type, a set  of parallel operators are provided and briefly classified to  three kinds: Flow, Hadoop and Assistant, listed in Table I.

First, the ?????? operator partitions a SECONDO relation  into a PS-Matrix, exports and distributes all matrix pieces  to sDSs as sub-files, and returns a DLF ???? object. In  contrast, the ??		?? reads a DLF ???? object, assembles  all required sub-files from the cluster and imports them  to the database. They are called Flow operators as they  can concatenate sequential and parallel queries. The parallel  platform is usually prepared for processing large-scale data,  since the single-computer system is more efficient on light-  weight queries.

Next, the Hadoop operators are used to describe parallel  queries and convert them to Hadoop jobs. Each operator  contains a template Hadoop job and an argument function.

During the runtime, the function query is embedded into the  Hadoop job and executed by Mini-SECONDO on sDSs in the  Map or Reduce stage of the job, according to which Hadoop  operator is used. For example, both the ???????????  and the ???????????? operators process the argument  function in the Reduce stage. The only difference between  them is that the first operator is an unary operator, while  the other is a binary operator. Yet there is no operator that  can represent both the Map and Reduce stages, because it  is impossible to implement a SECONDO operator containing  two continued argument functions. However, a simple so-  lution is used to circumvent this problem by extending the  ????????? operator with an argument called ?executed?.

If this parameter is set as false, then the operator does  not create the Hadoop job, instead it delivers its Map  stage function to the next Reduce operator, which makes  up a Hadoop job containing both the Map and Reduce  procedures. At last, the assistant operator ???? indicates  ???? objects used in Hadoop operators? argument functions.

C. Example  In order to introduce the usage of the above parallel  operators, a simple example is explained at first, which  performs a hashjoin between two relations:  query plz feed {p} filter[.PLZ_p > 10000]  Orte feed {o} hashjoin[Ort_p, Ort_o]  count;  Here two relations plz and Orte are used in the query,  describing some German cities. The plz has two attributes,  Ort and PLZ . The Ort indicates the city names, and the PLZ  contains the cities? postal codes. The Orte relation contains  more information about the cities, but it also uses the Ort  attribute for city names.

The query is expressed in SECONDO executable language,  using postfix notation. For example, the ???? operator is  stated after the relation plz, so as to construct a so called  stream of tuples from it and let the subsequent operators  process the tuples iteratively in pipelined mode. Since both  relations use the attribute Ort to indicate the city names, the  rename operation is used to avoid the naming conflict in  the join operator. It is used like ??x??, then every attribute of the input relation adds ? x? on its name. The ? ??  operator picks out cities from the plz relation that have postal  codes larger than 10,000. The ?.PLZ p? denotes the attribute     PLZ, which is extended because of the rename operation.

Next, the ???????? operator takes two streams of tuples,  and finds pairs of tuples that have the same value on the  join attributes Ort, with the hash-join algorithm. At last, the  ??	? operator counts the number of the result tuples.

For the parallel processing, both relations are first dis-  tributed on sDSs with the Flow operator ?????.

let plz_PLZ_dlf = plz feed  spread[; PLZ, TRUE;];  let Orte_Ort_dlf = Orte feed  spread[; Ort, TRUE;];  Both queries partition the given relation into an ? ? ? PS-  Matrix, and output a DLF ???? object. The two parameters  indicated in both ????? operations define the distribution  function ??????. The plz is divided by its PLZ attribute, while the Orte is divided by the attribute Ort. The second  parameter decides whether the partition attribute is kept  in the distributed data after the spreading. Here in this  example, both partition attributes are retained. The created  ???? objects? names are triples consisting of the original  relation name, the partition attribute and the ???? type. In  such way, it is easier for the user to distinguish ???? objects  that are created for the same data set, but partitioned in  different ways.

In Hadoop, it is usual to process the join query in  the Reduce stage. The statement of this approach for the  example is:  query plz_PLZ_dlf hadoopMap[DLF, FALSE  ; . {p} filter[.PLZ_p > 10000]]  Orte_Ort_dlf  hadoopReduce2[Ort_p, Ort, DLF, PS_SCALE  ; . .. {o} hashjoin[Ort_p, Ort_o]]  collect[] count;  This parallel query is mainly composed by two Hadoop  operators, and each accepts three parts of arguments. The  first part is the ???? object(s) stated in front of the operator,  called the input argument. The other two parts are set inside  the square brackets behind the operator and are separated by  a semi-colon. The second part contains a series of parameters  divided by commas and determines various behaviors of the  operation. The last part is an argument function that will  be encapsulated into the Map/Reduce tasks of the created  Hadoop job. Within it, the symbols ?.? and ?..? denote the  first and second input arguments, respectively.

The ????????? operator here declares the procedure  for the plz PLZ dlf in the Map stage. Each of the ? Map  tasks runs on one sDS, reads one row of data from the input  ???? object and performs the ?? ? operation. The DLF in  the second part arguments means that this operator intends  to create a DLF ???? . However, this operator does not create  the Hadoop job, since its ?executed? parameter in the second  part arguments is set to be false. Therefore, its Map tasks  are merged into the Map stage of the Hadoop job created  by the next ????????	?? operator.

The ????????	?? operator accepts two input ????  objects, the Orte Ort dlf and the ?unexecuted? one created  by the previous ????????? operation. It also creates a  DLF ???? , so as to be accepted by the following ?????   operator. The ?? ??? is an integer, indicating the  number of the created Reduce tasks. It is usually set as  the number of processor cores inside the cluster, and is  larger than the quantity of sDSs in order to process as  many as possible Reduce tasks in parallel, so as to take  the full usage of the cluster?s computing resources. It and  the first two second part parameters indicate the distribution  function ???????? for the Map results. In the runtime, each Map task reads one row of data from the plz PLZ dlf,  then re-distributes the row to ?? ??? columns af-  ter the ?? ? operation. At the same time, it also reads  one row from the Orte Ort dlf and directly re-distributes  the data to ?? ??? columns. Both are partitioned  based on the Ort attribute. Consequently, two intermediate  ???? ??? PS-Matrices are created in the Map stage.

Afterwards, each of the ?? ??? Reduce tasks reads  one column data from both the intermediate PS-Matrices,  and performs the ???????? operation. In the end, the  ????? operator gets all distributed results to the master  database.

For the same sequential join, it is possible to use var-  ious parallel approaches to process it [5], since Hadoop  supports both the map-side and the reduce-side join pro-  cedures. Therefore, the above example can also be com-  pletely processed within the Map stage, i.e., using only the  ????????? operator. If so, then both relations should be  partitioned based on the join attribute Ort at first, and one  is distributed as a DLO ???? object.

let plz_Ort_dlf = plz feed  spread[; Ort, TRUE;];  let Orte_Ort_dlo = Orte_Ort_dlf  hadoopMap[; . consume];  Here the ????????? operator converts a DLF ????  object to a DLO ???? object. In its Map stage, each sDS  reads one row of sub-files from the PS-Matrix, and then uses  the operator ????	? to store the tuples into a relation as  a sub-object of the returned ???? . Afterwards, the map-side  parallel join query is:  query plz_Ort_dlf  hadoopMap[DLF; . {p} filter[.PLZ_p > 10000]  para(Orte_Ort_dlo) feed {o}  hashjoin[Ort_p, Ort_o]]  collect[] count;  This query simplifies the expression, and its argument  function is almost the same as the sequential query, except  all involved objects are replaced by ???? objects indicated  with the assistant operator ????. However, it partitions the     join procedure into at most ? Map tasks, since all DLO ????  objects can only be divided as a ??? PS-Matrix. Each Map  task reads one row of data from both the input PS-Matrices,  one is a sub-file in the PSFS node and another is a sub-  object in the local Mini-SECONDO database. Nevertheless,  this parallel query reduces the overhead for the network  traffic as there is no intermediate result produced between  the Map and the Reduce stages. Besides, it is also useful for  join processing involving objects that can only be distributed  as DLO ???? , for example, if one side data comes from an  index instead of a relation.

Here the DLO ???? Orte Ort dlo is referenced in the  argument function, and each Map task reads only one row  from it. It is also possible to use a DLF ???? there, but  then each Map task will read the complete PS-Matrix and  duplicate the Orte relation in every task. This mechanism  is prepared for join between two relations having a very  different scale, like that the Orte relation is much smaller  than the plz relation. In such a situation, the parallel query  can be stated as:  query plz_Ort_dlf hadoopMap[DLF;  . {p} filter[.PLZ_p > 10000]  para(Orte_Ort_dlf ) {o}  hashjoin[Ort_p, Ort_o] ]  collect[] count;  It is almost the same as the last parallel query, except the  ???? operator quotes a DLF instead of a DLO ???? object.



IV. PRACTICAL PROBLEMS  BerlinMOD [9] is a benchmark prepared for evaluating  moving objects databases, with simulated data and a set of  common moving objects queries. The size of the generated  BerlinMOD data is decided by a scale factor. If the factor  is 1, historic trajectories of 2000 vehicles running on the  street network of Berlin in 28 days are simulated, taking 11  GB disk space, and requiring more than two hours for data  generation with one computer.

In order to accelerate the data generation, a set of scripts  are prepared in Parallel SECONDO, and published on our  website 1 [1]. Thereby, generating the above data set with  Parallel SECONDO on our six-computers cluster costs only  15 minutes. More evaluation results about the parallel data  generation are shown in the next section.

Besides generating the data, the benchmark also provides  17 range queries, covering all kinds of interesting prob-  lems in moving objects databases. All these queries can  be converted into the corresponding parallel queries. For  demonstrating the practicality of Parallel SECONDO, two are  explained in this section. The first range query finds a set  of query vehicles? models by probing a B-Tree built on all  vehicles? licences. Its sequential query is:  1For historical reason, objects in the published scripts are named differ- ently from this paper, but they will be adjusted later.

let OBACRres001 = QueryLicences feed {O}  loopjoin[ dataSCcar_Licence_btree  dataSCcar  exactmatch[.Licence_O]]  project[Licence, Model]  consume;  The relation dataSCcar contains all vehicles? complete  information, and each tuple describes a vehicle?s com-  plete trajectory during the observation period. Besides, the  dataSCcar Licence btree is a B-Tree index built for this  relation, based on vehicles? licence numbers. At last, the  QueryLicences relation contains the licence numbers of a  set of query vehicles.

The sequential query traverses the relation QueryLicences  with the ???????? operator, and uses each tuple?s Licence  value to probe the dataSCcar Licence btree. At last the  results are extracted out from the dataSCcar relation by  using the ??????? operator. Accordingly, the parallel  query for this range query is:  let OBACRres001_Moid_dlf =  dataSCcar_Licence_btree_Moid_dlo  hadoopMap[ "OBACRres001", DLF  ; para(QueryLicences_Id_dlf) feed {O}  loopjoin[ . para(dataSCcar_Moid_dlo)  exactmatch[.Licence_O]]  project[Licence, Model] ]  collect[] consume;  All ???? objects are named in the same way as the  last section. The dataSCcar Licence btree Moid dlo is a  distributed B-Tree index, it is built up based on vehicles? li-  cences and partitioned based on their identification numbers.

The dataSCcar Moid dlo partitions the relation dataSCcar  based on the Moid attribute, but this attribute is removed  after the partition. The QueryLicences Id dlf is a DLF  ???? prepared for the relation QueryLicences, and is used  inside the argument function of the ????????? operation,  therefore the complete relation is duplicated for every Map  task. It does not cause much overhead for the duplication  since this relation is very small.

This query creates a Hadoop job with only the Map  stage, and also its argument function looks the same as  the sequential query. Each Map task scans the duplicated  query vehicles, probes the sub-B-Tree index and extracts  the result from the sub-relation. At last, the ?????????  operation returns a DLF ???? , with which the parallel results  are gathered to the master database by using the ????	??  operator.

The first benchmark query is simple, and its sequen-  tial query performs efficiently on a single computer. By  comparison, its parallel query is much slower. This is  because running a Hadoop job needs a certain overhead,  like communicating with Data Servers, assigning tasks etc.

For efficient sequential queries like above, such overhead is  more expensive than the query itself, and it is not worthwhile  to process them in parallel. In fact, in all range queries     of BerlinMOD, fourteen of them are efficient enough by  running in a single-computer SECONDO database, and their  performances reduce after being transformed to parallel  queries. However, such queries only cost about 4% of  the whole elapsed time in the benchmark. In contrast, the  other three queries, the sixth, ninth and tenth, take the left  96% cost time. Therefore, converting these three queries is  important.

Below, the tenth benchmark query is introduced and  converted to the parallel expression. It finds when and where  the top ten query vehicles meet the other vehicles, i.e., the  two vehicles at least once have a distance between each other  less than three meters. Its sequential query is:  1let OBACRres010 = 2QueryLicences feed head[10] 3loopsel[ dataSCcar_Licence_btree dataSCcar 4exactmatch[.Licence] 5project[Licence, Journey]] {V1} 6dataSCcar feed 7project[Licence, Journey] {V2} 8symmjoin[(.Licence_V1 # ..Licence_V2) ] 9filter[ (everNearerThan( 10.Journey_V1, .Journey_V2, 3.0)) ] 11projectextend[; QueryLicence: .Licence_V1, 12OtherLicence: .Licence_V2, 13Pos: .Journey_V1 atperiods deftime( 14(distance(.Journey_V1,.Journey_V2) 15< 3.0) 16at TRUE) ] 17filter[not(isempty(deftime(.Pos)))] 18project[QueryLicence, OtherLicence, Pos] 19sort rdup 20consume;  This query first finds the trajectories of the top ten query  vehicles also by probing the dataSCcar Licence btree index  on the relation dataSCcar. Afterwards, it uses a ????????  operation, which basically performs a Cartesian product,  to compare every trajectory-pair composed by one query  vehicle and another vehicle from the dataSCcar relation.

If the two vehicles have met each other, their licences  and the meeting points are returned as the result. A set  of SECONDO operators are used in this query. First, the  ?	? ??? ? ??? determines whether the two vehicles?  trajectories have ever been closer than the certain distance.

Next, the ???????? returns the varying distance between  the vehicles within the observation period. Afterwards, the  ??????? tells the meeting time periods, i.e., when the  vehicles have a distance less than three meters. At last, the  ???? ???? returns the position of the query vehicle at the  meeting time. All these operators are explained with more  details in [11].

Most elapsed time of the above query is spent on the  ???????? operation, which should be converted and pro-  cessed in a parallel way. It is of course possible to convert  the query straightforwardly and keep using the ????????  operator, so as to make the converted query look very  like the sequential statement. However, in such a way, one  side of data needs to be duplicated on every sDS and will  cause a heavy network traffic since each vehicle contains its  complete trajectory during a long period.

Regarding this issue, a state-of-the-art parallel spatial join  method PBSM (partition based spatial merge join) [16, 19]  is considered. It basically uses a 3D grid to distribute all  trajectory pieces, called units, from both arguments, i.e.,  duplicate units for every grid cell that they intersect. Grid  cells are then grouped into partitions, e.g. by an assignment  ? ?? ? ??? ?, where ? is the partition index, ? the cell  index, and ? the number of partitions. The goal is to  distribute densely and scarcely populated cells evenly to  partitions so that all partitions get roughly the same number  of entries. Further, it sequentially loads each partition into  memory and processes the join by some method such as  plane sweep. Here care has to be taken to avoid duplicate  reports, i.e., the same two objects ?meeting? in two different  cells [19]. Note that PBSM can also be represented as a  sequential query, but it is not demonstrated here due to the  page limit of the paper. The parallel query with the PBSM  method is:  1let OBACRres010_Cell_dlf = 2QueryLicences_Top10_Dup_dlf 3hadoopMap[DLF, FALSE; . loopsel[ 4para(dataSCcar_Licence_btree_Moid_dlo) 5para(dataSCcar_Moid_dlo) 6exactmatch[.Licence] ] 7extendstream[UTrip: units(.Journey)] 8extend[Box: enlargeRect(scalerect( 9bbox(.UTrip), 10SCAR_WORLD_X_SCALE, 11SCAR_WORLD_Y_SCALE, 12SCAR_WORLD_T_SCALE), 1.5, 1.5, 0.0)] 13projectextendstream[Licence, Box, UTrip 14;Cell: cellnumber(.Box, 15SCAR_WORLD_GRID_3D)]] 16dataSCcar_Moid_dlo 17hadoopMap[DLF, FALSE; . feed 18extendstream[UTrip: units(.Journey)] 19extend[Box: enlargeRect( 20scalerect(bbox(.UTrip), 21SCAR_WORLD_X_SCALE, 22SCAR_WORLD_Y_SCALE, 23SCAR_WORLD_T_SCALE), 1.5, 1.5, 0.0)] 24projectextendstream[Licence, Box, UTrip 25;Cell: cellnumber(.Box, 26SCAR_WORLD_GRID_3D)]] 27hadoopReduce2[ 28Cell, Cell, PS_SCALE, "OBACRres010", DLF 29; . sortby[Cell] {V1} 30.. sortby[Cell] {V2} 31parajoin2[ Cell_V1, Cell_V2; . ..

32realJoinMMRTreeVec[ 33Box_V1, Box_V2, 10, 20] 34filter[(.Licence_V1 # .Licence_V2) 35and gridintersects( 36SCAR_WORLD_GRID_3D, 37.Box_V1, .Box_V2, .Cell_V1) 38and sometimes(     39distance(.UTrip_V1,.UTrip_V2) 40< 3.0) ] 41projectextend[; 42QueryLicence: .Licence_V1, 43OtherLicence: .Licence_V2, 44DPos: (.UTrip_V1 atperiods 45deftime((distance( 46.UTrip_V1,.UTrip_V2) < 3.0) 47the_mvalue at TRUE )) 48the_mvalue] 49filter[not(isempty(deftime(.DPos)))] 50project[QueryLicence, 51OtherLicence, DPos] 52] 53] 54collect[] 55sortby[QueryLicence, OtherLicence] 56groupby[QueryLicence, OtherLicence 57; Pos: group feed project[DPos] 58sort transformstream concatS] 59consume;  This parallel query is composed by two unexecuted  ????????? and one ????????? ?? operation. All  three make up a Hadoop job at last. The first ?????????  operation (lines 3-15) works similar as the first ex-  ample query, probing the distributed B-Tree dataSC-  car Licence btree Moid dlo and finding the top ten query  vehicles? trajectories from the distributed relation dataSC-  car Moid dlo.

Besides, each trajectory from both sides arguments is de-  composed into units, named UTrip, by the ??????????  operation. Next, each unit is appended with its 3D bounding  box (lines 8-15 and 19-26) which is enlarged by 1.5 meters  on both the X and Y axes in order to not omit the meeting  units that are partitioned to two adjacent cells. The units? cell  numbers are assigned by the ????	???? operator based on  the 3D grid SCAR WORLD GRID 3D and their bounding  boxes. In each Map task, units from both arguments are  divided based on their cell numbers, defined in the second  part arguments of the ????????? ?? operator (line  28). Consequently, two ? ? ?? ????? PS-Matrices are  generated as the intermediate result of the Map stage, the  ?? ????? still indicates the scale of the Reduce tasks,  and each column corresponds to one partition in PBSM.

Afterwards, ?? ????? Reduce tasks are created and each  reads one column data from both intermediate PS-Matrices.

The ????????? operation accepts both sides units ordered  by their cell numbers, in order to control the memory usage.

The ????????????????? processes the spatial join  within each cell, also the ?????????? ? helps to remove  duplicated join result without blocking the procedure.

Since the trajectories are decomposed into units on which  the meeting points are got based (lines 44-47), multiple  result tuples are produced for a pair of vehicles that meet  several times during the observation period. Regarding this  problem, at the end of the query, each vehicle pair is  aggregated by the ???	??? operation, and then all its        1  2  3  4  5  6  E la  ps ed  T im  e (s  )  Cluster Scale  Cost  (a) Speed-up        1  2  3  4  5  6  E la  ps ed  T im  e (s  )  Cluster Scale  Cost  (b) Scale-up  Figure 3: Data Generation in BerlinMOD  meeting positions are merged with the ?? ?? operator  (lines 54-58).

This parallel query is complicated, almost two times  longer than the sequential statement. Basically, this is be-  cause it uses the PBSM method to achieve a better perfor-  mance. The detailed evaluation about this query is discussed  in the next section.



V. EVALUATION  In order to test and evaluate various parallel queries, a  private cluster consisting of six computers is built up in our  group. Each computer has a AMD Phenom(tm) II X6 1055T  processor with six cores, 8 GB memory and two 500 GB  hard disks, and uses Ubuntu 10.04.2-LTS-Desktop-64 bit as  the operating system. Since there are two hard disks on every  computer, each computer sets two Data Servers, one on each  hard disk; the master DS is also used as a slave DS. Besides,  a Hadoop 0.20.2 framework is installed as the underlying  parallel platform. In total, this Parallel SECONDO testbed  has 6 machines, 6 processors, 36 cores, 48 GB memory, 12  disks and 12 slave data servers.

As said before, it is not always worthy to transform and  process a sequential query in parallel, since there exists  a certain constant overhead for performing the job itself.

Therefore here in this paper, only the data generation of the  BerlinMOD and its 10th range query are evaluated in the  cluster as they both are expensive for a single computer.

In most studies about parallel processing, the coefficients  speed-up and scale-up are two essential indicators for par-  allel systems [7]. Hence they are also used in this paper.

Figure 3 illustrates the performance for the generation of the  BerlinMOD data, while Figure 4 demonstrates the evaluation  for the 10th range query. In both evaluations, the scale of the  cluster increases from one computer to all six computers, and  uses the elapsed time to measure the performance. Besides,  the data set increases from the scale factor 1 to 6 in the  scale-up evaluation, but keeps the factor as 6 in the speed-  up evaluation.

The evaluation results illustrate that Parallel SECONDO  keeps a stable linear speed-up and scale-up performance for  both procedures. Besides, when a small cluster less than            1  2  3  4  5  6  E la  ps ed  T im  e (s  )  Cluster Scale  Cost  (a) Speed-up         1  2  3  4  5  6  E la  ps ed  T im  e (s  )  Cluster Scale  Cost  (b) Scale-up  Figure 4: The 10th Example Query in BerlinMOD  three computers is used, the system performs differently as  more computers get involved. This is because in the small  cluster, each computer has to process more data and causes  more disk I/O.



VI. CONCLUSION  In the age of Big Data, more and more database re-  searchers are facing the challenge of integrating their exist-  ing systems to the parallel platform. For instance, our SEC-  ONDO system proposed abundant data types and operators to  represent and process moving objects data. It is difficult and  also inefficient to implement them for the parallel platform  again.

Regarding this issue, Parallel SECONDO is proposed, by  integrating the Hadoop framework with a set of SECONDO  databases. It not only remarkably improves the performance  of the expensive queries that may last hours on a single com-  puter, but also connects the user?s existing system with the  parallel system seamlessly. Therefore, for different queries,  the user is able to choose either the sequential or the parallel  statements to process, according to the size of the involved  data. For end-users, it is possible to write parallel queries  like sequential queries.

Parallel SECONDO has been published along with SEC-  ONDO 3.3.2 [2]. Besides, plenty of documents and tutorials  are provided on the website [1], where the user can find all  materials mentioned in this paper. At last, a virtual machine  image is also provided for the user?s primary exploration.

In addition to installing Parallel SECONDO on a small  private cluster, it is also possible to set it up on clusters  containing hundreds of AWS (Amazon Web Services) EC2  instances, and the AMI (Amazon Machine Image) contain-  ing the latest Parallel SECONDO has been freely published.

In fact, we have once used 110 instances to generate a  BerlinMOD data set of the scale factor 30 in 5 hours, proving  that Parallel SECONDO can be used on large-scale clusters.

