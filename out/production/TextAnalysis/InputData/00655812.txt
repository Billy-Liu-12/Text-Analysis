Mining for Strong Negative Associations in a Large Database of

Abstract Mining for association rules is considered an impor-  tant data mining problem. Many di?erent variations of this problem have been described in the literature. In this paper we introduce the problem of mining for nega- tive associations. A naive approach to ?nding negative associations leads to a very large number of rules with low interest measures. We address this problem by combining previously discovered positive associations with domain knowledge to constrain the search space such that fewer but more interesting negative rules are mined. We describe an algorithm that e?ciently ?nds all such negative associations and present the experi- mental results.

1 Introduction Wide spread use of computers in business opera-  tions and the availability of cheap storage devices have led to an explosive growth in the amount of data gath- ered and stored by most business organizations today.

There has been a trend in recent years to search for interesting patterns in the data and use them for im- proved decision making. (e.g., see [3]). Data mining is de?ned as the process of ?nding hidden, nontrivial and previously unknown information from a large collec- tion of data [10]. It has been recognized as one of the promising areas of research encompassing databases, statistics and machine learning [6, 12, 15].

Recently, there has been considerable interest in ?nding associations between items in a database of customer transactions, such as the sales data collected at super market check out counters [1, 2, 5, 7, 11, 8, 14, 4]. Association rules identify items that are most often bought along with certain other items by a sig- ni?cant fraction of the customers. For example, we may ?nd that \95 % of the customers who bought bread also bought milk." Every rule must satisfy two user speci?ed constraints: one is a measure of statisti- cal signi?cance called support and the other a measure of goodness of the rule called con?dence. The support constraint ensures that the rule occurs relatively of- ten to be considered useful. The con?dence measures how well the rule predicts the association between the items. The support of a rule X =) Y is de?ned as the fraction of transactions that contain X [ Y , where X and Y are sets of items. The con?dence is de?ned as  ?Current address: Data Mining Group, Tandem computers  Inc., 14231 Tandem Blvd., Austin, TX 78728  the ratio support(X [ Y )/support(X). The goal is to ?nd all rules that satisfy minimum support and mini- mum con?dence. These types of rules specify the like- lihood of ?nding Y in a customer basket given that it contains X. Applications of such rules include cross- marketing, attached mailing, catalog design, add-on sales, store layout, etc., [3].

In this paper we consider the complementary prob- lem, i.e., what items a customer is not likely to buy given that he buys a certain set of items. We call these types of rules as the negative association rules. An ex- ample of such a rule is \60 % of the customers who buy potato chips do not buy bottled water." Such nega- tive association rules can provide valuable information about customer buying patterns and help managers in devising better marketing strategies. In this paper, we consider negative associations in the context of retail sales data. However, the solutions developed here can be applied to other domains. To the best of our knowl- edge, we are unaware of any work that addresses the issue of mining negative rules from data. In this paper we discuss the di?culties of mining negative rules and describe an algorithm which e?ciently mines negative rules in large datasets.

Finding negative associations is not straight for- ward due to the following reason: in a large retail store, there may be tens of thousands of items. Even if billions of customer transactions are considered, many of the item combinations may not appear even once.

For example, if there are 50,000 items the possible combinations of items is 250;000 a majority of which will not appear even once in the entire database. If the absence of a certain item combination is taken to mean negative association, then we can generate mil- lions of negative association rules. However, most of these rules are likely to be extremely uninteresting.

The problem is therefore one of ?nding only interest- ing negative rules. We call such rules strong negative rules. In the following section we de?ne the problem more precisely and then discuss the notion of interest- ingness and the type of negative rules that fall within this de?nition.

1.1 Measure of Interestingness The objective measure of interestingness of a rule is  de?ned in terms of the \unexpectedness" of the rule1  1This is only one measure of interestingness. There may be  other factors which make a particular rule intersting.

Simply stated, a rule is interesting if it contradicts or deviates signi?cantly from our expectation based on previous belief. The previous belief is usually stated in terms of the a priori probabilities based on our knowledge of the problem domain. For example, in the retail marketing context, suppose there are 50,000 distinct items, and there are 10 million transactions each containing 5 items on an average. Without any prior knowledge we would expect all items are equally likely to be bought. Then the number of transactions containing a speci?c item is 1000. Now however, af- ter scanning the customer transactions, if we ?nd that 500,000 transactions contain that particular item, we say that we have discovered an interesting fact because it signi?cantly deviates from our earlier expectation.

In information theoretic terms the a priori probabili- ties represent our state of ignorance and the deviation of the a posteriori probabilities represent the degree of information gained.

In the case of negative rules we are interested in ?nding itemsets that have a very low probability of being bought with certain other items. That is, we are interested in cases where two speci?c sets of items appear very rarely in the same transaction. However, this poses two problems as follows.

1. In the absence of any knowledge about customer buying preferences, we expect the items to be bought independently of each other. In the above example the expected support for a speci?c pair of items is 1000/10 million ? 1000/10 million = 1/100,000,000. Even if the actual support turns out to be zero, the deviation from expectation is extremely small. Consequently, a rule which states that these two items are negatively associ- ated does not carry much information and hence uninteresting.

2. If we are looking for item combinations which have very low support, there will be a very large number of combinations having very low or even zero support. In the above example, even if we take only pairs of items there are approximately 2.5 billion combinations. Since we have only 10 million transactions, most of the combinations will not appear even once. Therefore we may gen- erate billions of negative rules most of which will be useless. The problem gets even worse if we include combinations of larger sizes.

To summarize, mining for negative rules is impossi- ble with a naive approach due to the following reasons: (a) We may ?nd billions of negative associations in a large dataset; and (b) Almost all of these negative as- sociations will be extremely uninteresting.

Even though the preceding argument shows that negative associations may be inherently uninteresting, there are many situations where this is not true. We motivate this with the following examples:  Example 1: Suppose we consider the sales of a par- ticular brand of chips, say Ru?es and two brands  of soft drinks, say Coke and Pepsi2. After scanning through a large number of transactions suppose we ?nd that when customers buy Ru?es they also usually buy Coke but not Pepsi. We can then conclude that Ru?es has an interesting negative association with Pepsi. The reason this is interesting is that by consid- ering only the associations between Ru?es and Coke, we expected the association between Ru?es and Pepsi also to be high. In other words, we reformulated our belief from one of independence to a positive associa- tion based on (a) our knowledge that Coke and Pepsi fall under the same \category" and therefore they can be expected to have similar types of associations with other items and (b) the positive association between Ru?es and Coke.

Example 2: After scanning the data suppose we ?nd a strong positive association between the frozen yo- gurt category and the bottled water category. Sup- pose each of these categories contains two individ- ual brands, say brands Bryers and Healthy Choice of frozen yogurt and brands Evian and Perrier of bot- tled water. For simplicity assume that the sales of each individual brand accounts for 50% of the sales of its category. Based on the association between the two categories we can reasonably expect each of the four combinations of frozen yogurt and bottled water brands to account for 25% of the support for ffrozen yogurt, bottled waterg. However, if we actually ?nd the support for fBryers, Perrierg accounts for only 2%, we can conclude that Bryers and Perrier are negatively associated.

Example 3: In the above example, we can also expect the support for fFrozen yogurt, Perrierg to be 50% of the support for ffrozen yogurt, bottled waterg. How- ever, suppose we actually ?nd this support to be 10%, we can deduce that most customers must be buying Evian when they buy either brands of frozen yogurt which indicates an interesting negative association be- tween the frozen yogurt category and Perrier.

The preceding examples shows that we can indeed ?nd certain interesting negative associations. In each of these examples we based our conclusion on the infor- mation already present in the data and additional do- main knowledge which groups similar items together.

We use these ideas to mine for a class of negative rules which are interesting and likely to be useful. The basic idea behind our approach is to look at only those cases where we expect a high degree of positive associ- ation. If the actual support is found to be signi?cantly smaller then we can conclude that they have a nega- tive association. We make the following observations with respect to this approach.

1. Domain knowledge. The type of domain knowledge we depend on to mine the negative rules is a grouping of similar items. This informa- tion is most naturally conveyed by a taxonomy on the items. In this paper we assume such a tax-  2The actual brand names used in this paper are for the pur-  pose of illustration only.

onomy is available. In most retail organizations, items are already classi?ed under departments, categories, sub-categories, etc.

2. Uniformity assumption. One of the funda- mental assumptions in our approach is that the items that belong to the same parent in a taxon- omy are expected to have similar types of asso- ciations with other items. This is similar to the principle of uniformity of nature found in induc- tive reasoning.

3. Class of negative rules. Since we consider only those cases where an expected support can be computed based on the taxonomy, the class of negative rules we can generate is not all possible negative rules but a subset of those rules. If addi- tional domain knowledge is incorporated, it may be possible to mine additional types of negative rules.

It should be noted that our approach solves the two problems described earlier. That is, we generate fewer negative rules and all those rules are likely to be interesting.

1.2 Previous Work The problem of mining for association rules was in-  troduced in [1]. Since then it has been recognized as one of the important types of data mining problems and many algorithms have been proposed to improve its performance, for e.g., [2, 7, 8, 11]. Related work also includes [9, 13]. In [14] the problem was extended to include taxonomies on the items and mining rules between di?erent levels in the taxonomy. This enables more expressive types of rules to be mined by allow- ing a limited form of disjunction in the association rules, e.g., \(overcoats OR jackets) AND shoes =) perfume." However, we are unaware of any work that tries to ?nd negative associations in data. The clos- est work is the technique to prune uninteresting rules proposed in [14].

2 Problem Statement Formally, the problem can be stated as follows: Let  I = fi1; i2; : : : ; img be a set ofm distinct literals called  items3. Let T be a taxonomy on I. Let L ? I be the set of leaf items in T and let C ? I be the set of internal nodes called categories. D is a set of vari- able length transactions over L. Each transaction, T, contains a set of items ii; ij; : : : ; ik ? L. A transaction also has an associated unique identi?er called TID. In general, a set of items is called itemsets. The number of items in an itemset is called the length of an itemset.

Itemsets of some length k are referred to as k{itemsets.

For an itemset X ? Y , if Y is an m{itemset then Y is called an m-extension of X. An itemset X ? I, has support(X) = s, if the fraction of transactions in D containing X equals s. A negative association rule is an implication of the form X 6=) Y , where X;Y ? I, and X \ Y = ;. X is called the antecedent and Y is  3This description and the terminology used are largely based  on [1] and [2].

called the consequent of the rule. Every rule also has a rule interest measure. We de?ne the interest measure RI of a negative association rule, X 6=) Y , as follows:  RI = E [support(X [ Y )]? support(X [ Y )  support(X)  Where E [support(X)] is the expected support of an itemset X. We describe how expected support is com- puted in the following section. Note that the rule in- terest RI is negatively related to the actual support of the itemset X [ Y . It is highest if the actual sup- port is zero and zero if the actual support is same as the expected support reecting our earlier de?nition of interestingness.

The problem of ?nding negative rules can be now be stated as follows: given a database of customer transactions D and a taxonomy T on the set of items, ?nd all rules X 6=) Y such that (a) support(X), and support(Y ) are greater than minimum support Min- Sup; and (b) the rule interest measure is greater than MinRI where MinSup and MinRI are speci?ed by the user. Condition (a) is necessary to ensure that the generated rule is statistically signi?cant. For exam- ple, even if the rule perfectly predicts 10 out of 10 million cases, it is not very useful because it is not general enough.

The problem can be decomposed into two subtasks: (a) ?nding itemsets whose actual support deviates at least MinSup ?MinRI from their expected support.

We call such itemsets as negative itemsets and (b) gen- erating the negative rules after the negative itemsets are found. The ?rst condition eliminates testing for rules which will have an RI less than MinRI. We con- sider each of these tasks in the following sections.

2.1 Finding Negative Itemsets Finding negative itemsets involves following steps:  1. We ?rst ?nd all the generalized large itemsets in the data (i.e., itemsets at all levels in the taxon- omy whose support is greater than the user spec- i?ed minimum support).

2. We next identify the candidate negative itemsets based on the large itemsets and the taxonomy and assign them expected support.

3. In the last step, we count the actual support for the candidate itemsets and retain only the nega- tive itemsets.

The ?rst step of discovering generalized large item- sets has been discussed in [14, 4]. The second step is the most important step and is discussed in the next section. The last step is relatively straight forward as any data structures to count support can be used.

2.1.1 Generating Candidate Negative Item- sets  The candidate negative itemsets are generated based on the previously determined large itemsets. For each    A  D                  E  F  G                       H  J                     K  B                     C  Figure 1: Taxonomy  large itemset lk, we generate candidate itemsets com- posed of the immediate descendents and siblings of the items in lk. The candidate itemsets will also be of size k. In general any itemset whose expected support can be computed based on the support of the large item- sets and whose computed expected support is greater than MinSup?MinRI is a candidate itemset.

We can identify the following cases for generating candidate itemsets. All the examples refer to the tax- onomy shown in Figure 1 where fCGg has been found to be large. We show only pairs of items in these exam- ples. However, they can be extended to any number of items. It should be noted that all the 1-itemsets in a candidate must have minimum support. Other- wise no rule will be produced for this itemset because both antecedent and the consequent of a rule must have minimum support. Therefore if all 1-itemsets of a candidate are not large, that candidate is immedi- ately rejected.

Case 1: Here, the candidates are formed from the immediate children of large itemsets. For example, the candidates in this case will be fD Jg, fD Kg, : : : , fE Jg, fE Kg, : : : , etc. The expected supports are computed from the support of their parents. For example, the expected support of fD Jg is computed as  E[sup(DJ)] = sup(CG)? sup(D)? sup(J)  sup(C)? sup(G)  In general, if fp?; q?; : : : ; t?g is a large itemset,  E[sup(p; q; : : : ; t)] =  sup(p? [ q? [ ? ? ? [ t?)? sup(p)? sup(q)? ? ? ? ? sup(t)  sup(p?)? sup(q?)? ? ? ? ? sup(t?)  where p?; q?; : : : ; t? are parents of p; q; : : : ; t, respec- tively. The candidate itemsets are formed by taking every possible combination of the children of the items in the large itemset.

Case 2: The candidate itemsets are fC Jg, fC Kg, : : : , fG Dg, fG Eg, : : : , etc. The expected support for, say, fC Jg is computed as  E[sup(CJ)] = sup(CG)? sup(J)  sup(G)  In general,  E[sup(p; q; r; : : : ; t)] =  sup(p [ q [ r? [ ? ? ? [ t?)? sup(r)? ? ? ? ? sup(t)  sup(r?)? ? ? ? ? sup(t?)  where p?; q?; : : : ; t? are parents of p; q; : : : ; t, respec- tively. The candidate itemsets are generated as all combinations of the children of the items in the large itemset.

Case 3: In this case, the candidates are formed from the siblings of the items in the large itemset. For the above example, the candidates are fC Hg, fC Ig, and fB Gg. The expected support for fC Hg is given by  E[sup(CH)] = sup(CG)? sup(H)  sup(G)  In general,  E[sup(p; q; r; : : : ; t)] =  sup(p [ q [ r0 [ ? ? ? [ t0)? sup(r)? ? ? ? ? sup(t)  sup(r0)? ? ? ? ? sup(t0)  where p0; q0; : : : ; r0 are siblings of p; q; : : : ; t, respec- tively.

It is possible that same candidates generated from di?erent large itemsets. For example, we may generate the candidate fC Hg from the large itemset fC Gg according to case 3. Now, if the itemset fA Fg is also large, then the candidate fC Hg will be generated according to case 1. This may result in di?erent values of expected support for the same candidate depending on how it is generated. In such situations the largest value of the expected support is chosen.

The main idea behind generating candidate nega- tive itemsets is that the items close together in the tax- onomy will have similar associations with other items.

However, the candidates are generated only if we can assign an expected value of support based on the sup- ports of other large itemsets. Therefore, even though we can identify many more cases where itemsets can be formed from the neighbors of large itemsets, they are not considered as candidates. For example, the following itemset combinations are not considered as candidates.

1. itemsets containing only the siblings of the items in the large itemset.

2. itemsets containing the immediate ancestors and immediate children of the items in the large item- set.

3. itemsets containing the immediate ancestors and siblings of the items in the large itemset.

4. itemsets containing immediate descendents and siblings of the items in the large itemset.

Items Support Bryers 20,000 Healthy Choice 10,000 Evian 10,000 Perrier 5,000 Frozen yogurt 30,000 Bottled water 20,000 Frozen Yogurt and Bottled water 15,000  Table 1: Supports for the example  Items Expected Actual Support Support  Bryers and Evian 6,000 7,500 Bryers and Perrier 4,000 800 Healthy Choice and Evian 3,000 4,200 Healthy Choice and Perrier 2,000 2,500  Table 2: Expected and actual supports  Example We illustrate the candidate and negative itemset generation using the an example. Figure 2 shows the taxonomy and Table 1 shows the supports for the individual brands (we have shown absolute val- ues of support for simplicity). The minimum support is speci?ed as 4,000.

Beverages  Corbonated NonCorbonated  Bottled juices  Desserts  Frozen yogurt  Avian  Bryers Healthy ChoiceBottled water  Ice creams  Perrier  Figure 2: Taxonomy for the example  The negative candidates in this case are fBryers, Eviang, fBryers, Perrierg, fHealthy Choice, Eviang and fHealthy Choice, Perrierg. Their expected sup- ports are shown in Table 2. Among these fBryers, Eviang and fHealthy Choice, Eviang will already be found to be large. Hence they are not considered neg- ative candidates.

If the actual supports are as shown in Table 2 and the minimumRI is speci?ed as 0.5, then the only neg- ative association rule will be Perrier 6=) Bryers.

2.1.2 Number of Candidates  The actual number of candidates generated depends on the characteristics of the data and the taxonomy being used. However, we can estimate the number of candidates generated as  kX 1=1  ? k i  ? f i + k(f ? 1)  where f is the average fan-out in the taxonomy and k is the itemset size. As can be seen, the number of candidates is exponential in the size of the can- didates being considered. However, as the size is in- creased the number of large itemsets rapidly decreases.

A large number of candidates generated can also be pruned by applying optimizations. For example, since all 1-itemsets contained in a candidate must be large, the candidates are generated by discarding all small 1-itemsets in the taxonomy. This has the e?ect of re- ducing the fanout and hence the candidates generated.

Other optimizations will be discussed in Section 2.2.

2.1.3 E?ect of the Taxonomy  The taxonomy is considered as the domain knowledge which is used to induce negative rules. Therefore the rules generated will be a?ected by the quality of the taxonomy. One of the implicit assumptions in our approach is that the items belonging to the same cat- egory are \substitute" items, i.e., customers may sub- stitute one item in place of the other within that cat- egory. Taxonomies on a given set of items can be generated based on any arbitrary criterion. However, our approach works best if the taxonomy is generated based on the similarity of usage of the items. This also ensures that the items grouped under the same cate- gory are substitutes. Most real life taxonomies are, in general, of this nature. Therefore, the approach works satisfactorily in most situations.

Another issue that needs to be considered is the level of detail in the taxonomy. A taxonomy with ?ner \granularity," i.e., one in which categories are classi- ?ed into smaller and smaller sub-categories leads to the generation of better rules compared to a shallow taxonomy where hundreds or thousands of items are grouped under the same category. This is to be intu- itively expected because the information content of a ?ne granular taxonomy is more than a taxonomy of coarser granularity, leading to better domain knowl- edge and hence better quality of rules.

This can be explained in more practical terms as fol- lows: we generate the negative candidate itemsets and assign them expected supports based on the supports of either their parents or siblings. In a taxonomy with ?ner granularity, each category will have fewer chil- dren and also fewer siblings compared to a taxonomy with coarser granularity. As the number of children or siblings in a category increases, the relative support of an individual child or sibling decreases. For example, in a category with two children the relative support of each child may be 50% on average. However, if there are 100 children the relative support will drop to 1%.

Therefore, the expected supports which are computed from the relative supports of the items in a candidate will have relatively larger error terms in a taxonomy with coarser granularity leading to less accurate rules.

The second reason why ?ne granularity taxonomies are preferred is that the number of candidates gener- ated increases rapidly with the increase in the aver- age fanout. Fine granularity taxonomies alleviate this problem.

2.2 Algorithm We assume that the transactions are in the form  hTID; ij ; ik; : : : ; ini and the complete taxonomy is available. Generating negative association rules in- volves ?nding all the negative large itemsets and gen- erating the negative rules. We ?rst consider the prob- lem of ?nding negative large itemsets. As explained in section 2.1, generating negative large itemsets in- volves ?nding generalized large itemsets, generating negative candidates, counting support for the candi- dates and generating the negative large itemsets. To ?nd all large itemsets, we can use one the algorithms, Basic, Cumulate or EstMerge, proposed in [14]. To generate the negative large itemsets we describe two algorithms below. Both the algorithms use similar ap- proaches. The ?rst algorithm is a straight forward im- plementation and the second algorithm incorporates some optimizations to improve the performance. Both the algorithms require multiple iterations.

2.2.1 Naive Algorithm  Each iteration of this algorithm consists of two phases.

In the ?rst phase of iteration k, we compute the gen- eralized large itemsets of size k (using one of Basic, Cumulate or EstMerge). In the second phase, ?rst the negative candidate itemsets of size k are generated as describe in section 2.1.1. Next, support for the can- didates is counted by making a pass over the data.

Therefore, this algorithm requires two passes over the data during each iteration for a total of 2 ?n passes, where n is the total number of iterations. The im- proved algorithm reduces the number of passes over the data as described below.

2.2.2 An Improved Algorithm  This algorithm incorporates two optimizations over the naive algorithm: ?rst, all small 1-itemsets are deleted from the taxonomy; second, the instead of gen- erating the negative itemsets during each iteration, they are generated in a single step after generating the large itemsets of all sizes. The ?rst optimization reduces the number of negative candidates generated.

The second optimization reduces the number of passes over the data from 2 ?n to n + 1. The algorithm is shown in Figure 3.

2.3 Generating Rules Once the negative itemsets are generated ?nding  all negative rules is straight forward. For a negative itemset n, we output the rule a 6=) (n ? a) where a, and (n ? a) are nonempty subsets of n having min- imum support if its rule interest measure is greater than speci?ed minimum, MinRI. Our algorithm for generating negative rules is an extension of the ap- genrules algorithm described in [2]. The extensions handle the requirement that the antecedent and the  L1 = flarge 1{itemsetsg;  k = 2; // k represents the pass number  // First generate all large itemsets while (Lk?1 6= ;)  begin  // Generate new candidates of size k using Basic, // Cumulate or EstMerge  Ck = GenCands (Lk?1);  forall transactions t 2 D begin  Ct = subset(Ck , t);  forall candidates c 2 Ct c.count++;  end  Lk = fc 2 Ck jc.count ? MinSupg k = k + 1  end  // Now generate negative itemsets Delete all small 1-itemsets from the taxonomy  k = 2; while (Lk 6= ;)  begin  // Generate negative candidates of size k as  // described in section 2.1.1  NCk = GenNegCands (Lk); NC = NC [NCk;  k = k + 1;  end  forall transactions t 2 D  begin  NCt = subset(NCk , t); forall candidates c 2 NCt  c.count++;  end  Nk = fc 2 NCkjc.count < MinSup?MinRIg  Figure 3: An improved algorithm  consequents of the rule must be large. Note that if a turns out to be small none of the extensions need to be generated because they will not have the minimum support. Similarly if a rule a 6=) (n ? a) does not have minimumRI, then none of the subsets of a need to be considered because none of those rules will have minimum RI either. The rule generation algorithm is shown in Figure 4. The apriori-gen function is the join followed by the prune step described in [2].

2.4 Data Structures The algorithm to generate negative association  rules is very similar to generating generalized asso- ciation rules with additional steps to generate nega- tive itemsets. Since every 1-itemset of candidate must have minimum support, instead of testing every item- set for minimum support while generating candidates, it is considerably faster to compress the taxonomy by ?ltering out all items which do not have minimumsup- port. Then no candidate in which one of the 1-item    forall negative itemsets nk of size k, k ? 2 do  H1 = fconsequents of rules generated from nk with one item in the consequentg;  call genrules (nk, H1, L2, Lk?2);  end  procedure genrules (nk, Hm, Lm+1, Lk?m?1) // nk: negative k-itemset, Hm: set of m-item consequents  if (k > m + 1) then  begin  Hm+1 = apriori-gen (Hm);  forall hm+1 2 Hm+1 if (hm+1 2 Lm+1 then if ((nk ? hm+1) 2 Lk?m?1) then  RI = (E[sup(nk)]? sup(nk))/sup(nk ? hm+1);  if (RI ? MinRI) then output rule (nk ? hm+1) 6=) hm+1;  else  delete hm+1 from Hm+1; else  delete hm+1 from Hm+1;  end  call genrules (nk, Hm+1);  end  Figure 4: Rule Generation  subsets does not have minimum support will be gen- erated. As mentioned earlier, same candidate may be generated in more than one way. Therefore all candi- dates are put in a hash table for fast lookup. When- ever a candidate is generated, ?rst the hash table is checked. If the candidate is not found then the candi- date is inserted in the hash table. Otherwise, the ex- pected support for the candidate is set to larger of the two. During the rule generation process, the counts of the subsets of the negative itemset are required. All large itemsets are also placed in a hash table for fast lookup.

2.5 Memory Management Since negative candidates of all sizes are generated  at the same time and their support is tested in a single pass, it is possible that the number of candidates gen- erated is too large to accommodate in available mem- ory. However, if such a situation arises, the negative candidate generation process is stopped and the sup- port for the candidates already generated is counted.

The generated negative itemsets are either written back to the disk or if they are su?ciently small, are kept in the main memory. The candidate generation process is now continued. This will necessitate more than one pass over the database.

3 Experimental Results In this section we describe the experimental results  of our technique for generating negative associations.

We performed the experiments using synthetic data on Sun SPARCstation 5 with 32 MB of main memory.

3.1 Synthetic Data The synthetic data is generated such that it simu-  lates customer buying pattern in a retail market en- vironment. We have used the same basic method as described in [14]. However, to simulate the buying pattern more accurately we adapted the hierarchical model of consumer choice called the nested logit model to generate the data. In this model, consumers ?rst decide on which category to buy and then decide which particular brand to buy within that category.

We ?rst generate a taxonomy over the items. For any internal node, the number of children are picked from a Poisson distribution with mean set to F. This process is generated starting from the root level. Then at level 2 and so on until there are no more items. We next generate a set of potentially maximal large item- sets from which itemsets are assigned to a transaction.

To generate the set of potentially maximal large item- sets, we ?rst generate potentially maximal clusters of categories comprising of items one level above the leaf level. The clusters sizes are picked from a Poisson distribution with mean equal to average cluster size.

Next for each cluster we generate a set of potentially maximal itemsets from the children of the items in the cluster. The number of such itemsets is picked from a Poisson distribution with mean set to average number of itemsets. The size of each itemset is also picked from a Poisson distribution with mean set to average large itemset size. Each cluster has an as- sociated weight that determines the probability that this cluster will be picked. The weight is picked ac- cording to an exponential distribution with mean set to 1. The weights are normalized such that the sum of all weights equals 1. The itemsets associated with each cluster are also given weights which determine the probability this itemset will be picked once that particular cluster is picked. The weights are picked from an exponential distribution with mean set to 1.

The weights are then normalized such that the sum of all weights of all itemsets for a cluster equal 1. The length of a transaction is determined by Poisson dis- tribution with mean ? equal to jT j. Until the trans- action size is less than the generated length, a cluster is picked according to its weight. Once the cluster is determined an itemset from that cluster is picked and assigned to the transaction. Not all items from the itemset picked are assigned to the transaction. Items from the itemset are dropped as long as an uniformly generated random number between 0 and 1 is less than a corruption level, c. The corruption level for itemset is determined by a normal distribution with mean 0.5 and variance 0.1. The transactions contain only leaf items from the taxonomy.

The parameters used in the generation of the syn- thetic data are shown in Table 3.

3.2 Performance We generated two sets of data, \Short" and \Tall"  based on di?erent average fanouts in the taxonomy on the items. Both datasets contain the same number of items (leaf items in the taxonomy) and the same number of transactions. The parameter values used to generate the data are shown in Table 4.

jDj Number of transactions jT j Average size of transactions jCj Average size of maximal potentially large  clusters jIj Average size of maximal potentially large  itemsets jSj Average number of itemsets for each cluster jLj Number of maximal potentially large clusters N Number of items R Number of roots F Fanout  Table 3: Parameters  Parameter \Short" \Tall" jDj 50,000 50,000 jT j 10 10 jCj 5 5 jIj 5 5 jSj 3 3 jLj 2,000 2,000 N 8,000 8,000 R 10 10 F 9 3  Table 4: Data parameters  We ran both algorithms on the two data sets for various values of minimum support. The minimum RI was set to 0.5 in all cases. The results are shown in Figures 5 and 6. The times shown include both genera- tion of negative itemsets and negative rules. Since our goal was to study the performance of generating nega- tive associations, we have not included the time taken to generate the generalized large itemsets. The \Tall" dataset which has a taxonomy with smaller fanout took longer to complete than the \Short" dataset. The reason was the far larger number of generalized large itemsets that were generated for the \Tall" dataset.

For example, at a support level of 1.5 %, 15,476 large itemsets were generated for the \Tall" dataset as op- posed to 1,499 for \Short." If normalized for the num- ber of generalized large itemsets, the times for the \Tall" dataset are much smaller than those of \Short" as per our expectations.

Next, we compared the e?ect of the di?erent fanouts in the taxonomy. The number of negative can- didates generated and the number of negative rules for each data set are shown in the Figure 7. To keep the results comparable we have normalized these numbers with respect to the number of large itemsets. As can be seen, the experiment con?rmed that the number of candidates increases with the increase in fanout.

4 Conclusion We introduced the problem of mining negative as-  sociation rules in a large database of retail customer           0.010.0150.0250.040.060.10.2  T im  e (s  ec )  Minimum Support (%)  Naive Better  Figure 5: Execution times: \Short" data set         0.010.0150.0250.040.060.10.2  T im  e (s  ec )  Minimum Support (%)  Naive Better  Figure 6: Execution times: \Tall" data set  transactions. Mining negative information is non- trivial and in the case of retail transactions data the problem becomes impossible to solve. Our approach is to use the grouping information such as a taxonomy over the items and the existing positive associations in the data to induce negative rules between items \close" to the items in the positive associations. This approach solves the problem pruning the combinato- rial search space to a small subset of cases which have a high potential of being interesting. We present an algorithm for mining negative rules and improvements and study their performance on synthetic data.

4.1 Future Work Many problems remain unsolved in the problem of  mining negative rules. Two of the biggest problems are as follows.

? We assume only the taxonomy over the items as the domain knowledge available to mine for neg- ative rules. However, there may be many other         1e+06  2 3 4 5  N um  be r  of c  an di  da te  s (N  or m  al iz  ed )  Size of the itemset  Fanout = 9 Fanout = 3  Figure 7: Number of negative candidates  types of information available. For instance, a knowledge of substitute items. How to incorpo- rate other types of information to improve the quality of rules needs to be explored further.

? The number of candidates generated is exponen- tial over the length of the large itemsets being considered. More e?cient candidate generation techniques need to be developed.

