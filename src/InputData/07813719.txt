

Abstract?Searching frequent item-sets in large size heterogeneous databases in minimal time is considered as one  of the most important data mining problem. As a solution of  this problem, various algorithms have been proposed to speed  up execution. Most of the recent proposed algorithms focussed  on parallelizing the workload using large number of machine  in distributed computational environment like MapReduce  framework. A few of them are actually capable to determine  the appropriate number of required computing computers,  considering workload balancing and execution efficiency. But  internally not capable to determine exact number of required  iteration for any large size datasets in advance to find out the  frequent item-set based on iterative sampling. In this paper, we  propose an improved and compact algorithm (ICA) for finding  frequent item-set in minimal time, using distributed  computational environment. It is also capable of determining  the exact number of internal iteration required for any large  size datasets whether data is in structured or unstructured  format.

Keywords?Frequent Item-set Mining; Big Data; MapReduce; Distributed Computational Environment

I.   INTRODUCTION  Frequent item-set mining is one of the most important  techniques to find out frequent item-sets in data mining [1].

Industries use the extracted frequent item-sets in decision  making or setting policies. For example a retail-sector  company is interested to know customer buying habits in  particular area to sell out their product. Here, frequent item-  set mining helps the company to know customer buying  habits. On the other hand, even government of nations use  the frequent item-set technique to extract useful information  that further help to provide better services to people.

Frequent item-set mining is the part of frequent pattern  mining where frequent pattern represents those sub-  sequences and sub-graphs which are occurred many times  frequently in a given data sets. Traditional data mining tools  fails to extract frequent item-sets when the size of  transactional database is too large to compute. In Big Data  era, we need a new approach to compute frequent item-sets  where data-sets consist of millions of records [2].

Researchers proposed various approach to deal with Big  Data challenges, but all these approaches suffers from  synchronization, work load balancing and fault-tolerance  problem [3]. To overcome this problem MapReduce model  come into existence, originally proposed by Google [4].

This MapReduce model supports parallelization of tasks  under Hadoop architecture [5].

To scale frequent item-sets mining to large size data sets,  we propose an Improved and Compact (ICA) algorithm.

This algorithm supports parallelization and can be  implemented on Hadoop architecture using MapReduce  programming model. The working of this algorithm includes  breaking of large size data-sets into manageable data chunks  which are executed on the different machines in distributed  computational environment. The concept of sampling the  whole data sets into manageable data chunks reduces the  space complexity because it is easier to compute small data  set effectively at single node instead of loading and  scanning or computing the whole data set at once and also  reduces the time complexity by supporting parallelization of  tasks and sub-tasks execution. Mapper function on each  individual node or machine receives these data chunks and  generates the output, in accordance with local minimum  support. Further, reducer function aggregates all the values  and eliminates those values which do not support the global minimum threshold or support before generating the final  set of frequent item-sets as an output.

This paper is organised as follows. Section 2 describes  the Hadoop architecture in brief. Methodology of working  algorithm is proposed in section 3. Implementation and  experimental result is given in section 4. Finally, Section 5  concludes the paper.



II.   BACKGROUND STUDY  A.  Apache Hadoop and its Key Features  Hadoop is an open source software framework used to  develop data processing applications which are executed in  a distributed computing environment where large data sets  are distributed across clusters of commodity nodes or  computers. Some key features of Hadoop are given as  follows:  ? Suitable for Big Data Analysis using less network  bandwidth.

? Hadoop platform provides scalability means  Hadoop clusters can be scaled to any extent by  adding additional cluster nodes with ease.

? Fault Tolerance that makes the whole system  reliable.

B.  Components of Hadoop  Apache Hadoop includes the various components such  as Hadoop Distributed File System (HDFS), MapReduce,  HBASE, HCatalog, Pig, Hive, Oozie, ZooKeeper, Kafka,  and Mahout. The core parts of Hadoop platform are (HDFS)  and MapReduce. (i) HDFS: In Hadoop, HDFS provide large  data storing capacity, which is used by compiled version of  program written in a high level computer language [6].

HDFS takes care of stored data by creating multiple copies  of the data blocks and increases the reliability by  distributing them on various nodes in a cluster. (ii)  MapReduce: MapReduce is a computational model for  coding the applications which are executed on Hadoop  platform [7]. To handle data effectively at large scale,  MapReduce programs are designed to run in parallel on  large clusters of computation nodes in a distributed  computational environment.

C.   MapReduce as a Programming Model  MapReduce programming model comes under generic  processing model that used to address the general  application problems. MapReduce programs can be seen in  two phases, Map phase and Reduce phase which consist of  Map function and Reduce function respectively and input to  each function are key-value pairs. Fig. 1 shows the logical  data flow in MapReduce. First, data are loaded into HDFS  in blocks and then distributed to the data nodes by input  split method [8]. Input data passes from various steps before  generating final output which is summarised as follows:  1) Input Splits: Input to a MapReduce job divides fixed  size pieces named as input splits which is consumed by a  single map.

2) Mapping: In this process, data in each split is  delivered to mapping function to generate output values.

3) Shuffling:  After consumption of output values of  mapping phase, it consolidates the relevant records from  Mapping phase to output values.

4) Reducing: This phase aggregates values from  Shuffling phase and generate a single output value. Final  output or result values are stored in HDFS and this result is  replicated to the configuration. Users read the result from  the HDFS. Advantages of MapReduce are as follows: (i) It  Reduces network communication cost by supporting data  locality.  (ii) It supports scalability that enables it to adjust  resources dynamically during job execution. (iii) It is also  capable enough to handle failure at execution time having  fault tolerance strategy in distributed computational  environment. Furthermore, it also capable to handle data for  heterogeneous system means it is possible to analyse data  stored at different nodes using MapRedue programming  model. Fig. 1 is given to show the logical data-flow in  MapReduce programming model.

Fig. 1. Logical data-flow in MapReduce programming model

III.   METHODOLOGY  We propose an improved and compact (ICA) algorithm  that mainly targets to extract frequent item-sets. In general,  the complexity of algorithm to find the frequent item-set is         given as: O(N(1-a)N). Here, N denotes the number of input  transactions and ?a? denotes the global threshold frequency.

In dealing with big data, the complete transactional database records are too large to fit into memory and cause the space  complexity. As a solution of this problem, ICA divides the  whole records into sample records and extracts item-sets  with threshold ?b? and threshold ?a?. The new complexity is  given as: O((yN)(1-b)yN), where N? = yN and 0<y<1.

????????????????????????.

The proposed ICA Mapper algorithm is summarized in  Algorithm 1.

????????????????????????.

Input:  Set of Transactions D  Other intermediates or values: local support threshold ?b?,  Global support threshold ?a?,  Set of sampled transactions D?  MapperClassdo  extends Mapper<Object, Text, Text, IntWritable>  { IntWritable one = new IntWritable(1);  Text word = new Text();  while (count <nitem) { count=count+1;  for (i=0;i<(list.size()-1);i++)  {  items = new StringTokenizer(list.get(i)); }}  for I ? 1 to T do  {Let D? = { N? transactions randomly selected from D  without replacement };  Perform frequent item set extraction on D? with threshold  ?b?, FP? = FIM(D?,b); Evaluate patterns in FP?, and save the  selected ones to X; }  ????????????????????????.

A.  Evaluate Method  To filter out non-informative item-sets, we implement  two methods having name Mapper and Reducer as given in  algorithm 1 and 2. Mapper method performs nothing but  sends all local frequent patterns whereas Reducer aggregates  all the local frequent item-sets and calculates their frequency  in terms of whole data sets. For those that fail to reach the  global supporting threshold, Reducer prunes them and keeps  the rest ones as the final output.

????????????????????????.

The proposed ICA Reducer algorithm is summarized in  Algorithm 2.

????????????????????????.

Reducer class do  Reduce (Text key, Iterable<IntWritable> values,  Context context) throws IOException, InterruptedException;  { int sum = 0;  for (IntWritableval : values) { sum += val.get(); }  result.set(sum);  if (sum> a)  context.write(key, result)  }  }  ????????????????????????.

The proposed ICA algorithm having iterative sampling  method replaces the one pass algorithm which finds outs all  frequent item-sets at once. This algorithm can be  implemented in parallel using MapReduce model, because  each iteration process is independent in it.

B.  Organisation of Work  MapReduce programming model works on Hadoop  platform where job is divided into tasks. Mainly, there are  two types of tasks named as Map task which has the  responsibility of splitting and mapping of data and Reduce  tasks which is responsible for shuffling and reducing of  data. There are two types of entities which controls the  complete execution of Map tasks and Reduce task, named  as:  ? Job-tracker: it is responsible for complete  execution of submitted job and acts like a master  [9].

? Multiple Task Trakers: they are responsible for  performing the job for each and every job assigned  for execution in the system. A job-tracker is  present in Namenode and a number of task-trackers  reside on Datanode [10].

Hadoop architecture divides the job into number of tasks  which runs on multiple Datanodes in distributed  environment. Job-tracker coordinates the different activities by managing and scheduling different tasks to execute on  data nodes. Further, executions of these individual tasks are         managed by task-tracker which resides on each and every  Datanode. In addition, Job-tracker receives the progress  report to task-trackers. Apart from that, task-tracker has the  responsibility to send ?heartbeat? signal periodically to job-  tracker to notify him of current state of the system. Hence  job-tracker keeps status of overall progress of each job. In  case of task failure, job-tracker reschedules the jobs or tasks  on different task-tracker.



IV.   IMPLEMENTATION  A.  Data Sets  Synthetic retail data-sets are used here that is originally  generated by using synthetic IBM data generator.

B.  Experiment  ICA algorithm is implemented using MapReduce model  to evaluate its performance with parallel processes. This  experiment is performed on Hadoop (version 2.6) using  MapReduce programming model that supports the  parallelization. Tableau tool is used here to visualize the  output of frequent item-sets mining originally resides in the  Hadoop file Distributed System (HDFS). Whole experiment  is performed using 4 Giga Bytes internal RAM and Core 2  Duo processor. First and foremost it is important to code the  whole algorithm in terms of map and reduce function [11].

These two functions take the input in key-value pairs and  also produced the output in key-value terms [12].  A jar file  which consisting of three class file named as Mapper class,  Reducer class and Main class is executed on MapReduce  model which supports the fault-tolerance under the hood of  Hadoop architecture. Apart from that input file  (transactional data set) is stored in Hadoop Distributed File  System (HDFS) which is responsible for making replica of  the original data. HDFS is also responsible to divide the  whole datasets into manageable data chucks (sample data  sets) and send them to others nodes in distributed  computational environment [13]. Each and every nodes  consisting of Mapper receive the data chunks or sample data  in terms of key value. Here Mapper reads the input in  accordance with local minimum support and assigned value  1 after tokenization to the keyword that occurs first time.

Mapper function generates the output in terms of key-value  on each and every node. These Mapper output is passed to  combiner function where shuffle sort is used to shuffle all  the key-values. A new key-value form is generated after  combining the values and these values is further aggregated  by reducer function. Reducer also checks the occurring  frequency of each and individual item-sets to the global  minimum support values. If the occurring frequency of an  item-sets is equal to or greater than the global minimum  support, item-set with its occurring frequency is sent to  HDFS (storage unit) otherwise it is eliminated. This method  of pruning is repeated with each and every data-sets and  final result is stored in HDFS. It is also important to know,  final output is generated only when all the reducer which are  running on different nodes are completed their respective  jobs. Frequent 1-itemsets L1 is finally obtained by merging  the output of all reducers. To generate frequent k-item-sets  Lk, each Mapper reads frequent item-sets Lk-1 from previous  iteration and produces candidate item-sets Ck from Lk-1. A  candidate item-set in Ck is chosen as key and assigned a  value 1, if it is present in the transaction assigned to the  Mapper. Consequently, now we get (key, value) pair where  key is k item-sets and value is 1 and all the remaining  procedures are the same as generation of L1.

C.  Experimental Result and Analysis  We have implemented ICA algorithm to find frequent  item-sets on Hadoop architecture (version 2.6) using Ubuntu  (Linux) operating system. In Hadoop architecture both input  data as well as output data is stored in HDFS. Hence final  output that is automatically stored in HDFS, is read by ?-  cat? command and retrieved by ?copyToLocal? command  that has the basic functionality to copy a file from HDFS to  local file system [14]. For visualization purpose, Tableau  tool is used here and various outputs with different graphical  interface is given from fig. 2 to 5. All these output are  generated using different set of retail database as an input  with minimum support threshold. Fig. 2 shows the  occurrence frequency of data set along x-axis under the  parameter F2 and various transactional item-sets along the  y-axis under the parameter F1. All the output records are  generated only after surpassing the minimum support  threshold value that is 7 and already set in algorithm. On the  other hand, records which do not satisfy the minimum  threshold value are discarded from the record by reducer  function before generating the final output. Therefore all the  final data-set which are present in final record have the  value 1 under the title number of records and given in fig. 2  along with parameter F2. Fig. 3 visualize the number of  frequent item-sets which support the minimum threshold  value and saved in HDFS as a final individual record that is reflected by value 1 along the y-axis. It is also possible to  visualize the output record with another angle as shown by  fig. 4. All the parameters meaning for fig. 4 is same as  described for fig. 2.  Fig. 5 shows the total number of input  records in single bar form. In this figure letter ?k? stand for  thousand and parameter F1 and F2 both have the same meaning as discussed, before. For example a horizontal dark line in between 600k and 800k is showing the three different  item-sets under the parameter F1 and occurrence frequency  of these item-sets together is 64, as reflected by parameter         F2. It is also possible to visualize the occurrence frequency  of each item-set just by moving the horizontal dark line  along the bar using Tableau.

Fig. 2. Output visualization 1          Fig. 3. Output visualization  2             Fig. 4. Output visualization 3      Fig. 5. Output visualization 4

V.  CONCLUSION AND FUTURE ENHANCEMENT  In this paper, we have proposed ICA algorithm to handle  two big data issues in frequent item-sets mining: (1)  Efficiency: large size of data restricts exhaustive search or  complex data structures from practical use; (2) Space  Complexity: the massive intermediate and outputted item-  sets bring out-of-memory or data overflow problems for  many state-of-art approaches. We have validated and  demonstrated the effectiveness of the proposed approach on  transactional data-set under Hadoop architecture.  Future  enhancement can be carried out in two dimensions to  enhance the performance of ICA algorithm. One dimension  leads to modification in joining and pruning steps of existing algorithm to enable it to support pipelining or use of  alternative ICA like algorithms which are free from the  problem of multiple times scanning of database. Second dimension of enhancement leads to use of advanced MapReduce framework such as i2MapReduce model which supports incremental problem based algorithm to enhance  the overall throughput of system.

