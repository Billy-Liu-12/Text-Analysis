A Survey on different trends in Data Streams

Abstract - Due to the advancement of Technologies, many companies and organizations today have huge databases, that grow to a limit of millions of records per day. This data generation and storage has become faster than ever. This resulted in databases of unbounded growth. The data even changes over time. As a consequence, if the data cannot fit into memory, it may be necessary to sample a smaller training set.

The data streams have recently emerged to address the problems of continuous data. The mining of such data streams is a unique opportunity and even challenging task. Mining with data streams is the process of extracting knowledge structures from continuous, rapid data records. Due to increased streaming information, data stream mining has attracted the research community. Analyzing data streams helps in applications like scientific applications, business and astronomy etc.

In this paper, we present the overview of the growing field of Data Streams. We cover the theoretical basis needed for analyzing the streams of data. We discuss the various techniques used for mining data streams. The focus of this paper is to study the problems involved in mining data streams.

Finally, we conclude with a brief discussion of the big open problems in the area.

Keywords- Data Streams, Association Mining, Classification Mining, Clustering Mining.



I. INTRODUCTION  In the last decade, we have been facing data explosion which was a great innovation for information technology.

However, the question is what to do with all this data. Data mining is the process of discovering useful information (i.e.

patterns) underlying the data. It uses machine learning algorithms. These are set of techniques that use computer programs to automatically extract models representing patterns from data and then evaluate those models.

Information systems have become more complex, amount  of data being processed have increased and data became dynamic, i.e. because of common updates. This streaming information has following characteristics.

? The data arrives continuously from data streams.

? No assumptions on data stream ordering can be  made.

? The length of data stream is unbounded.

Efficiently and effectively capturing knowledge from data streams has become very critical; these include network traffic monitoring, web click-stream analysis, highway traffic congestion analysis, market basket data mining, credit card   Dr.C.R.K.Reddy Department of Computer Science.

Mahatma Gandhi Institute of Technology Hyderabad, India  crkreddy@gmail.com  fraud detection, stock tickers in financial services, link statistics in networking, sensor readings in environmental monitoring, and surveillance data in homeland security etc.

Systems, models and techniques have been proposed and developed over the past few years to address these challenges [1, 2].

The remaining parts of this paper are organized as follows: In Section 2, we briefly discuss the necessary background needed for data stream analysis. Then, Section 3 and 4 describes techniques and systems used for mining data streams Open and addressed research issues in this growing field are discussed in Section 5. These research issues should be addressed in order to realize robust systems that are capable of fulfilling the needs of data stream mining applications. Finally, section 6 summarizes the paper and discusses interesting directions for future research.



II. ESSENTIAL CONCEPTS OF DATA STREAMS (METHODOLOGIES)  From the statistical and computational approaches the problems of mining data streams can be solved using the methodologies which uses  ? Examining a subset of the whole data set.

? Algorithms for efficient utilization of time and  space, detailed discussion follows in section 3.

The First methodology refers to summarization of whole  data set or selection of a subset of the incoming stream to be analyzed. The techniques used are Sampling, load shedding, sketching, synopsis data structures and aggregation. These techniques are briefly discussed here  A. Sampling Sampling is one of the old statistical technique, used for  a long time which makes a probabilistic choice of data item to be processed. Boundaries of error rate of the computation are given as a function of the sampling rate. Very Fast Machine Learning techniques [3] have used Hoeffding bound to measure the size of sample.

Problems related to sampling technique while analyzing the data stream are  ? Unknown size of dataset ? Sampling may not be the correct choice in  applications which check for anomalies in surveillance analysis.

? It does not address the problem of fluctuating data rates  Relations among data rate, sampling rate and error bounds are to be generated.

B. Load shedding Load shedding is a process of dropping a fraction of  data streams during periods of overload [4, 6]. Load shedding is used in querying data streams for optimization.

It is desirable to shed load to minimize the drop in accuracy [6]. Load shedding also has two steps. Firstly, choose target sampling rates for each query. In the second step, place the load shedders to realize the targets in the most efficient manner. It is difficult to use load shedding with mining algorithms because the stream which was dropped might represent a pattern of interest in time series analysis. The problems found in sampling are even present in this technique also.

C. Sketching Sketching [1, 2] is the process of randomly projecting a  subset of the features. It is the process of vertically sampling the incoming stream. Sketching has been applied in comparing different data streams and in aggregate queries.

The major drawback of sketching is that of accuracy because of which it is hard to use this technique in data stream mining.

D. Synopsis Data Structures Synopsis data structures hold summary information  over data streams. Construction of summary or synopsis data structures over data streams has been of much interest recently. Complexities calculated cannot be O(N) where N can be space/time cost per element to solve a problem.

Some solution which gives results closer to O(poly(logN)) are needed. Synopsis data structures are developed which use much smaller space of order O(logk N). These structures refer to the process of applying summarization techniques.

The smaller space which contains summarized information about the streams is used for gaining knowledge. There are a variety of techniques used for construction of synopsis data structures. These methods are briefly discussed.

? Sampling methods: This method is easy to use with wide variety of applications as it uses the same multi-dimensional representation as the original data points. In particular reservoir based sampling methods [7] are very useful for data streams.

? Histograms: Another key method for data summarization is that of histograms. Histograms have been used widely to capture data distribution, to represent the data by small number of step functions. These methods are widely used for static data sets. However most traditional algorithms on static data sets require super-linear time and space.

This is because of the use of dynamic   programming techniques for optimal histogram construction. Their extension to the data stream case is a challenging task. A number of techniques [8] discuss the design of histograms for the dynamic case.

? Wavelets: Wavelets [9] are a well known technique which is often used in databases for hierarchical data decomposition and summarization.

The basic idea in the wavelet technique is to create a decomposition of the data characteristics into a set of wavelet functions and basis functions. The property of the wavelet method is that the higher order coefficients of the decomposition illustrate the broad trends in the data, whereas the more localized trends are captured by the lower order coefficients. In particular, the dynamic maintenance of the dominant coefficients of the wavelet representation requires some novel algorithmic techniques.

? Sketches: Randomized version of wavelet techniques are called sketch methods. These methods are difficult to apply as it is difficult for intuitive interpretations.

? Micro Cluster based summarization: A recent micro-clustering method [10] can be used be perform synopsis construction of data streams.

This technique uses Cluster Feature Vector (CFV) [11]. This micro-cluster summarization is applicable for the multi-dimensional case and works well to the evolution of the underlying data stream.

? Aggregation: Summarizations of input streams are generated using mean and variance. If the input streams have fluctuating nature then this technique fails. This can be used for merging offline data with online data which was studied in [12].

Many synopsis methods such as wavelets, histograms, and sketches are not easy to use for the multi-dimensional cases.

The random sampling technique is often the only method of choice for high dimensional applications.



III. ALGORITHMS FOR EFFICIENT UTILIZATION OF TIME AND SPACE  The existing algorithms used for data mining are modified for generation of efficient algorithms for data streams. New algorithms have to be invented to address the computational challenges of data streams. The techniques which fall into this category are approximation algorithm, sliding window algorithm and output granularity algorithm.

We examine each of these techniques in the context of analyzing data streams.

A. Approximation algorithm  Approximation techniques are used for algorithm design. Solutions obtained with this algorithm are approximate and are error bound. These have attracted     researchers as a direct solution for data streams. Data rates with the available resources cannot be solved. To provide a solution to these other tools should be used along with this algorithm. For tracking most frequent items dynamically this algorithm was used in [5].

B. Sliding Window  In order to analysis recent data, sliding window protocol is used. Analysis over the new arrived data is done using summarized versions of previous data. This techniques are used in comprehensive data mining systems MAIDS [13]. Using sliding window protocol the old items are removed and replaced with new data streams. Two types of windows called count-based windows and time-based windows are used. Using count-based windows the latest N items are stored. Using the other type of window we can store only those items which have been generated or have arrived in the last T units of time [14].

C. Algorithm Output granularity (AOG)  AOG is used with fluctuating very high data rates. It works well with available memory and with time constraints [15]. The stages in this process are mining data streams, adaptation of resources and streams and merging the generated structures when running out of memory.

These algorithms are used with association, clustering and classification.



IV. APPLICATION OF METHODOLOGIES FOR STREAM MINING  The need to understand the enormous amount of data being generated every day in a timely fashion has given rise to a new data processing model-data stream processing. In this new model, data arrives in the form of continuous, high? volume, fast and time-varying streams and the processing of such streams entail a near real-time constraint. A number of algorithms for extraction of knowledge from data streams were proposed in the domains of clustering, classification and association. In this section, an overview on mining these streams of data is presented.

A. Clustering  Guha et al. [16] have studied analytically clustering data streams using K-median technique. The proposed algorithm makes a single pass over the data stream and uses small  space. It requires O(nk) time and O(n E) space where "k" is the number of centers, "n" is the number of points and E <1.

They have proved that any k-median algorithm that achieves a constant factor approximation can not achieve a better run time than O(nk). The algorithm starts by clustering a calculated size sample according to the available memory into 2k, and then at a second level, the algorithm clusters the above points for a number of samples into 2k and this process is repeated to a number of levels, and finally it clusters the 2k clusters into k clusters.

Domingos et al. [17] have proposed a general method for scaling up machine learning algorithms. They have termed this approach Very Fast Machine Learning VFML. This method depends on determining an upper bound for the leamer's loss as a function in number of data items to be examined in each step of the algorithm. They have applied this method to K-means clustering VFKM and decision tree classification VFDT techniques. These algorithms have been implemented and evaluated using synthetic data sets as well as real web data streams. VFKM uses the Hoeffding bound to determine the number of examples needed in each step of K-means algorithm. The VFKM runs as a sequence of K? means executions with each run uses more examples than the previous one until a calculated statistical bound (Hoeffding bound) is satisfied. Gaber et al. [19] have developed Lightweight Clustering LWe. It is an AOG-based algorithm. AOG has been discussed in section 2. The algorithm adjusts a threshold that represents the minimum distance measure between data items in different clusters.

This adjustment is done regularly according to a pre? specified time frame. It is done according to the available resources by monitoring the input-output rate. This process is followed by merging clusters when the memory is full.

Christian S. Jensen [18] has proposed a fast and effective scheme that is capable of incrementally clustering moving objects. They used notion of object dissimilarity, which is capable of taking future object movement which improves the quality clustering and runtime performance. The Experiments conducted show that the proposed MC algorithm is 106 times faster than K-Means and 105 times faster than Birch. They have used average-radius function which automatically detects cluster split events.

TABLE I. CLUSTERING  Sno Author Methodolol!Y Key Features  I Guha K medians Single pass and uses  small space.

Domingo Hoeffding bound Constant memory and  2 (Sampling constant time for an s  Techniqu?) example.

3 Gaber AOG visualization of cluster  dynamics  Object Efficiently maintaining a  4 Christian clustering of a dynamic dissimilarity  set.

B. Classificaions  In the real world concepts are often not stable but change with time. Typical examples of this are weather prediction rules and customers' preferences. The underlying data distribution may change as well. Often these changes make the model built on old data inconsistent with the new data and regular updating of the model is necessary. This problem is known as concept drift.

Wang et al. [20] proposed a general framework for mining concept drifting data streams. This was first framework which worked on drifting. They have used weighted classifier to mine streams and old data expires based on     distribution of data. The proposed algorithm combines multiple classifiers weighted by their expected prediction accuracy.

Domingos et al. [21] have developed Vary Fast Decision Tree (VFDT) which is a decision tree constructed on Hoeffding trees. The split point is found by using Hoeffding bound which satisfies the statistical measure. This algorithm also drops the non-potential attributes.

Aggarwal has used the idea of microclusters introduced in CluStream in On-Demand classification [22] and it shows a high accuracy. The technique uses clustering results to classify data using statistics of class distribution in each cluster.

Peng Zhang [23] have proposed a solution for concept drifting where in they categorized the concept drifting in data streams into two scenarios: (1) Loose Concept Drifting (LCD); and (2) Rigorous Concept Drifting (RCD), and proposed solutions to handle each of them by using weighted-instance and weighted classifier ensemble framework such that the overall accuracy of the classifier ensemble built on them can reach the minimum.

TABLE II. CLASSIFICATION  Sno Author Methodolo2Y Key Features  I Wang et Weighted classifier to Concept drifting al. mine streams  Domingo Decision trees using Single pass  s et al. Heoffding tree algorithm 3 Aggarwal Micro clusters High accuracy  used weighted-instance and weighted classifier  Peng such that the overall  Concept drifting Zhang accuracy of the  classifier built can reach minimum.

C. Association  Manku and Motwani [24] have proposed and implemented an approximate frequency counts in data streams. The implemented algorithm uses all the previous historical data to calculate the frequent patterns incrementally.

Cormode and Muthukrishnan [25] developed an algorithm for counting frequent items. This is used to approximately count the most frequent items.

Jianyong Wang [26] have proposed an algorithm called BIDE which efficiently mines frequent items without candidate maintenance. It uses a novel sequence called BI? Directional Extension and prunes the search space more deeply than the previous algorithm. The experimental results show that BIDE algorithm uses less memory and faster when support is not greater than 88 percent.

Carson Kai-Sang Leung and Dale A.Brajczuk [27] have proposed an algorithm called DSP which efficiently mines frequent items in limited memory environment.

Carson Kai-Sang Leung, Dale A.Brajczuk and Jialiang Yu [27] have proposed an algorithm which focuses on   mining process, discover from data streams all those frequent patterns that satisfY the use constraints, and handle situations where the available memory space is limited.

TABLE III . ASSOCIA nON  Sno Author Methodol? Concentration Problems arised when data-  Randomized stream sketches  I Manku and techniques using are used to  Motwani sketch synopsis  process multiple such queries concurrently  maintains a small space data structure  that monitors the transactions on the  Cormode and relation, and when quality and  Muthukrishnan required, quickly performance outputs all hot items, without rescanning the relation in the  database Mines frequent items  Less memory 3 Jianyong Wang without candidate  and fast maintenance  Carson Kai-Sang Mines frequent items Less memory  4 Leung and Dale in limited memory and fast  A.Braiczuk Carson Kai-Sang Mines frequent  5 Leung, Dale itemsets in limited Less memory ABrajczuk and memory for and fast  Jialiang Yu constrained data

V. RESEARCH ISSUES  The following are some of the research issues in data streams  ? Low memory requirements while processing large data sets  ? variants in mining tasks that are desirable in data streams  ? High accuracy in the results generated while dealing with continuous streams of data  ? Transferring data mining results over a wireless network with a limited bandwidth.

? The needs of real world applications ? Efficiently store the stream data with timeline  and efficiently retrieve them during a certain time interval in response to user queries is another important issue.

? Online Interactive processing is needed which helps user to modifY the parameters during processing period.



VI. CONCLUSIONS  In this paper we discussed the issues that are to be considered when designing and implementing the data     stream mining technique. We even reviewed some of these methodologies with the existing algorithms.

From the discussion we can conclude that most of the  current mining approaches use one pass mining algorithms and few of them even address the problem of drifting. The present techniques produce approximate results due to limited memory.

