

Abstract? Association rule mining using weighted  frequent itemset plays the vital role to determine the association and correlation among the items having different weights. These weights associated with the items signifies its importance in the transaction which is based on different criteria like rating, popularity etc. The weighted frequent itemset mining in the distributed environment is one of the most popular research methodology for finding hidden information. In this paper we have focused on the number of instances of an item purchased in a particular transaction of market-basket data as weight of the items in that transaction.We have represented the weighted frequent itemset mining in the distributed large multi partitioned memory based upon the hadoop framework.

Keywords?weighted frequent itemset, Association Rule, BigData,  hadoop, YARN,split.



I.  INTRODUCTION The main goal of data mining in Bigdata environment  is to extract meaningfull, hidden and uncertain knowledge from huge and unstructured data in an appropriate and understandable format. Frequent itemset mining is the exploratory technique for extracting potentially useful and hidden information from various data collected from different sources. The frequent pattern mining[1, 2] was aimed in discovering frequent patterns or itemset whose occurrence frequency is more then the user specified threshold. Apriori[1] and the FP-Growth[3] algorithms are very widely used and popular algorithm for mining the frequent itemset.These frequent itemset mining algorithms are based upon the anti- monotonity otherwise known as downward closure property for pruning the infrequent itemset by comparing the user specified support known as minimum support. Based upon the frequent itemset mining various studies have been performed on Association rule mining[1], sequential pattern mining[4, 5], weighted frequent patterns[6, 7, 8, 13, 14, 15] etc. The Apriori algorithm scans the transactional database multiple times for generating the candidate itemset and frequent itemset. The candidate itemset will also very large when the items in the  transaction is increasing significantly. It leads to a large execution time. In comparison to this FP-Growth avoids the generation of the candidate itemset but required more space for the construction of FP-tree. This memory constraints becomes the limitation when the transactional data size grows exponentially and becomes very large. Transactional data in the real time have their significant weight with respect to various criteria like customer rating, usage, market demand, importance, price etc. So the demand of itemset mining with the associated weight plays the significant role in finding the item or pattern correlation and association for extracting hidden information. The weighted frequent itemset mining also plays the vital role in the analysis of product recommendation system and understanding the buying habits of users from various regions. Rather then considering the normal itemset we have considered the number of instance of the item purchased in the transaction as weight of the transaction.

In the era of BigData it has been shown that dataset for various data analysis application is increasing very first in an exponential fashion and in various formats. This increase rate in the data leads to the difficulty in storage and processing of the data in a single sequential machine. Many parallel and distributed algorithms has been proposed for mining the frequent itemset in a very large database[18,19,20] but having limiting issues like prepare the appropriate partition size for the data, distribute the data appropriately, load balancing, dynamically increase the partition when the data size grows etc. So in the context of BigData, the analysis of weighted frequent itemset mining in a distributed multi partitioned environment do not fit into the memory while processing the large amount of data in single machine. This issue can be solved by analyzing the Weighted Frequent Itemset Mining(WFIM) using MapReduce programming model over YARN framework of Hadoop. The MRV2 known as YARN is more optimized and scheduled framework then MRV1. The MapReduce of YARN is a massively parallel and distributed programming model for processing very large amount of structured and unstructured data.

In this paper we have analyzed the MPWFIM(Multi Partitioned Weighted Frequent Itemset Mining) algorithm on the extensive and optimized distributed framework of Hadoop known as YARN. We have also considered the weight of the transactional item as the number of instance of that item occurred in one transaction.



II. RELATED WORK  A. Association Rule Mining Agrawal et. al. first introduce the problem of association rule mining over a market basket transaction database. Let  0 1 1= { , ,..., }total nI I I I ?  be the set of items. Let TDB be a transaction data base contaning all the transaction, where each transaction rT  in TDB is a set of item, i.e. r totalT I? . A set of item X is also referred as an itemset. An itemset that contains k items is called as k-itemset. A transaction rT supports an itemset X if rX T? . An association rule is denoted as the form X Y?  where totalX I? , totalY I? and =X Y ??  , e.g let = { , , , , }totalI A B C D E ,  = { , }X A C  and = { , }Y B E , a rule X Y?  includes two important attribute values support and confidence denoted as Sup( X Y? ) and Conf( X Y? ), respectively. Given two user specified minimum support ( )minSup  and minimum confidence ( )minConf  thresholds, a rule X Y?  holds in TDB  iff ( )Sup X Y minSup? ?  and  ( )Conf X Y minConf? ? . The support value s% of X Y?  means s% of transaction in TDB contains X Y? .

The confidence values c% of X Y?  means that the transactions contain X in TDB in which c% of them also contain Y.

B. Hadoop Hadoop is the most popular and widely used open source  framework for the BigData storage and analysis. It was inspired from Google?s GFS and MapReduce implementation to store and process huge volume of data[19]. The Hadoop becomes very popular because it can handle very large amount of structured, semistructure and unstructured data on a cluster having low cost or general purpose computers. Hadoop basically stores the data in HDFS in a multi partitioned fashion[16,18] i.e if a data is very large it is divided into multiple number of partitioned and distributed over all the computing nodes with multiple replica. For processing these data it uses a parallel processing framework known as MapReduce. Hadoop consiests of a well organized ecosystem for analysis of BigData.

C. MapReduce on YARN MapReduce is the processing fromework of hadoop which process huge amount of data in parallel fashion on the commodity hardware. The MapReduce uses key/value pairs for the processing of huge amount of data in a distributed fashion. As compared to the MRV1, the MRV2 i.e Mapreduce  on YARN(Yet Another Resource Navigator) is more optimized in terms of job execution, resource management and scheduling. In order to do all the processing and data distribution YARN employs the global ResourceManager, per node NodeManager, ApplicationMaster and Container along with the NameNode and DataNode. The YARN fail over and fancing for the recovery.

MapReduce is the defacto standard framework for the implementation of Apriory like ferquent pattern mining algorithms. The Apriori algorithm need many mapper and reducer phases to compute the frequent itemset[11]. In [21] an efficient Apriori like algorithm was proposed known as MRApriori which need only two phase and is based upon the data spliting mechanism. The Mapper is created based on number of split for the stored transactional data in HDFS. This algorithm each Mapper takes one split as input in the first phase and all the output from the Mapper were supplied to the reducer in the second phase which produce the frequent itemset. The MRApriori is very efficient but generate very large amount of partial frequent k-itemset in each node for which the performance is degrade.



III. PROPOSED WORK We have implemented the WFIM algorithm in multi  partitioned environment and used the local aggregation to increase the efficiency using the MRV2 of Hadoop with container management. We have studied many WFIM algorithms in [14,15,16], but they are not efficient in terms of issues like partition management and local aggregation while generating frequent itemset in parallel. The main focus of our implementation is the way we have determined the item weight to find the hidden knowledge from the data. All these WFIM algorithms are not considering the number of instances of individual item in the transaction. We have taken the number of instance of that item as the weight of the item.

Association rule mining derives important information from the itemset in the form of support and confidence in a transaction for the two item 1 2,I I  the rule is represented 1 2I I? . Here the support and confidence is not related to the weight associated with item in the transaction.

We have considered weight of the item as occurrence of the item in the transaction e.g  1 2 1 1 2= { , , , , }rT I I I I I  . Here the occurrence of 1 = 3I  and 2 = 2I  can be represented as  1 2= {< ,3 >, < , 2 >}rT I I In this situation support and confidence are not  much informative to find out hidden knowledge from the transactional dataset. In our approach we have not only generate support and confidence but also the correlation between LHS and RHS of the rule. When number of transaction are larger then the conventional algorithms are not sufficient and suitable to process the input and output.

We have taken the following synthesized transactional dataset where the weight of the item represents how many number of an item has been purchased in a transaction i.e the number of instances of a perticular item appears in a transaction.

1 1 2 5  2 2 4  3 2 3  4 1 2 4  5 1 3  6 2 3  7 1 3  8 1 2 3  9 1 2 3  10 1  = {< ,5 >,< ,4 >,< ,3 >} = {< ,3 >,< ,3 >} = {< ,2 >,< ,5 >} = {< ,3 >,< ,4 >,< ,3 >} = {< ,5 >,< ,6 >} = {< ,4 >,< ,3 >} = {< ,2 >,< ,1 >} = {< ,1 >,< ,3 >,< ,2 >} = {< ,2 >,< ,4 >,< ,5 >} = {< ,3  T I I I T I I T I I T I I I T I I T I I T I I T I I I T I I I T I 2 4>,< ,7 >,< ,5 >}I I    Suppose 1 2I I?  is the generated association rule with confidence 4 / 6 = 0.67 . The hidden information that we are getting from this association rule is in terms of the occurrence of the item as a whole. Out of the total transaction  1 2I I?  present as follows  1 1 2  4 1 2  8 1 2  9 1 2  10 1 2  :< ,5 > < , 4 > :< ,3 > < , 4 > :< ,1 > < ,3 > :< , 2 > < , 4 > :< ,3 > < ,7 >  T I I T I I T I I T I I T I I  ? ? ? ? ?     In this case it is quite difficult to find out in what way  both the items are correlated in terms of their associated weights. We found in some of the transaction the occurrence of 2I  is larger then occurrence of 1I  and in some other cases the occurrence of 1I  is larger then 2I . Here it creates confusion whether 1I  participation is high or 2I  is high when both the items 1I  and 2I  are purchased together. Here we can divide 1 2I I?  into two different categories.

1case ?  : 1< , >I m   2< , >I n  where >m n 2case ?  : 1< , >I m   2< , >I n  where <m n From our study we are going to determine which  categories of weighted items are dominating the transaction dataset. Suppose in a transaction database TDB  1 2case and case? ?  exist but 2case ?  majority is high then we can claim that in the Association rule 1I  2I  count of  1I  is smaller then count of 2I .

In both the cases the association rule with highest  weight count is considered.

Let us considered the mentioned rules for our  analysis  1 2  1 2  1 2  1 2  1 2  < ,5 > < , 4 > < ,3 > < , 4 > < ,1 > < ,3 > < , 2 > < , 4 > < ,3 > < ,7 >  I I I I I I I I I I  ? ? ? ? ?    The above association rules can be grouped as per the mentioned cases  case - 1:           1 2< ,5 > < , 4 >I I? case - 2:           1 2< ,3 > < , 4 >I I?  1 2< ,1 > < ,3 >I I?  1 2< , 2 > < , 4 >I I?  1 2< ,3 > < ,7 >I I? Now let us take the total count of each categories  11: < , >case I m?   2< , >I n  with >m n 1 2< ,5 > < , 4 >I I?  12 : < , >case I m?   2< , >I n  with <m n 1 2< ,9 > < ,18 >I I?   It is clearly visible that case - 2 is dominating case - 1  with highest number of count of 1I  and highest number of transactions. So the useful knowledge can be retrieved as an association exist among 1I  and 2I  where the count of 1I  is smaller then the count of 2I  ignoring exceptional cases.

Theorem-1: The number of transaction of the transactional database are not frequent iff the items are not frequent.

Let totalI  = Set of total items.

iI =Individual items (where i=0,1,2,...,n-1)  totalI =  =0  n  i i  I ?  ? totalT =Total number of transaction in the  transactional database TDB  rT =Individual transaction (where r=0,1,2,...,m-1)  totalT =  = 0  m  r r  T ?  ?  As (0 1)r j m totalT T? ? ? ?  (0 1) ( )r j m totalT Subset I? ? ?? ? Let any item i totalI I?  where 0 1i n? ? ? , is not frequent As iI  is not frequent, any transactional itemset with item iI is also not frequent. A transaction containing infrequent itemset also not frequent. Hence may be said as infrequent transaction.



IV. THE PROPOSED ALGORITHM PSEUDO CODE Mapper-1  1. class OneItemsetMapper{ 2.  Read transaction from split of TDB 3.  count=1 4.  WIList=0 5.  (0< < 1)r r mT TDB?? ? 6.      i rI T? ? 7.            iI .count++ 8.            WIList.add( < , . >i iI I count ) 9.       Emit(WIList,NULL) 10. }  Reducer-1 1. class OneItemsetReducer{ 2.  read the Mapper output from HDFS 3.  read(WIList. iI , < . >)iList I count 4.  . < . >i iI count List I count? ? 5.           iI .count++ 6.  ( , . )i iI I count? 7.  if . <iI count minCount 8.       ( , . )i idelete I I count 9.  ( , . )i iEmit I I count 10.}  Mapper-2 1. class RuleMapper{ 2.       Receive from OneItemReducer 3.      1C =genFOneItemset() 4.      ARList=generateAR( 1C ) 5.}  Partitioner 1. class RulePartitioner{ 2.  rules ARList? ? where the rules represented as < , >iI m   < , >jI n 3.   if <m n 4.       write rules to 1part ? 5.   else 6.       write rules to 2part ? 7. }

V. EXPERIMENTAL EVALUATION For our experimental evaluation we have configured  5-node cluster with hadoop-2.6.0-cdh. All the nodes are having 4GB of RAM, 2.5GHz of CPU. The heap size for the JVM is 512MB which is for the general purpose application execution, 128MB of HDFS block size. The data is evenly distributed in all the datanodes of the cluster. We have executed our algorithm of supermarket dataset by normalizing it. The item weights are computed by MapReduce program considering the number of instances of a particular item  occurred in a transaction and added with the item as shown in the example(Sec-III). We have used execution time and minimum support count as the measure for the performance evaluation as shown in Fig-1. We have used the local aggregation in Mapper side and partitioned the Mapper output with costumed partitioner to increase the efficiency in execution. We have also analyze the optimization of the job execution in the MRV2 container management.

It has been shown that with a single DataNode execution time is more if support is less and with multiple DataNode execution time is significantly improving with support count by varying from 0.01 to 0.10 as shown in the graph. This graph gas been generated using the plot of Revolution R and the the programming framework used is YARN on top of Hadoop.

In Fig-2 we have compared the performance measure between the MPWFIM and WFIM by considering the transaction size and number of rules generated. It has been found that MPWFIM algorithm generate less number of rules as compared to WFIM algorithm.

Fig-1: Performance measuse of MPWFIM in single and multi  node cluster    Fig-2: Performance measure between MPWFIM and WFIM

VI. CONCLUSION In this paper we have emphasized how the weight of an itemset can impact on the transactional data by considering the number of occurrences of a particular item in a transaction. By determining the frequent weighted itemset we have collected hidden information like how the associated rules are dominating to one another. We have propose how to choose an    efficient rule to increase the marketting starategy by recomending a better product association. In our approach we have also proposed how efficiently the job can be optimized by using the costumed aggregation and partitioning. The partitiones are generated in such a way that the hadoop container of the NodeManager manage and execute the jobs from each partition, the rules will be stored in the partition by checking a proper condition such that they can be accessed and analyzed by the Reducer properly. We have used the YARN framework for the better performance evaluation. Our proposed parallel algorithm distributes the process of generating frequent itemsets and rules to multiple tasks executed on commodity machine. As part of the future work we try to propose automated fashion of performing partition as per the number of rules generated during the analysis.

