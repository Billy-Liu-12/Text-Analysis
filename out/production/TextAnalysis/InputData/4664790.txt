Granular Computing in Privacy-Preserving Data Mining

Abstract  Granular computing is an emerging computing paradigm of information processing. It concerns the processing of complex information entities, called "infor- mation granules", which appear in the process of data abstraction and derivation of knowledge from information.

The granular computing paradigm has been applied to many applications and we will address the application of granular computing in privacy-preserving data mining.

We will use privacy-preserving association rule mining and privacy-preserving k-nearest neighbor classification to illustrate how the paradigm of granular computing has been applied.

1 Introduction  Granular computing [2, 7,10, 11] is an emerging com- puting paradigm of information processing, concerning the processing of complex information entities called informa- tion granules, which appear in the process of data abstrac- tion and derivation of knowledge from information. Gran- ular computing is an approach to looking at data that rec- ognizes different and interesting regularities in the data that can appear at different levels of granularity. In this paper, we will discuss the application of granular computing in privacy-preserving data mining systems.

Data mining successfully extracts knowledge to support a variety of domains but it is still a challenge to mine cer- tain types of data without violating the data owner's privacy.

In recognition of growing privacy concerns, directives such as the American Health Insurance Portability and Account- ability Act (HIPAA) and the European Union Privacy Direc- tive both mandate privacy protection for data management and analysis systems. As data mining becomes more perva- sive, such concerns are increasing.

The granular computing paradigm has been widely ap-  *The author has research affiliation with Chinese Academia of Science.

Tsau Young Lin Department of Computer Science  San Jose University tylin@cs.sjsu.edu  plied. In this paper, we will address the application of gran- ular computing in privacy-preserving data mining. Specifi- cally, we utilize two typical examples in privacy-preserving data mining, which are privacy-preserving association rule mining and privacy-preserving k-nearest neighbor classifi- cation, to illustrate that the paradigm of granular computing has been exploited in privacy-preserving data mining.

In the following section, we will introduce the privacy- preserving association rule mining problem. In section 3, we will describe the privacy-preserving k-nearest neighbor classification problem. In section 4, we will present gran- ular computing and the composition theorem for privacy- preserving data mining. We make a conclusion in section 5.

2 Privacy-Preserving Association Rule Min- ing  Since its introduction in 1993 [1], association rule min- ing has received a great deal of attention. It is still one of most popular pattern-discovery methods in the field of knowledge discovery. Briefly, an association rule is an ex- pression X =? Y, where X and Yare sets of items. The meaning of such rules is as follows: Given a database D of records, X =? Y means that whenever a record R contains X then R also contains Y with certain confidence. The rule confidence is defined as the percentage of records con- taining both X and Y with regard to the overall number of records containing X. The fraction of records R supporting an item X with respect to database D is called the support of X. Privacy-preserving association rule mining has been addressed in [12,14], where the following scenario was con- sidered: Multiple parties, each having a private data set (de- noted by D 1 , D 2 , ... , and D n , respectively), want to col- laboratively conduct association rule mining on the union of their data sets. Because they are concerned about their data's privacy, neither party is willing to disclose their raw data set to others. Without loss of generality, we can make the following assumptions on the data sets:    1. D 1 , D 2 , ... and D n are binary data sets, where n is the total number of parties.

2. D 1 , D 2 , ..? and D n contain the same number of records. Let N denote the total number of records for each data set.

3. The identities of the ith (for i E [1, N]) record in D 1, D 2 , ... and D n are the same.

The assumptions can be achieved by pre-processing the data sets D 1 , D 2 , ... , and D n , and such pre-processing does not require one party to send their data set to other parties.

2.1 Association Rule Mining Procedure  The following is the procedure for mining association rules on [D1 U D 2 ... U D n ]:  1. L 1 =large 1-itemsets  2. for (k =2; Lk-1 =I=- ?; k++) do begin  3. Ck =apriori-gen(Lk - 1)  4. for all candidates c E Ck do begin  5. Compute c.count  6. end  7. Lk ={c E Cklc.count 2: min-sup}  8. end  9. Return L =UkLk  The procedure apriori-gen is described as follows: apriori-gen(Lk - 1: large (k-1)-itemsets)  1. for each itemset II E L k - 1 do begin  2. for each itemset l2 E Lk - 1 do begin  3. if ((l1 [1] == l2 [1]) 1\ (l1 [2] == l2 [2]) 1\ ... 1\ (ll[k - 1] == l2[k - 1]) 1\ (ll[k - 1] < l2[k - 1]){  4. then c =II join l2  11. end  12. end  13. return Ck  In order to solve this privacy-preserving association rule mining, we need to use the paradigm of granular comput- ing. We do not need to provide a secure algorithm to pro- tect the privacy of the whole procedure. Instead, we pro- vide component protocols that can protect privacy in certain key steps. We also provide a composition theorem that can guarantee that the combination of the component protocols can achieve the privacy guarantee for the whole privacy- preserving association rule mining systems.

2.2 How to Compute c.count  If all the candidates belong to the same party, then c.count, which refers to the frequency counts for candidates, can be computed by this party. If the candidates belong to different parties, they then construct vectors for their own attributes and apply the number product protocol to obtain c.count. We use an example to illustrate how to compute c.count among three parties. Party 1 (Alice), party 2 (Bob) and party 3 (Carol) construct vectors X, Y and Z for their own attributes, respectively. To obtain c.count, they need to compute 2:~1 X[i] . Y[i] . Z[i], where N is the total number of values in each vector. For instance, if the vec- tors are as depicted in Fig.1, then 2:~1 X[i] .Y[i] . Z[i] == 2:~=1 X[i] .Y[i] . Z[i] == 3.

Drug Injection Taker Hepatitis Credit Rating  Yes Have Bad  No Have Good  Yes Have Bad  Yes Have Bad  Yes Haven't Good  Vector Construction j  Figure 1. Raw Data For Alice, Bob and Carol  5.

6.

7.

8.

for each (k-1 )-subset s of c do begin  ifs tJ- L k - 1  then delete c  else add c to Ck  x  Alice  Y  Bob  z  Carol  9. end  10. }   The challenge that we want to address is how two or more parties jointly compute c.count without revealing their    raw data to each other. The number product protocol de- scribed in this section is the main technical tool used to compute it. We will describe an efficient solution of the number product protocol based on a commodity server [3], which is a semi-trusted party. Instead of participating in the actual computation among the parties, the commodity server only supplies commodities that are independent of the parties' private data.

2.3 Secure Number Product Protocol  Let us first consider the case of two parties where n == 2 (more general cases where n 2: 3 has been discussed in [12]). Alice has a vector X and Bob has a vector Y. Both vectors have N elements. Alice and Bob want to com- pute the product between X and Y such that Alice gets I:~I Ux[i] and Bob gets I:~I Uy[i], where I:~I Ux[i] + I:~I Uy[i] == I:~I X[i] . Y[i] == X . Y. Uy[i] and Ux[i] are random numbers. Namely, the scalar product of X and Y is divided into two secret pieces, with one piece going to Alice and the other going to Bob. We assume that random numbers are generated from the integer domain.

Protocoll (Secure Two-party Product Protocol)  1. The Commodity Server generates two random num- bers Rx[l] and Ry[l] ,and lets rx[l] + ry[l] == Rx[l] .

Ry[l], where rx[l] (or ry[l]) is a randomly generated number. Then the server sends (Rx [1] , r x [1]) to Alice, and (Ry [1], r y [l]) to Bob.

2. Alice sends X[l] == X[l] +Rx[l] to Bob.

3. Bob sends Y[l] == Y[l] +Ry[l] to Alice.

4. Bob generates a random number Uy [l], and computes X[l] .Y[l] + (ry[l] - Uy[l]), then sends the result to Alice.

5. Alice computes (X[l].Y[l]+(ry[l]-Uy[l]))-(Rx[l].

Y[l]) + rx[l] =X[l] .Y[l] - Uy[l] + (ry[l] - Rx[l] .

Ry[l] + rx[l]) =X[l] .Y[l] - Uy[l] == Ux[l].

6. Repeat step 2-5 to compute X[i] . Y[i] for i E [2, N].

Alice then gets I:~I Ux[i] and Bob gets I:~I Uy[i].

3 Privacy-Preserving K-nearest Neighbor Classification  The k-nearest neighbor classification is an instance- based learning algorithm that has been shown to be very effective for a variety of problem domains. The algorithm assumes that all instances correspond to points in the n- dimensional space. The key element of this scheme is the   availability of a similarity measure that is capable of iden- tifying neighbors. The nearest neighbors of an instance are defined in terms of the standard Euclidean distance. More precisely, let an arbitrary instance x be described by the feature vector (al (x), a2(x),? .. ,am(x)), where ar(x) de- notes the value of the rth attribute of instance x. Then the distance between two instances Xi and xj is defined as d(Xi, Xj), where  m  d(Xi' Xj) == L(ar(xi) - ar (Xj))2. (1) r=1  3.1 Problem Definition  The privacy-preserving k-nearest neighbor classification problem has been addressed in [13], where we consider the following scenario: Multiple parties, each having a private data set (denoted by D I , D 2 , .?. and D n , respectively), want to collaboratively build a k-nearest neighbor classifier on the concatenation of their data sets. Because they are concerned about their data privacy, neither party is willing to disclose its raw data set to others. Without loss of gen- erality, we make the following assumptions about the data sets:  ? D I , D 2 , ... and D n contain the same number of records.

? Let N denote the total number of records for each data set.

? The identities of the ith (for i E [1, N]) record in D I , D 2 , ... and Dn are the same.

? The class labels are known by all the parties.

? The total number of parties, n, is greater than 2.

? Each party receives only a portion of the query input according to her attribute set. In other words, one party does not know what values of the input query are re- ceived by other parties.

3.2 K-nearest Neighbor Classification Procedure  In this paper, we consider learning discrete-valued target functions of form f : Rn ----+ V, where V is the finite set VI , V2, . .. ,Vs. The following is the procedure for building a k-nearest neighbor classifier on [D I U D 2 ... U D n ].

1. Training algorithm:  ? For each training example (x, f(x?, add the ex- ample to the list training-examples.

2. Classification algorithm: Given a query instance x q to be classified,  ? let Xl, ... X k denote the k instances from training-examples that are nearest to x q  ? return  k  j(xq ) ~ argmax L 6(v, f(Xi)), vEV  i=l  where 6(a, b) == 1 if a == band where 6(a, b) == 0 otherwise.

3.3 How to Obtain the K-nearest Neigh- bor Instances  Given a query instance x q , we want to compute the dis- tance between x q and each of the N training instances.

Since each party holds only a portion of a training instance, each party computes their portion of the distance measure (called the distance portion) according to their attribute set.

To decide the k-nearest neighbors of x q , all the parties need to sum their distance portions together. For example, as- sume that the distance portions for the first instance are Sll, S12, ... , SIn, and the distance portions for the second in- stance are S2l, S22, ... , S2n. To compute whether the dis- tance between the first instance and x q is larger than the distance between the second instance and x q , we need com- pute whether 2:~1 Sli 2 2:~1 S2i. How do we obtain this result without compromising data privacy? An unsophis- ticated solution is that those parties disclose their distances portions to each other, and they can then easily decide the k- nearest neighbors by comparing the distances. However, the unsophisticated solution will lead to private data disclosure.

This is because one party can make multiple queries, and if he gets the distance portions from each query, he can then identify the private data. Assume that the query instance contains 2 non-zero values, e.g., x q == 1.2,4.3, 0, ... ,0, and PI holds the first two attributes. Then, the query re- quester can learn the private values of PI with two queries.

Firstly, he uses x q to get a d value. He uses another x~ == 5.6,4.8,0, ... ,0 to get a value d'. He can solve the following two equations to get the first and second elements (denoted by Yl and Y2) of Xi that are supposed to be private:  d' == y'(Yl - 5.6)2 + (Y2 - 4.8)2.

The challenge is securely computing the k-nearest neigh- bors without the help of a trusted party. In the next section, we develop a secure protocol to tackle this challenge.

3.4 Secure Computing Protocol  In our secure protocols, we use homomorphic encryption [8] keys to encrypt the parties' private data. In particular, we utilize the following property of the homomorphic encryp- tion functions: e(al) x e(a2) == e(al + a2) where e is an encryption function; al and a2 are the data to be encrypted.

Because of the property of associativity, e(al +a2 +.. +an) can be computed as e(al) x e(a2) x ... x e(an) where e(ai) =1= O. That is,  e(al + a2 + ... + an) == e(al) X e(a2) X ... x e(an) (2)  Without loss of generality, we assume that Pi has a pri- vate distance portion of the ith training instance, Sil, where i E [1, N]' l E [1, n]. The problem is to decide whether 2:~1 Sil ~ 2:~=1 Sji for i,j E [1, N](i =1= j) and select k smallest values, without disclosing each distance portion.

We will provide a solution that uses homomorphic encryp- tion and random perturbation techniques. Before describing the protocol, we randomly select a key generator, e.g., Pn .

Protocol 2 (Secure Computing Protocol) Step I: Compute e(2:~l Sil) for i E [1, N].

1. Key and random number generation  (a) Pn generates a cryptographic key pair (d, e) of a semantically-secure homomorphic encryption scheme and publishes its public key e. Let e (.) denote encryption and d(.) denote decryption.

(b) Pi generates N random numbers ril, for all i E [1, N]' l E [1, n].

2. Forward transmission  (a) PI computes e(sil + ril), for i E [1, N], and sends them to P2?  (b) P2 computes e(sil +ril) x e(si2 +ri2) == e(sil + Si2 +ril +ri2), where i E [1, N], and sends them to P3 .

(c) Repeat (a) and (b) until Pn - l obtains e(sil + Si2 + ... + Si(n-l) + ril + ri2 + ... + ri(n-l))' for all i E [1, N].

(d) Pn computes e(Sin), i E [1, N], and sends them to Pn - l .

3. Backward transmission  (a) Pn- l computes e( -ri(n-l))' for i E [1, N] and sends them to Pn - 2?  (b) Pn- 2 computes e( -ri(n-l)) x e( -ri(n-2)) = e( -ri(n-l) - ri(n-2))' i E [1, N], and sends them to Pn - 3?    (c) Repeat (a) and (b) until PI obtains eil = e(-ril - ri2 - ... - ri(n-l)),for all i E [1, N].

(d) PI sendseil,fori E [l,N], toPn- l .

4. Computation ofe(~~=l Sil), for i E [1, N]  (a) Pn- l computes ei(n-l) == e(sil + Si2 + ... + Si(n-l) + Til + ri2 + ... + ri(n-l)) x e(Sin) = e(Sil + Si2 + ... + Si(n-l) + Sin + ril + ri2 + ... + ri(n-l))' i E [1, N].

(b) Pn- l computes ei(n-l) x eil == e(~~l sil),for i E [1, N] and l E [1, n].

Step 11.? Compute e(~~=l -Sjl) for j E [1, N].

1. Random number generation  (a) Pl generates N random numbers rjl' for all j E [1, N], l E [1, n].

2. Forward transmission  (a) PI computes e(-Sjl + rjl)' for j E [l,N], and sends them to P2.

(b) P2 computes e( -Sjl + rjl) x e( -Sj2 + rj2) == e( -Sjl - Sj2 + rjl + rj2)' where j E [1, N], and sends them to P3.

(c) Repeat (a) and (b) until Pn- l obtains e(-Sjl - S "2 - ... - S o( 1) + r 'o l + r 'o 2 + ... + r 'O ( ))J J n- J J J n-l ' for all j E [1, N].

(d) Pn computes e( -Sjn), j E [1, N], and sends them to Pn- l .

3. Backward transmission  (a) Pn- l computes e( -r;(n-l))' for j E [1, N] and sends them to Pn- 2.

(b) Pn- 2 computes e( -r;(n-l)) x e( -r;(n-2)) = e( -r;(n-l) - r;(n-2))' j E [1, N], and sends them to Pn- 3.

(c) Repeat (a) and (b) until PI obtains ejl = e(-rjl - rj2 - ... - r;(n-l))' for all j E [1, N].

(d) PI sends ejl, for j E [1, N], to Pn- l .

4. Computation ofe(~~l -sjl),for j E [1, N]  (a) Pn- l computes ej(n-l) == e( -Sjl - Sj2 - ... - Sj(n-l) + rjl + rj2 + ... + rj(n-l)) x e( -Sjn) =e(-Sjl - Sj2 - ... - Sj(n-l) - Sjn + rjl + rj2 + ... + rj(n-l))' j E [1, N].

(b) Pn- l computes ej(n-l) x ejl == e(~~=l -Sjz), for j E [1, N] and l E [1, n].

Step III: Compute the k nearest neighbors   ~n ~n ~n ~nl SIZ l S2l l S3l ... l SNl ~n +1 +1 -1 ... -1l Sll ~~ S2l -1 +1 -1 ... -1 ~~ S3l +1 +1 +1 ... +1 .. . .. . .. . .. . .. . ...

~~SNl +1 +1 -1 ... +1  Table 1. Index table of distance measures of N instances  1. Pn- l computes ei(n-l) x ej(n-l) == e(~~l Sil - ~~=l Sjl), for i,j E [1, N], and collects the results into a sequence <p that contains N (N - 1) elements.

2. Pn- l randomly permutes this sequence and obtains the permuted sequence denoted by <pI, and then sends <P' to Pn .

3. Pn decrypts each element in sequence <pI, then assigns the element +1 if the result of decryption is not less than 0, and -1, otherwise, andfinally obtains a +1/ - 1 sequence denoted by <P".

4. Pn sends <P" to Pn- l who computes k smallest ele- ments. (Details are given in Section 3.5.) They are the k nearest neighbors for a given query instance x q? Pn- l then decides the class label for X q .

3.5 How To Compute the Smallest k Ele- ments  Pn-l is able to remove permutation effects from <P" (the resulting sequence is denoted by <P"') since Pn-l has the permutation function that Pn - l used to permute <P, so that the elements in <p and <p'" have the same order. It means that if the qth position in sequence <p denotes e(~~=l Sil - ~~=l Sjl), then the qth position in sequence <p'" denotes the decrypted result of ~~=l Sil - ~~=l Sjl. We encode it as +1 if ~~=l Sil 2 ~~=l Sjl, and as -1 otherwise. Pn- l has two sequences: one is <P, the sequence of e(~~=l Sil - ~~l Sjl), for i, j E [1, N] (i i= j), and the other is <p"', the sequence of +1/ -1. The two sequences have the same number of elements. Pn- l knows whether or not ~~l Sil is larger than ~~l Sjl by checking the corresponding value in the <p'" sequence. For example, the first element <p'" is  -1, Pn- l concludes ~~=l Sil < ~~=l Sjl. Pn- l examines the two sequences and constructs the index table (Table 1) to compute the k-nearest neighbors.

In Table 1, +1 in entry ij indicates that the distance mea- sure of the row (e.g., ~~ Sil of the ith row) is not less than the distance measure of a column (e.g., ~~ Sjl of the jth column); -1, otherwise. Pn - l sums the index values of each row and uses this number as the weight of the distance mea-    8 1 8 2 8 3 84 Weight 8 1 +1 -1 -1 -1 -2 8 2 +1 +1 -1 +1 +2 8 3 +1 +1 +1 +1 +4 8 4 +1 -1 -1 +1 0  Table 2. An Example  sure in that row. Pn-1 then selects the instances that corre- spond to the k smallest weights as the k-nearest neighbors.

Assume that: (1) there are 4 elements denoted by  8 1 == L~=l Sll, 8 2 == L~=l S2l, 8 3 == L~=l S3l, and 8 4 == L~=l S4l? (2) 8 1 < 8 4 < 8 2 < 8 3 ; (3) the se- quence <1> is [e(81 - 8 2), e(81 - 8 3), e(81 - 8 4 ), e(82 - 83 ), e(82 - 8 4 ), e(83 - 8 4 )]. The sequence <1>'" will be [-1, -1, -1, -1, +1, +1]. According to <1> and <1>"', Pn - 1 builds Table 2. From the table, Pn - 1 knows 8 1 < 8 4 < 8 2 < 8 3 according to their weights, e.g., 8 1 is the smallest element since its weight, which is -2, is the smallest. Thus, Pn-1 knows the k nearest neighbors for a given query in- stance x q .

4 Granular Computing Paradigm in Privacy-Preserving Data Mining  In the above privacy-preserving data mining examples, we presented two different approaches. For the first exam- ple, which is privacy-preserving association rule mining, we identify the key computation step, which is how to com- pute c.count, then design a privacy-preserving protocol to securely compute c.count. To achieve privacy-preserving data mining, we need to reduce the whole algorithm to a set of component privacy-oriented protocols. We say the privacy-preserving data mining algorithm preserves privacy if each component protocol preserves privacy and the com- bination of the component protocols does not disclose pri- vate data. In secure multiparty computation literature, a composition theorem [5] describes a similar idea.

Theorem 1 Suppose that 9 is privately reducible to j and that there exists a protocol for privately computing j, then there exists a protocol for privately computing g.

We then combine the individual protocols and the above composition theorem to provide a privacy guarantee for the whole procedure of privacy-preserving association rule mining.

Similarly, we can combine the component protocols with the composition theorem to solve the second example that is privacy-preserving k-nearest neighbor classification.

To date, most research proves the security of protocols based on the simulation paradigm: a protocol is secure if   there exists a machine simulating an adversary's view in the ideal model that is indistinguishable from the adversary's view in the real model [6]. Furthermore, Canetti proposed a well accepted design methodology for secure protocols [4]:  1. Design a "high-level" protocol for the given function- ality by utilizing oracle calls to securely compute prim- itive functionalities.

2. Design primitive protocols implementing the primitive functionalities.

3. Construct a composite protocol by replacing the ora- cle calls in the high-level protocol with the primitive protocols.

The composite protocol is a provably secure protocol im- plementing the given functionality after the primitive proto- cols and the high-level protocol are proven to be secure in the real model and the hybrid model, respectively [6]. The methodology is so elegant that we can design a large-scale protocol in a recursive manner. When primitive protocols are proven to be secure, as the boundary condition in re- cursion, the security proof of the high-level protocol results in a secure composite protocol, which can serve as another secure primitive protocol to construct "higher-level" proto- cols.

Shen et al. [9] has designed a composition model that can be used for privacy-preserving data mining systems. Their model is actually the composite protocol constructed from Canetti's methodology under the condition that no commu- nication is allowed in the high-level protocol. The proposed composition model applied the idea of granular computing to privacy-preserving computing systems.

5 Conclusion  Granular computing is an emerging computing paradigm of information processing. It is an approach to looking at data that recognizes different and interesting regularities in the data appearing at different levels of granularity. In prac- tice, the paradigm of granular computing is widely utilized.

In this paper, we have discussed the application of gran- ular computing in privacy-preserving data mining systems.

We have used two examples of privacy-preserving data min- ing to illustrate how the granular computing paradigm has been applied in privacy-preserving data mining. In the fu- ture, one of key goals is to take advantage of the granular computing paradigm in designing more effective privacy- preserving data mining and machine learning systems.

