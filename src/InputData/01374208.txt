A Hybrid Approach for Mining Maximal Hyperclique Patterns

Abstract  A hyperclique pattern [12] is a new type of association pattern that contains items which are highly affiliated with each other. More specifically, the presence of an item in one transaction strongly implies the presence of every other item that belongs to the same hyperclique pattern. In this paper, we present a new algorithm for mining maximal hy- perclique patterns, which are desirable for pattern-based clustering methods [11]. This algorithm exploits key ad- vantages of both the Depth First Search (DFS) strategy and the Breadth First Search (BFS) strategy. Indeed, we adapt the equivalence pruning method, one of the most efficient pruning methods of the DFS strategy, into the process of the BFS strategy. As demonstrated by our experimental results, the performance of our algorithm can be orders of magni- tude faster than standard maximal frequent pattern mining algorithms, particularly at low levels of support.

Keywords Data Mining, H-confidence, Hyperclique Pattern  1. Introduction  The association-rule mining problem [3] is concerned with finding relationships among the items in a large- scale data set. In the past decade, association-rule mining has been the subject of extensive research in data mining [1, 2, 3, 4, 8]. Given a set of transactions, the objective of association-rule mining is to extract all rules of the form? ? ?  , where ?  and ?  are sets of items, that satisfy user-specified minimum support and minimum confidence thresholds. Support measures the fraction of transactions that obey the rule, while confidence provides an estimate of the conditional probability that a transaction contains  ? ,  given that it contains ?  . Both metrics are useful because they provide an indication of the strength and statistical sig- nificance of an association rule.

Standard association-rule mining algorithms have the  1This work was partially supported by NSF grant # ACI-0305567. The content of this work does not necessarily reflect the position or policy of the government and no official endorsement should be inferred.

emphasis on discovering frequent patterns. However, these approaches may lose efficiency when the support threshold is low. Also, frequent patterns usually contain objects which are weakly related to each other [12]. Instead, a hyperclique pattern [12] was proposed as a new type of association pat- tern that contains items that are highly affiliated with each other. Specifically, the presence of an item in one transac- tion strongly implies the presence of every other item that belongs to the same hyperclique pattern. The h-confidence measure captures the strength of this association and is de- fined as the minimum confidence of all association rules of an itemset. An itemset is a hyperclique pattern if the h- confidence of this pattern is greater than a user-specified minimum h-confidence threshold. A hyperclique pattern is a maximal hyperclique pattern if no superset of this pat- tern is a hyperclique pattern.

Maximal hyperclique patterns are desirable for pattern preserving clustering [11], which can produce easily inter- pretable clustering results. However, to our best knowledge, there is no efficient algorithm for mining maximal hyper- clique patterns in the literature. As a result, the objective of this paper is to design an efficient algorithm for mining maximal hyperclique patterns in large-scale data sets.

In general, for association pattern mining, there are two search strategies: Breadth First Search (BFS) and Depth First Search (DFS). The BFS strategy performs pattern search in a level-wise manner. In other words, it first discov- ers all the size-1 patterns at level 1, followed by all the size- 2 patterns at level 2, and so on, until no pattern is generated at a particular level. If mining maximal hyperclique pat- terns using the BFS strategy, we could apply ? ? ? ? ? ?  ?  ? ? ? ? ? ? ? ; that is, an itemset can be pruned if one of its subsets is not a hyperclique pattern. This pruning is based on the anti-monotone property of support and h-confidence measures. The drawback of this strategy is that we need to generate all the subsets of a maximal hyperclique pattern.

In contrast, the DFS strategy avoids generating all the inter- mediate patterns and can directly find maximal hyperclique patterns. For the DFS strategy, a lot of pruning methods, such as equivalence pruning, leftmost pruning, full pruning, and dynamic ordering [4, 6, 9], can be applied. However,      the DFS strategy cannot apply the ? ? ? ? ? ? ? ? ? ? ? ? ? ? 	 ?  method, since we do not generate all subsets of a candidate pattern for this strategy.

In this paper, we exploit key advantages of both the DFS strategy and the BFS strategy and design a hybrid Maximal Hyperclique Pattern (MHP) mining algorithm. Figure 1 il- lustrates our MHP algorithm, which has two phases. In the first BFS phase, for a given depth L, we use the Apriori- like approach [2] to generate all the size-L hyperclique pat- terns. In the second phase, the MHP algorithm takes the DFS search strategy. All the DFS pruning methods will be used in this phase. Also, since we have all the size-L hy- perclique patterns, an itemset can be pruned by prevalence pruning method if any of its size-L subset has not been gen- erated. Considering the DFS strategy is much more efficient than the BFS strategy for finding maximal patterns and the major computation savings of the DFS strategy is due to the equivalent pruning method [1, 6, 9], we adapt the equiva- lent pruning method to our algorithm. In addition, we prove the correctness and completeness of our MHP algorithm.

Finally, our experimental results show that the MHP algo- rithm can be orders of magnitude faster than maximal fre- quent pattern mining algorithms, such as MAFIA [6], par- ticularly at low level of support.

NULL  1 2 1st Phase: BFS 3 4  1, 4  1, 2, 3, 4  1, 2, 3 1, 2, 4 1, 3, 4 2, 3, 4  3, 42, 42, 31, 2 1, 3 2nd Phase: DFS  Figure 1. An Illustration of the Hybrid Mining.

Overview: The remainder of this paper is organized as follows. Section 2 defines some basic concepts. In section 3, we propose a framework for mining maximal hyperclique patterns. We describe the algorithm details and prove the correctness and completeness of the algorithm in Section 4.

Our experimental results are presented in Section 5. Finally, in section 6, we draw conclusions and suggest future work.

2. Basic Concepts  To facilitate our discussion, we first present some basic concepts in this section.

Definition 1 The h-confidence of an itemset ? ? ? 	  ? 	 ? ? ? ? ? ? 	 ? ? , denoted as ? ? ? ? ? ? ? ? , is a measure that re-  flects the overall affinity among items within the item- set. This measure is defined as ? 	 ? ? ? ? ? ? ? 	  ?  ? ? ? ? ? ? 	 ? ? ? ? ? ? ? ? 	 ? ? 	  ? 	 ? ? ? ? ? ? 	 ? ? ? ? ? ? ? ? ? ? ? ? 	 ? ? ? ? ? ? ? 	 ? ?  ? ? , where ? ? ? ? is the classic definition of as- sociation rule confidence [2].

Definition 2 An itemset ? is a hyperclique pattern if ? ? ? ? ? ? ? ? ? ?  ! and ? ? ? ? ? ? ? ?  " # , where ! is a user-specified minimal support threshold and " # is a user- specified minimal h-confidence threshold.

Definition 3 For a hyperclique pattern, HP, if none of its supersets is a hyperclique pattern, we say HP is a Maximal Hyperclique Pattern (MHP). This means, a pattern P $ MHP % & P $ HP and ' P? ( P, P? )$ MHP.

Definition 4 The order of items: for two different item and 	 ? , if ? ? ? ? ? ? ? ? 	  ? * ? ? ? ? ? ? ? ? 	 ? ? and the name of 	  is preceding of the name of 	 ? in the lexicographic order, we say 	  is lexicographic before 	 ? . We write it as 	  + 	 ? .

In the rest of this paper, we arrange the items in an item- set in order, except for special mention.

Definition 5 The order of patterns: for two different pat- terns ?  ? ? 	  ? 	 ? ? ? ? ? 	 , ? and ? ? ? ? 	 -  ? 	 - ? ? ? ? ? ? 	 - . ? , if ( ?  /  ? ? ) 0 ( 1 m, ? + 2 and ? + ? , ' ? ? 3 * ? * ? 4 3 ? 	 5 ? -5 ? ? 6 	 ? + 	 -? ? , we say ?  lexicographic before ? ? . It could be also written as ?  + ? ? .

3. A Framework for Mining Maximal Hyper- clique Patterns  In this section, we present a two-phase maximal hyper- clique pattern mining framework. In the first BFS phase, we retrieve all the size-L hyperclique patterns. In other words, the first L levels of the ? ? 7 	 ? ? ? ? ? ? 	 ? ? ? ? ? [10] will be searched using Apriori-like methods [2]. In the second phase, we apply the DFS strategy to extract all the Maximal Hyperclique Patterns (MHP).

3.1. Basic Definition  For a pattern, there are three concepts related to items of this pattern: the item set, the equivalence item set, and the tail item set. We first introduce these three concepts.

Definition 6 The Item Set ( ? .item) of a pattern ? is the set of all the items in the pattern.

Definition 7 The Tail Item Set ( ? .tail) of a pattern is the set of items which can be used to generate the super pattern of this pattern in the DFS phase.

In the DFS phase, we retrieve all the patterns by gener- ating the super patterns of a given pattern( ? ) with its tail items [10, 1]. All tail items are included in ? .tail.

Definition 8 For a pattern ? , if an item appears in all the transactions that contain ? .item, but not in ? .item, we say that this item is an equivalence item with ? .

Table 1 shows a sample data set and Table 2 shows the support of items in the sample data set. Also, for this sample data set, Figure 2 illustrates the two-phase maximal hyper- clique pattern mining process. In the figure, a node rep- resents a pattern, ? . As can be seen, in the upper layer, numbers represent the items in ? .item and numbers in the braces are the items in ? .equivalence. In addition, num- bers following the short line represents the items in ? .tail.

There are two rows in the lower part of the figure: the first one shows the support of patterns and the second row shows the corresponding h-confidence of patterns.

Table 1. A Sample Data Set  TID Items 1 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 2 3, 4, 7, 8, 9, 11 3 3, 4, 5, 6, 7, 8, 9, 11 4 1, 2, 3, 4, 5, 6, 7, 8, 9, 11 5 1, 3, 4, 7, 8, 9 6 2, 3, 4, 7, 8 7 3, 4, 7, 9 8 3, 4, 8, 9 9 3, 7, 8, 9  10 4, 7, 8, 9  Lemma 1 If an item is an equivalence item of a pattern, it should also be an equivalence item of its super patterns.

Table 2. Support of Items in the Sample Data Set  Item TID Support 1 1, 4, 5 0.3 2 1, 4, 6 0.3 3 1, 2, 3, 4, 5, 6, 7, 8, 9 0.9 4 1, 2, 3, 4, 5, 6, 7, 8, 10 0.9 5 1, 3, 4 0.3 6 1, 3, 4 0.3 7 1, 2, 3, 4, 5, 6, 7, 9, 10 0.9 8 1, 2, 3, 4, 5, 6, 8, 9, 10 0.9 9 1, 2, 3, 4, 5, 7, 8, 9, 10 0.9 10 1 0.1 11 1, 2, 3, 4 0.4? ? ? ? ? ? ? ? ? ? ? ? ? ?  ? ? = 0.15  ? ? ? ? ? ? ? ? 	 ? ? 	 ? ? ? ? ? ? ?  ? ? ? = 0.55  If an item ? is an equivalence item with a pattern ? and? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  ? ! ? ? , a minimum h-confidence threshold, then  the union of " ? # and ? .item cannot be a hyperclique pattern.

Definition 9 If an item is an equivalence item of a pattern ? , and the union of this item and ? .item is also a hyper- clique pattern, we say this item is a Pure Equivalence Item, PE item, of the pattern ? .

If we generate the closed frequent itemset or max- imal frequent Itemset with the DFS approach, the$ % ? ? & ' ? 	 ? ? 	 ? ? ? ? ? ? ( method could move the PE item from ? .tail to the ? .item directly [6, 9]. However, the direct use of this method may break the limitation of h-confidence when we generate super patterns. As shown in the sample data set, item ) ) is a PE item of " 5 # , but not an PE item for " 1, 5 # . In other words, item ) ) cannot be added into  " 5 # , since we do not know whether the super patterns of " 5 # have item ) ) as PE item or not. In addition, we apply the equivalence pruning method in the BFS phase. Consider that adding items may change the size of patterns, we need to maintain a set to store a pattern?s PE items.

(0.2,0.67) 1,2  (0.2,0.67) 1,2(5,6)  (0.7,0.78) 3,4,7,8,9  Tail Set3,4(8,9)?6 (0.6,0.67)  Equivalent Set  Item Set  H?confidence Support  1,5(6) (0.2,0.67)  2,5(6) (0.2,0.67)(0.2,0.67)  1,6 (0.2,0.67)  2,6 (0.8,0.89)  3,4 (0.8,0.89)  3,7 (0.8,0.89)  3,8 (0.8,0.89)  3,9 (0.8,0.89)  4,7 (0.8,0.89)  4,8 (0.8,0.89)  4,9 (0.8,0.89)  7,8 (0.8,0.89)  7,9 (0.8,0.89)  8,9  (0.7,0.78) 3,4,7  (0.7,0.78) 3,4,8  (0.7,0.78) 3,4,9  (0.7,0.78) 3,7,8  (0.7,0.78) 3,7,9  (0.7,0.78) 3,8,9  (0.7,0.78) 4,7,8  (0.7,0.78) 4,7,9  (0.7,0.78) 4,8,9  (0.7,0.78) 7,8,9  (0.7,0.78) 3,7,8?9  (0.7,0.78) 4,7,8?9  (0.7,0.78) 3,4,8?9  (0.7,0.78) 3,4,7?8,9  (0.6,0.67) 3,4,7,8?9  (0.9,1)  (0.9,1)  (0.9,1)  (0.9,1)  (0.9,1)  (0.4,1)(0.3,1)  (0.3,1)  (0.3,1)  (0.1,1) (0.3,1)  (0.3,0.75) 6(11)5(6,11)  (0.3,0.75)(0.2,0.5) 2,11  (0.2,0.5) 1,11  6 is transferred  B FS Phase  D FS Phase  5 is absorbed  Absorb  Null  Figure 2. An Illustration of the Two-Phase Maxi- mal Hyperclique Pattern Mining Process.

Definition 10 The Equivalence Item Set of a pattern, ? .equivalence, is the item set for storing ? ?s PE items.

If we find an item is a PE item of a pattern ? , we could add it to ? .equivalence. While the super patterns of ? are generated, they will succeed their own PE items from  ? .equivalence. In this case, the items of ? are separated in ? .item and ? .equivalence, and the target pattern of ? should be ? .item * ? .equivalence.

Definition 11 Union of a pattern, ? .union, is ? + ? ? 	 , * ? + 	 % ? ? & ' ? 	 ? ? 	 . ? .union is the target itemset of ? . Indeed, support( ? .union)=support( ? .item).

Definition 12 Size of a Pattern(P): we define the size of P on the size of P.item, no matter how many items in P.equivalence.

Definition 13 Sub pattern: For two pattern ? ? ? ? ? , if ? ? .item is a subset of ? ? .item, we say that ? ? is a sub pat- tern of ? ? , even ? ? .union is not a subset of ? ? .union, and  ? ? is a super pattern of ? ? . If the size of ? ? is smaller than the size of ? ? , ? ? is a pure sub pattern of ? ? .

Definition 14 For a Pattern ? ? , if i? is ? ? ?s equivalence item and all the items in ? ? ? .item are lexicographic before item ? ? , we say item ? ? is a Pro equivalence item of ? ? .

Definition 15 For a Hyperclique Pattern ? ? ? , if item ? ? is both ? ? ? ?s PE item and Pro equivalence item, the item ? ? is a Pro Pure equivalence item (PPE item) of ? ? ? .

3.2. Pruning Methods in the BFS phase  At the initial stage, the algorithm generates the size- 1 patterns and counts the support of these patterns.

All the items which have support less than the user- specified support threshold are pruned. Meanwhile, these items are sorted during this stage. For instance, con- sider the example dataset shown in Table 1, item ? ? can be pruned since ? ? ?  ? ? ? ? ? ? ? . Also, as shown in Figure 2, the algorithm constructs the size-1 hyperclique patterns and sort all items in lexicographic order: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? .

In the BFS phase, the algorithm applies an apriori-like approach to generate the size-L hyperclique patterns from size-(L-1) hyperclique patterns. There are three pruning strategies applied in this phase as the following.

Prevalence Pruning. In the apriori-like algorithm, a size-k pattern ? ? ( ? ? ? ? ? ?  ! ? ? ? ? ? ? ? ? ? ? ? ? ? ? ) is generated by joining two size-(k-1) patterns: ? ? " ? and  ? ?? " ? , ? ? " ? ? ? ? ?  ! ? ? ? ? ? ? ? ? ? ? ? ? ? " ? ? and ? ?? " ? ? ? ? ?  !

? ? ? ? ? ? ? ? ? ? ? ? ? " ? ? ? ? ? . If ? ? " ? and ? ?? " ? exist, the algorithm first checks whether all the other size-(k-1) sub patterns of  ? ? exist. If one of the sub patterns does not exist, ? ? is not a hyperclique pattern and can be pruned. This is a standard pruning method [1].

H-confidence Pruning. Before generating a size-k pat- tern ? ? , we could calculate the ratio:  # $ % % & ' ( ) * + , - . / # $ % % & ' ( ) 0 , / . If  this ratio is less than 1 2 , hconf( ? ? ) should be also less than ? 2 , since support( ? ? ) ? support( ? ? " ? ) [12]. For instance, as shown in Figure 2, support(1)=0.3, support(3)=0.9, hconf( ? 1,3 ? )= # $ % % & ' ( ) 3 ? 4 5 6 /# $ % % & ' ( ) 5 / ?  # $ % % & ' ( ) 3 ? 6 /# $ % % & ' ( ) 5 / 7 ? ? ? ? 7 ? 2 , therefore the pattern ? 1,3 ? is pruned.

Equivalence Pruning. We apply the 8 9 ? : ; < ? = > ?  ? = ? = ? method to reduce the number of patterns generated. If support( ? ? ? )=support( ? ? ? " ? ), ? ? should be a PPE item of ? ? ? " ? , and be absorbed  into ? ? ? " ? .equivalence. In Figure 2, support( ? 5, 6 ? )=support( ? 5 ? )=0.3, we add item 6 to ? 5 ? ?s equivalence set and prune ? 5, 6 ? .

When generating a size-k hyperclique pattern ? ? ? , if the items in its size-(k-1) sub hyperclique patterns? equiva- lence sets are PE items of ? ? ? , ? ? ? can succeed these items to its own equevalence set. For instance, in Fig- ure 2, both ? 1, 5 ? and ? 2, 5 ? succeed item 6 from  ? 5 ? .equivalence, but do not succeed item ? ? since it would break the limitatin of h-confidence.

When ? ? ? " ? absorbing item ? ? , all the equiv- alence items of the other size-(k-1) patterns-  ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? " ? ? ? ? ? , are also equivalence items of ? ? ? " ? . ? ? ? " ? could tranfer these items to ? ? ? " ? .equivelance if they are PE items. In Figure 2, while generating the pattern ? 1, 2, 5 ? from ? 1, 2 ? , ? 1, 5 ? and ? 2, 5 ? , the pattern ? 1, 5 ? will absorb item ? , and transfer item ? ? from the pattern ? 1, 5 ? .

Indeed, when generating ? ? ? , if item ? ? is in ? ? ? " ? .equivalence, it is unnecessary to generate the ? ? ? , but transfer the PE items in the other size-(k-1) patterns? equivalence set to ? ? ? " ? .equvialence.

After generating the size-k hyperclique patterns, we could check all the size-(k-1) hyperclique patterns in lexico- graphic order. For a size-(k-1) pattern ? ? ? " ? , if its union is not a subset of any size-k pattern?s union, it will be im- possible to generate a hyperclique pattern whose union is the superset of ? ? ? " ? .union in the following process. If this union is not a subset of an itemset in current Maxi- mal Hyperclique Pattern Set (MHPS) either, this union is a maximal hyperclique pattern and could be added to the MHPS. For example, in Figure 2, after generating the size-2 patterns, the union of ? 1, 2 ? is ? 1, 2, 5, 6 ? , and no superset in either size-3 patterns? union or MHPS. Hence, the algo- rithm adds the union into MHPS. For pattern ? 1, 5 ? , the union of this pattern is ? 1, 5, 6 ? , and this pattern has no su- perset in size-3 patterns? union, but has a superset in MHPS, hence this pattern is pruned.

3.3. Pruning Methods in the DFS phase  In the BFS phase, the algorithm has identified all the size-L hyperclique patterns. At the beginning of the DFS phase, the algorithm adds the tail items to these pat- terns? tail sets. For a size-L hyperclique pattern ? ? ,  ? ? .item= ? ? ? ? ? ? ? ? ? ? ? ? @ ? , if there is an item ? ? such that: (1) item ? ? A B ? ? .equivalence, (2) all the items in ? ? .item are lexicographic before i?, and (3) all the size-L sub patterns of  ? ? ? ? ? ? ? ? ? ? ? ? @ ? ? ? ? have been generated, the algorithm adds item ? ? to ? ? .tail. For instance, in Figure 2, item 8 and 9 are added to ? 3, 4, 7 ? ?s tail set.

The super patterns of a hyperclique pattern( ? ? ) are generated with the item in ? ? .tail, and succeed the PE item from ? ? .equivalence.

Equivalence Pruning. Similar to the BFS phase, if a tail item ? ? is a PE item, we will add ? ? the pattern?s equivalence set. If the size-1 pattern ? ? ? ? ?s equivalence set is not null, the super patterns will succeed PE items from this set.

Full Pruning. When we process the Pattern ? ? , if the union of ? ? .item, ? ? .equivalence and ? ? .tail is a subset of a pattern in current MHPS, all of the patterns generated by ? ? cannot be MHP since they have a super Hyperclique Pattern. We could prune this pattern directly. In Figure 2, when we process ? 3, 7, 8 ? , which tail set is ? 9 ? , ? 3, 4, 7, 8, 9 ? has already been added to MHPS. We will find ? ? ? ? ? 	 ?   ? ? ? is a subset of ? 3, 4, 7, 8, 9 ? , and prune ? 3, 7, 8 ? .

LeftMost Pruning. When processing a hyperclique pat-  tern ? ? , if the pattern in the end of this path is found to be MHP, all the patterns in the other paths should not be MHP.

In this case, we could skip these patterns[6]. In Figure 2, the end of left most path of ? 3, 4, 7 ? is ? 3, 4, 7, 8, 9 ? , and we find this pattern is MHPS, we can skip all the other paths of ? 3, 4, 7 ? , and continue to process the next pattern.

Dynamic Reordering. Bayardo showed that the benefit of dynamically reordering super patterns of ? ? is signifi- cant [5]. The mining speed will be 2 to 4 times faster. We sort the super patterns in the increasing order of support.

H-confidence Pruning. Similar to the BFS phase, for a tail item ( ? ? ) of a hyperclique pattern( ? ? ), if ?  ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ? ?  , we could prune ? ? from ? ? .tail.

Prevalence Pruning. Since we have generated the size- L hyperclique patterns in the BFS phase, for a hyperclique pattern ? ? , if one size-L sub pattern of ? ? is not gener- ated, we need not generate it.

In the DFS phase, if a hyperclique pattern cannot gen- erate any super hyperclique pattern, or none of these super hyperclique patterns could succeed all the items in its equiv- alence set, it will be impossible to find a super union of this pattern?s union in the future. We will check this union with MHPS. If there is no super pattern in MHPS, we will add the union to MHPS.

4. The MHP Algorithm  Figure 3 shows an overview of the hybrid MHP algo- rithm for mining maximal hyperclique patterns. As can be seen, there are two phases: the BFS phase and the DFS phase in the algorithm.

4.1. Algorithm Description  In the first BFS phase, Initial Function generates the size-1 frequent patterns, which are also size-1 hyper- clique patterns, and items are sorted in order. In Gener- ate and Prune Super Function, the prevalence pruning, h- confidence pruning, and equivalence pruning are applied to prune the search space and size-k hyperclique patterns  are generated from size-(k-1) hyperclique patterns. Af- ter extracting the size-k patterns, the algorithm extracts all size-(k-1) hyperclique patterns which have no super union in size-k hyperclique patterns to ? ? ? ? ?  ! . In Check and Add Function, the algorithm checks the patterns in ? ? ? ? ?  ! , if their unions are not subsets in MHPS, these unions are added into MHPS.

In the second phase, the Append Tail Function adds the size-L patterns? tail item. Extract MHP is the ma- jor function for DFS mining. The traditinal optimal methods, full pruning, leftmost pruning, and equivalence pruning, and new methods, prevalence pruning and h- confidence pruning, are implemented in Function Gener- ate and Prune Super. The Sort and Append Tail Function implements the dynamic sorting and add tail items for the super patterns. Finally, the algorithm checks whether the pattern being processed is in MHPS or not by the function Check and Add.

4.2. Completeness and Correctness  Here, we prove the completeness and correctness of our MHP algorithm. To facilitate our discussion, we first intro- duce some lemma and a new concept, Covering Pattern.

Lemma 2 If a hyperclique pattern ? ? ! is generated in the BFS phase, none of the item in ? ? ! .itemset could be a PPE item of any sub pattern of ? ? ! .

Proof: This lemma proof as well as some following lemma proofs are presented in our Technical Report [7]  Lemma 3 When a hyperclique pattern ? ? ! is generated in the BFS phase and the size of ? ? ! ? " , if # an item i?, (1) all the items in the ? ? ! .item are lexicographic before i?, (2)i? is not an equivalence item of ? ? ! , and (3) ? ? ! .item   ? i? ? is also a hyperclique pattern, ? ? ! .item ? i? ? will be generated by our algorithm.

Lemma 4 If a hyperclique pattern ? ? ! is generated in the BFS phase, all of its PPE items would be added to the  ? ? ! .equivalence by the algorithm.

Lemma 5 For an equivalence item which is transferred by a hyperclique pattern ? ? , it could also be added to equiv- alence set with absorbing or succeeding if we do not use transferring method.

Lemma 6 For a hyperclique pattern ? ? , all items in ? ? .equivalence are PPE items of some sub pattern of this pattern ? ? .

Definition 16 For a hyperclique pattern ? ! , if (1) ? $ is a hyperclique pattern, (2) ? $ .item is a subset of  ? ! .item, and (3) the union of ? $ is a super set of ? ! .item, ? $ is a Covering Pattern of ? ! . Obviously, support( ? $ .item)=support( ? ! .item)=support( ? $ .union).

MHS ALGORITHM Input: (a) ? = ? A Pattern ?  (b) ? ? ? ? = ? A DataSet represent a set of transaction ? (c) ? : A minimal support threshold (d) ? ? : A minimal h-confidence threshold (e) 	 : The retrieve level of the BFS phase  Output: (1) A set of Maximal Hyperclique Patterns(MHPS) with support ? , hconf ? ? , and its superset without both such two properties.

Variables: k: the itemset size ? ? ? : a set of size- ? hyperclique patterns. ? ? ? ? : a set of size- ? candidate maximal hyperclique patterns.? ? ? ? : set of maximal hyperclique patterns.

? ? ? ? ? ? : a set of superset generate from ?  Phase I: generate Hyperclique Patterns by BFS 1. ? ? ? = Initial(  ? ? , ? , ? ? , ? ? ? ? ); 2. for (k=1; ? ? 	 ;k++) do 3.

? ? ? ? = Generate and Prune Super( ? ? ? , ? , ? ? ); 4.

? ? ? ? = set of patterns in ? ? ? without superset union in ? ? ? ? ? ; 5. Check and Add(  ? ? ? ? , ? ? ? ? ); Phase II: extract Maximal Hyperclique Patterns from ? ? ? by DFS 6. Append Tail( ? ? ? ); 7. for ? ? ? ? ? ? ? 8. Extract MHP( ? );  Function Extract MHP(Pattern ? ) 9. ? ? ? ? ? ? =Generate and Prune Super( ? , ? , ? ? , ? ? ? ); 10. Sort and Append Tail( ? ? ? ? ? ? ) 11. for ? item ? ? ? ? ? ? ? ? ? 12. Extract MHP( ? ? ); 13. if ? .union haven?t a super union in ? ? ? ? ? ? 14. Check and Add( ? ,MHPS);  Figure 3. Overview of the MHP Algorithm  Lemma 7 If a pattern is a hyperclique pattern, one of its Covering Pattern must be generated.

Lemma 8 The MHP algorithm is complete. In other words, all the Maximal Hyperclique Patterns will be identified by the MHP algorithm.

Lemma 9 The MHP algorithm is correct. In other words, any patterns identified by the MHP algorithm are maximal hyperclique patterns.

Note that if we set the search depth in the BFS phase large enough, our algorithm becomes a pure BFS algorithm.

This means equivalence pruning will work correctly in an apriori-like algorithm for mining maximal hyperclique pat- tern. Additionally, if we set the h-confidence threshold to zero, the algorithm finds maximal frequent itemsets.

5. Experimental Evaluation  In this section, we present extensive experiments to eval- uate the performance of the MHP algorithm. Specifically,  we demonstrate: (1) a computation performance compari- son between the MHP algorithm and standard maximal fre- quent pattern mining algorithms and (2) the effect of the MHP algorithm on finding maximal hyperclique patterns.

5.1. The Experimental Setup  Our experiments were performed on two real-world date sets ? ?  ! " and ? ?  ! " # , which are benchmark dense data sets for evaluating pattern mining algorithms.

These two data sets are obtained from IBM Almaden at http://www.almaden.ibm.com/cs/quest/demos.html. Re- cently, the MAFIA algorithm [6] was proposed to efficiently discover maximal frequent patterns. As shown in their pa- per, MAFIA can be several orders better than some alter- natives, such as DepthProject, for mining maximal frequent patterns. Hence, we chose MAFIA as the base line for our performance evaluation. Finally, please note that only the size-2 patterns are generated in the first BFS phase.

Experimental Platform We implemented the MHP al- gorithms using C++ and all experiments were performed      on a Pentium III ? ? ? MHz PC machine with ? ? ? megabytes main memory, running Linux Redhat 6.1 operating system.

5.2. A Performance Comparison      1e+06  1e+07  1e+08  1e+09  1e+10  1e+11  0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06  N um  be r o  f C he  ck ed  P at  te rn  s  Support threshold  Mafia Min_Conf=0  Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 4. The Number of Checked Patterns on the Pumsb* Data Set       0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06  R un  T im  e (s  ec )  Support Threshold  Mafia Min_Conf=0  Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 5. The RunTime Comparison on the Pumsb* Data Set  Figure 4 shows the number of patterns that MHP and MAFIA have to check during the pattern mining process on the pumsb* data set. As can be seen, for MHP, the num- ber of checked patterns is increased with the decrease of the h-confidence threshold. However, the number of checked patterns of MHP can be significantly smaller than that of MAFIA even if a low h-confidence threshold is specified.

To check a pattern, we need to count the support of the pat- terns. Counting the support of a pattern is the most time- consuming task during the pattern mining process, since we need to retrieve all the transactions which include one of its sub-pattern, or for Mafia, retrieve all the bit of the bitmap of this pattern [6]. Therefore, an algorithm is more efficient if smaller number of patterns need to be checked.

1e+06  1e+07  1e+08  1e+09  0.1 0.2 0.3 0.4 0.5 0.6 0.7  N um  be r o  f C he  ck ed  P at  te rn  s  Support Threshold  Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 6. The Number of Checked Patterns on the Pumsb Data Set      0.1 0.2 0.3 0.4 0.5 0.6 0.7  R un  T im  e (s  ec )  Support Threshold  Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 7. The RunTime Comparison on the Pumsb Data Set  The runtime comparison of MHP and MAFIA on the Pumsb* data set is shown in Figure 5. In the figure, we can observe that the runtime of MHP can be significantly reduced with the increase of h-confidence thresholds. Also, the runtime of MHP can be several orders of magnitude less than that of MAFIA even if the h-confidence threshold is as low as 0.3. The reason is that the number of checked pat- terns of MHP is signficantly smaller than that of MAFIA.

Similar results are also obtained from the pumsb data set, as shown in Figure 6 and Figure 7. For the pumsb data set, the number of checked patterns of MHP is much smaller than that of MAFIA and the runtime of MHP can be signif- icantly less than that of MAFIA.

5.3. The Effect of the MHP Algorithm on Finding Maximal Hyperclique Patterns  Figure 8 and Figure 9 show the number of maximal pat- terns identified by MHP and MAFIA on Pumsb* and Pumsb data sets respectively. As can be seen, the number of max- imal hyperclique patterns identified by MHP can be orders of magnitude smaller than the number of maximal frequent patterns identified by MAFIA. In other words, the number          1e+06  1e+07  1e+08  1e+09  0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06  N um  be r o  f C ou  nt ed  P at  te rn  s  Support Threshold  Mafia Min_Conf=0  Min_Conf=0.1 Min_Conf=0.3 Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 8. The Number of MFI/MHP Patterns in the Pumsb* Data Set.

1e+06  1e+07  1e+08  0.1 0.2 0.3 0.4 0.5 0.6 0.7  N um  be r o  f C ou  nt ed  P at  te rn  s  Support Threshold  Mafia Min_Conf=0.5 Min_Conf=0.7 Min_Conf=0.9  Figure 9. The number of MFI/MHP Patterns in the Pumsb Data Set.

of maximal hyperclique patterns is much easier to manage than that of maximal frequent patterns. Indeed, in real- world applications, it is difficult to interpret several million maximal frequent patterns. However, it is possible to inter- pret the results of maximal hyperclique pattern mining.

In addition, due to the memory limitation, we cannot ex- tract maximal frequent patterns with MAFIA on the Pumsb data set if the support threshold is less than 0.4, as shown in Figure 7. In contrast, MHP can identify maximal hyper- clique patterns when the support threshold is 0.1, if we set the h-confidence threshold to 0.5. In other words, MHP has the ability to identify patterns which can be difficult to iden- tify for MAFIA. Hence, MHP can better explore the pattern space and find interesting patterns at low levels of support.

6. Conclusions and Future Work  In this paper, we present a two-phase Maximal Hyper- clique Pattern (MHP) mining algorithm, which combines best features of both the BFS strategy and the DFS strat- egy. More specifically, we adapted DFS pruning methods,  such as equivalence pruning, to an apriori-like approach.

In addition, we proved the correctness and completeness of the MHP algorithm. Finally, our experimental results show that the MHP algorithm can be several orders of magnitude faster than standard maximal frequent pattern mining algo- rithms and has the ability to identify patterns at extremely low levels of support in dense data sets.

There are several directions for future work. First, in this paper, we only generate the size-2 patterns in the BFS phase. It will be interesting to investigate the impact on the performance if the first phase is stopped at a deeper level.

Also, the projection is a very efficient method for finding patterns, especially for parallel implementation of pattern mining algorithms [1]. We plan to adapt the projection ideas into our algorithm and design an efficient parallel algorithm for mining maximal hyperclique patterns.

