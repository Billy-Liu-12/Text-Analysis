Discovering Pnteresting Prediction Rules with a Genetic Algorithm

Abstract- In essence, the goal of data mining is to discover knowledge which is highly accurate, comprehensible and ?interesting? (surprising, novel).

Although the literature emphasizes predictive accuracy and comprehensibility, the discovery of interesting knowledge remains a formidable challenge for data mining algorithms. In this paper we present a genetic algouithm designed &om the scratch to discover interesting rules. Our GA addresses the dependence modelling task, where different rules can predict different goal attributes. This task can be regarded as a generalization of the classification task, where all rules predict the same goal attribute.

1 Imtroduction  In essence, the goal of data mining is to discover knowledge which is highly accurate, comprehensible and ?interesting? (surprising, novel). Many data mining algorithms are explicitly designed to discover accurate and comprehensible knowledge. Actually, the goal of discovering comprehensible knowledge is significantly facilitated by the use of rule induction algorithms (including decision trees).

This kind of algorithm discovers high-level prediction rules, in the form:  IF some conditions on the values of predicting attributes are true  THEN predict a value for some goal attribute .

However, the discovery of truly interesting rules remains a formidable challenge for data mining. Recently, several authors have presented different viewpoints on what makes a discovered rule interesting. For instance, [Freitas, 981, [Suzuki & Kodratoff, 981 discuss some objective measures of rule interestingness (or surprisingness), while [Liu et al.

971 discusses a subjective criterion for evaluating rule interestingness.

0-7803-5536-9/99/$10.00 0 1999 IEEE  PUC-PR 2  PPGIA-CCET.

R. Imaculada Conceicao, 1 155  Curitiba - PR.

80.215-901. Brazil  alex@ppgia.pucpr.br http://www.ppgia.pucpr. br/-alex  This paper proposes a Genetic Algorithm (GA) designed to discover interesting prediction rules. The proposed algorithm combines some characteristics of the GA-Nuggets algorithm [Freitas, 991 with some ideas on how to evaluate rule interestingness in an objective (data-driven, domain- independent) manner [Freitas, 981.

Our GA was designed for dependence modelling, a data mining task which can be seen as a generalization of the classification task. In this latter the aim is to predict the value of a special attribute, called the goal attribute, given the values of other attributes, called predicting attributes. Hence, all rules have the same attribute in their consequent (?THEN part?). In dependence modelling, similarly to classification, the aim is to discover rules that predict the value of a goal attribute, given the values of predicting attributes. However, unlike classification, there is more than one goal attribute. Hence, different rules can have different attributes in their consequent.

In our approach for dependence modelling, the user specifies a small set of goal attributes, which (s)he is interested in predicting. Note that this task is different from the discovery of association rules. In the latter the task is complety symmetric with respect to the attributes, i.e. any attribute can occur either in the antecedent or in the consequent of the rule. In contrast, in our dependence modelling task, just a few user selected attributes can occur either in the antecedent or in the consequent of the rule. All the other, non-goal attributes can occur only in the rule antecedent.

Therefore, for a given data set, the dependence modelling task has a search space much larger than the classification task. This is one of the motivations for using a more robust, global-search method like GAS. In contrast, most data mining methods are based on the rule induction paradigm, where the algorithm usually performs a kind of local search (hill climbing).

Another (related) motivation for using GAS is that they tend to cope better with attribute interaction, when   http://www.ppgia.pucpr   compared with most rule induction methods. Indeed, in a CA the fitness hnction evaluates the individual (in our case, a candidate rule) as a whole, i.e. all the interactions among attributes are taken into account. In contrast, most rule induction methods select one attribute at time and evaluate a partially constructed candidate rule, rather than a hll candidate rule. As a result, most rule induction methods tend to be quite sensitive to attribute-interaction problems.

AI A2 A3  2 The Genetic Algorithm  This section presents our GA designed for dependence modelling. The algorithm is based on CA-Nuggets [Freitas, 991. The current version of the system handles only categorical attributes. Future enhancements will allow the use of continuous attributes.

... . . . . . . A?  2.1 Individual?s Encoding Each individual in this algorithm represents a candidate rule of the form ?if A then C?. The antecedent of this rule can be formed by a conjunction of at most n - 1 attributes, where n is the number of attributes being mined.

Each condition is of the form A, = Kj ,  where A, is the i-th attribute and K, is the j-rh value of the i-th attribute?s domain. The consequent consists of a single condition of the form Gk = Vk,, where Gk is the k-th goal attribute and Vkl is the I-th value of the k-th goal attribute?s domain. The user specifies a set of potential goal attributes, whose prediction is considered interesting. Of course, when a potential goal attribute occurs in the consequent of the rule, it cannot occur in the antecedent of it. However, if a potential goal attribute does not occur in a rule consequent, it can occur in the antecedent of that rule, as if it was a predicting attribute.

A string of fixed size encodes an individual with n genes representing the values that each attribute can assume in the rule (see Figure 1). The algorithm automatically chooses the best goal attribute to put in the consequent, for a given rule antecedent (see section 2.3).

If an attribute is not present in the rule antecedent, the value of its gene is ?-I?. This value is a flag to indicate that the attribute does not occur in the rule antecedent. Hence, this encoding effectively represents a variable-length individual (rule).

2.2 Fitness Function The fitness hnction consists of two parts. The first one measures the degree of interestingness of the rule, while the second measures its predictive accuracy.

The computation of the degree of interestingness of a rule, in turn, consists of two terms. One of them refers to the antecedent of the rule and the other to the consequent. The degree of interestingness of the rule antecedent is calculated by an information-theoretical measure, which is a normalized version of the measure propose by [Freitas 981.

Initially, as a preprocessing step, the algorithm calculates the information gain of each attribute (ZnzoGain) [Cover & Thomas, 911. Then, the degree of interestingness of the rule antecedent (AntZnt) is given by:  AntInt = 1 -  InfoGain(A, )  Where n is the number of attributes in the antecedent and lDom(Gk)[ is the domain cardinality (i.e. the number of possible values) of the goal attribute Gk occurring in the consequent. The log term is included in formula [ I ]  to normalize the value of Ant& so that this measure takes on a value between 0 and 1. The ZnfoGain is given by:  InfoGain(Ai) = Info(Gk) - Info(Gk(A,) [2]  Where  [31 r = l  Where mk is the number of possible values of the goal attribute Gk , n, is the number of possible values of the attribute A,, Pr(X) denotes the probability of X and Pr(A denotes the conditional probability ofXgiven Y.

The Antlnt measure can be justified as follows.

Attributes with high information gain are good predictors of class, when these attributes are considered individually, i.e.

one at a time. However, from a rule interestingness point of view, it is likely that the user already knows what are the best predictors (individual attributes) for its application domain, and rules containing these attributes would tend to have a low degree of interestingness for the user.

On the other hand, the user would tend to be more surprised if (s)he saw a rule containing attributes with low information gain. The user probably considered these attributes as irrelevant, and they are kind of irrelevant for classification when considered individually, one at time.

However, attribute interactions can render an individually irrelevant attribute into a relevant one, and this phenomenon is intuitively associated with rule interestingness.

Therefore, all other things (such as the prediction accuracy, coverage and completeness of the rule) being equal, [Freitas 981 argues that rules whose antecedent contain attributes with low information gain are more interesting (surprising) than rules whose antecedent contain attributes with high information gain.

The computation of the consequent?s degree of interestingness is based on the following idea [Freitas 991: the larger the relative frequency (in the training set) of the value being predicted by the consequent, the less interesting it is. h other words, the rarer a value of a goal attribute, the more interesting a rule predicting it is. For instance, a rule predicting a rare disease is much more interesting than a rule predicting a healthy condition, when 99% of the patients are healthy. More precisely, the formula for measuring the degree of interestingness of the rule consequent (ConsInt) is:  ConsInt = (1 - Pr(G,))?P 151  where Pr(G,S is the prior probability (relative fi-equency) of the goal attribute value Gkl, and p is a user-specified parameter, empirically set to 2 in our experiments. The exponent 1/p in the equation [5] can be regarded as a way of reducing the influence of the rule consequent interestingness in the value of the fitness function.

The second part of the fitness function measures the predictive accuracy (PredAcc) of the rule, and it is given by:  I A&CI-1/2 PredAcc = C61 I A I  where v&CI is the number of examples that satisfy both the rule antecedent and the consequent, and is the number of cases that satisfy only the rule antecedent. The term % is subtracted in the numerator of equation [6] to penalize rules covering few training examples - see [Quinlan 871.

Finally, the fitness function is:  AntInt + ConsInt  -- + w2 .PredAcc r71  W1.

Fitness =  wl + w2 Where W, and W, are user-defined weights. In our experiment they are set to 1 and 2, repectively. Note that  PredAcc is given a greater weight than Antlnt and Conslnt.

All the components of equation [7] are normalized.

2.3 Genetic Operators and Rule Consequent Formation Our GA uses the tournament selection where a pair of individuals is randomly picked and the one with the higher fitness is chosen for reproduction.

The crossover operator follows the idea of uniform crossover [Syswerda 891. There is a probability for applying crossover to a pair of individuals and another probability for swapping each gene (attribute)?s value in the genome (rule antecedent) of two individuals. After crossover is done, the algorithm analyses if any invalid individual was created. If so, a repair operator is used to produce valid-genotype individuals. The rates used were 0.7 for the crossover operator and 0.5 for attribute value swapping.

The mutation operator randomly transforms the value of an attribute into another value belonging to the domain of that attribute. The mutation rate used was 0.05.

Besides crossover and mutation, there is the insert and remove operators that directly try to control the size of the rules being evolved, so influencing the comprehensibility of the rules. These operators randomly insert or remove, respectively, a condition in the rule antecedent.

The probability of applying each of these operators depends on the number of attributes in the rule antecedent.

The insert operator has a null probability of application when the rule antecedent has the maximum number of attributes (as specified by the user). The removal operator works in the opposite way. When the number of attributes in the rule antecedent is minimum (as specified by the user) it has a null probability of application. The insert and remove rates used in our experiment are 0.5 and 0.7 respectively.

The previous genetic operators act in the rule antecedent.

Once these operators have been applied and the rule antecedent is formed, the algorithm chooses the best consequent for each rule in such a way that maximizes the fitness of an individual (candidate rule). In other words, the rule consequent is carehlly chosen, in a deterministic manner, to produce the best possible rule for a fixed antecedent. In effect, this approach gives the algorithm some knowledge of the data mining task being solved. A similar approach has been used by some GAS designed to perform a classification task - see e.g. [Green & Smith 931.

A kind of elitism is used in our algorithm. In the task of dependence modelling, there are several possible pairs of <goal attribute, goal attribute v a l u e  that can occur in the rule consequent. In each generation, the best rule predicting each pair of <goal attribute, goal attribute v a l u e  is passed unaltered to the next generation. This corresponds to using an elitism factor of K, where K is the number of distinct rule consequents occurring in the current population.

3 The Data Sets Used in the Experiments The data sets used to test the algorithm were obtained tiom the UCI repository of machine learning databases (http://www. ics. uci.eddAI/Machine-Learning. html). The data sets used are the Zoo and Nursery. They are normally used for evaluating algorithms performing the classification task. In the absence of a specific benchmark data set for the dependence modelling task, these data sets were chosen because they seem to contain more than one potential goal attribute.

The zoo database contains 101 instances and 18 attributes. Each instance corresponds to an animal. In the pre processing phase the attribute containing the name of the animal was removed, since this attribute has no generalization power. The attributes in the zoo data set are all categorical. The attribute names are as follows: hair, feathers, eggs, milk, predator, toothed, domestic, backbone, fins, legs, tail, catsize, airborne, aquatic, breathes, venomous and type. Except type and legs, the attributes are boolean.

In our experiments the set of potential goal attributes used was predator, domestic and type. Predator and domestic are boolean attributes, whereas the type attribute can take on seven different values.

The nursery school data set contains 12960 instances and 9 attributes. The attributes are all categorical. The attribute names are as follows: parents, health, form, children, finance, housing, social, has-nurs and recommendation. In our experiments, the attributes used as potential goal attributes were finance, social and recommendation with 2, 3,s possible values respectively.

4 Results and Discussion  In a classification task, there are some well-defined measures of predictive accuracy. For instance, a commonly used measure, despite its defects [Hand 971, is the accuracy rate, i.e. the ratio of the number of correctly classified test instances over the total number of test instances.

The target of this work is the dependence modelling task which, as mentioned before, is a generalization of the classification task where different rules can predict different attributes. In both tasks the evaluation of the discovered rules must take into account their predictive accuracy on a separate test set. The difference is as follows. In classification we usually aim at discovering a rule set that can classitj, any test instance that appears in the future.

Hence it makes sense to compute an accuracy rate or related measure over all instances in the test set.

In dependence modelling, in the sense addressed in this paper, we do not aim to classify the whole rest set. Rather, the goal is to discover a few interesting rules to be shown to a user. We can think of the discovered rules as the most valuable ?knowledge nuggets? extracted from the data.

These knowledge nuggets are valuable even if they do not cover the whole test set. In other words, the value of the discovered rules depends on their predictive accuracy on the part of the test set covered by those rules, but not on the test set as a whole. Afier all, there are several goal attributes, and it is not realistic to expect that the discovered rules can predict the value of all goal attributes for all instances in the test set. In fact, we could mine such a large rule set by running one classification algorithm for each goal attribute, but we would get too many rules, and the task being solved would be simply ?multiple classification?. In contrast, in the dependence modelling task addressed in this paper we aim at discovering a much smaller set of interesting rules.

Hence, it does not make much sense to evaluate the performance of the discovered rule set as a whole in test set, and the discovered rules are better evaluated on a rule-by- rule basis.

Within this spirits, for each data set (zoo, nursery), we performed two experiments. The first one consisted of using cross-validation with factor 5 to evaluate the quality of the discovered rules. Hence, the data set is divided into 5 mutually exclusive and exhaustive partitions.

The GA is then run 5 times. Each time a different partition is used as the test set and other 4 partitions are merged and used as the training set. The results of the 5 runs are then averaged.

The second experiment consisted of using the full data set (i.e. all the 5 partitions associated with cross validation) to discover the final rules to be reported to the user.

Obviously, the predictive power of these final rules is not estimated by their predictive accuracy on the full data set, but rather by the average predictive accuracy on the test set computed in the first experiment (cross-validation procedure).

However, this second experiment (discovering rules from the full data set) is necessary and advisable for two reasons. First, although we can use cross-validation to compute the average of some rule-quality indicators (such predictive accuracy), we cannot use cross-validation to compute ?average rules? (note that each of 5 runs associated with cross-validation discovers a potentially different set of rules).

Second, The rules discovered from the full data set have the advantage of being discovered from a somewhat larger data set than the rules discovered during cross-validation.

If the reader is not familiar with above-described usage of cross-validation and subsequent learning i?om the full data set, (s) he is referred to [Hand 971.

The Results for the Zoo and Nursery data sets are reported and discussed in the next two subsections. For the Zoo data set the population consisted of 100 individuals, and the number of generations was 100. For the nursery data set the population consisted of 50 individuals, and the number of generations was 100.

http://www   4.1 Results for the Zoo Data Set Table 1 shows the average results, over the 5 runs of the cross-validation procedure, for the zoo data set. Each row of this table is associated with the best-discovered rule for a different rule consequent (a pair <goal attribute, goal attribute value). In the Zoo data set there are 11 distinct rule consequents (since the goal attributes predator, domestic and type can take on 2, 2 and 7 distinct values, respectively). Hence table 1 has 11 rows. (Note that each row is associated with an individual of the last generation, since we used an elitism strategy for preserving the best rule for each rule consequent.)  The first column of Table 1 indicates the pair <goal attribute, goal attribute va lue  characterizing the rule consequent. The second column shows the average value of the degree of interestingness of the rule antecedent, AnZnt (equation [l]), for the rules having the consequent specified in the first column. This average was computed over 5 rules, namely the best rule having that consequent in each of the 5 m s  of the cross-validation procedure.

The thud column of table 1 shows the value of the degree of interestingness of the rule consequent, ConsZnt (equation [5 ] ) ,  for all the rules having the consequent specified in the first column. (Note that the value of ConsZnt is constant for all the rules with a given consequent.) The fourth and fifth columns of Table 1 show the predictive accuracy on the training and test sets for the rules having the consequent specified in the first column. The sixth column shows the average number of examples covered for the rules having the consequent specified in the first column. The average results reported in the fourth, fifth and sixth columns were computed in a manner similar to the above-described computation of the average for the second column.

?fie predictive accuracy on the training set was computed as defined in equation [6].  The predictive  accuracy on the test was computed by a simplified version of equation [6],  namely c4 h CJ / PI. Note that, in the case of the test set, there is no need to subtract % fiom h CJ, as it was done for the training set. That subtraction was used in training set to compensate for the fact that the measure c4 & CJ / is a too optimistic estimate of the predictive accuracy of a rule on unseen data [Quinlan 871. Hence, this correction is not necessary in the test set, which contains data unseen during training.

As can be seen in Table 1, overall the discovered rules have a very high value of AntZnt, i.e. their antecedent are considered (by the measure specified in equation [l]) to be very interesting (surprising).

In addition, Table 1 shows that most of the discovered rules have a good generalization performance on the test set.

Five out of the 1 1 rules have a predictive accuracy of 100% in the test set. Two other rules have a high predictive accuracy in the test set, keeping the same performance level as in the training set. Only four rules have a predictive accuracy in the test set significantly smaller than the one in the training set. Note that the value of JAJ is not as small as it might look at first glance, since the test set has only 20 instances.

The final rules discovered fiom the full data set are show in Table 2. The table shows the best rule for each possible rule consequent. As explained before, the quality of these rules is best estimated by looking up the corresponding rows in Table 1. In any case, for the sake of completeness, we have included in table 2, for each rule, the antecedent?s degree of interest, AntZnt; the consequent?s degree of interest, ConsZnt; the number of examples covered by the rule antecedent, MI; and the number of correctly predicted examples, 1.4 & CI.

Note that most of the rules are not only interesting and highly accurate, as shown in Table 1, but also comprehensible, at least in the sense of representing high- level knowledge and not being very long.

Table 1: Results of cross-validation for the Zoo data set.

AntZnt I ConsZnt I PredAcc I PredAcc I VI I     Table 2: Results of learning from the full zoo data set.

<Goal, Value>  <predator, f a l s e  <predator, true>  Discovered Rule Antlnt Conslnt p&cl !AI If (hah=O) and (eggs=]) and (milkO) and 0.9843 1 1 0.74461 8 4 4  If(airbone=O) and(aquatic=l) and (backbone=l) and 0.952456 0.667491 1 1  1 1  (backbone=l) and (tail=l) and (domestic=l) Then (predator=O)  I (catsize=l) Then (preda to~l )  I 1 I I <domestic, false> I lf(eggs=l)and(airbone=O) and (predatot-1) and I 0.957968 I 0.358766 I 22 I 22 <domestic, t r u e  <type, 1>  (type, 2>  <type, 3>  <type, 4>  <type, 5>  <type, 6> <type, 72  (venomous=O) Then (Domestic=O)  Then (domestic=l) lf(eggs=O)and(aquatic=O) and (tail=O) 0.959166 0.933428 1 1  If(eggs=O)and(venomous=O)and (domestic=O) Then 0.949641 0.770752 32 32  If(feathers=l)and(venomous=O)and (Domestic=O) 0.95521 3 0.895533 17 17 (tYPe=l)  Then (type=2)  (toothed= 1) and(fins=O) and(domestic=O) and(catsize=O) Then(type=3)  and(tail= 1) Then(type=4)  (breathes= 1 )and(catsize=O) Then(type5)  If( eggs= 1 )and(aquatic=O)and(predator= 1 )and 0.936044 0.97493 3 3 3  Iflaquatic= 1 )and(breathes=O)and(venomous=O) 0.939000 0.93 3428 12 12  Iflairbone=O)and(aquatic=l)and(toothed=l)and 0.92 1090 0.979998 4 4  I f(predator= I )and(breathes=O)and(tail=O)and 0.953098 0.949205 7 7 If(airbone=l)and(fins=O)and(tail=O) Then(type6) 0.928637 0.959579 6 6  (Dom est ic=O) Th en(type=7)  4.2 Results for the Nursery Data Set Table 3 shows the average results, over the 5 runs of the cross-validation procedure, for the Nursery data set. The meaning of the columns of table 3 is analogous to the meaning of Table 1?s columns, as explained in the previous section. In the nursery data set there are 10 distinct rule consequents (since the goal attribute finance, social and recommendation can take on 2, 3 and 5 values, respectively).

The results reported in Table 3 are even better than the results reported in Table 1. Again, overall the discovered rules have a high value of Antlnt and have a very good  generalization performance on the test set. Seven out of the tem rules have a predictive accuracy of 100% in the test set.

The other three rules have a much lower performance in the test set.

The final rules discovered fiom the full data set are shown in table 4. The table shows the best rule for each possible consequent. The meaning of Table 4?s columns is analogous to the meaning of Table 2?s columns. Again, we warn the reader that the last 4 columns of Table 4 were included only for the sake of completeness, since the quality of the rules shown in Table 4 is best estimated by the correspond rows in Table 3.

Table 3: Results of cross-validation for the Nursery data set.

I G o a l  atiribute, Attribute value > I Antlnt  <recommendation, recommended> , <recommendation, very recorn> I 0.9423296 I 0.9805278 I 0.9566708 I 1 .o I 3.0  <recommendation, priority> I 0.921443 I 0.7744442 I 0.9949844 I 1 .o I 21.4 <recommendation, spec prior> I 0.92 12322 I 0.8788452 1 0.9949754 I I .O 1 22.8     convenient  0.8 16497  0.816497  0.816497  I  1 0.999882  0.980528  I L  I- 0.878845 1 0.774445  <social, nongrob> r--  Discovered rule If(has-nurs=very-crit) and(children= more)  and(health=recommended) and(class=priority) Then( financeconvenient)  If(has-nurs=very_crit) and(housing=convenient) and(social=slightlygrob) and (health-7ecommended)  and( class=specqrior) Then (finan ce=incov) If(form=complete) and(housing=critical)  and(class=ver y-recom) Then (sociahorn prob)  <social,  problematie  Antlnt 0.998888  0.999 1 1 1  0.996424  <recommendation, not-recom>  Then (class=n<t-recim> If(children=l) Then (class=recommended) 0.997195 <recommendation,  recommended> <recommendation,  very-recom>  - <recommendation,  priority>  <recommendation, specgrior>  If(parents=usual)and (hasnurs=critical) and 10.997318  If(parents=pretentious)and (has-nurs=lessgroper) and (housing=convenient) and (finance=convenient)  and (social=slightlygrob) and (health=recommended)  (health=recommended) and (class=specgrior)  (housing=less-conv) and (finance=incov)and (health=not recom)  0.943364  Then (class=very recom) I If (parents=pretensious) and (has-nurs=lessgroper) I 0.93421 1  and (form=more) and (health-riority)  (form=more) and (fmance=incov) and  5 Conclusion and Future Work  Our genetic algorithm (GA) was designed from the scratch to discover a few interesting rules, which might be called ?knowledge nuggets?, rather than to discover a large set of accurate (but not necessarily interesting) rules.

The results reported in this paper are very promising, since the GA did discover rules which were both highly accurate on an unseen test set and interesting.

However, a more extensive empirical evaluation of our GA would be useful, and will be object of future research.

We also intend to extend the GA described in this paper to cope with continuous attributes (the current implementation can handle only categorical attributes).

Conslnt 0.706733  0.707481  0.8 16497  T  720 2160  -7-p  Another direction for further research would be to evaluate the performance of the GA with other rule interestingness measures proposed in the literature.

Bibliography  [Cover & Thomas, 911 Cover, T. M. and Thomas, J. A..

Elements of Information Theory. John Wiley &Sons. (1991).

[Freitas, 981 Freitas, A. A. On objective measures of rule surprisingness. Roc. 2?* European Symp. on Principles of Data Mining and Knowledge Discovery (PKDD-98).

Lecture Notes in Artificial Intelligence. 15 10, 1-9. (1 998).

[Freitas, 991 Freitas, A. A. A genetic algorithm for generalized rule induction. In: R Roy et al. Advances in Soft Computing - Engineering Design and Manufacturing.

340-353. Springer-Verlag, (1999).

[Greene & Smith 931 Greene, D. P. & Smith, S. F.

Competition-based induction of decision models @om examples. Machine Learning 13,229-257 (1 993).

[Hand 971 Hand, d. Construction and Assessment of ClassiJcation Rules. John Willey &sons. (1 997).

[Liu et al, 971 Liu, B., Hsu, W. & Chen, S. Using general impressions to analyze discovered class fication rules. Roc.

31d Int. Conf on Knowledge Discovery & Data Mining (KDD-97), 31-36. AAAI Press, (1997).

[Quinlan 871 Quinlan, J. R. Generating production rules from decision trees. Roc. IJCAI-87,304-307 (1987).

[Suzuki & Kodratoff, 981 Suzuki, E. & Kodratoff, Y.

Discovery of surprising exception rules based on intensity of implication. Roc. 2"d European Symp. on Principles of Data Mining and Knowledge Discovery (PKDD-98). Lecture Notes in Artificial Intelligence. 1510, 10-18. (1998).

[Syswerda 891 Syswerda, G. Ungorm Crossover in genetic algorithms (ICGA89). 2 - 9. (1989).

