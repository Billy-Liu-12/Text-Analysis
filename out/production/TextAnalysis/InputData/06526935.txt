Comparing the Application of Classification and  Association Rule Mining Techniques of Data Mining

Abstract? The Untouched data of the University can be used to achieve the competitive advantage by having data analysis using Data Mining Techniques. Comparing Application of Classification and Association Rule Mining Techniques to identify fitness of the technique in education domain is an attempt towards uncovering hidden patterns in an Indian University. By experimental results authors have found the Classification comparatively brings more clear and straight pictures of hidden patterns subject to the dataset chosen.

Keywords?association rule mining; classification; data mining; indian university; hidden patterns

I.  INTRODUCTION  In this era of globalization, organizations can sustain growth if they are closely monitoring their ins-and-outs. This can be achieved through efficient data analysis. Out of these organizations, the University plays vital role in society to generate knowledge for the society. Every year lots of students get enrolled at a University from various backgrounds, regions, etc. for obtaining education, where they perform [1].

After a particular batch has passed out the data remains in the databases for longer period of time. It remains untouched and unseen, once again new batch turn up, again new data is injected and this is keep of going [9].

The main theme our research is to use this untouched data [2, 12]. This is where data mining comes in to picture. The Data Mining is one of the important phases of Knowledge Discovery in Databases (KDD) [8], which aims at discovering interesting and useful patterns from large databases. Data Mining deals with uncovering hidden patterns and trends among the databases. Data Mining has various techniques like Clustering, Classification, Association Rule Mining, Outlier Analysis, etc. Each of the above mentioned techniques have been implemented through various algorithms. For example, Classification can be implemented by Decision Tree using ID3 algorithm, C4.5 algorithm, etc.

Our attempt in this study is to see and compare the various data mining techniques and their application on the data of the University. The objective of doing so is to assess of applicability fitness of the technique. In this paper we begin with the application of Classification and Association Rule Mining over the data of an Indian University.

Classification is one of the data mining techniques that assign cases in a collection of data to target categories or classes. The goal of classification is to accurately predict the target class for each case in the data. Association mining is another technique of data mining for discovering interesting relationship between two variables.

For the above said purpose, we have implemented Classification by Decision tree using C4.5 algorithm and Association Rule Mining by Apriori Algorithm (Both the algorithm are given in Section-II and III) on the dataset. The selection of the above said algorithms are purely for the assessment of fitness of the technique. The dataset was collected from a known university of the region and having around 5500 records for 4 consecutive years each. The dataset on which the above algorithms were tested using WEKA [11] has the following attributes of post graduation admission application data: Course for which the student has applied, Region from which student is belonging, Gender, Category of student, Grade obtained in graduation (discrete), University of graduation.

The result (discussed in Section-V) of the study will be useful for the research to find out the fitness of the techniques and researchers who wish to determine the technique for application of data mining in uncovering hidden patterns.



II. DECISION TREES  The task of constructing a tree from the training set has been called tree induction or tree building. It represents a set of classification rules in a tree form. Most existing tree induction systems adopt a greedy (i.e. non-backtracking) top- down divide and conquer manner. Starting with an empty tree and the entire training set, following algorithm is applied on the training data (where each tuple is associated with a class      label) until no more splits are possible.[6, 7] The best known algorithms to generate decision trees are ID3 and C4.5.

ID3 algorithm introduced by J. R. Quinlan is a greedy algorithm that selects the next attributes based on the information gain associated with the attributes. The attribute with the highest information gain or greatest entropy reduction is chosen as the test attribute for the current node. C4.5, the most popular algorithm, is a successor of ID3. C4.5 made a number of improvements to ID3. C4.5 uses Gain ratio as an attribute selection measure. Also C4.5 can handle both discrete and continuous attribute [7]. The following algorithm [6, 7] is a Java implementation of C4.5 as J48 algorithm in WEKA:   Algorithm:  1. Create a node N.

2. If all the tuple in the partition are of the same class  then return N as a leaf node labeled with that class.

3. If attributes list is empty then return N as a leaf node  labeled with the most common class in samples.

4. Identify the splitting attribute so that resulting  partitions at each branch are as pure as possible.

5. Label node N with splitting criterion which serves as  test at that node.

6. If splitting attribute is discrete valued then remove  splitting attribute from attribute list.

7. Let Pi be the partitions created based on the i  outcomes on splitting criterion.

8. If any Pi is empty then attach a leaf with the majority  class in the partition to node N.

9. Else recursively apply the complete process on each  partition.

10. Return N.



III. APRIORI ALGORITHM  To identify the relationship between two variables, association rule mining techniques have many popular algorithms like Apriori, FP-Growth, Magnum Opus, Closet etc. In all above algorithm  user needs to set two thresholds, viz. minimum support and minimum confidence. We have used Apriori algorithm as implemented in WEKA [13] by following algorithm to generate the rules from the dataset:   Input: Itemset: a set of items k-itemset: an itemset which consists of k items Frequent itemset (i.e. large itemset): an itemset with sufficient support Lk or Fk: a set of large (frequent) k-itemsets ck: a set of candidate k-itemsets Apriori property: if an item X is joined with item Y, Support(X U Y) = min(Support(X), Support(Y)) Negative border: an intemset is in the negative border if it is infrequent but all its ?neighbors? in the candidate itemset are frequent.

Interesting rules: strong rules for which antecedent and consequent are dependent Apriori algorithm:  //Perform iterations, bottom up: // iteration 1: find L1, all single items with Support > threshold // iteration 2: using L1,  find L2 // iteration i: using L i-1,  find L i // ? until no more frequent k itemsets can be found.

//Each iteration consists of two phases: 1. Candidate generation phase:  //Construct a candidate set of large itemsets, i.e. find all the items that could qualify for further consideration by examining only candidates in set Li-1*L i-1 2. Candidate counting and selection  //Count the number of occurrences of each candidate itemset  //Determine large item sets based on predetermined support, i.e. select only candidates with sufficient support   Set Lk is defined as the set containing the frequent k item sets which satisfy  Support > threshold.

Lk*L k  is defined as:   Lk*L k   = {X U Y, where X, Y belong to L k  and | X Y| = k-1}.

Apriori algorithm, in more detail:  //find all frequent itemsets Apriori(database D of transactions, min_support) {  F1 = {frequent 1-itemsets} k = 2 while Fk-1  EmptySet Ck= AprioriGeneration(Fk-1) for each transaction t in the database D{ Ct= subset(Ck, t) for each candidate c in Ct { count c ++  } Fk = {c in Ck  such that countc min_support} k++ } F = U k  1 Fk  } //prune the candidate itemsets ApprioriGeneration(Fk-1) { //Insert into Ck all combinations of elements in Fk-1  obtained by self-joining itemsets in Fk-1 //self joining means that all but the last item in  the itemsets considered ?overlaps,? i.e join items p, q from Fk-1  to make candidate k-itemsets of form p1p2 ?p k-      1q1q2?q k-1 (without overlapping) such that p i =q i  for i=1,2, .., k-2 and pk-1 < qk-1.

//Delete all itemsets c in Ck such that some (k-1) subset of c is not in Lk-1  }   //find all subsets of candidates contained in t Subset(Ck, t) { ?  }

IV. METHODOLOGY  The obtained data is pre-processed with according to the need of the system. For instance, in particular region field is derived from the field district, where the district field is cleaned using context free data cleaning [10]. The numerical values are converted into discrete values of grade. At the end of entire pre-process dataset is converted and stored in .csv format. It is then supplied to WEKA. To obtain decision trees in WEKA J48 algorithm (Java implementation of C4.5 algorithm) with Support 0.5 and to obtain rules using Apriori algorithm in with support 0.33.



V. EXPERIMENTAL RESULTS & DISCUSSION   After applying above algorithms over the data following  results were obtained:  For, Classification ?   ? Category in [OBC] o Uni. in [OUG]  Region in [Central] then Course = MA (66.67 % of 78 examples)  Region in [West] then Course = MSC (76.47 % of 34 examples)  Region in [North] then Course = MSC (70.00 % of 30 examples)  Region in [South] then Course = MSC (46.88 % of 32 examples)  o Uni. in [SPU] then Course = MA (69.77 % of 397 examples)  o Uni. in [OTG] then Course = MSC (0.00 % of 0 examples)  ? Category in [OPN] o Uni. in [OUG]  Region in [Central] then Course = MA (45.67 % of 300 examples)  Region in [West] then Course = MSC (70.41 % of 98 examples)  Region in [North] then Course = MSC (73.13 % of 67 examples)  Region in [South] then Course = MSC (53.97 % of 63 examples)  o Uni. in [SPU] Region in [Central] then Course =  MA (44.98 % of 1016 examples) Region in [West] then Course =  MSC (33.33 % of 54 examples) Region in [North] then Course =  MA (51.52 % of 33 examples) Region in [South] then Course =  MSC (44.68 % of 47 examples) o Uni. in [OTG] then Course = MSC (64.71 %  of 17 examples) ? Category in [SC]  o Region in [Central] then Course = MA (74.67 % of 225 examples)  o Region in [West] then Course = MA (55.56 % of 9 examples)  o Region in [North] Uni. in [OUG] then Course = MSC  (76.92 % of 13 examples) Uni. in [SPU] then Course = MA  (92.86 % of 14 examples) Uni. in [OTG] then Course = MSC  (0.00 % of 0 examples) o Region in [South] then Course = MA (63.64  % of 11 examples) ? Category in [ST] then Course = MA (89.27 % of 466  examples) ? Category in [PH] then Course = MA (86.36 % of 22  examples)  For, Association Rule Mining ?   ? If Uni=?SPU? and Category=?Open? then Region=?Central? (Support 33.576, Confidence 88.348, Lift 1.249)  ? If Region=?Central? then Uni=?SPU? (Support 54.494, Confidence 77.056, Lift 1.130)  ? If Uni=?SPU? then Region=?Central? (Support 54.494, Confidence 77.056, Lift 1.130)  ? If Region=?Central and Category=?Open? then Uni=?SPU?  (Support 33.576, Confidence 76.737, Lift 1.126)  ? If Category=?Open? then Region=?Central? (Support 43.754, Confidence 78.112, Lift 1.104)    By looking over the above results and their analysis:  1. The experimental results give inputs to the decision  makers for which course the demand is there and not, from which region and which category.

2. The University can upgrade the resources like faculty, infrastructure, etc. in order to fulfill the demand.

3. The demand in a particular course from particular  region shows inadequate seats in the region or high investment in particular industries in the region, which help the state and/or central government to take actions.

4. The algorithm implemented above gave satisfactory  results in terms of uncovering the hidden patterns.

5. The authorities can also look upon various reservation policies for various categories   Similarly, these techniques can be applied over various  dataset of various years to identify various patterns.

Looking towards the comparison of the results obtained by implementing the algorithms:   1. The decision-tree algorithms give clearer and specific  picture and help more to understand the hidden patterns from the dataset with accuracy more than 50%  2. The apriori algorithm also uncovers the patterns but due to the algorithmic limitation, as mentioned below, it does not classify/predict the consequent [4].

(i) Finding appropriate parameter settings for algorithm (ii) Discovering too many/few poorly understandable/obvious rules  3. The rules that have been generated by the apriori algorithm that has lift very near to 1. Hence, to rely on the rule becomes doubtful.

4. Also, to understand the results and present them to the common men seems straightforward in the decision tree representation.



VI. CONCLUSION  With a view of above results and discussion, to uncover the hidden patterns classification is found more effective to understand the patterns and build a predictive model in an Indian University. The experiments done above can be replicated with different algorithms as and when needed with different inputs to uncover other patterns. The experiments done above with available and limited data set. Of course the time and space complexity of the algorithms are important measures for effectiveness of the algorithms, but the case is not the routine and the purpose is to assess fitness of them uncover hidden patterns from the dataset. The results may differ if the different data set is used or another strategy is applied, which may be tested. There may be other determinants which may not be present in the dataset or may be overlooked by the authors while experimenting for generating rules. Based upon the specific dataset, with above framework and methodology we  can have more specified results, which can be used for various strategic decisions. The Authors wish to see fitness of other algorithms for the domain.

