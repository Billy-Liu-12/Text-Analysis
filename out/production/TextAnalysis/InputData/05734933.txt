Detection of Verbatim or partial Duplication from Multiple Source Documents using  Data Mining Techniques and Case-Based Reasoning Methodologies

Abstract?This paper aims to specify a Case-Based Reasoning strategy for correctly classifying, storing and  preventing duplication efforts of electronic text material.   Preservation of complete source documents for checking similarity between them pose a daunting amount of spatial and computational complexity to researchers in this area. The problem is partially solved by applying certain preprocessing steps to reduce the volume of data handling  substantially. Reduction of volume in text documents is achieved by applying some stemming algorithm  and elimination of stopwords from the document utilizing certain text-mining measures such as TF-IDF. A third technique involves  extraction of keywords and storing them in a properly indexed base.  These then can serve the dual purpose of providing solutions to Lazy Learning classification for automatic subject-wise  archiving and formation of relevant word sequences  for detection of plagiarism using Association Rule-mining techniques.

Keywords: Case-BasedReasoning strategies, TF-IDF, Plagiarism, Association Rule-mining techniques.



I.  INTRODUCTION  A. Text Mining Preliminaries Text Mining techniques have been traditionally utilized  by Information Technologists to search and retrieve information from document repositories for purposes as diverse in nature as are the structures used in those repositories [1].

A raw text document, although already stripped of non-ascii components, still suffers from the presence of irrelevant words which contribute little to the actual subject content. Two common procedures followed in reducing the bulk of individual  words in a document are ?stemming?  i.e. preservation of root words only and removing  the so called ?stopwords? or common words.

Conventional stemming   algorithms such as   those by Porter (1980) [2] and Lovins(1968) [3] are quite elaborate, thus we have proposed a  simpler heuristic instead.

Machine learning tools such as KEA and Keyterm Methods for extraction of keywords tried out in [5] have also been avoided due to non-availability of  large training datasets. Instead, the string-frequency measure of  TF-IDF score indicated in both [5] and [6] has been adopted, albeit using formula suggested in [1], to eliminate stopwords and retain the probable keywords.

B. Basics of Case-Based Reasoning Strategies Case-Based Reasoning (CBR) strategy utilizes  techniques where a memory-base of preserved cases in the form of problems and their associated solutions can be referred to for creating solutions to a matched problem and preserve the same as a new case in the base [7].

By adaptation of  frequent pattern generation algorithms outlined in [1], it has been possible to generate the so-called useful key-phrases, in the form of bi-grams and tri-grams as depicted in [6], and preserve these for the required document in a memory based structure. The backbone of the case base is a database maintained in a format somewhat similar to the one indicated in [6].

The following section II describes in detail the methodology followed in matching and classifying the test document with respect to the case base which essentially contains the condensed set of all documents. In section III we have described the experimental setup with four sets of data used to benchmark the frequent pattern generation algorithms. The Results and Performance Analysis is presented in section IV. Finally  section V concludes with remarks for possible domain where the system provides optimal performance.



II. METHODOLOGY  A. Preliminary Storage Format used for the words The text document is parsed sentence by sentence -  each sentence being terminated by checking against a reserved list of punctuation marks or special characters.

The words in the sentence are mapped onto a hashed-code index value for memory based arrays of a special data structure. These are then backed up in a compatible secondary storage file structure after performing the necessary reduction using the methods outlined in the following sub-section.

The hashed-code value of a word is generated as a summation of a sequence according to the formula given in the  following equation where p is a  prime number chosen arbitrarily at the beginning and ci is the ascii code value of the i-th character of a n-character word.

n -1 hash_value = ? ci  x  pi                                         (1)  i=0   DOI 10.1109/EAIT.2011.31     For all practical purposes the number of individual words in a corpus needs to be restricted to a maximum upper limit MAX say, and the above hash_value then needs to be divided by MAX and the remainder retained so that no hash values larger than MAX is ever generated.

For avoiding too large values for long words the i value may similarly be restricted to an upper limit while calculating pi  .

Collisions, are resolved by applying formula (1) with the next higher prime being used in place of p and re- calculating the hash value. The technique is repeated until no further collision is detected and a new hash value has been assigned as the index to a new word in the base.

The documents now reduce to a collection of words, each word being placed at an indexed position in the base table followed by the integral count of that particular word in the corpus and also a  positional reference in the form of a link pointing to the beginning of a list consisting of a chain of  triplets  having  the following structure : < document-id > < sentence-no > < word-seq >.

In this scheme, the test documents are the new cases that need to be stored in a properly indexed form within the word base. The document-id corresponds to a classification tag that must be used to correctly locate each separate document in the corpus. This document-id later serves to trace out the individual words stored for the corresponding case  within the base. The sentence-no simply refers to a monotonically increasing integral count for each portion of text identified to be segregated from the previous or the next portion by the special punctuation delimiters known to the system. The word-seq  is another monotonically increasing count of the position of the word within each sentence.

B. Words stemmed and screened by  TF-IDF scoring Stemming is achieved at this stage with the help of a  crude heuristic which  matches the words against a  list of common word-ends. Thus all stemmed words are mapped onto the same root with their positional records merged in order, achieving considerable reduction in time and space complexity at all succeeding stages.

The TF-IDF (term frequency-inverse document frequency)  scoring technique is applied to determine the importance of a word in a document.

The TF-IDF score associated with a word or term is generally computed by multiplying term frequency and inverse document frequency. The evaluation technique adopted here is based on those indicated in [1].

The term frequency of a term ti in a particular document dj  has been calculated as follows : TFi,j =  0                       if freq(dj, ti) = 0 = 1 + log(1 + log( freq(dj, ti)))      Otherwise   (2) where, TFi,j   is the term frequency of the term ti in document dj freq(dj,ti)  is number of times term ti occurs in document dj  Inverse document frequency has been obtained by dividing the total number of documents by  the number of documents containing that term, and then taking the logarithm of that, as shown  below:  IDFi = log   1 + | D | {|d : ti ?d |}                    (3)  where, IDFi  is the inverse document frequency for term ti | D |  is the total number of documents and |d : ti ? d | is the number of documents containing term ti  The foundation of the case-base is built by considering a collection of two types of document  ? those which are candidate for retention in the case-base i.e the relevant documents and others which have been hand- picked to form a general cluster with no subject bias i.e.

the irrelevant documents.

Based on equation (2)  two averages are calculated : Avg1 ? is  average TF of a word in relevant documents.

Avg2 ?  is  average TF of a word in irrelevant documents.

The difference of these two averages decides whether a term is a key term or not. A low difference would indicate that the term is equally frequent in all documents - i.e. a common term. On the other hand, a high positive difference indicates a key term for the relevant topic.

From equation (3), it can be inferred that IDF value is low for common terms and presumably high for key terms.

Hence by calculating the TF-IDF value as follows, (TF-IDF)ij = (Avg1 ? Avg2 ) x  (IDF)i    (4)  a key term is likely to  possess  a high TF-IDF value and a common term will usually have a low TF-IDF value.

Equation (4) is used for each and every term in the document and fixing the lower threshold frequency limit to a value decided on the basis of several observations during the trial stage the terms in the documents are classified into two categories ? the common terms and the likely key terms. The words, whose scores are less than the threshold value, are then rejected thus  reducing the bulk of the document. The remaining terms are passed onto the next stage for extracting the frequent (and hence relevant) term sequences.

C.        Extraction of relevant term sequences  The formation of the relevant term sequences from the candidate key terms collected so far is adapted from some of the existing frequent pattern generation techniques [1].

The comparative study of three such algorithms viz. the Apriori using Horizontal File format, the Apriori using Vertical File format and the FP-Tree Growth Algorithm, are made at this stage.

At first the performance of the Apriori algorithm using Vertical File format seems to be the natural choice as the base table of  words is already organized in a compatible format. The frequency of the word is noted as its support and the transactions in which it is occurring have been registered in tuples as <sentence-no>. The <word-seq> is a domain dependant appendage which requires to be     checked, as words too far apart in a sentence cannot be considered to be of much value in a key term sequence.

On practical investigation, however, it is found that a FP(Frequent Pattern)-tree like data structure, besides being equally effective for larger data sets, is more suitable for the memory based  checking scheme adopted in the present instance. Each transaction, consisting of the words in a sentence, is stored as a path starting from a common root to a leaf of the tree. The vertical structure is also used here to link up the first occurrence of each word to the base table via a pointer. An improvement in  the scheme is achieved by preserving the condition that no path can exceed beyond the length of three since no more than two to three associated word sequences can occur in a key phrase. An additional  restriction is imposed by considering neighboring words only - ie. words either next to each other or separated by one or two words at most, in each of these sequences.

Every time a new case (ie. a new document) is presented, it is converted to this tree structure. Each path in the tree, starting from the root node, is next scanned, one after another, to find the component words. The words are matched against the case entries for each individual document, consisting of the <sentence-no> <word-seq> pairs and if the same pattern is found in a case-document a score for that document is incremented. If this score exceeds a certain threshold percentage for a particular case-document, a direct charge of plagiarism may be brought against the new document under the present scheme. The percentage is calculated on the basis of the number of key-phrases matched out of the total number of key-phrases in the new document.

But often plagiarism involves partial copy from more than one case-document. The procedure adopted here, to detect such clever tactics, is to keep a separate score for each compared case-document. Besides these,  an additional count is maintained at the end of each matched key-phrase path in the new document. If the percentage of such matched keyphrases exceeds a certain threshold, an indirect charge of plagiarism may be brought against all the documents contributing to the total  score, with the individual details displayed to establish the case. It is then left to the discretion of the user of the system, often a Subject Expert,  to decide whether the new document should be included in the case base or not.

A final note on the choice of  bi-grams and tri-grams alone  as key-phrases from the sequence of words in a sentence ? a  sequel of this domain-dependant restriction is that the presence or absence of a particular key-phrase sequence is of more essence than its actual frequency count in the document and leads to the detection of plagiarism. Thus,  the support counts noted in the experiments described in the following sections are with respect to single key terms rather than the key-phrases constructed therein.

During classification or categorization of the document, however, these individual terms play a key-role in detecting the subject a document deals with. Hence matching these key terms are more important while  categorizing the document and selecting a proper identifier for it. Matching percentage more than a pre- determined value (and obviously lower than the plagiarism threshold) leads to the inclusion of the document in the case base with the correct categorization and identification. Categorization along with the percentage of plagiarism forms the adapted solution of the case being considered.



III. EXPERIMENTAL SETUP A variety of E-documents has been used in  checking  the efficacy of the system. The details of the four subject- oriented documents used in comparing the frequent pattern generation algorithms are  mentioned next.

TABLE I.  E-DOCUMENT   DETAILS  Data set Book Name Size (KB)  Unique Words  I Few Chapters of  [1] 1726 513  II A paper on ?Image Analysis for Face Recognition? 58 522  III An article on ?X-ray? 21 382  IV Chapters of e-book ?Linux Programming Unleashed? 1499 741   A collection  of 22 general non-specific documents are  used in the second phase for calculating TF-IDF scores of the unique words or candidate  key terms.

A sample data structure of table and FP-tree of words is  shown below to indicate growth for our proposed case- base.

TABLE II.  BASE TABLE FOR WORDS  Word Index  Word String Frequency of Occurence  Pointer to first occurrence in FP-tree  W1 Data 565 ?  W2 Cluster 123 ?  W3 Search 97 ?  : : : :    Fig1   Growth of  FP-tree for words in frequent sequences

IV. RESULTS AND PERFORMANCE ANALYSIS Here Apriori algorithm using both Horizontal File  format and Vertical File format as well as FP-Tree Growth Algorithm  are applied  on all four test documents listed in the previous section. The absolute time required in each case for individual text is plotted in four different blocks as follows :  Data Set I       sp 20 25 30  min. support count  tim e  (in m  s)  fp-growth algorithm apriory on vertical apriory on horizontal   Fig.2 For a large data set, the FP Growth Algorithm far out performs the  other two algorithms.

Data Set II   sp 10 20 25  min. support count  tim e  (in m  s)  fp-growth algorithm apriory on vertical apriory on horizontal   Fig.3 For a small data set with a large enough unique wordset FP  Growth gives results comparable with Apriori using Vertical Format File.

Data Set III   sp 5 7 10  min. support count  tim e  (in m  s)  fp-growth algorithm apriory on vertical apriory on horizontal   Fig. 4 The complexity of generating FP-trees for a small data set is  reflected in the present case.

Data Set IV       sp 60 80 100  min. support count  tim e  (in m  s)  fp-growth algorithm apriory on vertical apriory on horizontal   Fig. 5   Growing FP-trees is the best choice for large data sets and large unique word sets.

Figures 2, 3 and 5 clearly indicates the efficiency of  FP-Tree Growth Algorithm. The anomaly reflected in figure 4 is due to the small size of data set. The performance of the system have been found to be efficient and scalable within the constraints mentioned earlier.

The time complexity of the last phase  is reduced considerably by restricting the maximum depth of the tree structure to 3. Plagiarism can be detected quite effectively for sample synthetic texts formed by either copying verbatim from a single  case document or  through cut and paste actions on several such documents.



V. CONCLUDING REMARKS From the foregoing results it is obvious that the  performance of the scheme improves with the increase in the size of the document as well as with the growth of  the count of unique words set. The set of keywords  would then be rich enough to help classify a new document easily and also to establish  the plagiarism charge brought against that document.

As has been rightly pointed out in other instances of CBR [7], there is no need to gather prior expertise by building a  model through extensive training. Instead the system automatically learns  as it acquires knowledge in the form of preserved keywords from past documents.

This incremental learning procedure lends itself well to an  environment in which the documents are not presented all at once and they are neither too large nor too varied in nature. Thus a typical domain that we like to propose as the correct testing ground ? proceeding preparation for a Conference or a Journal,  where lots of papers arrive haphazardly and manually checking each one for its category and originality  may pose a difficult task involving the Subject Experts.

BIBLIOGRAPHIC REFERENCES [1] J. Han, and M. Kamber,  ?Data Mining Concepts and  Tecniques,? Morgan Kaufmann Publishers, SanFrancisco, USA, 2001, ISBN 1558604898  [2] M.F. Porter, ?An algorithm for suffix stripping?, Program, 14 no. 3, pp 130-137, July 1980.

[3] J.B. Lovins,?Development of a stemming algorithm?, Mechanical Translation and Computational Linguistics 11, 22-31,1968.

[4] P. Turney, ?Extraction of Keyphrases from Text : Evaluation of Four Algorithms?, Technical Report ERB- 1051 (NRC-41550), Institute for Information Technology, National Research Council of Canada, Ottowa, ON, Canada, October 23, 1997.

[5] Y. Zhang, N.Zincir-Heywood, E.Milios, ?Narrative Text Classification for Automatic Key Phrase Extraction in Web Document Corpora?,WIDM?05, November 5, 2005, Bermen Germany.

[6] LI Juanzi, FAN Qui?na, and ZHANG Kuo, ?Keyword Extraction Based on tf/idf for Chinese News Document?, Wuhan University Journal of Natural Sciences, Vol. 12, No. 5, 2007, doi: 10.1007/s11859-007-0038-4.

[7] I. Watson and F.Marir, ?Case-Based Reasoning : A Review?,The Knowledge Engineering Review, Vol 9, No.

4,1994.

