Clustering Frequent Itemsets Based on Generators

Abstract   How to reduce the number of frequent itemsets  effectively is a hot topic in data mining research.

Clustering frequent itemsets is one solution to the problem. Since generators are lossless concise representations of all frequent itemsets, clustering generators is equivalent to clustering all frequent itemsets. This paper proposes a new algorithm for clustering frequent itemsets based on generators. Firstly, based on minimum description length principle, the rationality of clustering generators is discussed. Secondly, the pruning strategies and mining algorithm for generators are proposed. Finally, based on a new similarity criterion of frequent itemsets, the clustering algorithm is presented. Experimental results show that the proposed method can not only reduce the number of discovered itemsets, but also is efficient.

1. Introduction   Frequent Itemset Mining (FIM) is one of the major problems in many data mining applications. It started as a phase in the discovery of association rules [1], but has been generalized independent of these to many other patterns.

For example, frequent sequences, episodes, and frequent subgraphs [2].

There have been many scalable methods developed for frequent itemsets mining [3]. However, the real challenge in FIM is the sheer size of its mining results. In many cases, a high min_sup threshold may discover only commonsense itemsets but a low one may generate an explosive number of output patterns, which severely restricts its usage. To solve this problem, it is natural to explore how to find a concise and succinct representation that describes the whole collection of itemsets.

Two major approaches have been developed in this direction: lossless compression and lossy approximation.

The former is represented by closed frequent itemsets [4, 5].

Its compression is lossless in the sense that the complete set of original frequent itemsets can be recovered.

However, the methods emphasize too much on the supports of patterns so that its compression power is quite  limited. The latter is represented by the maximal frequent itemsets [6]. These methods only consider the expressions of itemsets, while the support information in most of the itemsets is lost.

To achieve high-quality itemset compression, a general proposal for high-quality compression is to cluster frequent itemsets according to certain similarity measure, and then select and output only a representative pattern for each cluster [7, 8]. For the sake of efficiency, maximal itemsets [9] or closed itemsets [10] are usually selected as the clustering center.

To make the clustering results more succinct and robust to noise, a new algorithm for clustering frequent itemsets based on generators is proposed. Since generator is also a lossless compression of all frequent itemsets, clustering generators is equivalent to clustering all frequent itemsets. Firstly, based on minimum description length principle, the rationality of clustering generators is discussed. Secondly, based on FP-tree [11], the pruning strategies and mining algorithm for generators are proposed. Finally, based on a new similarity criterion of frequent itemsets, the clustering algorithm is presented.

Experimental results show that the proposed method can not only reduce the number of discovered itemsets, but also is efficient.

2. Problem statement   Let I be a finite set of items and D be a dataset containing N transactions, where each transaction t?D is a list of distinct items t = {i1, i2, ?, i|t|}, and each transaction can be identified by a distinct identifier tid.

Let T and X, T?D and X?I, be subsets of all the transactions and items appearing in D, respectively. The concept of closed itemset is based on the two following functions, f and g:  f (T)={i?I | ?t?T, i?t}                       (1) g(X)={t?D|?i?X, i?t}                       (2)  Function f associates with T the items common to all objects t?T and g associates with X the objects related to all items i?X.

Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.293   Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.293   Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.293     Definition 1. The support of an itemset X, denoted as sup(X), is defined as the number of transactions in which X occurs as a subset.

Definition 2. For a given D, let min_sup be the threshold minimum support value specified by user. If sup(X) ?  min_sup, itemset X is called a frequent itemset.

Definition 3. An itemset X is said to be closed if and only if  h(X)=f(g(X))=X                            (3) where the composite function h is called the Galois  operator or closure operator. A closed itemset X is frequent if its support is lager or equal to the given support threshold min_sup.

Definition 4. Let G be a itemset, C be a closed itemset, if h(G)=C, and A?G, such that h(A)=C, then G is a generator.

Table 1. Example database TID Items  1 A B C D E F 2 A E G 3 B C E F 4 A B C D F 5 A B C E F  An example database is given in Table. 1. For convenience, we write an itemset {A, B, C} as ABC, and a set of transaction identifiers {2, 4, 5} as 245. In the example database, f(125)=f(1)?f(2) ?f(5)=ABCDEF?AEG?ABCEF=AE, and g(AE)=g(A)?g(E)=1245?1235=125. Since h(AE)=f(g(AE))=AE, AE is a closed itemset, and both A and E are generators of AE.

3. Mining generators based on FP-tree  3.1. Generators and closed itemsets   The closure operator defines a set of equivalence  classes over the lattice of frequent itemsets. In each equivalence class, generators and closed itemsets are minimal and maximal itemsets respectively. As they are both lossless representations of frequent itemsets, why we prefer generators rather than closed itemsets for clustering?

The motivation is based on the Minimum Description Length Principle (MDL) [12].

A crude two-part version of MDL is as follows: Let H= {H1, H2, ?, Hn} be a set of hypothesis learned from a dataset D. The best hypothesis H?H to explain D is the one which minimizes the sum  L(H,D) = L(H) + L(D|H)                       (4) Where, L(H) is the length, in bits, of the description of  hypothesis H; and L(D|H) is the length, in bits, of the description of the data when encoded with the help of hypothesis H.

Let ModelG and ModelC are two models for certain database D by using generators and closed itemsets, then we have:  ModelG =min(L(G)+ L(D|G))                 (5) ModelC =min(L(C)+ L(D|C))                 (6)  Since generators and closed itemsets are minimal and maximal itemsets in one equivalence class respectively, we have L(G)<L(C), and L(D|G)=L(D|C). According to MDL principle, ModelG is better than ModelC.

This preference is particularly obvious in classification problems. For an application where only two classes of transactions are involved, suppose transactions in an equivalence class all have the same class label, say, positive class. Also assume that the closed itemset C has n items a1, a2, ?, an (n>2), and the generator G has only 2 items a1 and a2. Then, we can get two rules:  One derived from C: If a transaction contains a1?a2???an, then it is positive.

The other derived from G: If a transaction contains a1 and a2, then it is positive.

The second rule should be more predictive on independent test data than the first one, because a true test sample is more likely to satisfy the two items a1 and a2 than to satisfy the n items contained in C. So, when some noise is present in the test data, the second rule can better tolerate the noise errors than the first rule.

3.2. Pruning strategies for generators   Theorem 1. gen is a generator, if and only if for  ?s?gen, sup(gen)<sup(s) holds.

Proof: If for ?s?gen, sup(gen)<sup(s) holds: Suppose gen is not a generator, then there exists  itemset A?gen, such that sup(A)=sup(gen), it contradicts with hypothesis, so gen is a generator.

If gen is a generator: For ?s?gen, we have sup(gen)?sup(s). According to  Definition 4, sup(gen)?sup(s), so sup(gen)<sup(s).

Theorem 2. If P is a generator, and Q?P, then Q is  also a generator; If P is not a generator, and P?Q, then Q is not a generator either.

Proof: Suppose P is a generator, Q?P, and Q is not a generator, then there exists itemset X?Q, such that h(X)=h(Q). Assume P=Q?Y, Q?Y=?, then sup(X?Y)=|g(X?Y)|=|g(X)?g(Y)|=|g(Q)?g(Y)|=|g(Q?Y)| = |g(P)| =sup(P). Meanwhile, X?Y?Q?Y=P, it contradicts with P is a generator.

If P is not a generator, and P?Q, According to Definition 4, there exists P??P, such that g(P?)=g(P).

g(Q)=g((Q\(P\P?))?(P\P?))=g(Q\(P\P?))?g(P\P?). Since g(P?)=g(P)?g(P\P?), we have g(Q\(P\P?))?g(P?)?g(Q\(P\P?))?g(P\P?)=g(Q), that is g((Q\(P\P?))?P?))?g(Q). Because P??Q\(P\P?), g((Q\(P\P?))?P?))=g(Q\(P\P?))?g(Q) holds. Furthermore,     Q\(P\P?)?Q, so we have g(Q)?g(Q\(P\P?)). That is g(Q)=g(Q\(P\P?)), so Q is not a generator.

3.3 Algorithm for mining generators   We develop an FP-growth-like algorithm [11] to  discover representative patterns. Since FP-growth is a well-known method, we omit the detailed description here due to the limited space.

Algorithm 1. FP-Gen (gen, Dgen, min_sup) gen: generator Dgen: conditional database of generator gen; min_sup: support threshold   1) Scan Dgen once, and delete non-frequent items.

The set of remaining frequent items is denoted by Fgen;  2) for each i?Fgen do { 3)   if gen?i can not be pruned by Theorem 1 and  Theorem 2  then 4)       if (Dgen?i ==?) then 5)            MFG? gen?i; 6)       else  { 7)         FG? gen?i; 8)         FP-Gen (gen?i, Dgen?i, min_sup); 9)       } } Note that, for the sake of clustering (Algorithm 2 in  Section 4), maximal generators and other generators are stored in MFG and FG respectively.

4. Clustering based on generators   To clustering generators, the following similarity  measure is proposed:  S?G1, G2?= 1 2  1 2  | ( ) ( ) |  | ( ) ( ) |  g G g G  g G g G  ?  ? (7)  Take generators A and B for example, S(A, B)= | ( ) ( ) |  | ( ) ( ) |  g A g B  g A g B  ?  ? =  | 1245 1345 |  | 1245 1345 |  ?  ? =0.6.

Definition 5. For generator G, if there exists no generator X, such that G?X, and sup(X)?min_sup, then G is called maximal frequent generator, the set of all maximal frequent generators is denoted by MFG.

Definition 6. For generator G?MFG, M?MFG, G?M, if M ??MFG, such that G?M ?, and S(M?, G)> S(M, G), then M is called the cover of G, denoted by M=Cov(G).

If there are several covers for one generator, then the first one according to lexicographical order is selected.

In this paper, generators are clustered according to their covers.

Algorithm 2. FG-Cluster 1) for each gen?FG do  2)    for each mgen?MFG do 3)       Caculate S(gen, mgen) according to formula  (7) 4) put gen into the cluster represented by Cov(gen);   5. Experimental results   We chose chess and pumsb* datasets for testing the  performance of the proposed method. They are taken from the FIMI repository page http://fimi.cs.helsinki.fi. The chess dataset is derived from its game steps. Pumsb* contain census data without items with 80% or more support. Table 2 shows the characteristics of these datasets. The experiments were conducted on a Windows 2003 Server equipped with a Pentium 2GHz CPU and 1GB of RAM memory.

Table 2. Characteristics of datasets Datasets # Items # Records Avg. Length  chess 75 3,196 37 pumsb* 2088 49,046 50.5     (a)chess   (b)pumsb*  Fig 1. Comparisons of the number of generators and the number of clusters  The first set of experiments compares the number of output patterns. For each data, we vary the value of min_sup as the percentage of the number of total transactions. In most cases, the number of clustered     generators is almost two orders of magnitude less than the whole collection of generators.

(a)chess   (b)pumsb*  Fig. 2 Performance comparison of FG-Cluster with other two closed itemset algorithms  The corresponding running time of FG-Cluster and other two closed itemset algorithms are shown in Fig. 2.

FG-Cluster is about one order of magnitude faster than A- Close [4]. In most cases, the performance of FG-Cluster is comparable to that of AFOPT-Close [5].

6. Conclusions   To reduce the number of all frequent itemsets, and  improve the understandability of mining results, a new algorithm for clustering frequent itemsets based on generators is proposed. The method reduces the number of results in two steps: transform all frequent itemsets into generators at first; then clustering generators.

Experimental results show that the proposed method can not only reduce the output patterns, but also is efficient.

Acknowledgement  This work is supported by the National Natural Science  Foundation of China (60675030), by Funding Project for Academic Human Resources Development in Institutions of Higher Learning Under the Jurisdiction of Beijing Municipality, and by Key Youth Research Project of North China University of Technology.

