Enhancing Concurrency in Parallel Random Network  Coding

Abstract?It is known that network coding helps enhance various performance metrics in networked systems. However, to take advantage of network coding, especially random network coding, low encoding/decoding latency must be guaranteed. In this paper, we introduce a fast random network coding parallelization technique targeting multicore processors.

Keywords?Network Coding; Parallelization; Multi-core

I.  INTRODUCTION Network coding [1], especially random network coding [2],  is known to enhance various performance metrics in computer networks. For example, random network coding can reduce file downloading delay in Peer-to-Peer (P2P) file sharing systems [3]. To take full advantage of random network coding, however, it is vital to guarantee low latency in the decoding process. In network coded systems, the data is encoded before being transferred at the source and thus the received data at a destination has to be decoded to recover the original data. The decoding process is implemented as a variation of the Gaussian elimination which is quite expensive when the data size is large.

The time overhead spent for decoding may cancel out all the benefits of reduced transmission time in network coding unless decoding latency is sufficiently low. To address this, a number of works studied on reducing the decoding latency of random network coding. Parallelized decoding techniques for multi- core processors have been proposed in [4, 5]. Also, it has been shown that parallel decoding using General Purpose Graphics Processing Unit (GPGPU) such as CUDA [6] can radically enhance the decoding speed of network coding [7, 8, 9, 10].

However on embedded systems such as sensor network nodes, it is usually not possible to leverage GPGPU technology since GPUs are not available and thus full exploitation of CPU?s capability is the only way to improve system performance. In this paper, we focus on enhancing parallel random network coding performance on multi-core processors and introduce a random network coding parallelization technique that can take full advantage of multicore processors.



II. ENHANCING CONCURRENCY IN PARALLEL RANDOM NETWORK CODING  A. Random Network Coding In random network coding, a set of coded packets generated  from the original file is transmitted towards destination nodes.

To this end, the original file at the source is divided into a number of blocks. We use    to denote kth block. A coded packet     is a random linear combination of the original blocks.

That is     ?          , where n is the number of blocks and the coefficient     is a randomly chosen element in a finite field  F [2]. The coefficient vector    = [   , ?,    ] is transmitted along with the coded packet    stored in the header. For a destination to be able to decode a set of received coded packets and recover the original file, it needs to collect at least n coded packets with coefficient vectors linearly independent to each other. Let say a destination has collected n coded packets, and let     [           ]         [           ] and       [            ] . As the relationship among the coefficient matrix C, the coded data block matrix E, and the original data block matrix P can be expressed as     , the destination can recover the original data P by multiplying the inverse of E to C. To calculate P = E-1C, a variant of the Gaussian Elimination (GE) called progressive decoding is widely used in network coding. The conventional GE requires collecting n coded packets and having the n ? n coefficient matrix before starting the process. Whereas in progressive decoding since the time gap between the arrivals of the first transfer unit and the last one can be very large, instead of waiting all the required coded packets to arrive, partial decoding is performed on reception of each coded packet [4].

B. Role Division Progressive Decoding Current existing parallelization techniques following the  progressive decoding approach divide each row in the augmented matrix containing both encoding vectors and coded data blocks into a number of equal-size chunks and allocate them to a number of homogeneous threads, i.e., threads performing the same operation, for parallel decoding of the chunks. The size of each partition in a row can be the same for every row [4] or dynamically chosen based on the row number [5] in the matrix. The main problem in this approach is that there exists dependency among the threads. Although each thread is working on its own data chuck, the threads are not fully independent in that synchronization among all the threads is inevitable at the end or at the beginning of each stage of the progressive decoding. Therefore threads that have finished earlier their works must wait for others to be finished.

Addressing these problems, our proposal called Role Division Progressive Decoding (RDPD) aims minimizing dependencies among concurrent threads in the progressive decoding by using heterogeneous threads. In RDPD, two different types of threads, supervisor and worker, work on two independent matrices, the encoding vector matrix and the coded data block matrix: the supervisor thread runs the progressive GE on the encoding vector matrix and the worker threads work on the coded data blocks matrix governed by the supervisor thread. While conducting the progressive GE, the supervisor thread produces and stores into a work-queue a  This work was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT & Future Planning (2013005876)     sequence of work orders each of which informs the worker threads how to manipulate a row in the data matrix, i.e., coded data block. The sequence of work orders in fact is a decomposition of the progressive GE on a specific matrix into a set of atomic row operations. The content of a work order includes the type of operation, the multiplicand row (dividend row), the multiplier (divisor) and the result row. There are two types of operation: subtraction after multiplication and division. The first type of operation, subtraction after multiplication, defines the following calculation: the results row is updated by subtracting the multiplicand row multiplied by the multiplier from the result row. The second operation, division, is dividing the result row by the divisor.

Worker threads perform actual decoding of coded data block. Each worker thread performs calculations on a specific chuck of coded data block following the sequence of work orders produced by the supervisor thread. A coded data block is divided into a set of non-overlapping chucks such that each worker thread works independently on a specific region. To retain independency among threads, each thread maintains its own work-queue pointer. Owing to this work-queue based inter-thread communication scheme, RDPD can minimize dependency among threads and maximize concurrency in the parallel progressive decoding. The supervisor thread needs not wait for worker threads and can work on newly arrived packets as soon as they arrive and worker threads need not wait until other workers to progress to the same stage.



III. PERFORMANCE EVALUATION We show RDPD?s performance advantage over existing  schemes via real experiments on an Intel i7-960 CPU (quad- core, 3.2 GHz) based platform. The metric we use to compare is decoding throughput (Mbytes/sec) defined to be the total size of data being decoded divided by the time spent for decoding.

The size of the file being decoded varies with the generation size, i.e., the number of blocks comprising a file, and block size used in the experiments. If the generation size is 2048 and the block size is 16384 bytes, then the total data size is 2048 * 16384 = 32 Mbytes. Figure 1 depicts the decoding throughputs of the four schemes with the generation size varying from 16 to 512 and the block size varying from 1024 to 8192 bytes. We denote the scheme proposed in [4] as SP, the scheme proposed in [5] as DVP and the sequential implementation as SQ. In the figure, x-axis and y-axis represent the block size (bytes) and throughput (MB/sec) respectively. When the generation size is 16 and the block size is 1024 bytes RDPD shows around 1.7 times higher throughput than SP and DVP. Compared to the sequential version (SQ), RDPD achieves speed-up of 3.31 whereas DVP/SP only shows speed-up of 1.9, meaning that quad-core CPUs are underutilized with DVP/SP. The performance advantage of RDPD compared to SP and DVP becomes less dominant as either the generation size or block size gets larger. When the generation size is 64 and the block size is 8192, RDPD shows around 15% enhancement over SP and DVP. The speed-up ratio of RDPD over the sequential implementation (SQ) is 3.62, meaning the quad-core CPU is utilized in nearly full capacity.

(a) Generation size of 16/32   (b) Generation size of 64/128  Figure 1. Experimental results

IV. CONCLUSION  In this paper, we have introduced a fast parallel random network coding scheme and showed its performance advantage.

