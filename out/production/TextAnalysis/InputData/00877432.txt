Approximate Clustering In Association Rules  Lawrence J. Mazlack

Abstract Data mining holds the promise of extracting unsus-  pected information from very large databases. A difficulty is that ways for discouvery are often drawnfrom methods whose amount of work increase geometrically with && quantity. Consequentially, the use of these methods is problematic in very large data bases. Categorically bared association rules are a linearly complex data mining methodology. Unfotunately, rules formed from categori- cal data often gneerate many fine grained rules. The con- cem is how might fine grained rules be aggregated anri the role that non-categorical data might have. It appears that soft computing techniques may be useful.

1. Introduction  Data mining is an advanced tool for the management of large masses of data. Data mining is the process of secondary analysis of large databases. It is aimed at di- couvering previously unknown relationships of value.

Data mining is secondary analysis because the data were not collected to answer questions now posed. The data is examined in an attempt to discouver if there m patterns beyond those that were hypothesized before the data was collected. For example, perhaps we are at look- ing at long distance telephone call records. The records were originally collected for billing. Now, they can be examined to recognize calling patterns involving such things as: call length, time-of-day, customer calling plan, from where-to-where, etc.

There are several different strategies of performing data mining. One approach is to use classic machine learning methods, such as entropy based learning. How- ever, most classic machine learning techniques are com- putationally complex and are ill-suited to completely consider a large data set. Also, most machine learning results are not well suited for use by most human cli- ents. This is less than ideal as a goal of data mining is to help human analysts.

Consequently, data mining techniques suitable for the analysis of large databases have been developed. Some data mining techniques focus on the discouvery of simple rules. Of these, association rules are prototypical and the easiest to understand.

Association rules formed from categorical data often generate many fine grained rules. This work?s concern is an exploration of how might fine grained rules be aggre- gated and the potential role of non-categorical data.

12. Association Rules Association rules [Agrawal, 19931 meet a necessary?  data mining subgoal of having efficient data structures and algorithms. Their algorithms also have the advantage of being linearly upwardly scaleable.

The process of discouvering association rules is often based on Apriori [Agrawal, 19941. It is an unsupervised discouvery process. However, there is often a predefined hierarchical concept tree that is applied to cluster the data in preparation for mining; this is a form of supervised discouvery. For example, all brands of beer might be grouped into a single class called: ?Beer.? Essentially, hierarchy trees help cluster the data for analysis and re- duce the number of attributes. If some clustering is not somehow accomplished, there will be a multitude of fine grained, relatively infrequent rules.

Predefined hierarchy trees may not serve the purposes of data analysis well. Perhaps, implicit in the data, there may be more useful hierarchy trees that go undiscouvered because of the pre-processing assumptions. As with much real world data, the clusters that hierarchy trees represent can possibly be best formed by using fuzzy sets; or alternatively, rough sets.

It is a speculation of this work that it might be better to first identify fine grained rules, then aggregate them.

;This would percome one difficulty with association rules is that they may be overly fine grained. It may be possible to increase the granularity of the discouvered association rules either through (a) the use of dmouvered concept hierarchies, (b) soft clustering the rules them- s,elves, or increasing the granularity of the data before forming association rules.

Association Rules represent positive associations be- tween attributes. Most commonly, the rules are devel- oped from categorical data that is expressed as a 0/1 ma- trix. For example, consider Figure l.

Parts of this work were performed while the author was a visiting at BISC, Computer Science Division, EFXS Department, University of California, Berkeley  Efficiency is necessary because realistic databases may well con- tain millions of records. Less efficient algorithms often resort to stratified sampling. However, may become less satisfactory as the volume of records increase.

0-7803-6274-W00/$10.00 O 2000 IEEE 256  mailto:mazlack@uc.edu   Figure 1. Example transactions  This resulting tabular form, t(A), is the one often used in developing association rules. Notice that: (a) Regardless of initial magnitude, t(A) values are repre-  sented either by categorical values of a ?0? or a ?1?; for example, for t,, the magnitude of chips is ?1? while for t,, the transactional magnitude is ?2?. How- ever, in the t(A) matrix, the actual magnitude of chips is lost and both f,,chjp and t2,chjPs is set to ?1?.

(b) There can be implicit hierarchy trees; for example, in t,, Miller (an American beer brand) is purchased and in t,, Tuberg (a Danish beer is purchased. When these transactions are present in t(A), they are both repre sented as ?1?s as tj,brcr. Further, in t,, both Miller and Tuberg are purchased; and, they are represented by a single ?1? as t3,beer.

After the categorical data matrix is developed, the strength of association between term is determined.

Some restriction on result volume is useful lest too many rules to be examined by a human analyst may be developed. One way to do this is to only find association rules with a high level of support; i.e., occur with a frequency above a specified threshold. A potential prob- lem with thresholding is that rules of great interest, but with only moderate support might not be captured. This is particularly true in large, relatively sparse matrices.

The format of an assoication rule is: When <event,>, in <confidence factor7 will also <event,>; this occurs in <support> of cases. Where  Support: relative occurrence Conwence: degree rule true across individual cases  When a customer buys beer & sausage, For example:  in 80% of the cases, he will also buy mustard; this occurs in 15% of cases  Several efficient algorithms for mining categorical asso- ciation rules have been published [Agrawal, 19941 [Mannila, 19941 [Toivonen, 19961.

4. Quantifying Association Rules: In practice, the information in many databases is not  limited to categorical attributes; but usually also con- tains quantitative data. It is a possibly undesirable over- simplification to translate quantitative data into categori- cal data. It may over-simplify results when data magni- tudes (or, quantities) are lost. For example, should the purchase of two units of mustard be considered the same as the purchase of one unit? If not, how can magnitudes be best handled? There are at least two kinds of informa- tion that might be obscured: differences of behavior due to differing quantities; and, trends.

4. 1 lncreasing Data Granularity While Forming Quanti- fied Association Rules Several strategies have been pursued to deal with sca-  lar, non-categorical data by incorporating quantification into the mined association rules. For the most part, they have attempted to increase the granularity of the data before forming the association rules.

Srikant and Agrawal [Srikant, 19961 mapped the quantitative association rules? problem and categorical rules into a Boolean association rules? problem, thus extending the work started in Agrawal[1994]. They dis- cussed mining association rules over quantitative and categorical attributes. They dealt with quantitative attrib- utes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. For example, a rule might contain quantified age range: Age = [30,39] and a resulting Boolean rule might be:  They introduced a measure of partial completeness that quantifies the information lost due to partitioning. This measure is used to decide whether or not to partition a quantitative attribute and the number of partitions  Cai [ 19991 considers transactions with quantities as supporting ?weighted? rules. Cai wanted to balance be- tween weight and support, believing that a separation of the two might ignore some interesting knowledge. For example, a less frequently occurring or supported item set might still be interested if:  An item is under promotion or if it is unusually profitable.

If an item is not considered very important in terms of the weights of its item sets, but it is very a popular item in that many transactions contain it.

Aumann [1999] provides a Merent definition of quantitative association rules. It is based on statistics.

They look for events that differ significantly from a  (<Age: [30,39] > and <Married: Yes>) <NumCars: 2>     statistical norm and were then considered to be interest- ing. They consider that the best description of for a set of quantitative values is its behavior is its distribution. The approach is to look for a subset and its mean (or medlan, variance) and compare it to the mean of the whole. The idea is to identify a population subset that presents ?interesting behaviour.? Aumann finds a type of rule that is not found by other methods. For example, when:  Sex =female the female mean wage of $7.90/hour is interesting when overall mean wage = $9.02  Aumann handles both quantitatively based and caterogi- cally based rules, such as  age E [20,40] 3 Height: mean = 165 cm  Liu [ 19991 presented a statistical approach for quanti- tative mining. It is supervised search. The user specifies the target attribute(s). Liu is concerned that typically a large number of associations are found; particularly when the attributes are hghly correlated. To manage this, they first prune the number of associations; then find special subsets of the remaining rules. The chi-square test was used instead of a minimum confidence measure.

Another statistical approach has been presented by Yao [1999].

4.2.2 Soft Methods There is a considerable history applying either fuzzy  or rough set techniques to databases. To name a few ap- proaches, there are: databases of fuzzy values, fuzzy query rules, and rough set partitioning. Both fuzzy and rough sets have been used in various data mining efforts. Our focus is on the relatively unexplored area of rule aggrega- tion, the lightly explored clustering antecedents and/or consequents, and the role of quantification.

Chan [ 19971 presents an algorithm for increasing data granularity that eliminates the need for user-supplied thresholds for support and confidence, and to find nega- tive as well as positive association rules. Using fuzzy set theory, linguistic terms are used to find the degree to which they characterize records in the database.

Another approach was suggested by Fukuda [1996a,bl. It focused on computing two optimized ranges. One that maximizes the support on the condition that the confidence ratio is at least a given threshold; and, another that maximizes the confidence ratio on the condition that the support is at least a given threshold.

They mined association rules of the form  They used techniques from computational geometry (convex hulls). They proposed algorithms that discovered optimized gain, support and confidence association rules for two classes of regions.

( A E [ V , >  %I) - c  Zhang [ 19991 extended the equi-depth partition algo- rithm to mine fuzzy quantitative association rules. They built at times on the Apriori algorithm. They considered 1? + X ,  where an item <a,v> represents either a crisp value, an interval (if numerical), or a fuzzy term and a is an attribute with value v. If v is fuzzy, the item is fuzzy.

They use a minimum support for each attribute. The super-candidate technique [Agrawal, 19941 is used.

Kuok [1998] mines fuzzy association rules to avoid either ignoring or overemphasizing the elements near the boundaries in the mining process. After partitioning the attribute domain as suggested by Srikant [ 19961, Kuok suggests: Z f X  is A then Y is B where X, Y are a set of attributes and A, B are fuzzy sets which describe X and Y respectively. A user-supplied threshold is used to test each side of the rule as to being ?satisfied.?  Other efforts at association rules with fuzzy antece- dents and consequents include Au [1998, 19991, Fu [ 19981, Lee [ 19971. Au and Chan [ 19971 are working on the same problem. Fu?s work is closely related to Kuock [ 19981. Du [ 19991 has a somewhat different approach to fuzzified ranges. Somewhat related is the work on in- complete data of Ng [ 19981 and Ali [ 19971.

The difficulty with all the methods discussed in sec- tion 4 is that their complexities are expensive.

5. Frequent Sets An extension of simply forming association rules by  counting is the technique of frequent sets [Mannila, 19941. Because many matrices of interest are sparse, fkequent sets help focus on the development of heuristi- cally more interesting rules. (The heuristic is that pat- tems that occur more frequently are the more interesting.)  An example of a sparse matrix of interest is a collec- tion of grocery store transactions. Grocery stores typi- cally have well over 10,000 distinct items for sale. Cus- tomers typically purchase less than fifty items during a store visit. Combinations of items that are purchased together often are the most interesting. For example, perhaps when a customer purchases bananas (the most often purchased grocery store item), there is a good chance that she will also purchase milk and corn flakes.

The rules developed through the frequent sets algo- rithm may still be fine grained. Unfortunately, the ante- cedentlconsequent data clustering methods discussed in section 4 appear to be infeasible for frequent sets. In- stead, an approach focusing on rule aggregation or on initial rule reduction would be better.

6. Reducing Association Rules Generated If all possible association rules are developed for high  dimensional data (data with a large number of attributes), the number of rules may be too extensive to be useful.

The upper boundary for the number of association rules for a data set is (where n, is the number of attributes):  I =I Jd  The number of associations would be much greater if attributes can take on multiple values and if a distinct association rule is developed for each value.

Four techniques can be used to reduce the count of generated association rules: limiting rule dimensionality, using a support level, reducing quantification, applying concept hierarchies. Using a support level is a part of many methods already. We need to seek elsewhere.

7.1 Reducing Rule Complexity If there are n, attributes, only consider association  rules of maximum length m. This reduces the theoretical upper boundary of the number of association rules to:  &;$c;*),2 14 J=l  While producing rules that are more understandable, the count of rules is still too large.

If high support is used as rule interestingness, high support will almost always imply shorter rules. How- ever, you cannot know their maximum length be unless there is a limit on length. Also, it is possible that a sliding scale of support might be used. That is, less sup- port for longer rules. In any case, it might be necessary to experimentally determine what a useful support level might be for a particular data set and/or application. No particular number is written in stone.

Another approach to quantification that might be use- ful would be to granulate values, then apply q u a n ~ e d association rule techniques. For example, if academic grades are recorded from 100 to 0, they often are granu- lized into A, B, C, ... . Possibly the mountain method [Yager, Filev, 19941 might be applied to form the gran- ules. The mountain method may also me useful in clus- tering association rules together.

7.2 Concepty Hierarchies It is unclear whether hierarchy trees should be part of  the initial formation of the transaction matrix (categorical or non-categorical) or dealt with later. For example, should all beer brands be grouped together as a single class under all cases? Or, should they be presented separately?

Consider Figure 1.  Should Miller and Tuberg be grouped together into a single class: beer? Should all beverages be grouped together; i.e., should Miller, Tu- berg, and Coke be grouped together? Can items be grouped together depending on context; i.e., perhaps  sometimes beers should be in one group and Coke in another; and, sometimes they should all be together (e.g., ?picnic supplies?). Is there some way of computa- tionally deciding what should be grouped together?

Extending the example further: Consider the case where we have more beer brands, say: Budwiser, Miller, Sam Adams, and Tuberg. Most regular beer drinkers would agree that there is a substantial taste difference between the four of them. (There may also be price differentiation.).

Should Budwiser, Miller, Sam Adams and Tuberg be grouped into a single category; or Should each be grouped separately; or, Should Budwiser & Miller form one group while Adams & Tuberg form another group?

Currently, grouping is done by a human expert. This is not entirely satisfactory because (a) potentially useful groups may go unrecognized and (b) the expert may make an error.

Concept hierarchies group similar attributes together.

Intuitively, they are desirable. However, the question of how to computationally recognize them is the question.

For example, various types of wine might be all be placed into to concept of ?wine.? For example, if we have six wines?:  Meie? alcohol-free3 Burgundy4 Carminet? alcohol-free Chardonnay6 Gallo? Burgundy? Chalone? Pinot Noi?

Almadin? Chardonnay6 Carneros5 Chardonnay6 If the transaction set is as shown in Figure 2: It  might be possible to recognize that the premium, alco- hol wines can be grouped together. They are all associ- ated with both green grapes and brie (a premium cheese).

Similarly the basic alcohol wines can be grouped to- gether (they are all associated with basic cheddar cheese).

However, if a pre-defined hierarchy was used to group all wines together, the association between type of wine and type of cheese would not be discouvered; and, the following would result. This would result in loosing the information that premium wines are related to the purchase of brie and green grapes. Why might this be important to a store? Well, if you stop carrying green grapes or brie, you may also lose your premium wine customers.

? As defined by their label. * A ?basic? wine extracted  ? A ?premium? wine  Alcohol free wines are normally fermented, then their alcohol is  A red wine  A white wine     tl t 2 t 3 t 4 ts  e7 t a t 9  t6  Figure 2. Wine and cheese transactions f o r  items of difering perceived quality.

green brand D'Arc cottage free free Gallo Pinot Almadin Carneros grapes cheddar brie cheese Burgundy Chardonnay Burgundy Noir Chardonnay Chardonnay  0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0    A human observer might not be able to easily rec- ognize that the premium, alcohol wines can be grouped together. They are all associated with both green p p e s and brie (a premium cheese). Similarly the basic alcohol wines can be grouped together (they are all associated with basic cheddar cheese). (Computationally, it might be possible to do this.) However, if a predefined hierar- chy was used to group all wines together, the associa- tion between type of wine and type of cheese would not be discouvered; and, the Figure 3 would result. This would result in loosing the information that premium wines are related to the purchase of brie and green grapes. Why might this be important to a store? Well, if you stop carrying green grapes or brie, you may also lose your premium wine customers.

tl t 2 t3 t 4 t5  t 7 t a t 9  t6  green brand D'Arc cottage grapes cheddar b r i e  cheese wine 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1  0 1 0 1 0 0 0 1   0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1   Figure 3. All wine grouped together It gets even worse if all of the cheese varieties are  grouped together as well, as in Figure 4. Now, the analysis would tell us that any cheese and any wine am associated. These results are clearly over-generalized.

green  1 1 1 1  Figure 4. All cheeese and wine grouped  A hierarchy such as: items E food E wine can be many levels deep. There are essentially two ways of establishing hierarchies: (a) have a human predefine  toge ther  them or (b) learn them. There are a number of different ways that they can be learned.

As part of a rough-set based approach, Han [1992] used crisp concept hierarchies provided by the user be- fore the initiation of data mining. Alternatively, Mazlack [ 19971 developed and tested a methodology to learn useful concept hierarchies while partitioning data- bases. Potentially, the same methodology could be ap- plied here. Questions that need to be answered are:  How to best recognize intermediate level useful hi- erarchical categories. For example, in a grocery store, how do we learn and choose between:  items E food, non-food, food E ... cheese, wine, ...

wine E ... no-alcohol, basic, premium ...

The role of quantification in recognizing useful hi- erarchical categories  8. Summary Data mining holds the promise of extracting new in-  formation from very large databases. One of the difficul- ties of data mining is that traditional ways of discouver- ing new information from data are largely drawn from classic machine learning methods. The work involved in these methods increases geometrically with the amount of data considered. Consequentially, the use of these methods is problematic in very large data bases.

Association rules are a linearly complex, user under- standable way of doing data mining. The results are particularly useful when it is important that both the Ides themselves and the methodology behind the rules' development are understandable.

