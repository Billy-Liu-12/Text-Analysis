High Performance Data Mining Algorithms and  Similarity models Research

Abstract?For the complex data of multilevel and large volume distributed in different regions, how to seek and find both be interested and useful information is what scientists are devoted to. Existing efficient method of research is an association rule mining of distributed database system. This paper introduced the distributed association rule mining algorithm. By analyzing the Aproiri algorithm, we have proposed Fast Distributed Mining of Association Rule algorithm and Similarity model. The test results show it can effectively improve the mining accuracy rate.

Keywords- data mining; association rule; algorithm; data resource heterogeneous; similarit.



I.  INTRODUCTION The major symbol of informative society mainly represent  in the data which is more and more , at the same time , it is rapidly spreaded and it is also cheaper, and even more people don?t have the time to read all the data. Data capability level has increased rapidly to TB [1]. Excessive data is known as information exploding. The quantity of data is so large that people can't use it easily although it is likely that much valuable information is hiding in it. Then how can we find the potential useful knowledge and make use of it in our decision - making? The resolving method to the problem that computer science gives is Data Mining.

The data is distributed stored in large scale database of different regions, how to seek and find what we interested and useful information is of great significance. An efficient method is proposed, put association rule mining on distributed database system, which focused mainly on the Association Rule Mining Algorithm.

At present, the research has yield some positive results mainly includes: Collective data mining proposed by Kargupta [2], distributed cooperative Bayesian learning strategies proposed by Yamanishi [3], efficient association rule mining in distributed database system proposed by Cheung [4]. There are still many limitations on distributed database mining (DDM), though most DDM techniques have optimized the original model by using varies standards and methods, also, even the isomorphism of data mining has a considerably mature, the Heterogeneity of distributed data resource is far from mature.

This paper presents a Distributed Association Rule Mining model Based on Similarity as a new model for data mining  with the basic idea that: fully considered the heterogeneity of distributed database system, first groups the datasets by its similarity which is obtained from similarity calculating, and then, in the distributed environment, a new association rule of efficient and accurate is proposed making use of existing association rule mining method.



II. ASSOCIATION RULE MINING DEFINTION Association rule mining finds useful correlation among a  large set of data items, it?s popular in data mining and also the basic method.

Suppose I= {I1, I2 ? Im} is a dataset, for a given transaction database D in which each transaction has a unique transaction identifier and a itemsets: Itemsets?I ? Itemsets?, association rule is a form contains the following : X=>Y, in which Itemsets  ? X?Itemsets ?  Y?and X?Y=?.

Association Rules X=>Y's confidence, support, expected confidence and lift are defined as follows:  Definition 1: If transaction database D includes s% percent transaction  supported both datasets X and datasets Y at the same time, then the s% is called the support of X=>Y.

The support of the rule X=>Y is defined as a percentage of the transaction database included both X and Y, and described as an appearance probability of the union of itemsets X and Y.

Definition 2: If there are c% of what included in itemsets X is also  contained in itemsets Y, then c% is called the confidence of rule X=>Y.

The confidence of rule X=>Y is defined as the percentage both contained in itemsets X and itemsets Y. The confidence figure out what the appearance probability of Y in transactions which itemsets X is appeared.

Definition 3: In transaction database D, if there are e% of the transactions  support itemsets Y, then e% is called the expected confidence of rule X=>Y.

This work has been supported by Basic Research Development Fund of China (973 Program, Grant no. 2006CB705505 )   DOI 10.1109/CSSE.2008.1359     The expected confidence of rule X=>Y defined as the percentage of the transactions which included in D, it?s describes the appearance probability of itemsets Y in transactions on which there are no effects of any conditions.

Definition 4: If the confidence and expected confidence are coexist in  transaction database D, then the ratio of them is a lift.

The lift of rule X=>Y defined as a ratio of confidence and expected confidence, it describes what the effective of itemsets X?s appearance on the appearance of Y, the more it is the more effective is.



III. APRIORI AND APRTIDREC ALGORITHM  A. Apriori algorithms Association rule mining is to find the association rule of  minimal support degree and confidence given by users in transaction database D. The major task of Apriori algorithm is finding out all frequent itemsets in database D.

Distributed data mining algorithm is mainly divided into two types: Count Distribution algorithm (CD algorithm) and Data Distribution algorithm (DD algorithm).

CD algorithm is a typical parallel algorithm based on Apriori. First, each division scan database independently, to calculate the corresponding support, then add all the support value to get the whole support value, if the value more than minsup, we considered the item is global. CD algorithm needs a synchronization point after each scanning, then scan the next.

But as the increasing of the nodes may cause the increasing of communication volume of CD algorithm, therefore, CD algorithm can not be a distributed algorithm with well expansibility.

DD algorithm has a basic thinking that uniform distribution of candidate sets of each node, the processes can reduce the number of scanning candidate sets when the nodes increase, thus to improve the parallel efficiency of algorithm. Its disadvantage is each node needs data transferring and if the node number is increase the efficiency of the algorithm is reduce, so DD algorithm can take a role of parallel mining rule algorithm under undistributed environment only.

B. Modified AprTidRec Algorithm To the deficiency of Apriori algorithm, many improved  optimized algorithms are proposed such as AprioriTid and AprioriHybrid. These algorithms improve the executing efficiency from the following three points:  Try to reduce the number of database scanning Try to reduce number of candidate sets generated or cancel generation of the candidate sets.

Try to use a new access structure  In this paper we achieve the goal of algorithm optimization mainly by using record storage structures to reduce database scanning. The basic thinking is similar to Apriori algorithm, and what difference between them is the former contain connection step only rather than the connection step and pruning step which included in the latter. First define a record  storage structure tidRec for each candidate sets, the record tidRec of I is consists of I?s TID stored in database, presents the tidRec of I by I.tidRec which can find out though searching transaction database. Record storage structure of algorithm is <I, tidRec, count>, presents the support of itemsets as I. count, its value is equals to the length of tidRec, namely count=|tidRec|. When to generate a K items candidate sets from a K-1 frequency itemsets, the tidRec of K itemsets and its support can be obtained through an intersection between two tidRec of two K-1 itemsets.



IV. FAST DISTRIBUTED MINING OF ASSOCIATION RULE ALGORITHM  A. Distributed Environment of FDMAR Algorithm In order to reduce communication volume between  divisions, D.W.Cheung revealed the useful relationship between the distributed datasets and centralized datasets as well as provided a Fast Distributed Mining of Association Rule (FDMAR algorithm). This algorithm first executes the Apriori algorithm and find out local itemsets of each division, then exchange the support degree total between the local itemsets of each node. By generating less candidate sets, the communication volume has largely reduced while mining the association rule. Fig.1 shown.

Figure 1.  Distributed environment of FDMAR algorithm  The algorithm includes the following steps, of which the distributed mining symbols shown as Table ?.

TABLE I.  THE SYMBOLIC REPRESENTATION  D Transactions in DB S  The minimal support degree minsup L<k> Global K- datasets CA<k> Candidate datasets generated from I<k>

X.sup Global support totals refers to X Di Transaction numbers in DBi GLi<k> Candidate datasets generated from GLi<k-1> LLi<k> Local K-1 datasets in CGi<k>

X.supi The sum of local support totals at node Si GG<k> Candidate datasets generated from GLi<k-1>  B. Generation of FDMAR Candidate Datasets A relationship existed between frequency itemsets and  distributed database node, each global frequency itemsets must be a local frequency itemsets in a very region. Assuming that an itemsets X is both global and local frequency itemsets in a region Si, calls X in Si is global. All global itemsets included in a region as this region?s datasets resource, and each datasets resource is the start point of FDMAR algorithm.

Two characteristics of local frequency itemsets and global frequency itemsets are: firstly, if X is local, its whole subset is local; secondly, if X is global, its whole subset is global too.

C. Local Pruning of Candidate Datasets of FDMAR The basic thinking of local spurning to candidate datasets  is: for each node, if a datasets is not global in node Si which means no necessary to judge whether it is global by calculating its global support degree total. Therefore, node?s global K- datasets can be work out on each iterative layer as following steps:  The first stage of generation of candidate itemsets: generate a K- candidate datasets by using of improved AprTidRec algorithm according to a global datasets which is generated from K-1 times? iterative calculations of node Si. As using the improved AprTidRec algorithm, local pruning is omitted.

The second stage of exchange of support degree total: broadcasting the generated candidate elements of the local K- datasets so as to collect the sum of support degree totals and calculate the global support degree totals, and then get the global K-1 datasets in Si.

The third stage of broadcasting mining result: calculate and find out the global K-1 datasets and broadcast them to other nodes.

D. Global Pruning of Candidate Datasets of FDMAR After local pruning and broadcasting the total degrees of  local support and global support, we can execute global pruning during the following iterative.

Assuming that after some iteration, the system catch a global candidate datasets then will broadcast the local support total of each candidate elements to other nodes automatically.

And X is a candidate datasets with a value k which was iterated k times, thus each node has received a local support total in X of subsets size of k-1. For a branch database DBi, maxsupi(X) represents the entire minimal local support total of which the subset is k-1 in X. As the relationship between superset and subset, the bonding function of local support total X.supi is maxsupi(x), the sum of bonding functions is presented as maxsup(X), also the upper bound of X.sup, namely X.sup? maxsup(X)= ? maxsupi(X), if maxsup(X)<s*D (s is the minimal support degree, D is the total of transactions in database ), X won?t be a candidate datasets.

E. Totals Polling After the spurning stage included local and global spurning  FDMAR executes totals polling at each node Si by the following steps:  The first stage, sending the candidate elements to polling nodes: assuming for each polling address Sj at node Si seek out all candidate elements both belongs to sets LLi (k) and has a polling address Si, then store them to sets LLi (k), the same to local support totals, finally sent to corresponding polling address Sj.

The second stage, polling and collect support totals: if Si is a polling address, it will receive all sets LLi (k) from other nodes, for each received sets X, Si first find an original nodes  table which were sent out X, then broadcast polling request to nodes not existed on the table. So as to collect the support totals.

The third stage . calculating global datasets: Sj receives support totals from other nodes and calculate the totals, seek out the global datasets, finally broadcast totals.



V. DISTRIBUTED ASSOCIATION RULE MINING MODEL In distributed systems, for isomorphic database the above  distributed association rule mining can reach a fabulously correct level, provides useful information for other modules in application system. There are still a large portion of databases is heterogeneous, causes a lot of troubles for distributed association rule mining, and it can just solved by distributed association rule mining.

Similarity degree is a significant conception in data mining and knowledge discovery. In order to obtain regularity of data the describing of diversity among data is needed. In the past years some methods for object have existed such as attribute similarity and database similarity. This paper discussed a new similarity method, namely to calculate information entropy and calculate the support degree of distributed association rule to realize the optimization model.

Let A and B be frequency itemsets, and B is not contain A, present A as a maximal frequency itemsets MFA.

Set A and B be two similar datasets, then  MFA = {(A1, CA1), (A2, CA2)? (Am, CAm)}  MFB = {(B1, CB1), (B2, CB2)? (Bm, CBn)}  With Ai as the maximal frequency itemsets in A, CAi as the relative support degree in A, and the same, Bi as the maximal frequency itemsets in B, CBi as the relative support degree in B.

And 1?i?m, 1?j?n.

Then the similarity formula between A and B is shown as:   32),( II  I BASim  + =   of which,  },min{) || ||  1log( || ||  , 1 ji  ji  ji  ji ji  ji CACA AA AA  AA AA  I ? ?  ? ?  +=?   },min{) || ||  1log( || ||  , 2 ji  ji  ji  ji ji  ji CBCB BB BB  BB BB  I ? ?  ? ?  +=?   },min{) || ||  1log( || ||  , 3 ji  ji  ji  ji ji  ji CBCA BA BA  BA BA  I ? ?  ? ?  +=?

VI. EXPERIMENTAL TEST In this paper we assuming there are three nodes divided the  system into three regions DB1, DB2, DB3. one hundred and fifty transactions have included in this database, three local databases each contain fifty transactions, meanwhile assuming the minimal support degree as s=10% and let L(1)={A,B,C,D,E,F,G,H} be a big 1- datasets (which calculated by once iteration), in which A,B and C are local  at the notes, namely that (A),(B),(C) are local 1- frequency     itemsets at node S1, the same to node S2 and B,C,D are local, and for node S3 the E,F,G,H are local.

Iterating the second time, and node S1, S2, S3 generate the candidate itemsets, the itemsets is respectively presented as CG1 (2)= {AB?BC?AC}, CG2 (2)={BC?AD?CD}, CG3 (2)={EF?EG?FG?FH?GH}. Before calculating the global 2- candidate itemsets, find out local support total. Results were shown in Table ?.

TABLE II.   2-ITERSETS? GENERATION OF 2-FDMAR ALGORITHM

X.sup1 X.sup2 X.sup3 AB 5 BC 10 EF 8 BC 10 CD 8 EG 3 AC 2 BD 4 EH 4  FG 3 FH 4 GH 6  Table ? indicates AC is not global as AC.

sup=2<10%*50=5, so candidate elements at node S1 will be deleted after spurning, because BC and AB respectively has a big enough local support total, they can still exist in candidate itemsets after local spurning. Thus LL1(2)={BC,AB}, LL2(2)={BC,CD}, LL3(2) ={EF,GH}.

We obtain the respectively association rule of each node after local spurning rather than broadcast each element of candidate itemsets. The association rule from S1?S2?S3 shown in Table ?.

TABLE III.  LOCAL ASSOCIATION RULES  rules at S1 rules at S2 rules at S3 Rule 1:  A=>B(10%,83.3%) Rule 2:  B=>C(20%,100%) Rule 3:  A^B=>C(10%,80%)  Rule 1: B=>C(20%,100%)  Rule 2: C=>D(20%,66.7%) Rule 3:  C^D=>B(12%,75%)  Rule 1: E=>F(16%,100%)  Rule 2: G=>H(12%,85.7%)  Rule 3: E^F=>G(12%,75%)  Then broadcast each element of candidate itemsets to other nodes, get the sum of global support totals. Show as Table ?.

TABLE IV.  DATA EXCHANGE SITUATION OF THE NODES  Local candidate element  Broadcasting resource X.sup1 X.sup2 X.sup3  AB S1 5 4 8 BC S1,S2 10 10 2 CD S2 4 8 4 EF S3 4 3 8 GH S3 4 4 6  After this time iteration, only BC is global at S1 as BC.sup= (10+10+2)=22>10%*150=15 while AB.sup=(5+4+4)=13<10% *150=15, for node S2 the global 2- datasets is GL1 (2) ={BC}, simultaneously GL2(2)={BC,CD},GL3(2)={EF}. By broadasting the global datasets each node returns the global 2- datasets L(2)={BC,CD,EF}, further get local support totals at each node of subsets, which described as Table ?.

After global spurning and totals exchanging, finally get a nonempty subset {B}?{C}?{E}?{F}?{BC}?{CD}? {EF}, so get the Table ?.

TABLE V.  GLOBAL SPURNING  IN FDMAR  1- itemsets The sum of  local support totals of node S1 X.sup1 X.sup2 X.sup3 A 6 4 4 B 10 10 5 C 4 12 5  TABLE VI.  GLORAL ASSOCIATION RULES  Gloral association rules New global association rules Rule 1.  B=>C (14.6%, 88%) Rule 2.  C=>D (10.6%, 75%) Rule 3.  E =>F (10%, 75%)  Rule 1.  B=>C (20%, 100%) Rule 2.  C=>D (12%, 75%) Rule 3.  E =>F (12%, 75%)  Table ? is indicated S1 and S2 have a high relevance while S1 and S3, S2 and S3 respectively have a low relevance.

In the fifth part, the similarity formula has verified the above conclusion. So group S1 and S2, S3 as another group, repeat the FDMAR algorithm and obtain the association rule processed by similarity degree. Shown as table 6.

From the comparison of Table ?, improvement can be found according to original association rule, that?s provide a new method in different fields in the future, to mine the information which command a high accuracy level

VII. CONCLUSION Data mining technique is developing as the popularity of  large-scale database and the rapid increasing of data quantity.

Association rule mining is an important research direction in data mining. As the distributed systems and distributed databases? development, the association rule mining in distributed environment becoming a significant technique in the application field of association rule. Aiming at the heterogeneous of database under distributed environment, a model refers to distributed association rule mining based on similarity is proposed in this paper, fully considered the support in association rule which effected by the heterogeneous datasets. Collects the high similarity datasets and separates the low similarity datasets trough the similarity computation, then mines the final association rule through association rule mining. The results of experiments shows this method is feasible, what?s more, is significantly improved. It provides an efficient guarantee for high performance data mining.

