An Overview of Multi-Processor Approximate Message Passing

Abstract?Approximate message passing (AMP) is an algorith- mic framework for solving linear inverse problems from noisy measurements, with exciting applications such as reconstructing images, audio, hyper spectral images, and various other signals, including those acquired in compressive signal acquisiton sys- tems. The growing prevalence of big data systems has increased interest in large-scale problems, which may involve huge mea- surement matrices that are unsuitable for conventional computing systems. To address the challenge of large-scale processing, multi- processor (MP) versions of AMP have been developed. We provide an overview of two such MP-AMP variants. In row-MP- AMP, each computing node stores a subset of the rows of the matrix and processes corresponding measurements. In column- MP-AMP, each node stores a subset of columns, and is solely responsible for reconstructing a portion of the signal. We will discuss pros and cons of both approaches, summarize recent research results for each, and explain when each one may be a viable approach. Aspects that are highlighted include some recent results on state evolution for both MP-AMP algorithms, and the use of data compression to reduce communication in the MP network.

Index Terms?Approximate message passing, compressed sens- ing, distributed linear systems, inverse problems, lossy compres- sion, optimization.



I. INTRODUCTION  Many scientific and engineering problems can be modeled as solving a regularized linear inverse problem of the form  y = Ax+w, (1)  where the goal is to estimate the unknown x ? RN given the matrix A ? RM?N and statistical information about the signal x and the noise w ? RM . These problems have received significant attention in the compressed sensing literature [1, 2] with applications to image reconstruction [3], communication systems [4], and machine learning problems [5].

In recent years, many applications have seen explosive growth in the sizes of data sets. Some linear inverse problems, for example in hyper spectral image reconstruction [3, 6, 7], are so large that the M ?N matrix elements cannot be stored on conventional computing systems. To solve these large- scale problems, it is possible to partition the matrix A among multiple computing nodes in multi-processor (MP) systems.

The matrix A can be partitioned in a column-wise or row- wise fashion, and the corresponding sub-matrices are stored at different processors. The partitioning style depends on data availability, computational considerations, and privacy concerns. Both types of partitioning result in reduced storage requirements per node and faster computation [8?16].

Row-wise partitioning: When the matrix is partitioned into rows, there are P distributed nodes (processor nodes) and a fusion center. Each distributed node stores MP rows of the ma- trix A, and acquires the corresponding linear measurements of the underlying signal x. Without loss of generality, we model the measurement system in distributed node p ? {1, ..., P} as  yi = aix+ wi, i ? { M(p? 1)  P + 1, ...,  Mp  P  } , (2)  where ai is the i-th row of A, and yi and wi are the i-th entries of y and w, respectively. Once every yi is collected, we run distributed algorithms among the fusion center and P distributed nodes to reconstruct the signal x. Prior studies on solving row-wise partitioned linear inverse problems include extending existing algorithms such as least absolute shrinkage and selection operator (LASSO) [5] and iterative hard thresh- olding (IHT) to a distributed setting [8, 12].

Column-wise partitioning: Columns of the matrix A may correspond to features in feature selection problems [5]. In some applications, for example in healthcare when rows of the matrix correspond to patients, privacy concerns or other constraints prevent us from storing entire rows (corresponding to all the data about a patient) in individual processors, and column-wise partitioning becomes preferable. The (non- overlapping) column-wise partitioned linear inverse problem can be modeled as follows,  y =  P? p=1  Apxp +w, (3)  where Ap ? RM?Np is the sub-matrix that is stored in processor p, and  ?P p=1Np = N .

Many studies on solving the column-wise partitioned linear inverse problem (3) have been in the context of distributed feature selection. For example, Zhou et al. [17] modeled feature selection as a parallel group testing problem. Wang et al. [18] proposed to de-correlate the data matrix before parti- tioning, so that each processor can work independently using the de-correlated matrix without communication with other processors. Peng et al. [19] studied problem (3) in the context of optimization, where they proposed a greedy coordinate- block descent algorithm and a parallel implementation of the fast iterative shrinkage-thresholding algorithm (FISTA) [20].

This paper relies on approximate message passing (AMP) [21?24], an iterative framework that solves linear inverse problems. We overview the recent progress in under-     standing the distributed AMP algorithm applied to either row- wise or column-wise partitioned linear inverse problems.

The rest of the paper is organized as follows. After review- ing the AMP literature in Section II, Section III discusses the row-partitioned version, and the column-partitioned version appears in Section IV. We conclude the paper in Section V.



II. APPROXIMATE MESSAGE PASSING  To solve large-scale MP linear inverse problems partitioned either row-wise or column-wise, we use approximate mes- sage passing (AMP) [21?24], an iterative framework that solves linear inverse problems by successively decoupling [25? 27] matrix channel problems into scalar channel denoising problems with additive white Gaussian noise (AWGN). AMP has received considerable attention because of its fast con- vergence, computational efficiency, and state evolution (SE) formalism [21, 23, 28], which offers a precise characterization of the AWGN denoising problem in each iteration. In the Bayesian setting, AMP often achieves the minimum mean squared error (MMSE) [24, 29] in the limit of large linear systems. Various extensions to AMP have been considered since AMP was initially introduced. Below, we summarize recent developments in AMP theory and application.

Generalizations of AMP: Recently, a number of authors have studied the incorporation of various non-separable de- noisers within AMP [3, 30?34], generalization of the measure- ment matrix prior [35?39], and relaxation of assumptions on the probabilistic observation model [33, 38, 40]. AMP-based methods have also been applied to solve the bilinear inference problem [41?43], with matrix factorization applications.

Applications: The AMP framework and its many extensions have found applications in capacity-achieving sparse superpo- sition codes [34], compressive imaging [30, 31, 44], hyperspec- tral image reconstruction [3] and hyperspectral unmixing [45], universal compressed sensing reconstruction [32], MIMO de- tection [4], and matrix factorization applications [41?43].

Multi-processor AMP: Recently, Zhu et al. [14, 15] studied the application of lossy compression in row-wise partitioned MP-AMP, such that the cost of running the reconstruction algorithm is minimized. Ma et al. [16] proposed a distributed version of AMP to solve column-wise partitioned linear in- verse problems, with a rigorous study of state evolution.

Centralized AMP: Our model for the linear system (1) includes an independent and identically distributed (i.i.d.) Gaussian measurement matrix A, i.e., Ai,j ? N (0, 1M ).

1 The signal entries follow an i.i.d. distribution. The noise entries obey wi ? N (0, ?2W ), where ?2W is the noise variance.

Starting from x0 = 0 and z0 = 0, the AMP framework [21] proceeds iteratively according to  xt+1 = ?t(A T zt + xt), (4)  zt = y ?Axt +  ? zt?1?d?t?1(AT zt?1 + xt?1)?, (5)  1When the matrix A is not i.i.d. Gaussian, the use of damping or other variants of AMP algorithms such as Swept AMP [35] and VAMP [39] is necessary in order for the algorithm to converge. This paper only considers an i.i.d. Gaussian matrix A in order to present some theoretical results; the theoretic understanding of using AMP in general matrices is less mature.

where ?t(?) is a denoising function, d?t(?) = d?t(?)d{?} is shorthand for the derivative of ?t(?), and ?u? = 1N  ?N i=1 ui  for some vector u ? RN . The subscript t represents the iteration index, T denotes transpose, and ? = MN is the measurement rate. Owing to the decoupling effect [25?27], in each AMP iteration [22, 23], the vector ft = AT zt + xt in (4) is statistically equivalent to the input signal x corrupted by AWGN et generated by a source E ? N (0, ?2t ),  ft = x+ et. (6)  In large systems (N ? ?, MN ? ?), a useful property of AMP [22, 23] is that the noise variance ?2t of the equivalent scalar channel (6) evolves following SE:  ?2t+1 = ? W +   ? MSE(?t, ?2t ), (7)  where the mean squared error (MSE) is MSE(?t, ?2t ) = EX,E  [ (?t (X + E)?X)2  ] , EX,W (?) is expectation with re-  spect to X and E, and X ? fX is the source that generates x. Formal statements for SE appear in prior work [22, 23, 28].

The SE in (7) can also be expressed in the following recursion,  ?2t = ? W + ?  t ,  ?2t+1 = ? ?1E  [ (?t(X + ?tZ)?X)2  ] , (8)  where Z is a standard normal random variable (RV) that is independent of X , and ?20 = ?  ?1E[X2].

This paper considers the Bayesian setting, which assumes  knowledge of the true prior for the signal x. Therefore, the MMSE-achieving denoiser is conditional expectation, ?t(?) = E[x|ft], which is easily obtained. Other denoisers such as soft thresholding [21?23] yield MSE?s that are greater than that of the Bayesian denoiser. When the true prior for x is unavailable, parameter estimation techniques can be used [32, 46, 47].



III. ROW-WISE MP-AMP A. Lossless R-MP-AMP  Han et al. [48] proposed AMP for row-wise partitioned MP linear inverse problems (R-MP-AMP) for a network with P processor nodes and a fusion center. Each processor node stores rows of the matrix A as in (2), carries out the decoupling step of AMP, and generates part of the pseudo data fpt . The fusion center merges the pseudo data sent by all processor nodes, ft =  ?P p=1 f  p t , denoises ft, and sends back the denoised  ft to each processor node. The detailed steps are summarized in Algorithm 1. Mathematically, Algorithm 1 is equivalent to the centralized AMP in (4)-(5). Therefore, the SE in (7) tracks the evolution of Algorithm 1. Note that ap denotes the row partition of the matrix A at processor p.

B. Lossy R-MP-AMP In lossless R-MP-AMP (Algorithm 1), the processor nodes  and fusion center send real-valued vectors of length N to each other, i.e., fpt and xt+1, at floating point precision. However, in some applications it is costly to send uncompressed real numbers at full precision. To reduce the communication load of inter-node messages, we use lossy compression [49, 50].

Algorithm 1 R-MP-AMP (lossless) Inputs to Processor p: y, ap, t? Initialization: x0 = 0, zp0 = 0,?p  for t = 1 : t? do At Processor p: zpt = y  p ? apxt + 1?z p t?1gt?1, f  p t =  P xt + (a  p)T zpt At fusion center: ft =  ?P p=1 f  p t , gt = ?d?t(ft)?, xt+1 = ?t(ft)  Output from fusion center: xt?  Applying lossy compression to the messages sent from each processor node to the fusion center, we obtain the lossy R-MP- AMP [13, 15] steps as described in Algorithm 2, where Q(?) denotes quantization.

Algorithm 2 R-MP-AMP (lossy) Inputs to Processor p: y, ap, t? Initialization: x0 = 0, zp0 = 0,?p  for t = 1 : t? do At Processor p: zpt = y  p ? apxt + 1?z p t?1gt?1, f  p t =  P xt + (a  p)T zpt At fusion center: fQ,t =  ?P p=1Q(f  p t ), gt = ?d?t(fQ,t)?,  xt+1 = ?t(fQ,t)  Output from fusion center: xt?  The reader might notice that the fusion center also needs to transmit the denoised signal vector xt and a scalar gt?1 to the distributed nodes. The transmission of the scalar gt?1 is negligible relative to the transmission of xt, and the fusion center may broadcast xt so that naive compression of xt, such as compression with a fixed quantizer, is sufficient. Hence, we will not discuss possible lossy compression of the messages transmitted by the fusion center.

Assume that we quantize fpt ,?p, and use C bits on average to encode the quantized vector Q(fpt ) ? XN ? RN , where X is a set of representation levels. The per-symbol coding rate is R = CN . We incur an expected distortion  Dpt = E  [  N  N? i=1  (Q(fpt,i)? f p t,i)   ] at iteration t in each processor node,2 where Q(fpt,i) and f  p t,i  are the i-th entries of the vectors Q(fpt ) and f p t , respectively,  and expectation is over fpt . When the size of the problem grows, i.e., N ??, the rate-distortion (RD) function, denoted by R(D), offers the information theoretic limit on the coding rate R for communicating a long sequence up to distortion D [49?51]. A pivotal conclusion from RD theory is that coding rates can be greatly reduced even if D is small. The function R(D) can be computed in various ways [52?54] and can be  2Because we assume that A and z are both i.i.d., the expected distortions are the same over all P nodes, and can be denoted by Dt for simplicity. Note also that Dt = E[(Q(fpt,i)? f  p t,i)  2] due to x being i.i.d.

achieved by an RD-optimal quantization scheme in the limit of large N . Other quantization schemes will require larger coding rates to achieve the same expected distortion D.

Assume that appropriate vector quantization (VQ) schemes [51, 55, 56] that achieve R(D) are applied within each MP-AMP iteration. The signal at the fusion center before denoising can then be modeled as  fQ,t =  P? p=1  Q(fpt ) = x+ et + nt, (9)  where et is the equivalent scalar channel noise (6) and nt is the overall quantization error. For large block sizes, we expect the VQ quantization error nt to resemble additive white Gaussian noise with variance PDt that is independent of x+et at high rates, or at all rates using dithering [57].

State evolution for lossy R-MP-AMP: Han et al. suggest that SE for lossy R-MP-AMP [13] follows  ?2t+1 = ? W +   ? MSE(?t, ?2t + PDt), (10)  where ?2t can be estimated by ?? t =  M ?zt?  2 with ? ? ?p  denoting the `p norm [22, 23], and ?2t+1 is the variance of et+1. The rigorous justification of (10) by extending the framework put forth by Bayati and Montanari [23] and Rush and Venkataramanan [58] is left for future work. Instead, we argue that lossy SE (10) asymptotically tracks the evolution of ?2t in lossy MP-AMP in the limit of low normalized distortion PDt ?2t ? 0. Our argument is comprised of three parts: (i)  et and nt (9) are approximately independent in the limit of PDt ?2t ? 0, (ii) et + nt is approximately independent of x in  the limit of PDt ?2t ? 0, and (iii) lossy SE (10) holds if (i) and  (ii) hold. The first part (et and nt are independent) ensures that we can track the variance of et + nt with ?2t + PDt.

The second part (et + nt is independent of x) ensures that lossy MP-AMP follows lossy SE (10) as it falls under the general framework discussed in Bayati and Montanari [23] and Rush and Venkataramanan [58]. Hence, the third part of our argument holds. The numerical justification of these three parts appears in Zhu et al. [15, 59].

Optimal coding rates: Denote the coding rate used to transmit Q(fpt ) at iteration t by Rt. The sequence of Rt, t = 1, ..., t?, where t? is the total number of MP-AMP iterations, is called the coding rate sequence, and is denoted by the vector R = [R1, ..., Rt?]. Given the coding rate sequence R, the distortion Dt can be evaluated with R(D), and the scalar channel noise variance ?2t can be evaluated with (10). Hence, the MSE for R can be predicted. The coding rate sequence R can be optimized using dynamic programming (DP) [15, 60]. That said, our recent theoretical analysis of lossy R-MP- AMP has revealed that the coding rate is linear in the limit of EMSE? 0, where EMSE denotes excess MSE (EMSE=MSE- MMSE). This result is summarized in the following theorem.

Theorem 1 (Linearity of the coding rate sequence [15]): Supposing that lossy SE (10) holds, we have  lim t??  D?t+1 D?t  = ?,    where ? = NM MSE ?(?2?) and D  ? t denotes the optimal dis-  tortion at iteration t. Further, define the additive growth rate at iteration t as Rt+1 ? Rt. The additive growth rate for the optimal coding rate sequence R? satisfies  lim t??  ( R?t+1 ?R?t  ) =   log2  (  ?  ) .

Comparison of DP results to Theorem 1: We run DP (discussed in Zhu et al. [15]) to find an optimal coding rate sequence R? to reconstruct a Bernoulli-Gaussian (BG) signal, whose entries follow  xj ? ?N (0, 1) + (1? ?)?(xj), (11)  where ?(?) is the Dirac delta function and ? is called the sparsity rate of the signal. The detailed setting is: sparsity rate ? = 0.2, P = 100 nodes, measurement rate ? = 1, noise variance ?2W = 0.01, and normalized cost ratio of computation to communication b = 0.782 (a formal definition of b appears in [15]). The goal is to achieve a desired EMSE of 0.005 dB, i.e., 10 log10  ( 1 + EMSEMMSE  ) = 0.005. We use uniform ECSQ [49,  51] with optimal block entropy coding [51] at each processor node and the corresponding relation between the rate Rt and distortion Dt of ECSQ in the DP scheme. We know that ECSQ achieves a coding rate within an additive constant of the RD function R(D) at high rates [51]. Therefore, the additive growth rate of the optimal coding rate sequence obtained for ECSQ will be the same as the additive growth rate if the RD relation is modeled by R(D) [49?51].

The resulting optimal coding rate sequence is plotted in Fig. 1. The additive growth rate of the last six iterations is 6 (R  ? 12 ? R?6) = 0.742, while the asymptotic additive growth  rate according to Theorem 1 is 12 log2 ( ?  ) ? 0.751. Note  that the discrepancy of 0.009 between the additive growth rate from the simulation and the asymptotic additive growth rate is within the numerical precision of our DP scheme.

In conclusion, our numerical result matches the theoretical prediction of Theorem 1.

Algorithm 3 C-MP-AMP (lossless) Inputs to Processor p: y, Ap, {t?s}s=0,...,s? (maximum number of inner iterations at each outer iteration) Initialization: xp  0,t?0 = 0, zp  0,t?0?1 = 0, rp  0,t?0 = 0, ?p  for s = 1 : s? do (loop over outer iterations) At fusion center: gs =  ?P u=1 r  u s?1,t?s?1  At Processor p: xps,0 = x  p  s?1,t?s?1 , rps,0 = r  p  s?1,t?s?1 for t = 0 : t?s ? 1 do (loop over inner iterations) zps,t = y ? gs ?  ( rps,t ? r  p s,0  ) xps,t+1 = ?s,t(x  p s,t + (A  p)T zps,t)  rps,t+1 = A pxps,t+1?  zps,t M  ?Np i=1 ?  ? s,t([x  p s,t+(A  p)T zps,t]i)  Output from processor p: xp s?,t?s?

IV. COLUMN-WISE MP-AMP In our proposed column-wise multiprocessor AMP (C-MP-  AMP) algorithm [16], the fusion center collects vectors that  represent the estimates of the portion of the measurement vector y contributed by the data from individual processors.

The sum of these vectors is computed in the fusion center and transmitted to all processors. Each processor performs standard AMP iterations with a new equivalent measurement vector, which is computed using the vector received from the fusion center. The pseudocode for C-MP-AMP is presented in Algorithm 3.

State evolution: Similar to AMP, the dynamics of the C- MP-AMP algorithm can be characterized by an SE formula.

Let (?p  0,t? )2 = ??1p E[X2], where ?p = M/Np, ?p = 1, ..., P .

For outer iterations 1 ? s ? s? and inner iterations 0 ? t ? t?s, we define the sequences {(?ps,t)2} and {(?  p s,t)  2} as  (?ps,0) 2 = (?p  s?1,t?) 2, (12)  (?ps,t) 2 = ?2W +  P? u=1  (?us,0) 2 +  ( (?ps,t)  2 ? (?ps,0)2 ) , (13)  (?ps,t+1) 2 = ??1p E  [( ?s,t(X + ?  p s,tZ)?X  )2] , (14)  where Z is standard normal and independent of X . With these definitions, we have the following theorem for C-MP-AMP.

Theorem 2 ([16]): Under the assumptions listed in [58, Section 1.1], for p = 1, ..., P , let M/Np ? ?p ? (0,?) be a constant. Define N =  ?P p=1Np. Then for any PL(2)  function3 ? : R2 ? R, we have  lim N??   Np  Np? i=1  ?([xps,t+1]i, x p i )  a.s.

= E  [ ?(?s,t(X + ?  p s,tZ), X)  ] ,?p,  where xps,t+1 is generated by the C-MP-AMP algorithm, ? p s,t  is defined in (12?14), xpi is the i th element in xp, xp is the  true signal in the pth processor, X ? pX , and Z is a standard normal RV that is independent of X .

Remark 1: C-MP-AMP converges to a fixed point that is no worse than that of AMP. This statement can be demonstrated as follows. When C-MP-AMP converges, the quantities in (12? 14) do not keep changing, hence we can drop all the iteration indices for fixed point analysis. Notice that the last term on the right hand side (RHS) of (13) vanishes, which leaves the RHS independent of p. Denote (?ps,t)  2 by ?2 for all s, t, p, and plug (14) into (13), then  ?2 = ?2W +  P? p=1  ??1p E [ (?(X + ?Z)?X)2  ] (a) = ?2W + ?  ?1E [ (?(X + ?Z)?X)2  ] ,  which is identical to the fixed point equation obtained from (8), where (a) holds because  ?P p=1 ?  ?1 p =  ?P p=1  Np M =  N M .

Because AMP always converges to the worst fixed point of (8) [24], the average asymptotic performance of C-MP-AMP is at least as good as AMP.

Remark 2: The asymptotic dynamics of C-MP-AMP can be identical to AMP with a specific communication schedule. This  3A function f : Rm ? R is pseudo-Lipschitz of order-2, denoted PL(2), if there exists a constant L > 0 such that for all x, y ? Rm, |?(x)? ?(y)| ? L(1 + ?x?+ ?y?)?x? y?, where ??? denotes the Euclidean norm.

1 2 3 4 5 6 7 8 9 10 11 12  Iteration t          O p ti m  a l c o d in  g r  a te  R t*  Growth rate in last 6 iterations: 0.742 Theoretic asymptotic growth rate: 0.751  MSE achieved: -23.213dB (MMSE: -23.218dB)  Fig. 1. Low-EMSE growth rate of optimal cod- ing rate sequence per DP vs. asymptotic growth rate 1  log2  ( ?  ) . (BG signal (11), ? = 0.2, ? =  1, P = 100, ?2W = 0.01, b = 0.782.)  0 1 2 3 4 5 6 7  0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1  Iteration  M S  E      SE #inner?iteration=1  SE #inner?iteration=2  SE #inner?iteration=4  simulation  Fig. 2. Verification of SE for C-MP-AMP with various communication schedules. (P=3, N=30000, M=9000, SNR=15dB.)  0.2 0.3 0.4 0.5 0.6 0.7  0.01  0.02  0.03  0.04  0.05  0.06  Measurement rate ?  M S  E      MMSE 10dB  MMSE 15dB  simulation  Fig. 3. Verification that C-MP-AMP achieves the MMSE at various measurement rates ? = M/N and SNR levels. (P=3, N=30000.)  can be achieved by letting t?s = 1,?s. In this case, the quantity (?ps,t) is involved only for t = 0. Because the last term in (13) is 0 when t = 0, the computation of (?ps,0)  2 is independent of p. Therefore, ?ps,0 are again equal for all p. Dropping the processor index for (?ps,t)  2, the recursion in (12?14) can be simplified as  (?s,0) 2 = ?2W +  P? p=1  ??1p E [ (?s,0(X + ?s,0Z)?X)2  ] = ?2W + ?  ?1E [ (?s?1,0(X + ?s?1,0Z)?X)2  ] ,  where the iteration evolves over s, which is identical to (8) evolving over t.

Numerical results for SE: We provide numerical results for C-MP-AMP for the Gaussian matrix setting, where SE is justified rigorously. We simulate i.i.d. Bernoulli-Gaussian signals (11) with ? = 0.1. The measurement noise vec- tor w has i.i.d. Gaussian N (0, ?2W ) entries, where ?2W depends on the signal to noise ratio (SNR) as SNR := 10 log10  ( (NE[X2])/(M?2W )  ) . The estimation function ?s,t  is defined as ?s,t(u) = E[X|X + ?ps,tZ = u], where Z is a standard normal RV independent of X , and ?ps,t is estimated by ?zps,t?/  ? M , which is implied by SE. All numerical results  are averaged over 50 trials.

Let us show that the MSE of C-MP-AMP is accurately  predicted by SE when the matrix A has i.i.d. Gaussian entries with Ai,j ? N (0, 1/M). It can be seen from Fig. 2 that the MSE achieved by C-MP-AMP from simulations (red crosses) matches the MSE predicted by SE (black curves) at every outer iteration s and inner iteration t for various choices of numbers of inner iterations (the number of red crosses within a grid).

As discussed in Remark 1, the average estimation error of C-MP-AMP is no worse than that of AMP, which implies that C-MP-AMP can achieve the MMSE of large random linear systems [26] when AMP achieves it.4 This is verified in Fig. 3.



V. DISCUSSION This overview paper discussed multi-processor (MP) ap-  proximate message passing (AMP) for solving linear inverse  4AMP can achieve the MMSE in the limit of large linear systems when the model parameters (?, SNR, and sparsity of x) are within a region [24].

problems, where the focus was on two variants for partitioning the measurement matrix. In row-MP-AMP, each processor uses entire rows of the measurement matrix, and decouples statistical information from those rows to scalar channels. The multiple scalar channels, each corresponding to one processor, are merged at a fusion center. We showed how lossy com- pression can reduce communication requirements in this row- wise variant. In column-MP-AMP, each node is responsible for some entries of the signal. While we have yet to consider lossy compression in column-MP-AMP, it offers privacy advantages, because entire rows need not be stored. Ongoing work can consider lossy compression of inter-processor messages in column-MP-AMP, as well as rigorous state evolution analyses.

