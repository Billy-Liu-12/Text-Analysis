Rapid Scanning of Spectrograms for Efficient

Abstract ? Acoustic sensing is a promising approach to scaling  faunal biodiversity monitoring. Scaling the analysis of audio  collected by acoustic sensors is a big data problem. Standard  approaches for dealing with big acoustic data include automated  recognition and crowd based analysis. Automatic methods are fast  at processing but hard to rigorously design, whilst manual  methods are accurate but slow at processing. In particular,  manual methods of acoustic data analysis are constrained by a 1:1  time relationship between the data and its analysts. This constraint  is the inherent need to listen to the audio data. This paper  demonstrates how the efficiency of crowd sourced sound analysis  can be increased by an order of magnitude through the visual  inspection of audio visualized as spectrograms. Experimental data  suggests that an analysis speedup of 12? is obtainable for suitable  types of acoustic analysis, given that only spectrograms are shown.

Keywords?sensors; acoustic data; spectrograms; big data; big  data analysis; crowdsourcing; fast forward

I. INTRODUCTION  Acoustic sensors provide an effective way to scale biodiversity monitoring to large scales [1-3]. Acoustic sensors record large amounts of data continuously and objectively over extended periods. There are many ways to analyze these huge datasets, ranging from completely manual approaches, to fully automated methods of detection.

Automated vocalization detection and classification of fauna in recordings has been the subject of much research.

There are many examples of single species detectors [3-5], fewer algorithms capable of detecting multiple species [6], and some examples of general purpose tools capable of general audio data analysis [7-9].

However, automated methods of analysis are not perfect.

They can suffer high rates of false positives and false negatives [10, 11] and are time-consuming and expensive to develop.

Extracting good training sets is particularly time consuming, requiring extensive tuning and adaptation for different environments [10, 12].

An alternative approach to automated methods is to use crowd-based methods of analysis. The idea is that it is possible to outsource a complex classification task to a crowd of interested participants. In these scenarios, technology can be used to assist with the menial parts of the analysis tasks. We term this combination of manual and automated approaches as semi-automated analysis. Varying levels of automation and  human participation result in a spectrum of methodologies that exist between the two extremes.

In our research project, we use a semi-automated analysis methodology in addition to developing fully automated methods of detection [3]. Currently, in our semi-automated system, participants analyze data in a web interface by playing back audio collected from sensors. The audio is played, along with a visual representation of the sound displayed at the same time. This visualization is a spectrogram ? a time/frequency graph that can show the ?shape? and intensity of the underlying audio. The spectrogram is currently translated left (animated horizontally to screen left) at a speed that is equivalent to the audio playing (approximately 45px/s). We label this speed as real-time (or 1?). Fig. 1 shows a screenshot of this software.

The large amount of audio data that needs to be analyzed places strain on the limited resources of our volunteer participants. As we observed our participants analyzing data, a unique behavior was noticed when participants were trying to identify only one species at a time. They would rapidly ?scan? through each section of audio that was loaded into our online analysis tool. This scanning involved waiting for each 6 minute block to load (~3MB of audio, 1MB of images), dragging the seek/progress/navigation bar from start to end at a speed they were comfortable with, stopping only when they found their target pattern. Accordingly, without listening to the audio and by relying on the spectrograms alone to identify their target vocalisations, a participant could process the 6 minute block in seconds. This ad hoc method is suboptimal due to the loading of redundant data and the limiting size of audio segmants that can be loaded at any one time.

To optimize the process and determine the degree of accuracy that can be achieved, this paper tests this ad hoc ?rapid scanning? method of semi-automated analysis for viability.

A. The challenges of big acoustic datasets  Our project?s acoustic sensors collect on average eight days? worth of audio data every day. Meanwhile, semi-automated analysis currently takes a participant approximately two hours to analyze an hours? worth of data, at reduced resolution [13].

Hence, if we wish to scale our audio analysis, an increase of efficiency in the analysis process is required.

One limiting factor is the consumption of audio data. Audio data is ideally consumed in real-time (1? speed). Other speeds distort the sound resulting in a different interpretation of the original sound by a human. However, a spectrogram, since it is    only an image, remains visually identical no matter what speed it is translated at. The limiting factor is the amount a participant can perceive in an image with limited temporal exposure.

By disabling the audio and speeding up (fast-forwarding) the animation of the spectrograms, there is the potential to have our participants analyze the data faster, without a severe loss of accuracy. This paper presents an experiment that tests the aforementioned concepts for feasibility. If feasible, this paper will add another method for semi-automated data analysis to the existing toolbox of techniques.



II. RELATED WORK  A. Related Citizen Science Work  Galaxy Zoo is an example of a successful citizen science project that utilizes a crowd sourced image classification model ? similar to the model we employ for identifying patterns in spectrograms. The Galaxy Zoo project uses their volunteers to classify the morphology of galaxies from the Sloan Digital Sky Survey by showing them images of the galaxies and asking them to pick a similar shape [14]. Importantly, participants complete the tasks at their own pace and classification speed is not emphasized. Instead, Galaxy Zoo scales out their analysis by gathering large numbers of active participants. A focus on faster classification times would not work well with the current versions of Galaxy Zoo, as the classification task asks multiple questions about each presented image.

WhaleFM [15] is a derivative of the Zooniverse project which operates Galaxy Zoo. Again, the core concept is to harness the collective intelligence of volunteer participants to analyze images. However, WhaleFM differs in that it shows spectrograms of whale song to participants for classification into one of several classes. This is very similar to this papers? stated task. Whales create vocalizations on the lower end of the spectrum of human hearing, thus, it is not always easy to hear them. By visualizing the sound with a spectrogram image, it lets the participants match the image in their own time, not constrained to real time audio. The WhaleFM paper by Saigh et  al. [15] shuffled the order of the spectrograms shown to the volunteers used in the paper?s experiment. The paper did not reveal how long it took its participants to classify the whale song patterns. Like Galaxy Zoo, it has multiple possible classifications for vocalizations, making it potentially difficult to scale in speed.

A paper by Lin et al. [16] demonstrated a similar rapid- analysis technique. The paper uses human participants to detect acoustic events of interest in spectrograms. The user can jump to any point in the audio stream and adjust the zoom of the visualization at the same time. Their study was conducted in order to bypass the time constraints of listening to and analyzing audio data. Additionally, they found that spectrograms were a good choice for visualizing their data because even untrained participants were capable of completing their assigned tasks of locating acoustic events. Participants were given 8-minute blocks of time to identify as much content as possible in 80 minutes of audio. The spectrograms are enhanced and shown in a zooming-style interface that allows participants to control the scale of the spectrograms (and thus the audio) that is shown.

When identifying an event the user has the option to playback the associated audio. In practice, the authors stated acceptable results with their 10? speed increase. Importantly, their experiment was unstructured ? participants chose where and when they stopped and listened to audio data.

B. Perception and Reaction times  The widely accepted minimum reaction time for visual stimuli in humans is about 200ms [17, 18]. However, reaction time slows when a choice needs to be made, as when classifying something, reaching 400ms and higher depending on the complexity of the image [17].

Biederman [19] states that image processing in humans is component based. This means humans are good at looking for shapes in images, like the sort of shapes often seen in spectrograms. The paper also states that as the number of components presented increases, error goes up. Biederman suggests that at least one second is required for the analysis of a degraded image. A degraded image is defined as one missing parts, like contours, surfaces, or other gaps. Spectrograms can be complex and vocalizations within can often be missing components.

Konishi et al. [20] did a study on brain activity for a go/no- go task. They trained participants to respond within a 300ms reaction time to a go/no-go task (press a button for positive or another negative) for a simple visual stimulus.

Joubert et al. [21] have done several studies of times taken for participants to classify a scene. Generally, they flash an image up for a very short amount of time (20ms) and see categorization into one of two groups (i.e. go/no-go) in around 400ms.

In summary, the best reaction times cannot be less than 200ms for a classification task and an average of around 400- 600ms is expected for classification of an image like a spectrogram.

Fig. 1. A screenshot of out current annotation software

III. EXPERIMENT DESIGN  This experiment should assess the viability of the rapid scanning methodology through the construction a new and appropriate interface. To measure the net data processing speed, the test interface will show different speeds and measure which settings result in the best analysis. Ideally, the experiment should attempt to understand how the rapid scanning methodology would scale. The experiment must also be web browser compatible. Our existing analysis systems runs in an online environment and it would be ideal to integrate the work if it were feasible.

A small survey will also be issued to participants after they complete the experiment.

A. Limitations  1) Soundscape  Vocalizations of interest must be easy to identify by human participants. This means that the vocalization should be distinct and likely to occur in moderately empty audio signals. When working with relatively empty audio signals, it is still possible to have a complex and dynamic acoustic profile in the recordings. This variation is caused by a variety of non-target acoustic features such as rain, wind, crickets, or complex non- bioacoustic events. When combined these artifacts can prevent simple automated detection techniques from working effectively.

The human component of the rapid scan methodology is what makes this idea feasible. A human participant can intelligently distinguish between infrequent faunal vocalizations and sudden intense or complex periods of uninteresting audio. However, humans have limits of perception and focus. Analyzing with the intent of classifying every species present at once, or analyzing in dense areas of bioacoustic events will overwhelm a human participant ? especially when asked to do so quickly. Thus, the rapid scan method is thought to be most useful for speeding up the analysis of the sparse, time-consuming, night section of an acoustic day.

2) Participants? tasks  Typically when analyzing audio data, participants are tasked with annotating vocalizations. Each annotation action involves drawing a bounding box around the portion of the spectrogram containing the vocalization and then associating one or more textual tags with said bounding box. These annotations form the core data output for this research project; however, they are also time consuming to create. The rapid scan methodology is intended to analyze data rapidly. If a participant were to stop every time they detected a vocalization and then annotate it, the desired speed up in analysis would likely not be obtained. Instead of full annotations, a simpler method of detection was chosen: a simple positive ?hit? button.

Once points of interest are discovered (as hits), it is then possible to get any participant to return to the data later to properly annotate. The rapid scan process still provides a service by filtering out the large sections of audio that contain no interesting vocalizations. In other words, this is filtering with human vision to break up a time-consuming task into components of work.

3) Inclusion of a ?negative? answer  Ideally, there would only be a positive hit answer in the user interface as it is all that is needed to complete the rapid scan task. Experimentally, this would mean it is not possible to determine the difference between a participant failing to respond and a negative response. Thus, a negative response option was included to enable this information to be gathered.

4) Disabled audio playback  Enabling playback of audio for the rapid scan methodology was considered. It would be ideal for participants to hear the audio data ? it is a powerful discriminator for distinguishing between signal and noise. Audio also helps explain spectral components in the spectrogram and helps to keep the task interesting for participants. However, playback of audio is constrained to a 1? speed ? this is the very speed constraint the rapid scan methodology is trying to avoid. Any playback of audio would reduce the effectiveness of the rapid scan methodology.

B. Hypothesis  Research question: by manipulating the animation speed of the spectrograms, to make them display faster, will participants be able to detect interesting acoustic events at an increased speed, with an acceptable trade off in accuracy.

The null hypothesis (H0) for this experiment is: No difference in accuracy will occur at different exposure speeds.

The alternative hypothesis (H1) for this experiment is: That accuracy will be effected by speed of presentation such that accuracy will decrease at higher speeds.

C. Experimental Interface Design  Flashcards were chosen over the project?s traditional animated image translation for simplicity. A flashcard is simply a card that shows information ? they are often learned for memorization tasks. We use the term flashcard in a digital sense to refer to a series of spectrogram images that are to be flashed past an analyzer-participant. Flashcards are simpler than a translation animation; they simply need to be shown for some duration and then hidden again. This means they do not move distractingly during viewing, allowing participants to scan according to their personal preference rather than forcing them to scan left to right. For a traditional translated image approach, it is required to animate not just one image but neighboring off- screen images as well, in a demanding animation loop. A translating image approach requires a concept of scale (pixels per second) and is inherently limited by the rendering capabilities of the browser (often 60fps).

The amount of audio data shown with each flashcard was set to 24 seconds. This amount was chosen because a 24-second spectrogram, at standard scale (?43px/s) fits well within most screen resolutions; it is 1033px wide by 256px high. The spectrograms are created with a 512 sample window and no overlap. The duration of 24 seconds also divides conveniently into 120 seconds ? many of the smaller recordings available are two-minute long blocks of audio data.

A screenshot in Fig. 2. shows the instructions page that was given to each participant between each segment of analysis.

When presented to participants animations emphasized core    components of the instructions. In particular, the outlining in bright green of the example vocalizations was animated and labelled. Additionally, the exposure speed, number of flashcards in the segment, and the key bindings were bolded to make them stand out.

The classification page (Fig. 2) consisted of timestamps (the bounds of the flashcard), an exposure countdown timer, a pause / resume button, and a segment progress bar. A lead-in countdown appears on the classification page; it instructs users to place their hands on the keyboard and displays a ten second countdown to ready the participant before each segment of the experiment started.

D. Experiment protocol  The experiment was conducted according to the following protocol:  1. The experiment was advertised and participants were contacted via email, in person, and through social media.

2. The landing page was the first thing participants saw. On this page, the basic details of the experiments were shown.

Participants were encouraged to read the ethics statement and were required to consent to their participation in the experiment.

3. A segment order protocol was created for the participant.

See the following section ?Segment Order and Randomization Protocol? for more information.

4. The training round was conducted: each flashcard lasts 10 seconds and only three flashcards are shown.

5. The main experiment was then run. Three rounds were conducted using the datasets and the speed combinations defined by the segment order protocol. These rounds showed a total of 165 flashcards.

6. End of experiment: the end screen shown and survey link were displayed and the data was sent back to the server.

7. Survey optionally completed by participant  E. Speeds  The flashcard exposure speeds tested in this experiment are shown in Table 1. A range of speeds were chosen around the 2s exposure mark. The 2s mark was chosen based on observations of the ad-hoc rapid scan methodology. The data used in the experiment was annotated previously and thus a real-time speed was not included as a control. The real-time data was used as the baseline accuracy measure.

F. Datasets  The data chosen for this experiment was taken from a project that deployed sensors located at St Bees Island, Queensland Australia (latitude: -20.914, longitude: 149.442). This island has a population of Koalas  (Phascolarctos cinereus) relatively unaffected by mainland Australia, making it a source of interesting research [22]. Koala vocalizations were chosen because it is known that Koala usually call at night [23]. Koala vocalizations are also easy to distinguish and identify ? they are long, loud, and distinct.

Data was taken from two different sites at St Bees, from 30/September/2009 to 16/August/2011. Recording timestamps spanned from 17:00 through to 04:30. The sensors used were 3G phones that recorded 2 minutes of audio every half hour.

For the experiment, three datasets, one for each speed, totaling 66 minutes of data (22 minutes for each dataset) were chosen. The idea was to provide enough data for each participant to complete, in order to simulate what the experimental task might be like at a large scale, balanced against the time constraints of the participants.

The recordings were included in their entirety, unedited, into the dataset when a Koala Bellow was found. All sections chosen were previously annotated so that reference data was available. There were an unnatural number of positive hits in the experiment datasets. In a real world example, fewer recordings would have a Koala vocalization present. This experiment was designed so that the presence of Koala vocalizations occurs approximately 50% of the time. In actuality, vocalizations occur in 40% of the flashcards.

G. Segment Order and Randomization Protocol  In the experiment, it was desirable that each speed was tested on each dataset.

If all the participants experienced the varying speed tests in order, (i.e. 5s, 2s, 1s) they might have been unfairly trained for the faster speeds. To avoid a training bias, the combination of        TABLE I.  SPEEDS TESTED IN EXPERIMENT  Speed Exposure time ???? = ???????? ????  ???   Slowest 5.0s 4.8?  Medium 2.0s 12.0?  Fastest 1.0s 24.0?  Fig. 2. Screenshots of the experiment interface. Top: The spectrogram from  the training page. Bottom: An example ?yes? hit (a true positive) on the classification page.

speeds was set to be order important. Thus, the three speeds produce six permutations.

Given three datasets, there were 18 possible combinations for the experiment. The combinations were tracked and handed out evenly to each new participant of the experiment. This ensures that the participants completed roughly the same number of each the possible segment orders.

In each dataset, the order of the flashcards that are shown were randomized. This ensured that participants were very unlikely to receive either a) contiguous flashcards or b) an order of flashcards that might unfairly bias them (e.g. due to unintentional training).



IV. RESULTS  An error in storing the data on the experiment server rendered some of the experimental results unusable. This created a disparity between the number of survey results and the number of experimental results. There were 46 experimental results and 73 survey responses. The corrupted experimental data was discarded and the remaining experimental results verified for integrity. Since the survey responses are independent of the experiment data, all of the survey responses were used.

A. Main experiment overview  A script for data manipulation processed the JSON files sent back to our server from the website. The data was then subsequently analyzed by Microsoft Excel 2013 and verified by IBM?s SPSS Version 21.

Experimental results were collected from 2/April/2013 through to 21/April/2013. In total 73 experimental results were collected. Twenty-seven experimental results were deleted due to data corruption, leaving 46 valid responses. All subsequent reports on experimental data will include only the data from the 46 valid experimental results.

Throughout the experiment period 7 728 flashcards were shown generating 8 023 hits, where a hit is a decision made about a flashcard. Changes in decision were possible meaning on average there was 1.04 hits per flashcard. A miss was a flashcard that did not receive any hit. Misses occurred in 512 (7% of) flashcards.

Given that each flashcard showed 24 seconds of audio data, 51.52 hours? worth of data was analyzed during the experiment.

This analysis was completed with 7.02 hours of human effort;  this included, training time, pauses, breaks between segments, and download time. Without pauses included, only minimal time was spent downloading spectrograms and reading the instructions for each segment. The human effort spent without pause breaks of 6.01 hours, computed to an effective average exposure speed of 2.80s/flashcard (8.6?, average across all speeds, including training). The expected average exposure time across all flashcards was 2.55s/flashcard (9.4?).

On average, each segment order was completed 2.56 times.

B. Main experiment results breakdown  This section reports participants? accuracies at different speeds. Accuracy is the statistic we used for summarizing responses to flashcards. Accuracy is defined as:  ? = ?? + ??  ? + ? (1)  where a positive or negative was determined by the presence of a koala vocalization and a true or false was determined by marking a participant?s answer against the relevant flashcard.

Accuracy was chosen because it represented the statistic we were most interested in and because it was not defined by false cases. This is useful because there were two types of false cases: an incorrect decision and a miss ? where a participant has failed to respond within the exposure time.

a) Consistency of Datasets  As described, three datasets were created for use in the study. These datasets were then presented to participants at various speeds. These datasets were presented with their spectrograms randomly shuffled. Before testing the performance of participants at different speeds, it is important to confirm that no difference in accuracy was found between datasets (as this would indicate a confound resulting from the random allocation of spectrograms to each dataset). To ensure no systematic error was unintentionally introduced into the study in the form of datasets that were more or less difficult to analyze, regardless of speed, inferential statistics were used to confirm that all datasets were equivalent.

There were ten outliers in the data as assessed by inspection of boxplots. In addition, accuracy was not normally distributed for each dataset as assessed by Shapiro-Wilk?s test (p < 0.001).

Thus, an ANOVA was not a suitable test since its assumptions were not met. Instead, a Kruskal-Wallis test was run to determine if there were differences in accuracy for flashcards between datasets.

Initially, the datasets were collapsed across speed and compared. No statistically significant differences were found between the three datasets, ?2(3) = 5.638, p = 0.131, indicating that no dataset was more or less difficult to analyze than any other. Because a slightly different proportion of each dataset was used at each speed due to the final number of participants  TABLE II.  SEGMENTS BREAKDOWN FOR EXPERIMENT RESPONSES  Speed (s) training DS1 DS2 DS3 SUM  10 46 0 0 0 46  5 0 14 17 15 46  2 0 15 15 16 46  1 0 17 14 15 46  TABLE III.  MARKING STYLE   Positive  (non-ambiguous) Negative  (ambiguous or non-existent)  True TP TN  False FP FN  Miss MP MN  TABLE IV.  DATA SET BREAKDOWN  Dataset Instances Accuracy (mean)  SD Miss Rate (%)  training 138 0.80 0.26 0.06  DS1 2530 0.80 0.16 0.06  DS2 2530 0.79 0.19 0.07  DS3 2530 0.82 0.19 0.07    in the study, further Kruskal-Wallis tests were done between the datasets for each speed individually. These tests also revealed no significant difference between the datasets. In sum, as required to allow for a valid test of performance at difference speeds (see below), no difference in difficulty of datasets (participant performance) was found between the three datasets.

b) Effects of speed on accuracy  To determine the effect of exposure speed on accuracy and test hypothesis 1, a series of inferential tests were conducted. A repeated measures ANOVA was conducted to determine whether there were statistically significant differences in Accuracy over varying flashcard exposure speeds.

There were two outliers in the data as assessed by inspection of boxplots. One outlier (accuracy = 0.16) occurred at speed 2 where the user had stopped responding. The other outlier (accuracy = 0.44) occurred at speed 5, where it seems the participant got the positive hit and negative hit responses mixed up. Both participants were removed from the dataset. To assess the assumption of normality, skewness and kurtosis values were calculated at each speed. All variables were found to be skewed.

The data was then transformed with an arcsine (sin-1) transformation. Skewness and Kurtosis values were then recalculated and found to be acceptable for all variables. To allow for the violation of the assumption of normality, all analyses were conducted with both the transformed and the non-transformed variables. No differences were found in the pattern of results, so for ease of interpretation the results with the non-transformed variables are reported below.

Mauchly's Test of Sphericity indicated that the assumption of sphericity had been violated, ?2 (2) = 35.125, p < 0.001.

Therefore, a Greenhouse-Geisser correction is applied (? = 0.736). Accuracy was statistically significantly different at the different speeds during the experiment, F(1.277, 54.893) = 16.864, p < 0.001, partial ?2 = 0.282. Accuracy decreased from 5s (0.87 ? 0.01), to 2s (0.85 ? 0.18), to 1s (0.73 ? 0.21), in that  order. Post-hoc analysis with a Bonferroni adjustment revealed that accuracy statistically significantly decreased from the 2s speed to the 1s speed (0.12 (95% CI, 0.201 to 0.042), p = 0.001).

Additionally, accuracy statistically significantly dropped from the 5s speed to the 1s speed (0.15, (95% CI, 0.069 to 0.227), p < 0.001). However, there was no significant increase in accuracy from the 2s speed to the 5s speed (0.03, (95% CI, - 0.070 to 0.060), p = 0.170).

c) Summary  The results from the repeated measures ANOVA allowed us to reject the null hypothesis that accuracy is the same across all speeds. Furthermore, operating at speed 2 produces an accuracy that is not significantly different than operating at speed 5; thus accuracy is kept with the faster 2s speed. However, working at the fastest speed (1s) resulted in a significant drop in accuracy in comparison to working at the slower speeds.

2) Hit distributions  Every hit (classification) event of a flashcard was recorded with the event?s timestamp. These hits were compared between the different speeds in Fig. 3.

When analyzing the hit timestamps some inconsistencies were noticed with the timestamp data. Investigation into these inconsistencies suggested that some form of lag spikes or pauses intermittently affected the timestamp calculation. In total 200 hit instances were excluded from the 8023 instance hit dataset because they fell outside the logical bounds of the exposure period for their associated flashcards.

Fig. 3.  Histogram of hit distributions with an absolute time x-axis, broken into 0.1s bins. The y-axis is normlized as percentage of hits within each speed  TABLE V.  SPEED BREAKDOWN  Speed (s) Instances Accuracy (Mean)  SD Miss Rate (%)  10 138 0.80 0.26 0.06  5 2530 0.87 0.09 0.01  2 2530 0.83 0.16 0.05  1 2530 0.71 0.22 0.14  0%  5%  10%  15%  20%  0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8 3 3.2 3.4 3.6 3.8 4 4.2 4.4 4.6 4.8 5  P er  ce n ta  g e  o f  to ta  l h it  s (f  o r  ea ch  s p  ee d  )  Exposure time in seconds  Histogram of hits throughout their exposure periods - absolute time (10s truncated)  Reaction Time 10s 5s 2s 1s    C. Survey  The survey received 76 responses, 56% male, 43% female, and 1% other.

Half of the participants (52%) had never gone looking for wildlife recreationally. The rest were Birders (15%), bush walkers (31%), and snorkelers/divers (16%), with 2 responses from herpetologists, and 2 people that lived on farms. When asked about the years of experience they had doing recreational biology, 45% responded with ?No amount of time?. Twenty- four participants had experience greater than 5 years. Two professional biologists participated.

The provided instructions were adequate for 92% of participants. Two people commented on whether both parts of training pattern had to be included for the pattern to be considered valid. Participants commonly asked for more example training images.

Almost all of the participants (70%) preferred a 2 second exposure time out of the speeds they completed in the experiment. When asked about other speeds they would prefer, participants liked speeds 3, 2, 1.5 with 35%, 32%, and 20% respectively (Fig. 4.). Four participants advocated a variable speed.

Other comments included requests for bigger spectrograms and more training samples that included answers. Participants found the 5s speed boring and uninteresting. Participants also reported feeling stressed, uncomfortable, and frustrated during the 1s speed. Most agreed that the 1s speed was too fast. At least one participant gave up answering negative hits. Two participants wanted to progress through the flashcards at their own pace. Generally, participants wanted to listen to the audio.



V. DISCUSSION  This paper?s research question seeks to determine if it is viable to flash images of audio past participants at high speeds for analysis. Before answering the question of viability, it is necessary to determine the speed that performed best. The best speed can then be used to determine viability.

A. The best speed  The main experiment quantitatively showed that of the three speeds tested, there was a significant drop in accuracy for only the fastest speed, 1s (24?), when compared to the other speeds.

For the 2s (12?) & 5s (4.8?) speeds there was no significant difference in accuracy among the participants. This means there is no significant drawback in accuracy for tasking participants to operate at a 2s speed over 5s.

Additionally, the miss rate for participants at speeds 2s and 5s were 5% and 1% respectively. For the 1s speed, this rate jumped to 14%. These misses were partially explained by a single participant that declined to answer for negative flashcards for the 1s speed only ? that accounted for 0.8% of flashcards.

B. Hit distributions  The hit distribution data (Fig. 3) provides insight into when in the exposure period users were responding.

The median responses for all three experimental speeds were between 500ms and 800ms ? approximately what the literature suggested it should be. As the speed increased the median hit time decreases. We speculate that the increased speed is forcing the participants to lower their average reaction time.

When the tails of hit distributions were compared, we see that for speed 1, at its upper bound, the histogram shows a non- zero value of ~2.5%. This meant that on average a group of participants was not responding within the time constraint. For the 2s and 5s speeds, the histograms demonstrate a more relaxed tail of diminishing responses. The last data point for 2s is at 0.002% of hits and the 5s speed actually hits zero ? indicating all users had finished responding within the time constraints.

The difference in completion of hits within the allotted time between 2s and 5s was negligible (0.002%). This means, that the extra three seconds of time between the speeds is wasted, given all responses can be accounted for without the extra time.

Finally, the red line on the hit distribution graph represents the upper bound of human reaction time performance (200ms).

Discussed in the related work section, it is extremely unlikely to see a legitimate response within the first 200ms of exposure of a flashcard. We speculate any hits that occur within this 200ms period are invalid, either caused by panic, or delayed reactions ? i.e. a participant decided how to classify a card too late and accidently responded to the next flashcard in the sequence. With the 2s and 5s speeds only a very small percentage of hits occurred within this first 200ms; 0.5% for 5s, and 2% for 2s (cumulative where t < 200ms). However, for the 1s speed, the cumulative hits reached 8% (t < 200ms). This means, of the 2530 flashcards shown there were 202 responses for which it is impossible for them to be legitimate.

Tying the miss rate (14%) in with the impossible-response- time rate (8%) for the 1s speed, there?s a minimum 20% error rate for speed 1 ? much higher than speeds 5 or 2 (1.5% & 7% minimum error rates respectively).

C. Survey data  The qualitative data received from the survey produced a wide range of results. Gender was roughly split, age was  TABLE VI.  AGE AND VOCATION BREAKDOWN  Age Percentage  Qualifications Response  <18 0.00%  High School Diploma 28.77%  18 - 25 41.10%  TAFE Diploma 15.07%  25 - 35 24.66%  Graduate Degree 28.77%  35 - 45 12.33%  Post graduate degree 24.66%  45 - 55 9.59%  Post-doctoral qualifications 2.74%  55 - 65 9.59%  65+ 2.74%    Fig. 4. Preferred speed from the experiment and desired speed for long amounts of work    0% 20% 40% 60% 80%   3*   1.5*   <1*  faster*  slower*  Percentage of respondants  S p  e e d  Desired Speed  Experiment Speed    skewed towards the 18-25 bracket, and vocation was roughly evenly distributed. Importantly, 45% of respondents indicated they do not recreationally look for wildlife. This means a reasonable number of novices participated in the experiment.

Novices completing this experiment is ideal, as it is desirable for the rapid scan methodology to show good results for any skill level, not just for experts.

Comments on the design and layout of the experiment were noteworthy and will be addressed in future iterations of the experiment. However, ultimately, the most important responses were the speed preferences. Of the speeds tested, 70% of respondents indicated they preferred the 2s speed with associated comments indicating 5s was boring, and 1s stressful.

When asked about their preferred hypothetical speed, respondents answered most commonly with a range between 1.5s and 3s. Common requests included variable speeds to suit their preference and ability ? which would be ideal outside of an experimental environment.

D. Viability  Given that the 2s speed was the best option of the speeds tested, it would be the ideal speed to use in a production scale flashcard analysis system.

At the 2s speed, accuracy compared to real time is 83%.

Provided the requirements for rapid scan methodology are met (see the Limitations section), we argue that a 17% drop in accuracy is an acceptable trade-off for a 12? (an order of magnitude) increase in analysis speed. For Koala vocalizations in particular, they often last 20-60 seconds, fading in, reaching a climax, and then fading out. This long call means as many as 5 flashcards could have instances of the one group of vocalizations ? positive identification is only necessary for one of the flashcards shown within the vocalization period.



VI. CONCLUSION  The experiment indicated the viability of rapidly scanning spectrograms for the basic identification of Koala vocalizations.

A 12? (2.0s exposure) speedup is achievable with an acceptable trade-off in accuracy (17%).

Future work on the rapid scan methodology includes enhanced development of the interface, integration with our production website, and subsequent testing with different forms of analysis. Subsequent experimental tests could include testing: different species, different times of the day, variable exposure durations, noise-reduced spectrograms, spectrogram compression / length variation, and different numbers of classifications per flashcard.

Additionally, we think further study into the concept of a double run analysis of a dataset is worthwhile. By analyzing each dataset twice with a rapid scan methodology, it might be possible to decrease the drop in accuracy significantly for a trade-off of half the effective speed.

Despite the results, even when processing audio data at 12? speed, any substantial data analysis is still time consuming for a participant. We think the rapid scan methodology will be most useful when combined with multiple analysis techniques. Such techniques could include automatic filtering of the data, natural  integration with our current analysis system, and some form of sampling methodology (either random or smart).

