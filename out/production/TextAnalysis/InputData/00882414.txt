Mining Term Association Rules for Automatic Global Query Expansion:  Methodology and Preliminary Results

Abstract  In this paper we are looking at the mining of association between terms for the automatic expansion of queries. The technique we use for the discovery of the associalions is association rules mining [9]. The technique we propose is more flexible than previous techniques based on term co-occurrence since it takes into account not only the co-occurrence frequency but also the confidence and direction of the association rules. We have been able to consistently improve the effectiveness of the retrieval over the set of 48 test queries on the Associated Press I990 news wires corpus of the l?REC4 benchmark by query expansion using term association rules.

1. Introduction  Most user queries to search engines and information retrieval systems are short and incomplete attempts to describe or characterize the possible documents relevant to the queries. It seems then natural to try and expand the queries with additional terms, which are semantically and/or statistically associated with the original query terms. To a query such as ?What are the predictions concerning the economic effects of the U.K.-Continent underwater tunnel??, documents mentioning the Chunnel or the cities of Calais and Folkestone may have some relevance since the trans-Manche tunnel is called ?Chunnel? and it links Calais to Folkestone. These documents could be missed by a search restricted to the original query words. Why not add these words to the query if we know they could be related to the relevant documents? Such knowledge could be found in a thesaurus or a domain ontology but it could also be the result of the observation that documents in the collection (the corpus), which mention the word ?tunnel)?, often mention the word ?Chunnel? or vice-versa, for instance.

Since the manual construction of a thesaurus or of an ontology is an expensive task requiring several linguists  0-7695-0577-5100 $10.00 0 2000 IEEE  and domain experts menhonth, the effectiveness of an automatic method based on the statistical analysis of the co-occurrence of words in the corpus is very attractive.

In this paper we are looking at the mining of association between terms for the automatic expansion of queries. The technique we use for the discovery of the associations is association rules mining [9]. The technique we propose is more flexible than previous techniques based on term co-occurrence since it takes into account not only the co-occurrence frequency but also the confidence and direction of the association rules.

We use the corpus of Associated Press news wires from TREC4. The corpus is composed of 78,321 documents and 50 topics (queries) are available together with the list of relevant documents.

In section 2, we introduce and summarize our position by presenting an example. In section 3, we present the background techniques and tools, association rule mining, WordNet thesauri, and discuss the relevant related work, local and global query expansion, relevance feedback, or automatic thesaurus construction.

In section 4, we give the details of our methodology for the evaluation of the effectiveness of the query expansion method we are studying. In section 5, we review the preliminary results of our empirical evaluation. Finally, we conclude and outline the next steps of our research.

2. An example This example is the summary of the described results  presented in [ 141.

2.1 Query and Relevant Documents  The corpus we are using for our evaluation is composed of news wires of the year 1990. Let us consider a query looking for news about the ?Status of nuclear proliferation treaties -- violations and monitoring?. There are 52 documents, which are relevant to this query. Here is one such document (we chose a short document):     ?The government said Wednesday that Portugal had sold Iraq uranium concentrate, a material that could be used to produce nuclear weapons.

The Ministry for  Industry and Energy said the government sold 252 tons of the concentrate to the Iraqi Atomic Energy Commission from 1980 to 1982. It said no sales were made a$er 1983.

Under the contract, Iraq agreed to use the uranium for  purely peaceful ends and abide by the nuclear Non-Proliferation Treaty, the ministry said in a statement. A ministry spokesman said the concentrate sold to Iraq was not the enriched variety that can be used directly in nuclear weapons. The government was responding to a report in a Spanish newspaper that said Portugal had exported 1,189 tons of uranium from 1980 to 1988. The report said much of the uranium was purchased by Iraq. It suggested Iraq could have used the material to produce nuclear weapons.?  2.2 Query Expansion  Figure I shows the hypernyms/hyponyms and synoniyms in the WordNet [2,1 I ]  thesaurus for the word ?treaty?. The term ?treaty? is a hypernym of the terms ?alliance?, ?peace treaty?, ?peace?, and ?convention?. A convention is some kind of treaty. WordNet also indicates ?SALT I? and ?SALT 11? which are the acronyms for the first and second treaties between the US and USSR resulting from the Strategic Arms Limitation Talks.

The term ?treaty? is a synonym of the respective terms ?pact? and ?accord?. A treaty is an accord. It is an hyponym of ?written agreement?. A treaty is some kind of written agreement.

written greement  -* pact f accord+------- * treaty + ___-------  / Hypernym  Synonym 4--*  Figure 1  Figure 2 shows the network of associated terms with the corresponding coefficients as obtained from the mining of association rules in the entire corpus as described in section 3.2.

war union ...

U:\ .+219/ ,01462, .55474 .56298 .4 269  00545 33835 treaty unification :,/ . o/54\ P 687 .4 574 .0020  verification Ural .27902 Vienna . . .

Rule: A => B, support=S,confidence=C S,C  Figure 2  Now we can try and improve the system precision and recall by expanding the initial query using the associated terms from the thesaurus or from the mined rules.

The expansion resulting from the use of one or more type of associations in the thesaurus suffers from the generality of this one. Indeed many terms are not used in the corpus despite their semantic relevance. For example, although very relevant to the query term ?treaty?, ?SALT-I? and ?SALT-11? are never mentioned in the large corpus we use them for the expansion. The mined rules on the contrary by constructer only consider terms from the corpus. However, the question of the performance improvement leads to the problem of choosing the type of rules and selecting and tuning the various parameters. For instance, which of the following rules should be chosen to expand the query?

treaty => U.S.

support=.01484, confidence=.56298  treaty => war support=.01219, confidence=.46269  treaty =>  union support=.01462, confidence=.55474  treaty => unification support=.00545, confidence=.20687  unification =>  treaty support=.00545 confidence=.33835     verification => treaty support=.0009 confidence=.44936  Ural = >  treaty support=.00054 confidence=.42574  Vienna =>  treaty support=.00205 confidence=.27902  3. Background and Related Work  3.1. The Information Retrieval Model  The main information retrieval models can be classified into three categories: the Boolean model, the probabilistic model, and the vector space model. For this research we have opted for the vector space model. In the vector space model the documents and queries are seen as vectors in a highly multidimensional space. The (orthogonal) basis of this vector space is the set of unit vectors corresponding to individual terms. A document is expressed as a vector whose coefficients are calculated from the occurrence and the frequency of the different terms inside one document (called the term-frequency).

Such vectors are sparse relatively to the high number of terms from the entire corpus kept for indexing (i.e. the number of dimensions). In other words a document is a vector with kth coefficients d, or 0 depending whether the krh term appears d, times in the documents or not.

Similarly, a query is seen as a document and represented as a vector.

Documents are retrieved according to their similarity to the query. Among the many metrics available for formally defining the notion of similarity, we use a variant of the cosine similarity. The cosine similarity computes the cosine of the angle between the query vector and the document vector.

Given a document vector d = (d,) I=1 and a query q - (q ,) , = I  to ,, the cosine similarity value s -  If q is co-linear to d, i.e. if the query contains the same terms as the document, possibly with different but proportional frequencies, then the document is absolutely similar to the query and the cosine value is 1. As the cosine value decreases towards 0 the angle between the two vectors increases and the document is said to be less similar to the query.

The number of occurrences of a term t in different documents is called the document frequency of the term.

The ratio of the total number of documents by the document frequency of a term t is called the support of  the term and is written as s(t). The variant we use tries to balance the importance of the roles played by the different terms in the query according to their selectivity in the corpus. The coefficients or weights of the respective terms in the query vector are functions of their support. As a result, terms appearing in many documents play a lesser role in the direction of the query vector. The weighting function must be a monotonous function decreasing as the frequency increases. In [ 121 the authors discuss and study several term weighting functions. We adopted the commonly use weighting function In( l/s(t)).

We confirmed empirically for our corpus the superiority of the weighted cosine similarity to the plain cosine similarity. Figure 3 shows the respective average precision of the two similarities for 10 recall levels over 48 queries.

0 05  0 1  02 03 0 4  05 06 07 0 8  09 1 Recall  Figure 3  The recall is defined as the number of relevant documents retrieved over the total number of relevant documents. The precision is defined as the number of relevant document retrieved over the total number of documents retrieved.

3.2. Term Associations Rules  A term association rule is a rule of the form tl => t2 where tl and t2 are terms. A rule is the product of the statistical analysis of a set of documents. It is characterized by both the confidence and the support.

The support of a term or a set of terms is the ratio of the number of documents in which the term or each term of the set appears to the total number of documents. The support of a term t is denoted s(t). The support of a set of terms {ti, ..., tn} is denoted s(tl, ..., tn). The support of a rule t i  => t2 is the support of set { t i ,  t2 }, s(tl, t2). It indicates the amount of positive examples that suggested the association.

The confidence of a rule tl => t2 is the ratio of the support of the rule to the support of ti:     c = s(t,, t?) / s(tJ  A rule with a high confidence indicates that term t2 often occurs in a document where term t, occurs. A rule with high support indicates that many examples were found to suggest the rule.

3.3. WordNet  The WordNet thesaurus is the product of a research project at Princeton University. It is an electronic lexical reference system for the English language whose design is inspired by current psycholinguistic theories of human lexical memory [2,11]. Its relative comprehensiveness, its free of charge distribution, and the numerous tools and associated data sets accompanying it have made it a popular tool among text and natural language processing researchers. It has the potential to become a standard and has already inspired compatible developments in other languages [IO], paving the road to multi- and cross- lingual applications.

In WordNet, English nouns, verbs, adjectives, and adverbs are organised into synonym sets called synsets.

Each synset consists of a list of synonymous word forms, representing one underlying lexical concept. Semantic pointers describe relationships between the synsets. For instance the synset No. 0059280 1 represent the synonym set { ?industry?, ?manufacture?, ?manufacturing?}.

WordNet contains approximately 122,000 terms grouped into approximately 99,000 synsets and it is constantly being updated and extended. The synset represents the synonym equivalence relation. The main relations between synsets are the hyponym and hypemym , and meronym and holonym reciprocal relations. For nouns only, there are 78,000 Hyponym/Hypemym pairs, 12,000 MeronymslHolonyms pairs in WordNet.

An ?oak? (synset 07925865) is a kind of ?tree? (synset 09396070). This is an example of hyper/hyponym relation. Synset 07925865 is a hyponym of synset 09396070 and synset 09396070 is a hypernym of 07925865. These relations are anti-symmetrical. The depth of this hierarchy is less than 12 synsets.

A ?pitcher?(synset 004642 13) is a part of a?basebal1- team? (synset 06022036). This is an example of mero/holonym relation. Synset 00464213 is a meronym of synset 06022036 and synset 06022036 is a holonym of synset 004642 13.

3.4.Query Expansion and Thesauri  Whether they are well-formed affirmations or questions, or ungrammatical lists of words, queries in the information retrieval models we consider boil down to often short and incomplete sets of terms. Query expansion hypothesizes that the addition of terms to the query has the potential to overcome these weaknesses and improve the effectiveness of the retrieval. The empirical results from various authors show unfortunately that improvement is difficult to achieve.

Several authors seem to suggest from their experience that improvements of 5 to 10% are very significant achievements [ 151.

Let us classify the expansion methods according to two criteria into four categories. An expansion method may be using local or global knowledge, i.e. look at the answers to a query or at the entire corpus. An expansion method may be using human input or be fully automatic.

Global methods use the entire corpus or external knowledge to expand the query. The knowledge used comes in the form of a thesaurus. A thesaurus is a data structure recording associations between terms.

Man made Thesaurus such as WordNet, whether they are general purpose of domain specific, records linguistic or conceptual associations between the terms as identified by human experts. In [5] and [3] the authors report various experiments with the WordNet thesaurus.

In [ 5 ]  they manually expand the queries using all relations in the thesaurus: hyper/hyponyms and synonyms. After tuning of parameters and weights, they report up to 1.7 % improvement of the average precision (over several levels of recall). The experiments use a TREC3 corpus. In [3] they only use the synonym relations. The expansion is automatic but applies to both the query and the documents.

Several authors have attempted to automatically mine the associations for query expansion from the corpus.

Most approaches are based on clustering of terms in the document space. Intuitively, clustering captures synonymous associations. In [7] the authors present a global analysis approach based on association rules mining. However, their objective is not to find associations for query expansion but rather to construct a classification of the documents.

The local set of documents is the small set retrieved with the initial unexpanded query. Local methods use the local set to discover the additional candidate terms to be added to the query. The user may indicate which documents are relevant in the local set. This is called relevance feedback. In that case the frequent words in the relevant documents can be used to expand the initial query and/or re-weight the query terms. The first positive result with such a technique was obtained by Salton as early as 1971in the SMART system. Alternatively,     associations of terms within the local set can be mined and chosen automatically by methods similar to the one we are studying in this paper. Like the global methods, most of the proposed techniques are based on clustering.

In [8] the authors use the confidence of the association between terms (they call the conditional probability) to construct a term hierarchy from the local set. They do not take the support of the terms and of the association into account. Although they indicate that such a hierarchy can be used for manual or automatic query expansion they do not further study the issue.

For a more exhaustive overview of earlier work on local and global query expansion as well as thesauri construction see [12].

The method we are studying is a global automatic query expansion method. There are few available studies on such methods for significantly large corpus, as the experiments are lengthy and greedy in memory utilization.

4. Methodology  4.1. The Corpus and the Topics  For this experiment we chose one of the TREC4 corpus, namely AP90, a collection of news wires from 1990 by the Associated Press. The corpus contains more than 78,000 documents. After stemming and the filtering of stop words (see below), we found more than 133,000 different terms. The average number of different terms in a document is 217. The largest documents contain approximately 900 terms and the smallest 5 terms.

Figure 4 shows the distribution and the cumulative distribution of the ration of the number of documents over the document frequency of the terms. There are many terms, which appear in very few documents  Term dstribution (freq from 1 to 50)  0 5 1 0 1 5 2 3 2 5 3 0 3 5 4 0 4 5 5 0 Term irecyency  Figure 4  From this graph, we can see most terms are very infrequent. More than 70% of them appear in less than 6 documents and only 10% appear in more than 50 documents.

There are 50 queries, called topics in the TREC terminology, associated with AP90 which consist of natural language propositions and questions. For each of these topics, a list of relevant documents has been constructed manually and endorsed by human experts.

This list is provided together with the topic. On average a topic for AP90 calls for 32 documents. The largest topic calls for 145 documents. Two topics call no relevant document at all. Our measure being the relative recaWprecision performance, we eliminated these two topics from our statistics (at recall loo%, i.e. an empty answer, any method results in a maximum precision of 100%).

4.2. Summary Architecture and System Implementation  We use the Oracle 8 database as a repository for the documents and indices we build. The application programs for the processing of documents and queries are written in Java and use SQL through JDBC to query the database.

The system is composed of four main modules: the indexing module, the association rule miner, the query expander, and the query processor.

The indexing module constructs an inverted index of the corpus after tokenizing and stemming of the words in each document. The tokenizer identifies the terms on the basis of a set of simple rules (essentially white space and punctuation separation) as well as eliminates the stop words. The stop words list we use is the one provided with the WordNet thesaurus. It contains some 600 words.

Although we could have later used the term frequency to eliminate stop words on a statistical criterion, we felt that the filtering of a stop word list would be practical.

The tokens or words are then stemmed using the standard Porter stemming algorithm which we extended using the stemming rules included into the WordNet distribution. These rules essentially govern the declension of irregular nouns and adjectives and the conjugation of irregular verbs. We call terms the stemmed tokens. Finally, the inverted index simply consists of a table recording in which document a term appears. It is clear to us that, although very simple and convenient, this data-structure is not the most appropriate for efficient query processing.

The query processor computes a weighted-cosine similarity. The query is processed in the same way     documents are processed. Each term in the query is weighted according to a function of its actual frequency in the corpus. Both the query and the documents are seen as vectors and the cosine of the two vectors is computed.

The query expansion module uses alternatively one of the term relation either constructed by the association rule miner or from WordNet (we translated the main WordNet relations into terms and stored them into Oracle.)  5. Experimental results  In the experiments, each query and document is treated as a vector in the space of terms. We compute the cosine value of the query and each document to measure the similarity between them. See the formula in section 3.1. The query terms are weighted using the method mentioned in the same section.

After the cosine value of each document computed, they are ranked by their value in descend order. Using this ranked list, we compute precision value at recall point from 0.1 to 1 .O. Take query 202 for example, it has 52 relevant documents. So the recall point 0.1 means the point of the first 5 relevant documents are retrieved, say it?s the nth document. Then the precision is 5111. The graphs below show the average precision of each recall point among the 48 topics. The higher the corresponding plots the more effective the retrieval.

5.1. Query Expansion using WordNet  Average precision using expansion methOd(W0rdrlet)  Noexpansim + Expansion with Wordhlet(H pernym) ---x---- - Expansion wim Wmme*(Jynmym) * - Expanson wlthWordNet(Hypmym) - U  I 0 1  0 2  0 3  0 4  05 06 0 7  0 8  0 9  1  Recall  Figure 5  If we use the WordNet associations to expand the query, we do not observe consistent improvement.

Figure 5 shows the effectiveness of the expanded query in comparison to the initial query.

The random performance has numerous causes, one important one being the fact that the general thesaurus, although semantically ideal, may not reflect the  vocabulary, jargon, and usage within the corpus. The ?SALT-I? and ?SALT-11? are examples of this kind.

Expansion with Hypernym or Synonym changes the performance little. But expansion with Hyponym hurts the performance much. It?s because Hyponyms are narrower meaning words of the query terms. A query term might be expanded with many of it?s specific meaning words, which are not related to the query. An example is term ?status? in topic 202, which is ?Status of nuclear proliferation treaties -- violations and monitoring?. There are 146 kinds of ?status? such as ?safe?, ?danger? etc. Many of them are not relevant to the topic.

5.2. Query Expansion using Association Rules Q=>X  No expansron t  g 0 . 2 ; 0.15  0.1 0.05  0.1 0.2 0.3 0.4 0 . 5  0 . 6  0.7 0.8 0.9 1 Recall  Figure 6  Figure 6 shows the performance of expansion with terms implied by the query terms. When using the association rules, we need set a low-bound of support.

Rules with support below this bound are thought appear by casual. We select rules whose support is bigger than 0.001. That means the query word and expanded words co-occurred in at least 0.1% of the documents. Since the AP90 is topic diverse, this support is not low.

The parameters are chosen by hand, but it?s applied to all 48 queries. This method improves the first 4 recall points and degrades a little in the rest. In real retrieval system, users focus on the former ones. They wish a good quality in high ranked documents rather than the overall performance.

We have eliminated the stop words when created the inverted index. But there are still some words appear in too many documents in the corpus. A typical example is the word ?report?. It?s often in the news but not a common stop word. In AP90, it appeared in30,302 out of 78,321 documents. These words are noise and will hurt the retrieval so we tried to set a high-bound for the support of expanded words. The performance is shown in Figure 7.

c 0  : a  Average p e m o n  using expansion methcd(Q:d C > 4  OOl<S< 01)  Noexpansion -+- Expansion with rules(P=>X Cz 4 O O k S <  011 --*--  I I I I i  0 1  02 03 0 4  0 5  06 07 O B  0 9  1 Recall  Figure 7  With the support filtering, words expanded in this method are general meaning of query terms and not appear frequently in the corpus. This method improved the precision on every recall points. The average improvement is 5.39%. As an informal rule among IR researchers, a 5% improvement in average precision is considered to be significant and a 10% improvement very significant fiom the users? perspective [15].

This method degraded the performance consistently.

Words expanded through this way are mostly specific to the query words. In our intuition, specific words should perform better than more general meaning words. But on the other hand, expansion with specific words is more risky than general meaning words. This is because many words expanded here are specific to the query words but not to the query itself. This condition is similar to the expansion with Hyponym of WordNet.

5.4. Query Expansion using Association Rules X*Q  Average precisron YSing expansLon ~ e t h o d l ~ = ~ X , C 1 ~ . 4 . C 2 ~ . ~ , ? l X l < O . 2 1  NO expansLon + Erpansion uLth rules IW->X.Cl>.I, C2,.4,SIXI (0.2) ---Y--  0 . 6  -  2 0.3 -  0.2 -  0.1 -  0.1 0.2 0.3 0 . 4  0 . S  0 . 6  0.1 0 . 8  0.9 1 Recall  5.3. Query Expansion using Association Rules X=>Q  Figure 9  0.1 0.2 0.3 0 .4  0 .5  0 . 6  0.7 0 . 8  0.3 1 R e C . l l  Figure 8  Figure 8 shows the performance of expansion with words implying query terms. From the word frequency distribulion we found that the query words are relatively common words, i.e. have high frequency (support). In a rule, it?s very often that low support word implies high support word when choose a high confidence threshold.

So in this method, the candidates of expansion words have low support and there are too many such words. We set a higher confidence here than in the former method so as to limit the number of words expanded and did not set the high-bound of support of expanded words because the support of these rules are not that high.

Here we expand words implying and implied by query words with both confidence bigger than 0.4. We think these words expanded with both rules direction are context synonyms to query words. The parameters chosen here are very strict that we expanded only 6 queries. Topic 247 ?What are the predictions concerning the economic effects of the U.K.-Continent underwater tunnel?? is one of them. ?folkeston? and ?transmanch? were expanded from ?chunnel?. Words expanded here are not really synonyms, but closely related. The ?Chunnel? links Calais to Folkestone. This method improves the average precision 12.2%, which is very significant according to [ 151.

6. Conclusion and Future Work  6.1 Support and Confidence  We have been able to consistently improve the effectiveness of the retrieval over the set of 48 test queries on the Associated Press 1990 news wires corpus of the TREC4 benchmark by query expansion using term association rules.

However, the tuning of the main parameters for the selection of the rules, the ranges of the support and the confidence, was done in a non-systematic way due to the duration of each experiment. Such a running is possible with association rules since they add more control parameters (confidence and direction) to the previously proposed methods based on the sole term co-occurrence.

Encouraged by these preliminary results yielding a rare performance increase of up to 5%, we are now embarking on the systematic evaluation of the influence of the parameters.

6.2 Polysemy and Disambiguation  An important property of words and terms is their ability to have several concurrent meaning. This maybe undesirable feature is called polysemy. The term ?Java? for instance can refer to the popular programming language, the popular coffee, which gave its name to the programming language, or the popular tourist destination, which gave its name to the coffee. In a query ?Development programs in the island of java? only the co-occurrence of the terms ?island? and ?java? (which happen to have a direct hypernym relation in this case) can help resolve the ambiguity which is unfortunately reinforced by the terms ?program? and ?development?.

We would like to study the opportunity for such disambiguation using direct or indirect relations from mined term associations. This could potentially help prevent the expansion to words like ?soyabean? in queries of the form ?nuclear plant?. Indeed the general form of association rules can be t,, . . ., tk => tL+,, . . ., t,  6.3. Multi Lingual Applications  Finally a next objective of this research is to mine term associations for corpus written in languages for which Thesaurus such as WordNet are not readily available or too expensive. We wish to study text information retrieval and query expansion for languages such as Malay/Indonesian and Mandarin with, respectively, two hundred and twenty million and one billion native speakers joining the information society.

