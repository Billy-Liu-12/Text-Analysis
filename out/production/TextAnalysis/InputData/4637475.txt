Research on Application of Improved Association rules Algorithm in  intelligent QA system

Abstract  With increasingly prevalent of E-learning, the intelligent Q/A system arise at the historic moment. In this paper we  proposed  an improved text cluster algorithm, with the improved association rules algorithm It can classify the information in the database accurately according to the questions, locate the user? question fast and therefore speeds up the inquiry rate. The experimental result indicates that this plan has the merits of intelligence, self-renewing, and efficient Q/A and so on.

1. Introduction  With the development of Internet technology, E- learning becomes increasingly popular. Facing the difficult questions appearing in study, most learners hope to obtain a satisfied answer fastly through a learning platform. There is no doubt that the intelligence Q/A system can meet request of learners. In this paper we designed an intelligence Q/A system with text data mining algorithm , which applies the improved text cluster algorithm and effectively speeds up the inquiry rate.

2. Association rules algorithm and its improvement  The association rules and its algorithm always count for much in data mining as a result of its huge commercial value and the theoretic value since being proposed.

According to the definition of association rules, its discovery duty may be defined as: Assigning a business database D, extract all association rules which satisfy the smallest support--Smin and the smallest confidence? Cmin  2.1 The classical association rules mining algorithm the Apriori algorithm and its flaw  The Classical Apriori algorithm is an association Rules Mining algorithm with Width first.

Step 1: when scanning the database, calculate the support of all single projects in the database and form a single dimension frequent item-set with the projects whose support is bigger than the minsupp, namely L1.

Then repeat scanning the database. When scanning the kth time, namely L k, the frequent item set is produced with k length. When scanning the (k+1)th time, firstly produce the candidate item set Ck+1 with k+1 length from L k, secondly use Hash tree to scan the C k+1 it produces a frequent item set with k+1 length namely L k+1, until no production of frequent item set. The final frequent item set is L k. Its merit lies in the association rules, which can effectively prune item sets and does not produce and calculate those candidate item sets which are impossible to become frequent item-set. And thus we can obtain the smaller candidate item sets.

But the Apriori algorithm has some shortcomings as follows:  (1) The algorithm has some problem in the efficiency, possibly producing the massive candidate sets. Too many scanning times are the primary cause, in that the database needs to be scanned once when seeking every k frequent item set (k=1, 2,?, K), totally K times. In addition, the produced candidate item sets are too many when the pattern is too long. Therefore when the database or K is too large, the running time of the algorithm would be too long or is unable to be completed. So the algorithm has not great extendibility and is difficult to be promoted.

(2) The algorithm produces too many false (redundancy) rules. When the database is too large or the support or the confidence threshold value is too low, too many rules will be produced. And it is very difficult for users to discriminate and judge these rules and find truly useful information.

2.2 The improved association rules algorithm?? the Apriori+ algorithm  2.2.1 The basic conceptions  Aiming at the two major faults above, We propose an improved association rules algorithm the Apriori+   DOI 10.1109/WGEC.2008.74     algorithm. According to the decision-making attribute in mining, the insignificant records in data source are eliminated through the reduction operation. Thus we can reduce the running time of programs, save the space, and thereby raise the mining efficiency greatly and solve the redundancy in a certain extent.

To describe the algorithm well, there are several conceptions as follows.

Definition 1 the weighting function Supposing D is a record set of historical transaction  data; ti is the ith record and also a vector constituting a  group of attributes: ti = {t1[a1],t2[a2],?, ti [ai], t1 [b1], t2 [b2],?,tm[bm]} ti [ai] is the value of decision-making attribute a i  when in record t i ; tm[bm] is the value of condition attribute bm when in record tm. Take weight W i as a new attribute to t i and form a new record tw i. The record set of historical data added with weight attribute is called Dw:  twi= {t1[a1],t2[a2],?, ti [ai], t1 [b1], t2 [b2],?,tm[bm],wi}  The correspondent weight wi of the ith record is defined as  wi=f(t1[a1],t2[a2],?, ti [ai])  //The weight is defined as a kind of function of one or several decision-making attributes in the record  Definition 2 the weight support The weight support of the Apriori+ algorithm is:    [ ] ( )  n m  i j jj i  n  i i  w X t a Supp x y  w  ? ?  ?  ? ?  ? ? 1  Definition 3 the confidence the confidence of the Apriori+ algorithm is  ( ) ( )  ( )  Supp X Y Conf X Y  Supp X ? ?  ?  2.2.2 The description of the Apriori+ algorithm  There are two main functions: (1) Pretreat the original data source, clean those  records whose items are less than the experiential rules requested, and obtain a subset  (2) In the subset database, add weight wi to each record according to the function of certain decision- making attributes. Compute the support and the confidence through weight and delete the records with low support.

The Algorithm flow is shown as Figure 1.

Input : database: attri_supp Overall mini_supp: Overall mini_conf  Fig.1 The Apriori+  algorithm flow chart  The algorithm is described as follows: Output: large item set {kLk k 1 andLk  }.

DBset is the regular subset; nRulesItemCount is the least number of items needed  to meet the rules; f(ti[ai]) is the weighting function of decision-making  attribute; w i  is the weighting function value; Lwk is the large item set of the weighting function;  Cwk is the candidate set of the weighting function; Wfoun-tionsup is the support of the weighting function; fCWConf is the confidence of the weighting function;  //Produce regular subset database with experiential rules --DBset,  D= {order by the number of items in each record}; Hk= {take rules from the training storehouse and  obtain the least number of items-- Item-Count} while (! DBsettable.IsEOF()) do begin {if(nRecordsItemCount<nRulesItemCount) DBsettable.Delete();//Delete this record, and thus  obtain the subset storehouse --DBset} // discover the large item set from DBset; for(i=0;! DBsettable.IsEOF();i++) do begin     {wi={f(t1[a1],t2[a2],?,ti[ai])}  // Establish the weighting function of decision-making attributes in DBset;}  total-weight=   // Sum the weighting function in the subset database;  Lw1={large1-itemsets of subdatabase }; Cw1=Lw1 of subdatabase; for(k=2;L (k-1);k++) do begin; Cwk=apriori-gen(Lwk-1); // let Cwk=Lwk-1*Lwk-1  when the first k-2 items of Lwk-1 are the same. For each produced candidate k- item set, inspect wether its all (k- 1)- item subsets are frequent (k-1)- item sets without rules.

If they are not, delete the candidate item set;  for all twi DBset do begin // determine which item sets in candidate ones are contained in transactions that take twi as symbol  Cwt=subset(Cw2,t)//produce all (k+1)- item subsets, and for each such subset,judge whether its all k- item subsets are candidate item sets.And filter the possible candidate (k+1)- item sets. If there are some items which have never appear in these candidate (k+1)- item sets, then delete these items, deposit the others in t;  for all candidatesc Cwk do begin if (Cwt  ) then do begin c.weight+=tw.weight; end end Wfountionsup=c.weight/total-weight Lwk={c Cwk ( Wfountionsup) >minisup} // delete  the records whose support is smaller than the minsupp.

end // judge whether Lw2 is the largest item set. If it is,  compute its confidence and produce rule model if (Lw2 is the large item set) {fCWConf= { calculate the confidence based on  Wfountionsup} if (fCWConf> Wfountionsup) {ValuabeRules= {the produced valid rules} fCWConf=0.0f} else fCWConf=0.0f} else Wfountionsup=0.0f} end This algorithm has remarkable merits.

Firstly it cleans the records whose nRecordsItemCount  is smaller than nRulesItemCount--the least number of items needed for known rules.And thus obtain the subset storehouse-- DBset.

Also use Weight to express the orginal weight.

Substitute n (the sum of records in traditional algorithm)  with total-weight the sum of weights; Substitute c.count(the sum of transactions with tradition algorithm )with  c.weight(the accumulation of weights), and its weight support is c.weight/total-weight.

Then delete the records whose weight support is smaller than minsupp so that we can reduce the time and the complexity of space.

3.  The design of intelligent Q/A system  On the basis of the above association rules algorithm, this system firstly segments the questions of learners with data mining text cluster algorithm. The cut words will be saved together with their correspondent documents in a table. Then carry out the association rules algorithm and the weight calculating algorithm separately, and obtain weight value of each words and association rules based on the key words in each document. Secondly write the first component the consequent of association rules in the association table, then calculate similarity of the text, and thus get the similarity of sentences. After obtaining the similarity, use it to discover the current best answer to the question. Thus meet the request of learners and the intelligence Q/A system can be realized.

This function frame can be designed as Figure 2.

Fig.2 The function frame of intelligence QA system  4. The experimental data and analysis  In this experiment, there are 300 records about questions and 1035 records about answers.

Figure 3 shows the comparison of mining time with two algorithms, with different smallest complete weighting support threshold values;  Figure 4 shows the change of the number of the candidate item sets about the test algorithm,under     different smallest complete weighting support threshold values.

Fig 3. Mining time comparison under differentsupport thershold  Fig 4 Size of the candidate items under different support threshold From the above experiment, we can know that the  mining time of the Apriori algorithm is 21.53% more than that of the Apriori+ algorithm in average, 38.42% most.

With the same quantity of mined frequent item sets, the number of candidate item sets produced by the Apriori algorithm is 27.02% more than the Apriori+ algorithm produces in average, 45.61% most. The experimental results indicate that the Apriori+ algorithm truly has higher mining efficiency than the Apriori algorithm.

4. The conclusion  In this paper we adopt the improved association rules in text cluster,it can reduces the cluster time, and raises the data mining efficiency. Thus can locate the learner's question fast and speed the Q/A to a certain degree. It has the merits of intelligence, self-renewing and high Q/A efficiency and so on. As a result of scientific research condition and limited time it has some fault yet the accurate and fast Q/A is still the aim of the system.

