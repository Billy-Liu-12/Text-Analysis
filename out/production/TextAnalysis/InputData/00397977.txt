FUZZY -NEURAL PATTERN RECOGNITION

Abstract - The common techniques of neural networks seem to can be apply perfectly to the classification and to the pattern recognition, but the learning can sometime be long and the complexity of the final network very big. An association between the neural networks and the fuzzy way seems feasible and looks to can make significantly easier the learning period and to allow also to simplify its structure. The fuzzy logic expresses the knowledge in an explicit manner by its rules of inferences, while the neural networks assimilate that knowledge in the weight during the learning, the knowledge is so here implicit. In merging these two approaching, we can introduce the connectionnist techniques in the fuzzy logic operating in the best way for every underlying analogies between these two approaching.

It is that last possibility that is handled in that work We can, by that association build a neural network of simple structure where we meet the stage of the fuzzy reasoning .The work here described tries in a first time to identify a paraboloid by a simple structure in the aim to well understand the developed association in the aim to apply it subsequently to the recognition of characters.

I~monucrrox  Fuzzy Logic is used in a lot of fields (Controllers, pattern recognition, ...) since its apparition in 1974. Some applications have even been marketed in Japan in particular in controlled appliances.

But its used in pattern recognition remains modest, results not being very promoted.

At the opposite, neural networks (NNs) make the object of a lot of researches. They bring up a different approach than linguistic or a l g o r i t h c  systems. Their two main qualities are their parallel structure which offer a good noise resistivity and their ability to learn from examples. NNs have, nowadays, a large field of applications from model and control systems prediction to pattern recognition.

Since little time, researchers work about NNs and Fuzzy Logic association . In fact, both t q  to reproduce the human reasoning.

There are several ways to include their own qualities : We can introduce Fuzzy Logic in the elaboration of the network, or put connections into fuzz) rules, but it could be possible to make a fuzzy-neural network. This is this thud solution that we've study.

The target of this article is to suggest a feasible association for pattern recognition and more specifically for characters recognition.

Beyond NNs, we '11 tackle, in a first time, the F u z q  Logic linguistic to display in a second time, a fuzzy-neural network. A thud part consists of testing this association by applications, in particular by the recognition of a paraboloid and by a type characters recognition approach.

THE YEXJRX NETWORKS  Introduction  The target of NNs is at once to understand and then to try to reproduce the human brain working whch always intrigues a lot of researchers by its incredible quickness and its fabulous ability to work in spite of the lack of explicit instructions. Then, we've tried to  fax: 35.14.66.18  reproduce the mechanics of human brain apprenticeshp thanks to an artificial network . There are different productions of NNs distributed in two basic groups : layers networks and loops networks. We'll do not here study these different types. We '11 just describe in part layers networks and more especially multi-layers networks which will be used for our association.

.bfulti-luyers network:  The multi-layers network is characterized by a neural distribution in layers. We distinguish three sorts of layers : The input layer which transmits only data froin input to neural of the first hidden layer. One or more hidden layers which extract characteristics from input vector which square with the treatment of the inpattemation. The output layer which give the answer of the network.

Each neural is connected either to the nearer layer (then we talk about simple layers networks), or to any layer located downstream (these networks are then called not simple layers networks). The connections between neural could also be done from a layer to itself but they almost look like loops networks connections.

i-lpprenticeship:  The big problem of NNs is the apprenticeship of weights a i because all the knowledge is memorized in the synaptic coefficients which are fixed during the apprenticeship step . We distinguish two sorts of apprenticeship : The supervised apprenticeship where input- output associations are presented to the network by a "teacher", and the not-supervised apprenticeship where we donY know outputs in relation to inputs. There are different learning methods (the minor square method, the retropropagation of gradient, the Windrow-Hoff method, .) whch target is to minimize a criterion (often called "cost function") by weights adjustment. We won't describe here these methods  THE FUZZY LOGIC  Introduction:  The human reasoning bases generally on imperfect knowledge , either because we have a doubt about their validity, then they are uncertain ("perhaps she 'I1 come tomorrow"), or because we meet a difficult). to express them clearly, then they are inexplicit ("she '11 come tomorrow but I don't h o w  when"). Probabilistic methods grapple to uncertain events when the Fuzzy Logic is a concept whlch allows to treat the human inexplicit reasoning. Fuzzy sets theory permits to generalize events from natural causes appealing to mathematic tools. It presents at least 2 interests : It allows to modelize inexplicit or vague knowledge ; It is the only mean to treat at the same time numeric and symbolic knowledge.

Here is a scheme (see figure 1) representing the general methodolom for the Fuzzy Logic utilization :  n z ~ + ~ l d  Inference W u z z f i a t i m k -Figure 1-  0-7803-1328-3/94$03.000 1994 IEEE    Piczzi$cution  ?This is a stage of transition from a numerical to a symbolic field .

This stage is indispensable as soon as we want to manipulate precise or not physical data thanks to the fuzzy set theory. It allows for one thmg to determinate the belonging function to a fuzzy set.

Fuzzy set Here is the definition of a fuzq set according to  Given a set X countable or not, and x an element of X, then a ZADEH :  fuzzy set A member of X is a set of couples such as equation ( 1 ):  vs E X  { ( x , d x )  ) 1 (1) with d x )  the belonging function which takes data in the  interval [O; 1 1.

Ifthis interval is reduced to the @ ; I )  value. then the set ?4 is  an ordinmy set.

Belonging Function It determinates 3 parts : - One area where referential elements belong completely to a  fuzzy set ; a cc(x)=l - One area where elements are excluded, 3 cc(x) = 0 - One fwzy area (called transition area ) where belonging to  the set is estimated at different degrees . 3 O<cc(x)<l The belonging function is the belonging degree of s to the fuzzy  set. Its part consists in precising numerically the meaning of a fuzzy set expressed in principle in linguistic pattem. This is the transition of numerical to symbolic field. There are dffaent belonging hc t ions  such as function in trapezium pattem ; triangular function ; Gaussian function (continually derivable). Pattem and parameters of different belonging functions must be adjusted in particular case to treat.

Inference  It consists in treating the inpattemation by f m y  rules Fuzq rules They reveal a liilk: between elementaq fuzzy  propositions : ?The error IS big andpositive, then the control is zero?. . . They pattem the mference systems core used in control and they relate the reasoning a hunan could make about an imperfectly defined knowledge. A fuvy rule breaks up in 2 parts :  The premises : The place where one or several fuzzy propositions are set out.

The conclusion or the consequence : The answer.

A rule describes a causal llnk between the premises of data si  and the conclusion thlnks to fuzzy implication operators.

Fuzz), implication operaror.s There are different operators of f w q  implication : T-nomi, T-conorm, Minimum of Mandani and Zadeh, Maximum of 1,ukasiewicz and Giles, product of Goguen. . . .

The 2 operators usually used in the applications of the fuzzy sets theory are the Max. and Minus functions . Nevertheless, T-norm functions could to be used as conjunction operators instead of minus functions, while T-cononns could take place of the Max. in disjunctions.

I ~3fuzzrfication  Unfuzzification is the operation which allows to get through a linguistic datum pattem perception, a numerical variable physically applicable . this is the opposite step of fuzzification. There are several feasible methods including the Max. and the center of gravity. When the conclusion of the rule is distinct, specific and sure (that is when all of the fuzzy rules facing one to another have a no-fuzzy conclusion), unfuzzification is then implicit.

Example : If the outside temperature is high efuzzy proposition.

Then turn off the heating. *distinct, specific and  3 The fuzzification will give the belonging degree which allows to obtain the consequence thanks to fuzzy rules. The unkification will give the numerical result.

sure decision.

Association  We can represent thls analyze in the pattem as follow @@re 65  - Figure 2-  The first hdden layer neural represent different areas of the Fuzzy Logic. Then, the different outputs of th~s layer give us the belonging degree correspondent to the different areas. We introduce in the second hidden layer, rules of the Fuzzy Logic inference.

And then, we obtain the rule: IF x AND IF y THEN Currsequence.

The output layer will give us the result wluch could be obtained by unfuzzification. The scheme above (see figure3) hghlight the mass parallelism between the fuzzy reasoning and the coilnectionnist perceptioii. We recopze  NNs in the weights incorporation by apprenticeship.

- Figure 3-  We can add more or less predefined weights which give a tipping between exDlicit knowledpe (@Fuzzy Logic The weights are all predefined) and implicit reasoning (@Neural Networks : Weights are established b,y apprenticeship).

Conclusion.

This association seems to bring about the apprenticeship which is thus less complex (some weights being preestablished) and about the comprehension of the knowledge course in NNs (raising gradually the degree of complexity, we could see the evolution of the reasoning)  APPLICATIONS  In a first time: we have test this fuzzy-neural network to identify a paraboloid  R ~ c o ~ ~ r r r o s  OF .A CLRVE : represented by the equation (2)  y = s12 + x22 (2) This network will be composed of  - two input x l  and s.2 such as . - 1 -I<+ 1 and - 1 &-z<+ 1     - a output y : 05- This application which gives a simple structure, will allow us to  well understand the Fuzzy-neural association for a subsequent implementation for characters recognition.

Specification  First, we must built the fuzzy-neural network correspondent at &IS example . That's why, we '11 follow Fuzzy Logic reasoning  Fuzzification It's the matter to break up the sets composed by x1 and x2 in fuzzy areas. x1 and x2 have the same probability on their set [-1;+1]; So, there's no reason to privilege a part rather than another : That's why each area [-1;+1] is decomposed in equal parts. We have cut up the area in 5 equal parts :  courses.

- the "Zero" area : center (0);  - the "Big Positive "area : center (+l); - the "Middle Positive " area : center (0.5);  - the "Middle Negative " area : center (-0.5); - the "Big Negative " area : center (-1 ),  There are several n ~ e r s  to represent areas in Fuzzy Logic. We have choose the Gaussian function which eliminates the problem of derivability (important for weights apprenticeship), of intersection, of disymmetical area, _._  and which presents the advantage to have no distinct cutting up at the area (limits at the ends).

The equation of a Gaussian is like the model as follow (see equation (3)):  y = exp[-(an + by] ( 3 )  and b= -a.m .\I - h ( E )  with a= ak  The symmetrical curve of Gauss h c t i o n  is then represented by  mi : center of the fuzzy area i considered (= localization of three parameters (m,&,&) whch correspond at :  the area) each parameter mi of the 5 fuzzy areas have been fixed as :  m~ = - I  c3 center of the "Big Negative" area.

ml = -0.5 c9 center of the "Middle Negative" area.

m2 = 0 a center of the "Zero" area.

m3 = 0.5 c9 center of the "Middle Positive" area.

m4 = 1 c3 center of the "Big Positive" area.

E : residual va lue allowed at the & distance from the Gaussian center. We've chosen F 0.1 for & = 0.5 , Then E and mC will be constant because all the areas are identical.

bi = -a.mi, bi is then fixed for each area, a being a constant  The output of a neural in the frst hidden layer must answer to equation (4) :  output-neural j = exp[-(a.input-neural j + bj y ] (4) That well corresponds to a belonging degree of an input x i to ttUs  area. A neural, making in general two tasks well specified (l .A weighted sum; 2.A processing). will be represented by the next pattern:  -+ with equation (5) G = exp[- 2'1  - Figure 4-  The weights a and b (seefigure 4 )  are here preestablished. So, it won't be weights apprenticeship between the input layer and the first hidden one. We see that each neural of the first hidden layer will have 2 input : xi and 1  We have then created 5 areas by input data that we '11 connected two by two, this which corresponds at the creation of 25 neural (5x5) in the second hidden layer. This one corresponds to the fuzzy mference rule : If. . .AND. . .'ITEN.. . .

Inference : we had to choose a rule of inference adaptable to our problem. That's why, we have executed a reasoning which gave us the type of decision to take in contact with the different areas. The 5 Gaussian functions which determine the belonging degree, are V ~ I Y displayed into the space; Then, a point of the space will belong to several Gaussian functions but the more even to insignificant way.

We 'd try to eliminate these residues. That's why , at once, the belonging degree will be added to each component. Then, the insignificant data will be deleted thanks to the Max.hction (see equation (6)).

If we standardize this equation, we recognize the rule of inference of Lukasiewic. such as equation (7).

(7)  (9 )  In fact, there are several possibilities (We could take a T-conorm for example) but this case seems to suit to our problem because :  - It's simple and easily computable, - It fits perfectly itself to neural case : = weighted input  sums (oequation (8) ) following by the f function (o equation (9)).

For the moment, we restrict to this inference rule.

Now, we incorporate this method to the network. The activation  of the neural which implements the Max function of Lukasiewicz, is realized by a weighted sum corresponding of the equation (8). The processing is then represented by the h c t i o n  f (equation (9)). The implementation of this rule could be represented by the next scheme (seefigure 3):  -Figure 5-  The synaptic weights are preestablished and equals to -1, 1 and 1 (seefigure 4) .  There won't be weights learning between the first and the second hidden layer. Moreover, we notice each neural of th~s hidden layer have three inputs.

Unjiuzzijication That's the fkzification opposite operation, that is a symbolic variable will be transpattem in a numerical language. In general, this function comply at the end of the processing, so, it's situated at the output layer. The two more standard methods use either the Maximum or the Barycenter function. Taking the last one permits to take all the receipted inpatternations into account whereas the Maximum method just take the more important inpattemation into consideration. That's why we have choose to take the center of gravity method. As there is just one output (see equation (2)), the unfuification hold only one neural. We reach then the final network (seefigure 2).

The difference between Fuzzy Logic and NNs is the add of weights between the different links. The output y will take different     inpatternations into account (barycenter) but with various degrees thanks to the weights. Which data to give to these synapses? That's the part of the apprenticeshp.

Apprenticeship : The apprenticeship consists in determining the weights in order that the network respond correctly to our problem (two inputs XI , x2 and one output y such as y resolve the equation (2)). We'll use a supervised method which the target is to minimize a cost function by weights progressive adjustment. The weights raised during the transition between the input layer and the f i s t hidden layer and between the f i s t  hidden layer and the second one, are preestablished by the Fuzzy Logic rules. Apprenticeship is situated between the last hdden layer and the output neural.

The first hidden layer represents fuzzy areas set. As there are two inputs, a representation of these areas could be done in a two dimensions space. Each parcel will correspond to an intersection between a Gaussian for the "1 input and another for the x2 one. The second hidden layer executes at once the sum of the equation (8) which is equivalent to a bumbs field (3D Gaussian function) filed down by the Max function. We notice here there will have intersections between the different "bumbs". The barycenter method for unfuification will allow to take the different areas superposition into account.

Weights will modify the height of the bumbs so as to draw closer in the representation of a parabolofd.

Th~s 3D representation allows us to estimate the value of different weights (in particular at the ends and in the center) so that the "bumbs field" tends towards a paraboloid. The weights of bumbs center in ( l , l ) ,  (1,-l), (-l,l),  (-1,-1) will approximately be equal to 2 ( They '11 probably be slightly below on account of the barycenter method which will take the neighboring bumbs into consideration). In the same way, the weights of the "central bumb" (m=O) will be nearer to zero. We conipared these weights with different apprenticeship methods.

Comparison of different apprenticeship method At once, we tq to measure the error E between the NNs output and the wished one (see equation (IO)). We assign it a cost function J we'll try to minimize by an apprenticeshp algorithm.

E = wished-output - Nns-output (10) The target of this part is to determinate the best leanling  algorithm for our m e .  In fact, no method seems to prevail. We have then tested the different methods we know.

In a fust time, we have been interested by the lowers square method. The cost function is equal to the equation (I I).

J=(d-y)2=E ( E  : square Error; this equation IS equivalent to equation (I 0)) (11)  The minimization of this criterion corresponds to try to fmd the weights value which cancels out the cost derivative. We then come up against a inversion problem of the matrix contained the different second hidden layer output values.

Another prestige method used for the NNs apprenticeship is the Gradient retropropagation method. It minimizes the same function than Lower Square method (at about coefficient). The difference is situated in level of the weights modification : The Lower Square method tries to minimize the function for the weights set so that the Gradient retropropagation method works on the w-eights taken individually.

The Windrow Hoff method is another learning method of the same kind of the Gradient retropropagation one which more to minimize the error, takes the previous weights variation into account. That's why this method is commonly called method of the retropropagation of the gradient of the moment. The advantage using this method instead of the Gradient retropropagation method is because between two weights modifications, the inputs change.

In order to visualize their advantages and their draw backs , we have tested these two methods simultaneously.

Analysis of different apprenticeship methods results- After the network has been studied, we wonder about the real advantage of the baryccnter method during the unfuification. In fact, this method don't make a linear interpolation between the different "bumbs" but rather add a non-linearity into our processes. We have think that a simple weighted sum would accurately smoothed our "field to bumbs". We have then testing these two cases. Moreover, the quick convergence of OUT network induced us to question ourselves about the initialization of weights. In order to check the good working for each learning method, we've voluntarily decided to initialize the weights at a random value contained between 0 and 10 .  Th~s interval has judiciously been chosen to check the convergence known that the final values set of different weights is contained approximately between 0 and 2. The superior value of the interval has been chosen for not increase too much the duration of the apprenticeship.

An idea as regard size of results is the same for most of the parameters. So, we can't draw any conclusion regarding the number of iterations. We notice that however the method, weights values are practically identical. The error variation is because every the weights haven't yet reached their ended value. The barycenter method using Windrow Hoff always gives a middle error bellow 10- 6, It seems the best method of apprenticeshp. It would probably increase the base of recognition to rule on the recognition level.

Possible Im rovements Our filzzy-neural network gives good results : Ixvel :f not recognition reach about to Nevertheless, we tried to fmd feasible mutations to improve our system.

Trend to the NNs One advantage of NNs is a good immunity to failures thanks to the repartition of the knowledge. In our network, we lost th~s benefice. It happens then naturally to add connections between the different layers. But, the only feasible links adjunctions could intelligently be done between two hidden layers.

This mutation entails necessary the modifiable weights number increasing. The Retropropagation of Gradient method for the connections adjustment must be extended to the previous layer.

Then, the correspondent equation must be generalized. But this correction requires the utilization of the derivative of the function of the second hidden layer. Now, the function Maximum not being continually derivable on its studied set, it needs being smooth by an equivalent but derivable function. As h s  function is necessarily contained into the [O;1] set, a good approximation consists in using the sigmoid function (shifted fonvard right beforehand). The problem as our opinion, is that the add of links lnhibits in part the fuzzy reasoning , we tend then towards a nearly neural network. The association won't be there interesting except for the network construction.

Rather than adding connections which weigh down the network, we could modify the weights preestablished by the F u v y  Logic. We can : Modify the weights situated between the two hidden layers.

That would consist in giving more or less importance to some fuzzy areas. In fact, that entails "filing down" to different "ficld of bumbs" by the Max function which data won't belonging to 10.. 1 ] set. We can suppose that the second hidden layer will tend touards already to a approximation crude of the paraboloi'd or rather she'll carry out a final curve adjustment estimated by the output layer weights. A draw back then appears : That's the introduction of not- linearity into the network. In fact, this weights modification will be done individually on each "bumb" and it' will have then discontinuities in level of the bumbs intersection. *Modify the input layer weights. That would consist in adjusting the coefficients a and b of the tiaussian function of the fwzification. This correction allows then possibly another cutting out of the working interval in fuzzy areas, this whch will be in part equivalent touse a scale factor. I h s  improvement is the only one solution proposed which don't hit the fuzzy constituting of the network. Modify either the     input layer weights and the weights situated between the two hidden layers, which would well strengthen eighter the fuzzy side and the neural side of our network.

We could modify too the number of fuzzy areas of the first hidden layer. It will be interesting analysis obtained results. In fact, the number of "bumbs" (25) seems slight to describe all the paraboloid function by a well smooth curve (the field will always be a little granular). More the number of bumbs is; more w-e could act on the curve. In adding under fuzzy sets, we would then reduce the error. A comparative study of these diverse improvements and possibly of their association will be profitable.

Conclusion We've obtained very good results (middle Level of no-recognition about ; middle learning error of -IO6 for a number of iteration contained between 4 and 50) . The quick apprenticeship ( I5 minutes maximum) allows UT to test different methods. The results show the fuzzy-neural association is positive not only by the reached results but also by the 3D-representation of the network.

RECOGNlTION TO CHARACTERS  Introduction  The recognition of characters is the subject for many researches ~fl particular in level of handwritten characters. The association neural-fuzzy seems to bring about good results for the form recognition and could then be applied in case of characters.

Nevertheless this recognition is very powerful by connectionist methods although the results are less interesting with the Fwq Logic. In fact, the learning of classical methods of neural networks could sometime be long and the complexity of final network very big. It must be then study the interest of a so network for the recognition of characters. At first, these tests will be applied to a mono-fount, mono-size recogrution, to simpli@ the problem.

Yetwork's structure.

To determinate a network, i t  is necessary at once to do a study of that we want obtain (tj outputs of network), of that we have as inputs then on the stnicture of network.

network's outputs  It must be in less as so many outputs as wished classes (the different letters of the alphabet and added eventually the figures).

We shall add another class of rejection for the subjects which have not been recognized. A class could be active (output=l) or passive (output+). But one among the advantages of neural networks is that a class could be more or less active. Each outputs of network will be then contained between 0 and 1. It must be then another processing to decide the valid class when several are actives. In case of equality, we shall choose another class that we shall call confused class (the network does not know to take a decision). We can also complicate the method : IF several classes have nearby outputs (c= a term to make explicit may-be with the Fwzy Logic ...) to the maximal value, THEN to do another processing to differentiate the different classes. The processing whch was differentiating the confused classes could be made by the device of other neuro-fuzzy networks which would function in a parallel way with the principal network. Our system will be then composed of several cells in parallel, each of them would be of simple composition and would make a specific processing. Each of networks could then have in output a boundary number of classes containing several characters.

An interconnection between them in a decision's cell would allow then to select the opportune character. That would require less outputs for each networks, the cell of decision treating then the interconnection thanks to the Fuzzy Logic IF... AND... THEN _...

Network's input  In order to build the network, it is necessary to define the inputs exactly we shall enter the characters that we want recognize. That is certainly the heart of problem to the recognition because it is necessaq to define: The number of inputs which will constitute our network; The perception of used characters. The difficulty comes Gom the nearly freedom of choice. We must research the criterion which seems to be the best adapted in problem so that the system obtain a good recognition ratio. The alone obligation comes from subsequent processing of data by the fuzzy methodology: IF ........

AND ..... .... THEN .............. It must be then defined inputs from which we can calculate the degree of belonging to a set. In a first time, we have decide to not use a preprocessing of data and then to work directly with the pixels of picture. In order to use the previously done work, we have wanted use networks with two inputs. It comes then of course to mind that these two inputs could express the coordinates of a pixel of the letter; Each letter will be then described by a succession of "lighted" pixels. The work is then made "in series" in the aim to use only that two inputs. The rule of inference could be : IF coordx AND coordY THEN select ion the classes XYZ. Rut a problem comes then: How to memorize the information. It could easily be made by the device of a counter by class which counts the times when number of that the class is active. we shall choose then the one whose the counter will be maximal ( with a eventual processing for two classes having the same probability: Cf. previous paragraph).Two inconveniences appear here: The time of processing : it is necessary to read pixel after pixel all the whole picture image (even if we treat only the "lighted" pixels); The network has only that two inputs; But there are as many "ways" as pixels constituting each letters. That would render the network very complex in spite of its two inputs. We must then: treat the picture in a parallel way; ( 3 to decrease the number of ways), cut it in areas and not in pixels. ( * to decrease the time of processing) We can then calculate the coordinates of barycenter of pixels of the letter to recognize for each areas. Each area would have two inputs corresponding to the coordinates xl and x2 of barycenter. (The processing will then be made by pixel in each area but as the sub-networks will be parallel, the time of processing Mill be so decreased).

The problem is to determinate the number of necessary and sufficient cuts to differentiate the characters one from the others (eventuallv using fuzzy areas that is with intersections), to cut each areas in fwzy subsets. The pixels of the neighborhood of outline of the picture do not mostly correspond to the character to recopze, The cut-out of the picture must then be thmner around the center of the picture. To determinate the number of divisions, it is firstly necessary to analyze the set of characters (= classes) to recognize.

Another method that the we could use for the recognition of characters would consist to use a preprocessing which build an histogram of the picture. Each class will be then the recognition of a curve representing the hstogram. We should not use then the Fuzzy Logic for the preprocessing but we recognize very heavily the recognition of curves done in the previous chapter. It must then as many networks as characters to recognize. We could also use these two methods in parallel. We have then in input : The barycenter of each area (= medium position); The number of lighted pixels in each area (= densit)). The set of these characteristics must allow to distinguish in a fine way all of the classes.

General structure  The association of inputs and outputs gives us the nexT schemes  A preprocessing by hstogranx A fwzy preprocessing.

when we have:     This second method seem more complex but it 'is not obvious that the number of networks could be more considerable than with the first method. It does a processing by fragment of each character then that the first method studies the picture in its set. We could then envisage the processing of characters of different sizes (the position of barycenter could be relative), and of different fonts (if 9 fragments between 10 recognize 1 character, even if the tenth select ion the rejection class then the character will be identified. It seems then that the second method would be more attractive. We must however make tests to verify these results and to test the neural- fuzzy association in the recognition of characters.

Conclusion  One of advantages of the neural-fuzz).. association for the form recogrution will be the decomposition in sub-classes. In fact, the fuzq rule JF .. AND , .  THEN . . .  allows to determinate different classes thanks of characteristics: IF caracteristicl AND caracteristic2 THEN Class X. We can then envisage to elaborate the neuro-fuzq network in a rest of classes of more and more specialized. ' I lus  method have the advantages to obtain a series of simples networks, to know approximately the localization of the knowledge ... Even if we smooth and add connections in each of sub-networks.

CONCLUSION  The two advantages of neuro-fuzzy network were: The simplification of the learning which is progressively established t layer after layer; The approximate localization of the knowledge. As it appears that the best method of learning is a method whch adapt itself perfectly to the connectionist networks (without Fuzzy Logic).

The advantage in this field does not seem evident so many more, the learning in the neural networks is without big problems. In an other part, the knowledge is "deflected" at the time of adding new others connections. We don't know more then where she is situated. Two obvious advantages would be: The decomposition in sub-classes (for the recogrution of character); The not-complexity of network ( a more memory, bigger quickness,. . .) Tlus study could bring about an interest in front of the use of a such network to the recognition of handwitten characters.

BIBLIOGRXPHY  Mr GLORENNEC. Les reseaux nacro-jlous holutlfs : un pont entre leflou et le neuronal, GR CSN,CNRS 92 France  Mr 1,AMOTTE. Lejlou : quelques de$nitions et applications, Logique floue et reseaux de neurones; ecole d'hiver de St Sorlin d'Arves.1993, Apprentissage et reseaux de neurones Pour la Science n"l81 (Nov. 92)  Melle LEGENTIRE Reconstitution de mesurandes a l'aide d'un reseuu de neurones niulticouche, appliquee a un systeme de mesure depression afibres optiques, Rapport de DESS AU de (Sept. 92)  Melle PAKKF,R, Reconnaissance des caracteres alphanumeriques par une analyse structurelle hdterarchique, These de (Mars 85)  Mme BELLON & Mr BOSC & Mr P W E  , Le boum drrjlou au Jupon , GR CSN,CNRS 92 France  Mr D17BOIS , Typologie et utilisation de regles jloues , GR CSN,CNRS 92 France h4r GLORENNEC & Mr BARET & Mr BRUNET , Application  of neuro,fuzzi networks to identification and control of nonlinear dvnamical xvstems. La lettre du club logique floue n"2 (Fev. 92)  Fuzzy rules for  guiding reinforcmrenf leaming, La lettre du club logique floue n"2 (Fev.

