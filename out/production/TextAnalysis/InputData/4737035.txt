A-close+: An Algorithm for Mining Frequent Closed Itemsets

Abstract   Association Rule Mining (ARM) is the most essential technique for data mining that mines hidden associations between data in large databases. The most important function of ARM is to find frequent itemsets. Frequent closed itemsets (FCI) is an important condense representation method for frequent itemsets, and because of its importance in recent years, there have been many algorithms implemented for it.

One of the most fundamental algorithms for frequent closed itemset is A-close. In this paper, we optimize this algorithm using both optimized techniques "reducing pruning time" and "reducing database size", called "A-close+".. Results show that the performance cost of our algorithm is considerably less than A-close.

1. Introduction   Association rule mining (ARM) is one of the most important data mining techniques. ARM aims at extraction, hidden relation, and interesting associations between the existing items in a transactional database.

It is highly useful in market basket analysis for stores and business centers. For example, database mining of a department store customers reveal that those who buy milk would buy butter in 60% of occasions, and such principle is observed in 80% of transactions. In this example, the above-mentioned probability is called confidence percentage, and a percentage of transactions which cover this rule is termed support percentage. To find the rules, user should set a minimum amount for support and confidence which are called minimum support (min-sup) and minimum confidence (min-conf) respectively [1].

The main step in association rule mining is the mining frequent itemsets. In effect, with frequent itemsets in hand, generating association rules would be highly straightforward. Frequent itemsets mining often generates a very large number of frequent itemsets and rules. As such, it reduces the efficiency and power of mining. To overcome the problem, in recent years  condensed representation has been used for frequent itemsets [3, 7]. A popular condensed representation method is using to frequent closed itemsets. Compared with frequent itemsets, the frequent closed itemsets is a much more limited set but with similar power. In addition, it decreases redundant rules and increases mining efficiency. Many algorithms have been presented for mining frequent closed itemsets, and A-close proved to be a fundamental one [6]. In this article, we have used two optimal techniques, i.e.

"reducing pruning time" and "reducing database size", to arrive at our developed algorithm A-close+.

The paper is structured as follows; Section 2 introduces frequent closed itemsets and related concepts. A-close algorithm and A-close+ are presented in Sections 3 and 4 respectively. Section 5 briefs on research results and conclusions are given in section 6.

2. Problem development   Let D be a transactional database. Each transactional database includes a set of transactions.

Each transaction t is represented by <TID, x> in which x is a set of items and TID is the unique identifier of transaction. Further, let us consider I = {i1, i2, ?, in} as the complete set of distinct items in D. Each non- empty subset y of I is termed an itemset, and if includes k items, it would be called k-itemset. The number of transactions existing in D including itemset y, is called the support of itemset y, denoted as sup(y) and it is usually represented in percentage. Given a minimum support, min-sup, an itemset y is frequent itemset, if sup(y) ? min-sup.

Definition1- Closed Itemset: An itemset y is a closed itemset if there is not any superset of y like y' that sup(y) = sup(y').

The precise definition of closed itemset, however, is based on Relations (1) and (2). Let us consider T, y and then respectively IyDT ?? ,  are subset of all   DOI 10.1109/ICACTE.2008.135    DOI 10.1109/ICACTE.2008.135       items and transactions appeared in D. Then functions f and g are defined based on Relations (1) and (2).

},|{)( tiTtIiTf ????=  (1)  },|{)( tiyiDtyg ????=  (2)   Considering these two functions, an itemset y is a  closed itemset, if and only if Relation (3) is held.

h(y )= f(g(y)) = fog(y )= y  (3)   The combinational function h = fog is called  closure operator. If a closed itemset is frequent, it is called frequent closed itemset.

Definition 2- Generator: An itemset p is a generator of a closed itemset y if p is one of the smallest itemsets (there may be more than one) that determines y using Galois closure operator: h(p) = y.

3. A-close algorithm   A-close algorithm is based on Apriori algorithm [1] involved in frequent itemsets mining. As it is intended to improve A-close to arrive at A-close+, first the former is introduced. A-close is run in two steps: (1) producing frequent generators, and (2) determining the closure of frequent generators.

To produce generators, a level-wise approach, similar to that of Apriori algorithm is taken. Generators with size i are called i-generators. In each pass (i+1) of the algorithm, candidate (i+1)-generators are produced with the help of frequent i-generators produced in the previous pass. To set of generators with the size of i are labeled Gi. Then three steps of pruning are conducted on candidate generators, and useless generators are pruned thereby. The operation of generator production is repeated until no other generator is produced. The detail of pruning is as follows:  In the first step of pruning, Gi+1 is pruned by removing every candidate (i+1)-generator c such that some i-subset of c is not in Gi . As such, all supersets of infrequent generators and also all generators that have similar support with one of their own subsets are pruned. In the second step, the support of the remaining candidate generators in Gi+1 are determined, and those with support less than minimum support are deleted from Gi+1 . In the third step, a test is done for each candidate generator c in Gi+1 to see whether it has a similar support with one of its i-subsets like s. If it is so, c will be removed from Gi+1.

It should be pointed out that in each iteration that candidate generators are formed, one pass is made on  database for computing the support of candidate generators. After producing generators G1 to Gn (n is maximum generator size), closure of all these frequent generators should be computed. The closure of all frequent generators results in all closed frequent itemsets. The method for computing closure is based on proposition 1.

Proposition1. The closure of generator p which is achieved by applying function h from Relation (3) on p is intersection of all database transactions that include p.

4. Our suggested A-close+ algorithm   As described below, A-close+ algorithm is realized  by applying two optimal techniques on A-close algorithm. It should be noted that the techniques are applied on the generators? production step of A-close algorithm.

4.1 First optimal technique in A-close+  Taking A-close generators' production step into  account, it could be seen that in both first and third steps of pruning a similar operation is repeated; In both in each pass i, all (i-1)-subsets of candidate generators with size i should be analyzed. In the first step of pruning it is analyzed that all subsets with size i-1 of a candidate generator with size i, are frequent generator of the previous pass that is they exist in Gi-1. In the third step it is analyzed that the support of candidate generator with size i is not equal to the support of any of its own (i-1)-subsets in Gi-1. The operation of finding (i-1)-subsets for generators of each pass i in two turns is time-consuming. In pass n, that is when frequent n-generators are produced, for analyzing all (n-1)-subsets of each candidate generator with size n, n analysis with the size of n-1 is required.. Therefore, the repetition of "the analysis of all subsets" should be avoided, and it must reduced to one step. But how?

The third step of pruning should be accomplished with first step, or the first step with the third one?

It is obvious that conducting the third step of pruning before the second step and merging it with the first step is impossible, as for the third step, the support of candidate generators should be determined, and this job is done in the second step. Likewise, conducting the first step with the third step, i.e. after second step of pruning is not optimal; because in the first pruning step a great number of candidate generators were removed and there was not any need for computing their support in the second step. Now, however, conducting the second pruning step initially, and going ahead with the first and third steps we are faced with a non-optimal situation in which support computation for all candidate itemsets is required. Then merging the first       and third steps is far from easy. To remove the problem Proposition 2 is presented:  Proposition2. If D be a transaction database over a set of items I, so that Iyx ?,  are two itemsets, then,  )(sup)(sup xportyportyx ???  Hence, in pass i, the support of generator with size i is always smaller or equal to that of all its subsets with size i-1. And therefore, the generator support c in Gi might be equal to that of one of its own subsets in Gi?1 such as s, only if s has minimum support between subsets of c. Then in the third step of pruning, instead of analyzing support of c with the support of all its subsets in Gi-1, it will be compared with just "minimum support of its subsets in Gi-1".

Pruning in A-close+ is done as follows: In the first step of pruning, for each candidate  generator c in Gi an analysis is done so that if all subsets with size i-1 are in Gi-1, the minimum support related to subsets is stored as a variable called min- supp-sub in node c and if it is not the case, i.e. if at least one of i-1 subsets of c is not available in Gi-1, c will be removed from Gi.

Therefore, subsets analysis in the third step is skipped, and instead of costly operation of finding all subsets, the support of candidate generator is compared only with the minimum number stored in the first step of pruning. The temporal complexity of this simple comparison is O(1) that is done in the second step of pruning. In other words, in the second step of pruning if one of two conditions is valid, i.e. if candidate generator support is smaller than minimum support, or it is equal to the min-supp-sub, the generator is removed.

Thus three steps of pruning are reduced to two steps in A-close+, leading to less time and more efficiency.

4.2 Second optimal technique in A-close+   A drawback in A-close algorithm and generally all  level-wise traverse algorithms is that the number of reading of database is high, and for example in A-close algorithm in each iteration that candidate generators are produced, a pass on database for computing candidate generators support is necessary. Therefore, reducing database size is very important in decreasing execution time of algorithm.

The second technique used in A-close+ deals with reducing database size and is related to frequent generators properties that states "all subsets of a frequent generator should be frequent" (first step of pruning in A-close) [6]. This technique in some cases is able to remove a whole transaction, and sometimes it can remove some transaction items. Following this  principle, if for example 3}{ GABC ? , then 2},,{ GBCACAB ? . These three members, indeed, are all double subsets of G3. the number of k-subsets of a (k+1)-itemset, is k+1. Then the following conclusion is expected:  "A transaction is used to determine frequent (k+1)- generators only when it consists at least (k+1) of the frequent k-generators in the previous pass in Gk.".

Needless to say, this condition tends to be necessary, but not sufficient. With such condition, in fact, in any pass items of transaction that are useless for producing next frequent generators are pruned. To be more precise, let us say; if a transaction contains some frequent (k+1)-generators, any item contained in these (k+1)-generators should appear in at least k of the candidate k-generator in Gk, then,  "An item in transaction t in pass k can be removed, if it does not appear in at least k of the candidate k- generator of Gk".

Therefore, in any pass within the array it is necessary to keep occurrence frequency of transaction items in the candidate generators of the same pass. The frequency array is used for this, and frequency[i] keeps the occurrence frequency of the i-th item of transaction t in any pass' candidates. For example, if Table 1 represents a part of a database, and in the second pass (k = 2), there is G2 = {AB, AC, BC}, in the case of transaction 1, since frequency[0] = 1 and frequency[1] = 1, then both items are removed of transaction. In transaction 2, frequency[0] = 2, frequency[1] = 2, frequency[2] = 2, and frequency[3] = 0. Hence, items A, B and C are kept in transaction and E is removed.

Table 1. A part of a database  items TID  BC ABCE     Therefore, the second optimal technique in A-close+ tries, in different ways, to reduce database size and execution time of algorithm. Furthermore, with the reduction of database stored in the main memory, the memory usage of the algorithm gradually decreases.

5. Performance evaluation   In this section, A-close+ algorithm is compared with two algorithms, i.e. A-close and Apriori.

To test programs a computer with the following properties is used: it has a Pentium processor 3GHz, and 1GB RAM working with operating system       Windows XP. All programs are implemented and compiled with C++.

5.1 Performance results  5.1.1 Performance time. Figures 1, 2 and 3 provide a comparison of A-close+ and A-close as far as performance time in four datasets is concerned. As it is shown, A-close+ is temporally more efficient than A-close.

60% 65% 70% 75% 80%  minimum support  R un  tim e (  se co  nd )  A-close+ A-close   Figure 1. Performance time comparison in  pumsb dataset         0.6% 0.7% 0.8% 0.9% 1.0%  mininmum support  R un  tim e (  se co  nd ) A-close+  A-close   Figure 2. Performance time comparison in  T40I10D100k dataset      0.050% 0.055% 0.060% 0.065% 0.070%  minimum support  R un  tim e (  se co  nd )  A-close+ A-close   Figure 3. Performance time comparison in  kosarak dataset    5.1.2. Memory Usage. In A-close+ algorithm, due to the gradual reduction of database size the memory usage decreases gradually. In A-close, however, the occupied memory remains occupied until  the end of operation. The reduction of memory usage can be computed through analyzing the decrease of database items stored in the memory. Figure 4 shows the gradual reduction of memory usage by A-close+ in T40I10D100k dataset for minimum support = .6%. As the dataset items stored in the main memory are encoded as integer, each item occupies two bytes of the memory.

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  pass number  m em  or y  (M B  )  T40I10D100k    Figure 4. Reduction of memory usage in T40I10D100k dataset   6. Conclusion   The most important function of association rules  mining is producing frequent itemsets, and due to great deal of work, high cost and low efficiency, in recent years the closed frequent itemsets is employed to show these itemsets. The closed frequent itemsets have the same power as the whole of frequent itemsets, in additional they increase the mining efficiency through removing redundant rules. A fundamental algorithm in frequent closed itemsets mining is A-close based on which, in this research, and using two optimal techniques A-close+ algorithm is developed. The first optimal technique in A-close+ decreases pruning time from three steps to two ones, and the second optimal technique gradually reduces the database size stored in main memory. The test results indicate that A-close+ has better performance in execution time, and its memory usage decrease gradually too. Furthermore, unlike A-close that outperforms Apriori only in dense datasets, A-close+ outperforms Apriori in all types of datasets.

7. References [1] R. Agrawal and R. Srikant, "Fast algorithms for mining  association rules", Proc. 20th int'1 conf. very large data bases, 1994.

[2] F. Bodon, "A trie-based Apriori implementation for mining frequent item sequences", Proceedings of the 1st international workshop on open source Data mining(OSDM'05), 2005.

[3] C. Lucchese, S. Orlando, and R. Perego, "Fast and memory efficient mining of frequent closed itemsets",       IEEE Transaction On Knowledge And Data Engineering, Vol. 18, No. 1, PP. 21-35, 2006.

[4] P. Palmerini, S. Orland, and R. Perego, "Statistical Properties of Transactional Databases", ACM, 2004.

[5] J.S. Park, M-s. Chen, and P.S. Yu, "An effective hash based algorithm for mining association rules", ACM Data, Vol. 24, PP. 175-186, 1995.

[6] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, " Discovering frequent closed itemsets for association rules", Proc. Int'l conf. Database Theory, PP. 398-416, 1999.

[7] J. Wang, J. Han, and J. Pei, "CLOSET+: Searching for the best strategies for mining frequent closed itemsets", proc. Int'l Conf. Knowledge Discovery and Data Mining, PP. 236-245, 2003.

[8] http://fimi.cs.helsinki.fi, 2003.

[9] http://www.cs.bme.hu/~bodon, 2005.

