Unsupervised Partitioning of Numerical Attributes Using Fuzzy Sets

Abstract?The current paper presents an enhanced partition- ing mechanism for numerical data. The efficiency of our method will be illustrated through a solid set of tests that have been performed. We have planned this partitioning phase as an initial step in a more complex algorithm to be further studied and implemented.

The final goal is to use it for future decision making in automatic image annotation. Fuzzy Sets theory has been used as a base for our clustering algorithm and partitioning. We included this mechanism as a component of a framework we developed for image processing, more exactly for the image segmentation evaluation model we are building.



I. INTRODUCTION  O NE OF the most important problems in data mining is  taking decisions based on association rules. In order to  achieve this, an efficient partitioning mechanism needs to be  used.

This choice can have consistent influence over the final  result and we considered this a crucial approach for our further  study.

The core topic considered by our paper, partitioning quan-  titative attributes, has been initially studied by [2]. The author  proposed in [2] an initial partitioning into small intervals and  combine adjacent intervals into bigger ones so that the domain  support to be more relevant, leading to boolean logic when  replacements are done for the initial attribute with attribute-  interval.

It seems that current associations algorithms introduce fur-  ther more other problems. The might ignore the elements near  the boundary or use not very intuitive for human perception  approaches like shape boundary interval.

We proposed through our work an efficient mechanism of  partitioning using fuzzy sets logic. We have worked to improve  the clustering algorithm starting from the basic Fuzzy C-  Means algorithm and applied on top of that auxiliary logic  for determining the best partitioning scheme.



II. RELATED WORK  Mining boolean association rules over larger knowledge  bases was early mentioned in [1], and later studied and  presented in [3], for the case of databases with only categorical  attributes.

Practically, the information in databases is not limited to  categorical attributes, but also contains much quantitative data.

As mentioned before, mining quantitative association rules  was introduced and an algorithm proposed in [2]. The al-  gorithm studies the discretization of quantitative attributes  domains into intervals in order to reduce the domain for a  more categorical one.

The analysis of clusters is based on partitioning a set of  numerical data into a number of subgroups, based on the fact  that the objects within that group have certain similarities.

This approach doesn?t represent all the times the real data,  where boundaries between subgroups might be fuzzy and more  detailed description of the objects inside clusters are required.

That is why many similar problems have been partially or  totally solved in fuzzy environments. Several related papers  present this approach: [4], [5], [6]. There are three major  difficulties encountered during fuzzy clustering of real data:  1) the number of clusters cannot be defined apriori and  optimal number has to be determined,  2) location and clusters centroids type cannot be known  before and initial guess has to be made and  3) there is a great variance of cluster items that needs to  be handled.

Since we are dealing with real data from a public dataset  we are using, we are trying, through our work, to handle the  three concerns presented above in an efficient manner.

What we are using for this is Fuzzy C-Means as clustering  algorithm and related logic for determination of the optimal  clusters set.



III. NUMERICAL ATTRIBUTES CLUSTERING USING FUZZY  C-MEANS  A. Fuzzy Sets  Fuzzy sets theory has been initiated by an observation made  in 1965 by Zadeh, saying that ?more often than not, the classes  of objects encountered in the real physical world do not have  precisely defined criteria of membership?.

Proceedings of the Federated Conference on  Computer Science and Information Systems pp. 751?754  ISBN 978-83-60810-48-4     The particularity of fuzzy sets is to capture the idea of partial  membership. The characteristic function of a fuzzy set, often  called membership function, is a function whose range is an  ordered membership set containing more that two (often a  continuum of) values (typically, the unit interval). Therefore,  a fuzzy set is often understood as a function.

The fuzzy sets and their corresponding membership func-  tions provided by the experts may not be suitable for decision  association rules in a database. The quality of the results relies  on the appropriateness of the fuzzy sets to the given data.

Some attributes have discrete nominal domain, and others  have continuous numeric domain. In our study, we assume that  discrete nominal domain attributes are characterized by crisp  values and continuous numeric domain attributes are charac-  terized by crisp values, interval values and fuzzy numbers.

In the case of training data, each data has the class in-  formation along with its confidence degree. In description of  fuzzy values for continuous numeric attributes, trapezoidal  fuzzy numbers are widely used since they can sufficiently  well represent fuzzy values and they are simple to describe  and process.[7]  Trapezoidal fuzzy numbers Trap(?, ?, ?, ?) are defined as follows:  Trap(?, ?, ?, ?) =  ?  ?  ?  ?  ?  ?  0, ifx < ? (x? ?)/(? ? ?), if? ? x ? ? 1, if? ? x ? ? (x? ?)/(? ? ?), if? ? x ? ? 0, ifx > ?  (1)  Fig. 1. Trapezoidal fuzzy numbers representation  B. Fuzzy C-Means Algorithm  Fuzzy C-Means (FCM) is a method of clustering that allows  one piece of data to belong to one or more clusters. This  algorithm has been developed by Dunn in 1973 and improved  by Bezdek in 1981. It is based on the following objective  function:  Jm =  N ?  i=1  C ?  j=1  umij?xi ? cj? 2, 1 ? m ? ? (2)  Fuzzy partitioning is implemented through an iterative opti-  mization of the objective function.

The algorithm is composed of the following steps:  1) Initialize U = [uij ] matrix, U (0)  2) At k-step: calculate the centers vectors C(k) = [cj ] with U (k)  cj =  ?N  i=1 u m ij ? xi  ?N  i=1 u m ij  (3)  3) Update U (k), U (k+1)  cij =  ?C  k=1( ?xi?cj? ?xi?ck?  )  m?1  (4)  4) If  ?U (k+1) ? U (k)? < ? (5)  then STOP; otherwise return to step 2.

In order to obtain good results, the initial centroids of  the clusters have been generated randomly as and iteratively  improved through the cycles of the algorithm. Also, as an  improvement, empty clusters generated by the algorithm have  been removed. This was a very useful step for further calcu-  lations.



IV. FUZZY SETS PARTITIONING USING CLUSTER OPTIMAL  INDEX  A very common problem in clustering is finding the optimal  set of clusters that best describe the data set. Many clustering  algorithms generate a required set of clusters passed as input.

In order to solve this problem, the solution would be to  repetitively run the algorithm with a different set of inputs  until the best schema is found. In order to validate that, an  auxiliary measure needs to be taken care of. We called this  cluster optimal index.

A number of cluster validity indices are described in the  literature. A cluster validity index for crisp (non fuzzy) clus-  tering is proposed in [13]. An alternative has been proposed  in [12]. The implementation of most of these measures is  very expensive computationally, especially when the number  of clusters and the number of objects in the data set grow  very large. For a given attribute X, the following measures  have been taken into consideration:  Variance of attribute X  ?2(X) =  n  N ?  k=1  (xk ? x) 2 (6)  Variance of cluster i  ?2(Xi, ri) =  ?ni k=1(xik ? ri)   ni (7)  The average separation for c clusters  Scat(X,R) = c  ?c  i=1 ? 2(Xi, ri)  ?2(X) (8)  Scat(X,R) indicates the average compactness of clusters.

A small value for this term indicates compact clusters as the  scattering within clusters increases (they become less compact)  the value of Scat(X,R) also increases.

Total separation between clusters  Dis(R) = Dmax Dmin  c ?  i=1  (  ?c  j=1 |ri ? rj | ) (9)  The term ?total separation? sounds like a measure we want  to maximize. In this case the opposite holds: a smaller value  is better. Dis(R) indicates the total separation between the  752 PROCEEDINGS OF THE FEDCSIS. WROC?AW, 2012    c clusters and generally, this term will increase within the  number of clusters.

Cluster optimal index has been calculated as follows:  OptIndex(X,R) = ? ? Scat(X,R) +Dis(R) (10)  considering ? a factor equal to Dis(cmax) for the maximum number of input clusters.

The generalized membership function for the clusters is  given by the following formula:  f(r1, x) =  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  ?  0, ifx ? d+i?1 d +  i?1 ?x  d +  i?1 ?d?  i  , ifd+i?1 < x < d ? i  1, ifd?i ? x ? d + i  d ?  i+1 ?x  d ?  i+1 ?d+  i  , ifd+i < x < d ? i+1  0, ifx ? d?i+1  (11)  The steps of finding fuzzy sets can be shortly summarized  as follows:  1) Finding the best clustering scheme using optimal cluster  index  2) finding fuzzy sets with c clusters centers and  3) calculating the corresponding membership functions.

In details, the algorithm can be described as:  Main (FCM, X, minc, maxc, p)  ? Phase I: calculate the optimal number of clusters and its  centroids  ? Initialize: c = maxc ? Repeat  ? Run FCM (clustering algorithm) for data set X to produce c cluster centers  ? Calculate optimal cluster index OptIndex(X,R) ? if(c=maxc then  ? = Dis(maxc) BestOptIndex = OptIndex(c) bestc = c  ? endif ? else if (OptIndex(c) < BestOptIndex) then  bestc = c BestOptIndex = OptIndex(c)  ? endif ? c = c-1  ? until c = minc ? 1  ? Phase II: calculate fuzzy sets with the c cluster centers  ? for i = 1 to bestc do  ? if i < bestc then calculate d + i using overlap  percentage p  ? if i ? 2 then calculate d?i using overlap percent- age p  ? endfor  ? Phase III: calculate membership function for each fuzzy  set  ? for each x ? X do  ? foreach ri ? R do  ? calculate membership function f ? endfor  ? endfor

V. IMPLEMENTATION METHOD  The development of the application that represents the  basis for our framework was done in C#.NET using .NET  Framework 4.0 and the major advantages that it offers.

The decision was influenced by some of the many reasons  people are using MS based technologies for their work,  especially .NET Framework:  ? Consistent Programming Model  ? Direct Support for Security  ? Simplified Development Efforts  ? Easy Application Deployment and Maintenance  We?ve developed several auxiliary tools that helped us in  evaluation and measurements.



VI. EXPERIMENTAL RESULTS  A. Testing Data Set  In order to test the mechanism we build, we decided to  use a public dataset providing real world data. The one we  selected is called ImageCLEF - Image Retrieval in CLEF[8].

It contains segmented and annotated images for evaluation of  automatic image annotation and for studying their impact on  multimedia information retrieval.

All the images have been manually processed, segmented  and annotated based on a predefined vocabulary of labels.

Visual features have also been extracted from each region.

We have chosen this data source because it consists a well  reference for our further development. Basically, we would  have consistent ground truth images for further analysis and  evaluation.

The collection contains the following sets of data:  ? Segmentation masks: one per region: 99.535 files; one  per image: 20.000 files.

? Annotations: one per region: 99.535 regions were manu-  ally annotated.

? Spatial relationships: one per image: 20.000 files.

? Visual features: a vector of features per region: 99.535  vectors of attributes.

In 2 is displayed an example of segmented and annotated  image:  B. Performance Results  We have measured the performance of the mechanism we  built on the dataset that we considered for testing. Initially we  calculated how is the execution time increasing if the number  of attribute values is increasing but for the same number of  clusters. We obtained the graphic in 3  Another set of tests have been performed on overall par-  titioning mechanism. We increased the elements count from  10 to 20000 and observed that for a bigger number of values,  the execution time is not increasing rapidly, which makes us  BOGDAN POPESCU, ANDREEA POPESCU ET AL.: UNSUPERVISED PARTITIONING OF NUMERICAL ATTRIBUTES 753    Fig. 4. Partitioning different value sets  Fig. 5. Optimal number of clusters  Fig. 2. Segmented and annotated image from testing dataset  Fig. 3. Clustering execution time for 12 clusters  believe that we would have good performance on processing  a large number o images. 4 The optimal number of partitions had a constant behavior,  mainly depending on the values we processed. since their  variance was not so big, the number of cluster didn?t vary  so much, which was expected.5

VII. CONCLUSION  As a first step from a more complex project we are devel-  oping, the resulting metrics were very satisfying for us and  make us believe that we would have further good results. The  dataset is very comprehensive and offers a lot of useful data  for our testing.

This algorithm is quite powerful since the the merging cost  evaluations requires simple identifications of complex models  which is easy to implement and computationally cheap to  calculate.

We propose a mechanism to find the optimal partitions as  fuzzy sets based on clustering techniques. From experiments  we found that the method produces meaningful results and has  reasonable efficiency.

