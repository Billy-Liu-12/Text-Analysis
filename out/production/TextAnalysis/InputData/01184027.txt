Exploring Interestingness Through Clustering: A Framework

Abstract  Determining interestingness is a notoriously difficult problem: it is subjective and elusive to capture. It  is also becoming an increasingly more importanrpmblem in KDD as the number of mined patterns increases. In this work we introduce and investigate a framework f o r  association rule clustering that enables automating much of the laborious manual effort normally involved in the exploration and un- derstanding of interestingness. Clustering is ideally suited for  this task: it is the unsupervised organization ofpatterns into groups, so that patterns in the same gmup are more similar to each other than to patterns in other groups. We also define a data-driven inferred labeling of these clusters, the ancestor coverage, which provides an intuitive, concise representation of the clusters.

1 Introduction  Determining which patterns are interesting is an impor- tant, but notoriously difficult problem [9, 13, 31. Many ap- proaches, requiring varying degrees of user intervention, have been formulated to tackle the problem, see 1161 for a review. In this work we were motivated by the approach of [I51 that identifies similar groups of rules to develop a first step in replacing a significant amount of the tedious, manual effort commonly involved when investigating in- terestingness. As [I31 emphasize, the difficulty in analyz- ing the mined rules is in how to organize them effectively.

We tackle this challenge by introducing a clustering frame- work that automates interestingness exploration of masses of rules by organizing them into groups by similarity. This can be seen as an example of higher order mining [17].

Clustering is the unsupervised organization of similar objects into meaningful groups, or  clusters [7, 10, 8.41, so that objects within a cluster are more similar to each other than to objects in other clusters. Since clustering does not require the pre-labeling of patterns it is ideally suited for ex- ploring interestingness: the mining process outputs a list of unlabeled (as interestingnot-interesting) association rules.

In KDD clustering is frequently used to gain insight into the distribution of the data [7, 1 I].  In the context of associ- ation rules, clustering was previously used to partition inter- vals over numeric data domains, as input for mining algo- rithms as in [141. In [I21 association rules with two numeric attributes in the assumption were clustered by joining a.- sumption attribute intervals. [61 proposed mining over sets of clustered data. In our work, we use clustering differently and for a different purpose. [IS] were the first to use group- ings to improve the understandability of rule covers with a common consequent over data domains constrained by a rule-confidence monotonicity assumption. In this work we introduce a general framework for clustering unconstrained association rules over unconstrained domains to enable ex- ploring interestingness. To make the clusters easy to inter- pret we define a concise, data-driven cluster representation, the ancestor coverage, defined in Section 4. Additionally, we investigate the drawbacks and advantages of existing and new similarity measures, and the results of their appli- cation on real databases. [IS] defined a similarity measure as the number of transactions two rules with equal conse- quents differ on. [3J proposed a distance metric to detect unexpected rules in a defined neighborhood, not for clus- tering. [5 ]  proposed a measure to differentiate between at- tributes with overlappinghon-overlapping numeric values.

2 Definitions and Preliminaries  Let A be a set of attributes over the boolean domain and D he a set of transactions over A. For A, B E A, A n B = 0, [ I ]  define the association rule A --t B to have support s% and confidence c% if s% of D contain AUB, and e% of D that contain A also contain B. Given sup- port and confidence thresholds, Ill?s algorithm outputs all the association rules that have at least those thresholds. We refer to A as the assumption of the rule and to B as its consequent. We also use @ to denote the xor operation: X @ Y = (X \ Y )  U (Y \ X). Let 0 he the list of associa- tion rules mined over A. Let a, b E A, and p = a --t b. [I51 defines the family of p in R as: familyn(p) Dsf { T  E Rlr = A + B: a E A ,  b E B }  U { p } ,  and the ancestor rule of the  0-7695-1754-4/02 $17.00 0 2002 IEEE 677    family is defined to he p. An indicator of the size of a fam- ily is [I51 R S C h ( p )  Dzf /jfamilyn(p) \ {p}ll/llRII.where JJXJJ is the number of elements in the set X. We introduce the Relative Size of the Family of p over a Set of associ- ation rules C, which we define as RSFSc(p) Dzf ] I { T  E C l r  = A + B , Q  E A; b E B } ~ ~ / ~ ~ C ~ ~ .

3 Interestingness Exploration Via Clustering  We introduce a clustering framework for grouping and naming of association rules. an effective automated tool for quickly exploring their interestingness. The clustering pro- cess typically includes the following steps [IO,  81 ( I )  pro- filing, (2) similarity measure definition. (3) clustering algo- rithm, and (4) representing and evaluating the clustering.

3.1 Association Rule Profiles  A cluster of association rules is a set of similar rules.

Before we can define the ?similarity? of rules, we need to define what characterizes the rules, or their profiles. There are no theoretical guidelines to determine the features best suited for use in a similarity measure, how those features should be normalized or which is the most appropriate sim- ilarity measure to be used [lo]. A skillful selection of the feature vector is necessary for simple and easily understand- able clustering that will provide insight into the interesting- ness of the rules: a poor selection can yield to complex clus- tering that provides little or no insight. We choose to con- centrate on the five characteristics that fully define an asso- ciation rule r = A + B (1) A,  (2) B,  (3) the rule support, (4) the assumption support: the percent of V that contain A , and, ( 5 )  E?s suppon: the percent of ?D that contain B .

3.2 Similarity Measures for Association Rules  We follow the reasoning in [4] and abandon the adher- ence to a distance metric and use a nonmetric similarity function?. We review two representative measures available in  the literature in  the context of our expectation of a good similarity measure: the dissimilarity between rules in the same cluster will be significantly less than that of rules in different clusters. Since neither of those measures satisfies this expectation, we introduce a new measure that does.

3.2.1 Support-Difference Measure  [IS] defined a similarity measure between rules with an equal consequent as: d(.4 -+ B > C  -t B )  = [support(AB) + suppor t (BC)  - 2 . su~~o~(.4BC)l~IIVll.

?In some ncenaios Ihe rimiluily of I and P? may be considered differen1 fmm Lhal ofr? to 7. necessitating non-rymmetic rimiluiLy mearurei. Howeuer. clusfeting i s  n ~ m l l y  p r f o m e d  using $ y m e t r i c  similarity measures, which we will adhere LO.

which is naturally extended to unconstrained rules as: &(A --t B, C + D) = support(AB) + supporr(CD) - 2 . supporr(ABCD),  but does not differentiate between attributes in the assumption and the consequent. For exam- p l e , l e t r 1 , = t - + c , r l  = t , a + c , r 2  = t , c - + a .  Auser classifying the family of rtc as not-interesting indicates that r1 is not-interesting. However. this user could be interested in T 2 .  The two rules r1 and TZ are therefore not similar to this user, and yet dsd(rlrr2) = 0. This does not match our expectation of a good similarity measure: d,d can rank rules that are less similar (T I  and r2) as closer than rules that are more similar, (dsd(rle,  vl) > 0). Note that d s d  does not use all the profile features defined in Section 3.1.

3.2.2 Attribute Distance Measure  [3] introduced dSeet(A + B,C + D )  = 61 . Il(.4 U B )  63 ( C U  D) \ \  + 62. IIA 63 GI1 + 6 3 .  IIB cl3 Dll, and recommended:  = 1.62 = (n - l)/n2, and, 63 = l/d, where n = (JAlJ.

For the Grocery DB (Section 4), where n = 1757, diSet im- poses an extremely strong similarity bias for rules over the same attribute sets. This property is nor favored by all users.

To avoid this problem we also used a;.,* with equal 6, weights. However, we still run across a problem where two rules, T S  = (cucumbersAtornatoes) --t (onions) and r4 = (cucumbersAtomatoes) + (ctr#3),  such that supporr(r3) = 0.17, confrdence(rs) = 0.41, supporr(r4) = 0.037, and confrdence(r4) = 0.09, are ranked very closely by whereas there is a significant difference between them, as marked by dsd(r3:  rq) = 0.21.

Note that d;,,t only uses the first 2 features of the associa- tion rule profile defined in Section 3.1.

3.2.3 New Similarity Measure  In Equation 1 we introduce a new similarity measure, d,,, that addresses the drawbacks of the measures mentioned above to provide a natural similarity measure?.

llAQ Cllyl+ & , ( A  + B,C + D )  DGr [l +d~&,(A,c)]- 11.4 U CII  where diff& (difference-in-support) is defined as in dad: d i f s , ( A : B )  = supporr(A) +suppon(B)-Z.supporr(AU B). 1 is added to the di@& value for cases where d i f u p ( X :  Y) = 0 and at the same time X 63 Y # 0. As in 131, choices of values for the gamma-weights reflect pref- erences, in our case for inclusion. Sets S,S? E A are  ?The disi imlad~ between two rules i s  Le weighM mm ofdi r r imi ld icr  br- tween the assumptions, conseguenir, und atwibule sets !hat ma!e up the two rules, where each component i s  a weighted mrasuce of the dirsimilanties 01 Le support [d&) and ule atwibute ret (XOI rado).

said to be included if ((S C S?) V (S? c S)). Rules X --f Y and Z + W are said to be included if ((X c 2 ) A (Y & W ) )  V ( (Z  c X) A (W & Y ) ) .  Inclusion is not captured by the xor relationship. For example, let  he four assumptions. / /AI @ Azll / ) /A~ U A*// = 1/2 = IIA3 @ Aall/llA3 U A4ll. However, Az c A I ,  whereas, AB < i14 and An < AS. In that sense, A1 and Az are closer than .43 is to Aq. We therefore set y1 in  Equation 1 t o y  if ( (A C C) V (C 5 A)), and to 1 otherwise. Note that we do not treat .4 = C differently since then IIA @ CII = 0, and the assumption will not contribute to dSc. Since the same analysis holds for consequents, we set y2 to the same value y if ((B C D) V (B C D)) and to 1 otherwise. Since for T? = X + Y and T? = Z -+ W ,  where X Z and Y C W, T? and T? are not included in each other, even though their assumptions and consequents are included, we set y3 in Equation 1 to the same value y if the two rules are included and and to 1 otherwise. For the rest of the paper we refer to the value y = 1/2. Note that d,, uses all the features of the association rule profile and the problematic similarities for the rules from previous sections are reme- died. Forreference,d,,(r1:(.2) = 2.38.d8,(r3:r4) = 1.96.

3.3 Clustering Algorithms  Ai  = ( a , b } ,  Az = (U}, A3 = {u ,b :d} ,  Aq = { a , b : c }  There are two general types of clustering algorithms, hi- erarchical and partitional. Hierarchical algorithms create a hierarchical decomposition of the data, so that if two objects are in the same cluster at a certain level in the hierarchy, they will remain in the same cluster at all higher levels ofcluster- ing. Every hierarchical clustering spans out a dendrogram that shows how the objects are grouped. There are two main classes of hierarchical algorithms: ( I )  agglomerative algo- rithms that start with llR/l singleton clusters and form the hierarchy by successively merging clusters, and, (2) divi- sive algorithms that start with one cluster that includes all the patterns and form the hierarchy by successively split- ting clusters. Partitional algorithms find data partitions that optimize a chosen similarity criterion. See [7, IO, 8, 41 for detailed reviews of clustering methods. We chose to use the agglomerative hierarchical clustering method for exploring interestingness for several reasons: ( I )  the dendrogram that portrays the rule-grouping at each level is extremely useful for thorough investigation of the clusters, and, (2) we cir- cumvent difficult questions that are p a ~ I  of partitional algo- rithms such as what is a good starting point for the iterative optimization and what is the ideal choice for a final number of clusters. For completeness, we outline the basic agglom- erative hierarchical algorithm we started with (1) Initialize llflll singleton clusters, each containing a single association rule. (2) Find the nearest pair of clusters using the chosen similarity measure. (3) Merge the rules in the two distinct  clusters into one cluster. (4) If all the number of remaining clusters is 1, stop. Otherwise go to step 2. See Section 4.2 for implementation changes made to the algorithm.

4 Discussion of Empirical Analysis  We used the following three databases for the analy- sis: ( I )  The Grocery Database that describes 67,470 shop- ping baskets of an online Israeli grocery store using 1,757 boolean attributes. We mined 3,046 rules from this DB us- ing 3.5% thresholds. (2) The Adult Database, compiled from the 45,222 entries with no missing values from the Adult dataset [ 2 ] ,  discretisized into 171 boolean attributes.

Mining this DB with 20% thresholds yielded 13,906 rules.

(3) The WWW Database that describes the accesses of the 2,336 heaviest users to 15 site categories. Mining this DB with 6% threshold yielded 9,206 rules to be clustered.

4.1  4.1.1 Cluster Representation and Interpretation  To provide a compact representation, clusters need to be ah- stracted or named. The rule covers of [IS] will not serve as a representation in the general case since they only work in domains where there are no confidence rule exceptions, which is not always the case. Let C be a cluster of as- sociation rules. We define the ancestor coverage as the cluster representation of C to be the set of ancestor rules: (a + 6 / q b  E A ; V T  = X --f Y E C;r E family(a + b)}.

For the representation to be effective, we need to find a minimal ancestor coverage set. For computational effi- ciency over large clusters we use a greedy algorithm. To find the minimal ancestor coverage we need to depall from RSCL and use RSFS. Although the difference between RSCL and RSFS over C is at most l / ~ ~ C ~ ~ ,  it can trans- late into a substantially larger ancestor coverage. For ex- ample, let CI = {rag = a + b;Tabe  = a + b;c}.  Now, ~ ~ ) ~ : r ~ b  E fami@(. --f 6) and E fumily(a --t c). By definition, RSCL(u + 6 )  = RSCL(u + c). so eitheran- cestor rule could be chosen to be in the ancestor coverage.

However, if a + c is selected first. the ancestor coverage will include both ancestor rules, even though { U  + b} is the minimal ancestor coverage. Using RSFS ensures a mini- mal greedy ancestor coverage; we set the ancestor coverage of acluster C by iteratively finding the ancestor rule with the largest RSFS value of the rules remaining to be covered.

The ancestor coverage cluster representation is the de- sired concise, intuitive. entirely data-driven inferred cluster labeling or representation. For example, one of the larger clusters in  the WWW Database after almost 8000 iterations, using d,,, contained I180 rules, hut its cluster coverage contained only 20 ancestor rules. At that level in  the hier- archical clustering, the average cluster size was 31.7 rules,  Representation and Evaluation of Clustering     and the average cluster coverage only contained 2.4 rules.

The relative small size of the ancestor coverages remained the same over the other databases, and over the different iterations as well. For example, the average ancestor cov- erage was 5% of the cluster size for the Adult Database, and just over 18% for the very sparse Grocery Database.

The ancestor rules are representative of the many rules they cover and their families are easy to understand [15], en- abling a quick decision on whether to further explore the cluster. Further exploration of the cluster is enabled using the information from the dendrogram. Thus, users can drill into clusters that are potentially interesting and easily avoid spending time examining not-interesting clusters.

4.1.2 Outlier Analysis  An interesting application of clustering is in the discovery of ?outlier? association rules, rules that are significantly dif- ferent from the rest of the mined association rules (distance based outlier detection is mentioned in 171). The rule out- liers are manifested in the form of singleton clusters or very small clusters during an advanced stage of the clustering.

These rules can be very interesting as they point out be- havior that is unlike the ?normal? behavior portrayed in the data, likely to describe a small portion of the population, and therefore is unlikely to be known to the database users.

4.1.3 Cluster Evaluation  An important pan of the clustering process is the evaluation of the results. Although the final evaluation can only be provided by the users, measures of clusters quality can pro- vide insight as to how legitimate vs. arbitrary the clustering is. The quality measure we used is the votes of confidence measure: the number of pairs of rules within a cluster (prior to its merging with the next cluster) that, according to the similarity measure, should be in the same cluster, that is, be the next candidates to be merged into one cluster that are already in the same cluster. For a measure of goodness we used this number with the maximum number of votes of confidence for a cluster of size vi: r r i ( m  - 1)/2 - 1.

4.2 Clustering Implementation Issues  The biggest challenge we have come across in imple- menting and running the clustering was in the processing- time and memory demands (we used a Compaq ProLiant DL580.4 700MHz Xeon CPUs with 2GB RAM). Storing a similarity matrix (n(n - 1)/2 proximity values), and deter- mining the two most similar items in it over many iterations requires prohibitive amounts of memory and computational powerltime. We therefore changed the algorithm to main- tain only the similarity measures most likely to be used in the hierarchical clustering process to reach 1 cluster from  the IlfllI initial singletonclusters. Those are the smaller sim- ilarity measures. Their number is obviously larger than \IQ(( since some measures will be between membersof the same cluster, as in Section 4.1.3. Note that although this is a mi- nor technical modification lo the algorithm, it reduced the memory requirements and execution time by several orders of magnitude and was critical in making the clustering fea- sible over large number of rules. Other such technical mod- ifications were made to enhance the algorithm runtime.

5 Conclusions  In this work we presented an association rule clustering framework exploring interestingness of rules. This cluster- ing automates much of the tedious manual labor otherwise needed. and is an alternative to methods that require signif- icant amounts of domain knowledge to determine interest- ingness of unconstrained rules. We introduced the ancestor coverage, a concise, data-driven representation of the clus- ters, as well as a new and natural rule similarity measure.

From our analysis, clustering of association rules stands to be a very promising and fruitful venue for exploring inter- estingness. with many open areas for future research.

