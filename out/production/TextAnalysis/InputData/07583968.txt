2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (WCFTR?16)

Abstract?Frequent Itemset Mining is one of the most popular techniques to extract knowledge from data. However, these mining methods become more problematic when they are applied to Big Data. Fortunately, recent improvements in the field of parallel programming provide many tools to tackle this problem.

However, these tools come with their own technical challenges such as balanced data distribution and inter-communication costs. In this paper, we are presenting a detailed survey of Hadoop, which helps in storing data and parallel processing in distributed environment. Here we have explored various Frequent Itemset Mining techniques on parallel and distributed environment. The aim of this paper is to present a comparison of different frequent itemset mining techniques and help to develop efficient and scalable frequent itemset mining techniques.

Index Terms? Frequent itemsets, association rule mining, mining algorithms.



I. INTRODUCTION Data mining is the process of extraction of information  from large databases and it is a powerful new technology having a great potential to help researchers as well as companies on the most important information in their data warehouses [1]. Data mining tools are used to predict the future trends and behaviors thus allowing businesses to make knowledge-driven decisions.

Frequent itemset mining in distributed environment is a problem and must be performed using a distributed algorithm that does not require exchange of raw data between the participating sites. Distributed data mining is the process of mining data in distributed data sets. According to Zaki in [2], two dominant architectures exist in the distributed environments i.e., distributed memory architecture (DMA) and shared memory architecture (SMA).

In DMA, each processor has its own database or memory and has access to it. DMA systems access to other local databases is possible only via message exchange. DMA offers a simple programming method, but limited bandwidth may reduce the scalability. On the other hand, in SMA each processor has direct and equal access to the database in the system. Thus, parallel programs on such systems can be implemented easily.

A set of items in a database is known as itemset. If the occurance of items in a particular transaction is frequent, it is called as frequent itemset and the support (or count) of frequent itemset is greater than some user-specified minimum support.

Frequent Pattern Growth (FP-Growth) algorithm is one of the most popularly used data mining approach for finding frequent itemsets from large datasets [3]. But the main challenge faced by various frequent itemset mining algorithm is its execution time in distributed environment.

Distributed sources of voluminous data have created the need for distributed data mining. The conventional data mining algorithms/techniques which work efficiently on centralized databases have some limitations of its own when applied on distributed databases. In distributed data mining, data is located at distributed locations and mining is performed on every local database to find globally mined data. Figure 1 depicts the architecture for distributed data mining.

This paper is organized as follows: Section 2 discuss about the various techniques/algorithms proposed by authors for frequent itemset mining over the years, Section 3 presents a brief overview of Hadoop and its architecture, and at last, Section 4 gives a conclusion.

Figure 1: Architecture for distributed data mining [21]   Local DM  Local DM  Local DM  Local DB 2    Local DB 1   Local DB N    Global Data Mining     2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (WCFTR?16)

II. LITERATURE REVIEW In the following section we will discuss various  algorithms/techniques that are proposed by authors for frequent itemset mining in the last few years.

In 1994, authors presented the Count Distribution algorithm [4][5], each site finds local support count of Ck i.e., Candidate itemsets in its own local database. Each Site exchanges its local support with other sites to obtain entire support for all candidate itemsets. Thus, each site obtains entire support for all the candidate itemsets and its local support with other sites to obtain the common support for all candidate itemsets. Each site obtains Lk, the frequent itemset along with the candidate itemset with length of k+1 obtained from each site by execution of Apriori gen() function on Lk.

The Count Distribution algorithm?s main advantage is that it doesn?t exchange data tuples between processors instead it exchanges the counts. CD algorithm?s communication overhead is O(|C|*n) at each phase, where  is the size of candidate itemsets |C| and n is the number of datasets. The limitation of the CD algorithm is that the complexity O(Ck/n) becomes high if there are more number of candidate itemsets Ck generated at site n.

To overcome the limitation of the CD algorithm, in 1996, authors proposed the fast distribution mining algorithm (FDM) [6]. In FDM algorithm, each site play different roles i.e., in the beginning a site is considered as ?home site? for a specific produced set of candidate sets and then it subsequently changes to a polling site to get response time from other sites and at last it becomes remote site. The remote site returns support count of the items and the polling site generate the frequent itemsets for large databases.

FDM?s advantage over Count Distribution algorithm [22] is that the communication overhead is reduced to O (|Cp|*n), where |Cp| is the potentially large candidate itemsets and n is the number of sites, respectively. FDM generates fewer number of candidate itemsets as compared to CD, when the number of disjoint candidate itemsets is large among various sites. FDM reduces the communication overhead to some extent, but it can further improve its efficiency if the candidate itemsets are reduced and global pruning of itemsets is done.

In the same direction, authors proposed an Optimized Distributed Association Mining (ODAM) [7] algorithm in 2003. ODAM calculates 1-itemset from each site and then broadcasts those itemsets and generates global frequent 1- itemsets. In the next step, each site generates candidate 2- itemsets, computes its support count and at the same time eliminates infrequent itemsets. ODAM algorithm generates frequent 2-itemsets globally and iterates through the main memory transaction. At last, it generates the support count and the final frequent itemset is generated.

ODAM exchanges fewer messages than FDM which improves its efficiency to a large extent. But the message exchange size increases linearly as we increase the number of sites. Thus, the message passing overhead becomes high as  compared to FDM. Another limitation of ODAM is that it doesn?t show the local frequent itemsets that are generated at each site.

To overcome the above mentioned limitations, authors proposed a Distributed Trie Frequent Itemset Mining (DTFIM) [8] algorithm. In DTFIM algorithm, each site scans its local database and determines the local count (1-itemsets) and a vector is kept to make support count of every item. At the end, each site synchronizes their data structure and the trie copies are alike. In the second pass, the candidate 2-itemsets are calculated and a 2-d array is used for this purpose. At the end, counts are synchronized and the global support count for candidate 2-itemset are calculated and the trie copies are updated. In the next step, for each pass k (k>=3) candidate itemsets are calculated. The process is repeated and pruning is performed simultaneously at each stage. The final output is frequent itemset.

The complexity of DTFIM is O(n2) which is efficient than ODAM. DTFIM?s efficiency also increases as the algorithm uses trie structure which is useful for pruning at local site.  The limitation of this algorithm is high message passing overhead.

A slight variation in DTFIM algorithm can improve the time required for mining frequent itemsets. In 2012, authors proposed a technique which mine frequent itemset using a gossip based protocol [9] (i.e., A communication between sites which is purely based on random communication). The trie based apriori structure is used to improve the performance.

First the local frequent itemsets are computed and the support count is checked and the gossip-based global aggregation is performed. This algorithm is more efficient as it is employing trie data structure and grouping of nodes.

The complexity of this algorithm is O(nlogn) which is far better than DTFIM algorithm?s complexity. Also, the Gossip based communication helps in reducing the computation overhead of finding frequent itemset. But, the scalability is high as it requires minimum communication cost and comparison costs.

The gossip based algorithm [9] can further be improved by grouping the nodes and arranging them into a hierarchical structure at each site in the distributed environment. It can be achieved by using Hadoop which allows distributed processing of large databases across the nodes in a distributed environment. By using Hadoop, we can reduce the message passing overhead as the Hadoop Distributed File System (HDFS) [12] will store the data in trie structure at each site.

Another advantage of using Hadoop is that it helps to reduce the fault tolerance. Since Hadoop uses replication between the sites in the distributed environment. Thus, every node has the copy of data if any node fails to communicate.

In the next section, we discuss about Hadoop technology, its architecture and its working in a distributed environment.



III. HADOOP Hadoop is a free open source platform, which helps in  storing data and parallel processing in a distributed    2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (WCFTR?16)  environment. Hadoop splits the large database into blocks of data and distributes over the clusters in the distributed environment. To process the data, MapReduce is used for parallel processing on the clusters, thus reducing the execution time.

The Hadoop Distributed File System (HDFS) is basically a distributed file system which is designed to run on commodity hardware. It is many similar to the existing distributed file systems. But, there are some differences between HDFS and other distributed file systems which makes it significant. HDFS is highly fault-tolerant and is designed in such a way that it can be deployed on low-cost hardware.

HDFS also provides high throughput access to application data and is very suitable for applications that have large data sets.

Figure 2: HDFS Architecture [23]  Figure 2 shows the HDFS master/slave architecture [19].

An HDFS cluster consists of two parts viz., a single NameNode and more than one DataNode. NameNode is a master server that regulates access to files by clients and manages the file system namespace. There are a number of DataNodes in HDFS, usually one per node in the cluster. The DataNode manages the storage which is attached to the nodes that they are running on. HDFS exposes a file system namespace and allows the user data to be stored in files.

Internally in an HDFS, a file is split into one or more blocks and these blocks are then stored in a set of DataNodes.

The NameNode is also used to execute file system namespace operations which include opening a file, closing a file and renaming files and directories in the HDFS. It also performs the mapping of blocks of data to the DataNodes. On the client?s side, the DataNodes are responsible for serving the read and write requests from the HDFS. The DataNodes also perform operations such as block creation, deletion, and replication upon the instruction provided from the NameNode.



IV. CONCLUSION As we have reviewed different techniques of frequent  itemset mining in parallel and distributed environments, most of the techniques/algorithms have shortcomings of their own.

Although, hadoop technology can provides a better platform to overcome the shortcomings of the above mentioned mining techniques.

