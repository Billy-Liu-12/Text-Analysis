<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Using  Association Rules for Completing Missing Data

Abstract We present in this paper a new method for completing missing data using the concept of association rules. The basic idea is that association rules describe the dependency relationships among data entries in a dataset where all data, including the missing ones, should hold the similar relationships.

For a missing datum, we guess its possible value according to related association rules. A new completing procedure and a new evaluation function are developed and presented. The evaluation function is scored according to the support, confidence, and lift of association rules, which reasonably reflects the dependency relationships among existing and missing data. Experimental results show that our method is feasible in completing some incomplete datasets.

1. Introduction With the explosive growth of data available from various sources, the issue on ensuring the quality of data becomes more and more important. Recently, knowledge discovery from databases (KDD) [6][17] receives a lot of attentions, which tries to extract hidden but probably valuable information or knowledge from databases. Evidently, the mined results are useful and applicable only when the input data are of high quality. Unfortunately, most real- world databases contain ?dirty? data [11]. Several types of ?dirty? data are found in databases, such as incorrectness, duplication, inconsistency, and missing data [18]. This paper focuses on the cases when data are missing.

Missing data are introduced into databases because of the carelessness of human or the failure of machines in processing data. In a dataset, a normal record is complete if all its fields (or attributes) are fulfilled with proper data. A missing datum indicates that a field of a record is empty. An incomplete case in a dataset is a record containing missing data. When KDD is performed on an incomplete dataset, two typical approaches are employed for handling missing data.

One approach is to develop robust algorithms which are tolerant of noisy or missing data. Usually, such algorithms can perform on limited types and amount of missing data and are costly to develop. The other approach is to try to improve the quality of input data and then applies existing KDD algorithms, which is more feasible.

There are two types of approaches being able to maintain the quality of data. One is to abandon or remove the whole incomplete case, such as list-wise deletion (LD) and variable deletion (VD) [15]. Both LD and VD reduce the size of data samples for KDD and may lead to biased results. They usually remove critical data or variables and mislead the mining algorithms to improper conclusions. On the contrary, if all cases are needed, the missing data have to be fulfilled by some proper values before they are processed for KDD. There are several approaches being able to estimate the possible values for missing data [12]. Basically, they are all based on the missing information principle [15], i.e., the value for replacement is one of the existing data. Based on statistic or heuristic inference, these approaches usually produce satisfactory results. Clearly, if we can correctly guess what values are needed for completing    correctly guess what values are needed for completing the missing ones, we can ensure a better quality of data.

This paper describes how association rules can serve as an effective mechanism for guessing what the missing data should be. A new completing procedure and a new evaluation function are developed and presented. Experimental results show that our method is feasible in completing some incomplete datasets.

2. Related Work An intuitive but simple method for completing missing data is to substitute the missing data by 0-7695-2291-2/04 $ 20.00 IEEE specific values. For example, zero substitution (ZS) [9] is to replace all missing data by ?zero?, which is usually the initial value of the field, e.g., ?0? for integral data, ?F? for Boolean data, etc. Mean substitution (MS) [9] is to replace missing data by the mean value of the data defined under the same variable.

Random substitution (RS) is to replace missing data by a value randomly selected from the existing values defined under the same variable. ZS, MS, and RS are simple and fast; but unfortunately, usually produce misleading results. Method like that in [11] utilities the k-nearest neighbor approach and finds the most similar k-nearest neighbor cases to replace missing values.

Gibbs sampling (GS) [5][10] is a sampling approach for Bayesian inference, which provides a sample from the posterior distribution of the unknown parameters and computes empirical estimates of posterior mean of input parameters. The Expectation Maximization (EM) algorithm [8] iterates the E step and the M step. The E step finds the conditional expectation of the missing values according to the observed data and current estimated parameters. The M step investigates maximum likelihood estimation as if there were no missing data. Bound &amp; Collapse (BC) [20] is a deterministic method that estimates some pre- defined parameters and computes the minimal and maximal possibilities of the possible values by bounding the set of possible values and then collapsing into a unique value via a convex combination of the extreme points with weights. Maximum likelihood estimation (MLE) [23] is a method for statistical estimation, which can determine the parameters that maximize the probability of the sample data. MLE estimates the probability of obtaining a particular set of data given a chosen probability model. In addition, they provide efficient methods for quantifying uncertainty through confidence bounds to replace missing data. Robust Bayesian Estimator (RBE) [22] is based on Bayesian estimation, which learns conditional probability distributions from incomplete data sets by providing probability intervals that describe the upper and lower bounds of estimation of possible values for each missing datum. In many applications, RBE out- performs the others when data are closely related.

Other than completing missing data for data recovery, completing missing data is also needed in mining association rules, such as that in [13][14][19].

There approaches develop robust mining algorithms or define new measure for evaluating support and confidence heuristically or probabilistically, and generate reliable association rules.

3. Association Rules for Completing Missing Data 3.1. Association Rules for KDD Recently, the exploration on the associations among data in transactional datasets receives a lot of attentions. Identifying the associations among data can help data analysts understand the inter-relationships of    help data analysts understand the inter-relationships of data in the underlying dataset. Here, we give a brief review on the definition of association rules. Let I = {i1, i2,? , im} be a set of distinct literals called items and D = {T1, T2, ? , Tn} a set of transactions, where Tj ? I, 1djdn. An association rule [1] is an implication of the form A? C, where A, C ? I are disjoint itemsets, A ?

C = I. A and C are called the antecedent and consequence, respectively. Each itemset X has an associated measure of statistical significance s(X) called the support of X, where s(X)= .

|| |}|{| D DddXd ???

To measure the meaningfulness of an association rule r: A?C, the indicators, support, confidence, and lift, are defined as follows. The support, s(A?C) = , || |}|{| D DddCAd ????

of r reflects the statistical significance, the confidence c(A?C)= )( )( As CAs ?  of r indicates the statistical strength, and the lift lift(A?C)= )()( )( CsAs CAs ?  describes the dependence between A and C. Finding association rules can be done in two steps: (1) find the frequent itemsets; and (2) find the rules that satisfy the constraints which are usually controlled by the values of minimum support, minsupp, and minimum confidence, minconf. Refer to [1][3][7][16][24], for algorithms of finding association rules.

3.2. Data Associations The purpose of discovering association rules is to find items in a transaction that imply the presence of other items in the same transaction. The items appearing the same association rule reflect inter-relationships among the items. In the following, we define ?data association? based on a similar concept of association rules as the dependency relationships among data entries in a dataset. Suppose D is a dataset defined over n variables X1,?, Xn. Let J = {Xi=xik| it1, xik is an 0-7695-2291-2/04 $ 20.00 IEEE instantiation of Xi} be a set of items. A data association is an association rule of the form H?{e} defined over J, where {e}, H ?J and e?H. For example, in a dataset describing the payment status of purchase, data associations ?{Pay_Type = cash} ? {Customer_Age = young}? and ?{Pay_Type = credit} ? {Customer_Age = senior}? state that young customers usually pay by cash and the senior ones usually pay by credit. Note that, Pay_Type and Customer_Age are variables defined in the dataset.

The applicability of data associations can be scored in different domain problems.

3.3. Obtaining Data Associations There are two sources where data associations can be obtained. One is to inquire from domain experts who    obtained. One is to inquire from domain experts who explicitly describe the data associations related to the underlying dataset. In such data associations, the dependency relationships among data entities and their applicabilities are defined heuristically. If such relationships cannot be explicitly described, the other means is to obtain from the underlying dataset using the techniques of mining association rules. However, the datasets discussed in this research are defined in the relational form. Mining for association rules are for transactional datasets. We first have to transform the dataset into the transactional form and then apply the algorithms for finding large itemsets. The following procedure can be used.

Suppose that D is a dataset containing m cases, c1, c2,?, cm, which are defined over n variables, X1,?, Xn.

For each case ci, 1d i dm, we convert D into a transactional dataset TD by taking ci as the transactional ID and the instantiations of X1,?, Xn in ci as the items.

That is, we let J = {Xj=xjk|1djdn, xjk is an instantiation of Xj found in D} be the itemsets and exclude missing data from J. In this transformation, a transaction in TD looks like ??ci , {X1=x1ki,?, Xn=xnki}??. Next, given a specific threshold of minsupp, we explore the large itemsets, L1,?, Lg, gt1, in TD. The Apriori algorithm [3] is employed for finding large itemsets. Next, for each element euv in Lu, 1d udg, vt1, we compute, for each element duvw, wt1, of euv the confidence of ?euv\{duvw}? {duvw}?. If the confidence is large than a pre-defined minconf, we put euv\{duvw}?{duvw} and associated support and confidence into the set of data associations. For convenience, the antecedent of a data association t is denoted as H(t) and d(t,X) is the consequence (tail) of t, where X is a variable. The procedure is presented as pseudocodes in Figure 1.

Note that, there are some other algorithms being able to find large itemsets effectively, such as the ones in [1][2][3][24], which can be used in the above procedure. Finding data associations from existing datasets is reasonable for relational databases, since the variables in such a database describe specific relations among data in the dataset. Such relations are held by the existing data and the missing ones. Also, data associations can be viewed as special association rules for transaction analysis. The differences between data associations and association rules are as follows.

INPUT: A dataset D of m cases defined over n variables, the value of minimal support, minsup, and the value of minimal confidence, minconf  OUTPUT: A set T of data associations Let TD be an empty transactional dataset For each case ci=?X1=x1ki,?, Xn=xnki?, 1d i dm, in D, Do TD = TD + ?ci , {X1=x1ki,?, Xn=xnki}?; // Finding large itemsets from TD FindLargeItemsets(Dt, minsupp) = {L1,?, Lg}, gt1; // Collect data associations from frequent itemsets Let T={}; For each large itemset euv in Lu, 1dudg, vt1, with support supp(euv)t minsupp, Do For each element duvw of euv, wt1, Do Let conf(t) be the confidence of t=?euv\{duvw}?{duvw}?; If (conf(t)tminconf) T=T?{t}; Return T; Figure 1. The procedure for obtaining data associations x The items in data associations are variable instantiations in datasets; their variances are limited.

The items in association rules for transaction analysis are various goods; The maximal length of transactions in data associations are the number of variables in the underlying dataset.;    are the number of variables in the underlying dataset.; while the maximal length of transactions in association rules are the number of items. Usually, the former is less than the later.

3.4. The Completing Procedure After data associations are obtained, we can utilize the data associations for completing missing data. Also, it is assumed that all data in similar datasets follow similar dependency models. Suppose that D0 is an incomplete dataset containing c cases, which are defined over n variables, X1,?,Xn. Let d1, d2,?,dm be m incomplete cases, mdc, in D0 and T is a set of data associations. In the incomplete case di, 1didm, for the missing data defined under the variable Xji, 1d j dn, i.e., Xji=?, we find if a data association t=H(t)?{Xji=xjk} can be found in T. If there are x such links t1, t2,?, tx, we may have x possible combinations for completing 0-7695-2291-2/04 $ 20.00 IEEE Xji. Then, we investigate all H(tv), 1d vd x, whether the variable instantiations in H(tv) can consistently interpret the case di. The ones that can consistently interpret di are evaluated and one of them is used for completing Xji.

The applicability of the data association is determined by considering the associated support, confidence, and lift since they indicate statistical significance, strength, and the co-relation between H(t) and {Xji=xjk}, respectively. Moreover, the length of H(t) and the degree of consistently interpreting the variables of di, should also be taken into account. Let wt be the number of elements in H(t) and the support, confidence, and lift values of t be supp(t), conf(t), and lift(t), respectively. Based on the above considerations, we use the following formulas, appl(t) and score(t), to score the applicability of t.

appl(t)= ?  u tw j i jk i jk xXappltI  )()( ????(1) where I(t) is a selecting function. We define I(t)=1 if H(t) can consistently interpret di and I(t)=0 if any element Xijk= xijk of H(t) conflicts with the existing instantiation of Xijk in di. The applicability of each variable instantiation in t is defined as follows.

)( ijk i jk xXappl  = ?? ? ? ?   in?if0.5 inappearedhasif1 i i jk i i jk i jk dX    dX dxX Then, we define score(t)= n w t t tlift w tappl )( )( u ??.????(2) If di has more than one missing data, a1 and a2, there may have two or more data associations tj1, tj2, ? which are composed of the instantiations of different variables can complete a1 and a2. We select the combination that can consistently and simultaneously interprets all variable instantiations of di with the largest sum of scores for completing the missing data in di. If there is no data association containing Xji in its consequence can be found in T, we check the appearing frequency of the instantiation of Xji  in Lk, kd2, and the instantiation of Xji that appear most frequently is selected to complete the missing data in di.

The completing procedure is presented as pseudocodes in Figure 2 and illustrated in Figure 4.

The complexity of finding data associations depends on the settings of minsupp and minconf and the algorithm for finding large itemsets; which is similar as that in [1][2][3][24]. Suppose that there are c incomplete cases with h un-instantiated variables (in average) and r data associations related to these un- instantiated variables, the complexity of completing procedure can be calculated as follows. Eq.(1)&amp;(2) examines wt elements of a data association t to calculate appl(t) and score(t). Since wt and h are less than the number of variables of the underlying dataset, they can be considered as constants. Therefore, the complexity of the proposed procedure is about O(cur).

4. An Example  Suppose that a test dataset is given in Table 1 (a). The dataset is defined by 4 variables, X1, X2, X3 and X4.

Each variable can be instantiated by 1 or 2. The dataset can be converted into the transactional form, as shown in Table 1 (b). Suppose that minsupp and minconf are set as 30% and 75%, respectively. By applying the algorithm in Figure 1, the corresponding data associations can be obtained, as shown in Table  2.

Suppose that we try to complete the missing datum, X1=?,  in case c6. We search in Table  2 if some data associations include X1 in the consequence. In this case, r4 is considered. In r4, we obtain lift(r4)=2.5 and appl(r4) =I(r4)u( appl(X2=1) + appl(X1=2)) = 1*(1+0.5)=1.5 and score(r4 = (appl(r4)/wt)*lift(r4)wt/n = (1.5/2)*2.52/4 = 1.186. Since r4 can consistently interpret c6, we use X1=2 for the missing value. In c1, two data are missing, X3=? and X4=?, are found. Among the data associations, 3 rules, r12, r13, r14, are examined for X3 and 3 rules, r15, r16, r17, are for X4. By applying the same procedure, we obtain score(r12)=1.645, score(r13)=1.186, score(r14)=0.791, score(r15)=1.325, score(r16)=0.791, and score(r17)=1.027. Among these rules, since we have to select the one that can consistently and simultaneously interpret c1 with a highest score, we use X3=2 and X4=1 for completing c1.

Figure 3. The completing procedure INPUT: An incomplete dataset D, a data association graph T, a score threshold s? OUTPUT: A completed dataset D? For each incomplete case ci in D, Do Let Xi1, Xi2, ?, Xih be h un-instantiated variables in ci;    For each un-instantiated variable Xij of ci?, Do For each link t = H(t)? Xij =d(t, Xij) which consistently interprets ci ?, Do Compute the score sijk of t according to Eq.(1)&amp;(2); If sijkts? Put (t,sijk) into a set Sij; Find in each Sij an element (tij, sij) so that H(ti1), H(ti2),?, H(tih) can consistently interpret ci and the sum of scores 6j sij is maximal; Replace the missing value of Xi1, Xi2,?, Xih in ci by Xi1=d(ti1,Xi1), Xi2=d(ti2,Xi2), ?, Xih =d(tih,Xih), respectively; Return D?=the updated D; 0-7695-2291-2/04 $ 20.00 IEEE Incomplete case di Incomplete case di Does t: H(t)?{Xj=zjk} exist in T Does t: H(t)?{Xj=zjk} exist in T For each missing datum Xj=? in di For each missing datum Xj=? in di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Find elements in L1 or L2 that contain {Xj =zjk} and can consistently interpret di Compute score(t) according to Eq.(1)&amp;(2) Compute score(t) according to Eq.(1)&amp;(2) Does H(t) consistently interpret di ?

Does H(t) consistently interpret di ?

Apply the value to complete the missing data Apply the value to complete the missing data Figure 4. The proposed completing procedure Table  1. (a) a test dataset of 10 cases; (b) the transactional form of the dataset Case X1 X2 X3 X4  TID Items c1 2 1 ? ?  t1 X1=2,X2=1 c2 2 1 2 1  t2 X1=2,X2=1, X3=2, X4=1 c3 2 1 2 1  t3 X1=2,X2=1, X3=2, X4=1 c4 1 2 1 ?  t4 X1=1,X2=2, X3=1 c5 1 2 1 2  t5 X1=1,X2=2, X3=1, X4=2 c6 ? 1 2 1  t6 X2=1, X3=2, X4=1 c7 1 ? 1 ?  t7 X1=1, X3=1 c8 1 2 1 1  t8 X1=1,X2=2, X3=1, X4=1 c9 1 ? 1 ?  t9 X1=1, X3=1 c10 1 2 1 2  t10 X1=1,X2=2, X3=1, X4=2 Table  2. Data associations of D ID Data association supp conf r1 {X3=1}?{X1=1} 0.60 1.00 r2 {X2=2, X3=1}?{X1=1} 0.40 1.00 r3 {X2=2}?{X1=1} 0.40 1.00 r4 {X2=1}?{X1=2} 0.30 0.75 r5 {X3=2, X4=1}?{X2=1} 0.30 1.00 r6 {X3=2}?{X2=1} 0.30 1.00 r7 {X1=2}?{X2=1} 0.30 1.00 r8 {X4=1}?{X2=1} 0.30 0.75 r9 {X1=1}?{X3=1} 0.60 1.00    r9 {X1=1}?{X3=1} 0.60 1.00 r10 {X2=2}?{X3=1} 0.40 1.00 r11 {X1=2,X2=2}?{X3=1} 0.40 1.00 r12 {X2=1, X4=1}?{X3=2} 0.30 1.00 r13 {X2=1}?{X3=2} 0.30 0.75 r14 {X4=1}?{X3=2} 0.30 0.75 r15 {X2=1, X3=2}?{X4=1} 0.30 1.00 r16 {X3=2}?{X4=1} 0.30 1.00 r17 {X2=1}?{X4=1} 0.30 0.75 5. Experiments In order to test the effectiveness of our method, several experiments are performed and the results are compared with RBE. The datasets, DA and DB, are generated syntactically. In DA, data are randomly generated and the variables in DA have no particular dependency relations. In DB, three variables, X1, X2, and X3, are defined and each variable has three different instantiations, xij, i=1,2,3 and j=1,2,3. Cases in DB are generated (1) by the associations {X1=x11}?{X2=x22}, {X2=x21}?{X3=x32}, and {X3=x33}? {X1=x13}, and (2) randomly if the above associations do not hold. The datasets, Monk, TAE, and Solaris are from [4]. Table 3 describes the basic information of these datasets. In these datasets, we randomly remove some data from completed cases and test if our method can successfully recover the missing ones. For nm missing data, the accuracy of recovery D is defined as 1- nw/nm if nw data are incorrectly guessed.

From the experimental results shown in Table 4, our  method is more accurate than RBE is.

Table 3. Description of the test datasets Dataset ID cases variables niv missing ratio source DB -1 15% DB -2 30 3 3 20% DA -1 15% DA-2 100 6 2 20% Syntactic Monk -1 10% Monk -2 415 7 3 20% TAE -1 20% TAE -2 151 5 3 30% Solaris -1 5% Solaris -2 1066 13 4 10% UCI [4] niv: # of instances in each variables (in average) Table 4. Experimental results Our method RBEDataset ID (s, c, nda) D (s, c, nda) D D DB -1 (25,65, 6) 92.3% (15,50,8) 92.3% 61.5% DB -2 (25,65, 6) 94.4% (15,50,8) 94.4% 55.6% DA -1 (20,20,350) 81% (30,40,179) 78% 74% DA-2 (20,20,350) 83% (30,40,179) 75% 79% Monk -1 (15,15,115) 57.7% (10,35,181) 47.1% 46.1% Monk -2 (15,15,115) 57.5% (10,35,181) 45.6% 26.2% TAE -1 (10,30,73) 70.2% (5,60,115) 60.9% 64.2% TAE -2 (10,30,73) 65.2% (5,60,115) 55.1% 52.3% Solaris-1 (40,40,2926) 78.3% (60,60,988) 71% 72.5% Solaris-2 (40,40,2926) 80.4% (60,60,988) 73.2% 70% (s, c, nda): s: minsupp and c: minconf used in generating data associations and nda  is the number of data associations obtained 6. Discussions &amp; Conclusion We presented in this paper a new method for completing missing data using data associations. The basic concept is that association rules describe the dependency relationships among data entries in a 0-7695-2291-2/04 $ 20.00 IEEE    0-7695-2291-2/04 $ 20.00 IEEE dataset and missing data should hold the similar relationships. One of the advantages of using this approach for completing missing data is that data associations can be easily and reasonably obtained.

Most reasonable datasets include reasonable association rules. Clearly, data associations truly describe the cross-relation among data instantiations more accurately. The score function that we presented in Eq.(1)&amp;(2) is based on the support, confidence, lift.

There may exist other measures, such as [16], that reveal more information for the applicability of data associations. The score function can be further improved. Unfortunately, the accuracy of the proposed method depends on the number of data associations.

When the threshold of minsupp and minconf are low, there will be a large number of data associations being generated, resulting in inefficiency of the completing procedure. The future work includes developing a better informative score function and reducing the number of data associations and determining suitable values of minsupp and minconf. Currently, in this paper, only discrete data are discussed and the data existing in datasets are assumed to be correct and  noise-free. We are extending the proposed method to handle non-discrete, noisy, and incorrect datasets.

7. References [1] Agrawal, R., and Srikant, R. (1994), ?Fast algorithm for mining association rules,? in the Proceedings of the [2] Agrawal, R., Imielinksi, T., and Swami, A. (1993), ?Dataset mining: a performance perspective,? IEEE TKDE, vol. 5, no. 6, pp. 914-925.

[3] Agrawal, R., Srikant, R., and Vu, Q. (1997), ?Mining association rules with item constraints,? in the Proceedings Beach, California, pp. 67-73.

[4] Black, C., Keogh, E., and Merz, C.J. (1999), UCI repository of machine learning databases, URL http://www.ics.uci.edu/~mlearn/MLRepository.html [5] Buntine, W.L. (1994), ?Operations for Learning with Graphical Models,? Journal of AI, vol. 2, pp. 159-225.

[6] Chen, M.S., Han, J., and Yu, P.S. (1996), ?Data mining: an overview from a database perspective,? IEEE TKDE, vol.

8, no. 6, pp.866-883.

[7] Coenen, F., Goulbourne, G., and Leng, P. (2004), ? Tree structures for mining association rules,? Data Mining and Knowledge Discovery, vol. 8, no. 1, pp.25-51.

[8] Dempster, A. P., Laird, N.M., and Rubin, D.B. (1977), ?Maximum likelihood from incomplete data via the EM algorithm,? Journal of the Royal Statistical Society, vol. 39, no. 1, pp.1-38.

[9] Frick, J.R., and Grabka, M.M. (2003), ?Missing Income Information in Panel Data: Incidence, Imputation and its Impact on the Income distribution,? in the Proceedings of the Workshop on Item-Non-response and Data Quality in Large Social Surveys, October 9-11.

[10] Giudici, P., and Castelo, R. (2003), ?Improving Markov Chain Monte Carlo model search for data mining,? Machine Learning, vol. 50, no. 1-2, pp.127-158.

[11] Hern?ndez, M.A., and Stolfo, S.J. (1998), ?Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem,? DMKD, vol. 2, no. 1, pp.9-37.

[12] Intelligent CAM Systems Laboratory, ?Methodologies for dealing with the Missing Data?, http://www.eng.uc.edu/icams/resources/missing_data.htm.

[13] Kryszkiewicz, M., (2000), ?Probabilistic Approach to Association Rules in Incomplete Databases,? Web-Age Information Management, pp. 133-138.

[14] Kryszkiewicz, M., and Rybinski, H. (1999),    [14] Kryszkiewicz, M., and Rybinski, H. (1999), ?Incomplete Database Issues for Representative Association Rules,? ISBN: 3-540-65965-X, pp. 583-591.

[15] Little, R.J.A., and Rubin, D.B. (2002), Statistical analysis with missing data, Wiley, New York, ISBN: 0471183865.

[16] Omiecinski, E.R. (2003), ?Alternative interest measures for mining associations in databases,? IEEE TKDE, vol. 15, no. 1, pp.57-69.

[17] Piramuthu, S. (1998), ?Evaluating feature selection methods for learning in data mining applications,? in the Proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, vol. 5, pp. 294-301.

[18] Pyle, D. (1999), Data Preparation for Data Mining, Morgan Kaufmann Publishers, Inc.ISBN:1-55860-529-0.

[19] Ragel, A., and Cremilleux, B. (1999), ?MVC - A preprocessing Method to deal with missing values,? knowledge based system, vol. 12, pp.285-291.

[20] Ramoni, M., and Sebastiani, P. (2000), ?Bayesian Inference with Missing Data Using Bound and Collapse,? Journal of Computational and Graphical Statistics, vol. 9, no.

4, pp. 779-800.

[21] Ramoni, M., and Sebastiani, P. (2001), ?Robust Bayes Classifiers,? AI, vol. 125, no. 1-2, pp. 207-224.

[22] Ramoni, M., and Sebastiani, P. (2001), ?Robust Learning with Missing Data,? Machine Learning, vol. 45, no.

2 , pp. 147-170.

[23] Scott, R.E. (1993), Maximum Likelihood Estimation : Logic and Practice, SAGE Publications, ISBN: 0803941072.

[24] Zaki, M.J., and Hsiao, C.J. (2002), ?CHARM: An efficient algorithm for closed itemset mining,? in the on Data Mining.

