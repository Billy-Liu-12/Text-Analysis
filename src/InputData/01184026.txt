Intersection Based Generalization Rules for the Analysis of Symbolic Septic  Shock Patient Data

Abstract  In intensive care units much data is irregularly recorded.

Here, we consider the analysis of symbolic septic shock pa- tient dara. We show rhar ir  could be worth considering rhe generalizarion paradigm (individual cases generalized to more general rules) instead of the association paradigm (combining single attributes) when considering very indi- vidual cases (e.g. parients) and when expecting longer rules than shorter ones. We present an algorirhm f o r  rule gen- erarion and classification based on heurisrically generated set-based intersections. We demonstrate the usefulness of our algorirhm by analysing our septic shock patient data.

1. Introduction  The septic shock is of prime importance in intensive care medicine [l]. Most of the data in our database was symbolic data (therapies, operations, medication, admission data).

Since the patients are very individual in their behavior the patient data is very inhomogenous.

Although association rules (a-priori 121) are a common tool for analysing data, we tried a mechanism just the other way round: we generalize the patient data, beginning with the individual cases. We claim that it is worth and in fact reliable considering this generalization paradigm (see e.g.

[31). Our algorithm computes heuristically a kind of closed frequent itemsets 141 but not all of them and in a different, more natural way (using no backtracking or generators) [5] .

In Sect. 2 we present the new set-based heuristic gen- eralization algorithm ?GenIntersect? 151 for classification, able to handle uncertain data with many attributes when ex- pecting rather longer than shorter rules that should used not only for classification but also for medical interpretation. A winner-takes-all classification procedure that is based on a new measure ?weighted confidence? is presented in Sect. 3.

With the help of this measure we evaluate the importance of the attributes and select features.

In Sect. 4 we discuss the main differences of our ap- proach and the a-priori approach, mainly the robustness and the rule-context characteristics. We apply our algorithm in Sect. 5 to our symbolic septic shock patient database.

Additionally, we want to emphasize that our aim was not the comparison of several algorithms by evaluating bench- mark data, although this is an important task. Our aim is to show that the fundamental idea of generalization (to- gether with classification and feature selection) is useful in the medical domain.

2 Intersection Based Generalization Rules  Let us consider an example with two sets of items, II = A B C ( s h o n f o r A A B A C ) a n d I ~  = B C D , e . g . A = ?green?, B = ?big?, C = ?4 doors?, and D = ?fast.? Ii and I2 describe the characteristics of two entities, e.g. two cars. What are the common characteristics of the two cars?

Both cars are big and have 4 doors. Only car 1 is big and car 2 is fast. In set theoretic language we have generated an intersection of Ii and 12: 1, n I2 = ABC n BCD = BC.

Thus, intersection theory is the natural access to rule generation. Of course, we can consider different classes for our itemsets. The sets of items need not to have the same number of elements. Hence, a missing item causes no problems when calculating intersections. Also we can reduce the number of attributes in one step much more than only one attribute. The intersection operator could be interpreted as a kind of recombination operator, known from evolutionary computing.

Example 2.1: Let us consider the 8 itemsets ABCD, AEFG, BEFG, CEFG, DEFG, BE,  CF.  and DG. In [4] the closure cl(1) of an item with regard to itemset I is defined as the  0-7695-1754-4102 $17.00 Q 2002 IEEE 673  mailto:paetz@cs.uni-frankfurt.de   intersection of all the itemsets that contain I. Thus, the clo- sure of A is cl(A) = .4BCD n AEFG = A. If cl(I) = I, then an itemset I is called closed. In our example A is closed. Using the algorithm [4] we have to compute all the closures of frequent itemsets, successively for itemsets of length 1; 2: 3,. . .Thus, the algorithm [4] computes as much candidates as a-priori. Using our intersection paradigma, building intersections directly from samples, we have a probability of 6/(7+6+5 + 4 +  3 + 2  + 1) = 6/28 FZ 0.21 for generating the closed, more interesting, itemset EFG directly. The probability for generating the closed, less interesting, item .4 is only 1/28, This example emphasizes that it is plausible and reliable to generate heuristically gen- eralizations instead of generating a-priori-like candidates.

In the following we denote a finite set of itemsets as ZS.

We will consider datasets where every sample is an item- set. Of course, identical samples may exist. We assume that identical samples are storaged only one time in the database together with the frequency for every class. We call an in- tersection K of two sets I, J nontrivial if K # I, K # J and K # 0. We use the performance measures ?frequency? and ?confidence? for rules I e. c, I E ZS, c a class la- bel, as introduced in [Z], and we write f r eq ( I  * c) resp.

m f ( I  * c ) .

2.1 Generalization Algorithm  Our aim is finding rules with high performance mea- sures, obtaining a good classification by the rules that we generate and a reliable force of expression of the rules for physicians. The following algorithm, that is based on the principle of intersections, uses the set I S  of all (training) itemsets as a starting point.

Algorithm 2.2 (Genlntersect) Input parameters: Set of itemsets ZS, maxlevel, yz (mini- mum thresholds for the performance measures) Output parameters: ZSF (generalized rules, including the initial rule itemset IS), level, startlevel.



I. initialization  := ZS; level := 1; startlevel(leve1) := 1; endofalg := false: 2. generate level while endofalg = false do  startlevel(level+ 1) := @Soew)+l; oldIS := flZS.,,; pass through actual level fori  = startlevel(level) to startlevel(level+l)-2  pass through itemsets without considering itemsets of the preceding levels  forj  = i + 1 to startlevel(level+l)-1 Inter := TSnev(i) n ZSnew(j); if Inter is a nontrivial intersection  and (Inter ZS,,,) then ZS,,,(#zS,., + 1) := Inter;  end if, end for j ,  end for i 3. check termination  if (#zS,,, = oldIS) or (level 2 maxlevel) then endofalg := m e ; I S F  := ISnew;  else  end end while 4. filter all the rules that have performance measures  level := level f l ;  higher then all 7;.

2.2 Heuristic Extensions  A disadvantage of the algorithm GenIntersect is the com- binatorial explosion. Monotonic frequency pruning is not possible because the frequency is not monotone with re- spect to the generalization process. Thus, we should use adaptive heuristics. This makes sense, because we do not need all the optimal rules for classification. It i s  sufficient to go on with generalization until not much more rules are generatedor until the classification result gets not much bet- ter within a level.

We introduce the outergeneralization index to determine online if another generalization level makes sense. Addi- tionally our inner generalizarion index determines within one level when an intersection process could be stopped, so that it can proceed with the next level.

The idea of the inner generalization index is the calcula- tion of a sliding mean, using the number of newly generated itemsets per itemset. If this sliding mean value M becomes too small, we stop the intersection process within the actual level and proceed with another level. As our experiments showed, this is very performant in combination with a maximal number muznew of itemsets that one itemset is allowed to generate.

Definition 2.3: We call the sliding mean M ,  mentioned above, the inner generalization index G(??) := M .

j  How many levels do we need? A heuristic answer is our outer generalization index.

Definition 2.4: Let m be the maximal possible number of new itemsets within one level and e the number of effectively generated new itemsets, that are not nontrivial and not already gener-     ated. Then, we define the nuter generalization index as  If G(O??) E [O; 11 falls under a predefined threshold, then the algorithm terminates. If the index is approx. 1, then the algorithm should go on with another level.

More properties (use of negotiations, optimality) of our algorithm can be found in [SI.

3 Classification  Now, we describe how we can classify a new (test) itemset ZS using a generalized itemset R (representing a rule basis). One problem always arising in this situation is the different a-priori probability, e.g. 80% class 1 samples and 20% class 2 samples. The confidence measure is related to the a-priori percentage and is therefore not suitable for a classifier. We introduce an elegant way to solve this problem using an extension of the confidence, the weighred confidence.

Definition 3.1: The weighted confidence wcoli f for class c could he de- fined using the confidence together with the class propor- tions or easier as the proportion of the c-frequency (=class c coverage # frequency!) considering all &frequencies:  The weighted confidence is ideal for a winner-takes-all classification. The best fitting rule with respect to the weighted confidence specifies the class. We call our algorithm ?WeCoCI?, abbr. for ?Weighted Confidence Classifier:? It is a classifier for itemsets with a multiple class alignment.

Algorithm 3.2: (WeCoCI) Parameters: ZS (all itemsets, that will be classified). R (with GenIntersect generated set of rule itemsets, filtered with the performance measures).

1. for all itemsets of the test itemsets calculate the sets of all ?containing? weighted cd-confidence values for all itemsets I E ZS do  for all classes d do WCONFd := 8; for all itemsets (rules) J E R do  if J c I then  end if, end for all J, end for all I WCONF,  = WCONFdU{d- ,~col l f ( J )} ;  2. calculate the maximal weighted d-confidence per class; these values are noted as muzWCONFd.

3. Be 7iluzi71d = i71dez(maxd{mu~L.WCONFd)) and  SiTldez = i 7 l d ~ z ( ~ l a X d , d 2 , , , i , d { 7 ~ l U z W C O N F d ] ) .

if mazW?CONF,,,;,d - ~ I L u x W C O N F . ~ , ~ , ~  > E  then classify I as class mnzind, otherwise we classify I as ?not classifiable.? This is always the case if no itemset of the rule set is contained in I.

end  4 Discussion: Generalization vs. Association  A-priori generates great many, superfluous rules. Let us consider the rules ABC, BCD, both of the same class e. GenIntersect generates only the additional rule BC.

A-priori considers the items .4: B: C and D of length I and then the combinations of length 2. .4D is an itemset, that is not contained in ABC or BCD. Frequent is the itemset BC, but also are the items B and C. We do not need to generate all frequent itemsets for classification. It leads to an effect, we call context smearing, resulting from the association process starting with the items, while the context is fixed by the itemsets.

Example 4.1: Let B be the item ?high blood pressure? and C ?pH value.? The important knowledge is the combinntion of both items BC. If we would choose only B or C as a classification rule, we claim that a high blood pressure or a low pH value are standalone interesting. This may be the case but i t  need not to be! If we present only B as a generalized rule to a physician, then he could conclude in a complete false way: he could think that giving a blood pressure lowering medicament is a benefit for the patient. although this medicament would again lower the pH value for the patient?s disadvantage, because he was not aware of the complete information BC. A longer generalization rule is more useful to a physician.

How senseful is a rule set for the classification of un- known data, i.e. how robust is a-priori resp. GenIntersect?

Example 4.2: Let us consider again the itemsets ABC und BCD (of the same class e). GenIntersect generates BC. an a-priori classifier would choose the rule B or C. Assume, the unknown itemset to classify is .4B. It is BC q! .4B, i.e.

AB would be classified as ?not classifiable.? In fact, AB could be of another class than BC, so that this decision is justified. But it is B c AB, i.e. A B  would he classified as class d, although A B  need not to he a class d itemset.

There is no reason at all to classify in such a way. If we choose C instead of B. which seems to be an equal choice, we have C @ .4B. Thus, in fact B is not an equal choice compared to C, if we expect unknown itemsets. the usual case in medicine due to individual patients.

I I mean result I  96.70141.57  Table 1. Results of the rule generation with Genlntersect and WeCoCI.

Shorter a-priori rule sets are not robust due to context smearing. We state that in the medical domain (e.g. rules for therapy planning) generalization rules are more reliable than association rules. Shorter association rules may even be harmful for patients!

5 Septic Shock Patient Data  Septic shock is of prime importance in intensive care medicine 111. Our database consists of 362 septic shock patients. The data of each patient was given as admission data (e.g. chronic diagnoses) and daily measurements (e.g.

acute diagnoses, medication and therapies).

We made three complete experiments with the patient data. The results were mean results of these experiments.

We used the heuristics of Sect. 2.2. Initially we used 50% of the data for training our system, the other 50% for calcu- lating the test results. Let us abbreviate the class "survived" by "class s" and the class "deceaed" by "class d."  We made a first run of our algorithm with all the 96 items. After filtering the rules by the frequency and con- fidence thresholds 23 tules remained. Calculating the im- portance of the items by  only 60 of the 96 items had an importance > 0 for one of the two classes. Only 27 items had an importance value that was noticeable > 0. With this 27 items we made three repetitions of the entire experiment. The mean results on the test data is shown in Tab. 1. The complete parameter and heuristics setting is documented in [51.

The NO items (e.g. not thrombocyte concentrate, not en- doscopy, not min. 3 antibiotics given) appeared more often for class s. the YES items (e.g. respiration, catecholamines given) more often for class d.

An example for a rule of class d is: "if not traumatic and number of organ failures > 2 and respiration and haemofiltration and not dialysis and catecholamines given then class d with wcmi f = 89.73% and frey = G.G3%."  A comparison of Genlntersect to a-priori can be found in (51. GenIntersect needs less memory (2MB instead of approx. 500MB). For class d a-priori was not reliable since for class d rules we need frequency thresholds less than 2%.

6 Conclusion  We presented an algorithm for generalizing itemsets. Us- ing heuristics, it is reliable to generalize even larger data sets. The algorithm is very useful if classification tasks are considered. We confirmed our inventions by application to important medical real world data, the septic shock ICU pa- tient data. A fundamental comparison to association rules was given. It showed that it does make sense to general- ize instead of associate due to the two big problems con- text smearing and robustness, two important circumstances in medical applications. The whole work is documented in greaterdetail in [5].

Work for the future could be the invention of even more suitable adaptive heuristics.

