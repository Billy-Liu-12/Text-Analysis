Smart Green Infrastructure for Innovation and  Transformation Hosting Environments

Abstract ? This paper provides an overview on the importance and evolution of Smart Green Infrastructure in Innovation and Transformation Hosting Environments. The paper describes how IBM, for its internal data center environments, has been integrating technology areas such as Mobile, Predictive Analytics, Cloud computing, big data such as Hadoop, Monitoring, and Agile environments. The future of Smart Green Infrastructure will depend on how dynamically an IT team can carve out the needs for implementing complex projects in a quick, easy, structured and dynamic environment while focusing in the innovation space using advanced technology. The paper describes how such Smart Green Infrastructures are managed and maintained to promote Green initiatives for Innovation while leveraging the use of predictive analytics in the environment.

Case studies are described that include discussions on challenges faced and lessons learned.

Keywords-component; green infrastructure; carbon footprint; mobile computing; cloud computing; big data; analytics; SSD, agile; green IT.



I. INTRODUCTION In the past, data centers were spread across different  regions and there were servers in the data centers which were mostly physical servers. With this setup, there was always a great need for space, power consumption and more resources.

Maintaining these servers thereby resulted in increased operating costs for data centers. To address these problems the Smart Green Infrastructure was envisioned. The main purpose of the Smart Green Infrastructure is to reduce the foot print of physical servers in the data center by adapting a more virtualized environment. This has resulted in significantly reduced maintenance costs and power consumption, along with much improved PUE (Power Usage Effectiveness) metrics. The scope of the Green infrastructure project is not limited to current projects requirements, but is built in a ?Smart? way such that it aligns with the technology road map areas planned by the IBM business and the organization for its data centers. In our environment we achieved this with a limited space in the following ways:  1. Identified all the legacy applications that were deployed on the physical environment and setup a migration plan to move to a more virtualized environment.

2. Reduced the server foot print by vertically and horizontally scaling the applications so that resources are effectively utilized.

3. Divided the data center into high density and low density thermal zones. The high density thermal zone hosts the IT equipment and servers that consume a lot of power and dissipate more heat.  The low density thermal zone hosts the IT equipment and servers that consumes less power and dissipate less heat. By having two thermal zones, we could identify which component goes where and how this could impact the overall infrastructure.

The smartness in Green infrastructure is not just limited to  the use of the latest technology and server components but it also defines how this infrastructure could serve different purposes by leveraging the use of new technology in Mobile, Predictive Analytics, Cloud computing, Big data (such as Hadoop), Monitoring, storage hardware, and Agile environments, with reduced server footprint.  For this paper we?ll follow this Outline:   1. Introduction 2. Evolution and Planning for Smart Green Infrastructure 3. The Significance of Virtualization and Cloud  Computing 4. Ways to promote Green Initiatives 5. Predictive Analytics, Mobile Access, Agile Methods,  and SSD 6. Private Cloud Case Studies 7. Conclusions

II. EVOLUTION AND PLANNING FOR SMART GREEN INFRASTRUCURE  A first step in the implementation of any new business process for energy efficiency, is to ?baseline? the energy use at your data centers.  A measurement and monitoring process will allow you to find out where you are and how you?re doing along the way.

Data centers consume much more energy per square foot than any other part of an office building. Figure 1 shows the distribution of space and energy costs at IBM buildings.

Although data centers are less than 5% of the space cost at IBM, they account for almost 50% of the energy cost.  A study at a major bank indicated that data centers accounted for just 1% of the bank?s physical infrastructure space but consume about 25% of the power. So, there is a great deal of incentive to create green data centers.

Figure 1.  IBM Distribution of Space and Energy Costs (Source: IBM Corporate Environmental Affairs, Year end 2010)  The process used by IBM for creating energy efficient ?green? data centers consists of five basic steps [3]. The five process steps are:  diagnose, manage & measure, use energy efficient cooling, virtualize, and build new or upgrade facilities when feasible.

A. Diagnose the Opportunies and Problems The step here is to do a data-center energy-efficiency  assessment.  Many data center energy consultants will bring in tools such as Mobile Measurement Technology (MMT) to look at ?hot spots? in the data center and areas where cold and hot air mix too early and thereby waste energy.  The assessment should include a list of unused IT equipment that can be turned off.  In addition, the diagnostic phase can help encourage organizations to retire unused software applications and focus on adopting more effective software that requires fewer CPU cycles.  A typical x86 server consumes between 30% and 40% of its maximum power when idle. IT organizations should turn off servers that don't appear to be performing tasks.  If anyone complains, organizations should look into whether the little- used application can be virtualized.  Check with your electric utility.  Some utilities offer free energy audits.

B. Manage and Measure Many hardware products have built-in power management  features that are never used. Most major vendors have been implementing such features for quite some time.  These features include the ability of the CPU to optimize power by dynamically switching among multiple performance states. The CPU will drop its input voltage and frequency based on how many instructions are being run on the chip itself. These types of features can save organizations up to 20% on server power consumption.

C. Cool Many data centers may use hot aisle/cold aisle  configurations to improve cooling efficiency, but there are also some small adjustments they can make. Simple "blanking panels" can be installed in server racks that have empty slots.

That's a great way to make sure the cold air in the cold aisle doesn't start mixing with the hot air in the hot aisle any sooner than it needs to. Organizations should also seal cable cutouts to minimize airflow bypasses. Data organizations should consider air handlers and chillers that use efficient technologies such as variable frequency drives which adjust how fast the air conditioning system's motors run when cooling needs dip.

D. Virtualize Virtualization continues to be one of the hottest green data  center topics. Many current server CPU utilization rates typically hover between 5% and 15%. Direct-attached storage utilization sits between 20% and 40%, with network storage between 60% and 80%. Virtualization can increase hardware utilization by five to 20 times and allows organizations to reduce the number of power-consuming servers.  Cloud computing is the ?ultimate? in virtualization and we?ll discuss virtualization and cloud computing in more detail later in this paper.

E. Build Going Green is easiest if you are building a new data  center. First, you make a calculation of your compute requirements for the foreseeable future. Next, you plan a data center for modularity in both its IT elements and its power and cooling. Then you use data center modeling and thermal assessment tools and software ? available from vendors such as APC, IBM, HP and Sun ? to design the data center. The next step is to procure Green from the beginning ? which partly means, buy the latest equipment and technologies such as blade servers and virtualization.

Once you have the equipment, you integrate it into high density modular compute racks, virtualize servers and storage, put in consolidated power supply, choose from a range of modern cooling solutions and, finally, run, monitor and manage the data center dynamics using sensors that feed real-time compute, power and cooling data into modern single-view management software that dynamically allocates resources.  An overview of the energy efficiency strategy used by IBM for green data centers is shown in Figure 2.

Figure 2.  An Energy Efficiency Strategy for IBM Smart Green Infrastructures

III. THE SIGNIFICANCE OF VIRTUALIZATION AND CLOUD COMPUTING FOR SMART GREEN INFRASTRUCTURE  The most-significant step most organizations can make in moving to green data centers is to implement virtualization for their Information Technology (IT) data center devices. The IT devices include servers, data storage, and clients (or desktops) used to support the data center.  There is also a virtual IT world of the future - via private cloud computing - for most of our data centers.  Although the use of cloud computing in your company?s data center for mainstream computing may be off in the future, some steps towards private cloud computing for mainstream computing within your company are currently available.  Server clusters are here now and being used in many company data centers. Although cost reduction usually drives the path to virtualization, often the most important reason to use virtualization is IT flexibility.  The cost and energy savings due to consolidating hardware and software are very significant benefits and nicely complement the flexibility benefits.  The use of virtualization technologies is usually the first and most important step we can take in creating energy efficient and green data centers.

A. Reasons for Creating Virtual Servers Consider this basic scenario.  You?re in charge of procuring  additional server capacity at your company?s data center.  You have two identical servers, each running different Windows applications for your company.  The first server ? let?s call it Server A ? is lightly used, reaching a peak of only 5% of its CPU capacity and using only 5% of its internal hard disk.  The second server ? let?s call it Server B ? is using all of its CPU (averaging 95% CPU utilization) and has basically run out of hard disk capacity (i.e. the hard disk is 95% full).  So you have a real problem with Server B.  However, if you consider Server A and Server B together, on average the combined servers are using only 50% of their CPU capacity and 50% of their hard disk capacity.  If the two servers were actually virtual servers on a large physical server, the problem would be immediately solved since each server could be quickly allocated the  resource each needs.  In newer virtual server technologies - e.g.

Unix Logical Partitions (LPARS) with micro-partitioning ? each virtual server can dynamically (instantaneously) increase the number of CPUs available by utilizing the CPUs currently not in use by other virtual servers on the large physical machine.  The idea is that each virtual server gets the resource required based on the virtual server?s immediate need.

My favorite diagram for describing the reasons to use server and data storage virtualization is shown in Figure 3.

This diagram shows typical server utilization for stand-alone servers (i.e. NO virtualization).  The multi-million dollar mainframes are typically utilized on a 24/7 basis at least partly because of the large financial investment.  Mainframe ?batch? processes such as running daily, weekly, and monthly corporate summary reports are typically CPU intensive and are run at night and on the weekends.  The small department Windows server (labeled ?Intel-based? in the diagram) is not typically used at night or on the weekends.  Creating virtual servers of those Intel-based servers not only allows much better and easier sharing of resources for a mix of lightly and heavily used servers (as in the Server A/Server B example above) but also tends to spread out the utilization over 24 hours on the large physical server that houses the virtual servers.

Server Virtualization - the Reason  Current Asset Utilization (Stand-Alone Servers)   ?52% ?N/A ?N/A ?Storage  ?2-5% ?5-10% ?30% ?Intel-based  ?<10% ?10-15% ?50-70% ?UNIX  ?60% ?70% ?85-100% ?Mainframes   24-hour Period  Utilization   Prime-shift Utilization   Peak-hour Utilization  Source: IBM Scorpion White Paper: Simplifying the Corporate IT Infrastructure, 2000 Figure 3.  Server Virtualization ? The Reason.

B. Cloud Computing ? The Ultimate in Virtualization Cloud computing is a new (circa late 2007) label for the  subset of grid computing that includes utility computing and other approaches to the use of shared computing resources.

Cloud computing is an alternative to having local servers or personal devices handling users' applications. Essentially, it is an idea that the technological capabilities should "hover" over everything and be available whenever a user wants. Although the early publicity on cloud computing was for public offerings over the public Internet by companies such as Amazon and Google, private cloud computing is starting to come of age.  A private cloud is a smaller cloudlike IT system within a corporate firewall that offers shared services to a closed internal network.  Consumers of such a cloud would include the employees across various divisions and departments, business partners, suppliers, resellers, and other organizations.

Shared services on the infrastructure side such as computing power or data storage services or on the application side such    as a single customer information application shared across the organization are suitable candidates for such an approach. Of course IT virtualization would be the basis of the infrastructure design for the shared services and this will help drive energy efficiency for our green data centers of the future.

Because a private cloud is exclusive in nature and limited in access to a set of participants, it has inherent strengths with respect to security and control over data.  Also, the private cloud approach can provide advantages with respect to adherence to corporate and regulatory compliance guidelines.

These considerations for a private cloud are very significant for most large organizations.



IV. WAYS TO PROMOTE GREEN INITIATIVES "If You Can?t Measure It, You Can?t Manage It?     (Quote  attributed to many, including W. E. Deming and Peter Drucker). In order to measure green IT, we must have standards as to what constitutes green IT.  The standards are IT energy-use metrics.  In addition to measurements for IT energy use, there are also the measurements for green buildings, such as the Leadership in Energy and Environmental Design (LEED) Green Building Rating System, developed by the U.S.

Green Building Council (USGBC).  LEED provides a suite of standards for environmentally sustainable construction. LEED does not relate directly to green data centers, but rather to the overall building.

Another metric for green IT is EPEAT, the Electronic Product Environmental Assessment Tool.  EPEAT was created through an Institute of Electrical and Electronics Engineers (IEEE) council because companies and government agencies wanted to put green criteria in IT requests for proposals.

EPEAT got a big boost in January 2007 when President Bush signed an executive order requiring that 95% of electronic products procured by federal agencies meet EPEAT standards, as long there's a standard for that product. First, however, we?ll look at some standards and metrics specifically developed for measuring IT and data center energy efficiency, such as SPEC marks and the metrics being developed by the EPA. The EPA is pushing for metrics for all aspects of data center use and the EPA metrics should be the guideline for green data centers.

The Standard Performance Evaluation Corp. (SPEC) benchmark information has been used for years to compare servers from a power aspect. In the author?s experience, companies are very interested in SPEC marks when comparing new servers. Customers use the SPEC marks in order to compare relative power.  Here's the SPEC homepage: http://www.spec.org/  In 2008 SPEC started a benchmark to compare the power consumed by a server with its performance - a metric designed to aid users in boosting data center efficiency.

A. Energy-Use Dashboards at Montpelier, France The demo data center in Montpelier, France is called the  PSSC (Products and Solutions Support Center) Green Data Center of the Future [5]. The main idea was to create a customer friendly real-time green showcase production data center that will demonstrate a large percentage of the currently  available best practices in IT and facilities energy conservation, integrating at least one bleeding-edge major conservation technology. The information on the use of a live camera, thermal camera, and green IT energy use real-time dashboards available to IT personnel through a portal are interesting innovations that can help communicate the energy efficiency of the data center to all interested employees.

The Power Usage Effectiveness (PUE) is the metric used to measure the energy efficiency of a data center. Both IT and non-IT resources? energy consumption are gathered. Two PUEs are measured: overall and high-density zone.

For demo purposes you can show in real time the PUE of the PSSC Green Data Center.  Although the PSSC green data center is for demo purposes, these energy use dashboards would be useful for any data center to raise company awareness on IT energy efficiency. These dashboards remind me of the real time gas mileage graphics on the Toyota Prius automobile I recently rented. The graphics on the Prius? dashboard constantly reminded me that I was getting around 42 miles per gallon and informed me when the car was using the electric motor and when it was using the gas engine (and charging the battery).  The data center energy use dashboard for the Montpelier demo center is shown in Figure 4.

Figure 4.  Montpelier PSSC Green Data Center Energy Use Dashboard (Source: IBM Montpelier PSSC Green Data Center Team)  B. Smartbank ? Smart Green Infrastructure Monitoring at the Solution Level Monitoring at can be done at the Solution Level.

Smartbank is a live showcase involving many platforms. For demo purpose, the monitoring of an entire solution can be demonstrated.  Active Energy Manager is implemented for the energy management of z10 and Blades.  The Smartbank solution level monitoring is shown in Figure 5. As stated for the previous dashboard, these data center energy dashboards can communicate current energy efficiency to interested IT team members just as the hybrid automobiles constantly remind the driver with dashboard gas mileage graphics. The real-time dashboard can give the IT team immediate feedback on how data center energy management techniques impact    data center energy use just as the hybrid automobile gas mileage dashboard gave me immediate feedback on how my driving technique impacted automobile energy use.

Figure 5.  Montpelier Green Data Center Smartbank - Solution Level Monitoring (Source: IBM Montpelier PSSC Green Data Center Team)  Innovation at the PSSC data center is summarized in the following list.

? Web Camera showing the implemented technologies.

? A solution to track people and assets.

? Dynamic and graphic visualization of the  temperature, pressure and humidity in the Green Data Center.

? A thermal camera to visualize hot spots at the server and rack room levels.

? TEPS (Tivoli Enterprise Portal Server) workplaces are used in order to show the monitoring and management of IT and non-IT resources.

Now, we?ll look at the significant advantages of technologies such as analytics, mobile access and cloud computing for the smart green infrastructure.



V. PREDICTIVE ANALYTICS, MOBILE ACCESS, AGILE DEV METHODS, AND SSD FOR THE SMART GREEN INFRASTRUCTURE  This section discusses the energy efficiency properties of new technologies such as predictive analytics, mobile access, agile development methods, and SSD.  These technologies area all part of the Smart Green Infrastructure strategy.

? SSDs (Solid State Drives) have no moving parts and don?t get hot, which means they are quieter and don?t require much cooling. They also use less space, which impacts how much data center real estate is needed. The Storage Networking Industries Association (SNIA) estimates that 13 percent of the total power consumption in a data center comes from storage. So SSDs can be a significant contributor to a green data center.

? Agile development methods can greatly increase application development efficiency and save energy.

? Mobile access and predictive analytics (including big data and Hadoop technologies) not only increase flexibility in IT but also contribute to overall IT energy efficiency [4].



VI. PRIVATE CLOUD CASE STUDIES FOR THE SMART GREEN INFRASTRUCTUE  This section describes the significant advantages of using a cloud for test and development environments.

A. Computing and Private vs. Public Clouds Cloud computing [1, 2] is a style of IT delivery in which  applications, data, and IT resources are rapidly provisioned and provided as standardized offerings to users over the web in a flexible pricing model. The resources are provisioned ?on demand?.

A cloud computing platform dynamically provisions, configures, reconfigures, and deprovisions servers as needed.

Servers in the cloud can be physical machines or virtual machines. Clouds can also manage other computing resources such as storage area networks (SANs) and network equipment.

A public cloud (external cloud) describes cloud computing in the traditional main-stream sense, whereby resources are dynamically provisioned on a fine-grained, self-service basis over the Internet.

Private clouds describe offerings that emulate cloud computing on private networks. These (typically virtualization automation) products offer the ability to host applications or virtual machines in a company's own set of hosts. Enhanced security is a major benefit provided by private clouds.

B. Cloud Considerations Here is information from IBM?s Systems Engineering  Architecture and Test (SEA&T) team members on how cloud helps improve testing efficiency:  ? For performance testing, our teams can test in a wide variety of test environments, ranging from a development environment, to an integration test environment, to a customer hosted environment.

? If testing occurs in an integration test environment, the normal process involves engaging the IBM Test team on a planning/technical call, then to determine when the environment would be needed and built, and also what kind of server HW/SW (hardware/software) and configuration is needed (number of processors, memory needed, storage needed, software needed, etc., present significant challenges without use of cloud technology).

One of the primary challenges is to test on a production like environment to ensure that test results are predictive of production performance.

? Depending on schedule and resources available, the time for provisioning and setup of servers could take anywhere from several days to several weeks (resolving schedule challenges is a significant benefit of moving to cloud)    ? Depending on the number of servers requested, costs can range from several hundred to several thousands of dollars per month (resolving cost challenges is a significant benefit of moving to cloud)  ? Once servers are provisioned and assigned to the project, additional time is needed for application install. This also presents a possible set of challenges.  Normally, application teams do not have direct access to the servers and must provide install files and instructions.  Cloud technology can be used to automate this process.

? Breakdown and reuse of servers - Normally, after a project completes, servers are "broken down" and re- provisioned for use by other projects or applications.  A cloud automates this breakdown and reuse process.

C. Example Case Study: Private Test/Dev Cloud for Cell Phone Company This case study is based on the author?s experience  working with a cell phone company in South Africa on ways to improve efficiency of their IT environment. At that time, the cell phone company was experiencing very typical concerns for large companies with their test and development environments.  The concerns included:  ? Test environment resource availability.  This was for both hardware and people resource, especially in an end to end testing environment.

? There was no good scheduling process for shared resources.

? There was significant amounts of test server waste due to lack of governance and management ? for example every project gets servers for testing and the servers were often not released when testing is completed.

A detailed spreadsheet was developed to determine the  projected financial benefit from the use of a private cloud. The spreadsheet indicated projected server and labor resource savings due to use of a private cloud for test and development servers.  The projections indicated that when 280 application test/dev environments were moved to a private cloud, the number of servers could be reduced by 62%.  The time (days) to provision the test environments would be reduced from 33 days to 5.4 days and the number of test team members needed to provision the test environments would go from 16 team members to 2 team members.

The cloud estimations spreadsheet described above proved  to be a good way to present savings due to the use of cloud for test/dev.  Much of the projected savings was due to reduced labor (i.e. testers).  Overall, a private cloud provides a way to greatly improve test and development environments for:  1) Speed in setting up test environments 2) Saving hardware, software, and labor for testing.

The cloud is an excellent environment for test and development environments.  The trend to use cloud first for test and development environments is a worldwide phenomenon.  Cloud allows for automation, efficiency, and energy savings [1, 3]. Because of the dynamic aspect of IT, we have already made significant progress with cloud computing and can build on that past.

For success with cloud test and development environments,  strong governance, scheduling and management of the systems are required.



VII. CONCLUSIONS The discussion on the Smart Green Infrastructure in this  paper focused on the importance and techniques that can be used to significantly improve energy efficiency at data centers and at the same time automate the process. Cloud Technology is key for both energy efficiency and automation. All of our studies and implementation have indicated the great benefits of cloud technology for test and development environments.

Innovation at data centers for energy efficiency and  automation has been especially active within IBM at the PSSC demo data centers around the world and at the IBM CIO Lab?s Innovation data centers. The innovations include measuring the energy efficiency and automation improvements with the use of mobile, analytics, cloud computing, and smart monitoring for the applications on an agile platform.

Going forward, we all have a role in data center and IT  energy and cost efficiency and can help save the planet!

