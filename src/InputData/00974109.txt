RUL  SET THE WITH GEN

Abstract  A methodology for using Rough Set for preference modeling in decision problem is presented in this paper; where we will introduce a new approach for deriving knowledge rules from medical database based on Rough Set combined with Genetic Programming. Genetic Programming belongs to the most newly techniques in applications of Artificial Intelligence. Rough Set Theory, which emerged about twenty years ago, is nowadays rapidly developing branch of Artificial Intelligence and soft computing. At the first glance the two methodologies we talk about have not in common. Rough Set constructs representation of knowledge in terms of attributes, semantic decision rules, etc. On the contradictory, Genetic Programming attempts to automatically create computer programs from a high-level statement of the problem requirements. But, in spite of these differences, it is interesting to try to incorporate both approaches into combined system. The challenge is to get as much as possible from this association.

1. Introduction Knowledge discovery in database called data mining, aims at the disco information from large collection of data /1,2].

Rough set methodology for knowledge discovery provides a powerful tool for knowledge discovery from large and incomplete data. A number of algorithms and systems have been developed based on the rough set theory (for example see [3,4,5,6,7,8,9,10,11,12,13,2]), which may induce a set of decision rules from a given data, and may use induced rules to classify future examples. Most of them are attempting to find and select the best minimal set of rules, the use only minimal subset of attributes from the given data.

The advantage of employing various sources of knowledge and various structures of knowledge in data mining and knowledge discovery implies that new algorithmic method are desirable for hybrid systems in which rough set will be applied along with methods based on the following: Fuzzy Set; Neural Nets; Genetic Algorithms; etc. [8,2]. The main reason for combining different techniques in the hybrid systems is that a single technique is often not appropriate for every domain or dataset.

Another reason is that such a hybrid system has an advantage over a single method approach because the technologies complement each other's shortcomings. Here we will present a new approach to task of incorporating rough set and genetic programming methods into one system for decision or classification support. We want not only to be determining the outcomes of new knowledge based on data, but also we are interested in analyzing the  structure of the model to gain new insight into the problem at hand. By model structure we mean the type of the model's different components, how they related to each other, and how they can be interpreted. Finally we will describe application of our approach for mining decision rules on medical data.

2. Genetic Rouvh Induction (GlU) 2.1. Rough Set Theorv Rough set concept, which introduced by Pawlak [14], and one of its essential merits is its direct relation to classification problems [ 151, is founded on the assumption that each object is associated with some information (data, knowledge). Objects that characterized by the same information are said to be indiscernible in the view of available data.

This induces the indiscernibly relation which is the mathematical base of rough set theory.

Information system IS=(U,C) is used for representing knowledge, where U is non-empty finite set of objects called universe, and C is non- empty finite set of attributes. Decision table A=(U, Cu{d)) is a special case of information system introduced in rough set theory as tool to present data, where C is called condition attributes, and dE C is called decision attribute. Let v, be the value set for attribute c, then the attribute c can be considered as a map C:U+~, i.e. c(x)=vc,k E C .

Let X c U  be a set of objects and BcC be a set of attributes, the indiscernibly relation is defined as:  I (B) = { (x, y) E U x U: c(x) = c(y), VCE B).

Objects x, y satisfying the relation I(B) are indiscernible by attributes from B. An order pair AS=(U,I(B)) is called an approximation space.

Seventh Australian and New Zealand Intelligent Informa~on Systems Conference, 18-21 November 2001, Perth, Western Australia    According to I(B), we can define two crisp sets - B X and B X called lower and upper approximation of the set of objects X in the approximation space AS as:  B X ={x E U: IB(x) G X}, and  -  - B x ={x E U: IB(X) n x # $1.

& X consists of all objects that can be with certainty classified as elemenfs of set X, and B X consists of all objects that can be possibly classified as elements of set X. The difference BNB(X)=( B X- @ X) is called boundary of X, which contains all objects that cannot be classified either to X or complement of X.

The indiscernibly relation I(B) partitions the set U into number of disjoint equivalent classes denote by U/I(B)={X1, X2, ..., Xn), where X,+XJ for ever  -  -  i=l  Decision rules can which represent r  is a decision table and V=u{ V,:CE c)Ltvd, is a set of values for attributes, then the decision rule is a logical form:  There exist several measure evaluate the decision rule [7,8,2,12]. The classification accuracy and coverage of rule r are defined as follows:  IF (CI= V I )  &.. .& (cn= VJ  The simple strength of set of rules is defined as:  for ut E U is a tested object, Rul (X,) is the set of all rules for decision class X,, and MRul (X,,u,) G Rul (X,) is a set of rules that matching tested object for decision class X,.

2.2. Genetic Rough Induction (GRI) Genetic programming [16,17,18,19] is one of several problem solving methods based on analogy computation to natural evolution under title evolutionary computations, developed by John Koza, which automatically creates a computer  program from a high-level statement of problem's requirements. Since Genetic programming depends on tree-structure representation, we use C4.5 algorithm [20] to convert the data from decision table into tree-structure.

For our further considerations let us assume that we are given a data in the form of decision table A=(U, Cu{d}), by running C4.5 to construct number of trees we convert the data into tree representation.

Each tree T over the decision table A consists of terminal nodes (classes of decision attribute), non- terminal nodes (the set of attributes C) ,  and edges (the attribute values V).

The complete path [2] in the tree T is a sequence s -0, 4, .. ., v,, d,, v,+l, where vo is the root of tree, v,+l is the terminal one, 4. . .d, are the edges. If v, is the initial, then v,+l is the terminal of edge d,, i=O,.. .., m. Let h(s) = m be defined as the length of path s. if PATH(T) is the set of all complete paths in the tree T, then h(T)=max{h(s):sE PATH(T)} is called the depth of tree T.

A decision rule r is associated with a complete path s over the tree T which is denoted by r = rule(s).

The set of paths PATH(T) gives us a set of rules S, where each rule rE S is associated with each path s EPATH(T). Sometimes the rules derived from some paths may have a high error rate, or may duplicate rules derived from other paths, so the algorithm usually yields fewer rules than the number of paths in the tree.

Let ST(A) be the set of all trees over a decision table A that are constructed by run C4.5 number of times. The set of trees ST(A) represents a population in genetic programming concept, and each tree is an individual from this population. The number of trees in ST(A) is called the population size M. STo(A) is the set of trees over A in generation 0 or initial population, and so ST,(A) is the population at generation i. The terminal nodes in each tree from ST(A) are assigned values from vd, so we can say that the classes of decision attribute give here a set called terminal set in genetic programming. In the same manner the fimction set is the set of attributes C where each attribute is a function c(x). By applying genetic operators, we build a new population from old ones.

We will define three types of genetic operators: crossover operator, mutation operator, and reproduction operator.

Crossover operator [18] is a mapping C:ST(A)' + ST(A)2, i.e. C(T,, T 2 )  =(TI ' ,  T i )  , where T,,T2 E ST,(A) are called parents and TI', Ti  E ST,+,(A) are called offspring. It operates on two parental trees and creates two new two-offspring consisting of parts of each parent. The offspring are inserted into the new population at the next generation ST,+,(A). These offspring trees are typically of different sizes and shapes than their parents.

Seventh Australian and New Zealand Intelligent Information Systems Conference, 18-21 November 2001, Perth, Western Australia    Mutation operator is a mapping M:ST(A) -ST(A).

It generates a unique offspring tree T '  from exist tree T, where M(T)= T '  for TE ST,(A) and T '  E- ST,+I(A). It operates on one parental tree and creates one new offspring to be inserted into the new population at the next generation.

Reproduction operator is a mapping R:ST(A) + ST(A), where it selects one individual T and makes a copy of the tree for inclusion in the next generation of the population. I.e. R(T)=T, for T E ST(A).

To evaluate each individual in the population, we covert the tree into set of rules and evaluate it. Let define the significant of the set of rules p as:  (4) _--------- t=l P =  n where strength(X,,uJ is defined in formula (3), but here since we only use training cases to generate rules and testing cases to measure the efficient of the method (see the experiments section), so we select ut? U and make summation over t to all cases in the training examples where n is the number of cases. Since we divide by number of cases n, so 0 <p SI. The value p can be called the fitness value of this set of rules that derived from tree T, so each individual has a value p called the fitness value of this individual (i.e. is the fitness value of individual i). Depend on the fitness value the genetic operators select the individuals to be processed (the better the fitness, the more likely the individual is to be selected).

The task is to find all rules such that the significant of set of rules is at least a threshold. This is the maximum significant that is found in all previous generations, and there is a chance that offspring are fitter than parents. Where there is no predefined limit for combinations of attributes in the left-hand side of rule, and the right-hand side is not fixed, either; this is important so that unexpected rules are not ruled out before the processing start.

Sometimes the combination of two unimportant attributes may result in a very good model, or may two attributes that are found to be important may depend on each other and combining them would not add anything. And here the search space of the rules has exponential size in the number of attributes.

We mainly achieve two points beside the strength of the set of rules in the fitness measure: The number of rules, and average rule length. This depends on the data we use, so we will define the fitness function f, as: f, = al(strength of rules p) + a*(no. of rules) + a3 (l/average rule length) +&(Uno. of rules), where a],  q, q, and CL, are parameters and by controlling these parameters we can control which rules we  will get (e.g. a2 and (yqcontrol what we need: large or few number of rules as a result).

2.3. The algorithm In this subsection we present a classification algorithm based on the techniques described in the previous subsection.

ln~& Decision Table.

Output: Set of rules.

Process: Stepl: Read the decision table; determine the upper and lower approximation for each class in the decision table, determine the thresholds h for classification accuracy and 8 for the coverage.

Step2: Generate number of trees = M (the population size) by using C4.5 and running it M times. In each run of 04.5 method we change the probability of pruning for tree to get differ one. In the end of run C4.5 method, we store all trees as initial population.

Step3: lteratively perform the following sub-steps until reach the maximum of generation: a. Evaluate each individual in the population by  the following: i. Convert the tree into set of rules (each  rule is associated with a complete path).

ii. Remove the duplicated rules.

iii. Compute the classification accuracy and rule coverage for each rule, and remove the rule if its accuracy or coverage less than the thresholds h and 8.

Use the formula (4) as fitness measure to assign value for each set of rules.

b. Create a new population by applying the following operations: i. Reproduce an existing individual  (selected based on its fitness) and copy it into the new population.

ii. Create two new individuals from two existing individuals by genetically recombining randomly chosen parts of two existing trees using crossover operation.

Create new individual from existing one by randomly mutating a randomly chosen part of selected tree using mutation operation.

Step 4: The best-so-far individual is designated as the result of run (i.e. the set of rules).

3. Experiments To verify the usefulness of the presented methodology, a computational experiment has been performed. We report results of experiments on the Medical data (Meningitis dataset). This data was colleted at the Medical Research Institute, Tokyo Medical and Dental University. It has 140 cases; each of which is described by 38 attributes: 19 numerical and 19 categorical. The attribute  iv.

iii.

3 87 Seventh Australian and New Zealand Intelligent Information Systems Conference, 18-21 November 2001, Perth, Western Australia    descriptions exist in Table 1. Some instances of the data are with missing values. V e  divided this data into 121 training cases and 19 testing cases. The problem is to find factors important for three decisions: Diagnosis (DIAG), Detection of bacteria or virus (CULT-FIND), and Prediction prognosis (COURSE).

The dataset was divided into a training set and testing set, and the computations of rules have been done only to training data. The results of computations of rules were applied to the classification the objects from the testing dataset.

To evaluate the classification process, we use a measure called classifier?s error rate [8], which is defined by the ratio of the number of error to the number of all cases. We will compare the results of our method with that standard rough set me set, we use ROSETTA software to induce the rules from the dataset. In the classification process we use the same technique as in C4.5 that the rules are ordered and the first rule that covers a case is taken as the operative one. The default rule, rule without conditions that is put in the end of the list of rules, comes into play when no other rule covers a case.

The algorithm requires a set of parameters that have to be manually specified and may have considerable impact on the performance of the algorithm. Furthermore, it is desirable to repeat the same process with different sets of parameters. We give in details the best set of parameters that we found through the run of our method.

I I -, Neisseria, Strepto, Staphylo,  Memory-loss.

N, LC, Bechet, Sinusitis, COURSE N, P.

Tb, Influenza, Measles, Pi (El), I Varicella. Rubella, Adeno. 31 (CULTURE   DAR.4-P, ABPC+FMOX,  Amncsia, Headche, Ataxia,  DM, Hepatits, TB.

N, P. RISK (Grouped)  37 I RISK 1 Broncho, Myeloma, LC-DM, I  Decision attribute Diagnosis (DIAG={Bacteria, Virus)) We use 33 attributes (Personal Information, Present History, Physical Examination, Laboratory Examination, and Therapy and Course), and take only group attributes (e.g. we take attribute COURSE and delete attribute C-COURSE see Table 1). The best set of rules is obtained in generation 2 (Table 3) with number of rules fewer and shorter average rule length than which is obtained from C4.5 or standard rough set method.

The error rate of the set of rules that is obtained from our method is the same as that is obtained from C4.5 method but the rules are obtained from standard rough set method have very high error rate with this data. The fitness function here depends on parameters al=0.9, q=O.l, a3 =0, and &=O. In another run for our method we get from generation 58 the best set of rules, where it has also the same error rate = 0.00, but number of rules is 4.00 and average rule length is 1.00 (see third row of Table 3). Table 2 shows the run parameters that are used in our experiments where we mainly depend on crossover operator (probability rate is 0.8). Two sets of rules that are obtained from our method and C4.5 method are showed in Table 4.

Table 2: Run parameters for medical data Decision Diagnosis and Prediction)  Max generation Po dation size Max de th for trees Crossover rate 0.8  Seventh Australian and New Zealand Intelligent Information Systems Conference, 18-21 November 2001, Perth, Western Australia    Mutation rate  Error I ##rules 1 Average rule size 1 rate 1  0.1  I c4.5 I 4 I 1.75 I 0.00% I  Reproduction rate  0.00 Yo 0.00 Yo  2.00 78.9 % Rough Set  0.1  Table 4: Sets of rules are resulted from experiments  ~  with medical data ( GRI  CELL-POLY >220 -+ Class BACTERIA CT-FIND = Abnormal CELL-MONO <= 1 2 -+ Class BACTERIA CELL-MONO > 1 2  VIRUS -+ Class  # rules  Default Class: VIRUS Decision attribute Detec  Error rate Average rule size  :cision Diagnosis).

C4.5 method  CELL-POLY >220 -+ Class BACTERIA CT-FIND =Abnormal CELL-MONO <= 1 2  -3 Class BACTERIA CELL_POLY<=220 CELL-MONO >12  VIRUS CT-FIND = Normal CELL-POLY <=220  VIRUS Default Class: VIRUS on (CULT FIND={T,  -+ Class  -+ Class  #I rules Error rate Average rule size  I a . 5  I 1 I 6 [ 26.3% [ GRI I I I I J  c4.5  GRI Rough Set  medical data (decision attribute Detection).

I GRI I c4.5  5 2.40 21.1 Yo  92 11.00 63.2 Yo  AGE<=46 COLD <=7 NAUSEA <=9 LOC <=o ONEST = ACUTE BT >36.1  GCS <=lo  COLD >7 COLD <=9  ONEST=CHRONIC  AGE >37 COLD >7  Default Class: F )ecision attribute Predic  -+ Class F -+ Class F  -+ Class T -+ Class T  -+ Class T  AGE<=46 COLD <=7 NAUSEA <=9 LOC <=o ONEST = ACUTE BT >36.1 -+ Class F  Default Class: F ion (COURSE =In. D})  We use with this data 16 attributes (Personal Information, Present History, Physical Examination) and we ignore Laboratory Examination. The rules resulted from our run with new method and C4.5 method are showed in Table 9. The best set of rules is obtained in generation 4 (Table 8) with number of rules, average rule length, and error rate is same as C4.5 method, but better than standard rough set method, where from Table 9 we observe that the rules obtained from our method differ than that are obtained from C4.5 method except only one rule is the same. The parameters for run are showed in Table 2 as in decision attribute DIAGNOSIS.

~ rr; 7: Parameters for run in mi:alii , Decision attribute Detection).

Max eneration Po ulation size Max de th for trees Crossover rate Mutation rate  5.26% 5.26 Yo  14.00 73.7 % Rough Set Table 9: Rules resulted from experiments with  FOCAL =+ FEVER <=8  SEX =M FOCAL =- AGE >62 -3 Class N  3 89 Seventh Australian and New Zealand Intelligent Information Systems Conference, 18-21 November 2001, Perth, Western Australia    -3 Class P SEX =M AGE >62  LOC >2 BT <= 38.6  Default Class: N  -+ ClassP  -+ Class P  ~  LOC >2 STIFF = 2  AGE <=62 FEVER <=2 1  -+ Class P  FOCAL - -+ ClassN Default Class: N Legarding the application we introduced, our  approach achieves a more accuracy than heuristics of C4.5 and standard rough set theory. But C4.5 is faster, however the execution time was not a major tasks involved in its utilization. These results are suggesting that our method can be treated as a promising tool for extracting laws from experimental datasets and its performance is fully comparable with the performance of other classification systems.

4. Conclusions On the basis set theory, we have presente ich provides an efficient and for knowledge discovery in database system. In the hybrid framework, rough set theory and genetic programming are integrated into hybrid system and used cooperatively to generate a-set of rules from database. We believe that, successful research requires good co-operation between theoreticians and practitioners, so we presented in this paper the analysis of structure of the model for our new method and compared standard method for extracting laws from decision table based on rough set theory and that based on C4.5 algorithm with our new method on medical dataset. We observe that the rules extracted by our new method are relatively better predisposed in classification than both C4.5 and standard rough set approaches.

Reference [l] Mannila, H., Inductive Database and Condensed Representations for Data Mining, processing of International Logic Programming Symposium 1997, pp 21-30.

[2] Ziarako, W., Ed. Rough Sets, Fuzzy Sets, and Knowledge Discovery, Springer Verlag, Berlin, 1994.

[3] Hassan, Y., and Tazaki, E., Knowledge Discovery ?Using Rough Set Combined with Genetic Programming, Accepted from JSAI International Workshop on Rough Set Theory and Granular Computing, 2001.

[4] Dong, J.Z., Zhong, N., and Ohsuga, S., Probabilistic Rough Induction, Yamakawa, T., and Matsumoto, G. (edt.) Methodologies for the Conception, Design and application of SoR Computing and Information/Intelligent Systems (LISUKA?98), World Scientific, 1998,pp 943-946.

[5] Dong, J.Z., Zhong, N., and Ohsuga, S., Rule Discovery by Probabilistic Rough Induction,  Japanese Soc. Artificial Intelligence, 2000, pp 274- 286.

[6] Garcia, A., and Shasha, D., Using Rough Sets to Order Questions Leading to Database Queries, In Nagib C. Callaos (ed.), Proceedings of the Analysis and Synthesis (ISAS?96), July 22-26,  [7] Nakayama, H., Hattori, Y., and Ishii, R., Rule Extraction based on Rough Set Theory and its Application to Medical data Analysis, IEEE SMC?99 Conference Proceedings, 1999.

[8] Polkowski, L., and Showron, A., Rough sets in Knowledge Discovery, Physica Nerlag, 1998.

[9] Siromoney, A., and Inoue, K., Elementary Sets and Declarative Biases in a Restricted GRS-ILP model, Slovene Soc. Informatika24, 2000, pp 125- 135.

[IO] Stepaniuk, J., and Maj, M., Data Transformation and Rough Sets, Principles of Data Mining and Knowledge Discovery. Second European Symposium, PKDD?98, pp 441-9,1998 [ll] Tsumoto, S., and Tanaka, H., Incremental learning of probabilistic rules from clinical databases. In Proceedings of the Sixth International Conference, Information Processing and Management of Uncertainty in Knowledge-Based Systems (IpMU?96), Granada, Spain, 1996, pp 1457-1462.

[I21 Tsumoto, S., and Tanaka, H., Discovery of Approximate Medical Knowledge based on Rough Set Model, Principles of Data Mining and Knowledge Discovery. Second European Symposium, PKDD?98, pp 468-76,1998 [13] Ziarako, W., Discovery Through Rough Set Theory, Communications of the ACM: Vol. 42. No.

[I41 Pawlak, Z., Rough Sets, International Journal of Computer and Information Science, Vol. 1 1, No.

[ 151 Pawlak, Z., Rough Classification, International Journal of Man-Machine studies 20, 1984, pp 469- 483.

[16] Kinnear, Jr., K.E., Advances in Genetic Programming, Mit Press, 1994.

[I71 Koza, J.R., Genetic Programming 111 Darwinian Invention and Problem Solving, San Francisco, CA, 1999.

[I81 Dumitrescu, D., Lazzerini, B., Jain, L.G., and Dumitrescu, A., Evolutionary Computation, CRC Press, 2000.

[I91 Longdon, William B., Genetic Programming and Data Structure: Genetic Programming + Data Structure = Automatic Programming!, Amsterdam: Kluwer, 1998.

[20] Quinlan, J.R., C4.5: Programs for Machine Learning, Morgan Kaufinann, San Marteo, CA, 1993.

Orlando, USA, 1996, pp 555-560.

11, 1999, pp 54-57.

5 ,  1982, pp 341-356.

