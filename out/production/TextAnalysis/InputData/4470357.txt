A Novel Algorithm for dynamic Mining of  Association Rules

Abstract- Nowadays, communication network turns to be more complex, once there occurs a failure, there will result in multi-alarm-events which require relevant transactions. This paper describes the alarm correlation in communication networks based on data mining. The association rules from self-adaptation for dynamic network resource and service that build new rule by fully utilizing and maintaining rules formed before while transactions grow/delete or minimum support changes which enables the framework is easily updated and new discovery methods be readily incorporated within it. Both theoretical analysis and computer simulations illustrate outstanding performance of the proposed models, which can be further optimized by experiments for specific environment.



I. INTRODUCTION  A. Background Data mining is the science of extracting implicit, previously  unknown, and potentially useful information from large data sets or databases, also known as knowledge discovery in databases (KDD). For network, we utilize mined information to position the root alarms in it which can help to improve performance of administration of networks.

B. Problem description One of the most studied data mining problems is mining for  association rules which represent an important class of knowledge that can be discovered from database. Given a collection of items and a set of records (also called transactions), each of which contain some number of items from the given collection, the association rules indicate affinities that exist among the collection of items.

The task of mining association rules can be formally stated as follows : Let I = {i1, i2,? , im} be a set of literals, called items. Let D be a set of transactions where each transaction T that has a unique identifier-TID is a set of items such that T ? I.

We say that a transaction T contains X, a set of some items in I, if X? T. An association rule is an implication of the form X =>Y, where X? I, Y? I and X? Y=? . The rule X=>Y has two properties, support and confidence. When s% of transactions in D contain X? Y, the support of it is s%. If some of the transactions in D contain X, and c% also contain Y, then the confidence in the rule is c%. In general, the confidence is expressed in the form confidence(X=>Y) =support(X ? Y)/support(X). The problem of mining association rules is to generate all association rules that have support and confidence no lesser than the user-specified  minimum support and minimum confidence respectively.

In all the different algorithms given in the literature, the  problem of discovering association rules is decomposed into two sub-steps:  Find all sets of items that have support above the minimum support. Itemsets with minimum support are called frequency itemsets.

Use the frequency itemsets to generate the desired rules.

For every frequency itemset L(the length of L is larger than 1), find all non-empty subsets of L. For every such subset a, output a rule of the form a =>L-a, if the ratio of support (L) to support (a) is at least minconf.

In the rest of the paper, Apriori algorithm and Fast Update Algorithm (FUP) are explained in Section?. Next, our new adaptive algorithm is introduced, analyzed and compared in Section? . After that, Experimental results for algorithms mentioned above are showed in Section ? .Finally, the conclusion and future work are described in Section?.



II. RELATED WORK  C. The Apriori algorithm The Apriori algorithm concentrates primarily on the  discovery of frequent itemsets according to a user-defined minsup. The algorithm relies on the fact that an itemset could be frequent only when each of its subset is frequent. In the first pass, the Apriori algorithm constructs and counts all 1-itemsets.After it has found all frequent 1-itemsets, the algorithm joins the frequent 1-itemsets with each other to form candidate 2-itemsets. Apriori scans the transaction database and counts the candidate 2-itemsets to determine which of the 2-itemsets are frequent. The other passes are made accordingly.

Frequent (k-1)-itemsets are joined to form k-itemsets whose first k-1 items are identical. If k ? 3, Apriori prunes some of the k-itemsets; of these, (k-1) -itemsets have at least one infrequent subset. All remaining k-itemsets constitute candidate k-itemsets. The process is reiterated until no more candidates can be generated.

D. The FUP algorithm The purpose of the incremental mining is not only to  validate the existing rules, but also to find new rules both in increment and the updated database. We scan the increment to find which rules continue to prevail and which ones fail in the merged database. FUP is proposed for the maintenance of discovered association rules in large databases. It relies on Apriori and considers only these newly added transactions. In  2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.32   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.32   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.32   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.32   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.32     the first pass, FUP scans db to obtain the occurrence count of each 1-itemset. If some of them are in L1(frequent in DB), the total count of them can easily be calculated; else DB must be rescanned. By combining newly frequent itemsets with those remained frequent in L1, the set of all frequent 1-itemsets,  ' 1L ,  is generated. Similarly, in iteration k, Ck is generated by first using the join-based apriori-gen function on ' 1kL ? and then pruning Ck by removing those itemsets which are already present in Lk. The process is reiterated until all frequent itemsets have been found. In the worst case, FUP does not reduce the number of the original database must be scanned.



III. OUR NEW APPROACH  E. Definitions and Notations In order to describe easily, we give the following definitions DB: the original database DB+: the increment database DB-: the decrement database DB': the updated database L: the set of all frequent itemsets in the original database DB L+: the set of all frequent itemsets in the increment database DB+ L': the set of all frequent itemsets in the updated database DB' s: the minimum support threshold d: the number of transactions in DB d+, d? and d': denote the numbers of transactions in DB+,DB?, and DB' respectively  countX , countX + , countX  ? and 'countX :denote the support counts of itemset X in DB,DB+, DB?, and DB' respectively  With the same minimum support threshold s, a frequent itemset X of DB remains a frequent itemset of DB' if and only if 'countX ? (d+d+-d?)*s= d'*s. All the itemsets in DB' can be divided into four groups as shown in Table 1.

The itemsets of group 4 are indifferent. The itemsets in group 1 and group 3 can be obtained easily for the support count of each frequent itemset X in the original database has been recorded. However, the itemsets of group 2 need further consideration  for their support counts in DB are unknown in advance. Thus, how to efficiently generate the itemsets of group 2 is an important problem for the maintenance of association rules. In the following section, we propose efficient approaches to solve the problem.

F. A variant algorithm: The IDARM algorithm In this paper, we developed a new Incremental Dynamic  Association Rule Mining algorithm(IDARM) for adaptive association rules mining. The following lemma is described to illustrate the main principle of dynamic mining algorithm.

Table 1  Four groups associated with an itemset in DB'          Lemnin 1 An itemset X is frequent in DB' only if it is either in L or in L+ or in both L and L+. It mans that all large itemsets in the updated database, must be included in L?L+.

Proof.            X is neither in L nor L+  => countX < s*d, and countX + <s *d+  ' countX = countX  + countX  + < s * (d+ d+)=s*d'  Then X is not frequent in DB'.

There are three innovative points of our approach.

First of all, in each iteration of FUP, only candidate itemsets  of one size are handled. Therefore the number of passes over DB equals the size of the highest order itemsets to be handled.

To reduce the passes over DB, we perform exactly one scan over DB to count all candidates generated from DB+. While some excessive candidates may be counted, the number of passes over DB is guaranteed to be minimum. It is expected that the execution time saved in scanning DB should be much larger than the increased time in counting the extra candidates in DB+ for d+ is much smaller than d at most situations.

The IDARM algorithm works as follows: 1. Scan DB+ by using Apriori to find all frequent itemsets in  it, for each frequent itemset record its support count countX  + .

2. For each itemset X in L ? L+, if countX + countX +  ? s*(d+d+), it is added to the set L' with an identifier ?S1?(represent Group1).

3. Check all itemsets in L-L+-L', by scanning DB+ one time more to updated their counts. Similar, for each itemset X, add it to the set L' with an identifier ?S3? (represent Group3)if countX + countX  + ?  s*(d+d+).

4. Check all itemsets in L+-L-L', by scanning DB once to  updated their counts. After the counting is finished, new frequent itemsets that satisfied the s are identified and added to the set L' with an identifier ?S2? (represent Group2).

However, as the declination of data distribution in alarm database, one possible drawback for such approach is that a large amount of it unnecessary candidates may be counted in DB, causing considerable overhead.

Therefore, we propose another algorithm-IDARM', which allow only the k-itemsets with all its size k-1 subsets being large in DB?DB+ to be counted in DB. This means that the candidates actually being counted in DB are reduced to only the essential ones and there are not too many of them.

Remember that the major target of the FUP algorithm is to minimize the number of candidate itemsets to be generated and counted. This is done by priming the candidates in each pass through DB+ and DB. On the other hand, our algorithms try to deal with this problem in allowing extra candidate itemsets to be generated from DB+, while the efficiency is maintained by the reduced passes through DB. Usually, a large amount of candidates are generated from DB+ and we have to be efficient when we count those candidates in DB.

The IDARM' algorithm works as follows:     The first three steps keep the same with IDARM algorithm, the difference occurs only in step 4.

1. Scan DB+ by using Apriori to find all frequent itemsets in  it, for each frequent itemset record its support count countX  + .

2. For each itemset X in L ? L+, if countX + countX +  ? s*(d+d+), it is added to the set L' with an identifier ?S1?(represent Group1).

3. Check all itemsets in L-L+-L', i.e., those itemsets that are frequent in DB but not frequent in DB+ and not already frequent in DB' by scanning DB+ one time more to updated their counts. Similar, for each itemset X, add it to the set L' with an identifier ?S3? (represent Group3)if countX + countX  + ?  s*(d+d+).

4. In this step, we determine which itemsets in L+-L-L' are in  frequent DB'. Before counting, each k-itemset in L+-L-L', which has all its size (k-1)-subsets being in L', added to the candidate set to be counted, provided that it is not already in L'. Note that any k-itemset can contain at most k subsets of size k-1. All 1-itemsets in L+-L-L' are also added to the candidate set. The counts of all the candidates are initialized with their supports in L+. After the counting of each iteration is finished, new frequent itemsets that satisfied the s are identified and added to L' with the updated support as well as an identifier ?S2? (represent Group2).. New candidates are generated from L' by function apriori-gen, which takes as input the set of all frequent (k-1)- iternsets and returns a superset of the set of all large k-itemsets. The new candidates are then added to the candidate sets if it is not already in L'.

Continue the process until there are no more candidates to be counted.

Secondly, our approaches store the itemsets that are not frequency at present but may become frequency after updating the database, so that the cost of processing the updated database can be reduced. We discuss the cases where the frequency itemsets can be obtained without scanning DB.

We define a bound parameter?(0 <?<s), which is used to control the number of itemsets whose supports are less than s but no less than (s-?).We call these itemsets the potential frequent itemsets. The potential frequent itemsets and their support counts are stored in the set P. It is easy to see that the more the number of the potential frequent itemsets is, the less the cost of processing the original database will be.

We will first discuss the insertions. When the increment database DB+ is scanned, the support counts of the itemsets in L? P are accumulated. Then L and P are updated according to the support count thresholds s*(d+d+) and (s-? )*(d+d+), respectively. Let X be an itemset of DB+  and X?L? P. By Theorem 1, if the support count of X in DB+ ??is greater than d??+*(s-?), then X may be a frequent or potential frequent itemset of the updated database DB'. In the case, X is stored in set M. Let mx be the maximal value of the support counts of itemsets in M. By Theorem 2, if mx?d+*s+ d*?, we need not  scan DB. Namely, in this case all itemsets in M can not be frequent of the updated database.

Theorem 1 If X?L? P and countX  +  ? d+*(s ??), then X is not a frequent or potential frequent itemset of DB'.

Proof.         'countX = countX + countX  +  <d*(s-?)+ countX +  ? d*(s-?)+d+*(s- ?) =(d+d+)*(s-?)  Therefore, X is not a frequent or potential frequent itemset of DB'.

Theorem 2 If mx<d+*s+d*?, then we need not scan the database DB.

Proof.         X?L? P=>Xcount<d*(s-?)  ' countX = countX + countX  + <d*(s-?)+mx  < d*(s-?)+d+*s +d*? =(d++d)*s= d'*s  Then all the itemsets X in M is not a frequent itemset in the updated database DB'.

One important thing refers to how to set the value of parament?. Since d+ is much smaller than d at most situations, if the minimum support threshold set in DB+ is the same as set in DB, the number of frequent itemsets in DB+ may become incredible. Here gives an example, d=1000,d+=10 and s=0.1, respectively. It is easy to see every item in DB+ is frequent, which results in candidate 2-itemsets in DB+ is 210C =45, candidate 3-itemsets in DB+ is up to 245C =990, as it goes on the size of candidate k-itemsets is numerous. Hence, we propose a principle to solve this problem.

When 1/d+ has the same quantity level with s, we call DB+ small database, in this situation frequent itemsets in DB+ are huge. So I set?= (1-s)*d+/d. By Theorem 3, all frequent itemsets in DB' are included in L? P, we need not find set M of DB+. On the contrary, I set?a quantity lower than s and run the algorithm.

Theorem 3 If ?=(1-s)*d+/d,we need not process the increment database DB+.

Proof. All itemset X in DB+DB+ be frequent must satisfy  ' countX = countX + countX  + ? s*(d+d+)=s*(1+d+/d)*d  =(s+(s-1)*d+/d)*d+d+  =(s-?)*d+d+ ' countX ? countX +d+=> countX ? (s-?)*d  This means all frequent itemsets in DB' are included in L? P of DB, so we need not process DB+ and only scan it once to update 'countX of X?L? P.

Although classical FUP only considers the insertion cases, the user may remove out-of-date data from the database. In the following, we discuss the deletions. When the decrement     database DB- is scanned, the support count of the itemset in L? P lessens if it appears in DB-. Then L and P are updated according to the support count thresholds s*(d-d?) and (s-?)* (d?d?) , respectively. Let X be an itemset of DB and X?L? P.

By Theorem 4,if d? ? d*?/s, then we need not process the updated database (DB-DB-).If we do not want to process DB- we can set minimum support threshold of DB to s(1-?), where ?=d-/d. By Theorem 5, we need not process DB-.

Theorem 4 If d? ? d*?/s, then we need not scan the updated database DB'.

Proof. ?X? L? P, 'countX = countX - countX  ? , since countX and countX  ? are available, we need not scan DB'  ?X?L? P, 'countX = countX - countX ?  <d*(s-?)- countX ?  ? d*s-d*? ? d*s-d*s*d-/d=d'*s.

X is not frequent in DB' Theorem 5 If s'= s(1-?) is set to minimum support threshold of DB, we need not process the decrement database DB-.

Proof. All itemset X in DB-DB- be frequent must satisfy  ' countX = countX - countX  ? ? s*(d-d?)  =s*(1-d-/d)*d=s'*d ' countX ? countX => countX ?  s'*d  This means all frequent itemsets in DB' are included in L of DB, so we need not process DB- and only scan it once to update 'countX of X?L? P.

When both insertions and deletions are exist, there is a special case if d+?d- or d+<d- and mx<d*?+d+*s-d-*s, we need not scan the updated database DB'. Consider the following two cases.

Case 1: d+?d- Theorem 6  Let X is an itemset of DB+  and X? L ? P.

If countX  +  <(d+-d-)*(s-? ), it could not be a frequent or potential frequent itemset of the updated database.

Proof.      'countX  = countX  + countX  + - countX ?  < d *(s-?)+ (d+-d-)*(s-?) < (d+ d+-d-) *(s-?).

Thus X is not a frequent or potential frequent itemset of the updated database.

Theorem 7 Let X is an itemset of DB but not an itemset of DB+ and X?L? P. Then X can not be a frequent itemset of the updated database.

Proof.      'countX  = countX  + countX  + - countX ?  <d*(s-?)?(d+ d+-d-) *(s-?).

Thus X is not a frequent or potential frequent itemset in the updated database.

Case 2: d+<d-  Theorem 8 Let MS be the maximal value of the support counts of itemsets in M. By Theorem 8, if mx<d*?+d+*s-d-*s, we need not process the updated database.

Proof. Assume that X is an itemset of DB? DB+.

?X?L? P, 'countX = countX  + countX + - countX  ? , since  countX , countX + and countX  ? are available, we need not scan DB'  ?X?L? P, 'countX = countX  + countX + - countX  ?  <d*(s-?)+ countX + - countX  ?  ? d*s-d*?+mx- countX ?  ? d*s-d*?+mx ? d*s-d*?+(d*?+d+*s-d-*s)  =d*s+d+*s-d-*s=d'*s.

X is not frequent in DB' and We need not scan DB'.

Last but not least, we apply prune techniques described below to reduce the size of the original database as well as the updated database.

At the first step, all the candidate sets which do not have enough support in DB+ are stored in the set D. Later, during the scan of DB, all items in D can be removed from all the transactions, because they will not appear in any frequent itemset.

At any k-th iteration, during the scan in DB+, while FUP is counting the support for sets in the candidate sets C, for each transaction T the prune(DB+) function is called. It counts, for each I?T the number of sets in C and Lk which contain I. This number gives an upper bound on the number of frequent k-itemsets that contain I. If this number is smaller than k+1, then I cannot belong to any frequent (k+l)-itemset, and hence can be removed from all the transactions.

All the steps in the outline above are described graphically in Figure 1 for easy understanding.



IV. PERFORMANCE STUDY  G.  Generation of synthetic data                   Figure 1    process of IDARM algorithm                  Figure 2    Topology of our communication network In the experiments, we used synthetic databases to evaluate  the performance of the algorithms. Topology of our network is showed in figure 2-pink nodes are center nodes while others are edge nodes. The alarm data is generated using the following principle:  Alarms in lower level of arbitrary node cause corresponding alarms in upper level of the same node  Alarms of the edge node may be transmitted to the center node which it connect with a probability p  Alarms of the center node will be transmitted to one of the edge node which it connect randomly  Alarms of the center node will be transmitted to all center nodes which it connect  The way we insert the increment database DB+ is to put the d+ transactions in the rear of the original database. And the way we delete the decrement database DB- is to remove the first d- transactions from the original database.

H. Experimental Results All the experiments were performed on a 1.5GHz Pentium  IVPC with 512 MB of main memory, running under Windows XP professional. The algorithm was coded in Java 2 SDK.

Two sets of experiments have been performed to analyze IDARM versus FUP for different supports as well as IDARM versus FUP for different incremental sizes. First, the IDARM algorithm and the FUP algorithm are tested for different s, ranged from 0.01 to 0.1.The interval sizes for counting DB+ and DB were fixed at 1000 and 10000, respectively. The result is shown in Figure 3. Note that, our algorithm has a steady advantage over FUP at different minimum support thresholds.

The difference between the speedup ratio of IDARM and that of FUP is more than 2.5 for all the supports tested.

We then set different incremental size ranging from 1000 to 30000, with DB and s fixed at 10000 and 0.02, respectively.

The performance ratio is plotted in Figure 4. The IDARM algorithm clearly outperforms the FUP algorithm for all incremental sizes tested, especially when d+=1000.We also see the trend that as the incremental size continues to increase, the performances of both the IDARM and the FUP decrease.

However, a gradually level off only appears when the increment size is about 3 times the size of the original database.

The fact that IDARM still exhibits performance gain when the increment is much larger than the original database shows that it is very efficient.

The number of scans of original database was 0 or 1, only the incremental database is scanned ,also by using parameter?               Figure 3  IDARM and FUP with different supports and pruning, we expect to have better performance for our algorithm. To compare it with FUP more clearly we perform another experiment on the dataset with minimum support 1%.

The experiment run five insertions of increment databases and the relative execution time of each insertion is recorded for our algorithm and FUP algorithm.

A new database is generated for each insertion. Fig.5 shows the relative execution times in the cases of?=0.1%and? =0.5%. The size of each increment database is 1% of the size of the original database. Fig.6 shows the deletion cases with the same parameters. We finally assess the performance of dynamic algorithm considering insertion and deletion at the same time. In Fig.7, the sizes of the increment database and the decrement database are 0.5% of the size of the original database. For all the figures show, it can be seen that IDARM algorithm takes much less time than FUP algorithm since the original database which size is much larger than that of the increment database and the decrement database does not be scanned.



V. CONCLUSION  In the real world, databases are periodically and continually updated. As it grows, the discovered rules need to be verified and new rules need to be added to the knowledge base. Their primary aim is to avoid or minimize scans of the large older database by using the intermediate data constructed during the earlier mining. To efficiently discover and maintain frequent itemsets in a large database, we present IDARM algorithms to solve the problem:                 Figure 4  IDARM and FUP for different incremental sizes                    Figure 5 Relative execution time for Insert and FUP(s=0.01)               Figure 6 Relative execution time for Delete and FUP(s=0.01)               Figure 7 Relative execution time for Update and FUP(s=0.01)                  Experimental results show that our algorithms outperform other algorithms, especially when the size of the original database is much larger than that of the increment database and the decrement database or there is no need to scan original database. Applying data mining to realize automatization and intelligence of network management can accurately diagnose and locate failures as well as shorten the recover times, therefore has a significant meaning. Another possible research topic is the incremental mining of association rules in a distributed environment. Besides, algorithms of mining weighted association rule are also worth studying.

