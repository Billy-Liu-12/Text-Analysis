AN ADAPTIVE IMPLEMENTATION CASE STUDY OF APRIORI ALGORITHM FOR  A RETAIL SCENARIO IN A CLOUD ENVIRONMENT

Abstract - Retail transactional databases are voluminous and traditional algorithmic approaches to mine pattern in them are time consuming. The current study presents an approach to scale Apriori a Frequent Itemset Mining (FIM) algorithm, which is often used for market basket analysis. The study also compares the performance of scaled version of the algorithm (running on multiple on- demand simultaneous Azure cloud instances) with that of traditional setup (running on a fixed Azure cloud instances) using simulated data sets. The experimental results show that the response times were significantly lower and in favor of the scaled approach as the data volume increases.

Keywords- Retail industry, Frequent itemset mining, Cloud Computing, Data mining, Apriori Association Rule Mining.



I. INTRODUCTION Growth in retail stores is often driven by customers with huge expectations and widely varying tastes.

Efficient distribution of products and high quality services to end customers can help the retail stores to not only sustain the dynamic demand but also keep growing in this competitive market. Every transaction that is happening in a retail store is now being stored in database and is used to study customer buying pattern. Analysis of point of sales data (Market Basket Analysis) plays a significant role in deciding promotions and placement of products in a particular store which are essential ingredients of a successful retail business strategy [1]. One of the popular approaches has been to apply frequent itemset mining algorithms on transactional data to discover patterns/associations present in them [2-5]. Apriori is a highly popular algorithm to mine association rules by using breadth-first search approach to count the support of itemsets [6]. Owing to an increase in consumerization, transaction database size has increased in recent years, which has led to a growing interest in the development of infrastructure that is capable of rapid extraction of knowledge from data [7].  Existing studies suggest several optimization techniques to frequent item mining, which focus on optimizing the data structure or the methodology (for example FP-Growth and Dyna-FP-Growth) [8, 9, 10].

These above mentioned studies do not seem to focus on application and infrastructure scalability which is a vital part in handling large amount of data. Classic mining algorithms like Apriori require iterative scanning of the data set, which is very resource intensive and time consuming. These resource problems are normally overcome by having a higher support and confidence values. Higher support and confidence values result in reduction of the operational dataset and a potential loss of information. With the advent of cloud computing, access to virtually limitless resources could help overcome the resource limitations in a more novel form [11]. The current study presents an adaptive scalable model for Apriori algorithm based on cloud infrastructure. The proposed model was compared with the performance of Apriori algorithm in a traditional approach running on a cloud infrastructure using simulated retail transaction datasets

II. ASSOCIATION RULES - Definition Association rule mining finds association or correlation relationships among a large set of data items [12]. The association rules are considered to be of interest if they satisfy both a minimum support threshold and a minimum confidence threshold [13].

A more formal definition is the following [14]. Let ? = {i1, i2, ?, im} be a set of items. Let D, the task- relevant data, be a set of database transactions where each transaction T is a set of items such that T ? ?.

Each transaction is associated with an identifier, called TID. Let A be a set of items. A transaction T is said to contain A if and only if A ? T. An association rule is implication of the form A ? B, where A ? ?, B ? ?, and A ? B = ?. The rule A ? B holds in the transaction set D with support s, where s is the percentage of transactions in D that contain A ? B (i.e., both A and B). This is taken to be the probability, P (A ? B). The rule A ? B has confidence c in the transaction set D if c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P (B|A). That is,   2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing  DOI 10.1109/CCGrid.2013.104     Support (A ? B) = P (A ? B) Confidence (A ? B) = P (B | A   A set of items is referred to as an item An itemset that contains k items is a example the set {a1, a2} is a 2-itemset approach to finding association rules i [15]:  1. Find frequent itemsets: By definition itemsets will occur at least as freque determined minimum support count [12 2. Generate strong association rules fro itemsets: By definition, these rules minimum support and minimum confid  The overall performance of the mini rules is determined mostly by the time the frequent itemsets. The subsequent one of the commonly used methods to itemsets and how it could be optimized

III. METHODOLOG The solution is described next, where implementation and cloud implementat  A common approach used to a performance is to load the datase computer?s memory [8]. If the transac of a few megabytes, all of the data coul to the memory and operated on. Howev world scenarios where the transac exceeds gigabyte, loading in to the me a challenge due to the technical lim addressable memory space, availabilit memory, etc.).  The second commo approach is to use a pre-fixed pool of processing the transaction data set irre input load or the program throughput.

could be to use an adaptive approach u computing infrastructure services.

Figure 1 - Traditional fixed pool archi   For the current experiment the pool siz 3 for the traditional approach. The ?S program transposed the transaction data  A)  mset (pattern).

k-itemset. For . The common is in two parts  n, each of these ently as a pre- 2].

om the frequent  must satisfy dence [12].

ng association e taken to find section details  o find frequent for scale.

GY the traditional  tion is detailed.

achieve better et in to the ction dataset is ld be loaded in ver, in the real ction datasets mory becomes mitations (like ty of physical  on (traditional) f resources for spective of the An alternative  using the cloud   itecture  ze was fixed to Splitter? .NET asets and splits  the input data set in to mu ?Splitter? program passed a sub subsets to the ?Engine? at one t .NET programs (3 instances) pro sub-sets to identify candidate ite itemsets. After completing the current input dataset the ?Engine? output to the ?Aggregator? progra next set of input dataset form the This cycle repeats till all the processed. The ?Aggregator consolidates and generates the fin Figure 1 represents this traditiona  Figure 2 illustrates the propo architecture. In the proposed a multiple stages.

Figure 2 - Adaptive Cloud A   Stage 1: ?Splitter? .NET progra transaction data from the cloud reorganizes it in to smaller sub could be done in multiple ways; f range of transaction IDs. For this set was organized based on range The subsets of transactions are queue of stage 2.

Stage 2: Each data subset is pick processing engine. ?Engine? computes the individual item cou ?Engine? instances required is ?stage manager? program (discus output is pushed in to the output q  Stage 3: ?Order? .NET program c counts from stage 2 output consolidated output is sorted and set threshold.  The itemset and le (L1, L2 ? Ln). The sorted dataset individual itemsets and put to stage 4.

ultiple subsets. The bset of 3 input data time. The ?Engine? ocess the input data emsets and frequent  processing of the ? program passes the am and waits for the e ?Splitter? program.

e input dataset are ? .NET program nal frequent itemset.

al approach.

sed adaptive cloud approach, there are   Architecture  am picks up the raw d blob storage and sets. Reorganization for e.g. date range or experiment the data e of transaction IDs.

put in to the input  ked up by a separate ? .NET program unts. The number of s controlled by the ssed later). The stage queue.

consolidates the item queue. And the  filtered based on the evels were identified t is again split by the the input queue of     Stage 4: ?Engine? .NET program compares the identified itemsets to the transaction subsets to compute the individual itemset counts. The number of processing engine is controlled by the ?stage manager? program (discussed later). The intermediate itemset count is pushed to the output queue.

Stage 5: The ?Aggregator? .NET program consolidates the counts from stage 4 output queue.

The consolidated output is written to a file on the cloud blob storage.

The resource intensive stages (stage 2 & 4) have input & output queues and individual ?Stage Manager? programs. The ?Stage Manager? program continuously monitors the respective queues to determine the through put of the stage. If the ?Stage Manager? program determines a back logs a new ?Engine? is instantiated to take up additional load.

And as requests reduce the ?Engines? are automatically shut down.



IV. EXPERIMENTAL SETUP A.  Data preparation Retail stores houses many products and the rate at which these products are bought is dependent on many factors like price, brand, geography, customer profile etc. Meaningful buying patterns could be mined only if the historical (point of sales (POS)) data is available for a reasonable period of time. POS data is intertwined with base sales data (sales during non-promotional periods) as well as promotion sales data. Traditionally during certain periods in a year, products are sold at discount rates in order to boost the sales. There could be several such promotions for each product. The current study simulated datasets based on certain assumptions which are as follows;  ? Promotions for each product were grouped under  four categories namely 10% discount, 15% discount, total price reduction by three units and buy one get one free (BOGO) [16]  ? There should be a gap of at least a month between promotions  ? Frequencies at which products are bought follow a normal distribution  ? Retail scenario described in the current study pertains to a chain of stores  ? Daily transactions in a shop varied from 2000 to 3000.

? The number of items/products in each store varied from 500-1000.

? The products that were bought during a transaction varied from 2-10.

A java program based on these assumptions was used to simulate POS data for a period of 1-3 years. The fields that were used for generating simulated data include date, time, transaction ID, customer ID, Stock keeping unit (SKU), price, item quantity, store location, store ID, total cost, discount and net total.

Random functions were used to populate all the fields. SKU was used to identify each and every product uniquely was generated using a code which produces alphanumeric. Simulated transaction datasets of varying length (>5GB) were used as test samples for the experiments that were performed on the cloud infrastructure.

B. Infrastructure Setup The experiments were performed on the Windows Azure cloud infrastructure for both the traditional approach and the proposed alternative adaptive approach.

Windows Azure is a Microsoft public cloud computing platform used to build, deploy and manage applications through a global network of Microsoft-managed datacenters [17]. Windows Azure allows for applications to be built using many different programming languages, tools or frameworks and makes it possible for developers to integrate their public cloud applications in their existing IT environment.

The infrastructure used for this experiment where Microsoft Windows Azure small instances (1.6GHz CPU, 1.75GB RAM with 225 GB HDD). The stage queues (First-In-First-Out data structure) for the experiment were implemented using Windows Azure Service Bus where the average message size was 64kb. The raw transaction data was stored in the Windows Azure blob storage.

Cloud infrastructure was chosen as it provides an elastic and limitless resource which facilitates for the adaptive approach. The proposed approach is implemented on the Windows Azure public cloud, but can be setup on other public or private clouds.

Apriori algorithm was run on each dataset with convergence thresholds as shown in table 1. The processing time in both the cases were then compared.

Table 1 - Convergence conditions used for the validation of the scaled model  Minimal Support (%) Minimal Confidence (%) 1 10 5 10     The retailers would be more interes insights on a high value item. Ha minimal support % and minimal c increases the transaction dataset volu reduce potential data loss. The minim and minimal confidence % listed in chosen based on the above mentioned r

V. RESULTS Performance metrics for a converg support on the sample dataset is show Table 3 captures the performance convergence of 5% support on the s Time taken to complete candidate g different datasets by applying Apriori convergence conditions as shown in ta the approach (traditional and adaptive found to be significantly different as volume increased. While at smaller (number of candidates less than 100k) had similar performance, the perfor adaptive approach was far superior to approach as the data set volume increas candidates more than 100k).

Table 2 - Response for a sample dataset with 1%  Parameters Traditional approach  Raw Data Size 6 GB Transaction Size 8 million Minimal Support 1% Items for Candidate Generation 1294 Number of Candidates 770 K Time to Completion 11 Hrs.

Table 3- Response for a sample dataset with 5%  Parameters Traditional approach  Raw Data Size 6 GB Transaction Size 8 million Minimal Support 5% Items for Candidate Generation 1294 Number of Candidates 370 K Time to Completion 6 Hrs.

Table 4 - Response time comparison with 1% m  # CC Processing time A TRD ADP TR  1 100k 2 Hrs. 23 Sec 5 2 500k 8 Hrs. 41 Sec 5 3 770k 11 Hrs. 58 Sec 5  Table 5 - Response time comparison with 5% m  # CC Processing time A TRD ADP TR  1 100k 1 Hr. 14 Sec 5 2 250k 3 Hrs. 21 Sec 5 3 370k 6 Hrs. 33 Sec 5  sted in getting aving a lower confidence % ume used and mal support % n table 1 was reasons.

gence of 1% wn in table 2.

metrics for a ample dataset.

generation for and using the  able 1 for both e model) were s the data set data volumes both approach  rmance of the the traditional  sed (number of  % minimal support  l  Adaptive approach 6 GB 8 million 1% 770 k 58 sec  % minimal support  l  Adaptive approach 6 GB 8 million 5% 370 k 33 Sec  minimal support  Azure Instance RD ADP 5 12 5 18 5 18  minimal support  Azure Instance RD ADP 5 9 5 13 5 16  Note: - CC ? Candidate count; TRD ? Tr ? Adaptive approach.

Table 4 and 5 illustrates the diffe experiment and the time for can itemset generation using the tw times measured where to the minute.

Multiple candidate counts (100k 1% support and 100k, 250k and 3 where chosen for the experim human error that could have bee experimental setup.

Figure 3 illustrates the response ti candidate generation for the con listed in table 1 for the sample d tables 2.

Figure 3 - Response time for simulated d support

VI. DISCUSSION In the traditional approach all started at program initialization, w approach instance are started number of ?Engines? is set to traditional approach will have time and require higher recours engines is set too low there backlog and longer time to proces  The proposed approach has a run creating and destroying ?engin experiment might have a differen lesser volume dataset (few megab  The experimental results from number of azure instance for the in the last couple of cases (500k count) are the same. This could  raditional approach; ADP  erent iteration of the ndidate and frequent wo approaches. The  nearest second or  , 500k and 770k for 370k for 5% support)  ment to reduce any en introduced in the  ime for the complete nvergence conditions dataset mentioned in    dataset with 1% minimal  NS the ?Engines? are  while in the adaptive on-demand. If the a high number the  a very high startup e. If the number of will be significant  ss the dataset.

ntime overhead while ne? programs. The nt outcome on a with bytes).

table 4 show that e proposed approach  and 770k candidate be because, ?Stage     Manager? instantiated excess ?Engine? programs suspecting a backlog in one the cases. This could mean there were excess resources committed by the Stage manager? and there is a scope for optimization on the backlog determination logic.

The experiment captures the response time metrics across the traditional approach and the proposed adaptive approach. Capturing CPU, Memory and Disk I/O metrics will help further optimize the proposed approach which could be an extension to this experiment.



VII. CONCLUSION The proposed scalable version of the Apriori algorithm seems to indicate rapid mining of transaction data could happen in a very short time without any loss of information. This could be very useful in case of multi-store / multi-locality retail outlets. While the proposed approach doesn?t focus on high availability, the cloud services based design give native high availability and fault tolerance.

Hence Cloud computing infrastructure could be a great platform to run retail databases which are very huge in size.



VIII. REFERENCES [1] Yen-Liang Chen, Kwei Tang, Ren-Jie Shena,  Ya-Han Hua, "Market basket analysis in a multiple store environment", Decision Support Systems, Volume 40, Issue 2, August 2005, Pages 339?354.

[2] Jiawei Han, Jian Pei, "Mining frequent patterns without candidate generation", Proceeding SIGMOD '00 Proceedings of the on Management of data, Pages 1-12.

[3] Grahne, G., Zhu, J., "Fast algorithms for frequent itemset mining using FP-trees", IEEE Transactions on Knowledge and Data Engineering, Volume 17, Issue10, pages: 1347: 1362.

[4] Burdick, D., Calimlim, M., Flannick, J., Gehrke, J., Yiu, T., "MAFIA: a maximal frequent itemset algorithm", IEEE Transactions on Knowledge and Data Engineering, Volume7, Issue11, pages: 1490: 1504.

[5] Bart Goethals, Mohammed J. Zaki, "Advances in frequent itemset mining implementations: report on FIMI'03", ACM SIGKDD Explorations Newsletter - Special issue on learning from imbalanced datasets, Volume 6 Issue 1, June 2004,Pages 109-117.

[6] Agrawal, Rakesh; and Srikant, Ramakrishnan," Fast algorithms for mining association rules in large databases", Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994, pages 487-499.

[7] Cornelia Gy?r?di, Robert Gy?r?di, prof. dr.

ing. Stefan Holban., ?A Comparative Study of Association Rules Mining Algorithms?  [8] Yanbin Ye, Chia-Chu Chiang. ?A Parallel Apriori Algorithm for Frequent Itemsets Mining?. IEEE Conference publication - Engineering Research, Management and Applications, 2006.

[9] Haoyuan Li, Yi Wang, Dong Zhang, Ming Zhang, Edward Chang. ?PFP: Parallel FP- Growth for Query Recommendation?  [10] Changsheng Zhang, Jing Ruan. ?A Modified Apriori Algorithm with Its Application in Instituting Cross-Selling Strategies of the Retail Industry?. IEEE conference publication Commerce and Business Intelligence, 2009.

ECBI 2009.

[11] Buyya, R., Chee Shin Yeo,   Venugopal, S., ?Market-Oriented Cloud Computing: Vision, Hype, and Reality for Delivering IT Services as Computing Utilities?, 10th IEEE Computing and Communications, 2008, pp: 5- 13.

[12] J. Han, M. Kamber, ?Data Mining Concepts and Techniques?, Morgan Kaufmann Publishers, San Francisco, USA, 2001, ISBN 1558604898.

[13] Gy?r?di, R. Gy?r?di. ?Mining Association Rules in Large Databases?. Proc. of Oradea EMES?02: 45-50, Oradea, Romania, 2002.

[14] R. Gy?r?di, C. Gy?r?di. ?Architectures of Data Mining Systems?. Proc. of Oradea EMES?02: 141-146, Oradea, Romania, 2002.

[15] M. H. Dunham. ?Data Mining. Introductory and Advanced Topics?. Prentice Hall, 2003, ISBN 0-13-088892-3.

