<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Mining  Associations by Linear Inequalities

Abstract The main theorem is: Generalized associations of a re- lational table can be found by a finite set of linear inequal- ities within polynomial time. It is derived from the fol- lowing three results, which were established in ICDM0?02 and are re-developed here. They are (1) Isomorphic Theo- rem: Isomorphic relations have isomorphic patterns. Such an isomorphism classifies relational tables into isomor- phic classes. (2) A variant of the classical bitmaps in- dexes uniquely exists in each isomorphic class. We take it as the canonical model of the class. (3) All possible at- tributes/features can be generated by a generalized proce- dure of the classical AOG (attribute oriented generaliza- tion). Then, (4) the main theorem for canonical model is established. By isomorphism theorem, we had the final re- sult (5).

Keywords: association, deduction, feature, granules, bitmaps 1 Introduction Though there is no formal de?nition of data mining, but its informal version of [?] is rather universal: ? Drawing useful high level information (patterns, knowledge and etc.) from data.

In this paper we will attempt to analyze such a informal notion critically on one of the core techniques, namely, as- sociation rule mining [4]. As a byproduct of the analysis, we have the results stated in the abstract.

Our methodology is very rigorous, but we take the informal style to explain the rigorous method. We use illustrations, if formal proofs do not give good insight. The main goal is to get the idea cross.

1.1 Basics Terms in Association Mining (AM) In AM, two measures, support and con?dence, are the criteria. It is well-known among researchers, support is the essential one. In other words, high frequency are more im- portant than the con?dence of implications. We call them undirected association rules, associations, or high frequency patterns.

Association mining is originated from the market basket data [1]. However, in many software systems, the data min- ing tools are applied to relational tables. For de?nitive, we have the following translations of terms and will use them- interchangeably: 1. an item is an attribute value, 2. a q-itemset is a subtuple of length q, 3. A q-subtuple is a high frequency q-pattern or a q- association, if its occurrences are greater than or equal to a given threshold.

2 Anatomy of Association Mining(AM) In order to fully understand the mathematical mechan- ics of AM, we need to understand how the information are developed. First we set up a convention: ? A symbol is a string of ?bit and bytes;? it has no real world meaning. A symbol is termed a word, if the in- tended real world meaning participates in the formal reasoning.

In summary a word is an interpreted symbol and its inter- pretation is part of reasoning processes. Notations of word and symbol appear to be the same, but their meanings are very different.

0-7695-2142-8/04 $ 20.00 IEEE 2.1 Information Flows of AM 1. Representation phase: real world? a relational table    1. Representation phase: real world? a relational table of words. Each symbol (column names and attribute values) in the table represents some real world facts; so we refer to them as words. The representations are incomplete. The semantics of words are not imple- mented and relied on human support (traditional data processing professionals).

2. Data Mining Phase: a table of symbols? patterns of symbols. In this phase, table of words is used as a ta- ble of symbols, because data mining algorithms do not consult human for the semantics of symbols; they are treated as ?bits and bytes? in AM algorithm. Patterns, which are algebraic or logic expressions of symbols, are derived mathematically. Brie?y, the table of sym- bols is the only ?axioms,? and the patterns are the ?the- orems.? 3. Interpretation Phase: patterns of symbols ? patterns of words. Patterns are discovered as expressions of symbols in the previous phase. In this phase, those individual symbols are regarded as words again(using the meaning acquired in representation Phase).

4. Realizations Phase: patterns of words ? real world phenomena. Do patterns of words represent some real world phenomena?

2.2 Axiomatic Approach to Data Mining To examine the foundation of each data mining technique, we take the axiomatic approach. We require each technique explicitly speci?ed the following items: 1. Input data: Any information utilized by data mining al- gorithms are considered the input. In AM, the input is a table of symbols. However, in clustering techniques the input is the given set of points plus the background knowledge - the geometry of the ambient space of the given points.

2. The logic/reasoning system: AM uses mathematical deduction (Counting is a very simple deduction).

3. The output patterns: The model of output patterns needs to be speci?ed. The model of the traditional AM is the set of associations. They are ?conjunctions? of input symbols. We will generalize it to the set of alge- braic or logic expressions of symbols.

Such an approach has been called Deductive Data Min- ing [8].

2.3 Interpretations of Patterns In representation phase, each word, to human, does cor- respond to a real world phenomenon; we express it by say- ing the meaning of the word is the real world phenomenon.

Again such meaning is known only to human, not to the sys- tem. For systems words are symbols. The patterns are dis- covered in the form of expressions of symbols. To interpret- ing them, we convert the expression of symbols to expres- sion of words based on the interpretations established in the representation phase. Any pattern (expression of symbols) is said to be an un-interpreted pattern, if the transformation of the expression of symbols to the expression of words has not been done. A pattern is un-interpretable, if such a trans- formation cannot be done.

2.4 Realization of Patterns A pattern is un-realizable, if the pattern (= expression of symbol) does not correspond to a real world phenomenon.

Note that an expression of words is, of course, interpretable, but may not be realizable; we refer to.

3 Tables of Symbols - Understand the input First, we will examine how the real world is represented: 1 Select a set of attributes, called relational schema.

2 Represent a set of real world entities by a table of words.

These words, called attribute values, are meaningful words to human, but their meaning are not imple- mented in the system.

In traditional data processing (TDP) environment, for ex- ample, the attribute name, COLOR, means exactly what hu-    ample, the attribute name, COLOR, means exactly what hu- man thinks. Therefore its possible values are yellow, blue, and etc. More importantly, ? DBMS processes these data under human commands, and carries out the human-perceived semantics.

To stress such view, we call it 3 Computing with words At the same time, it is equally important to stress that 4 the human views or interpretations of those words are not implemented in the system. They are merely symbols in the system.

In the system, COLOR, yellow, blue, and etc are ?bits and bytes? without any meaning; they are pure symbols. Us- ing AI?s terminology [3], those attribute names and values 0-7695-2142-8/04 $ 20.00 IEEE ? ? ? ? ? ? ? ?  ?  ? ? ?? ??? ? ? ? ?? ?  ?  ? ?? ??? ?  ? ?? ?  ?  ? ?? ??  ?  ? ?? ? ?? ?  ? ?? ??? ?  ? ?? ?  ?  ? ?? ??? ?  ? ?? ?  ?  ? ?? ??? ?  ? ?? ?  ?    ? ?? ??? ?  ? ?? ?  ?  ? ?? ??  ?  ? ?? ? ?? ? Table 1. A Relational Table K and its Isomor- phic Copy K? (column names, and elements in the tables) are the seman- tic primitives. They are primitives, because they are unde- ?ned terms inside the system, yet the symbols do represent (unimplemented) human-perceived semantics.

3.1 Isomorphic Tables and Patterns Let us start this section with obvious, but somewhat a surprise observation. Intuitively, data is a table of symbols(Section 2.2), so if we change some or all of the symbols, the mathematical structure of the table will not be changed. So its patterns, e.g., association rules, will be preserved. Formally, we have the following theorem [9]: Theorem 3.1. Isomorphic relations have isomorphic pat- terns.

We will illustrate the idea by an example. The following example is adopted from ([6], pp 702): Example 3.2. Suppose a relation consists of two attributes, ? and ?, of type integer and string respectively. The cur- rent instance has eight tuples. These tuples are knowledge representations of entities. Table 1 illustrates the represen- tations and isomorphism. To illustrate Theorem 3.1., let us assume the support is ?two tuples? and count the items. It is easy to see we have: 1. 1-assoication in K: ??, ??, ???, ?? , 2. 1-assoication in K?: ???, ???, ????, ?? ?, 3. 2-assoication in K: ??? ???? and ??? ?? ?, 4. 2-assoication in K?: ???? ????? and ???? ?? ??.

q-association rules (q=1,2) are isomorphic in the sense that adding prime ? to associations in K become associations in K?; this illustrate the theorem.

? -Value ? ?-Value =Bit-Vectors =Granules ?? ?? ? = 11000110 = ???? ?? ?? ? ?? ?? ?? ? = 00101001 = ???? ?? ?  ?? ?? ? = 00010000 = ?????? ?-Value ?-Value =Bit-Vectors =Granules ??? ??? ? = 10010000 = ????    = 10010000 = ???? ???? ??? ??? ? = 01001010 = ???? ?? ? ?? ??  ?? ? = 00100101 = ???? ?? ?  Table 2. Words in K and K? have the same bitmaps and granules 3.2 The Canonical Models In this section, we will introduce tables of Bitmaps (TOB), Granules(TOG), and Granular Data Model(GDM). We will illustrate the idea by examples. Let us consider the bitmap indexes for K (see Table 1) the ?rst attributes, F, would have three bit-vectors. The ?rst, for value 30, is 11000110, because the ?rst, second, sixth, and seventh tuple have ?=30; see Table 2.

Using Table 2 as the translation table, Table 1 is transformed into bitmap table, Table 3. It should be obvious that we will have the exact same bitmap table for K?.

Next, we note that a bit vector can be interpreted as a gran- ule (subset) of ? . For example, the bit vector, 11000110, of ?= 30 represents the subset ?? ?  ?  ?  ?  ?, similarly, 00101001, of ? = 40 represents the subset ??  ?  ?  ?. As in the bitmap case, Table 1 is transformed into granular ta- ble, Table 4.

Note that ? -granules forms a partition, and hence induces an equivalence relation, denoted by ? ? ; similarly, we have  . Pawlak called the the pair ?? ?? ?  ?  ?? knowl- edge base and note that it is equivalent to table of granule.

Since knowledge base often means something else, we have called it granular data model(GDM). Now we can summa- rize these observations in: Theorem 3.3.

1. Isomorphic tables have the same canonical model.

2. The canonical model has three forms, table of    2. The canonical model has three forms, table of granules (TOG), bitmaps (TOB), and granular data model(GDM). TOB, TOG, and GDM are isomorphic and regarded as synonyms.

Theorem 3.4. It is adequate to do AM in one of the canoni- cal model.

0-7695-2142-8/04 $ 20.00 IEEE ? ? ? ? -bit ?-bit  ? ?? ??? ???????? ????????  ?? ??? ???????? ????????  ?? ??  ???????? ????????  ?? ??? ???????? ????????  ?? ??? ???????? ????????  ?? ??  ???????? ????????  ?? ??? ???????? ????????  ?? ??  ???????? ???????? Table 3. Tables of Words and Bitmaps)  ? ? ? -granule ?-granule ?? ?? ??? ???? ??? ??? ??? ? ??? ??? ?? ?? ??? ???? ??? ??? ??? ? ??? ??? ??? ?? ?? ??  ???? ??? ??? ???? ??? ??? ?? ?? ??? ???? ???? ??? ?? ?? ??? ???? ??? ??? ???? ??? ??? ?? ?? ??  ???? ??? ??? ??? ???? ??? ???  ?? ??? ???? ??? ??? ??? ???? ??? ???  ?? ??  ???? ??? ??? ???? ??? ??? Table 4. Tables of Words, and Granules: For each attribute, the collection of granules forms a partition on ? This follows immediately from the isomorphism theorem; see Theorem 3.1.

4 The Theory of Features in GDM The notion of generalizations of features/attributes has not been formally de?ned; we will examine the classical case and reach our de?nition; see Definition4.1.

Let us ?rst examine the well accepted case. Then we will take an obvious extension.

4.1 Attribute Oriented Generalizations(AOG) Let be the given relational table. In the traditional at- tribute oriented generalization (AOG), concept hierarchy is introduced by recursively de?ning a sequence of named equivalence relations on a given single attribute of : 1. A level zero concept is a base concept(distinct attribute values).

2. A level one concept is a named (or interpreted) equiv- alence class of base concept; ? ? ? ?? ?    ? ? ? ?? ?  ? ? ?? ??? ???  ? ?? ??? ???  ? ?? ???? ??  ? ?? ??? ???  ? ?? ???? ???  ? ?? ???? ???  ? ?? ??? ???  ? ?? ???? ?? Table 5. A Generalized Table GK with a new named attribute 3. A level two concept is a named (or interpreted) equiv- alence class of ?rst level concepts, in general, are the second innermost relation, and etc.

4. A n level two concept is a named (or interpreted) equivalence class of  ?? ? level concepts.

5. These concept hierarchy groups the base concepts into a nested sequence of named partitions of based con-  cepts. For each named partition (that is, the partition and each equivalence class has a name), a new attribute is introduced into the given table.

In this example, ? -attribute values are the base concepts. A named partition is de?ned: The equivalence class ???? ??? is named ???, another equivalence class ???? ????, and the partition ?? . This generalization introduces a new named attribute ?? (column) into the given table (Table 5.) 4.2 AOG on GDM In traditional concept hierarchy, all partitions are named.

To be uniform, we will consider the unnamed case. We will take the following ? Convention: unnamed partition will be regarded as canonically named, that is, the partition and equiva- lence classes themselves are their own names.

To illustrate the idea. The newly named ?? in , will be ?unnamed? in its GDM. Let us use GDM  ?? ?? ? ? ?  of : the partition ????? ???? ????? of ? -attribute induces a new partition on ? as follows: From Table 2, ?? de?nes a granule ???? ??? ??? ???, ?? de?nes ????, and ?? de?nes ???? ??? ???. The new granule is ???? ??? ??? ??? ? ???? =???? ??? ??? ??? ???, and ???? ??? ???. These two new granules de?ne a new partition Table 6.

The GDM of new ? = ?? ?? ? ? ?    ? ????? ??? ??? ??? ???? ???? ??? ????? : 0-7695-2142-8/04 $ 20.00 IEEE ? ? ? ????? ???? ????? ?  ? ? ?? ???? ??? ???  ? ?? ???? ??? ??  ? ?? ???? ??  ? ?? ???? ??? ???  ? ?? ???? ??  ? ?? ???? ??  ? ?? ???? ??? ??  ? ?? ???? ??  Table 6. A Generalized Table GK with a un- interpreted attribute By the convention, a canonically named partition or equiva- lence class will be referred to as an un-interpreted attribute or attribute value. Following the same spirit, a TOG, TOB or GDM is called a un-interpreted table or data model.

4.3 The Feature Completion on GDM Traditional AOG focuses on one attribute. There are no reasons to stop at considering one attribute, here we will consider a concept hierarchy on a set ? ? ?? ? ? ?  ? ? ? ? ? ? ? of attributes. As we have observed in Theorem 3.4., it is adequate to do AM in GDM is ??? ?.

In this case, ? is a subset of ; we will denote it bu  .

Definition 4.1. A generalization over  in a GDM is a coarser partition of  ? ?  ? ? ? ?  ? , where  ? ?    ? ?  ? ? ? ?  ? ? is a non-empty subset of .

If we let  varies through all non-empty subsets of , we have all possible generalizations of  in GDM. We will denote the set of all generalizations by ???  ?.

Observe that the intersection of generalizations is still a generalization. For any given ?nite set of generalizations, there is the smallest generalization. So ???  ? is closed under meet (=the intersection) and join (=the smallest generalization). So ???  ? is a lattice. More importantly, the meet and join are the meet and join in the lattice ??? ? of partitions on ? . Let ??? ? be the smallest sublattice of ??? ? that contains , and ??? ? be the smallest sublattice of ??? ? that contains all coarsening of . Now we have Theorem 4.2. ??? ?= ??? ?.

Based on this observation, we de?ne Definition 4.3. GDM ??? ??? ?) is called Universal Model of ??? ? in the sense it contains all its generalizations.

?  ? is the feature completion of .

4.4 Intuitive Discussions on Features/attributes We often hear such an informal statement ?a new feature (attribute) ? is selected, extracted, or constructed from a subset ? ? ???? ??? ? ? ? ??? of attributes in the table ?.? What does such a statement mean?

First we observe that feature and attribute have been used interchangeably. In the classical data model, an attribute or a feature is a representation of property, characteristic, and etc.; see e.g., [15]. A feature represents a human percep- tion about the data; each perception is represented by a sym- bol, and has been called attribute and the set of attributes a schema. Based on our convention, they are words in TDP (Section 3), but are symbols in AM.

Let us assume a new feature ? has bee selected, ex- tracted, or constructed. Let us insert it into the table ?.

The new table is denoted by ?? ? ? ?. The informal state- ment probably means in the new table, ? is an attribute. As it is derived from ?, it is functionally depended on ?; as extraction and construction are informal words, we can use the functional dependency as formal de?nition of feature selection, extraction and constructions. Formally we de?ne    selection, extraction and constructions. Formally we de?ne Definition 4.4.1.

? is a feature derived (selected, extracted and constructed feature) from ?, if ? is functional dependent on ? in the new table ?? ? ? ?.

In Theorem 4.2, we have shown that ??? ? is feature completion of . By the convention in Section 4.2, ???  is uninterpreted feature completeion of ?.

This theorem is rather anti-intuitive. Taking human?s view there are in?nitely many features. But the theorem says there are only ?nitely many features (as ??? ? is a ?nite set). How one can reconcile the contradiction? Where did the ?nite-ness slip in? Our analysis says it comes in at the representation phase. We represent the universe by ?nite words. However, in phase two, suddenly these words are reduced to symbols. Thus the in?nite world now is encoded by a ?nite set of symbols. In particular, features can only be encoded in a ?nite distinct ways. A common confusing most likely comes from the confusing of data mining and ?facts? mining.

5 Generalized Associations in GDM As we have observed that it is adequate to conduct AM in the canonical model, such as GDM.

Main Theorem 5.1. Let ??? ??? ?? be the universal model, Let g be a granule in a partition ? ? ??? ? such that ??? ? ?. Then g is an un-interpreted generalized associations.

0-7695-2142-8/04 $ 20.00 IEEE Let us de?ne an operation of binary number x and a set S.

We write S*x to be de?ned by The two equations ? ? ? ? ?, if ? ? ? and ? ?? ? ? ? ? ?, if ? ? ? or ? ?.

Main Theorem 5.2.

Let ? ? ?? ? ?? ? ? ? ? ? ?? be the smallest element in  ? ???. Let ? ? ? ?  ? ? ? ? ?  be the granules in ?. Then the union  ? ? ? ? ? ? ? ? ? ?  ? ?  is a granule that represents a un-interpreted generalized as- sociation rule, if its cardinality ???? ?  ? ? ?  ?  ? where s is the threshold.

where s is the threshold.

Remark: The cardinal number of ????? is bounded by the Bell number [2] of ? ???, the cardinal number of ? ? ? ?  ? ? ? ??? ? . The total number of derived at- tributes is bounded by Bell number ?  . However the com-  plexity of (**) is not too high. Let s be the then the possible ?minimal solutions? is bounded by the combination ?  .

We will report the calculation on real world data in future report soon.

5.1 Find Generalized Association Rule by Linear Inequalities - an example We will illustrate the idea of the procedure of ?nding gener- alized association rules in Table 7 by linear inequality (sup- port: ? ?). The association can be expressed as granules: 1. Associations of length one: (a) TEN = ??  ? ?  ? ?  ? ?  ? ?  (b) SJ = ??  ? ?  ? ?  ? ?  ? ?  (c) LA = ??  ? ?  ? ?  2. Associations of length two: (a) (TEN,SJ) = TEN ? SJ ?  ? ?  ? ?  ? ?  ? ?  ?; where (TEN,SJ) ? ???  ? ?    ? ?  ? ?  ? ?  ?? ??  ? ?  ? ?  ? ?  ? ?  is in table format, that is equivalent to GDM format: TEN ? SJ.

3. No associations of length ? ?.

Now let us examine the universal model in Table ??. The column ? in Table ?? is the smallest element in the com- plete relation lattice ?????. So every element of ????? is a coarsening of ?. In other words, every granule in ????? is a union of some granules from the partition ? (by the expression ?a granule in ?????? we mean a granule be- longing to one of its partitions.

In this example, the granules in ? are  ? = TWENTY ? NY = ?? ? ?,  = TEN ? SJ = ??  ? ?  ? ?  ? ?  ? ?  ?,  = TWENTY ? LA = ??  ?,  = THIRTY ? LA = ??  ? ?  Let be the cardinality of . The following expression represents the cardinality of granules in ?????, which is a union of some granules from the partition ?.

?TWENTY ?NY ??? ? + ?TEN ? SJ ???  + ?TWENTY ? LA ? ? ?    LA ? ? ?  + ?THIRTY ? LA ? ? ?  ? ?.

By taking the actual value of the cardinalities of the gran- ules, we have,  ? ? ? ? ? + ??  ? ? ?  + ??  ? ? ?  + ??  ? ? ?  ? ?.

? ? ? ? + ? ? ?  + ? ? ?  + ? ? ?  ? ?.

We will express the solutions in vector form,  ? ? ?  ? ?  ? ?  ?. It is an ?integral convex set? in 4- dimensional space: The ?boundary solutions? are: 1 (0, 1, 0, 0); this solution means ??? ? ???s cardinality by itself already meets the threshold ( ? ?).

2 (0, 0, 1, 1); this solution means we need the union of two granules, TWENTY ? LA and THIRTY ? LA, to meet the threshold. In other words, we need a gen- eralized concept that covers both the sub-tuple (TWENTY, LA)= TWENTY ? LA and (THIRTY, LA)= THIRTY ? LA.

For this particular case, since LA = (TWENTY, LA) ? (THIRTY, LA), hence LA is the desirable generalized concept.

3 (1, 0, 0, 1); this solution means we need the union of two granules, TWENTY ? NY ? THIRTY ? LA, 0-7695-2142-8/04 $ 20.00 IEEE Table of Granules Table of Symbols? ? ( ?? ?? ??) ( ?? ?????? ???  )  (?    ? ?  ?  ? ?  ?) ( ?  ??  NY)  (?  ? ?  ?  ?  ?  ?  ? ?  ?  ?  ?  ?  ?) ( ?  ? SJ)  (?  ? ?  ?  ?  ?    ?  ? ?  ?  ?  ?  ?  ?) ( ?  ? SJ)  ?? (?  ? ?  ?  ?  ?  ?  ? ?  ?  ?  ?  ?  ?) ?? ( ?  ? SJ)  (?  ? ?    ? ?  ?  ?  ?  ?  ? ?  ?  ?  ?  ?  ?) ( ?  ? SJ)  (?  ? ?  ?  ?  ?  ?  ? ?  ?  ?  ?  ?  ?) ( ?    ? SJ)  (?  ? ?  ?  ? ?  ?  ?   ?) ( ?  ??  LA)  (?  ? ?  ?   ? ?  ?  ?   ?) ( ?  ?????  LA)   (?   ? ?  ?   ? ?  ?  ?    ?   ?) ( ?  ?????  LA) Table 7. Table of Granules at left-hand-side is isomorphic to ? at right- hand-side: By Theorem 3.1.

one can ?nd patterns in either table as a single generalized concept.

?Internal points? are:[4](1, 1, 0, 0); we skip the interpreta- tions; [5](0, 1, 1, 0) ;[6](0, 1, 0, 1); [7](0, 1, 1, 1); [8](1, 1, 1, 0);[9](1, 1, 0, 1); [10](1, 0, 1, 1) ; [11](1, 1, 1, 1)We re-express these formulas in granular form and simplify them into disjoint normal forms .

1 TEN ? SJ = TEN = SJ 2 TWENTY ? LA ? THIRTY ? LA =LA 3 TWENTY ? NY ? THIRTY ? LA 4 TWENTY ? NY ? TEN ? SJ ? ? LA 5 TEN ? SJ ? TWENTY ? LA = TEN ? TWENTY ? LA ? SJ ? TWENTY ? LA 6 TEN ? SJ ? THIRTY ? LA ? ? TWENTY 7 TEN ? SJ ? TWENTY ? LA ? THIRTY ?LA = TEN ? LA = S J ? LA 8 TWENTY ? NY ? TEN ? SJ ? TWENTY ? LA = TEN ? TWENTY ? ? THIRTY 9 TWENTY ? NY ? TEN ? SJ ? THIRTY ? LA ? ? (TWENTY ? LA) 10 TWENTY ? NY ? TWENTY ? LA ? THIRTY ? LA ? ? SJ 11 TWENTY ? NY ? TEN ? SJ ? TWENTY ? LA ? THIRTY ? LA = all If the simpli?ed expression is a single clause (in the original symbols), it is the (non-generalized) associations. We have the following associations 1. TEN (= SJ = TEN ? SJ) 2. SJ 3. TEN ? SJ 4. LA (=TWENTY ? LA ? THIRTY ? LA)) 6 Conclusions Data, patterns, method of derivations, and useful-ness are key ingredients in AM. In this paper, we formalize the current state of AM: Data are a table of symbols. The pat-  terns are the formulas of input symbols that repeat. The method of derivations is the most conservative and reli- able one, namely, mathematical deductions. The results are somewhat surprising: 1. Patterns are properties of the isomorphic class, not an individual relation - This implies that the notion of pat- terns may not mature yet and explains why there are so many extracted association rules.

2. Un-interpreted attributes (features)are partitions; they can be enumerated.

3. Generalized associations can be found by solving in- tegral linear inequalities. Unfortunately, the number is enormous. This signi?es the current notion of data and patterns (implied by the algorithms) are too primitive.

4. Real world modeling may be needed to create a much more meaningful notion of patterns. In the current state of AM, a pattern is simply a repeated data that may have no real world meaning. So we may need to introduce some semantics into the data model [12],[10],[11].

