Automatic Linguistic Resolution: Framework and Applications

ABSTRACT Like many natural cognitive processes, natural language processing involves a simultaneous consideration of large number of different sources of information. It is unrealistic to assume a single and simple recipe can solve the general problem of ambiguity. In this article, we shall show how linguistic resolutions can be achieved by using both rules and associations in a neurosymbolic framework and how context effects can be modeled and being carried over into the sentence analysis. Three applications attest the validity of our framework. This exploration contributes to our understanding in linguistic resolution as well as simulates the dynamic and complex processes that take place in text comprehension.

1. INTRODUCTION Although there is a lot of theories in studying the word meanings, for many investigators, the sense of words has at least one fundamental characteristic - it has a bundle of elementary semantic components. Imagine being asked what boy means. One might answer by listing the attributes that seem to be necessary for something to be a boy, say, male, non-adult, and human.

boy := male & non-adult & human The sense of boy has been broken up into the  components. This is the basic idea of the component approach for linguistic information and there is an ultimate desire for linguists to extract a set of semantic components that cannot be decomposed any further, and from which lexical associations can be achieved [ 11. On the other hands, scholars of language and psycholinguistics have been among the first to stress the importance of rules in describing human behavior [2,3].

The reason for this is obvious. Many aspects of language can be characterized by rules. When a writer uses the word boy, as in The boy jumped, the author does not have any incentive to refer the readers to any encyclopedic knowledge about the boy. Boy is tool which the writer uses to point to the knowledge that is pertinent to the assertion he is trying to make. This is the fundamental issue in the so-called functional approach to meaning. Simply semantic component approach in representing meaning is certainly inadequate. Instead of asking what boy means, the functional approach asks how boy does what the writer intended it to do. The writer certainly tries to convince his readers that the boy is an agent with an action jump in the sentence The boy jumped. The functional view of meaning is that boy 0-7803-3280-6/96/%5.00 @1996 IEEE -  accomplishes what it does against its pure encyclopedic concept.

Regardless of the knowledge adopted, it is now a consensus that the processing of natural languages should rely on these two component and functional approaches. In this article, we address the natural language understanding in terms of two fundamental issues: 0 the elaborative inferences which draw heavily on  the reader?s general background knowledge in connecting the focal statement, and the associations of knowledge surrounding the text.

We shall briefly delineate a framework, with several applications, in which the functional approach of meaning are achieved through a semantic relational network whilst the semantic associations, via subsymbolic microfeatures, are on the basis of component approach. They interact intimately with each other in order to resolve the linguistic ambiguities.

2. ARCHITECTURE OF THE FRAMEWORK The framework consists of several major constituents: the parser, logical and grounding tiers, working memory (WM), sentence memory and a long-term (LTM) as shown in Figure 1.

Input Sentences  memory  SENTENCE WORKING MEMORY MEMORY  Figure 1 : The Framework  The LTM has functional component. The LTM stores the causal hypotheses and a set of commonsense inference rules. All the linguistic cues are used to retrieve knowledge from the LTM after parsing. Vectors of knowledge or rules are recalled and displayed on the logical tier. A semantic relational network is formed in this tier. Table 1 shows an algorithm of the formation of the relational network. The component approach of the lexical elements appears in the grounding tier which contains all the subsymbolic representations of concepts in the logical tier. It is out of the scope of this paper to describe the detail formation of the grounding tier. The 625 -    interested readers can refer to the author's publications [4,51.

1.For each of the input sentence, define a set of parsed propositions PP.

2.For each of the parsed proposition a E PP, define a set of relevant concept nodes, Le,, proposition nodes and lexical nodes, X ,  E X and the corresponding possible case roles of each concept element, if any, in the parsed proposition a.

3.Allocate the possible case roles nodes as the child node C(Xi). Negative conditional matrices are among them to indicate the mutual inhibitions.

4.From the LTM, append all the relevant concepts into X for each lexical nodes and set up an ordering for the all the concepts, X i  E X .

5.Check whether there are concepts left in X .  In the affirmative case, do: (a) Pick a concept X i  and append it as a node to the network.

(b) Set the parent node P(Xi) to some minimal set of nodes already in the net such that the follow equation is satisfied.

P(XIJ2 )...) x")=flP(xilxj..xj E P(Xi)) i  6,For all i, define the conditional probability table for X , which reflects the knowledge from the long term memory.

rable 1: The algorithm of setting up a semantic relation2 network.

During comprehension, a knowledge matrix K of the input sentence is formed and a unification process takes place in working memory in order to resolve the linguistic ambiguities. The unification process is a special form of constraint satisfaction [6,7], which has been applied successfully in many subfields in cybernetic research. The unification makes use of a set of linguistic nodes extracted from both tiers and a set of constraining relations. The process is to assign every node a value from the given sentence such that each node satisfies the constraints based on energy minimization. As the result, the nodes are contextually relevant and all contradictions will be eliminated.

Further discussion is as follows.

3. THE SEMANTIC RESOLUTIONS The first step involved in the simulation of text comprehension is to construct an associative network in working memory of the representational units and their interrelation, as specified by the text. To clarify the foregoing, let us represent the associative network by a knowledge matrix K where K ( i j )  specifies the link between nodes i and j in the associative network. The construction of the matrix K makes use of the logical inferences and associations. This knowledge acquisition process involves instantiating a set of node corresponding to the linguistic input. The formation of knowledge matrix can be summarized in the algorithm as shown in Table 2.

1 .Propagate the initial evidence vector, which is equal to one, from the input proposition a of the semantic relational network down to its concept nodes.

2.For each extracted linguistic concept Xi, with the degree of belief DBi .c rc, of the input proposition a, initialize a corresponding initial evidence vector (el, e2, ...., e,) E E, i.e., if X i  E a, e, t 1, else ei = 0 when i # j .

if X I  is an element in logical tier, e, t DB,,  Each evidence vector serves as an independent evidence.

3.At the same time, for each extracted linguistic concept XI, activate some of the highly associative nodes in its grounding tier and define the evidence vector for all the activated subsymbols zk as if zk is the subsymbol triggered by Xi ,  then ek e DB, x S(Zi, zk) while others are Set to zero and S(x,y) is the similarity measure between x and y .

4.For each evidence vector, propagate the evidence through the semdntic relational network.

5.lf the maximal implication chains have been traversed, the result vector are assigned as the corresponding column of the knowledge matrix.

6.Goto step 4 until all the columns of the knowledge matrix are complete.

Table 2: Algorithm of the formation of knowledge matrix K At the start of the process, a set of concept nodes of  the text is constructed from an input proposition. The belief of each node is computed using Bayes' rule. Only the nodes with final degree of belief greater than a preset threshold tc are chosen. Each entity serves as an independent clue for retrieving the semantic meanings and the related deductions, drawn from the interwoven tier interactions in both tiers. Evidence is then communicated along the links of the semantic relational network through the conditipnal probability matrices.

The beliefs are updated until all implication paths have been traversed. They are assigned as the elements in the corresponding column of the knowledge matrix K. The process is repeated until all the columns of the knowledge matrix are completed.

The knowledge matrix so formed uses weak, robust construction rules and is followed by a constraint satisfaction stage in a unification process. The process of unification is used to integrate the meanings in the memory into a coherent whole. It strengthens the contextually appropriate elements and inhibits unrelated and inappropriate ones, so that smart and complex deductions can be achieved. The approach is to reduce the dimensionality of the stimulus so that a very complicated stimulus could act as if only a small number of independent elements are involved. These elements have some sort of activation values, i.e., central, important concepts being more highly activated than  else e, = 0 when i # j .

peripheral ones. The algorithm for the unification process is described in Table 3.

- 626 -    1.Set up the associative network and the knowledge matrix K in the working memory as described in Table 2.

These elements form the network which are fully connected. All these elements have degrees of belief greater than the preset threshold tc.

2.Associated with each working memory element is a real number called its activation level, which represents the element's strength. Let a vector U is modeled in n- dimensional Euclidean space R", then U(t) = (ul, u2, ..., U,,), where ui is the activation level for the element i and n is the number of nodes extracted at some discrete time t .

3.Production firings, defined as a mapping function, direct the flow of activation from one working memory element, the source, to another working memory elements through the knowledge matrix K and then subjected to a normalizing operation. Mathematically, Let the mapping function 'p is defined as:  (i) ql(v) = U K  where 'pl is a linear mapping associated to the matrix K.

(ii) (PZ(UI, uz ,... , u,,)=(max(ul, 01, max(u~, Oh. .. , " ( 4 l ,  0))  q: R" +R"Y q(v) = (P3((PZ((Pl(U))), where  (iii) (P3(U) = &i i<n  4. Continued excitation by repeated vector multiplication leads to equilibration. The process stops at iteration m if the I U(m) - U(m-1) 1 I tu where tu is the tolerance, another preset threshold which is used to control the number of inhibited elements in the process.

Table 3: The algorithm for the unification process in working memory.

4. APPLICATIONS In this section, we present a number of applications that demonstrates the capability of the framework in accounting for semantic disambiguation and anaphoric resolution, to supplement, verify and strengthen our theoretical considerations in above. The detection of inconsistencies in texts has been widely documented by researchers studying linguistic comprehension. In the three different applications with detail discussions presented below, we will exhibit how the system deals with these problems. Each of them shows one of the aspects of the human sentence processing.

Case-Role Resolution To illustrate how the model can account for applying contextual constraints in sentences to arrive at a conceptually consistent case-role resolution, let us first compare the following sentences: 1. Rosalind went to a store with her mum.

2. Rosalind went to a store with her stick.

In the former, the correct interpretation is to attach the prepositional phrase, with her mum, to the agent Rosalind as an agent modifier while, in the second case,  stick, to the verb went to indicate it is an instrument of the action Go. The two different interpretations in form of propositions can be shown as follows: l*. Go[Agent: with(Rosalind, Mum), Destination: Store] 2*. Go[Agent: Rosalind, Destination: Store, Instrument: Stick]  The problem remained is how can our system cope with these two subtle case-role interpretations whenever a sentence is input, say,  (Sl) Rosalind went to a store with her mum.

In order to resolve the case role in the sentence (Sl),  all the possible interpretations are represented using different proposition formats. The modifier-role of Mum for the agent Rosalind in the sentence is represented as  &[Agent: with(Rosalind, Mum), Destination: Store] while the instrument-role of Mum for the action Go is represented as  Go[Agent: Rosalind, Destination: Store, Instrument: Mum]  I  U Figure 2: The semantic relational network for the case-role resolution  Figure 2 shows the semantic relational network formed in the logical tier. In the knowledge acquisition phase, both of these interpretations are activated in parallel, and concept nodes are added. Obviously, the two potential propositions are connected by the inhibitory links to ensure the non-coexistence in the final interpretation. More importantly, in this example, the meaning of the last word is decisive. The word Mum is connected to two possible case-roles, Znstrument[Mum] and Agent[Mum] due to the two different proposition formats and they both inhibit each other as shown in the Figure. Figure 3 shows the resulting associative network for sentence (Sl) in the working memory. To simplify the example, all the elaborations are ignored and only the most essentials are displayed. The competing propositions and concepts are connected by inhibitory links, whilst the positive connections are established by the logical deductions and the associations in both tiers.

- it is more appropriate to attach the phrase, with her- 627 -    c2  P7 P5   - 0  \.*J  -  u(m) 0.981 0.011 0.807 1.WO 0.871 0.752 0.016 0.8% 0.814 0.054  Figure 3: The resulting associative network for case-role resolution.

After initial activation of the text propositions, the unification process deactivates the inappropriate instrument attachment proposition. The three nodes representing the incorrect interpretations Instrument, Instrument[Mum], and Go[Rosalind, Store, Mum] are deactivated while the propositions embedded With(Rosalind, Mum) remains highly activated. Due to the weak prior probability between the nodes Mum and Instrument, the activations of the node Instrument[Mum] and Go[Rosalind, Store, Mum] are imperiled. In contrast, buoyed by the closed associations with Agent[Rosalind] and AgentrMum], this leads to the dominant role of the node With[Rosalind, Mum]. As a result, Go[ With(Rosalind, Mum), Store] wins up the competition. The top left column in Figure 3 indicates the asymptotic activation vector in the case-role resolution of the sentence (Sl).

It is discernible that the simulation finds the correct agent attachment interpretation for sentence (Sl). Using both probabilistic deductions and associations, the constraints in the unification process are sufficient to disambiguate the case-role assignment within the linguistic structure. Obviously, if the sentence  (S2)  Rosalind went to a store with her stick.

is input, the semantic relational network in logical tier is broadly identical. The associative network in working memory differs only with respect to how the meaning of the concept Stick emerges as having strong association with the instrument. The discrepancy between the Agent[Rosalind] and Agent[Stick] deteriorates the activation of Go[ With(Rosalind, Stick), Store]. As a result, the proposition &[Agent: Rosalind, Destination: Store, Instrument: Stick] will accomplish against the proposition Go[Agent: With(Rosalind, Stick), Destination: Store].

Lexical Disambiguation Our second application concerns the use of context to achieve the word-sense disambiguation. Historically, most approaches to the disambiguation problem have been based on logical inference with no quantitative measures of certainty. Our approach provides an answer to one hard part of the problem - how to combine evidence from different sources in disambiguation. Let us consider the following narrative excerpt:  Rosalind went to a store with her mum. She wanted to give her mum a present. Rosalind found out that everything was too expensive. She decided to knit a sweater for  her.

It is quite difficult to differentiate what the Store is by just reading the first sentence. In order to demonstrate the disambiguating property of the framework, first suppose the word Store is in the relational network as in Figure 4. It activates the nodes Storehouse and Dep-store, plus some other associates both from the logical and grounding tiers.

I Figure 4: A fragment of semantic relational network which contains the concept Store.

The associative network is formed as in Figure 5 and unification is achieved with the initial activation U, where  Go[With(Rosalind, Mum), Store  Rosalind Mum  Store Dep- store Storehouse Shopping  Gifr Warehouse  1 With(Rosalind, Mum) 1 Destinatron[Store] I  U =  .. .

c7  C6 c2  Node Name Pi GdWlth(R, M), Stwe] PZ Dsstln%tion]store] P3 W l t h ( M ,  Mum) C1 Rcaahnd c2 store C3 Dep_slore C4 Shopping C5 Gm C6 Warehow c7 staehollse  No Oflteratlons 17 Tolerance 1. 0 005  u(m) 1.ooO 0.71 I 0.780 1 .WO 0 871 0.202 0.216 0.135 0.814 0.754  Figure 5: Connections in working memory in lexical disambiguation. (Only the related nodes in the disambiguation are shown)  It is highly apparent that Storehouse seems to dominate the Dep-store, based on the knowledge in both tiers. That is, the concept Dep-store is less compatible with the context so far. The process is continued to the next sentence. The concept Present is input. Since there is no related links in the logical tier - - 628 -    for the concept as shown in the Figure 4, the similarity measure to its activated closest node Gift is calculated as 0.932 in the grounding tier. A new knowledge matrix, K* and a new activation vector U*, are constructed as before. The activation values of the competing linguistic concepts in the unification process are displayed in Figure 6.

N3  N& PI n n P4 P5 F8 n PL) CO ClO c11 c12 C13 C14 C15 C16  4 6 8 10 12 14 16 18  0.9 0.E 0.8 0.9  U ( d  * t.m 0.015 0.Wl 0.61 0.647 0.812 0.W 0.120 1.m 0.942 o.1m 0.075  0.881 0.m 0.W 03%  0.60~1  new text as shown in Figure 7. The details are described as follows.

Ncdea d Cone+@ N o o l n "  zn T d U m  LOWS  Figure 6: The asymptotic activations of the related nodes after Present is involved.

After the unification is complete, the nodes representing the incorrect meaning of Store, such as Storehouse, are deactivated, while the propositions representing Dep-store, Cij? remain highly activated.

This shows the word Store is more closely related with the concepts Dep-store, C i f ,  Present or Shopping. At this time, the system successfully integrates the word Store with its context.

Anaphoric Resolution In the above examples so far, we have used one or two sentences to build up the associative network in the working memory, another simulation concerns the use of existing sentence context to arrive at a conceptually consistent interpretation. We try to resolve the problem of multiple pronoun referent, the She in the fourth sentence of the above narrative excerpt.

(S3)  She decided to knit a sweater for  her.

N3 N4 N5  Our approach is using different propositional  Propositions are constructed for both possibilities as the referents of She, DTaRosalind, Mum, Sweater] and  associative elaboration generates the additional  representations to designate alternative interpretations.

K4= N7  N I 1  DTaMum,  Rosalind, Sweater]. The process of NI2  information for each of them. Obviously, it is no way to _.

Figure 7: A portion of the semantic relational network of pronoun resolution. The nodes with (*) are carried over from the preceding analysis where DTK, WTG are the acronyms of decide to knit and want to give respectively.

Let r2, r3 be the distilled proposition matrices of the second and the third sentences in the excerpt, and the current knowledge matrix K4 for the fourth sentence has the following component.

N3  r2=$ N6  0.8 0.7 0.6 0.5 0.7 0.9 0.3 0.8 0.6 0.3 0.5 0.9 0.5 0.8 0.9 0.6  N3 N I  NS N l  N l l  NI2  W 0.7 a6 09 0.5 05 06 0.7 05 03 -3 - I 05 0.6 0.1 0.6 - I  - 9 OS 0.7 0 9  0.3  0.6 -9  -1 OR 0 5 0.5 -1 - 8  0.4 0.7  justify any interpretation solely by the sentence itself.

The meaning of the sentence relies heavily on each of previous analyzed sentence. To cope with this, we make use of proposition matrix ri which is defined by  where U,, U, are the final asymptotic activation values of the rth and sth linguistic elements for sentence i respectively, as shown in the right-most column in Figures 3 and 5. Indeed, the proposition matrix r contains the distilled information from the preceding sentences which are carried over into the current processing cycle. They are superimposed into the current knowledge matrix and blended together with the  r i ( r ,  s) = U, x U, Eqn. (1)  -  The submatrix in rz between N3..N6 shows the high correlation between the corresponding concepts and suggests Rosalind is the Agent to give a Present to Patient[Mum] in the second sentence. In the same reason, r3 suggests that Rosalind is the Agent in the third sentence. In the current sentence, since Rosalind and Mum are both involved into the two alternatives, DTaRosalind, Mum, Sweater] and DTaMum, Rosalind, Sweater] and there is high association between Sweater and Present in the grounding tier, the context effects from Tz, r3 are superimposed into the knowledge matrix K4 under the common concepts N3, N4, N5 in rz and N3, N4 in r3 even though the  629 -    proposition matrices r,, r, may not be conformable for addition. The new knowledge matrix K4* is defined by K ~ *  = K ~  CD rz CD r, whereC=A CDBiff  a.. + b.. if i ,  j are the common concepts in A & B cii= v rJ  Eqn. (2)  {a.. or otherwise rJ  One of the most important implications in this simulation is that all the previous analyzed sentences in the excerpt carry out as an group of experts in the interpretation and resolve the ambiguities in current sentence. Each of the proposition matrices inherits the factual and circumstantial facts from the corresponding sentence. They encode some of the context knowledge.

They can be additionively superimpose into the current knowledge matrix and carry over all the context knowledge into the analysis as shown in the above simulation. Obviously, the result knowledge matrix is potentially robust than an individual knowledge matrix because the linguistic information is derived from a multiplicity of sources from the analyzed sentences, making the links in the current associative network are less prone to error.

Activations 1.0 0.9 I 0.8 ............................................................................. I / P  ..............................

.........................

................. ......................

I 2 3 4 5 6 7 a 9 1 0 1 1 1 2 No. of Iterations  Figure 8: The competition between DTNRosalind, Mum, Sweater] and DTK[Mwn, Rosalind, Sweater] for the pronoun She.

The resulting activations for the two competing propositions are shown in Figure 8. After 12 iterations in the unification process, the propositions in which She has been identified as Mum have activation values of 0, whereas the corresponding propositions, Rosalind, DTK[Rosalind, Mum, Sweater] have activations values of 0.822 and 1.000 respectively. The framework finds the correct interpretation for the pronoun She.

5. CONCLUSIONS The simulations presented here establish a neurosymbolic model of discourse comprehension as a candidate for a viable theory of sentence processing. In a wide variety of its applications as shown above, this model has been shown to be both psychologically and experimentally valid. The weakly interactive architecture is used to model various types of linguistic-  disambiguation. For instance, context effects in the disambiguation of homonyms and anaphoric resolution have been explained. Moreover, the mechanisms of the model are sufficiently specified to enable simulating effects of syntactic ambiguity, while the knowledge representation in the framework is flexible enough to be useful in a wide range of language understanding.

