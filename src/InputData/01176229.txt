

? 2002 Tim Menzies, Gary D. Boetticher Page [1] SEW-27 Tutorials '02.

Smarter Software Engineering: Practical  Data  Mining Approaches  27th Annual IEEE/NASA Software Engineering Workshop. Dec 4-6, 2002, Greenbelt, MD, USA  Tim Menzies WVU, USA  tim@menzies.com http://tim.menzies.com  Gary D. Boetticher UHCL, USA  boetticher@cl.uh.edu http://nas.cl.uh.edu/boetticher  ? 2002 Tim Menzies, Gary D. Boetticher Page [2] SEW-27 Tutorials '02.

Contents ! Preliminaries (2)  ! theme(3), about the audience(4), about the authors(5)  ! Inside some learners (6) ! neural nets (7), genetic algorithms (11), decision trees(19),  association rules (22), treatment learning(28), bayesian tuning (34), inductive logic programming(38)  ! Case studies (44) ! predicting effort (45), predicting faults (51), model-based ML (54),  early lifecycle project planning (60)  ! Demos ! Conclusions (63)  ! Discussion (64), downloads (69), further reading (71), references (72)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [3] SEW-27 Tutorials '02.

Many ways to learn,  Many  case studies,  but there is still a problem...

Theme  ? 2002 Tim Menzies, Gary D. Boetticher Page [4] SEW-27 Tutorials '02.

About the audience (is this you??)  ! Industrial practitioner-oriented.

! Material is suitable for: ! AI-novice or ! the technical manager of software engineering projects.

! (Also, for ML researchers: ! A head?s up on today?s industrial realities)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [5] SEW-27 Tutorials '02.

About the authors (why should you listen to us??) ! Tim Menzies, Ph.d.

! Commercial consultant: ! ES ! OO  ! Academic: KA, ML, RE ! Ph.D. in KA: General  Principles for testing KBS ! Currently:  " SE research chair, NASA/WVU IV&V facility, USA  ! Applications work: ! ML for decision making, in  early  software life cycle ! Bias: treatment learning  ! Gary D. Boetticher, Ph.d.

! Commercial consultant:  ! U.S. Olympic Committee, LDDS Worldcom, Mellon Mortgage  ! Academic: ML, Software Metrics,Reuse ! Ph.D. in ML & SE: NN-Based Reuse  Economic Model ! Currently:  " Dept of CS, SE University of Houston-Clear Lake, USA  ! Applications work: ! Using   ML   in  cost/effort  prediction ! Bias: neural networks  ? 2002 Tim Menzies, Gary D. Boetticher Page [6] SEW-27 Tutorials '02.

Inside some learners neural nets (7)  genetic algorithms (11) decision trees (19)  association rules (22) treatment learning(28)  bayesian tuning (34) inductive logic programming(38)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [7] SEW-27 Tutorials '02.

Neural nets learning: if output weights wrong, juggle weights and try again  Error Curve You are here  You want to be here  ? 2002 Tim Menzies, Gary D. Boetticher Page [8] SEW-27 Tutorials '02.

Neural nets learning: locally guided descent  Cross-section of Error Curve  Alpha = Learning Rate = Rate of Descent  Momentum = Ability to get out of local minima  Problem when Alpha is large  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [9] SEW-27 Tutorials '02.

A Neural Network Node  Output = Threshold function = 0..1 range, sigmoid(a) ? 0  ? Learning= ? learn wi  ? Wired into layers ? inputs=layer[0] ? outputs=layer[N] ? ?hidden?= rest  ? Sub-symbolic ? no ?grandmother  cell?  effort = size*w1 + complexity * w2 + vocabulary*w3  sigmoid(a)  ? 2002 Tim Menzies, Gary D. Boetticher Page [10] SEW-27 Tutorials '02.

NN Training and Testing Process  1. Feed a vector to the NN as inputy*w3  2. Calculate Effort and adjust weights  3. Repeat steps 1 and 2 for all vectors  Size Cmplx. Vocab. Effort 133 11 23 135 142 8 17 122  : : : : 9664 35 59 377  } Looping through all vectors = 1 Epoch Testing the NN: Freeze the NN weights and loop  through NN with an independent data set.

Size Cmplx. Vocab. Effort 133 11 23 135 142 8 17 122  : : : : 9664 35 59 377  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [11] SEW-27 Tutorials '02.

Genetic programming (GP) ! The dream:  ! Stop writing programs ! Instead, go and catch them  ! Take a herd of variants on a program ! Let them fight it out for a while ! Go fetch  ! GP= mutation of program structure ! Special form of GA (genetic  algorithms)  ? 2002 Tim Menzies, Gary D. Boetticher Page [12] SEW-27 Tutorials '02.

Genetic Algorithms (GA)  Current Population        1. Initialize Population  New Population 2. Evaluate Fitness        3.

Elitism    4. Select Parents     5. Cross over    6.

Mutate        Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [13] SEW-27 Tutorials '02.

GA Selection Rank (population is sorted according to objective values)  Elitism (fittest carry over to next generation)  Roulette (fitness increases chances of selection)  Stochastic Universal Sampling (equal opportunity for all)  ? 2002 Tim Menzies, Gary D. Boetticher Page [14] SEW-27 Tutorials '02.

GA Crossover ! Single point crossover  (One random crossover  point) 11001010101010101101 11001010110011001101 10011001110011001101 10011001101010101101  ! Two point crossover  (Two random crossover points) 11001010101010101101 11001001110011101101 10011001110011001101 10011010101010001101  ! Uniform crossover (Probabilistic bit selection) 11001010101010101101 11001001100011101101 10011001110011001101 10011010111010001101  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [15] SEW-27 Tutorials '02.

GA Mutation Mutation: Randomly flip bits  Why Mutate? Promotes diversity in the next generation.

Approaches ! Order changing: Swap 2 bits from a chromosome.

10011001110011001101                     10010001110011011101  ! Adding/Subtracting: Slight changes to real # encoding (1.12 3.65 3.86 4.31 5.21)                (1.12 3.65 3.73 4.23 5.21)  ? 2002 Tim Menzies, Gary D. Boetticher Page [16] SEW-27 Tutorials '02.

X  Genetic Programming (focus on the algorithm)  (3 + X) / (Z - Y) (X MOD Y) * (X + 4) / 2  /  3 X  +  Z Y  -  *  X  MOD /  Y 2  ((X + 4)+X) / (Z - Y) (X MOD Y) * 3 / 2  N e w    E x p r e s s i o n s  +  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [17] SEW-27 Tutorials '02.

Recommended settings ! [DeJong90]  ! Population size: 50 ! Number of generations:  1,000 ! Crossover type: typically two  point ! Crossover rate: 60% ! Mutation type: bit flip ! Mutation rate: 0.001  ? 2002 Tim Menzies, Gary D. Boetticher Page [18] SEW-27 Tutorials '02.

Case study: software effort prediction [Burgess01]  Least squares regression vs NN vs GA Converge consistently: NN > GP Accuracy: GP > NN Readability:  ? NN=0;  GP builds big theories using all available knowledge chunks  Ease of configuration: ? LSR= easy!

? NN = much expertise needed ? GA = some expertise needed  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [19] SEW-27 Tutorials '02.

Decision tree learning = recursive diversity reduction ! Given examples with a mix of classes ! Find a ?Description?  ! Which, if applied? ! ?Makes parts of the mix more uniform  If circle2, then ?-?  If circle1, then ?+?  --+-  +  --+-  --+- -----  -- -----  + +++++  ---  -- - ++  X  Z  --  ++ + +    ? 2002 Tim Menzies, Gary D. Boetticher Page [20] SEW-27 Tutorials '02.

Decision tree learning ! Diversity=0= all examples in 1 class ! Diversity = maximum = all classes equally  represented ! Best ?splitter? decreases diversity the most.

! Many measures of diversity in the literature  ! Simpson diversity index: biologists ! 1- repeatRate: cryptographers ! Gini index: econometricians: as used in CART  [Breiman84] ! Entropy: information theorists: as used in C4.5  [Quinlan92] ! Repeat, recursively, to find more complex  descriptions  --+-  +  --+-  --+- -----  -- -----  + +++++  ---  -- - ++  X  Z  --  ++ + +    If circle2, Then If circle3  then ?+? else   ?-?  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [21] SEW-27 Tutorials '02.

! data bindings: domain-specific metric for assessing module interrelationship.

! interface errors: errors arising out of interfacing software modules.

! [Porter90]  Predicting software  faults  ? 2002 Tim Menzies, Gary D. Boetticher Page [22] SEW-27 Tutorials '02.

From decision trees to association rules ! Classifiers:  ! ?this and this? goes with that ?class? ! conclusions (RHS) limited to one class attribute ! target very well defined  ! Association rules ! ?this and this? goes with ?that and that? ! conclusions (RHS) may be any number of attributes  ! But no overlap LHS and RHS ! target wide open  ! Treatment learning ! ?this and this? goes with ?less bad and more good? ! ?less?,  ?more?: compared to baseline ! ?bad?, ?good?: weighted classes  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [23] SEW-27 Tutorials '02.

Association rule learning  ! www.amazon.com ! Customers who bought  this book also bought:  ! The Naked Sun by Isaac Asimov ! The Caves of Steel by Isaac  Asimov ! I, Robot by Isaac Asimov ! Robots and Empire by Isaac  Asimov  ? 2002 Tim Menzies, Gary D. Boetticher Page [24] SEW-27 Tutorials '02.

Support and confidence ! Examples = D , containing items I  ! 1: Bread, Milk 2: Beer, Diaper, Bread, Eggs 3: Beer, Coke, Diaper, Milk 4: Beer, Bread, Diaper, Milk 5: Coke, Bread, Diaper, Milk  ! LHS # RHS = {Diaper,Milk} # Beer  ! Support       =   | LHS U RHS|  / | D |       = 2/5 = 0.4 ! Confidence  =   | LHS U RHS |  / | LHS |    = 2/3 = 0.66  ! Support-based pruning- reject rules with s < mins ! Check support before checking confidence  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [25] SEW-27 Tutorials '02.

Example of support- based pruning  4Bread  1Eggs  4Diaper  3Beer  4Milk  2Coke  Count1Item  3{Beer,Diaper}  3{Milk, Diaper}  2{Milk,Beer}  3{Bread, Diaper}  2{Bread,Beer}  3{Bread,Milk}  Count2Item  2{Milk, Diaper ,Beer}  3{Bread,Milk, Diaper}  Count3Item  Support-based pruning ? Min support =3  Ignore subsets of items of size N, ? only if N-1 support > min-support  Without pruning: 6C1 + 6C2 + 6C3  = 41 With pruning: 6 + 6 + 2 = 14  ? 2002 Tim Menzies, Gary D. Boetticher Page [26] SEW-27 Tutorials '02.

Classifiers versus Association rules (again)  ! Classifiers: ! Assume entire example set can fit into RAM.

! Association rule learners ! can handle very big data sets.

! [Agrawal93]: the APRIORI algorithm: ! very large data sets ! 10,000,000 examples ! 843MB  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [27] SEW-27 Tutorials '02.

The Data Mining Desiderata [Bradley98]  ! Require one scan (or less) of the database if possible.

! On-line ?anytime? behavior:  ! ?best? is always available, with status information on progress, expected remaining time, etc. provided  ! Suspendable, stoppable, resumable; ! incremental  progress saved to resume a stopped job.

! Ability to incrementally incorporate additional data with existing models efficiently.

! Work within confines of a given limited RAM buffer.

! Ooops, good-bye traditional classifiers e.g. C4.5 ! Argued against by some.

! ?Memory is cheap?: [Webb00], TAR2  ? 2002 Tim Menzies, Gary D. Boetticher Page [28] SEW-27 Tutorials '02.

Conf1(outlook.overcast)=10=  ((8-2)*(4-0))  +  ((8-4)*(4-0)) (4+0+0)  Treatment learning sunny, 85, 86, false, none (21=$2) sunny, 80, 90, true, none sunny, 72, 95, false, none  rain, 65, 70, true,          none rain, 71, 96, true, none rain, 70, 96, false,    some (22=$4) rain, 68, 80, false,  some rain, 75, 80, false, some  sunny,      69, 70, false, lots    (23=$8) sunny,      75, 70, true, lots  overcast,     83, 88, false, lots overcast,     64, 65, true, lots overcast,     72, 90, true, lots overcast,     81, 75, false, lots  outlook temp humidity wind hours on course  A good attribute range: ? More frequent in good  that bad ? Weighted by  ?distance?good to bad  ? Normalized by total count  ? Summed for all good/bad class pairs  Lots $ none Lots $ some  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [29] SEW-27 Tutorials '02.

sunny, 85, 86, false, none (21=$2) sunny, 80, 90, true, none sunny, 72, 95, false, none  rain, 65, 70, true,           none rain, 71, 96, true, none rain, 70, 96, false,    some (22=$4) rain, 68, 80, false,  some rain, 75, 80, false, some  sunny,      69, 70, false, lots    (23=$8) sunny,      75, 70, true, lots  overcast,     83, 88, false, lots overcast,     64, 65, true, lots overcast,     72, 90, true, lots overcast,     81, 75, false, lots   # attribute ranges with  deltaf  -4 -2 0 2 4 6 8 1 conf1 ? treatments ? (attribute.range.conf1 > X) ? |treatments|=N ? TAR2 = O(2N) ? fails for large N  outlook temp humidity wind hours on course  Conf1(outlook.overcast)=10=  ((8-2)*(4-0))  +  ((8-4)*(4-0)) (4+0+0)  Lots $ none Lots $ some  ? 2002 Tim Menzies, Gary D. Boetticher Page [30] SEW-27 Tutorials '02.

Treatments for golf       none some lots  If outlook=overcast, Then lots of golf  4/(4)= 100%  Least monitor: watch the humidity- alert if rising over 90%  Least change: pick a vacation location with overcast weather  If humidity=90..97 Then lots of  golf  1/(4)= 25%      none some lots  none some lots  If no change, Then lots of golf  6/(6+3+5)= 43%  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [31] SEW-27 Tutorials '02.

6.7 <= RM < 9.8 And 12.6 <= Ptratio < 15.9  BEST ACTION  0.6 <= NOX < 1.9 and 17.16 <= LSTAT < 39  WORST ACTIONBASELINE  500 examples  of bad--, bad, ok, good  Stop staring at the scenery and tell me where to steer or what  to dodge  ? 2002 Tim Menzies, Gary D. Boetticher Page [32] SEW-27 Tutorials '02.

Require (overall) require2  require3 require5  require4 +  +  + +  action1 --  action1, action2, action3,  ?   Cost,    Benefit 1 Y              Y             N,        ?   23200,  250 2           N              N             Y ,       ?   11400,  150 ?..       ?             ?            ?        ?   ?         ?  action2 --  --  fault2--  fault3 --  fault1--  JPL requirements [Feather&Menzies02]  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [33] SEW-27 Tutorials '02.

Case study: 99 proposed actions for deep space satellite design; 299 = 1030 options  Each row is one project plan  action1, action2, action3,  ?   Cost,    Benefit 1 Y              Y             N,        ?   23200,  250 2           N              N             Y ,       ?   11400,  150 ?..       ?             ?            ?        ?   ?         ?  Learnt: ? Do 16 ? Don?t do 14 ? Ignore 66 options ? c.f. genetic algorithms  Each dot  is one randomly generated  project plan  ? 2002 Tim Menzies, Gary D. Boetticher Page [34] SEW-27 Tutorials '02.

Pr of tampering = 0.02 Pr of fire = 0.01  Pr of smoke  given [fire=yes] = 0.90 Pr of smoke  given [fire=no]  = 0.01  Pr of report given [exodus=yes] = 0.75 Pr of report given [exodus=no ] = 0.01 Pr of exodus given [alarm=yes ] = 0.88 Pr of exodus given [alarm=no  ] = 0.001  etc  tampering  fire  alarm  smoke  exodus (run away!)  report (hello, operator?

I want to report a fire)  0.02  0.01  Use Bayesian analysis to update probabilities, given new information.

Use Bayesian analysis to update probabilities, given new information.

Bayesian Tuning  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [35] SEW-27 Tutorials '02.

tampering  fire  alarm  smoke = NO!!

exodus report = YES!!0.50  (was 0.02)  0.03 (was 0.01)  Q1: What if there is a report, but no smoke?

Q1: What if there is a report, but no smoke?

Q2: What if there is a report, and smoke?

Q2: What if there is a report, and smoke?

tampering  fire  alarm  smoke = YES!!

exodus 0.03  (was 0.02)  0.97 (was 0.01)  report = YES!!

Example from : [Poole98],  p371 Source = http:// www.swi.psy.uva.nl/projects/SWI-Prolog/download.html  = http://www.cs.ubc.ca/spider/poole/ci/code.tar.gz Files    = code/acp/bnet.pl;  code/acp/bnet_t1.pl  Bayesian Tuning  ? 2002 Tim Menzies, Gary D. Boetticher Page [36] SEW-27 Tutorials '02.

Non-na?ve model (bayesian network)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [37] SEW-27 Tutorials '02.

Low testing effort EXPLAINS 1) some observed operational defects  and 2) low pre-release defects  ? 2002 Tim Menzies, Gary D. Boetticher Page [38] SEW-27 Tutorials '02.

! Ancestors: ! ancestor(X,Y):-parent(X,Y).

! ancestor(X,Y):-parent(X,Z),ancestor(Z,Y).

! Lists: ! member(X,[X|Z]).

! member(X,[Y|Z]):-member(X,Z).

! append([],X,X).

! append([X|Xs],Ys,[X|Zs]):-append(Xs,Ys,Zs).

ExampleExample actionaction hypothesishypothesis +p(b,[b]) add clause p(X,Y).

-p(x,[]) specialize p(X,[V|W]).

-p(x,[a,b]) specialize p(X,[X|W]).

+p(b,[a,b]) add clause p(X,[X|W]).

p(X,[V|W]):-p(X,W).

Inductive Logic Programming  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [39] SEW-27 Tutorials '02.

East-West trains 1. TRAINS GOING EAST 2. TRAINS GOING WEST  1.

2.

3.

4.

5.

1.

2.

3.

4.

5.

1. TRAINS GOING EAST 2. TRAINS GOING WEST  1.

2.

3.

4.

5.

1.

2.

3.

4.

5.

? 2002 Tim Menzies, Gary D. Boetticher Page [40] SEW-27 Tutorials '02.

ILP representation  ! Example: eastbound(t1).

! Background theory: car(t1,c1).      car(t1,c2).        car(t1,c3).      car(t1,c4).

rectangle(c1).   rectangle(c2).     rectangle(c3).   rectangle(c4).

short(c1).       long(c2).          short(c3).       long(c4).

none(c1).        none(c2).          peaked(c3).      none(c4).

two_wheels(c1).  three_wheels(c2).  two_wheels(c3).  two_wheels(c4).

load(c1,l1).     load(c2,l2).       load(c3,l3).     load(c4,l4).

circle(l1).      hexagon(l2).       triangle(l3).    rectangle(l4).

one_load(l1).    one_load(l2).      one_load(l3).    three_loads(l4).

! Output: eastbound(T):-car(T,C),short(C),not none(C).

Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [41] SEW-27 Tutorials '02.

Predicting Correctness [Almeida99]  NewID CN2 C4.5 C4.5_rule FOIL Accuracy: 52% 54% 66% 68% 73%  ? 2002 Tim Menzies, Gary D. Boetticher Page [42] SEW-27 Tutorials '02.

FOIL?s best rule high(A) :-  executable(A,B), maximum_statement_nesting_depth(A,C), lines_of_comments(A,B), commentsdivsize(A,E), n1(A,F), n2(A,G), less_or_equal(E,F), not less_or_equal(B,G), C <> 4, C <> 43, less_or_equal(C,D).

High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43  High faults when comment density <= #operators and executable statements > #operators and max nesting <= number of lines of comments and max nesting is not 4 or 43  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [43] SEW-27 Tutorials '02.

Inside  some learners ! neural nets ! genetic algorithms ! decision tree learners ! association rule learners ! treatment learners ! bayesian tuning ! inductive logic programming  ?sub-symbolic, locally guided descent  symbolic, global search  ?recursive diversity reduction ?this goes with that CLASS  ?this goes with that  ?differences between classes  ? a little model goes a long way  ?Horn clauses  ? 2002 Tim Menzies, Gary D. Boetticher Page [44] SEW-27 Tutorials '02.

Case studies predicting effort (45) predicting faults (51)  model-based ML (54) early lifecycle project planning (60)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [45] SEW-27 Tutorials '02.

Case study: How can we estimate earlier in the life cycle?

? 2002 Tim Menzies, Gary D. Boetticher Page [46] SEW-27 Tutorials '02.

Predicting development times (in months) [Srinivasan95]  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [47] SEW-27 Tutorials '02.

Bayes for effort: [Chulani99] ! The COCOMO-II project  ! Open-source software cost estimation  ! Reuse vs effort:  XH : multiple product lines VH : across product lines  H : across program N : across project L  : none  ! Regression over data from 83 software projects  ! Regression conflicted with ?Delphi values?  ! Tune regression values using Delphi expectations  0.8 0.9  1.1 1.2 1.3 1.4 1.5 1.6  Low N H VH XH  Delphi Regression Adjusted  Data: reuse lowers effort Expected: reuse  increase effort  ? 2002 Tim Menzies, Gary D. Boetticher Page [48] SEW-27 Tutorials '02.

COCOMO-II (1998)COCOMO-II (1997)  Pred(30)  Pred(25)  Pred(20)  Pred(X)     projects     projects   68 55  63 48  projects- based  on Bayesian  projects- based  on Delphi  Percentage of estimated effort  within X% of actual  Conclusion: (data + delphi + tuning) > data  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [49] SEW-27 Tutorials '02.

Neural Network Count the widgets in the Interface to estimate effort!

Labels Edit Boxes Grid Boxes  Check Boxes Buttons  ? 2002 Tim Menzies, Gary D. Boetticher Page [50] SEW-27 Tutorials '02.

Neural Network  Subsystem Pred(25) MARE Buyer Admin. 80% 17.6% Buyer Client 80% 14.6%  Distribution Server 20% 96.7% Supplier Client 90% 12.2%    12 Different Widgets Counted and associated  with effort  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [51] SEW-27 Tutorials '02.

Case study: Predicting software  faults [Tian95]  ? 2002 Tim Menzies, Gary D. Boetticher Page [52] SEW-27 Tutorials '02.

Predicting software  faults [Khoshgoftaar99]  Which dogs did not bark?

? 42 attributes in data set; ? Only 6 in the learnt theory  Different attributes than before ? ?causes fault?= domain-specific ? Method for finding faults= general  Which dogs did not bark?

? 42 attributes in data set; ? Only 6 in the learnt theory  Different attributes than before ? ?causes fault?= domain-specific ? Method for finding faults= general  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [53] SEW-27 Tutorials '02.

Issue of generality ! Specific conclusions may not  apply to general projects ! Proposal one:  ! Intra-project learning ! Lessons should generalize  across the same developer methodology, application, and tool set  ! Proposal two: ! Inter-project learning ! Need larger training set  ! COCOMOII uses 161 projects  ! Note: two = N * one  [Khoshgoftaar99]  good  bad  [Tian95]  bad  good ??

? 2002 Tim Menzies, Gary D. Boetticher Page [54] SEW-27 Tutorials '02.

Model-based ML [Bratko89,Pearce88]  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [55] SEW-27 Tutorials '02.

Model-based ML (simple e.g.)  %sum(X,  Y, Z).

sum(+, +, +).

sum(-, -, -).

sum(0, 0, 0).

sum(+, 0, +).

sum(-, 0, -).

sum(0, +, +).

sum(0, -, -).

sum(+, -, Any).

sum(-, +, Any).

+ if X >0 X?=      - if X < 0  0 if X= 0{ %switch(State,Volts,Amps) switch(on,       0,     Any).

switch(off,      Any,   0).

%blub(Mode,Light,Volts,Amps) bulb(blown,dark, Any, 0).

bulb(ok,     light, +,  +).

bulb(ok,    light, -,  -).

bulb(ok,    dark, 0,  0).

? 2002 Tim Menzies, Gary D. Boetticher Page [56] SEW-27 Tutorials '02.

A qualitative circuit  go  :- tell('circ.data'), go1, told.

go1 :- functor(X,circuit,9), forall(X, example(X)).

example(circuit(Sw1,Sw2,Sw3,B1,B2,B3,L1,L2,L3)) :-  classification(B1,B2,B3,Class), format('~a,~a,~a,~a,~a,~a,~a~n',  [Sw1,Sw2,Sw3,L1,L2,L3,Class]).

:-  %classification(B1, B2, B3,Class) % needs 2 our of three bulbs working classification( ok, ok, B3,   good):- !.

classification( ok, B2, ok,   good):- !.

classification( B1, ok, ok,   good):- !.

classification( B1, B2, B3,   bad).

Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [57] SEW-27 Tutorials '02.

Results from > 700 examples circ.names:  good,bad.

switch1: on, off.

switch2: on, off.

switch3: on, off.

bulb1: light, dark.

bulb2: light, dark.

bulb3: light, dark.

Command line:  c4.5 -f circ -m 2  Watching bulb1 tells us the rest.

Insightful?

Or dull?

Watching bulb1 tells us the rest.

Insightful?

Or dull?

? 2002 Tim Menzies, Gary D. Boetticher Page [58] SEW-27 Tutorials '02.

More Model-based ML  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [59] SEW-27 Tutorials '02.

can we revisit those warranties?

! Run 1: (35,000 solutions)  ! Learn 1:  ! Run 2: if Sw2c=off then 3264 solutions  ! Learn 2:  ! Run 2: if Sw2c=off and Sw1c=on then 648 solutions  ! Learn 3:  Can?t close Sw3c (warranty issues)  No budget for expensive hardware  ? 2002 Tim Menzies, Gary D. Boetticher Page [60] SEW-27 Tutorials '02.

* 3 ?tunings? * 5 SLOC guesstimates  150,000 runs  Treatments for software projects  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [61] SEW-27 Tutorials '02.

flex=1, +  pmat=3     =  sced=2  rest= anything from kc1  150,000 runs  150,000 runs  Treatments for software projects (ii)  ? 2002 Tim Menzies, Gary D. Boetticher Page [62] SEW-27 Tutorials '02.

pmat=2 +  acap=2      =  sced=2  rest= anything from kc1  30,000 runs 30,000 runs  Treatments for software projects (iii)  Proceedings of the 27th Annual NASA Goddard Software Engineering Workshop ? Tutorial Notes (SEW?02)     ? 2002 Tim Menzies, Gary D. Boetticher Page [63] SEW-27 Tutorials '02.

