2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?In the present work, the study on class imbalance problems in a distributed setting exploiting sparsity structure in the data has been carried out. We formulate the class-imbalance learning problem as a cost-sensitive learning problem with L1 regularization.

The cost-sensitive loss function is a cost-weighted smooth hinge loss. The resultant optimization problem is minimized within the Distributed Alternating Direction Method of Multiplier (DADMM) framework. We partition the data matrix across samples. This operation splits the original problem into a distributed L2 regularized smooth loss minimization and a L1 regularized squared loss minimization. L2 regularized subproblem is solved via Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) and random coordinate descent method in parallel at multiple processing nodes using MPI whereas L1 regularized problem is just a simple soft-thresholding operation. We show, empirically, that the distributed solution approximates the centralized solution on many benchmark data sets. The centralized solution is obtained via Cost-Sensitive Stochastic Coordinate Descent (CSSCD). Empirical results on small and large-scale benchmark datasets show some promising avenues to further investigate the real-world applications of the proposed algorithms such as anomaly detection, class-imbalance learning, etc. To the best of our knowledge, ours is the first work to study class-imbalance in a distributed environment on large-scale sparse data.

Index Terms?Class-Imbalance Learning, Distributed Algorithm, Anomaly Detection  F  1 INTRODUCTION  C LASS-imbalance learning in Big data has recently at-tracted a slew of attention from the data mining and machine learning community [1?4]. This can be attributed to the real-world data often being class-imbalanced, e.g., network intrusion data, credit-card fraud data, spam-email detection data, malicious URL data, etc. Above kind of data usually consists of two classes: the majority class and the minority class. Majority class examples are present in huge numbers while minority class examples are rare. For exam- ple, in credit-card fraud detection, fraudulent transactions comprise only a fraction of the total transactions and the problem is to classify each transaction correctly. Similarly, cancer detection task involves correctly predicting cancer vs non-cancer case, otherwise, the penalty incurred is huge.

In the past few years, data has grown in an unprece- dented way leading to buzzword ?Big data? in academia as well as industry. Big Data has certain characteristics such as high volume, high dimensional, streaming, heterogeneous, distributed, and sparse. We briefly describe these qualities of Big data.

? High Volume: High volume means data is so massive  that it cannot be stored on one machine.

? High dimensional: High dimension means data has  number of features in millions and above.

? Streaming: By streaming, we mean that the data is  continuously flowing, e.g., social media data, credit-  ? Maurya, C.K., Toshniwal, D. are with the Department of Computer Science & Engineering, Indian Institute of Technology, Roorkee-247667, Haridwar, U.K., India E-mail: {ckm.jnu, durgatoshniwal}@gmail.com  ? Venkoparo, G.V. is with Research & Technology Center, Robert Bosch Engineering & Business Solution, Bangalore, India E-mail: GopalanVijendran.Venkoparao@in.bosch.com  card transaction data, etc.

? Heterogeneous: Such data often contains mixed fea-  tures such as continuous, ordinal, nominal, etc.

? Distributed: Nowadays, data is often collected at mul-  tiple locations because of the distributed work environ- ment. As such, gathering all of the data at one facility is a cumbersome task.

? Sparse: Big data is not only high dimensional but also sparse that means only a few features are non-zero for one data point.

All of the above characteristics of big data render the task of class-imbalance learning difficult. In the present work, we have made an attempt to address the class-imbalance prob- lem in Big data in a distributed setting exploiting sparsity structure. Without exploiting sparsity structure, learning algorithms run slower since they have to work in the full- feature space.

After defining the problem we are going to tackle, we describe the ways to handle it. The class-imbalance problem in the literature has been addressed through (i) over-sampling/under-sampling [3, 4] (ii) Ensemble-based method (iii) Recognition-based method (one-class classifier such as one-class SVM [5]) (iv) Cost-sensitive learning based method (see for example [1] and references therein). The ma- jor drawback with sampling-based methods is that they may lose important information in the case of under-sampling or model overfitting in the case of over-sampling [6]. Ensem- ble methods, although good at learning class-imbalance to high accuracy, suffer from high training time in the case of Big data. Recognition-based methods require that data be available for majority class and based on their training on majority class, they detect the minority class. However,    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 2  recognition-based approach suffers from a serious issue, that is, they are not able to take into account the cost supplied by the user. Cost-sensitive learning tackles the class-imbalance problem by assigning costs to misclassification. These costs are usually unknown, however, we make an assumption that the cost is a user-supplied input in our current work.

It has been proved that cost-sensitive learning methods per- form better than over-sampling/under-sampling methods in the case of Big data [7].

In the present work, we solve the class-imbalance learn- ing problem from cost-sensitive learning perspective due to various reasons. First, Cost-sensitive learning can be directly applied to different classification algorithms. Second, cost- sensitive learning generalizes well over large data and third, they can take into account the user input cost. Keeping the above points in mind, we solve the following optimization problem in a distributed setting:  minimize w   2m  m? i=1  Cyi`(w; xi, yi) + ??w?1 (1)  where ` : Rn?Rn?R? R+ is the non-negative convex loss function, w is the model parameter that we wish to learn; (xi, yi) are example-label pair and ? ? ?1 denotes L1 norm.

Cyi denotes the cost associated with the label yi. We wish to solve (1) in binary classification setting in a distributed environment.

The last point we would like to mention is that most of the work in the literature evaluate the learning algorithms on a metric different from the one which they have been trained on [8]. This problem was recently tackled in [6] but in a different setting namely, cost-sensitive SVM. To achieve good classification performance, learning algorithms should train classifiers by optimizing the concerned performance measures [9]. For example, using accuracy as a class- imbalance measure may be misleading. Suppose there are 99 negative examples and 1 positive example. Our objective is to classify each positive example correctly. Now if a classifier classifies each example as negative will have an accuracy of 99% which is incorrect because our goal was to detect the positive example. Hence there have been alternative performance metrics to assess different methods for class- imbalance problem. Among them are the recall, precision, F- measure, ROC(receiver operating characteristics), and Gmean [10] (which stands for the geometric mean). It is found that Gmean is robust to class-imbalance problem [10]. Therefore, we use Gmean as our main performance measure as well as our optimization function within the problem (1).

Along the line of solution methodologies, we adopt the class-imbalance learning approach to solving the point anomaly detection problem. We argue that the class- imbalance learning problem is similar to the point anomaly detection problem and therefore, can be used to solve the point anomaly detection. Some works that have followed this approach include [11][12][4][3][13] and discussed in detail in [14].

In summary, our contributions are as follows:-  1) We present, to the best of our knowledge, the first regularized cost-sensitive learning problem in (1) in a distributed setting exploiting sparsity. The problem (1) is solved via Distributed Alternating Direction  Method of Multiplier (DADMM) by example split- ting across different processing nodes. This opera- tion splits the problem (1) into two subproblems: a distributed L2 regularized loss minimization and a L1 regularized squared loss minimization. The first subproblem is solved by L-BFGS as well as random coordinate descent (RCD) method while the second subproblem is just a soft-thresholding operation obtained in a closed-form. We call our algorithm using L-BFGS method as L- Distributed Sparse Class-Imbalance Learning (L-DSCIL) and us- ing random coordinate descent method as R-DSCIL.

2) Both L-DSCIL and R-DSCIL are tested over various benchmark datasets and results are compared with the start-of-the-art algorithm (CSFSOL) as well as a centralized solution over various performance mea- sures besides Gmean (defined in (2)).

3) We also show (i) the speedup, (ii) the effect of varying cost, and (iii) the effect of the number of cores in the distributed implementation.

4) At the end, we show a useful real-world application of DSCIL algorithm on KDDCUP 2008 data set.

In summary, the paper is divided into five sections.

Section 2 discusses the related work and background. The algorithmic framework is given in section 3. Section 4 presents the experimental results on benchmark data sets.

Real world application of our algorithms is shown in section 5. Finally, section 6 concludes the paper.

2 RELATED WORK AND BACKGROUND The work presented in the current paper aims at solving the class-imbalance learning problem in a distributed setting.

Hence, we flesh out the relevant works that have addressed the aforementioned problem. As discussed in section 1, the class-imbalance learning problem is ubiquitous in the data-laden domain and many attempts have been made to ameliorate this problem from four different perspectives.

Following sections describe them in detail along with their drawbacks.

2.1 Sampling-Based Methods  In the sampling-based methods for class-imbalance learn- ing, either majority examples are undersampled or minority examples are oversampled. The key idea is to balance the distribution of different classes so that classifiers available off-the-shelf can be used directly. Some examples of sam- pling based technique are SMOTE [15], SMOTEBoost [16], AdaBoost.NC [17], OOB/UOB [3] and so on. As mentioned before, sampling based methods are prone to the loss of information or overfitting without proper tuning of parameters [18]. Further, none of the above methods have been used in the distributed setting nor do they exploit sparsity present in the Big data.

2.2 Ensemble-Based Methods  Ensemble-based methods for class-imbalance learning build different classification models and combine their predic- tion in some way. For example, OOB/UOB method in [3]    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 3  is both sampling and ensemble-based method. The key idea behind ensemble-based methods is to modify bagging and boosting algorithms and tailor to the needs of class- imbalance problems. An SVM-based ensemble method for rare class detection is given in [19]. Negative Correlation Learning (NCL) based ensemble for class-imbalance prob- lem appears in [20] (see chapter 3 and references therein).

Despite the huge success of ensemble-based methods for class-imbalance learning, they suffer from high training time and poor scalability over high dimensional data [21].

2.3 One-class Classification-Based Methods One-class classification-based methods train the model over majority examples and data points that do not fit the trained model are classified as the minority class. Relevant work that are based on the idea mentioned thereof are one-class SVM [5] and its various extensions [22, 23], one-class Kernel Fisher Discriminants [24], one-class classification with Gaus- sian process [25], support-vector data description (SVDD) [26], etc. Many of the one-class classification-based methods use a kernel function to find the non-linear decision bound- ary. It is a well-known fact in literature that kernel-based classifiers are hard to train within reasonable time-limits on Big data due to working on large Gram matrix (also known as kernel matrix).

2.4 Cost-Sensitive Learning-Based Methods As mentioned previously, cost-sensitive learning-based methods assign different costs to different classes for han- dling the class-imbalance. Cost-sensitive learning was first proposed by Elkan [27] and subsequent works in the lit- erature have successfully applied it to the class-imbalance problem (see, for example, [2, 28] and references therein).

Cost-sensitive learning can be further categorized into the offline cost-sensitive learning (OffCSL) and the online cost- sensitive learning (OnCSL). OffCSL incorporates costs of misclassification into the offline learning algorithms such as cost-sensitive decision tress [29], [30], [28], cost-sensitive multi-label learning [31], cost-sensitive naive Bayes [32], etc. On the other hand, OnCSL-based algorithms use cost- sensitive learning within the Online learning framework.

Notable work in this direction includes the work of Jialei et al. [12], Adacost [33], SOC [11] (Sparse Online Classi- fication). It is to be noted that the cost-sensitive learning methods have been proved to outperform the sampling- based methods over Big data [7]. On the other hand, OnCSL- based methods are more scalable over high dimensions when compared to their counterpart OffCSL-based methods due to the processing of one sample at a time in case of the former.

Because of the above reasons, we propose to solve the problem (1) within the cost-sensitive learning framework.

Although our solution approach comes under OffCSL, it can generalize well over Big data due to (i) its distributed nature (ii) ability to handle high dimension data. The only competitive algorithm to process massive data for sparse class-imbalance learning is recently proposed by Dayong et al. [11]. The SOC algorithm of [11], although scalable to high dimensions, is a centralized solution. Due to the distributed nature of Big data, their SOC algorithm can not be applied  as such. To overcome the limitations of SOC, we have made the first attempt to handle the sparse class-imbalance in a distributed setting.

The work of Otey et al. on fast distributed outlier detec- tion in the mixed-attribute data is given in [34]. Essentially, their algorithm is based on links which they use to compute the anomaly score. A data point with no links has the highest anomaly score. The computation of links is based on the idea of association rule mining which is computationally expen- sive for Big data and on a data set that has only continuous attributes, their algorithm can not be applied. Secondly, they have demonstrated experiments on low dimension data only (KDDCup?99 has only 45 attributes). In the present work, we consider continuous attributes only where the number of attributes goes into millions. In [35], the author propose two- tier network anomaly detection which uses the Na??ve Bayes and KNN-based classification model to classify imbalanced data. The main limitation of [35] is that it does not account for the cost of misclassification specifically. Secondly, their proposed method is a centralized solution. Recently, [36] proposes the distributed minimum volume elliptical PCA algorithm which they solve via ADMM. However, they test their algorithm on low dimension dataset and did not exploit sparsity of the Big data explicitly or implicitly.

In the next section, we describe the assumptions made in our current work and algorithmic framework. Specifically, how do we incorporate the evaluation metric directly into the problem (1) with some modification?

3 PROBLEM FORMULATION AND ALGORITHMIC FRAMEWORK First, we fix some notation for the ease of exposition. In- put data is denoted as instance-label pair {xi, yi}, where i = 1, 2, ...,m, xi ? X ? Rd and yi ? Y ? {?1,+1}. We consider linear functional of the form f(x) = wTx, where w is the weight vector. The reason behind considering linear classifiers is that in high dimensions, the meaning of nearest neighbor becomes vacuous and linear classifiers are easy to work with [37]. Let y?i be the prediction for the ith instance, i.e., y?i = sign(fi(xi)), whereas the value |fi(xi)|, known as ?margin?, is used as the confidence of the learner in the ith  prediction step.

We make the following two assumptions to solve prob-  lem (1) efficiently.

Assumption 1. The loss function ` : Rn ? R is L- smooth,  if its gradient is L-Lipschitz, i.e.,  ??`(x)??`(y) ? L?x? y?.

Where ?`(?) denotes the gradient of the loss function ` with respect to x and the norm ? ? ? used is a general norm. The parameter L is called the lipschitz constant.

Assumption 2. Cost Cyi appearing in problem (1) is known  or user supplied.

3.1 Problem Formulation  We wish to solve problem (1) in a binary classification set- ting. In binary classification problem, there are two classes: positive and negative. Positive class is labeled as +1 and is    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 4  the minority class. Negative class is labeled as ?1 and is the majority class. We denote true positive by Tp, true negative by Tn, false positive by Fp, and false negative by Fn. These are defined as follows: Tp = {y = y? = +1}, Tn = {y = y? = ?1}, Fp = {y = ?1, y? = +1}, Fn = {y = +1, y? = ?1}.

To incorporate the performance metric Gmean directly into the optimization problem (1), Gmean is defined as:  Gmean = (sensitivity ? specificity)1/2 (2)  where sensitivity and specificity are defined as:  sensitivity = Tp  Tp + Fn , specificity =  Tn Tn + Fp  (3)  In class-imbalance learning, we want the learning algo- rithm to perform equally well both on the majority class as well as the minority class. Gmean is such a metric that gives equal weighting to both the classes. Therefore, we will maximize the Gmean. The lemma 1 can be attributed to [38] and will help us to maximize the Gmean.

Lemma 1. Maximizing Gmean as given in (2) is equivalent  to minimizing the following objective:? yi=+1  N  P I(yifi(xi)<0) +  ? yi=?1  P ? Fn P  I(yifi(xi)<0) (4)  where P is the total number of positive examples and N is the total number of negative examples in the dataset.

Proof: Proof of lemma 1 can be found in [39].

3.2 Cost-Sensitive Loss Function  The objective function in (4) is hard to optimize due to two reasons: (1) it contains the indicator function which is non-convex (2) it requires the knowledge of the total number of false negatives(Fn) in the training data. Both of these problems are difficult to tackle. Hence, we suggest an alternative to deal with it. The first problem can be solved by using surrogate loss functions to the indicator function such as the hinge loss, the logistic loss, etc. The best known tight surrogate loss function for the indicator function is the hinge loss. To solve the second problem, we notice that the ratios N/P and (P?Fn)P serve as a balance factor for the classification. More specifically, when the number of nega- tive examples is large, it gives high weight (N/P is high) to the minority class and vice versa. Instead of assigning weights based on examples, we assign weights to positive and negative examples from outside, that is, it is a user- supplied input. This typically makes sense in some real- world settings. For example, doctors can determine how much it costs to diagnose a patient following a particular treatment procedure. Note that the cost Cyi is not the same as the regularization parameter (? controls the amount of regularization). Label dependent cost Cyi is similar to weighing the majority and minority examples differently. In our case, the domain expert can serve as a cost-determiner. It is noteworthy to mention that our proposed algorithm is not optimizing the cost but it is optimizing the Gmean metric.

Hence, we use a slightly modified version of the smooth hinge loss with cost-sensitive weights as shown below.

`(yifi(xi)) = Cyi max (0, 1? yifi(xi)) 2 (5)  where,  Cyi =  { C+ yi = +1 C? yi = ?1  where C+ and C? are the cost associated with the positive and negative classes respectively. The smooth version of the hinge loss makes the problem (1) a smooth minimization problem. It will help us to use off-the-shelf solvers such as L- BFGS and gradient methods in our distributed subproblems.

In the next section, we describe such an algorithm that utilizes the cost-sensitive loss function given in (5) with a sparsity-inducing penalty.

3.3 Distributed Alternating Direction Method of Multi- plier Framework  Since our aim is to be able to process large data sets with huge dimension, we split the data set across examples and each chunk of the data set is assigned to different processing nodes. To be able to solve the problem (1) under such a setting, we invoke the distributed alternating direction method of multiplier (DADMM) algorithm proposed by Boyd et al. [40]. We present the main idea behind DADMM below.

DADMM aims to solve the following optimization prob- lem:  F (w) := minimize w  m? i=1  `(yix T i w)? ?? ?  =f(w)  +??w?1 (6)  where, xi represents the ith row of the data matrix X , m denotes the total number of examples in the training set, and `(?) is defined in (5). For simplicity, we use the linear classifier f(x) = xTw for further exposition. Notice that the optimization problem in (1) can be easily cast into the optimization problem (6) by subsuming the cost Cyi into the loss function itself. We partition the data matrix X and label vector y into N blocks such that:  X =  ????? X1 X2  ...

XN  ????? , y = ????? Y1 Y2 ...

YN  ????? (7) where, Xi ? Rmi?d, Yi ? Rmi?1,  ?N i mi = m,?i =  1, ..., N . We assume that all mi?s are equal for simplicity , i.e., mi = m/N . This means that all nodes receive equal chunk of data. Thus, we can rewrite the problem (6) into consensus form (please see chapter 7 in [40]) as follows:  minimize w  N? i=1  `i(Yi ?Xiwi) + ??z?1  subject to wi = z,?i = 1, ..., N (8)  where ? denotes the Schur product and z ? Rd. The loss function is separable into its components (it can be written as the sum of losses over data points) and is well-defined by (5).

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 5  Algorithm 1: DSCIL Parameter: ? Initialize: wi = 0,?i = 1, ..., N, z = 0,u = 0 for t := 1, ..., T do  wt+1i = argmin wi  `i(Yi ?Xiwi) + ?  ?wi ? zt + ut?22  ?i = 1, ..., N  zt+1 = argmin z  ??z?1 + N?  ?z? w?t+1 ? u?t?22  ut+1i = u t i + w  t+1 i ? z  t+1  (9) end Output: wT+1  Using the method of multipliers, we can convert the constrained optimization problem (8) to an unconstrained optimization problem below:  L?(w1, ..,wN , z,v) = N? i  `i(Yi ?Xiwi) + ??z?1+  N? i  vi(wi ? z) + (?/2) N? i  ?wi ? z?22  where, vi?i = 1, ..., N , the components of v, is the dual variable, and ? is the ADMM penalty parameter. Using scaled form dual variable (see for example page 15 of [40]), ADMM iterations for the above problem are given in Algorithm 1. We call the Algorithm 1 as DSCIL which stands for the Distributed Sparse Class-Imbalance Learning.

In Algorithm 1, (??)t+1 denotes the average value of the model parameter. Thus, we see that DADMM splits the original problem into simpler subproblems. We wish to implement DSCIL in a distributed setting. Specifically, DSCIL consists of N L2 regularized cost-sensitive loss min- imization which can be solved over N processing nodes via MPI in parallel. In the second step, model parameters wi and u are aggregated to form the average parameter w? and u? via MPI Allreduce. z minimization is a L1 regularized quadratic minimization and can be obtained in a closed- form. It is solved over one node and the optimal value is broadcasted to all the other nodes via MPI Bcast. In the third step, we calculate the dual variable u independently in parallel over N nodes.

We discuss some key points related to the implemen- tation of DSCIL. First, the solution produced by the dis- tributed algorithm should match (or approximate) the so- lution of the centralized algorithm. Therefore, in the next section, we provide an algorithm that solves the problem (1) in a centralized manner. The second issue in design- ing distributed algorithms is that they should have low communication time over the network and low computation time. Concretely, L2 regularized minimization is the most time-consuming part of DSCIL. Therefore, in our imple- mentation, we use two algorithms for solving the L2 mini- mization problem. The first is based on the classic L-BFGS algorithm of Nocedal et al. [41, 42] and the second is based on the RCD algorithm of [44]. L-BFGS is a quasi-Newton method based on the second-order information. Due to the  Algorithm 2: CSSCD: Cost-Sensitive Stochastic Coor- dinate Decent  Initialize: w = 0 for t := 1, ..., T do  Sample a coordinate j uniformaly at random from {1, 2, ..., d}.

compute gradient gj = f ?(wj) using (6).

Soft-threshold S?/L(wj ? gj/L).

end Output: wT+1  formation of the hessian matrix, solving L2 minimization step is costly and the implementation provided by Nocedal does not support sparse hessian which is the case with our objective function and the data set. Therefore, we use first-order algorithms to solve the L2 minimization step.

Among the available solvers, gradient-based algorithms are a popular choice for smooth problems like our L2 minimiza- tion. However, gradient-based algorithms such as vanilla gradient descent, stochastic-gradient descent (SGD), etc. [43] are costly when evaluating gradient of a function in huge dimensions [44][45]. Therefore, we propose to solve the L2 smooth minimization problem via Randomized Coordinate Descent (RCD) algorithm. For more details on coordinate descent, algorithms see [44][45] and references therein.

In the following section, we describe the modified algo- rithms in detail. Both of these algorithms are based on the coordinate descent approach.

3.4 A Centralized Algorithm  As discussed above, the solution of a distributed algorithm should match the solution of a centralized algorithm. There- fore, we discuss some algorithms that are able to solve the L1 minimization problem given in problem (1) with some modifications.

Algorithms such as cost-sensitive SVM with L1 penalty can solve the problem 1. For example, L1-regularized-L2 loss SVM available in Liblinear1 can be modified to solve the cost-sensitive optimization problem 1. Internally, it uses the dual coordinate descent (DCD) algorithm of Hsieh et al.

[46]. Stochastic Coordinate Descent (SCD) algorithm of [47] is also a coordinate descent algorithm that solves the primal form of the problem as it appears in (1), but, without taking the cost of misclassification into account. In this section, we modify the SCD algorithm for solving the problem (1) that takes into account the cost for each class. The reason for choosing SCD over DCD is that it is fast, has a low- memory requirement, simple to implement, and parameter- free. The modified cost-sensitive SCD (CSSCD) algorithm is presented in Algorithm 2.

The first step in Algorithm 2 is choosing a coordinate uniformly at random; the second step is to compute the per- coordinate-wise gradient of the smooth part of the objective function as shown in (6). Per coordinate-wise gradient is defined as: gj = (1/m)  ? i:xi,j 6=0 f  ?(w), where f ?(w) is the gradient of the cost-sensitive loss function with respect  1. https://www.csie.ntu.edu.tw/ cjlin/liblinear    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 6  Algorithm 3: CSRCD: Cost-Sensitive Random Coordi- nate Decent  Initialize: w = 0 for t := 1, ..., T do  Sample a coordinate j uniformaly at random from {1, 2, ..., d} compute gradient gj = f ?(wj) using (6).

update wj using (11)  end Output: wT+1  to the jth coordinate. The third step is a soft-threshold operation defined as below:  S?(u) =  ????? u+ ?, u < ?? u? ?, u > ? 0, ?? ? u ? ?  L is the lipschitz constant of the gradient of the objective function f(?) and ? is a regularization parameter. For our objective function in (6) which has lipschitz continuous gradient as defined in Assumption 1, L = max(C+, C?).

In the next section, we provide another algorithm that can scale over huge dimensions and is based on the random coordinate descent algorithm in [44].

3.5 Cost-Sensitive Random Coordinate Descent Algo- rithm  Due to the non-scalability of L-BFGS algorithm over huge dimensions as mentioned above, we seek an efficient solu- tion for the L2 subproblem in the DSCIL algorithm. It is based on the coordinate descent approach. Although there are many variants to choose a coordinate for an update such as cyclic, random, greedy, etc., we follow [44] and propose to solve the L2 minimization algorithm in DADMM algo- rithm using random update of coordinates. For the sake of completeness, we briefly describe the theory of random coordinate descent.

Suppose we want to minimize the following function:  F (w) := minimize w  ?(w) + ??w?1 (10)  Consider the prox-linear approximation of F (w) for each coordinate j of the model vector w:  wt+1j = argmin wj  < ??(wt?1j ), wj ? w t?1 j > +  Lj ?wj ? wt?1j ?   + ??wj?1 (11)  where ??(?) denotes the gradient of the cost-sensitive loss function and Lj is Lipschitz constant of the jth coordinate.

Based on the above approximation, we present the Cost- Sensitive Random Coordinate Descent (CSRCD) algorithm as shown in Algorithm 3. The main difference between CSSCD and CSRCD is that CSSCD uses Lipschitz constant of the loss function ?(?) while CSRCD uses per coordinate- wise Lipschitz constant. This renders the CSRCD a faster convergence rate than CSSCD [44].

3.6 Convergence Analysis of the DSCIL Algorithm DSCIL algorithm is based on the distributed ADMM framework. It is known in the literature that centralized ADMM converges linearly for objective functions contain- ing strongly convex term [48]. On the other hand, the most time consuming part of the DSCIL algorithm is solving the L2 regularized smooth subproblem, which we solve via L- BFGS algorithm and RCD algorithms. Both of these algo- rithms have been proved to converge linearly on uniformly and strongly convex problems respectively in [42] and [44].

However, there is a limited study on the convergence rate of the distributed ADMM such as [49]. In [49], it is shown that the distributed ADMM has linear convergence rate for strongly convex objective. Therefore, we conclude that DSCIL has linear convergence rate.

4 EXPERIMENTS In this section, we demonstrate the empirical performance of the proposed algorithm DSCIL over various small and large-scale data sets 2. All the algorithms were implemented in C++ compiled by g++ and run on a Linux 64-bit machine containing 48 cores (2.4Ghz CPUs) 3.

4.1 The Dataset A brief summary of the benchmark data sets and the class- imbalance ratio is given in Table 1. The last column in the table shows the class-imbalance ratio. We describe the data sets used in the experiment below.

? Page blocks dataset consists of blocks of the page  layout of a document. The block can be one out of the five block types: (1) text (2) horizontal line (3) pictures (4) vertical line (5) graphic. Each block is produced by a segmentation process. Some of the features are height, length, area, blackpix, etc. We converted the multi- class classification problem into binary classification problem by changing the labels of horizontal lines by the positive class (+1) and rest of the labels to the negative class (-1). The task is to detect the positive class (anomaly) efficiently. Note that positive class is only a small fraction (6%) of the total data.

? W8a is a dataset of keywords extracted from a web page and each feature is a sparse binary feature. The task is to classify whether a web page falls into a category or not.

? Covtype dataset contains information about the type of forest and associated attributes. The task is to predict the type of forest cover from cartographic variables (no remotely sensed images). The cartographic variables were derived from data obtained from US Geological Survey and USFS data. Some of the features include El- evation, Aspect, Slope, Soli type, etc. Covtype is multi- class classification dataset. To convert the multi-class dataset to binary class dataset, we follow the procedure given in [51]. In essence, we treat class 2 as the positive class and other 6 classes as negative class.

2. These data sets are available at csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/binary.html  3. Sample code and data set for R-DSCIL can be downloaded from https://sites.google.com/site/chandreshiitr/publication    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 7  TABLE 1: Summary of data sets used in the experiment  Dataset Balance # Train #Test #Features Sparsity(%) #Pos:Neg news20 False 6,598 4,399 1,355,191 99 .9682 1:6.0042 rcv1 True 10,000 10,000 47,236 99.839 1 : 1.0068 url False 8,000 2,000 3,231,961 99.9964 1:10.0041 realsim False 56,000 14,000 20,958 99.749 1:1.7207 gisette False 3,800 1,000 5,000 0.857734 1:11.667 webspam False 8,000 2,000 16,609,143 99.9714 1:15 w8a false 40,000 14,951 300 95.8204 1:26.0453 ijcnn1 false 48,000 91,701 22 39.1304 1:9.2696 covtype false 2,40,000 60,000 54 0 1:29.5344 pageblocks false 3,280 2,189 10 0 1:11.1933 Magic False 15,200 3,820 10 0 1:1.84  ? ijcnn1 dataset consists of time-series samples produced by 10-cylinder internal combustion engine. Some of the features include crankshaft speed in RPM, load, accel- eration, etc. The task is to detect misfires (anomalies) in certain regions on the load-speed map.

? Magic dataset comprises of simulation of high energy gamma particles in a ground-based gamma telescope.

The idea is to discriminate the action of primary gamma (called signal) from the images of hadronic showers caused by cosmic rays (called background). The actual dataset is generated by the Monte Carlo Sampling.

? News20 dataset is a collection of 20,000 news- group posts on 20 topic. Some of the topics in- clude comp.graphics, sci.crypt, sci.med, talk.religion, etc. Original news20 dataset is a multi-class classi- fication dataset. However, Chih-Jen Lin et al. have converted the multi-class dataset into the binary class dataset and we use that dataset directly.

? rcv1 dataset is a benchmark collection of newswire stories that is made available by Reuters, Ltd. Data is organized into four major topics: ECAT (Economics), CCAT (Corporate/industrial), MCAT (Markets), and GCAT (Government/Social). Chih-Jen Lin et al. have preprocessed the dataset and assume that ECAT and CCAT denote the positive category whereas MCAT and GCAT designate the negative category.

? Url dataset is a collection of URLs. The task is to detect malicious URLs (spam, exploits, phishing, DoS, etc.) from the normal URLs. The author represents the URLs based on host-based features and lexical features.

Some of the lexical feature types are hostname, primary domain, path tokens, etc., and host-based features are WHOIS info, IP prefix, Connection speed, etc.

? Realsim dataset is a collection of UseNet articles from 4 discussion groups: real autos, real aviation, simulated auto racing, and simulated aviation. The data is often used in binary classification separating real from simu- lated and hence the name.

? Gisette dataset was constructed from MINIST dataset.

It is a handwritten digit recognition problem and the task is to classify confusing digits. The dataset also appeared in NIPS 2003 feature selection challenge.

? Webspam dataset contains information about web pages. There exists the category of web pages whose primary goal is to manipulate the search engines and web users. For example, phishing site is created to duplicate the e-commerce sites so that the creates of  the phishing site can divert the credit card transaction to his/her account. To combat this issue, web spam corpus was created in 2011. The corpus consists of approximately 0.35 million web pages; each web page represented by bag-of-words model. The dataset also appeared in Pascal Large-Scale Learning Challenge in 2008 [50]. The task is to classify each web page as spam or ham. The challenge comes from the high dimension- ality and sparse features of the dataset.

4.2 Experimental Testbed and Setup  For our distributed implementation, we use MPICH2 library as done in [40]. We compare our DSCIL algorithm with a recently proposed algorithm called CSFSOL of [11]. CSFSOL is a cost-sensitive online algorithm based on mirror descent update rule. CSFSOL also optimizes the same objective func- tion as ours but the loss function is not smooth. Secondly, as mentioned in the introduction section, CSFSOL is an online centralized algorithm whereas DSCIL is a distributed algorithm. Besides, in [11], it has been shown that CSFSOL outperforms many of the recent online algorithms such as STG [52], FOBOS [53], CPA [54], PAUM [55], etc. Hence, we compare our proposed algorithm only with the CSFSOL algorithm on account of fair comparison. It is inappropriate to judge its performance across the class of algorithms, no attempt has been made to do so.

In the forthcoming subsections, we will show (i) The convergence of DSCIL on benchmark data sets (ii) The performance comparison of DSCIL, CSFSOL, and CSSCD algorithms in terms of accuracy, sensitivity, specificity, Gmean and balanced accuracy (also called Sum which is =0.5*sen- sitivity+0.5*specificity). Note that within the DSCIL algo- rithm, L2 minimization solved via L-BFGS algorithm is referred to as L-DSCIL and via CSRCD algorithm is referred to as R-DSCIL.

Note that the DSCIL algorithm contains ADMM penalty parameter ?which we set to 1 and the convergence of DSCIL does not require tuning of this parameter. Regularization parameter ? in DSCIL is set to 0.1?max where ?max is given by (1/m)?XT y??? (see [56]), where y? is given by:  y? =  { m?/m yi = 1  ?m+/m yi = ?1, i = 1, ...,m  Setting ? as discussed above does not require its tun- ing as compared to ? in CSFSOL where no closed form solution is available as such and one needs to do the    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 8  cross-validation. DSCIL algorithm is stopped when pri- mal and dual residual fall below the primal and dual residual tolerance (see chapter 3 of [40] for details). For the CSFSOL algorithm, we see that it contains another parameter, that is, the learning rate ?. Both of these parameters were searched in the range {3 ? 10?5, 9 ? 10?5, 3?10?4, 9?10?4, 3?10?3, 9?10?2, 0.3, 1, 2, 4, 8} and {0.0312, 0.0625, 0.125, 0.25, .5, 1, 2, 4, 8, 16, 32} respectively as discussed in [11] and best value on the performance metric is chosen for testing. As an implementation note, we normalize the columns of data matrix so that each feature value lies in [?1, 1]. All the results are obtained by running the distributed algorithms on 4 cores using MPI unless otherwise stated.

4.3 Empirical Convergence Analysis of DSCIL In this section, we discuss the convergence of L-DSCIL and R-DSCIL algorithms . The convergence plot with respect to the DADMM iteration over various benchmark data sets is shown in Figure 1. From the Figure 1, it is clear that L-DSCIL converges faster than R-DSCIL which is obvious as L-DSCIL is a second order (quasi-Newton) method while R-DSCIL is a first order method. We leave the theoretical analysis of L-DSCIL and R-DSCIL for future work.

4.4 Performance Comparison with respect to Gmean In this section, we show the performance comparison of var- ious algorithms compared with respect to various metrics such as Sensitivity, Specificity, Gmean and Sum in Table 2. A higher value of all of the above metrics indicates the better performance. Here, we want to focus on Gmean column as this is the metric of our interest. Now, several conclusions can be drawn. Firstly, L-DSCIL performance is equally good compared to the centralized solution of CSSCD on many of the data sets. For example, it gives superior performance than CSSCD on news, gisette, rcv1, realsim, and pageblocks data sets. Next, the performance of R-DSCIL is not so good compared to CSSCD. It is able to outperform CSSCD on realsim and webspam data sets only and Gmean achieved by CSFSOL on large-scale data sets such as news, rcv1, url, and webspam is higher than any of the other method. This observation can be attributed due to possibly: (i) the use of strongly convex objective function used in our present work compared to convex but non-smooth objective function in CSFSOL, (ii) We stopped the L-DSCIL algorithm before it reaches optimality, and (iii) ?max value calculated as discussed in subsection 4.2 is not the right choice. We believe that the second and third reason is more likely than the first one as can be seen in the convergence plot of the L-DSCIL algorithm in Figure 1. We stopped the L-DSCIL algorithm either primal and dual residual went below the primal and dual feasibility tolerance or maximum iteration is reached (which we set to 20 for large-scale data sets). To verify our point, we ran another experiment with MaxIter set to 50.

This setting gives the Gmean 0.923436 which is clearly larger than previously obtained value 0.916809 for rcv1 data set.

Similarly, by running L-DSCIL and R-DSCIL for a larger number of iterations, we can increase the desired accuracy ?.

Finally, we also observe that R-DSCIL fails to capture class- imbalance on gisette and covtype whereas CSSCD on covtype  and CSFSOL on realsim (due to the value of Gmean being 0) which clearly indicates the possibility of using L-DSCIL in practical settings.

4.5 Study on Gmean versus Cost In this subsection, we study the effects of varying the cost on Gmean. The results are presented in Figure 2 and 3 for L-DSCIL and R-DSCIL respectively. In each Figure, cost and Gmean appear on x and y-axis respectively. From these Figures, we can draw the following observations. Firstly, Gmean is increasing for balanced data such as rcv1 when the cost for each class equals 0.5. On the other hand, the more imbalanced the data set is, the greater the cost given to the positive class and the higher the Gmean (see the hist plot for news, pageblocks, url) in Figure 2. The same observation can be made from Figure 3. These observations allude to the fact that the right choice for cost is important; otherwise, classification is affected severely.

4.6 Speedup Measurement We discuss the speedup achieved by R-DSCIL and L-DSCIL algorithms when we run these algorithms on multiple cores.

Speedup results of R-DSCIL algorithm are presented in Figure 5 (a) and (b) and of L-DSCIL in Figure 4 (a) and (b). The number of cores used is shown on the x-axis and the y-axis shows the training time. From these figures, we can draw multiple conclusions. Firstly, from the Figure 5 (a), we observe that as we increase the number of cores, training time is decreasing for all the data sets except for webspam at 8 cores. The sudden increase in training time of webspam could be explained as follows: so long as computation time (RCD running time plus primal and dual variable update) remains above the communication time (MPI Allreduce operation), adding more cores reduces the overall training time. On the other hand, in Figure 5 (b), training time is first increasing and then decreasing for all the data sets with an increasing number of cores. This could be due to increasing communication time with the increasing number of cores.

After a certain number of cores, computation time starts leading the communication time and thereby decrease in the training time is observed. Further, in DSCIL algorithm, the communication time is data dependent (it depends on the data dimensionality and sparsity). Because of these reasons, we observe different speedup patterns on different data sets.

Speedup results of L-DSCIL in Figure 4 (a) and (b) show the decreasing training time with increasing number of cores for all the data sets. We also observe that training time for L-DSCIL is higher than that of R-DSCIL on all data sets and cores.

4.7 Number of Cores versus Gmean In this section, we present the experimental results showing the effect of utilizing a varying number of cores on Gmean.

This also shows how different partitioning of data affects the Gmean. We divide the data into equal chunks and distributed to various cores. For example, if we have a data set of size m and want to utilize n cores, we will allot samples of size m/n to each core. We choose the data size such that it is divisible by all possible cores utilized. Results    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 9   0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1  L-DSCIL  R-DSCIL  DADMM Iterations  O b  je ct  iv e  f u  n ct  io n  v a  lu e  s  (a) ijcnn1   0.02  0.04  0.06  0.08  0.1  0.12  0.14  L-DSCIL R-DSCIL  DADMM Iterations  O b  je ct  iv e  f u  n ct  io n  v a  lu e  s  (b) rcv1   0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  L-DSCIL  R-DSCIL  DADMM Iterations  O b  je c ti v e  f u  n c ti o  n v  a lu  e s  (c) pageblocks   0.01  0.02  0.03  0.04  0.05  0.06  0.07  L-DSCIL  R-DSCIL  DADMM Iterations  O bj  ec tiv  e fu  nc tio  n va  lu es  (d) w8a  Fig. 1: Objective Function vs DADMM iterations over benchmark data sets. (i) ijcnn1 (ii) Magic (iii) pageblocks (iv) w8a .

TABLE 2: Performance comparison of CSFSOL, L-DSCIL, R-DSCIL and CSSCD over various benchmark data sets  Algorithm news rcv1Sensitivity Specificity Gmean Sum Sensitivity Specificity Gmean Sum CSFSOL 0.982759 0.966137 0.974412 0.974448 0.960148 0.953312 0.956724 0.95673 L-DSCIL 0.568966 0.997927 0.753516 0.783446 0.925116 0.908578 0.916809 0.916847 CSSCD 1 0.444137 0.666436 0.722069 0.871548 0.930945 0.900757 0.901246 R-DSCIL 0.396552 0.999079 0.629433 0.697815 0.792215 0.960478 0.872299 0.8763465  Algorithm url webspamSensitivity Specificity Gmean Sum Sensitivity Specificity Gmean Sum CSFSOL 0.994505 0.970297 0.982327 0.982401 0.952 0.9952 0.97336 0.9736 L-DSCIL 0.824176 0.949945 0.884829 0.8870605 0.528 1 0.726636 0.764 CSSCD 0.967033 0.944994 0.95595 0.956014 0.808 1 0.898888 0.904 R-DSCIL 0.791209 0.959296 0.871208 0.8752525 0.968 0.879467 0.922672 0.923733  Algorithm gisette realsimSensitivity Specificity Gmean Sum Sensitivity Specificity Gmean Sum CSFSOL 0.896 0.73 0.808752 0.813 0 1 0 0.5 L-DSCIL 0.926 0.982 0.953589 0.954 0.900304 0.800324 0.848843 0.850314 CSSCD 0.918 0.972 0.944614 0.945 0.33617 0.986969 0.576012 0.66157 R-DSCIL 0 1 0 0.5 0.90152 0.814002 0.856644 0.857761  Algorithm ijcnn1 w8aSensitivity Specificity Gmean Sum Sensitivity Specificity Gmean Sum CSFSOL 0.607668 0.855475 0.721002 0.731571 0.0330396 1 0.181768 0.51652 L-DSCIL 0.576217 0.89877 0.719643 0.737493 0.682819 0.984755 0.820006 0.833787 CSSCD 0.827824 0.835267 0.831537 0.831546 0.563877 0.9851 0.745302 0.774489 R-DSCIL 0.982438 0.690525 0.823649 0.836482 0.348018 0.998344 0.589442 0.673181  Algorithm covtype pageblocksSensitivity Specificity Gmean Sum Sensitivity Specificity Gmean Sum CSFSOL 0.86642 0.910199 0.88804 0.888309 0.662069 0.81306 0.73369 0.737564 L-DSCIL 0.699578 0.961897 0.820318 0.830737 0.706897 0.95208 0.820379 0.829488 CSSCD 0 1 0 0.5 0.751724 0.907846 0.826105 0.829785 R-DSCIL 0 1 0 0.5 0.344828 0.993681 0.585362 0.669254    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 10  1 2 3 4 5  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   pageblocks  url  news  rcv1  Cost (positive, negative)  G M  e a  n  (a)  1 2 3 4 5  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  w8a ijcnn1 webspam realsim  Cost(positive,negative)  G m  e a n  (b)  Fig. 2: Gmean versus Cost over various data sets for L-DSCIL algorithm. Cost {negative,positive} is given on the x-axis where each number denotes cost pair such that 1={0.1,0.9}, 2={0.2,0.8}, 3={0.3,0.7}, 4={0.4,0.6}, 5={0.5,0.5}  1 2 3 4 5  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  pageblocks w8a ijcnn1 realsim  Cost(positive,negative)  G m  e a  n  (a)  1 2 3 4 5  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   rcv1  url  webspam  news  Cost(positive,negative)  G m  ea n  (b)  Fig. 3: Gmean versus Cost over various data sets for R-DSCIL algorithm. Cost {negative,positive} is given on the x-axis where each number denotes cost pair such that 1={0.1,0.9}, 2={0.2,0.8}, 3={0.3,0.7}, 4={0.4,0.6}, 5={0.5,0.5}  1 2 4 8       w8a ijcnn1 url news  # Cores  T ra  in in  g t  im e  ( in  s e  c o  n d  s )  (a)  1 2 4 8        rcv1 realsim webspam  # Cores  T ra  in in  g t  im e  ( in  s e  c d  o n  d s )  (b)  Fig. 4: Training time versus number of cores to measure the speedup of L-DSCIL algorithm. Training time in both the figures is on the log scale.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 11  1 2 4 8     news realsim rcv1 ijcnn1  # Cores  T ra  in in  g t  im e  ( in  s e  co n  d s)  (a)  1 2 4 8      w8a url webspam  # Cores  T ra  in in  g t  im e  ( in  s e  co n  d s)  (b)  Fig. 5: Training time versus number of cores to measure the speedup of R-DSCIL algorithm. Training time in Figure (a) is on the log scale.

1 2 4 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   news realsim rcv1 ijcnn1  # Cores  G m  e a  n  (a)  1 2 4 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   w8a url webspam  # Cores  G m  e a  n  (b)  Fig. 6: Effect of varying number of cores on Gmean in R-DSCIL algorithm.

are shown in Figures 6 and 7 for R-DSCIL and L-DSCIL respectively. In Figure 6 (a) and (b), we can observe that Gmean remains almost constant over various partitioning of the data (various cores). A small deviation is observed over url and w8a data sets in Figure 6 (b). These observations lead to the conclusion that R-DSCIL algorithm is less sensitive to the data partition. On the other hand, Gmean result from L- DSCIL algorithm for various partitions in Figures 7 (a) and (b) shows the chaotic behavior. For example, Gmean changes a lot with the increasing number of cores for news data set in Figure 7 (a) and for realsim in Figure 7 (b). For other data sets, fluctuations in Gmean values are less sensitive.

5 APPLICATIONS In this section, we demonstrate the applicability of our proposed algorithm for anomaly detection in X-ray images of KDDCup 2008 data set. KDDCup 2008 data set con- tains information on 102294 suspicious regions, each region described by 117 features. Each region is either ?benign? or ?malignment? and the ratio of malignment to benign regions is 1:163.19. We split the training data set into 5 chunks. First four chunks have size 20000 candidates each and are used for training on 4 cores. The last chunk has size 22294 and used for testing. We compare the performance of  R-DCSIL with CSFSOL and CSOGD-I that was proposed in [12]. It is shown in [12] that CSOGD-I outperformed many first-order algorithms such as ROMMA, PA-I, PA-II, PAUM, CPA PB, etc (see Table 6 in [12]). Hence, we only compared with CSFSOL and CSOGD-I in our experiments. We note that CSFSOL and CSOGD-I are online algorithms that do not require separate training set and test set. We set the learning rate in CSOGD-I to 0.2 as suggested in the original study and reproduced the results. What is important here is that the ratio of malignment to benign regions in our test set is 1:123 which is not very less than the ratio of malignment to benign in the original data set. Comparative performance is reported in Table 3 with respect to the Sum metric as defined in the Experiment section. From the results reported in Table 3, we can clearly see that the Sum value achieved by R-DSCIL is much larger than the value obtained through CSOGD-I. These observations indicate the possibility of using R-DSCIL in real-world anomaly detection tasks.

6 CONCLUSION In the present work, we propose algorithms for handling the class-imbalance in a distributed environment on small and large-scale data sets. DSCIL algorithm is implemented in two flavors: one uses the second order method (L-BFGS)    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688372, IEEE Transactions on Big Data  JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 12  1 2 4 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   w8a ijcnn1 url news  # Cores  G m  e a  n  (a)  1 2 4 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   rcv1 realsim webspam  # Cores  G m  e a  n  (b)  Fig. 7: Effect of varying number of cores on Gmean in L-DSCIL algorithm.

TABLE 3: Performance evaluation of R-DSCIL and CSOGD-I on KDDCUP 2008 data set.

Algorithm Sum CSOGD-I 0.5741092 CSFSOL 0.71089 R-DSCIL 0.7336652  and the other uses the first order method (RCD) to solve the subproblem in DADMM framework. In our empiri- cal comparison, we showed the convergence results of L- DSCIL and R-DSCIL where L-DSCIL converges faster than R-DSCIL. Secondly, Gmean achieved by L-DSCIL approxi- mates the Gmean of a centralized solution on most of the data sets. Whereas Gmean achieved by R-DSCIL varies due to its random updates of coordinates. Thirdly, coming to the training time comparison, we found in our experiments that R-DSCIL has cheaper per iteration cost but takes longer time to achieve ? accuracy compared to L-DSCIL algorithm.

Finally, the effect of varying cost and the number of cores on Gmean is also demonstrated. The empirical comparison shows the potential application of L-DSCIL and R-DSCIL for real-world class-imbalance and anomaly detection tasks where our algorithms outperform some recently proposed algorithms.

As our future work, we plan to learn cost jointly with the model parameter instead of providing it manually. We leave the theoretical analysis of the proposed algorithms for future.

