Fourth International ConfPme  on knauledge-Based Intelligent Engineering Systems b Allied Technologres, 30" Aug-I" Sept 2000. Brighton,UK

Abstract Genetic programming (GP) usual1-v has a wide search space and a highj7exibility. So. GP may search for global optimum solution. But, in general, GP 's learning speed is not so fast. Apriori Algorithm is one of association rule algorithms. It can he applied to 1aTe database. But, It is dzjicult to define itsparameters without experience. We propose a rule generation technique from a database using GP combined with association rule algorithm. It takes rules generated bv the association rule algorithm as initial individual of Gfl The learning speed of GP is improved by the combined algorithm. To verih the eflectiveness of the proposed method, we apply it to the decision tree construction problem from UCI Machine Learning Repository. We compare the result of proposed method with prior ones.

1 Introduction  Various techniques are proposed for the construction of the decision tree using classification learning. In general, the learning speed of a system using a ge- netic programming is slow. Moreover, both prob- lem background knowledge and design skill are de- manded of the system designer. However, a learning system which can acquire higherader knowledge by adjusting to the environment can be constructed, be- cause the structure is treated at the same time. On the other hand, there is the Apriori algorithm as a rule generating techruque from a large database. It is one of association rule construction algorithms. The Apriori algorithm uses two values: a support value and a confidence value. Depending on the setting of each index threshold. the search space can be re- duced, and the number of association rule candidate can be increased. However, the experience is needed for setting an effective threshold.

Each technique has advantages and disadvantages like this. In this paper, we propose the decision tree construction technique ftom database using genetic programming combined with association rule algo- rithms. By the learning method of the combined rule generation technique, it is expected the construction of the system which can search for a flexible decision tree from a large database.

To verify the effectiveness of the proposed method.

0-7803-6400-7/00/$10.00 02000 IEEE I46  we discuss the learning method by genetic program- ming combined with the association rule construc- tion method by the Apriori algorithm and the auto- matically defined function technique. We apply it to the decision tree construction problem from UCI Ma- chine Learning Repository. We compare the result of proposed method with prior ones.

2 Algorithm of Association Rule Construction  In the association rule analysis, each case generally targets the transaction form data. The transaction form data is item set as a minimum unit of the data description. CO-occurence pattem among item sets in the case is extracted as an association rule. (1)  is one of association rule samples. The association rule ex- presses as co-occurence pattem of the item set in the case data. 1' 21  B + H  R is condtion part of associahon rule.

I1 is conclusion part of associatlon rule  mailto:tazaki@intlab.toin.ac.jp    2.1 Apriori Algorithm  By the ordinary analysis technique, a lot of calcula- tion time needs to extract the association rule from a large database. The Apriori algorithm can extract the association d e  from a large database by achieving the high efficiency of the search in realistic time. In the Apriori algorithm, the candidate of the association rule is evaluated by using two values, support value and confidence value, while searching for the associ- ation rules. The support value of association rule R ( . , r ~ p (  R)) is defined as (2). The confidence value of association rule R (.sup( R ) )  is defined as (3).

. (  R U H) means a number of cases containing both R and H items, SV means a number of all cases.

TI (R U H) means a number of cases contammg both f? and U items, n[  n) means a number of cases contalnmg B items.

The Apriori algorithm, for support value and confi- dence value, is used minimum support value and min- imum confidence value as those thresholds. If a can- didate of association rule cannot fill these minimum values, we assume that its rule expresses low associ- ation. Then, its rule is excluded evaluation to reduce search space consecutively. Therefore, the Apriori al- gorithm can search faster than the other association rule analysis techniques.

By operating each minimum value, the candidate number of association rule can be increased or the range of the search space can be reduced. However, it is possible that an unexpected rule cannot be ex- tracted by reducing the range of the search space.

Moreover, the load of the expert who analyzes the rule increases when there are a lot of association rule candidates, and it is a possible that it becomes diffi- cult to search for a useful rule.

3 Genetic Programming  Genetic programming (GP) is a learning method based on the natural theory of evolution, and the flow of the algorithm is similar to genetic algorithm (GA).

The difference between GP and GA is that GP has extended its chromosome to allow structural expres- sion using function nodes and terminal nodes. F31 In  this paper, the tree structure is used to express the de- cision tree. Therefore, we express the decision tree by using each attribute value and the: class name as the terminal node and the condition sentence as the function node.

3.1 The Algorithm of GP  The decision tree construction with GP follows the following procedures.

1.

2.

3.

4.

3.2  An initial population is generated from a random gammar of the function nodes and the terminal nodes defined for each problem domain.

The fitness value, which relates, to the problem solving ability, for each individual of the GP population is calculated.

The next generation is generate?d by genetic op- erations.

(a) The individual is copied by fitness value  (b) A new individual is generated by intersec-  (c) A new individual is generated by random  (reproduction).

tion (crossover).

change (mu tat ion).

If the termination condition is met, then the pro- cess exits. Otherwise, the process repeats from the calculation of fitness value in step 2.

Automatically Defined1 Function  Ordinarily, there is no method of adequately con- trolling the growth of the tree, because GP does not evaluate the size of the tree. Therefore, dur- ing the search process the tree ma.y become overly deep and complex, or may settle to a too simple tree. There has been research on methods to have the program define functions itself for efficient use.

One of the approaches is automatic function defini- tion (or Automatically Defined Function: ADF), and this is achieved by adding the gene expression for the function definition to normal GP l4.1. By implement- ing ADF, a more compact program can be produced, and the number of generation cycles can be reduced.

More than one ADF can be defined in one individual.

Fourth Internotional Conference on knowledge-Based Intelligent Engineering Systems & Allied Technologies, 3@ Aug-I? Sept 2000, Brighton,UK  training all nodes data  4 Approach of Proposed Com- bined Learning  depths generations  To make up for the advantage and the disadvantages of the Apriori algorithm and GP, we propose the de- cision tree construction approach which is combined with each technique. By combining each techmque, it is expected searching for a flexible decision tree from a large database.

(Y?)  In the construction of the learning technique by the combination. we propose the following steps. At first.

the Apriori algorithm generates the association rule.

Next, the generated association rule is converted into the decision tree which takes into an initial individ- ual of GP. Finally, decision tree is constructed by GP learning. It is easy to contain the effective schema in an initial individual of GP. As a result, it is expected to improve of the GP?s learning speed and its classi- fication accuracy.

( Oh) 1  For conversion from the association rule to the deci- sion tree, we use the following procedures. At first process, the route of the decision tree is constructed with assuming the condition part of the association rule as the division attribute of the decision tree. Next process, the conclusion part of the association rule puts on the terminal node of tlus route. Finally, con- cerning the terminal nodes whch are not defmed by the association rule, they are assigned the candidates at random.

ADF-GP Apriori + ADF-GP  5 Apply to Decision Tree Con- struction from Database  98.0 92.9 9 2 235  To verify the valihty of the proposed method, we ap- plied the method to an evaluation experiment. For evaluation data, we used house-votes data from UCI Machine Learning Repository. [51 By using this eval- uation database, we compared the results of the pro- posed method with prior ones. The evaluation data contains 16 attributes and 2 classes. The attributes are for example ?handicapped-infants? and ??water- project-cost-sharing ? etc. They are expressed by 3 values: And the 2 classes are ?democrat? and ?republican ?. 50 cases out of the total 435 data of house-votes were used for training data. The following GP parameters were used. (Re- fer to table 1. )  ?y ?, ?n?, and ???.

At first, we extracted the association rule from the database by the Apriori algorithm. We applied the  Table 1: Parameters of GP  GP population i 500 Reorduction orobabilitv 1 0.1  Selection method Tournament method Function node IFEQ, ADFO, ADFl Tenninal node Attribute value of database  I I (16 attributes), y, n, ?, 1 dcmocratmublican Number of training data Number of test data IFEQ: ifequal to ( = I ADFO, ADFl: the function detinition gene expanded by ADF y, n, ?: yes(y), no@), others(?)  I democrat:3 1, republican: 19 I 435  Apriori algorithm to a data set excluding data with the ??? value, because ??? value means ?others?. In the following experiment, we experimented by using minimum support value = 30 and minimum c o d - dence value = 90. As a result of the experiment, 75 rules were generated.

Next, we constructed the decision tree from the database. For decision tree construction, we used learning by normal ADF-GP, and learning by ADF- GP which took the association rule as an initial in&- vidual. In this combined GP, 75 rules generated with the association rule were taken into the initial individ- ual. The result of the transition of each fitness value is shown in figure 1, and the result of best individual is shown in table 2.

By using normal ADF-GP, inference accuracy did not rise easily. However, the proposed method, showed fast learning, and achieved high accuracy. Compar- ing the best indwidual results, the proposed method showed better results than normal ADF-GP, except for accuracy against the training data. (See genera- tion 100 - 300 in figure l .  ) Concerning the training data results, normal ADF-GP may have shown over- fitting. But, its proof could not be obtained by the result.

Table 2: Experiment Best Individuals Result by Each Technique.

for Artificial Intelligence, Vol. 15' No. 1 ,pp. 187- 197,2000. (In Japanese)  [3] J. R. Koza. Genetic Programmulg. MIT Press, 1992.

[4] I. R. Koza, K. E. Kinner(ed.), et.al. Scalable Learning in Genetic Programming Using Auto- matic Function Definition. Advances in Genetic Programming, pp.99-117, 1994.

[5] C. L. Blake, C. J. Men. UCI Repository of ma-  edu/-mlearn/MLRepository.htmil], Irvine, CA: 100 200 300 400 500 chine learning databases. [http://www.ics.uci.

generations  Figure 1 : Transition of Each Fitness Value  6 Conclusions  In this paper. we proposed the decision tree con- struction technique kom the database using genetic programming combined with association rule algo- rithms. To verify the validity of the proposed method, we applied it to the house-votes data from UCI Ma- chine Learning Repository, and compared the results of proposed method and normal ADF-GP.

As a result, an improvement of learning efficiency was seen. It can be concluded that the proposed method is an effective method to the improvement of the learning efficiency of the decision tree generation system. Though the result of suppressing the overfit- ting was seen, we I d  not consider the analysis of its mechanism.

In the future, we will research the following 4 topics.

The fmt topic is to apply the method to other verifi- cations. The second topic is to discuss the conversion algorithm from the association rule to a decision tree with lugh accuracy. The thud topic is to extend the proposed method to multi-value classification prob- lems. The fourth topic is to study a theoretical analy- sis about the mechanism of the overfitting.

