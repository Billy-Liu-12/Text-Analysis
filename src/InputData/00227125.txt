TREATING WEIGHTS AS DYNAMICAL VARIABLES

ABSTRACT  The recall and learning dynamics of Artificial Neural Networks are described by means of a partial differential equation (PDE) that may incorporate weights either as parameters or variables. In the case that weights are interpreted as variables, a new type of neurodynamics is discovered: weights have to  obey second order differential equations called learning laws. Experiments on the association of time-varying patterns indicate the superiority of the learning law over the known types of learning rules. It is  also shown that a single first-order Hamilton-Jacobi 'parametric' PDE suffices to  derive the various neurodynamical paradigms used today [l-51.

INTRODUCTION  A popular approach to  the description of the learning dynamics of neural networks looks at the weights as being parameters that have to be fitted according to  some learning rule [1-6].

Moreover, weights are assumed to be time-invariant after learning. Since neural networks with their thousands or millions of weights run the danger of, firstly, over-fitting the experiments and, secondly, the use of parameters tends to  darken the actual dynamics of the system, we suggest to treat weights in the same manner as neurons, ie as dynamical variables of the system. Hence, 'dynamical' weights have to obey an ODE, like the neurons.

THE HAMILTONIAN CONCEPT AND ITS GEOMETRICAL INTERPRETATION  The basic variables for the dynamical description of a neural network are considered t o  be the time t, the neural states y and the weight states W. The change of the states is  influenced by two sources: external inputs and physical implementation. In this paper, dynamical effects caused by the physical implementation are not considered ( see [7], for example) . The external inputs may comprise reference signals for learning as well as actual input signals. Both types of inputs form the (generally time-varying) boundary conditions which control any change in the state space of the neural net. Besides, the action of the system invokes a set of observables J(t) each of which can be considered as the collective result of the states and the boundary conditions: JW: = J(t,y(t),W(t)).

Figure 1 depicts a trajectory J(t,y(t),W(t)). Changing the boundary conditions (for instance, by means of a new pattern input) or the initial states of the neural network one obtains a new trajectory. Imagining that all possible boundary conditions were experienced by the network, one obtains a surface J: = J(t,y,W) that represents the complete information about the dynamics of the neural network. Since beside t, y, W and J the partial derivatives of J with respect to  all i ts variables carry the knowledge of how the surface J(t,y,W) is formed it is natural t o  request that the surface J must obey an equation  D(t, y, W, J , U Jat, &TJ?y, aJ Jaw) = 0  where D is some operator function. We restrict ourselves to a particularly simple operator D:  0-7803-0559-0 /92 $3.00 Q 1992 EJlE III-497    Figure 1 Trajectory of an observable over the state space  aJ aJ aJ -+H(t ,y ,A ,W,M)=O , with A,:= - , M..:= - al ayi aw ..

In the theory of partial differential equations it is  shown [8,9] that Equation (l), which is called Hamilton-Jacobi, gives rise to  a fundamental set of ordinary differential equations, the so-called characteristic equations:  (2a) dYi a dAi a~ dWij a~ dMij a~ - -- _ - -  - -  - _ - -  - -  - - -  - aw.. '  ayi ' dt aM.. ' dt  dt ahi ' dt  d J a J  aH a - = - + i 11 dt at  A j ( t )*  -@)+E M..(t)- -(t) , aAj i j  aM..

Once the solutions for the characteristic equations (2a) are known, the trajectory J(t,y(t),W(t)) can be determined by direct integration of the Equation (2b). The corresponding surface J = J(t,y,W) is generated by the manifold of all solutions of (2a), obtained by varying the initial states of the system and the boundary conditions, ie by confronting the neural network with sufficiently many input as well as reference patterns.

Integration of (2b) yields:  Then, searching for an extremal trajectory between two points of the state space yields [lo]  - 6 W i j + ( ; W . . - - ) -  d aH 6 M u ] ] 11 aM .. v dt awu 1J  The most general set of necessary conditions for an extremal trajectory J(tf) therefore reads:  W i , J : O  = A.(t ) , 0 = M..(t ) ; 0 = H(t$ . (34 I f  11 f 1  111-498    Obviously, any choice of a Hamiltonian leads to  an extrema1 trajectory J(t). This underlines clearly the importance of the construction of the Hamiltonian that allows to  learn. It is noted also that recall and learning have to happen simultaneously.

Remark: If weights are not conceived as variables but as parameters, the type of neurodynamics envisaged by Optimal control [5] can be derived. Then the conjugate variable M associated with W would not exist, and therefore the characteristic equations be reduced to  equations for y, A and 1. Consequently, the derivative dHldW would remain unbalanced by a dynamic term, but give rise to  an extra term in  (3a):  aH f . awu W t . l t l t  * - ( t ) = O  Any method producing weights that satisfy (3b) constitutes a learning rule (for details see (1 11 ).

EXAMPLE OF A HAMILTONIAN THAT GENERATES A LEARNING LAW  We assume a recursive direct decomposition of the weight functions according to  what has been and what remains to be learned during an epoch of width T:  W(t) = WT(t) + w(t) , with WT(t):=W(t-T) - (44 Consequently, we arrive at a similar decomposition for the conjugate variables of the weights:  &J aw  m : = - .  &J M(t):= MT(t)+ m(t) , with MT:=-  , awT  Let us consider the Hamiltonian:  with  The Hamiltonian (5) yields the characteristic Equations:  dm.. a dm..

Y = - - = -  ZJ  dM; a aE -- _ - -  = -Ai(& ( ( t )  - y, (t) - -(t) , aw.. dt ? dt awi aws dt  The Equations (17c-d) can be combined to  yield second order ODES for the weights:  -=- V v -  ?I (t) = - -- [A{ t ) -  ( ( t ) .  y,W + -(t)] .

aE 2 W T  2 W T  2 w . .  1  dt2 dt2 ? dt2 0 awi 111-499    It follows that the ODES for the variables y, A, w and MT accumulate new knowledge, whereas the remaning ones for W' and m contain the knowledge already learned (ie the latter ones need not be computed).

A qualitative understanding of the Equations (6a-c) is obtained by assuming a constant reference output r and any feedfoward network. For simplicity the error function E(t) = - 112 ( y(t) -r)Z is chosen. If the reference r is set to 0 the weights w are expected to decrease in time. Now, the error dUdy = - y is never positive, since f is the sigmoidal function. Hence, by integrating the A-equation from A(T) = 0, A must be negative for the output layer. This implies MT to be negative, too. It follows that w has to  be positive in order that the weights w decrease.

In the next time epoch of length T, the 'new' weights w are added to the 'old' weights W', in accordance with the decomposition (4a) of the weights. Repeating the integration of the Equations (6a-c) one arrives at another set of 'new' weights which are smaller in magnitude than the first set, because of the error correction induced by the first update. Similarly, the variables A and MT of the hidden layer are forced by the A of the output layer to  change in such a manner that the error E is diminuished.

In order to  test the Equations (6a-c) and compare their performance to the ones obtained with learning rules, a pattern association task was performed. Three time-varying inputs were to  be associated to  three time-varying reference patterns (see Figure 3). The network chosen is composed of 1 input, 15 hidden and 1 output neuron, and denoted as R1-15-1. The hidden layer is fully connected. The error function for the output of the network was E(t) = -112 ( y(t) -r(t) I2 .

Output & Reference  Weights 10,   -5 kd -10 J  I 0 50 100 150 200  Output & Reference  Error   Figure2 Resultsfor R1-15-1 and the Equations6a-c (a-'= O.l,stepsize=O.l) Upper figures: output of the net after 4000 updates; Lower figures: Weight functions of the output neuron after 4000 updates (horizontal units = # of samples); Accumulated total and partial errors (horizontal unit = # of updates)  Figure 2 shows the results obtained for R1-15-1, with a-' =0.1 and constant step size of 0.1 . The equations were found to  converge to  zero update changes; more precisely: n + OD =a A(n.T +XI+ 0, for all 0Sr;ST. Note that in the limit one obtains the relation:  W(l) = W(t -T) telling that the weights functions are either constant or time-varying. In any case, they are  III-500    Inputs Reference  Figure 3 Input and reference patterns  repeated periodically unless a pattern is  presented that restarts the learning process.

Figure 4 presents the results obtained for R1-15-1 when using the following equations:  d dt -yi(t,s) = -yi(t,s) + fi ( 1 wii ( 4 s )  - y j ( t ,") ,  J  dA, aE  ai -(t ,s)  = Ai(t,s)- Z A j ( t , s ) -  f j ( t , s ) -  W . . ( t , s ) -  -(t,s) , Ai(T,d=0 .

J' i dt  Output & Reference Output & Reference  Weights  Error   Figure4 Resultsfor R1-15-1 incaseofthe Equations7a-c (a = lO,stepsize=O.l) Upper figures: Output of the net after 4000 updates. Lower figures: Accumulated weight functions of the output neuron after 4000 updates (horizontal units = # of samples); Accumulated total & partial errors (horizontal unit = # of updates).

III-501    Equations (7a-c) are the gradient version of the characteristic equations of the Hamiltonian  h(t)= Ai (t) Fj (YO); W(t) ) + E ( t , y ( t ) ;  WO) , (7d) i  which treats weights as time-varying parameters (see relation (3b) or [11,51).

In the case that weights are restricted to constant parameter functions the condition (3b) as well as Equation (7c) have to be integrated over time [l 11. The combined result reads:  2-  1-  0-  -1.

-2.

" T dWli ah a -(s) := / - ( t , ~ )  dt = - Io [ Ai(t,s). 6 0,s) yj(t,s) +  T 0 aW.. T ] dt . (8) aw..

?I v ds  50 100 150 200  A relation similar to (10) was obtained by Pearlmutter for a special cost function [41 . Figure 5 depicts corresponding results.

It is to  be noted here that the variable '5' has no interpretation as a dynamical process run by the network. This is in contrast to  the fully dynamical case which has a single time t for recall as well as learning. Hence, we may differentiate between learning rules fulfilling (3b) and learning laws similar to  (6d).

O u t p u t  & R e f e r e n c e  Output & Reference  / .,-----out 2  Error   \  ~~  O b  1000 2000 3000 4000 5000 6000  Figure 5 Resultsfor R1-15-1 and the Equations7a-b,8 (apT = O.l,stepsize=O.l) Upper figures: output of the net after 6000 updates. Lower figures: Accumulated weight functions of the output neuron after 6000 updates (horizontal units = # of samples); Accumulated total & partial errors (horizontal unit = # of updates)  COMPARING THE FIGURES 2,4,5: It is clearly seen that Equations (6a-c) give the fastest decline of the error functions in terms of updates as well as computation time. Further, fitting with constant weights lasts considerably longer in terms of equal error. In passing we note that the recurrent network R1-15-1 was found always to work better than the feedforward version of R1-15-1.

111-502    CONCLUSIONS  We have described a neural network as being a dynamical system. Its observables are introduced as solutions of a partial differential equation of Hamilton-Jacobi type. The observables are functions of time, the neural states and the weight states. The dynamics of an observable is interpreted as a surface over the state space of the neural net which is generated by all i ts admissible trajectories.

If the weights are treated as parameters, the only dynamical process that actually will be run by the network i s  the recall process. It is shown that the known types of learning dynamics can be reproduced in this framework, and that weights are generally time-varying during recall. This feature is shown to account for much faster learning as compared to weights being constant for each learning epoch.

If, on the other hand, weights are conceived as variables, a fully dynamical new picture of recall and learning results. Neurons as well as weights obey differential equations, ie recall and learning are dynamical processes of the neural net. It is  important to note that the differential equations to be obeyed by neurons and weights, respectively, constitute half of the characteristic equations associated with the Hamiltomlacobi equation. The remaining equations are obeyed by the conjugate variables of the neurons and weights, respectively. These latter are mediating the interaction between neurons and weights. Comparing the fully dynamical concept with the optimization approach (that exploits techniques from Optimization Theory, Control Theory or Dynamic Programming [12] ), the dynamical concept was found to  learn much faster than the optimization concept.

ACKNOWLEDGEMENT: We are particular grateful to  B. Schurmann for discussing with US the Hamiltonian concept and i ts  interpretation with respect to neural networks.

