FAR-HD: A Fast And Efficient Algorithm For Mining Fuzzy Association Rules In Large

Abstract?Fuzzy Association Rule Mining (ARM) has been extensively used in relational or transactional datasets having less- to-medium number of attributes/dimensions. The mined fuzzy association rules (patterns) are not only used for manual analysis by domain experts, but are also leveraged to drive further mining tasks like classification and clustering which automate decision-making. Such fuzzy association rules can also be derived from high-dimensional numerical datasets, like image datasets, in order to train fuzzy associative classifiers or clustering al- gorithms. Traditional Fuzzy ARM algorithms are not able to mine rules from them efficiently, since such algorithms are meant to deal with datasets with relatively much less number of attributes/dimensions. Hence, in this paper we propose FAR-HD which is a Fuzzy ARM algorithm designed specifically for large high-dimensional datasets. FAR-HD processes fuzzy frequent itemsets in a DFS manner using a two-phased multiple-partition tidlist-based strategy. It also uses a byte-vector representation of tidlists, with the tidlists stored in the main memory in a compressed form (using a fast generic compression method).

Additionally, FAR-HD uses Fuzzy Clustering to convert each numerical vector of the original input dataset to a fuzzy-cluster- based representation, which is ultimately used for the actual Fuzzy ARM process. FAR-HD has been compared experimentally with Fuzzy Apriori (7?15 times faster), which is the most popular Fuzzy ARM algorithm, and a Fuzzy ARM algorithm (1.1?4 times faster) which we proposed earlier and which is designed to work with very large but traditional (with fewer attributes) datasets.

Index Terms?Fuzzy Association Rule Mining, Fuzzy Cluster- ing, Fuzzy Partitioning, Fuzzy Relations, Partitions, Tidlists, High Dimensions, Large Datasets

I. INTRODUCTION  Association Rule Mining (ARM) enables the extraction of latent frequent patterns in the form of association rules from any dataset and domain. Because these patterns are based on their respective frequencies, they represent the latest trends in the given dataset. This has led to them being used very successfully in other mining tasks such as classification (associative classification [1], [2], [3]) and clustering (ARM- driven clustering, like document clustering [4], [5], [6], [7]).

ARM has gained a lot of popularity because of its accuracy, which can be attributed to its ability to mine huge amounts of data. Because frequent patterns capture all the dominant relationships between items in a dataset and deal only with  statistically significant associations, classification or clustering dependent on such patterns is robust. But ARM expects binary attributes, and cannot be used directly on datasets and in domains which make heavy use of numerical attributes, or have data with very high number of numerical dimensions, like image datasets. Domains like images involve the use of feature vectors with many dimensions (at least more than 60). To perform a task, like associative classification, in such domains requires an algorithm which can mine association rules from high-dimensional numerical data very efficiently and quickly. Moreover, because such data is numerical in nature, the ARM algorithm used would require the numerical data to be converted into data with binary attributes.

But, for all this we need an efficient algorithm (which to our knowledge is not currently available) to mine fuzzy association rules from these high-dimensional fuzzy features. Thus, in this paper we present FAR-HD, an algorithm which can mine fuzzy association rules from high-dimensional numerical data represented in the form of fuzzy features. FAR-HD is designed to mine fuzzy association rules from large datasets (more than 0.5 million vectors), with each vector having at least 60 dimensions, or even more.

FAR-HD uses fuzzy c-means (FCM) clustering to create fuzzy clusters from the feature vectors of the given dataset.

Each vector belongs to each of the ? clusters with a certain degree of membership. This helps in reducing the polysemy and synonymy, which generally occur in crisp clustering.

Knowing exactly the number of clusters which would be optimal for the current dataset is not an easy task. But, because each feature vector belongs to each cluster to some degree, instead of fully belonging to just one cluster (in the crisp case), polysemy is appropriately taken care of. This enables us to use less number of clusters ? 100 (even slightly fewer than the optimal number required), thereby reducing synonymy as well. In crisp clustering, choosing the number of clusters is a tricky and difficult job. Generally, a large number of clusters (? 1000?3000) is chosen in order to avoid synonymy. But, this gives rise to polysemy, there by making it very difficult to manage polysemy and synonymy simultaneously in crisp    clustering.

The salient features of FAR-HD are that it uses a two-  phased processing technique, and a tidlist approach for cal- culating the frequency of itemsets. It also uses a generic com- pression algorithm (zlib) to compress tidlists while processing them in order to fit more tidlists in the same amount of memory allocated/available. zlib provides very good compression ratio on all kinds of data and datasets. The distinctive feature of datasets with high dimensions is that they have association rules with many items. i.e. the average rule length is very high. In order to deal with such association rules, the itemset generation and processing in FAR-HD is done in a DFS- like fashion as in ARMOR [8], as opposed to BFS-like in Apriori [9] and [10], which is optimized for large datasets with fewer number of attributes/dimensions.

In Section II we describe the work related to FAR-HD, and in SectionIII-A we provide details of the pre-processing technique we have used to transform crisp datasets to their fuzzy versions. And, in Section IV we describe how FAR-HD is used for Fuzzy ARM from large high-dimension datasets. In Section V, we illustrate the experimental setup and analysis, before concluding in Section VI.



II. RELATED WORK  Until now, the only algorithms being used for Fuzzy ARM are various fuzzy adaptations of Apriori [11], [12]. Over the years it has been shown [8], [13] that Apriori, per se, is a slow ARM algorithm for most large datasets, even crisp ones. Thus, fuzzy versions of Apriori do not perform fast against very large datasets and datasets with a large number of dimensions/attributes. More details of Fuzzy Apriori, the de-facto algorithm used for fuzzy association rule mining, can be found in [14], [15], [16], [17].

[18], [14], [15] discuss in detail about fuzzy implicators and t-norms for discovering fuzzy association rules, especially negative association rules, from datasets. In [14] Yan et. al.

provide a strong motivation for the need of fuzzy association rules. Though most of the papers on Fuzzy ARM define the support and confidence measures used, De Cock et. al.

in [15] clearly define in detail the actual semantics behind such support and confidence measures. [18] tries to discover the inherent two-sidedness of knowledge by mining association rules by using positive as well as negative examples, which need not necessarily be complementary. In order to do so, the authors introduce new measures of quality, especially for negative association rules. In fact, Dubois et. al. [19], [20] make a very detailed analysis of t-norms and implicators with respect to fuzzy partitions, and provide a firm theoretical basis for their conclusions. They define various types of implications that can be used in the context of fuzzy association rules.

Verlinde et. al. [16] describe in a fair amount of detail as to how fuzzy Apriori can be used to generate fuzzy association rules. In [17] Hu?llermeier and Yi justify the relevance of fuzzy logic being applied to association rule mining in today?s data- mining setup. [21] describes in great detail the general model and application of fuzzy association rules.



III. FUZZY PRE-PROCESSING AND FUZZY MEASURES  In this section, we describe the fuzzy pre-processing methodology and fuzzy measures that are used for the actual Fuzzy ARM process.

A. Pre-processing Methodology  This pre-processing approach consists of two phases:  ? Generation of fuzzy clusters from numerical vectors.

? Conversion of a crisp dataset, containing vectors, into a  fuzzy dataset using a fuzzy-cluster-based representation.

As part of pre-processing, we have used fuzzy c-means (FCM) clustering [22], [23] in order to create fuzzy partitions from the dataset, such that every data point belongs to every cluster to a certain degree ? in the range [0, 1]. The algorithm tries to minimize the objective function:  ?? = ??  ?=1  ??  ?=1  ??????? ? ???2 (1)  where ? is any real number such that 1 ? ? < ?, ??? is the degree of membership of ?? in the cluster ?, ?? is the ??? ?-dimensional measured data, ?? is the ?-dimension center of the cluster, and ? ? ? is any norm expressing the similarity between any measured data and the center. For the current work, we use cosine similarity. The fuzziness parameter ? is an arbitrary real number (? > 1). The amount of fuzziness and Gaussian nature of fuzzy sets can be controlled using an appropriate value (? 1.1?1.5) of the fuzziness parameter ? (Eq. 1).

Assuming we need ? fuzzy clusters, we run FCM (cosine distance metric is used) on all the ? vectors present in the given dataset. On the generation of the ? fuzzy clusters, for each vector we have its membership value (?) in each of the ? fuzzy clusters. The ? membership values for each feature vector are then used to transform numerical vectors into a fuzzy-cluster- based representation. Each vector is represented as a separate record, with each record consisting of ? cluster (attribute) id and corresponding ? pairs <??????? ??, ?>, followed by a class label, if any, as shown in Fig. 1.

?1 ? ?1,1, ?2 ? ?1,2, . . . , ?? ? ?1,?, ????? ????? . . .

?1 ? ??,1, ?2 ? ??,2, . . . , ?? ? ??,?, ????? ?????  Fig. 1. Fuzzy-cluster-based representation  B. Fuzzy Association Rules and Fuzzy Measures  To process the dataset generated after pre-processing, we need new measures (analogous to crisp support and confi- dence), which are in terms of t-norms. During Fuzzy ARM, each of the ? dimensions corresponding to ? clusters is taken as an attribute. The membership values of a vector, from the original input dataset, in each of the ? clusters provide the values for these ? attributes (Fig. 1). Moreover, support and confidence, as defined for crisp association rules, have been    generalized in a suitable way for the fuzzy environment [15], [19]. Using a suitable t-norm pertaining to fuzzy sets ? and ? (Eq. 2) and the cardinality of fuzzy set ? in dataset ? (Eq. 3), we get fuzzy support and fuzzy confidence, defined in Equations 4 and 5. The more generally used t-norms are listed in Table I. ?? (min) t-norm, the most popular t-norm, has been used in FAR-HD to derive the rule-set ? (with ? rules) from the fuzzy-cluster-based representation of vectors.

TABLE I T-NORMS IN FUZZY SETS  t-norm ?? (?, ?) = ???(?, ?)  ?? (?, ?) = ?? ?? (?, ?) = ???(?+ ? ? 1, 0)  ?(?) ?? ?(?) = ? (?(?), ?(?)) (2) ? ? ?=  ?  ??? ?(?) (3)  ???(? ? ?) = ?  ???(? ?? ?)(?) ? ? ? (4)  ????(? ? ?) = ?  ???(? ?? ?)(?)? ???(?(?))  (5)

IV. FAR-HD AND FUZZY ARM IN LARGE HIGH-DIMENSION DATASETS  Our algorithm uses two phases in a partition-approach to generate fuzzy association rules. The dataset is logically divided into ? disjoint horizontal partitions ?1, ?2, . . . , ??.

Each partition is as large as it can fit in the available main memory. For ease of exposition, we assume that the partitions are equi-sized, though each partition could be of any arbitrary size as well. We use the following notations:  ? ? = Fuzzy dataset (fuzzy-cluster-based representation) generated after pre-processing  ? ? = Set of partitions ? ?? = Set of singletons in current partition ? ? ??[??] = tidlist of itemset ?? ? ?? = cumulative fuzzy membership (fuzzy support) of any  itemset in current partition ? ? ?????[??] = cumulative ? of itemset ?? over all partitions  in which ?? has been processed ? ? = number of partitions (for any particular itemset ??) that  have been processed since the partition in which it was added (including the current partition and the partition in which it was added).

FAR-HD uses a byte-vector-like data structure with each cell storing ? of the itemset corresponding to the cell index of the tid to which the ? pertains. Thus, the ??? cell of the byte-vector contains the ? for the ??? tid. If a particular transaction does not contain the itemset under consideration, the cell corresponding to that transaction is assigned a value of 0. All byte vectors are compressed, using zlib algorithm, before being stored in the memory. In this way, we achieve  a lot of saving in main memory space, and thus speed up the execution of the algorithm. We convert ?, a floating-point number, to a byte by multiplying it by 100, and rounding it to the nearest integer to get the final byte representation of the ?, which is actually stored in the byte-vector. Thus, each ? can be represented with a precision of two digits after the decimal point. Whenever we need the actual floating-point value of ?, we just divide the value obtained from the byte-vector by 100.

A. First Phase  In the first phase (Algorithms 1 and 2), FAR-HD scans each transaction in the current partition of the dataset, and constructs a tidlist for each singleton found. After all singletons in the current partition have been enumerated, the tidlists of singletons which are not ?-frequent are dropped. An itemset is ?-frequent if its frequency over ? partitions equals or exceeds the support adjusted for ? partitions, i.e. it is frequent over ? partitions of the dataset (where ? ? number of partitions in the dataset).

The count of each singleton ? is maintained in ?????[?].

To generate larger itemsets, we use depth-first search (DFS) technique, i.e. begin with a singleton ?? and generate all supersets of ??, before doing the same for the next singleton ??+1. Initially, each singleton ?? is combined with another singleton ?? to generate supersets of ?? (till the largest ?- frequent superset of ??) in DFS manner. This process is done for each ?? ; where ? = ?+ 1 to ? ?? ?.

During the intersection of the parent tidlists, for each cell with index ? of the two parent tidlists, we obtain the minimum of the two fuzzy membership values of the ??? cells of the parent tidlists to form the resultant fuzzy membership of ???  cell of the child tidlist. By taking the minimum, we are in effect applying the ?? fuzzy t-norm. Any other t-norm could also have been used. Similarly, the intersection is done for each cell of the parent tidlists to obtain the child tidlist.

Tidlist creation is done as soon as an itemset has been created. The tidlist of ????? is discarded if it cannot be combined with any of the remaining singletons to produce ?- frequent supersets. By doing so, we save main memory space, and reduce the number of inflations required (all tidlists reside in memory in compressed state). Thus, we traverse each branch of the DFS tree, combining singletons with other singletons (and their supersets), till all possible d-frequent itemsets for the current partition have been generated. Then we traverse the next partition and process it in a similar manner, till all partitions have been processed, signaling the end of the first phase.

B. Second Phase  Then we move on to the second phase of the algorithm (Algorithm 3). The second phase is quite different from the first one in many aspects. First, we traverse each partition one- by-one starting from the first partition. All itemsets added in the current partition in the first phase have been enumerated over the whole dataset ?, and thus can be removed. Of these removed itemsets, those which are frequent over the whole dataset ? are output.

1: for each partition ? ? ? do 2: for each transaction ? ? current partition ? do 3: for each singleton ? ? current transaction ? do 4: calculate ? for each ? 5: ?????[?] += ? 6: add ? and corresponding ? for ? to tidlist ??[?] 7: end for 8: end for 9: for each singleton ?? where ? = 1 to ? ?? ? do  10: if ?? is not ?-frequent then 11: remove ??[??] 12: end if 13: end for 14: for each singleton ?? where ? = 1 to ? ?? ? do 15: for each singleton ?? where ? = ?+ 1 to ? ?? ? do 16: generateNewItemsets(??, ??) 17: end for 18: end for 19: end for  Algorithm 1: Phase 1  1: combine ?? and ?? to get ????? 2: ??[?????] = ??[??] ? ??[?? ] 3: calculate ?? for ????? using ??[?????] 4: ?????[?????]+ = ?? 5: if ????? is ?-frequent then 6: for each singleton ?? where ? = ? + 1 to ? ?? ? do 7: generateNewItemsets(?????, ??) 8: end for 9: end if  10: remove ??[?????]  Algorithm 2: Function generateNewItemsets(??, ?? )  Then, for each remaining itemset ??, we identify its con- stituent singletons ?1, ?2, . . . , ?? and then obtain the tidlist of ??(??[??]) by intersecting the tidlists of all the constituent singletons. Additionally, the count of each singleton ?? is updated in ?????[??]. Thus, we alternate between outputting and deleting itemsets, and creating tidlists for itemsets, until no more itemsets are left. Then the algorithm terminates.



V. EXPERIMENTAL SETUP AND ANALYSIS  In this section, we describe the experimental setup and analysis used for comparing FAR-HD with two baseline Fuzzy ARM algorithms - the first is the algorithm described in [9] and [10] and the second one being a Fuzzy Apriori.

A. Performance Study  We use a consolidated dataset of images for experimenta- tion. This is a cumulative dataset composed of images taken from a host of datasets. The following images were combined together to form this consolidated dataset:  ? All images from the CALTECH-4 motorbikes dataset ? All images from the CALTECH Cars (Rear) background  dataset  1: for each partition ? ? ? do 2: for each itemset ?? ? ? in 1?? phase do 3: if ?? is frequent (based on ?????[??]) over the whole  dataset ? then 4: output ?? 5: end if 6: remove ?? 7: end for 8: for each remaining itemset ?? do 9: identify constituent singletons ?1, ?2, . . . , ?? of ?? ?  ?? = ?1 ? ?2 ? . . . ? ?? 10: tidlist ??[??] = intersect tidlists of all constituent  singletons {/* tidlist intersection is same as that in phase 1 */}  11: calculate ? for ?? using ??[??] 12: ?????[??]+ = ? 13: end for 14: if no itemsets remain to be enumerated then 15: exit 16: end if 17: end for  Algorithm 3: Phase 2  ? All images from the cars markus dataset of CALTECH Cars Rear  ? 100 images of giraffes downloaded from Google Images Feature vectors in the form of Speeded-Up Robust Features  (SURF) [24] were extracted from these images. Each SURF vector has 64 dimensions. The number of SURF vectors that can be extracted from an image depends on the characteristics of the image, like size and resolution. A total of 651425 SURF vectors were extracted from these images. FCM clustering (#clusters = 100 and ? = 1.1) was done on the SURF points, as described in section III-A, to obtain a fuzzy-cluster- based-representation (Fig. 1). Each 64-dimension vector was transformed into a 100-dimension (for 100 fuzzy clusters) record. All the three algorithms being compared were then applied on these 100-dimensional records.

Apart from the consolidated dataset, we have evaluated FAR-HD on the FAM95 dataset to assess its performance on datasets with less number of dimensions/attributes as compared to the other two algorithms. Of the 23 attributes in the dataset, we have used the first 18, of which six are quantitative and the rest are binary. For each of the six quantitative attributes, we have generated fuzzy partitions using FCM. These partitions were then used to create the fuzzy version of the dataset (using a threshold for membership function ? as 0.1) through the pre-processing detailed in [9] and [10]. The original dataset contains around 63K transactions, and the fuzzy version, on which the actual fuzzy mining was performed, has more than 425K transactions.

The performance metrics in the experiments are total exe- cution time and maximum memory used. As in most ARM experimental comparisons, total execution time is the main metric. The maximum memory used encompasses only the    memory occupied by the tidlists and counts of itemsets and the itemsets themselves, and serves as a metric only for the comparison of FAR-HD and FAR-Miner. An analysis of maximum memory used by fuzzy Apriori is trivial, and thus has not been pursued. The experiments were performed on a computer with Linux, AMD Athlon X2 Processor 4200+ and 2 GB DDR2 RAM.

B. Experimental Results  This section describes in detail the results of the experi- ments performed on various datasets mentioned in section V-A.

1) Consolidated Dataset: This dataset (an ensemble of other smaller datasets) is the largest dataset we have exper- imented on and is of the size of a typical dataset for which FAR-HD is designed to work best. Figures 2 and 3 illustrate the results obtained by running FAR-HD, FAR-Miner, and Fuzzy Apriori on the consolidated dataset. FAR-HD is 7?15 times faster than Fuzzy Apriori, and 1.1?4 times faster than FAR-Miner for minimum support values ranging from 0.01? 0.00005. Moreover, from a space perspective, the maximum memory used by FAR-HD as minimum support was varied is 0.2?0.99 times that used by FAR-Miner.

5e-005 0.0001  0.0005  0.001  0.005  0.01  T im  e (s  ec on  ds )  Minsupp (log scale)  FAR-HD F-ARMOR  F-Apriori  Fig. 2. Consolidated Dataset ? Time          5e-005 0.0001  0.0005  0.001  0.005  0.01  M ax  . M em  or y  U se  d (K  B )  Minsupp (log scale)  FAR-HD F-ARMOR  Fig. 3. Consolidated Dataset ? Memory  2) FAM95 Dataset: FAM95 is a more ?traditional? text- oriented dataset with much fewer attributes (just 18). It has been used to test the efficacy of FAR-HD, as compared to the other two algorithms, on datasets with much fewer attributes (Figures. 4 and 5). FAR-HD performs much better than fuzzy Apriori both on the basis on time. On the other hand its performance is comparable to that of FAR-Miner based on both the metrics. More specifically, for low support values FAR-HD outperforms even FAR-Miner by a huge margin, especially as far as the maximum memory used is concerned.

0.1  0.15  0.2  0.25  0.3  0.35  0.4 Ti  m e  (s ec  on ds  ) Minsupp  FAR-HD F-ARMOR  F-Apriori  Fig. 4. FAM95 Dataset ? Time             0.1  0.15  0.2  0.25  0.3  0.35  0.4  M ax  . M em  or y  U se  d (K  B )  Minsupp  FAR-HD F-ARMOR  Fig. 5. FAM95 Dataset ? Memory  C. Analysis of the Results  From the results it is apparent that FAR-HD outperforms Fuzzy Apriori on both the datasets used, and FAR-Miner on the large high-dimensional dataset (Consolidated dataset). The main reasons for the high performance of FAR-HD are that it uses a byte-vector representation of tidlists, which are put in main memory in compressed form using zlib. Moreover, it uses a DFS-like itemset generation strategy, which reduces the main memory required, especially for itemsets which are very long (typical in domains with high-dimensional data, like images).

Less memory usage means that we can use less number of    partitions and increase the partition size for FAR-HD. When a partitioning strategy is used for ARM, it is inevitable that some extra itemsets (?-frequent temporarily only over a few initial partitions) are processed, which finally become infrequent at the end of mining. The number of such unnecessary itemsets generated increases as number of partitions ? used increases.

When ? = 1, i.e. the whole dataset is processed in the main memory in one go, then such unnecessary itemsets are not generated at all. FAR-HD ensures that at any point of time, when a new itemset is being enumerated and generated, the tidlists for only its ancestor itemsets (from hierarchical tree structure) and ?-frequent singletons are only present in the main memory, that too in compressed form. FAR-HD uses much fewer partitions as compared to FAR-Miner. In fact, for all datasets and associated minimum support values, we have used just one partition for FAR-HD. But, for FAR-Miner we had to use 3?5 partitions (for lower minimum supports) for the consolidated dataset in order to avoid thrashing. Thus, FAR-HD executes faster, has lesser maximum main memory requirements, and generates far less number of page faults as compared to either FAR-Miner or Fuzzy Apriori.



VI. CONCLUSIONS  We have presented a novel Fuzzy ARM algorithm, called FAR-HD, for very large high-dimensional datasets (like those in the image domain), as a viable and efficient alternative to Fuzzy Apriori and FAR-Miner [9] and [10], both of which are not designed to deal with such datasets and domains. From an empirical perspective, we have shown the superiority of FAR-HD on the basis of a host of metric and parameters.

As future work, we intend to use FAR-HD to mine patterns (association rules) from a high-dimensional dataset and its associated features, so that a Fuzzy Associative Classifier can be built. Because fuzzy association rules represent latent and dominant patterns in the given dataset, such a classifier is expected to provide very good performance, especially in terms of accuracy.

