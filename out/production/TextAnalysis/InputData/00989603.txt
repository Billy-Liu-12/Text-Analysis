A Pattern Decomposition (PD) Algorithm for Finding All Frequent  Patterns in Large Datasets

Abstract  ESficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was proposed in 1994, there have been several methods proposed to improve its performance. However, most still adopt i ts candidate set generation-and-test approach. We propose a pattern decomposition (PO) algorithm that can significantly reduce the size of the dataset on each pass making it more efficient to mine frequent. patterns in a large dataset. The proposed algorithm avoids the costly process of candidate set generation and saves time by reducing dataset. Our empirical evaluation shows that the algorithm outperforms Apriori by one order of magnitude and is faster than FP-tree. Further, PD is more scalable than both Apriori and FP-tree  1. Introduction  A fundamental process in data mining is finding frequent patterns in a given dataset. Finding frequent patterns facilitates essential tasks such as discovering association relationships between items, correlation, and sequential patterns [7].

A significant problem with mining frequent larger patterns is that as the length of the pattern increases, the number of potential patterns grows at a combinatorial rate. Several different algorithms have been proposed to efficiently find all frequent patterns in a dataset [1,5,6,7].

Other algorithms output only maximal frequent sets, thus minimizing the number of potential patterns [2 ,  3 ,  41.

Max-Miner [2] uses a heuristic bottom-up search to identify frequent patterns as early as possible. Pincer- Search [4] uses a bottom-up search along with top-down pruning. Even though performance improvements may be substantial, maximal frequent sets have limited use in association rule mining. A complete set of rules cannot be extracted without support information of the subsets of those maximal frequent sets. FP-tree-based mining [9] is a different approach in that it first builds up an optimized data representation (FP-tree) from the dataset. All mining tasks are then performed on the FP-tree rather than on the dataset.

In this paper we propose an innovative algorithm called Pattern Decomposition (PD) that generates all frequent sets. The algorithm provides increased performance by reducing the dataset during each pass. The dataset is reduced by splitting transactions and combining similar transactions together, thus decreasing counting time and improving performance. In addition, the algorithm does not need to generate candidate sets; all subsets of any transaction in the reduced dataset are frequent thus should be counted. Intuitively, a transaction that contains infrequent itemsets can be decomposed to smaller itemsets if together they do not meet the minimum support threshold. Frequently, after splitting all the transactions in the dataset many itemsets are identical and can be combined, thus reducing the size of the dataset.

2. The Method  The PD algorithm shrinks dataset each time when infrequent itemsets are discovered. More specifically, it finds frequent sets by employing a bottom-up search. For a given transaction dataset D I ,  the first pass has two phrases: 1) the algorithm counts for item occurrences to determine the frequent 1-itemsets Li and the infrequent 1- itemsets -LI; 2) we decompose DI to D2 such that D2 contains no items in -Li. Similarly, in a subsequent pass, say pass k, frequent itemsets Lk and -Lk are generated by counting for all k-itemsets in Dk. Then. Dk+l is generated by decomposing D k  using -Lk such that Dk+] contains no itemsets in -Lk.

Now let us illustrate the complete process for mining frequent patterns. In Figure 1. we show how PD is used to find all frequent patterns in a dataset. Suppose the original data set is Di and minimal support is 2. We first count the support of all items in DI to determine Li and -LI. In this case, frequent 1-itemset Li={a,b,c,d,e} and infrequent 1-itemset -LI ={f,g,h,k}. Then we decompose each pattern in DI using -LI to get D2. In the second pass, we generate and count all 2-item sets contained in D2 to determine L2 and - L2, as shown in the figure. Then we decompose each pattern in D2 to get D3. This continues until we determine Ds from D4, which is the empty set and we terminate. The final result is the union of all frequent sets LI through L4.

0-7695-1 119-8/01 $17.00 0 2001 IEEE 673    The example illustrates three ways to reduce the dataset  {abc} 3 {acd) 1 @de) 3 {abd} 3: @cd} 2 ; @ce) -7; @de} -7: Figure 1. Pattern {Cde} 21 Decomposition Example  D c  @  In a, when patterns after decomposition yield the same itemset, we combine them by summing their occurrence.

Here, abcg and abc reduce to abc. Since both their occurrences are 1, the final pattern is abc:2 in D2.

In p, we remove patterns if their sizes are smaller than the required size of the next dataset. Here, patterns abc and abd with sizes of 3 cannot be in D4 and are deleted.

In 6, when a part of a given pattern has the same itemset with another pattern after  decomposition, we combine them by summing their occurrence. Here, bcde is the itemset of pattern 4 and part of pattern 1's itemset after decomposition, so the final pattern is bcde:2 in D4.

One simple way to decompose the itemset s by an infrequent k-item set t, as explained in [4], is to replace s by k itemsets, each obtained by removing a single item in t from s. For example, for s = abcdefgh and t = aeJ we decompose s by removing a, e ,  f respectively to obtain {bcdefgh, abcdfgh, abcdegh}. We call this method simple-split. When the infrequent sets are large, simple- split is not efficient. Thus PD needs Quick-split to decompose a pattern. [8]  3. Performance Study  We compare PD with Apriori and FP-tree since the former is widely cited and the latter claims the best performance in the literature. The test data sets were generated in the same fashion as the IBM Quest project 111. We used hvo data sets T10.14.DlOOK and T25.110.DlOOK. For the comparison of PD with FP-tree, since PD was written in Java and FP-tree in C++ and we don't have time to implement PD in C++, their results are adjusted by a coefficient about 10.

Our study shows that PD is about 30 times faster than Apriori with minimal support at 2% and about 10 times faster than Apriori at 0.25%. The execution time for  Apriori linearly increases with the number of transactions from 50K to 250K. Better than that, the execution time for PD does not necessarily increase as the number of transactions increases.

Both FP-tree and PD have better performance than Apriori. FP-tree takes substantially more time than PD does when minimum support in the range from 0.5% to 2%. When minsup less than 0.5%, the number of frequent patterns increased quickly and thus the execution times are comparable. PD is about 9 times faster than FP- tree with minimal support at 2% and the gap reduces to 2 times faster at 0.25%. When the number of transactions ranged from 60k-80k, both methods took almost constant time (most likely due to overhead). When we scaled up to 200K, FP-tree required more than 1884M of virtual memory and could not run on our machine while PD finished the computation within 64M main memory. For more details, interested readers please refer to [8].

4. Conclusion  We propose a pattern decomposition (PD) algorithm to find frequent patterns. The algorithm significantly shrinks the dataset in each pass. It avoids the costly candidate set generation procedure and greatly saves counting time by using reduced datasets. Our experiments show that the PD algorithm has an order of magnitude improvement over the Apriori algorithm on standard test data and is faster than FP-tree.

