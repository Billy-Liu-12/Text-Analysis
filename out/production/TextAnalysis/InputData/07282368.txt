A New Enriched Exploration Of Modified Algorithm For Generating  Single Dimensional Fuzzy Itemsets

Abstract? Mining frequent itemsets from transactional database is a fundamental task for association rules.

Apriori is an influential classic algorithm for mining frequent itemset.   But Apriori is a very slow and inefficient algorithm for very large datasets.  A modified algorithm for generating single dimensional fuzzy itemset mining find support count based on fuzzy t-norms namely intersection for finding frequent itemset to reduces the processing time.   The proposed method modifies the above mentioned algorithm for fast and efficient performance on large datasets. It adopts a new count-based transaction reduction and support count method for generating frequent fuzzy item set.  So, it can further reduce time when compared to Apriori and above said algorithm.

Index Terms? Apriori, Fuzzy Set, Fuzzy Intersection, Transaction Reduction.



I. INTRODUCTION Data Mining is a recently emerging field, connecting the three worlds of Databases, Artificial Intelligence and Statistics. The computer age has enabled people to gather large volumes of data. Every large organization amasses data on its clients or members, and these databases tend to be enormous. The usefulness of this data is negligible if ?meaningful information? or ?knowledge? cannot be extracted from it. Data Mining answers this need.

Discovering association rules from large databases have been actively pursued since it was first presented in 1993, which is a data mining task that discovers associations among items in transaction databases such as the sales data [1]. Such kind of associations could be "if a set of items A occurs in a sale transaction, then another set of items B will likely also occur in the same transaction". One of the best studied models for data mining is that of association rules. This model assumes that the basic object of our interest is an item, and that data appear in the form of sets of items called transactions.

Association rules are ?implications? that relate the presence of items in transactions. The classical example is the rules extracted from the content of market baskets. Items are things we can buy in a market, and transactions are market baskets containing several items.

Association rules relate the presence of items in the same basket, for example, ?every basket that contains bread contains butter?, usually noted bread ? butter. The basic format of an association rule is: An association is an implication of expression of the form A ?B, where A and B is disjoint itemset, i.e., A ? B=?. The strength of an association rule can be measured in terms of its support and confidence.

Support determines how often a rule is applicable to a given data set, while confidence determines how frequently items in B appear in transactions that contain A.

The formal definitions of these metrics are   Support s (A ?B)  =     ? (A ? B)/N Confidence c (A ?B)  =  ? (A ? B)/? (A)     In general, association rule mining can be viewed as a two step processes:  1. Find all frequent itemsets: By definition, each of these itemsets will occur at least as frequently as a predetermined minimum support count.

2. Generate strong association rules from the frequent itemsets:   By definition, these rules must satisfy minimum support and minimum confidence.

The remaining part of this paper is organized as follows: Section 2 contains related works.  Section 3 and 4 discusses about Apriori Algorithm and Prabamanieswari?s Algorithm.

Section 5 elaborates the proposed Algorithm. Section 6 contains worked example. Section 7 discusses about the performance analysis of proposed algorithm compared with Apriori algorithm and Praba?s Algorithm. Section 8 contained conclusion.



II. RELATED WORKS Agrawal proposed an algorithm, called AIS algorithm [1], for generating frequent itemsets. In the AIS algorithm, frequent itemsets are generated through iterations on scanning the database. The iteration terminates when no new frequent item- set is derived. After reading a transaction in the kth iteration, the AIS algorithm computes the candidate k ? itemsets by first deriving a set of (k?1)?itemsets which contains itemsets that are both in the frequent (k?1)?itemsets and in the transaction.

However, Apriori algorithm has the limitation of producing a      large number of candidate itemsets and scanning the database too many times.

Many researchers have given different approaches for improving the performance of Apriori algorithm.

To mine the sequential pattern in large database, an effective pruning mechanism called depth first strategy was introduced by J. Ayres [2]. This strategy defines the database in vertical bitmap format with effective support counting. For each item in the dataset, a vertical bitmap is constructed by which each data set transaction is represented as a bit. The value for items is set based on the item present in the transaction. The efficient support counting and candidate generation is obtained by partitioning the bitmap.

Changsheng Zhang and Jing Ruan [3] have worked on the improvement of Apriori algorithm by applying dataset reduction method and by reducing the I/O spending.

Changsheng and Jing Ruan have applied the modified algorithm for instituting cross selling strategies of the retail industry and to improve the sales performance.

One of the novel algorithm presented by Chen.J [4] is a BISC (Binary Item set Support Counting), which is responsible for efficient mining of frequent item set. According to this algorithm, support of all item sets in a database is derived with respect to direct supports. The context BISC transforms the database transaction into binary format. The memory consumption and the cost of support updating is minimized by integrating the two related techniques namely, one stage BISC (BISC1) and two stage BISC (BISC2). This technique is both time and space efficient because of BISC in conjunction with projection techniques that reduce the branching factor of database projection and maximum depth.

Dongme Sun, Sheohue Teng [5] have presented a new technique based on forward and reverse scan of database. It produces the frequent itemsets more efficiently if applied with certain satisfying conditions.

Improved algorithm (ABBM) presented by Hanbing Liu [7] which converted the database to a Boolean matrix.    It scanned the transaction database once, it does not produce candidate temsets, and it adopted the Boolean vector ?relational calculus? to discover frequent itemset. In addition, it stores all transaction data in bits, so it needs less memory space and can be applied to mining large databases.

Jaisree Singh [8] developed a Transaction Reduction Algorithm which reduced the scanning time by cutting down unnecessary transaction records   as well as reduce the redundant generation of sub-items during pruning the candidate itemsets, which can form directly the set of frequent itemsets and eliminate candidate having a subset that is not frequent.   But it has overhead to manage the new database  after every generation of Lk. So, there should be some approach which has very less number of scans of database.

Kavitha.K [9] proposed an efficient transaction reduction technique named TR-BC to mine the frequent pattern based on bitmap and class labels. The proposed approach reduces the rule generation by counting the item support and class support instead of only item support. Moreover, the database storage is compressed by using bitmap that significantly reduces the number of database scan. The rules are reduced by horizontal and vertical transaction and then finally combined rules are generated by eliminating the redundancy.

Ramaraj.E[13]   proposed a novel frequency itemsets generation algorithm called TRApriori that maintained its performance even at relative low supports.  The advantages of TRApriori include interactive mining with different supports; faster execution time and infrequently used item are not stored and hence improves the size of the query data.

Sixue Bai and Xinxi Dai [15] have presented a method called P-matrix algorithm to generate the frequent itemsets. It is found that the P-Matrix algorithm is more efficient and fast algorithm than Apriori algorithm to generate frequent itemsets.

Wanjun Yu, Xiaochun Wang [17] have proposed a novel algorithm called as Reduced Apriori Algorithm with Tag (RAAT), which improves the performance of Apriori algorithm by reducing the number of frequent itemset generated in pruning operation, by applying transaction tag method.

Zhi Lin, Guoming Sang, Mingyu Lu [19] proposed a vector operation based method for finding association rules.  The proposed algorithm finds the association rule more efficiently and requires only one database scan to find all the frequent itemsets.

Recently, the fuzzy set theory [17] has been used more and more frequently in intelligent systems.

Rolly Intan [14] presented an algorithm for generating the association rule by utilizing fuzzy sets in the market basket analysis. This algorithm is based on the concept that the larger number of items purchased in a transaction means the lower degree of association among the items in the transaction.

Based on the concept, two new formulae of calculating degree of support and confidence were proposed utilizing the fuzzy set theory.  Moreover, to generalize Boolean association rules, the concept of fuzzy itemsets was discussed in order to introduce the concept of fuzzy association rules.

Neelu Khare [11] proposed an algorithm for generating fuzzy association rules with multi dimensional attributes using fuzzy set based on human intuitive.  A database consisting of fuzzy transactions, the Apriory property is employed to prune the     useless candidates, itemsets.  Moreover, to generalize inter- dimension association rules, the concept of fuzzy Item sets is discussed, in order to introduce the concept of fuzzy multidimensional association rules.

Prabamanieswari.R [12] introduced  a modified algorithm for generating single dimensional fuzzy itemset mining find support count based on fuzzy t-norms namely intersection for finding frequent itemset to  reduce the processing time.

Vikram Pudi [17] proposed a new fuzzy ARM algorithm meant for fast and efficient performance on very large datasets.  A novel combination of features like two-phased multiple partition tidlist-style processing, byte-vector representation of tidlists, and fast compression contributes a lot to the efficiency in performance.  Through experiments, this algorithm is 8-19 times faster than fuzzy Apriori.

Based on the current study; a novel combination of features like transaction reduction, fuzzy sets and new support count method  contribute a lot of efficiency in performance

III. APRIORI ALGORITHM Apriori algorithm employs an iterative approach known as level-wise search, where k-item sets are used to explore (k+1)- item sets. First, the set of frequent 1-itemsets L1 is found.

Next, L1 is used find the set of frequent 2-itemsets L2. Then L2 is used to find the set of frequent 3-itemsets L3.The method iterates like this till no more frequent k-item sets are found  A.  Apriori Algorithm for FIM  Initialize: k := 1, C1 = all the 1- item sets; read the database to count the support of C1 to determine L1.

L1 := {frequent 1- item sets}; k:=2; //k represents the pass number// while (Lk-1 ? ?) do begin  Ck := gen_candidate_itemsets with the given Lk-1 prune(Ck)  for all transactions t ? T do increment the count of all candidates in CK that are contained in t;  Lk := All candidates in Ck with minimum support ;  k := k + 1; end Answer := Uk Lk

IV. A MODIFIED ALGORITHM FOR GENERATING SINGLE DIMENSIONAL FUZZY ITEMSET MINING  This algorithm uses the following membership ? function is a mapping: ?T: ? (i) -> {0,1}    is defined as   (1)   Such that if an item i, is an element of T then ?T (i) =1, otherwise ?T(i) =0, it considers all transactions.  It uses fuzzy t-norm for calculating support value, it increments the support value based on fuzzy intersection value is greater than 0.0.

When finding intersection among the items, if any one item has a value 0, then it does not continues to find the intersection for the remaining items.  Therefore it reduces the time. The proposed method can further reduces the processing time using a new support value method by combining transaction reduction and fuzzy sets.



V. PROPOSED ALGORITHM  A.  Fuzzy Item Set Strategy Property 1 The proposed algorithm uses the same membership functions as defined in (1), it also consider all transactions in the database and construct the fuzzy item set by replacing each item by  1/total   number of items purchased in a transaction.[8]     (2)  After constructing the fuzzy itemsets database, the whole database is scanned only once and the data is compressed in the form of a Bit Array Matrix M = (Tij)mxn. The frequent patterns are then mined directly from this Matrix.

B.  Transaction Reduction Strategy Property 2 Each value of RC column stores the corresponding number of similar rows.  If the transaction doesn?t repeat then repetition column for the transaction is set to 1.

(3)   Property 3  Support count of one itemset is calculated as counting each column based on its fuzzy value greater than 0.0.

(4)  C.  New Support Count Strategy Property 4 Support count of k itemsets can be got by using value in the RC column and fuzzy t-norm namely intersection.

(5)

VI. WORKED EXAMPLE In general, a transactional database consists of a file in which each record represents a list of items purchased in a transaction.  Simply, a transaction includes a unique transaction identity number and the list of items making up the transaction.  The process is started from a given transactional database as shown in Table 1.

TABLE I Transactional Database TID ITEMS T1 I1,I2,I5 T2 I2,I4 T3 I2,I3 T4 I1,I2,I4 T5 I1,I3 T6 I2,I3 T7 I1,I3 T8 I1,I2,I3,I5 T9 I1,I2,I3   Here the minsup is considered as 2.  The fuzzy item sets is constructed using property 1 as shown in Fig. 1.

Fig.1  Fuzzy Sets     Step 1 : The support count of one itemset is calculated as counting each one itemset based on its fuzzy value greater than 0.0 by using property 3  {I1} = {0.33/T1,0.33/T4,0.5/T5,0.5/T7,0.25/T8,0.33/T9}  sup_count = 6 {I2} = 0.33/T1,0.5/T2,0.5/T3,0.33/T4,0.5/T6,0.25/T8,0.33/T9}  sup_count = 7 {I3} = {0.5/T3,0.5/T5,0.5/T6,0.5/T7,0.25/T8,0.33/T9}  sup_count = 6 {I4} = {0.5/T2,0.33/T4}  sup_count = 2 {I5} =  {0.33/T1,0.25/T8}  sup_count = 6  Move all those transactions to L1 whose sum value is not less than min_support (minsup=2). Therefore L1 = {I1, I2, I3, I4, I5}.

Step 2: By using property 2, each value of RC column stores the corresponding number of similar rows.  If the transaction doesn?t repeat then repetition column for the transaction is set to 1 as shown in Fig. 2.

Fig. 2 Transaction Reduction using RC                    Step 3: Now for the generation of two item sets, consider the Matrix again.  Calculate the support count for 2 item sets by using property 4 Sup_count(I1,I2)=4 Sup_count(I1. I 3)=4 Sup_count(I1,I 4)=1 Sup_count(I1,I 5)= 2   Sup_count(I2, I 3)=4  Sup_count(I2, I 4)=2 Sup_count(I2, I 5)=2  Sup_count(I, I 35)=2  Then move only those item sets L2 whose support count value is    not less than minimum support. Therefore, {I1, I 2},{I1, I 3}, {I1, I 5} ,{ I2, I 3}, {I2, I 4}, {I2, I 5}, {I3, I 5} will be frequent 2 itemsets.

Step 4: Next, consider all the 3-itemsets combinations of the items the various combinations possible are {I1, I2, I5}; {I1, I2, I4}; {I1, I2, I3}; {I2, I3, I5}; {I1, I3, I5}. Now, by the property 4  Sup_count((I1, I2, I5) =2  Sup_count((I1, I2, I3) =2.

Therefore, {I1, I2, I5} & {I1, I2, I3} will be the collection of 3 itemsets.

Step 5: Similarly, 4-itemsets possible combinations are considered.

i.e. {I1, I2, I3, I5}.  Therefore L4= NULL. Hence all the frequent itemsets are generated.

In this approach, we use property 4 for new support count.  In property 4, we combined the fuzzy intersection and value of RC column. In fuzzy intersection, if any one item has a value 0, then the intersection is not continued for the remaining items in a transaction.  RC contains the count of the similar transactions; it can greatly reduce the number of transactions to be scanned.  So, when we combine the above said three  I1 I2 I3 I4 I5 RC  I1,I2,I5 0.33 0.33 0.00 0.00 0.33  I2,I4 0.00 0.5 0.00 0.5 0.00  I2,I3 0.00 0.5 0.5 0.00 0.00  I1,I2,I4 0.33 0.33 0.00 0.33 0.00 I1,I3 0.5 0.00 0.5 0.00 0.00  I2,I3 0.00 0.5 0.5 0.00 0.00  I1,I3 0.5 0.00 0.5 0.00 0.00  I1,I2,I3,I5 0.25 0.25 0.25 0.00 0.25  I1,I2,I3 0.33 0.33 0.33 0.00 0.00  SUM 6 7 6 2 2  I1 I2 I3 I4 I5 RC  0.33 0.33 0.00 0.00 0.33 1  0.00 0.5 0.00 0.5 0.00 1  0.33 0.33 0.00 0.33 0.00 1  0.5 0.00 0.5 0.00 0.00 2  0.00 0.5 0.5 0.00 0.00 2  0.25 0.25 0.25 0.00 0.25 1  0.33 0.33 0.33 0.00 0.00 1     features, it further reduces the time for finding frequent itemset when compared to above mentioned algorithm.

Algorithm  Input   : Fuzzy Database, Support Count   Output  : Frequent Fuzzy Itemsets.

Step 1: Begin Step 2: Read Matrix Step 3: Generate the set of frequent 1  itemset  (property 3) Add RC column //  (property 2) k:=2; while (Lk-1 ? ?) do begin  for each k itemset compute sup_count using RC and fuzzy t-  norm  (property 4) if  sup_count >= minsup then Lk := All candidates in Ck with  minimum support ; end if  end for k := k + 1; end  Answer := Uk Lk Step 4: End.

__________________________________________________

VII. EXPERIMENTAL RESULTS In order to appraise the performance of the proposed algorithm, we conducted an experiment using the Apriori algorithm, Prabamanieswari?s algorithm and the proposed algorithm.

A   Experiment 1 For this purpose, we select the supermarket, Sri Mahalakshmi Departmental Store, Ariyalur, data to study the object. The supermarket database contains 10,000 transactions, apply algorithms on same number of transaction and compare the execution time with support count 5, 10,15,20,25 shown in Fig. 3.

Fig. 3 shows performance of three algorithms; here proposed algorithm performs better in order to time efficiency.

TABLE II THE TIME REDUCING RATE OF PRABA?S ALG. ON THE APRIORI ACCORDING TO THE VALUE OF   MINIMUM SUPPORT; THE AVERAGE OF REDUCING TIME RATE IN THE PRABA?S ALG.  IS 76.4%  MIN_SUP Apriori Alg. (ms) Praba?s Alg.

(ms) TRR (%)  5 640 140 78  10 485 125 74  15 438 78 82  20 281 70 75  25 234 62 73  TABLE III THE TIME REDUCING RATE OF PROPOSED ALGORITHM ON THE APRIORI ACCORDING TO THE VALUE OF   MINIMUM SUPPORT; THE AVERAGE OF REDUCING TIME RATE IN THE PROPOSED ALGORITHM.  IS 88.8%  MIN_SUP Apriori Alg. (ms) Proposed Alg.

(ms) TRR (%)  5 640 78 87  10 485 47 90  15 438 32 92  20 281 31 88  25 234 30 87    TABLE IV THE TIME REDUCING RATE OF PROPOSED ALGORITHM ON THE PRABA?S ALGORITHM ACCORDING TO THE VALUE OF   MINIMUM SUPPORT; THE AVERAGE OF REDUCING TIME RATE IN THE PROPOSED ALGORITHM.  IS 54%  MIN_SUP Praba?s Alg. (ms) Proposed Alg.

(ms) TRR (%)  5 140 78 44  10 125 47 62  15 78 32 58  20 70 31 55  25 62 30 51   Fig.  3 Performances of Three Algorithms        All experiments are performed on Intel core i3, 3.07GHz processor and 2GB of RAM, the algorithms were implemented in Java and tested on a Windows XP platform.



VIII. CONCLUSION In this research paper, a combination of transaction reduction and fuzzy item set is proposed through reducing the time consumed by reducing the number of transactions to be scanned. Whenever the value of minimum support increases, the gap between our proposed and original Apriori algorithm decreases in view of time consumed.  The time consumed to     generate frequent item set in our proposed algorithm is less than the original Apriori and Praba?s algorithm; as this is proved and validated by the experiment and observed in Fig.3, TABLE II, TABLE III and TABLE IV.

.

