An Efficient Sequential Pattern Mining Algorithm Based on the 2-Sequence  Matrix

Abstract   Sequential pattern mining has become more and more popular in recent years due to its wide applications and the fact that it can find more information than association rules. Two famous algorithms in sequential pattern mining are AprioriAll and PrefixSpan. These two algorithms not only need to scan a database or projected-databases many times, but also require setting a minimal support threshold to prune infrequent data to obtain useful sequential patterns efficiently. In addition, they must rescan the database if new items or sequences are added. In this paper, we propose a novel algorithm called Efficient Sequential Pattern Enumeration (ESPE) to solve the above problems. In addition, our method can be applied in many applications, such as for the itemsets appearing at the same time in a sequence. In our experiments, we show that the performance of ESPE is better than the other two methods using various datasets.

Keyword: Sequential pattern, data mining, association rule, candidate enumeration, minimum support  1. Introduction   Data mining techniques have been developed for many researches with wide applications. In recent years, sequential pattern mining has become more important because it has been used extensively in business, bioinformatics, Web mining, computer intrusion detection and so on.

Sequential patterns represent sequential relationships between items. They can provide more information than association rules. The existing  methods of sequential pattern mining can be divided into two categories: (1) candidate generation-and-test approaches, such as AprioriAll [1] and GSP [2], and (2) divide-and-conquer approaches, such as PrefixSpan [3]. These methods are used to find frequent sequences for time-interval [4], fuzzy patterns [5], partial orders [6], and so on. As a result of parallel technique advances, many researchers have parallelized existing methods [7]. However, these approaches have some problems. For example, AprioriAll and GSP must scan the original databases many times. While PrefixSpan only scans a database once, it needs to generate and scan multiple projected databases which might be larger than the original database. In addition, they all need to set a minimal support threshold at the beginning of mining processes to prune infrequent candidates for the execution time speed-up. If the minimal support is changed, they must start afresh.

Their variations all suffer similar problems.

Hence, we propose a more efficient sequential pattern mining algorithm called Efficient Sequential Pattern Enumeration (ESPE) which is based on a 2-sequence matrix. It has the following characteristics:  1. Scan the database only once.

2. Reduce the number of candidate sequences  being generated that will not be frequent.

3. Require less memory space.

4. Can deal with long sequences.

5. Do not need to set the minimal support in  advance.

6. Can effectively find rules based on users?  demand.

7. Can do incremental mining for new items and  sequences.

In our approach, ESPE provides complete  information and finds interesting rules more easily.

The remainder of the paper is organized as follows:  Section 2 describes related work and Section 3 gives   DOI 10.1109/ICDM.Workshops.2008.111    DOI 10.1109/ICDMW.2008.82     the problem definition. Section 4 presents our proposed algorithm. Section 5 extends our method. We discuss our experiments in Section 6. Section 7 presents our conclusion and future work.

2. Related work   Sequential pattern mining was first described in [1] with the AprioriAll algorithm. Later the same authors proposed the GSP algorithm to improve the AprioriAll.

The GSP algorithm uses three concepts which are sliding time windows, time-constraints and taxonomies. It makes multiple passes over the database and uses candidate-generation-and-test approach to find sequential patterns. The first pass applies the minimal support to find all frequent 1-sequences. The second pass generates candidate k-sequences by joining the frequent (k-1)-sequences and deletes those candidates whose frequency is not greater than or equal to the minimal support. This generate-and-test process will be repeated many times until no more candidate sequence can be generated.

Prefix-projected Sequential pattern mining (PrefixSpan) is a divide-and-conquer approach using many projected-databases. PrefixSpan finds frequent sequential patterns without generating candidate sequences, and focuses the search space on projected-databases instead of the original database.

The main concept is checking the minimal support on the prefix and projected databases over and over. It only scans the original database once, and then projects it into many smaller sub-databases according to frequent items. Finally, the sequential patterns are mined by growing subsequences in each projected-database. By this method, it can reduce the size of search space and avoid generating candidates one by one.

PrefixSpan is different from GSP because it does not join (k-1)-sequences to generate k-sequences.

PrefixSpan uses the prefix of (k-1)-sequences to add different items, and then estimates whether the sequence patterns satisfy with the minimal support or not. This method generates projected-databases by growing the frequent items in the projected-database, and does not need multiple scans of the original database. However, the size of the projected-databases of PrefixSpan may be larger than the original database and requires more memory space.

3. Problem definition   A sequence S is an ordered list of itemsets, denoted as <s1,s2,?,sl>, where si is an itemset. An itemset si is called an element of the sequence, denoted as si =  (i1,i2,?,in), where ik is an item. Itemsets are made up of items; similarly, sequences are made up of itemsets.

For a sequence, an item could occur multiple times in different element. A sequence containing l itemsets is called l-sequence. For example, <ABC> is a 3-sequence.

A sequence database SDB is composed of a set of ordered pairs of sequence-ids and sequences, denoted as <sid, s>. An example sequence database is shown in Table 1. A minimal support, denoted as min_sup, is a user-specified threshold. If the support of sequence ? is greater than or equal to min_sup, we say the sequence ? is frequent. A minimal length is a user-specified threshold for the length of a sequence, denoted as min_length.

Table 1. A sequence database  SID Sequences 1 3 1 2 1 3 2 1 2 3 2 3 3 1 2 3 4 1 2 1 3 1   In addition, we define some expressions used in our  method. We partition a sequence into two sections X and Y, as defined below in Definition 1. Here X is a candidate 2-sequence called Partition X (PX) and Y is the remaining sequence called Partition Y (PY).

n  n  s  as Definition  .s..ss : s .s..ss  Y and ss X is,That  sequence remaining is Y sequence-2 candidate is X  Y:X denote  we,......ssss sequence aFor :1    n321  ?  ==  ?   4. ESPE algorithm   There are five steps in our proposed ESPE algorithm:  (1) Read a transaction sequence at a time from the sequence database.

(2) Enumerate all 2-sequences (PX) in each transaction sequence. Record all remaining sequences (PY) after the 2-sequences. Users do not need to define a minimum support at this step.

(3) Use the concept of the interlace matrix method to compute the index of support counter for the generated 2-sequences and increase the value of their corresponding counters by one. The method will be explained later. Then, record PY with linking method and use the interlace matrix method     again to encode PY.

(4) Repeat the above three steps until the last transaction is processed.

(5) When users are ready, they can define a minimum support, a minimum sequence length or any specific sequences they desire. ESPE can easily find frequent sequences and generate corresponding rules accordingly.

These five steps involve four techniques: enumerate  all 2-sequences, store remaining sequences, encode the sequences, and specify the user?s demand. We will explain them in the next subsections.

4.1. Enumerate all 2-sequences   At the first step, the algorithm reads a transaction and enumerates all of its 2-sequences. This procedure is similar to FSE?s candidate enumeration [8], but ESPE does not need to generate all candidate n-sequences at once. It only enumerates all 2-sequences and that can avoid generating too many candidates and wasting memory space. ESPE can represent all information necessary for sequential pattern mining by simply generating 2-sequences with the format of Definition 1. For example, Table 2 shows all the 2-sequences generated from Table 1 with our enumeration method. By Definition 1, our method only generates eight sequences in SID=1: <3 1:2 1 3><3 2:1 3><3 3><1 2:1 3><1 1:3><1 3><2 1:3><2 3>, and these sequences could replace the original sequences.

However, FSE generates twenty-three sequences: <3 1><3 2><3 3><1 2><1 1><1 3><2 1><2 3><3 1 2><3 1 1><3 1 3><3 2 1><3 2 3><1 2 1><1 2 3><1 1 3><2 1 3><3 1 2 1><3 1 2 3><3 1 1 3><3 2 1 3><1 2 1 3><3 1 2 1 3>.

Hence, using Definition 1 ESPE can enumerate sequences more efficiently and save memory space.

Table 2. Enumeration of all the 2-sequences  SID sequenceAll of the 2-sequences 1 3 1 2 1 3 <3 1:2 1 3><3 2:1 3><3 3><1 2:1 3><1  1:3><1 3><2 1:3><2 3> 2 1 2 3 2 <1 2:3 2><1 3:2><2 3:2><2 2><3 2> 3 3 1 2 3 <3 1:2 3><3 2:3><3 3><1 2:3><1 3><2 3> 4 1 2 1 3 1 <1 2:1 3 1><1 1:3 1><1 3:1><2 1:3 1>  <2 3:1><3 1>  It is obvious that if we enumerate all candidates in a  sequence, its runtime will grow exponentially. If the length of sequence gets longer, it will generate many more candidates. Hence, a long sequence may need a lot of memory space and cause a short of memory space. Moreover, the generated sequences of candidate form the shape like an inverted triangle. More  candidates are generated when they are close to the middle of the triangle as shown in Figure 1. In Figure 1, the length of sequence is 6 and it generates 52 candidates. The number of 3-sequence candidates is the largest and the numbers of other candidates get smaller toward both ends at 1-sequence and 6-sequence.

Figure 1. A list of all candidate sequences for 1 to  6-sequences   Next, we explain why we enumerate 2-sequence  instead of 3-sequence or longer sequences. For a 3-sequence needs to generate ll CC 23 +  (l is the length of sequence) candidate sequences, it spends more time and memory space than 2-sequence does.

For example, in Figure 1, there are 15 candidates in the 2-sequence while 3-sequence generates 32 (15+17) candidates. The more candidates being generated, the more space will be required. If we choose 3-sequence, it needs 32 NN +  (N is the number of items) units of memory space to store indexes. When the sequence is very long or contains many items, it will generate more candidates and require more space too. For example, in a sequence database with 6 items, 36 units of memory space are allocated for the 2-sequence approach while the 3-sequence needs 252 (36+216) units of memory space. Since the number of candidates and required memory space are accumulative when the sequence length is greater than 3, we use 2-sequence in our algorithm to save time and space.

4.2. Store Sequences Efficiently   The goal of our method is to store and retrieve data quickly and efficiently. Additionally, it needs to do incremental mining. Therefore, we use the index to put     the data in the right place. Then, we use a simple computation to calculate the index and store each 2-sequence?s support count efficiently. Next, we describe and use Lemma 1 for the index computation, followed by the explanation.

)1(:3  )1(:2 :1  :is sequence-2  theofindex   then the sequence,-2 a of items  two theare  and  If:1  ?+=?<  +??=?> ?=?=  iiindexiicase  iiiindexiicase iiindexiicase  iiLemma   We use three 2-sequentces <31>, <33> and <12> to  show how we calculate their indexes by using Lemma 1. Since <31> indicates i1=3, i2=1, and 3 > 1, it belongs to Case 2 and its index=3?(3-1)+1=7. For <33>, i1=3, i2=3, and 3=3, we use Case 1 and find the index=3?3=9.

For the last sequence <12>, i1=1, i2=2, and 1<2, then we use Case 3 to get its index=1+(2-1)2=2. Table 3 shows all the indexes of the generated 2-sequences of Table 2. They are represented in the form of <X:Y:Z> where X and Y are defined in Definition 1 and Z is the computed index. For a 2-sequence, Y is null and we use ??? to represent Y.

Table 3. The indexes of all the generated 2-sequences  SID Index 1 <3 1:2 1 3:7><3 2:1 3:8><3 3:?:9><1 2:1 3:2>  <1 1:3:1><1 3:?:5><2 1:3:3><2 3:?:6> 2 <1 2:3 2:2><1 3:2:5><2 3:2:6><2 2:?:4><3 2:?:8> 3 <3 1:2 3:7><3 2:3:8><3 3:?:9><1 2:3:2><1 3:?:5>  <2 3:?:6> 4 <1 2:1 3 1:2><1 1:3 1:1><1 3:1:5><2 1:3 1:3>  <2 3:1:6><3 1:?:7>    Figure 2. (a) FSPE (b) The interlace matrix of  2-sequences   Lemma 1 comes from the improvement over the  index computation used in the FSPE method [9] as shown in Figure 2. Figure 2 (a) is the FSPE and (b) is the interlace method used in our approach. In the FSPE method, it stores data in a row order continuously. For example, the index of <1,1> is 1, the index of <1,2> is 2, the index of <1,3> is 3 and so on. Although this method could easily compute the index for each sequence, it must know all of the distinct items in advance. In addition, the FSPE method must re-compute all the indexes when a new item is added in the sequence database. Therefore, we use the interlace matrix to solve the problem and make incremental mining possible. This matrix can be extended when new items come in. By observing the interlace matrix of Figure 2 (b), we have the following three cases:  For Case 1: i1=i2, means all the items smaller or equal to i1 and i2 come before them. Therefore, we multiply i1 by i2. In Case 2: i1>i2, means that all data before the [i1,(i1-1)] matrix are existing items, and then we can add i2 to i1?(i1-1) to find the index. In Case 3: i1<i2, means that all data before the [(i2-1),(i2-1)] matrix are existing items, so we can just add i1 to the square of (i2-1) to get the index.

When ESPE reads a transaction, it enumerates all 2-sequences and computes indexes for the 2-sequences immediately. Our method will use indexes to fill in all 2-sequences of PX and record PY. To save memory space, PY will be encoded in a more efficient way. The encoding of PY is explained in the next section.

Table 4. Sequences of length 2 combinations  Length 1 2 3 4 5 ? Index: Use a ?-? as  prefix  Index: By Lemma  Length-2 + Length-1  Length-2 + Length-2  Length-2+ Length-2+ Length-1  ?   4.3. Encode the Sequences   The main purpose of this process is to improve space efficiency and search time. In this step, we use Lemma 1 to encode the remaining sequences (PY).

First, we build two tables from sequences of length-1 and length-2 sequences, in order to generate sequences for other length. Then we use these two tables to encode all PY as described in Table 4 under various lengths. If the length of remaining sequence is 3, it is composed of the length-2 and length-1. If the length of remaining sequence is 4, then it is composed of two of the length-2, and so on. While the table of length-2 is built by Lemma 1, the element in the table of length-1 uses a ?-? as the prefix of the original item in order to  (a)                              (b)     distinguish from the item of length-2. For example, for the sequence <11113>, we can encode this sequence into <1, 1, -3> with two length-2 and one length-1 where -3 indicates that it is a length-1. In this method we don?t need to record all sequences and can save the memory space.

After computing the index, recoding, and encoding the sequences for each transaction in Table 1, we recode the support value in a similar matrix. The result is shown in Table 5. They are represented in the form of <X>:S and <Y>:S where X and Y are defined in Definition 1 and S is the support of X or Y. Take the first entry for example: 11:2 means the support of <1 1> is 2, <-1>:1 means the support of <1> is 1, <-3>:1 means the support of <3> is 1, and <7>:1 means the support of <3 1> is 1. Here the index of 7 is used to stand for <3 1> and save the space as described before.

Using this matrix, we can provide all information specified by user sequence on-demand.

Table 5. The structure of support counters  2-Sequence Remaining sequences (PY) 1 <11>:2 Length=1 <-1>:1  <-3>:1  Length=2 <7>:1 2 <12>:4 Length=1 <-1>:2  <-2>:1  <-3>:4 Length=2 <1>:1  <5>:2  <7>:1  <8>:1 Length=3 <5,-1>:1  3 <21>:2 Length=1 <-1>:1  <-3>:1 Length=2 <7>:1  4 <22>:1 Length=1 5 <13>:4 Length=1 <-1>:1  <-2>:1 6 <23>:4 Length=1 <-1>:1  <-2>:1 7 <31>:3 Length=1 <-1>:1  <-2>:2  <-3>:2 Length=2 <2>:1  <6>:2  <5>:1 Length=3 <2,-3>:1  8 <32>:3 Length=1 <-1>:1  <-3>:2 Length=2 <5>:1  9 <33>:2 Length=1  4.4. User sequence on-demand   One major feature of ESPE algorithm is that it does not need to define any threshold. Therefore, ESPE stores all possible rules and a user can set parameters based on their own demands to find interesting rules.

ESPE allows users to make specific queries on the support threshold, the length of sequence or some specific sequences. Then, ESPE can find the information efficiently. For example, if a support  threshold is given, we only need to check the support counter of 2-sequence. If the support counter of 2-sequence satisfies with the minimum support threshold, then we check the encoded PY.

Moreover, if the user sets the minimum length of sequence or asks for specific sequences, our method can show the result immediately. For example, in Table 5, if the minimum support is 3, then the algorithm only needs to check the entries 2, 5, 6, 7, and 8 in the encoded PY. At the entry 2, there is only <-3>:4 satisfying the minimum support threshold. Hence, we do not need to check the sequence whose length is longer than 1. If the minimum length of sequence is 4, we only query encoded PY with length-2. Besides, if a user wants to focus on any specific sequence such as <1 2> then we only need to check the second entry. All parameters can be set at the same time.

5. Extension to our method   In general, the sequences in a database are not only ordering items but also items that may appear together.

Our method can deal with this kind of application as well. We take the itemsets which appear at the same time as an element, and encode each element.

To extend our method, most of the previous processes are the same except some differences in index computation and storage structure. The first step transforms the data. The next step is the same as ESPE, but it needs another method to compute the index when items appear at the same time. Then, it must build a different table to store the sequence in the final step.

5.1. Transform Sequences   We use a different form to represent the items  which appear at the same time. In a transaction, if the sequence is <2 (3 2) 1>, then (3 2) and (2 3) can be considered as the same because 3 and 2 appear at the same time regardless of their order. Hence, we can not use Lemma 1 to calculate the index any more. The characteristics of these data have the relationship like a right triangle as shown in Figure 3. We use Lemma 2 to compute their indexes and use a prefix of ?*? to distinguish them from the indexes of Lemma 1.

Figure 3. A list of 2-itemsets         )2)(1(  :is sequence-2  theofindex   then the sequence,-2 a of items  two theare  and  If:2  i ii  index  iiLemma  + ??  =   Lemma 2 comes from Figure 3 which forms a right  triangle. We can use the concept of triangle area to show Lemma 2. Use the sequence <3 (1 2)> as an example, we transform (1 2) into *1 since the index of (1 2) is (2-1)?(2-2)/2=1. Hence, <3 (1 2) > becomes <3 {1,2,*1}>. There are three possible sequences <3 1><3 2><3 *1>. If the itemset is (3 1 2) as in the case of association rule, it is transformed into {1, 2, 3, *1, *2, *3, *(1:3) } accordingly. The proofs of Lemma 1 and Lemma 2 are included in the appendix.

Table 6. A sequence database  SID sequence 1 (3) (1 2) (3) 2 (1 2) (3) (2) 3 (3) (1 2) (3)    Table 7. Transform the sequences and build the indexes  SID Sequence Transform sequence  All of the candidate 2-sequences  1 (3)(12) (3) < 3 {1,2,*1} 3> <3 1: 3:7><3 2: 3:8> <3 *1: 3:index table> <3 3:?:9><1 3:?:5><2 3:?:6> <*1 3 :?: index table >  2 (12)(3) (2)  <{1,2,*1}3 2> <1 3:2:5><1 2 :?:2><2 3:2:6> <2 2:?:4> <*1 3:2: index table > <3 2:?:8>  3 (3)(12) (3)  <3 {1,2,*1}3> <3 1: 3:7><3 2:3:8> <3 *1: 3 : index table > <3 3:?:9><1 3:?:5><2 3:?:6> <*1 3:?: index table >   5.2. Store Indexes   We use the original ESPE to compute and store indexes when a sequence has no multiple items appearing at the same time. Otherwise, a different table is used to store the indexes by using Lemma 2. We use Table 6 as an example sequence database. First, we transform the original sequence by using Lemma 2 as shown in the third column of Table 7. Second, we enumerate all 2-sequences in each transaction with the  new indexes as shown in the last column of Table 7. In the next step, we build two tables, Table 8 and Table 9, to store these sequences. Table 8 is similar to Table 5 for regular sequences. If a prefix of ?*? is indicated in a sequence, it will be stored in Table 9. Note that Tables 8 and 9 present the support S in the form of <X>:S and <Y>:S. The remaining process is similar to the original ESPE. For example, if the minimum support threshold is 3, then <*1 3> satisfies the minimum support threshold in Table 9. We can get a result of <(1 2) 3>.

Table 8. Storage structure of support counters (a)  Code 2-Sequence Remaining sequences (PY)  1 <11>:0 Length=1 2 <12>:1 Length=1 3 <21>:0 Length=1 4 <22>:1 Length=1 5 <13>:3 Length=1 <-2>:1 6 <23>:3 Length=1 <-2>:1 7 <31>:2 Length=1 <-3>:2 8 <32>:3 Length=1 <-3>:2  Length=2 <5>:1 9 <33>:2 Length=1    Table 9. Storage structure of support counters (b)  Index Table 2-Sequence Remaining sequences (PY) <3 *1>:2 Length=1 <-3>:2 <*1 3>:3 Length=1 <-2>:1   6. Experiments and Discussions   All experiments were conducted on a 3.0GHz Intel Pentium 4 PC with 4GB DDR400MHz memory, running Microsoft Windows XP with SP2. The synthetic datasets were generated from IBM synthetic data generator. For example, one of the datasets is C100S1.5T10NI0.05 with 100K customers. The average number of transactions is 1.5 and the average number of items is 10. There are 50 different unique items.

6.1. Experimental Results   We first compare the performance of ESPE and  PrefixSpan by using different minimal support thresholds for the dataset C100S1.5T10I0.05 as shown in Figure 4. ESPE is better than PrefixSpan when the minimal support is low. This is because PrefixSpan needs to generate many combinations and constructs     many projected-databases. This means our algorithm is not limited by different minimal support thresholds especially when they are small.

C100S1.5T10N0.05         0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  Support(%)  R un  ti m  e( se  c)  ESPE  PrefixSpan   Figure 4. Scalability test for various min_sup  thresholds  The next experiment displays the scalability test of  different length of customer transactions. We use C10T1I0.02 datasets with the minimum support=5% and the average number of transactions per customer (S) ranges from 10 to 40. Figure 5 shows that ESPE is better than PrefixSpan when S increases. As the length of customer transaction increases PrefixSpan needs to generate a lot of projected-databases and scan them many times. Therefore, PrefixSpan is unfavorable to longer length of customer transaction. In addition, it shows the performance of our algorithm grows linearly when the length of S increases.

C10S10-40T1N0.02 Support 5%             10 15 20 25 30 35 40 Average transaction per customer  R u  nt im  e( se  c)  ESPE  PrefixSpan   Figure 5. Scalability test for various transactions per  customer   Our algorithm is not restricted by the min_sup, the sequence length, and the number of items. We use  C100S5T1 datasets with a support of 1% and set the number of items N between 10 and 100 to run the scalability test. From the result in Figure 6, when the number of items increases, the runtime of ESPE is still better than PrefixSpan.

C100S5T1N0.01-0.1           10 25 50 75 100 Number of items  R u  nt im  e( se  c  ESPE  PrefixSpan  Figure 6. Scalability test for various numbers of items   T10I4D0.1KN25    1 2 3 4 5Support(%)  R un  tim e(  se c)  FSE  ESPE   Figure 7. Compare ESPE with FSE for various  supports   Our method can not only deal with sequential patterns but also association rules. Hence, ESPE can compete with other algorithms of association rule. We use the dataset of association rule T10I4D0.1KN25 with varying supports to compare ESPE with FSE in Figure 7. Our algorithm is better than FSE 9% to 10%.

This is because FSE needs to generate all candidates and wastes a lot of time.

Figure 8 is the result of the scalability test for various transaction numbers between ESPE and FSE.

Using T10I4DN25 datasets with D ranging from 0.1K to 1K, we show that ESPE is 10% to 43% better than FSE.

T10I4D0.1-1KN25    0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Number of transactions(K)  R u  n tim  e( se  c)  FSE  ESPE    Figure 8. Scalability test for various transactions   6.2. Discussion   According to the above results, we can find that our method has the following advantages:  ESPE is scalable in terms of the length of sequence and the number of items since the experiments show a linear growth when the sequence length and the number of items increase.

Not restricted by the minimal support threshold even though the minimal support can be very low.

In addition, the space complexity of our algorithm is O( 12  1 +nn ). If a database has N items and the length of  the longest sequence is n, we can calculate the memory space complexity as the formula: N2+?  =  ??? ?  ?? ?n  i  n iC  i  *)1  ( ,  and O( 12 1 +nn ) comes from this formula. N  2 is the space  of length-2 and nC3  is the space of length-3. Because we have encoded remaining sequences, the length-3 only needs one memory space unit and the length-5 only needs two units of the memory space. If we do not use encoding method, it needs three units of the memory space in length-5 as shown in Table 10. That is why we only enumerate 2-sequences and use interlace matrix to encode remaining sequences.

Hence, we can deal with longer sequences better than other methods.

Table 10. Space complexity  Length  Memory space Encode  Not encode  2 N*N N*N 3 nC3*1  nC3*1 4 nC4*1  nC4*2 5 nC5*2 nC5*3 ? ? ? n n  nC n *)1  ( ??? ?  ?? ?  nnCn *)2( ?   7. Conclusion and Future Work   In this paper, we propose a novel approach, called ESPE, to mining sequential patterns without setting minimum support threshold in advance. The ESPE enumerates all 2-sequences and finds frequent sequences by scanning the sequence database only once. Simple mathematical equations are used to compute the index of all 2-sequences and use efficient storage structure for support counting. We can also find the mining result efficiently according to user requirements. More importantly, our method can deal with incremental addition of new items and sequences.

There are several advantages of ESPE over other approaches:  (1) No multiple database scans are required because ESPE reads the database only once.

(2) No need to generate all candidate sequences.

ESPE can execute effectively and reduce the waste of memory space. Besides, it can process long sequences.

(3) No need to predetermine support thresholds because ESPE keeps support counts for all possible interesting sequences.

We have implemented the ESPE algorithm and  compared its performance with the PrefixSpan and FSE algorithms for large databases. Our performance study shows that our method outperforms PrefixSpan, especially for low support thresholds and long sequences. ESPE outperforms FSE in all cases.

In the future work, we will further improve the storage structure of the remaining sequences. There are a lot of interesting research issues related to ESPE-based mining, including the parallel sequential pattern mining and the mining of partial order, time-interval sequential pattern, multi-dimension sequential pattern [10][11], and closed sequential pattern [12][13].

Appendix: Proofs for Lemma 1 and Lemma 2  Proof of Lemma 1: Let A be an m?n matrix where m and n are in N. When i>j we can see Ai,j must be a rectangle in which the area of the rectangle is i?(i-1). Then we can calculate the value of index by using the formula for calculating the area of a rectangle. This simple formula is S = l ? w where S is the area, l is the length of the base of the rectangle, and w is the width of the rectangle. Now we have l= i, w=i-1, then we can calculate the area of the rectangle as a result of i?(i-1). At last, we can add j to this result to calculate the value of index. In the same way, when i<j, there must be a square in which the area of the square is (j-1)?(j-1). Hence, we can calculate the value of index by (j-1)?(j-1)+i. When i=j, there must be a square in S. The value of index is i?j.

Proof of Lemma 2: Let A be an m?n (m and n are in N.) matrix. If this matrix belongs to left triangular matrix, Ai,j=0 (if i?j), then we can calculate the value of index by using the formula for calculating the area of a triangular. The simple formula is S =1/2?b?h where S is the area, b is the length of the base of the triangle, and h is the height or altitude of the triangle. Now we have b= (j-1), h=(j-2), then we can calculate the area of the triangle as a result of 1/2?(j-1)?(j-2). At last, we can add i to this result to calculate the value of index.

Acknowledgements   This work was supported in part by the National Science Council, Taiwan, under Grants NSC96- 2218-E-007-007 and NSC95-2221-E-035-068-MY3.

