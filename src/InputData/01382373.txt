AN INHERITABLE CLUSTERING ALGORITHM SUITED FOR PARAMETER  CHANGING

Abstract: DBSCAN is a classic density based algorithm and it  dusters the data set according to the user input parameters.

This paper investigates how to inherit the mining results of last time when parameters change. A new incremental clustering algorithm IPC-DBSCAN is proposed, which gets the same result as that of rerunning DBSCAN yet high efficiency is achieved. Theoretical analysis and experimental results show that the proposed method reduces search space greatly and has novel efficiency. By interaction, IPC-DBSCAN gets the most satisfying result quickly and especially snits large volume data set.

Keywords  .

Data Mining; DBSCAN, Inheritable mining;  1. Introdudion  With data collection becoming easier and easier, many companies gather a vast amount of data. As a result, there has been an enormous change in the environment and objects of data mining. New characteristics emerge: first, the data is of great volume and usually high-dimensioned.

Second, the whole environment is dynamic, large number of updates happen frequently. In this kind of large dynamic data environment, traditional data mining method, which assume that the input data and parameter is static, is unsuitable. Data mining should not be looked on as a one-pass solo procedure. It is a continuous process working on evolving data and changing parameter. In this situation, inheriting the previous knowledge is the key, which avoids re-mining, saves the time and makes DM a continually running system. In fact, inheritable data mining is an attractive goal and become more and more important. Its basic idea is as follow: when data or parameter changed, the algorithm updates the mining result based on the former outcome. Compared with rerunning the algorithm, inheritable algorithms have much higher efficiency and can fit dynamic and interactive environment better.

In this paper, we focus on clustering. As we all know,  most of the well-known clustering algorithms require parameters, which have a significant influence on the clustering result. Struggling with the problem of updating clusters without frequently performing complete reclustering when parameters change, we present an inheritable clustering algorithm. Our algorithm is based on the efficient clustering algorithm for metric databases -DBSCAN ['I. As a density-based classic algorithm, DBSCAN clusters objects according to user input parameters e and Minpts. Reasonable parameter is key to the algorithm and can affect clustering result greatly. Users have to adjust E and Minpb again and again for better result.

This is an interactive processing and usually very costly if the data set is of large scale. So, it is emergent for effective inheritable algorithm to solve the problem We demonstrate the high efficiency of proposed method on some synthetic and real world data sets.

The rest of this paper is organized as follows. We discuss related work on inheritable algorithms in section 2.

In section 3, we briefly introduce the clustering algorithm DBSCAN. The clustering algorithm for inheriting the previous results to deal with parameter changed situation IF'C-DBSCAN is presented in section 4. The experimental analysis is reported in section 5 and section 6 concludes with a summary.

2. Related Work  Now, inheritable data mining is a new research area.

D.W.Cheung[2' first considered the efficient updates of association rules, and put forward the frst incremental Apriori algorithm named FUF' (Fast Update), which focus on how to get association rules in the whole data set, when new transaction set d added to D. In 2001, Ming-Syan Chen Chang-Hung Lee['] explored an effective sliding-window filtering (abbreviatedly as SWF) algorithm for incremental mining of association rules. By partitioning a bansaction database into several partitions, SWF employs a filtering threshold in each partition to deal with the candidate  ' . 0-7803-8403-2/04/$20.00 02004 IEEE  mailto:mail,nankai,edu.cn mailto:huangyI@office.uankai,edu.cn   Proceedqs of the Third Intemational Conference on Machine Learning and Cybemetics, Shanghai, 26-29 August 2004  itemset generation. Ester M presents algorithms for incremental attribute oriented generalization with the conflicting goals of good efficiency and minimal overly generalization. Yu Cai Feng [51 exploited two other novel incremental algorithm, IUA and PIUA. They mainly considered how to update the result effectively when support and confidence changed. Manin Ester [??? devised the first incremental clustering algorithm for DBSCAN.

According to the local property of DBSCAN, the new algorithm needs only to detect the affected area and then make some adjustment, which minimizes the search space and improves efficiency greatly.

3. The algorithm DBSCAN  The key idea of density-based clustering is that for each object of a cluster the neighborhood of a given radius has to contain at least a minimum number of objects.

Definition 1: (directly density-reachable) An object p is directly density-reachable from an object q wrt. E and MinPts in the set of objects D if  1 )  p E NE(q) (NE(q) is the subset of D contained in the E-neighborhood of q.)  2) Card(NE (4)) 2 MinPts.

Definition 2 (density-reachable) An object p is  density-reachable from an object q wrt. E and MinPts in the set of objects D, denoted as p > D  4. if there is a chain of objects p l ,  ..., pn. p l  = q. pn = p such that pa  E D  and pr+l is directly density-reachable from p L  wrt. E and MinPts.

(Figure 1)  Definition 3: (density-connected) An object p is demityconnected to an object q wrt. E and MtnPts in the set of objects D if there is an object o E D  such that both p and 4 are density-reachable from o wrt. E and MinPts in D.

(Figure 2)  To find a cluster, DBSCAN starts with an arbitrary object p in D and retrieves all objects of D density-reachable from p with respect to E and MinPts. If p is a core object, this procedure yields a cluster with respect to E and MinPts. If p is a border object, no objects are density-reachable from p and p is assigned to the noise.

Then, DBSCAN visits the next object of the database D.

There are three kinds of objects sets: core objects set, denoted as COreD; border objects set (not a core object but density-reachable from another core object), denoted as BorderD; noise objects set (not a coreobject and not density-reachable from other objects). (Figure 3)  ,  Figure 1. Density-reachable Figure 2. Density-connected  Figure 3. Three kinds of object  4. IPC-DBSCAN  In DBSCAN algorithm, the users have two kinds of requirements for the result: I )  acquisition of clusters with high density, 2) acquisition of clusters with low density.

Both of the above needs can be met by adjusting E and Minpts and rerunning the algorithm until satisfactory result is achieved. But the cost of this method is?very high on very large database and no previous mined knowledge is saved and utilized, which wastes much processing time. So, from the viewpoint of timely response, inheritable.updating algorithm is desirable. Obviously, the density of clusters gained is in inverse ratio with E and direct ratio with Minpls.

To seek clusters with high density, we can decrease E or increase Minpts. On the contrary, to seek clusters with low density, we can increase E or decrease Minpts. The two parameters play an opposite role but can anain the same  -purpose in affecting clustering result. To simplify the analysis of the problem, we can only consider the case of Minpts increase and decrease.

Lemmal: on data set D, we get k, clusters by DBSCAN under parameter E andMinPts. Denote the set of points contained in cluster as C, then C= C l u C 2 u C ~  ... uC~,.where C, is the points set of f h  cluster.

The set of noise object is denoted as NoiseD; We get k2 clusters by DBSCAN under parameter E? and MinPts?.

Denote the set of points contained in cluster as C?, C?= C?luC?2uC?3.. .uC?m. The set of noise object is denoted as NoiseD?. D=CuNoiseD,C? uNqiseD? then:  1) if MinPts> MinPts?, then CcC: Noise&NoiseD? 2) if MinPts< MinPts?, then Q C ? ,  Noise&VoiseD? Definition 4: changing object: the point whose core  object property changed. Denote ChangedD=( ?dpl@sCoreD wrt. MinPtsAp c wn.MinPts?) v@ e wn.MinPts ?)  denote the affected area as:  wn. MinPtsAp E CoreD  Definition 5: affected area: ifpieChangedDthen we  ? Aflectedo(pi)= NE fpi) u [ q l  3 o E NE(pJ A4 > D b 1 Definition 6 (seed objects for the update) we define     Proceedings of the Tbird Intemationd Conference on Machine Laming and Cybemetics, Shanghai, 26-29 August 2004  the following notions: !

Upheed= [ qlqs CoreD WrthfinPts', 3q's Chnnged,Aqs  N W ) ) . We call the objects qE Updseed "seed objects for the  update". Note that these sets can be computed rather efficiently if we additionally store for each object the number of objects in its neighborhood when initially clustering the database.

Defdtion 7: key core objects and non-key core objects: an object p is density-reachable from an object q wrt. E and MinPts. Delete the core object pi  in the core objects chain p,.  p2, ...,p., if there is not an object p i  to maintain p,. . pi'.., p .  as a core object chain, then pi is the ,key core object of the chain. Otherwise, pi  is @led the non-key core object of the chain.

It is self-evident of Lemma 1's correctness. It demonstrates that when MinPrs decreases, the original clusters will expand and part of the noise objects may be absorbed into some clusters. So our algorithm should work on the set of noise object, i.e. Noise,, .On the other hand, when MinPts increases, the points in a cluster may become noise object and our algorithm only need to work on the set of points contained in clusters. Lemma 1 is the foundation of IF'C-DBSCAN. Because of the existence of changing objects, some density-reachability may be built or desmyed, which leads to the change of clustering result and so rerunning algorithm is needed. The affect of each changing object is local and can only cause the core object  , ' property change of data in the affected area Outside of this area. the core object property will not be affected. Update seed set is the, beginning of searching the affected area.

Expanding from this set, the searching proceed can cover all the affected area of all changing objects.

The basic idea of E-DBSCAN is as follow: 1) According to changing objects set, find seed objects for the update. 2) Perform density-connection expanding from each seed object. 3) During the expanding process, adjust objects' cluster label if needed in affected area. During the above procedure, key core objects and non-key core objects are of great importance to PC-DBSCAN: If a key core object disappears (deleted or changed into a non-core object), the density-connection of the cluster will be destroyed, which leads to the split of the cluster. If a new key core object is created, the merge of some clusters may happen. To a non-key core object, its addition or deletion won't cause chain-reaction. If the algorithm runs and the existence of only one pi' is detected (another density-connection path exists), the connection propeq of the original clusters is guaranteed and the search process  In the implement of PC-DBSCAN, to get the changing objects set, the following information of +its in  :  :  ' canstop.

each node's neighborhood is saved: 1) Number of objects in the neighborhood, denoted as Neighbors.counr, 2) Identity of each object in the neighborhood and its distance to the core object. According to the above information, the changing objects can be detected by only one scan of Neighbors.coun?. Additionally, when cluster merge occurs, we need only record the label of the cluster that is to be merged and change the label at one time when expanding finisb. In this way, the time cost of the algorithm can be greatly reduced and efficiency be greatly enhanced.

4.1. The case that MinPts decreases  When MinPts decreases, the threshold for a object to become a core object decreases, some sparser populated regions can become part of a cluster. In this case cluster expands and never shrinks. For the three different objects in the dataset: I )  Core objects: remain core.objects, and will not cause  any changes to the clustering result.

2) Border objects: If the number of points witbin its  E-neighborhood is smaller than the new MinPts value, a border object remains a border object. Otherwise, the border object will change into a core object and thus belong to the set of changing objects.

3) Noise objects: The noise objects may become core objects, and thus belong to the set of changing objects.

When h4inF'ts decreases, the changing objects will  cause the following cases to the clustering result:  0 _.~..._ 0 : 0 ..,,  I Ooo i ,,,,. ' _;:  -.._.-  C s $ s l  No rh-. cas* 1: CreafioW  _.-._ soo 0') Fb;oo,o Q~...' I.. iB.p-....p'. o.!

-"'0 ',.. o:, 0  . Q  ',. ....- 0.:' o$Q  ......,.

C u e  3: Hxpsndiw  Figure 4. Four cases for MinF'ts decrease  1) No change: If ChangedD is empty, no change bappens.( Fig. 4 case 1) -  2 )  Creation: If ChnngedD includes some of the former noise objects, new clusters will emerge.(Fig. 4 case 2)  calm 4 Merea.'     ,   3) Expanding: If Changedn include some of the former border objects, the cluster contains such border objects will expand.(Fig. 4 case 3)  4) Merge: The cases 2) and 3) both can lead to mergence of some closely located clusters.(Fig. 4 case 4)  Table 1. Pseudo code for MinF?ts decreases If Changedn=O then Msgbox ?No change? Else For each object oi in Changedn  Para-changed-expand() Next Endif Merge clusters in Merge-queue Para-changed-expand() If (oi is a noise) then Current id =a new cluster id //create cluster Else Current id=the id of oi //border obj expanding End if For each oj in NE (oJ If (oj is a noise) then  idoj = Current id if(oj E Changed,?)then  End if  Save idoj to Merge-queue /I to merge  put oj into Seed-stacid/ to expand  Else  End if Next While Seed-stack is not empty Current-object= Seed-stack.top() For each ok in NE (Current-object)  If (Ok is a noise) then idd = Current id if(ok E Changed, )then  End if  Save id, to Merge-queue  put ok into Seed-stack  Else  End if Next Seed-stack.pop() Retum  From the above analysis, we know that Changed&BorderDuNoiseo. So it is easier to find Changedn. Beginning with the changing objects, IPC-DBSCAN focuses its search in the affected area, which is much smaller than the whole data set. on which  re-DBSCAN work. Its performance advantage is obvious: By inheriting the previous clustering information, it greatly reduces search space. Most of its process is just limited in BordernuNoisen.

, -? , ...,  c u e  3 s a d  cusp: Re?&  . Figure 5. Four cases for MinPts increase 4.2. The case that MinPts increases  When MinPts increases, the threshold for an object to become a core object increases, only the denser regions can form a cluster and the previous clusters shrink. For the three different objects in the dataset: 1) Corepbjects: if still satisfying the new Mi& value,  it remains a core object, otherwise, it becomes a border object or noise, which means it belongs to Changedn.

border objects: A previous border object may become a noise object(when there are. no core objects in its E-neigbborhood)or remain a border object(when a new core object exists in its &-neighborhood). But such objects do not belong to the changing object set, and will not cause other points to change.

3) noise object: The noise objects will remain noise objects, and will not cause any change to other objects.

Wheu MinPts increases, the changing objects will  cause the following cases to the clustering result: 1) No change: When ChangedD is empty, no change  happens; (Fig. 5 case 1) 2) Removal: If the only core object in the previous cluster  becomes a non-core object, the cluster should be removed; (Fig. 5 case 2)  3) Shrink: Some of the previous core objects may become non-core objects, the border objects in the neighborhood of such core objects will become noise unless other core objects in the neighborhood exist.

In this case, the cluster shrinks. (Fig. 5 case 3)  2)     Proceedu& of the Third Intemational Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August 2004  4) Split: The disappearance of core objects that work as a bridge between two high density regions will cause a previous cluster divided into two clusters. (Fig.5 case 4) When MinPts increases, Changed& CoreD, and  changes are limited within the clusters in the previous result. In each cluster, we adopt proper update .strategies according top , the ratio of the changing objects number to the total point numbers in each cluster, which is defined as follow: .

p=IChangedciIACiI Where Changedais the set of the changing objects in  Ci. ICiI and? IChangedcil are the number of total number points in the cluster and the number of changing objects respectively. Then the update strategy is: 1) When p is greater than a thresholdTHRESHOLD, the  DBSCAN algorithm is rerun on the set of the new core objects to get a new cluster result.

2) When p is smaller than or equal to THRESHOLD, algorithm Et-DBSCAN is executed to update the cluster result.

In the second situation, IPC-DBSCAN gets  Changedci and the seeds for update Updseed by retrieving the inherited information. Then, the connection of core objects in Updseed is detected: there are two cases 1) during the expanding of one object, the queue of the update seed is covered completely. 2) There are still other objects in the queue of update seed when the expanding of one p i n t  ended. The first situation means that there are no key core .objects in Changedci and the connection of seed  ?objects is not destroyed. While the second situation indicates the existence of key core objects in Changedci, which induce the split of cluster. With each expansion ended, a new cluster is created. Different from DBSCAN algorithm?s depth first search, width fmt search is employed here, i.e. store the update seeds with queue not stack.

When checking the C O M ~ ~ ~ ~ O U  of seed objects, width first search is utilized. In the worst situation, an object exoands to the whole original cluster before the seed queue  experiments are conducted on a PC with Intel Pentium 3 Table 2. Pseudo code for MinF?ts increases .If ChangedD=O then Msgbox ?No change? Else For each Cluster Ifp,,,,>THRESHOLD then  Else  End if End if Para-changed-expand0 Updseed=cetSeedobj(om~~) For each oi in Updseed  If(not Expand(oi)) then  Else  DBSCAN(Dm-mmobj)  Pan-changed-expand()  Mark,obj in D,ew with a new id  Mark obj in DlnP with original id Exit for // without split  End if Next Retum  Put obj in Neighborhood(o3 to Dlew Put new core obj in NE (0;) to Seed-queue While Seed-queue is not empty  Current-object= Seed-queue.headO For each ok in NE (Current-vbject)  Put obj in NE (oJ to D,ew Put core obj in NE (oJ to Seed-queue  ExpaMoi, Dtew)  Next Seed-queue.delete0 If(UpdseedG Dtcw)  Expand=l// Updseed covered in expansion Return// finish searching in advance  End if Expand=O// Updseed not be covered  I  is empty. That is to say, only by checking all the core objects the algorithm aff i i s  the connection of the objects in the seed queue. In this situation, the complexity is similar to that of running DBSCAN. But mostly, Ipc-DBSCAN can fmish its searching process in adviuae because of using width first search.

5. Experimental result  5.1 Test Environment and Data Sets  processor and 128M SDRAM memory and Microsoft SQL Server 2000 database system.

In the fmt  two experiments, we used Matlab and randomly generated two groups of synthetic data, all of which follow Normal distribution (Figure 6). We use UCI standard dataset in the last three experiments, that is:  1) Wine dataset, 13 dimensions, and 178 entries 2) Vowel dataet, 11 dimensions, (Non-numerical  3) Letter-recognition dataset. 16 dimensions, and attributions are extracted), and 990 entries  2oooO?entries I  We experiment on five different data sets. All of our     53 experiment p d u r e  6. Conelusions  Now we will compare the performance between the IF?C-DBSCAN and DBSCAN rerunning. In order to analyze the accuracy and efficiency of the algorithm, we record the time cost and search space of Ipc-DBSCAN and DBSCAN separately (Table 3 and 4).

Table 3. Changi h4inpts from 15 to IO DBSCAN 1 IK-DBSCAN  MinPts I MinPts? I Time I Searchspace Data sets i w i i  )) 1 8 m  I l b s  I_ / E  1  Vowel 59ms 67ms 17ms letter-rec 6ooms 6 7 h  54ms s 1100 3oms 3 b s  lOms 30% s loow 2x)ms 2Mms 39ms 20%  I    Figure 6. Data distribution Figure 7. Different data scale  5.3. Analysis of the experiment result  There are several advantages to use E-DBSCAN: Fmtly it is its Correctness. The result of Ipc-DBSCAN is the same as that of re-conducting DBSCAN, which is the base for existence of inheritahle algorithm. Secondly, IPC-DBSCAN dramatically lower the searcbing space (see Table 3and 4, which has a small proportion in the dataset) and shoaen the running time, usually 5-10 time faster than traditional DBSCAN. Additionally, we know from Figure 7, the larger the database is the more effectively IPC-DBSCAN can enhance efficiency and outperform DBSCAN. While this just illustrates the application of IPC-DBSCAN: parameter changing on large volume data.

In this paper, an inheritable clustering algorithm is proposed, to be suitahle to parameter changed situation.

The experimental results show that IPC-DBSCAN can get the exactly same clustering result to that of rerunning DBSCAN and improve the efficiency tremendously. This algorithm performs better in larger volume data.

Acknowledgements  This paper is supported by the Ministry of Education of China under the grant 02038, and by the Tianjin Natural Science Foundation under grant 0036003 11.

