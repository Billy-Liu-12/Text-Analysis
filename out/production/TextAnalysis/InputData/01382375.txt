MINING FREQUENT PATTERNS BASED ON IS+-TREE

Abstract: Frequent patterns mining play an important role in data  mining research. It's groundwork of other data mining tasks.

In this paper, a novel algorithm is presented for mining frequent patterns based on static IS+-tree, and is compared extensively with other classical algorithms such as Apriori and FP-growth. The algorithm builds frequent patterns directly, instead of wing high-cost candidate sets generation-and-test method adopted hy Apriork it works on a static IS+-tree, instead of costly dynamic trees adopted by FP-growth; it consume smaller size of main memory and is more efficient than others. Above all, IS-tree is a general index model and has been widely wed in full text storage and index, time series patterns mining and many other fields.

Keywords: Frequent pattern: frequent item set; IS-*, LS-tree  1. Introdnetion  Frequent pattems mining is a key step in many data mining problems, such as association rules mining 11-41 , time series patterns mining[8], classification[6] and so on.

Let I=/ i , , i2 , .  ..i,/ be a set of items. Let database D be a set of transactions, where each transaction T is a set of items such that T c  I. Let X be an item set. A transaction T is said to contain X if X Z T .  The support of X is the numbers of transactions that contains X in D. X is frequent when the support of X is no less than user defined threshold value E .

Frequent patterns mining is usually divided into two categories in terms of the methods being adopted. One is to find frequent pattems by candidate sets generation-and-test.

The other is to build frequent pattems directly.

Apriori algorithm is the most famous one of the first category [l]. Its essential idea is to iteratively generate the set of candidate pattems of length (k+l) from the set of frequent pattems of length k. Most of the previous studies adopt an Apriori-like approach, such as 12.3.41. Whereas, the candidate sets generation-and-test is always a bottleneck of these methods.

FP-growth is one of the most efficient methods among the second category [5 ] .  It calculates the frequent-1 item set first, which forms a compact frequent patterns tree @"-tree). A partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks of mining confined pattems in conditional databases.

Whereas, FP-growth method needs to build the conditional FP-tree dynamically. The number of them grows in proportion of the length of pattems, which costs too much both for time and space.

In this paper, a FP-tree-lie, but more concise and more extensive mathematical model-IS'-tree is developed to assist the mining task. The algorithm generates frequent item set directly, instead of using the high-cost candidate sets generation-and-test; it works on a static IS+-tree, instead of creating trees dynamically as FP-growth. The empirical results show that our algorithm is more efficient, especially on dense databases at all levels of support threshold, and it consumes smaller size of main memory as well .Above all, IS-tree model is a general index model and used widely in full text storage and index, mining time series and many other fields[7,8].

The paper is organized as follows: basic concepts and definitions about the IS-tree model are discussed in section 2. The algorithm for mining frequent patterns using IS+-tree is developed in section 3. It is empirically compared with FP-growth and Apriori method in section 4. Section 5 concludes this paper.

2. Basic concepts and d e f ~ t i o n s  of ISt-tree  Inter-Relevant Successive Trees(IS-tree) is a novel mathematical model we proposed recently for solving the storage and index problem in full text datahaseL71. It makes use of the property of redundancy and lexicographic order which naturally exits in full text database. Just E tkc text, there also exists much redundancy among the transactions.

If we think the transaction database as full text, every transaction a sentence, every item a character, all distinctive items an alphabet, and introduce the concept of order into  0-7803-8403-2/04/$20.00 @ZOO4 IEEE  http://163.com http://163.com mailto:yfhu@fudan,edu.cn    transaction database, we can also extend the Inter-Relevant Successive Tree model to transaction databases. It's interesting that a FP-tree is essentially an IS-tree. The reason we call it IS+-tree rather than FP-tree is that, first, it is extension of IS-tree model into transaction database; second, it's more concise, not having the link pointer as FP-tree.

Definition 1 (Partial order relation) A relation 4 on a set S is called a partial order relation if it is reflexive, antisymmetric, and transitive. The set S together with the partial order 4 is called a partial order set, or simply a poset.

Definition 2 (ISt-tree) Suppose there he a relation 4 among items in transaction database D. IS'Ltree is a compressed representation of D. It is a ordered tree, which has a root node 4, and a strictly order exists among predecessor and successor and among brothers. Suppose introducing a puppet item 4in every transaction, and each transaction being a poset, then, each transaction is corresponding to a path from root 4 to some node, and same prefix of transactions shares same path. If each item has a unique identity ID, then each node of IS+-tree has two field: field ID is used to identify an item, field count to indicate the numbers of transactions which share the path string from root to that node.

Definition 3 (Leaf-node vector) All the led-nodes n , associated with an item, U, constitute a vector 5, called leaf-node vector of item a. And ni is called an entry.

Property 1 (Leaf-node property) For the last item U, among the alphabet in transaction database D, all transactions that contain it can be obtained by starting from the leaf-node vector Z and tracing the IS+-uee up to the rwt. We call the traversed part of the tree sub IS+-tree. And its associated database projected database. Suppose ? f j  be removed, a new leaf-node vector, ai, being available, and ai being the nearest item from a, in the alphabet, which forms another sub IS"-tree. This process can be performed recursively. Thus all leaf-node vector of static IS'-tree and its associated sub IS+-tree can he available.

Let the sub IS+-tree that associated with vector 5 be denoted as IS,,, , then there also exists led-node vector bound on IS,,), we denoted it as b (a),  the bound leaf-node vector also form a sub IS+-tree, and this process'also can be performed recursively. We denote these bound sub tree as IS,, ,.,, o , ) ,  and the bound leaf-node vector as b  IS (oj , , ,o,)  represents the projected database that is composed of all transactions that contains item set [ a ,  ... a jJ .

Each element in leaf-node vector has two fields: one is a pointer pointed to the entry; the other is base count which  -  - ,.,,  plays the same role as the count field of node entries. For convenience, we use b .ID denotes the item ID that associated with ii _.

Lemma 1 (base count number) The base count of an element in a bound leaf-node vector on a sub IS'-tree is the same as that of the bottom-most leaf-node vector that forms the sub IS+-tree.

Lemma 2 (count merger) Let n is some node of a sub IS'-tree which formed from a (bound) led-node vector 5 , the count of it is sum of all the base connts of elements that are descendants of n in z .

Example 1 Consider the transaction datahase(in lexicographic order) of table 1 that consists of a alphabet I=( O,u,b,c,d,efl. Its IS'-tree is like figure 1  Transaction Item set b ,  a ,  b, c, b , a, c, d,  T3 b ,  b, c, d T4 b ,  b, c, d  A a:2 n  Figure 1 IS+-tree of example 1 "heorem 1 (Completeness) IS+-tree contains all  information related to mining frequent pattems and frequent closed patterns..

Proof is omitted,  Ming frequent patterns based on ISt-tree  How to mine bequent pattems based on static ISt-tree?

3.

It's natural that we adopt a divide-and-conquer method, mining the frequent pattems on the static IS+-tree in a bottom-up, depth-first and pattern-growth way. What we called pattem-growth is: Let IS (q,,,q) be a bound sub IS'-tree, and if a is a frequent item in the projected database associated with IS (4 ,,,4 ~, then 0 U(b,, ... bkJ is a frequent patterns. When this process perfomed recursively, all combinations of frequent pattems can be generated.

1 209     Definition 4 (Ordered leaf-node queue) A queue containing leaf-node vectors which ranked by frrquency ascending order of its associated item is called ordered leaf-node queue.

Example 2. Consider a transaction database of table 2 that contains 6 transactions. The transaction has been ranked by frequency order with a alphabet ( 6 ,f,c,a,b,m,p) and items that do not exit in alphabet has been pruned .

Suppose the minimum support threshold is 3. It?s associated IS+-tree is like figure 2.

Table 2 database for example 2 I Transaction I Itemset  m  Figure 3-g Figure 3 -h Figure 3-i Figure 3 leaf-node queues of example 2  Then, each vector in the queue should be popped up and its associated projected database should be mined by bottom-up and depth fust strategy, until the global queue is  At present, only the leaf-nodes contained in the top vector is complete. The top vector p is popped from global queue, and it becomes the current vector. The item associated with it is put out as a frequent-I item. All nodes contained in vector p trace up one step(imagine they are removed from the tree), and thus new leaf-nodes are obtained, which are I& and m a .  These nodes are pushed into global queue, til the same time, a local ordered queue associated with the current item p is created, which represents the projected database associated with p. These new leaf-nodes are pushed into it also. The queues formed look like figure-3h and figure-3c.

For queue (p). the leaf-nodes contained in the top vector m are complete and popped up from it. The support of its associated item is summed up from these nodes. It is 3. Since it is qualified, the combined string (pm) is put out as frequent-2 itemset. AI1 nodes contained in vector E trace up one step, and new leaf-nodes are.

obtained, which are a@ and c a .  These new led-nodes are pushed into queue (p) and another new ordered queue associated with (pm). Current queues look like figure 3-d and figure 3-e.

Notes: fustly, according to lemma 1, the number of transactions share the same path is recorded in the base count field of bottom-most nodes. Secondly, what stored in vectors in queues is the node?s Object ID, that is, the object address or pointer. Hence, it incurs small space cost.

Thirdly, it should be avoided that the same node is put into same queue twice. This is the case that when tracing up, different nodes of the same item may meet at same node.

empty.

For example, when b@ and b@ trace up step by step, they will meet at node fa. Thus, fo should only be put into the queue associated with b once(it is easy to implement through the Object ID of fo, without extra data structure). However, the support they bring should be summed up according to lemma 2..

For queue (pm), the top vector 5 is popped, and the support of the associated item is calculated. Notice that though the support of item a is 3, it is indeed 2. This is because a@s support is decided by p@ on the same path.

Because it?s not qualified, no new local queue is created nothing is put out. All nodes contained in vector E still need trace up one step, and new leaf-nodes are obtained.

It?s CO. It?s only be pushed into the queue (pm). The cnrrent queue (pm) looks l i e  figure 3-f.

Since queue (pm) is not empty, it should be performed continually. Now the leaf-nodes contained in the top vector z are complete and popped up, and their support is calculated. It?s 3 which is brought from p@ an p@.

Because it is qualified, the string (pmc) is put out as frequent3 itemset. All nodes contained in vector C still need trace up one step, and new leahodes are obtained.

Now, c@ reach 4 ,  thus only one new node is obtained. It is f@ and pushed into both the queue @m) and a new queue associated with (pmc). Current queues look like figure 3-g and figure 3-h.

Next, queue (pmc) should be handled. The top vector is popped, and because the support of p@ that is at the  same path with f@ is only 2, it?s not qualified. The nodes contained in vector f are traced up one step. Since fo meets e ,  no node is pushed into the queue. The queue is looks like of figure 34. Now queue of (pmc) is empty and process about it is completed.

The procedure IeNIIIs to the state of figure 3-g. The queue of (pm) should be performed continually. The top vector f is popped, and the support of its associated item is not qualified. The queue becomes empty, and process about it is completed. The procedure returns to the state of figure 3-d.

Now, the top vector of queue associated with p is 5 .

It can he handled just l i k e E .  This process continues until queue of p is empty. The procedure will return to the state of figure 4-b. The queue of d, will be handled continually.

In this course, 5 ,  C, f are pushed into the queue one after another. And the leaf-nodes contained in the top vector will be complete. They are handled just like p.

When the global queue is empty, the procedure. is  -  -  completed.

Having example 2, it?s not difficult for us to present  algorithm 1. The algorithm uses IdList, the union of item ID, to denote a string of a item set. The gQueue is used to denote global queue. The lQueue is a local queue and nQueue is a new queue.

Algorithm 1: Frequent itemset mining based on 1s+-tree  Input: an tree IS; minimum support threshold E Output: complete frequent item set steps: I.

2.

3.

4.

5.

6.

7.

8.

Initialize IdLisf with 4 , and gQueue with current leaf-nodes; While gQueuefnull, do:  frequent-I item; Fetch the top vector i, putting i .ID as  IdLisf = IdLisr U T .ID; Create lQueue associated with IdLisr,  tracing up one step from I and getting new nodes;  Put new nodes into both gQueue and lQueue., for lQueue, recording the base count brought by the nodes in I :  End; call mineIS(ldLisr, lQueue, e )  //return to step 1 and continue Function: mineIS(ldLisr, lQueue, E ) Input: an string IdLisr; its associated queue lQueue;  Output: All frequent itemset that can be extended minimum support threshold E  from IdLisr steps: 1. While lQueuefnul1, do: 2.

3.

4. If i, 2 &,then: 5 .  IdList = IdList U i .ID; 6. put out frequent item set IdLisr;

I. Create a new queue, nQueue,  associated with IdLisr, 8. Trace up one step from f ,  getting new  nodes; 9. Put new nodes into both 1Queue and  nQueue, recording the base count brought by the nodes in I ;  Fetch the top vector i; iethe support of i ;  -  10. Call mineIS(ZdLisr, nQueue, h ) ; 11.

12.

Else /I is < E ; Trace up one step from i, getting new  nodes;      Dataset( items /transactions)  Chess (7613,196)  Connect (130/67,557)  Pumsb ' (2,114/49,046)  13. Put the new nodes into [Queue, recording the base count ;  14. End: I/ retumto 1,continue The algorithm is simple, clear and efficient. Notice  that since the mining task is performed on a compact static IS+-tree, avoiding to allocate memory and then release them for dynamic trees, which may cost too much both for time and space of memory when mining long pattems. The ordered leaf-node queue only stores the pointer of nodes in ISf-tree , so it incurs small size of memory and reduce the  ' space consumption.

4. Experimental evaluation  4.1. Environment and dataset characteristics  Pattem-number(supp)  255,985(0.6), 1,272,932(0.5) 6,472,980(0.4), 7,501.582(0.3)  27.127(0.9).534,231(0.8) 4,130,671(0.7)21,252,795(0.6)  2,608(0.9),142,194(0.8), 2,698,761(0.7), 9,537.36X0.6)  (120/8,124) 23,596,649(0.02)  (1,000) 12,301(0.004), 92,472(0.02)  4.2. Experimental results  We give experimental results for every dataset. The result is showed in figure 4 and figure 5. The horizontal ordinate records mining time with seconds, including the time putting out to file. The vertical ordinate records support, which is normalized with range from zero to one.

For dense database, the Apriori algorithm is degraded, too quickly to be compared with the other two. For example, when support is set to 0.6 for chess, Apriori has cost 653 second, whereas IS' and FP-growth only cost 8 second and 13 second respectively. Aprion is 81.6 times slower than ISf. Therefore, for dense datasets, our algorithm is only compared with FP-growth, as figure 4 shows.

As we can see, for dense datasets, IS' is always outperforms Fp-growth and when the support threshold is lowered, the gap between them widens.

For sparse dataset, the result is showed in figure 5.

When the support value is high, Apriori is as competitive as the other two. This is because the generated frequent patterns is usually short (4) and Aprioir need not scan the database many times. However, when the value of support is lowered, the number of the pattems become great and the length become long, which makes the performance of Apriori degrade sharply. IS' is only trivially faster than FP-growth on sparse dataset, since Fp-growth need not create too many dynamic trees on it. The memory usage (peak value) of IS+ is always smaller than FP-growth in all cases.

n   no0   f -4- FP-growth  0. 7 0. 6 0. 5 0 .4  0.3  Figure 4-a chess      t F P - g r o w t h            0. 9 0.8 0. 7 0. 6  Figure 4-b connect  t FP-growth  3. 1 0. 08 0. 06 0. 04 0. 02  Figure 4-c mushroom  1 +FP-growth +IS+  0 .9  0.8 0. I 0.6  Figure 4-d pumsb  +IS+  -S-Apriori  0.01 0.008 0.006 0.004 0.002  Figure 5 T25110D50K  5. Conclusions  In this paper, we present a novel algorithm for mining frequent items sets based on IS+-tree model. Compared with classical algorithm Apriori and FP-grow, our algorithm is the more efficient on both dense datasets and sparse datasets. IS+-tree model has become part of our general model we are studying that is adapted well to many types of data.

Acknowledgements  This paper is supported by national natural science fund(60173027), China.

