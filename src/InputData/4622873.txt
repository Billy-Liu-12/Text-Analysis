Probability Distribution Reconstruction for Nominal Attributes in Privacy Preserving Classification?

Abstract  Concerns about privacy of data used in Data Mining have emerged recently. Users are afraid of misuse of this data and discovered knowledge. Thus several methods of preserving privacy classification have been proposed in lit- erature. One of these methods enables miners to use con- tinuous and nominal attributes simultaneously in classifica- tion.

Reconstruction of probability distribution is an impor- tant task in privacy preserving classification for both nomi- nal and continuous attributes which were distorted with the randomization-based technique and are stored in central- ized database.

We present the new algorithm - EQ - for reconstruction of probability distribution of nominal attributes, which outper- forms former algorithm especially for high privacy levels.

Effectiveness of the new solution (information loss in reconstruction of probability distribution of nominal at- tributes and accuracy of classification) has been tested and presented in this paper.

1 Introduction  Data Mining is used in many areas of life. Thus large amounts of data have been collected and stored, often auto- matically.

Users are afraid of revealing sensitive values or discov- ered knowledge. It may lead to inaccuracy of the provided data and the models. To lower these concerns privacy pre- serving techniques have been proposed. Privacy is consid- ered in classification, clustering, association rules etc.

There are several methods of privacy preserving classi- fication for centralized data distorted with randomization-  ?Research has been partially supported by grant No 3 T11C 002 29 received from Polish Ministry of Education and Science.

based techniques. Reconstruction of probability distribu- tion for both nominal and continuous attributes is used in this process.

While building decision tree we may use nominal and continuous attributes simultaneously. Unfortunately in- corporating privacy with randomization-based technique causes information loss.

The new algorithm presented in this paper gives better results for reconstruction of probability distribution of nom- inal attributes stored in centralized database and distorted with randomization-based technique. It reconstructs prob- ability distribution with lower information loss, especially for low probability of retaining the original value i.e. high privacy levels.

1.1 Related Work  Privacy Preserving Data Mining in classification has been extensively discussed recently [2] [5] [1] [4] [8] [9] [7] [3].

Papers [5] and [8] represent cryptographic approach to Privacy Preserving. We use different approach ? randomization-based technique.

Privacy preserving for individual values in distributed data is considered in [9] and [7]. In these works databases are distributed across a number of sites and each site only willing to share mining process results, but does not want to reveal the source data. Techniques for distributed database require a corresponding part of the true database at each site. Our approach is complementary, because it collects only modified tuples, which are distorted at the client ma- chine.

Agrawal and Srikant [2] proposed how to build decision tree over centralized data disturbed with randomization- based technique. Presented algorithm uses only continuous attributes.

Paper [1] extends this solution, but it does not take into account nominal attributes either.

DOI 10.1109/ICHIT.100    DOI 10.1109/ICHIT.2008.253     Multivariate Randomized Response technique was pre- sented in [4]. It allows creating decision tree only for nom- inal attributes.

The solution showed in [3] differs from those above, be- cause it enables data miner to classify (centralized) per- turbed data containing both continuous and nominal at- tributes modified using randomization-based techniques to preserve privacy on individual level. This approach uses EM/AS algorithm to reconstruct probability distribution for nominal attributes and the algorithm for assigning recon- structed values to samples to build decision tree simultane- ously with nominal and continuous attributes.

In this paper we propose the new algorithm - EQ - for reconstruction of probability distribution of nominal at- tributes. It outperforms EM/AS algorithm, especially for high privacy level i.e. low probability of retaining original value of the nominal attribute.

1.2 Contributions of This Paper  Proposed solution gives better estimates of probability distribution of nominal attributes which were modified with randomization-based technique to preserve privacy on indi- vidual level.

1.3 Organization of This Paper  The remainder of this paper is organized as follows: Sec- tion 2 revises EM/AS and the algorithm for assigning recon- structed values to samples for nominal attributes. In Section 3, we present our new algorithm for reconstruction of prob- ability distribution of nominal attributes. The experimental results are highlighted in Section 4. Finally, in Section 5, we summarize the conclusions of our study and outline future avenues to explore.

2 Revision of Solutions for Nominal At- tributes  To build decision tree over the data containing nominal and continuous attributes distorted with the randomization- based technique algorithms shown in [3] can be used. This section presents revision of those algorithms. The remain- ing problems should be solved in the way proposed in [2] and [1].

2.1 Modification of Probability Distribu- tion Reconstruction Algorithm  To reconstruct probability distribution of nominal at- tribute we use EM/AS algorithm proposed in [3].

EM/AS algorithm is based on two algorithms AS pro- posed by Agrawal and Srikant in [2] and its extension EM  presented in [1]. Both algorithms reconstruct probability distribution of continuous attributes.

To reconstruct probability distribution of nominal at- tribute both EM and AS algorithms were modified to ob- tain EM/AS (Algorithm 1), because modifications of both algorithms (AS and EM) give the same result.

The algorithm solves the following problem: we have nominal attribute X with the values v1, v2, v3, ..., vk and n samples. Value for each sample is modified according to probability Pr(vp ? vr) (probability that value vp will be changed to value vr) and we want to reconstruct origi- nal probability distribution of attribute X . X(s) means the value of attribute X for sample s.

Algorithm 1 EM/AS - nominal attribute probability distri- bution reconstruction algorithm  Pr(X = vp)0 := 1k , p = 1, ..., k j := 0 //iteration number repeat Pr(X = vp)j+1 =  n  ?n s=1  Pr(vp?X(s))Prj(X=vp)? k t=1 Pr(vt?X(s))Prj(X=vt)  j := j + 1 until(stopping criterion met)  We start with the uniform distribution and calculate esti- mate of probability distribution in every iteration.

Stopping criterion is the same as for AS and EM algo- rithm (we stop when the difference between successive esti- mates of the original probability distribution becomes small ? 1% of the threshold of the ?2 test).

2.2 Assigning Reconstructed Values to Samples for Nominal Attributes  The algorithm for assigning reconstructed values to sam- ples for nominal attributes was presented in [3]. We revise this algorithm in this section.

Having probability distribution reconstructed, we can as- sign reconstructed values to samples, what allows us to choose the best test for a tree node using gini index1. As- signed values should not be treated as estimates of original values.

The problem to be solved is as follows:  We have modified values of nominal attribute, so we have probability distribution of modified attribute (i.e.

P (Z = vi), i = 1..k), the number of all samples n and re- constructed probability distribution (P (X = vi), i = 1..k).

We want to assign samples to reconstructed values taking into account reconstructed probability distribution.

To solve this problem we count the number of distorted samples separately for each value of attribute (nZ(vi)) and  1For details about gini index see [6].

0 1 2 3 4        Probability = 0,3  Indexed value  N um  be r  of s  am pl  es  ?  ?  ? ?  ?  ? Oryginal Modified EA reconstructed EQ reconstructed  0 1 2 3 4        Probability = 0,6  Indexed value N  um be  r of  s am  pl es  ?  ?  ? ?  ?  ? Oryginal Modified EA reconstructed EQ reconstructed  Figure 1. Probability distribution reconstruction for nominal attribute savings status (set credit-g) with probability of retaining original value equal to 0.3 and 0.6.

0.0 0.5 1.0 1.5 2.0 2.5 3.0      Probability = 0,3  Indexed value  N um  be r  of s  am pl  es  ?  ?  ?  ?  ? Oryginal Modified EA reconstructed EQ reconstructed  0.0 0.5 1.0 1.5 2.0 2.5 3.0         Probability = 0,6  Indexed value  N um  be r  of s  am pl  es  ?  ?  ?  ?  ? Oryginal Modified EA reconstructed EQ reconstructed  Figure 2. Probability distribution reconstruction for nominal attribute cap-surface (set mushroom) with probability of retaining original value equal to 0.3 and 0.6.

0.2 0.4 0.6 0.8 1.0  0.

0.

0.

0.

0.

Attribute savings_status (set credit?g)  Probability  In fo  rm at  io n  lo ss  ?  ?  ?  ?  ?  ?  ? ?  ? ?  ?  EMAS EQ  0.2 0.4 0.6 0.8 1.0  0.

0.

0.

0.

0.

0.

Attribute cap?surface (set mushroom)  Probability  In fo  rm at  io n  lo ss  ?  ?  ?  ?  ?  ? ?  ? ? ?  ?  EMAS EQ  Figure 3. Information loss caused by incorporating privacy and reconstruction of probability distri- bution for savings status attribute (set credit-g) and cap-surface attribute (set mushroom) and using EM/AS and EQ algorithms.

estimate the number of original samples (nX(vi) = P (X = vi)n).

Then we calculate the difference between nZ(vi) and nX(vi) and call it ?(vi). For ?(vi) > 0 we have too many samples. We look for samples corresponding to positive ?(vi) and assign them the value for which we have nega- tive ?(vj). We update values of proper ?(vi) and continue process until all values of ?(vi) are zero.

Having completed the process, we have samples with the reconstructed values assigned according to original (recon- structed) probability distribution.

In case of reconstruction of probability distribution for each class, we perform this process for every single class separately. Moreover, having reconstructed probability dis- tributions divided into classes, we can choose the best test without assigning the values.

When the probability of retaining the original values is less than 0.5, we look for samples starting from those which belong to the opposite group (e.g. when we have sample which does not meet the test, then we start from samples which meet the test).

Revised algorithms presented in [3] combined with those for continuous2 attributes allow data miner to mine databases containing both nominal and continuous at-  2Continuous attributes are modified with additive perturbation tech- nique [2].

tributes simultaneously.

3 EQ algorithm  This section shows EQ algorithm for reconstruction of probability distribution of nominal attributes, which can be used instead of EM/AS algorithm. Presented algorithm out- performs EM/AS, especially for high privacy levels (see re- sults of experiments in Section 4).

The problem to be solved is the same as for EM/AS algorithm: we have nominal attribute X with the values v1, v2, v3, ..., vk and n samples. Value for each sample is modified according to probability Pr(vp ? vr) (probabil- ity that value vp will be changed to value vr) and we want to reconstruct original probability distribution of attribute X .

Let assume that we have attribute Colour with 3 values: v1 = green , v2 = blue, and v3 = black.

For the original value of attribute e.g. green we know the probability that the value will be the same after mod- ification - Pr(v1 ? v1). We also know the probability of changing the value from green to blue and from green to black. Moreover, when the value of attribute after dis- tortion is e.g. green, the original value was one of the three possible values: green, blue, and black and we know all the probabilities how the value has become green ? Pr(v1 ? v1), Pr(v2 ? v1), Pr(v3 ? v1).

Let attribute Z be the attribute after modification with the values v1, v2, v3, ..., vk. In our example attribute Z has 3 values: green, blue, and black. We can write the equation:  P (Z = green) = a1,1P (X = green) + a1,2P (X = blue) + a1,3P (X = black)  where as,p = Pr(vp ? vs). For colours blue and black we write similar equations:  P (Z = blue) = a2,1P (X = green) + a2,2P (X = blue) + a2,3P (X = black)  P (Z = black) = a3,1P (X = green) + a3,2P (X = blue) + a3,3P (X = black)  Now we have 3 equations and 3 unknown variables (P (X = green), P (X = blue), P (X = black)), thus we can solve the system of linear equations.

In general we have the following system of k equations:  P (Z = v1) = a1,1P (X = v1) + a1,2P (X = v2) + ? ? ? + a1,kP (X = vk)  P (Z = v2) = a2,1P (X = v1) + a2,2P (X = v2) + ? ? ? + a2,kP (X = vk)  ...

P (Z = vk) = ak,1P (X = v1) + ak,2P (X =  v2) + ? ? ? + ak,kP (X = vk) with k unknown variables.

Let define P - matrix of retaining/changing values of nominal attribute.

Definition 3.1 P matrix of retaining/changing values of nominal attribute with order k x k is equal to:  ? ????  a1,1 a1,2 a1,3 ? ? ? a1,k a2,1 a2,2 a2,3 ? ? ? a2,k  ...

...

...

. . .

...

ak,1 ak,2 ak,3 ? ? ? ak,k  ? ????  where ar,p = Pr(vp ? vr) and the sum of the elements in columns is equal to 1.

Let X be the column vector with elements x1, ..., xk, where xi = P (X = vi). Z, in a similar way, is the column vector with elements z1, ..., zk, where zi = P (Z = vi). We can rewrite the system of equations in matrix form as  Z = PX (1)  To find values of P (X = vi), i = 1..k we need to solve the equation 1. We can solve it by left multiplying both sides by inverted P i.e. P?1 (only if inverted P exists).

Nonexistence of inverted matrix is not troublesome, be- cause we know the number of values of nominal attribute  before we start collecting the data and we can choose non- singular matrix P.

The name of the algorithm EQ comes from the phrase system of EQuations.

4 Experiments  This section presents the results of the experiments con- ducted with EQ and EM/AS algorithms. In both cases we use algorithm for assigning reconstructed values to samples for nominal attributes proposed in [3].

4.1 Probability Distribution Reconstruc- tion for Nominal Attributes  Figures 1 and 2 show original, distorted and recon- structed probability distribution of two nominal attributes: savings status (set credit-g) and cap-surface (set mush- room)3. The probability of retaining the original value p is equal to 0.3 and 0.6 (the remaining probabilities of changing the original value are equal, e.g. for p = 0.3 and attribute savings status with 5 values, the probabilities of changing the original value are equal to 0.7/4 = 0.175).

The reconstructed probability distributions for p = 0.6 are close to the original distributions for both algorithms. It means that we do not loose too much information. But for p = 0.3, what means higher privacy, only EQ algorithm can reconstruct the original distribution for both attributes.

The lack of precision in reconstruction is called informa- tion loss. The information loss is defined as follows [1]:  Definition 4.1 Information loss I(fX , f?X) equals half the expected value of L1 norm between the original probability distribution fX and its estimate f?X .

I(fX , f?X) = 12E[ ? ?X  | fX ? f?X |]  Information loss I(fX , f?X) lies between 0 and 1. 0 means the prefect reconstruction and 1 implies that there is no overlap between original distribution and its estimate.

The information loss caused by incorporating privacy for nominal attribute is shown in Figure 3. For probability equal to 1 the information loss is 0, because there was no distortion. In general, the less probability of retaining the original value, the higher information loss i.e. the privacy causes information loss. We have lower information loss for EQ algorithm, what confirms the results in Figures 1 and 2.

4.2 Accuracy and Time of Classification  The accuracy of classification (the percentage of correct classified samples) for Kr-vs-Kp and Led24 sets is shown  3All sets used in tests can be downloaded from UCI Machine Learning Repository (http://www.datalab.uci.edu/).

0.2 0.4 0.6 0.8 1.0  0.

0.

0.

1.

Kr?vp?kp  Probability  A cc  ur ac  y  EQ Local EMAS Local Random  0.2 0.4 0.6 0.8 1.0  0.

0.

0.

1.

Kr?vp?kp  Probability A  cc ur  ac y  EQ By class EMAS By class Random  Figure 4. Accuracy of classification for Kr-vs-Kp set containing only nominal attributes classified with EQ and EM/AS algorithm.

0.2 0.4 0.6 0.8 1.0  0.

0.

0.

0.

0.

0.

0.

0.

Led24  Probability  A cc  ur ac  y  EQ Local EMAS Local Random  0.2 0.4 0.6 0.8 1.0  0.

0.

0.

0.

0.

0.

0.

0.

Led24  Probability  A cc  ur ac  y  EQ By class EMAS By class Random  Figure 5. Accuracy of classification for Led24 set containing only nominal attributes classified with EQ and EM/AS algorithm.

Type of reconstruction Local By class Global Average  Kr-vs-kp EM/AS 0,832 0,250 0,367 0,483 EQ 0,285 0,183 0,365 0,278  Dna EM/AS 4,040 0,891 1,648 2,193 EQ 1,212 0,637 1,682 1,177  Led24 EM/AS 2,042 0,280 0,364 0,895 EQ 0,509 0,188 0,362 0,353  Soybean-large EM/AS 1,534 0,150 0,145 0,609 EQ 0,337 0,116 0,143 0,199  Table 1. Time [s] of classification for EM/AS and EQ algorithms.

in the Figures 4 and 5. Both sets contain only nominal at- tributes.

In the experiments we have several types of reconstruc- tion. Global means that we reconstruct probability distribu- tions only in the root of a tree. In By class we reconstruct separately for each class but only in the root node. For Lo- cal reconstruction is performed in every node divided into classes. Random means without reconstruction.

For Led24 set we observe the inverted bell shape - low accuracy for probability of retaining the original value equal to 0.5 (for Local and By class reconstruction), because this set contains only binary attributes (besides class/target at- tribute).

Kr-vs-Kp set contains not only binary attributes thus the inverted bell shape is not so clear, but we see lower accuracy for probability of retaining the original value equal to 0.5.

For By class reconstruction for Kr-vs-Kp set accuracy of classification are close for both algorithms. For Local re- construction EQ algorithm gives significantly better results for probability of retaining the original value between 0.5 and 0.7.

For Led24 set and both reconstructions EQ gives better results. The inverted bell is thinner.

Table 1 shows time of classification for EM/AS and EQ algorithms for different reconstruction types and data sets.

Classification with algorithm EQ is faster for Local re- construction (about 3-4 times) and for By class (about 1.3- 1.4). For Global reconstruction the results for both algo- rithms are similar. Classification with EQ algorithm is faster or the same compared to EM/AS algorithm.

5 Conclusions and Future Work  We presented the new algorithm EQ for reconstruction of probability distribution of nominal attributes distorted using randomization-based technique and stored in central- ized data.

Effectiveness of the new solution has been tested on synthetic and real databases. The results of these experi- ments show that proposed algorithm achieves better results  - smaller information loss, especially for low probability of retaining original value (i.e. for high privacy).

The results of accuracy of classification confirms that EQ algorithm outperforms EM/AS algorithm for high privacy.

In future works, we plan to investigate the possibility of extension of our results to preserve privacy for target (class) attribute and reconstruct probability distribution of continu- ous attributes.

