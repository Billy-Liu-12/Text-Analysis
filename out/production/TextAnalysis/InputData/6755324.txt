A MapReduce and Information Compression based Social Community Structure  Mining Method

Abstract?As the rapid development of social media, social community structure mining has become a popular research field in recent years. But traditional social community mining methods are not able to effectively deal with the data of large- scale networks. We firstly introduce an information compression based community mining model in this paper, and with the help of the model, we transform the community mining problem into optimal information coding problem. And then propose a parallel computing method ?InfoMR based on the MapReduce parallel framework to mine the social community structure. In the InfoMR, map tasks are responsible for splitting network data into a plenty of subsets, each reduce task is responsible for accomplishing community clustering by means of loop iteration on its subset, and finally all the results from the reduce phase are merged together to output . Theoretical analysis and related experiments verify the validity of the work in this paper. The results of the accuracy experiments show that, the accuracy of the InfoMR is much higher than that of Fast GN and PDST algorithm. The performance experiments on 2 real dataset and 2 simulative dataset show that InfoMR is able to accomplish the task of mining social community in a relatively short period of time on big data social networks. (Abstract)  Keywords- information compression; MapReduce; random walk; social community; social network  (key words)

I.  INTRODUCTION Along with the in-depth study on the physical meaning  and mathematical characteristics features of networks, it?s discovered that many real networks show some common characteristics, such as small-world effect [1], scale-free property [2] and community structure etc [3]. As shown in the example image Fig.1, community structure in a network refers to the occurrence of groups of nodes that are more densely connected internally than other nodes. Nodes and links surrounded by dotted circles represent a community.

All communities and the links connecting the communities constitute the entire network. Revealing community structure in a network is very important for studying network structure and analyzing network features [4] [5]. Especially, the  analysis of community structure in biology, physics, computer graphics and sociology has been widely applied [6].  With the rise of social media, social network community mining Community mining has become a hot spot.

In the past, research on the actual social network structure usually focused on networks containing as much as dozens or hundreds of nodes. However, the size of the current social networks has jumped from tens of thousands to million-nodes [7].

Figure 1.  Sketch of a small network with three groups of nodes.

Therefore, changes of network in size also impels network analysis methods to make appropriate changes.

With the era of big data, the research on social network ushered in new opportunities and challenges. On one hand, users of most of online social networks increased gradually and network size grew rapidly. For example, by the end of June 2013, the number of Internet users in China had reached 591 million, of which social networking site users had grown to 288 million according to "China Internet Development Statistics Report [8]" released by the China Internet Network Information Center (CNNIC). Authors included in the DBLP [9] has reached more than 1.5 million and papers more than 2.0 million, Sina microblog users reached 300 million, Twitter more than 500 million, Facebook more than 900 million. On the other hand, most of the traditional algorithms of mining community in social network [10, 11, 12] have high computational complexity, such as GN algorithm [13]   DOI 10.1109/CSE.2013.143    DOI 10.1109/CSE.2013.143     that has a high computational complexity of O(m2n) ( where m and n is the number of edges and the number of nodes in a network, respectively) and is only suitable for mining in the small network with several hundred of nodes [14]. The analysis of community structure in large networks with more than one million nodes still lacks an effective method [15].

As a result, it has to make a compromise between the calculating time and accuracy of the algorithm [6].

Therefore, the design of novel approaches and new algorithms with big data technology suitable for mining the communities in large scale social networks has important theoretical significance and practical application value.

sWith the help of information compression model we transformed the problem of mining community structure into finding optimal information coding problem, then used MapReduce to achieve community mining parallelization.

The main contributions of this paper are listed as follows.

(1) By means of information compression theoretical  model, the community mining problem is transformed into the optimal data compression coding scheme problem.

(2) With the powerful big data processing tool ? MapReduce Parallel Framework [16], we design and implement an optimal community mining algorithm InfoMR based on information compression coding and random iterative searching on large-scale social network, and present the analysis on the computational complexity.

(3) Related theoretical analysis and experiments proves that the InfoMR proposed in this paper is a community mining algorithm with several advantages such like high accuracy, low computational complexity and high scalability and so on.



II. RELATED WORK In accordance with different types of social networks,  social network community mining is divided into homogenous social networks community mining and heterogeneous social networks community mining.

Community mining methods in traditional homogenous network (like EA homogenous networks, Scale-Free networks et al) include Modularity-based method [17], Normalized Cuts based method [18], structural density based method [19] etc. Ahn et al [20] proposed a ?link community? concept, which is the basic elements determining the communities of the networks are not nodes as traditionally thought by us, but the links or edges connecting nodes. Since each link connects two nodes, link clustering will bring out node clustering naturally. Huang et al [21] proposed a hierarchical clustering algorithm ? SHRINK which didn?t need user-specified input parameters, and the algorithm can not only effectively discover community structure in social networks, but also to discover hub nodes and outliers.

Rosvall et al [22] by means of minimizing the minimum description length of random walk in social networks proposed a social network community mining algorithm.

Although related experiments showed that the algorithm had  high accuracy, the algorithm could not be effectively applied to large scale social network community mining scene because of its serial structure. Tian et al [23] proposed an OLAP-style aggregation methods based on user-specified dimension attributes. In the algorithm, nodes with the same properties of specified style were clustered into the same community, but the topology of the network was completely ignored. Zhou et al [24] proposed a universal description to describe both nodes and links in the network and proposed an SA-cluster concept. For finding all the SA-cluster in the specified social network, Zhou et al proposed a method to automatically learn the degree of contributions of structural similarity and attribute similarity, and evaluated the cluster quality by finding out a good weight balance between the structural similarity and attribute similarity.

Heterogeneous social networks (networks consisting of heterogeneous objects such as users, groups, and blogs, such like Twitter, Digg, and Cora [25]), also known as heterogeneous information networks, has become a hot research area arisen these years. In order to effectively excavate community structure in the heterogeneous social networks, Long et al [26] proposed a general model, Relation Summary Network (RSN), to describe the social network and used Bregman divergence mechanisms to cluster heterogeneous networks. Yin et al [27] taking advantage of objects (nodes) information in heterogeneous social networks proposed a clustering algorithm named CrossClus for the star-shaped heterogeneous social network. Sun et al [28] by means of clustering results of heterogeneous social networks proposed a sorting algorithm RankClus for different types of objects in heterogeneous social networks.

The downside of the algorithm was that it could not find the community number automatically and the performance would be susceptible to the initial community number. Dino et al [29] used the parameter ? in the Goodman-Kruskal theory to describe the community quality of the heterogeneous social networks, and excavated communities in the networks by optimizing ? which was the objective function at the same time. Chen et al [30] proposed NMF algorithm to achieve non-negative relationship matrix factorization of the heterogeneous social networks, and further to dig out communities in the networks.

Unfortunately, similar to [26] [28], the NMF algorithm still required an initial number of communities from the user, and could not adapt to the large scale community mining for its high computational complexity.



III. PROBLEM STATEMENT Def. 1 Social Network Let a triple { , , }G V E W=  present  the social network, where set V   is called the vertex or node set of the social network G  , and set E V V? ?  is the edge set of G  , while set 1,2 ,{ ,..., ,...}i jW w w= or W( )E presents the weight of all the edges, and ijw    presents the weight of the edge ,i je E? .

Def. 2 Big Data Social Network Big data social network refers to the social network in which user-specified tasks can?t be handled at all or can be handled but beyond the acceptable duration of the user.

Similar to people?s current understanding on big data, Def.2 gives a broad definition of big data social network based on Def.1.  Social network community usually refers to a set of nodes closely connected by edges internally, but sparsely connected to external nodes or nodes in other communities. The goal of community mining in big data social network is to identify all the community structures within an acceptable duration.

Imagine personal addresses limited in a city in the real world, how can we distinguish different addresses? A feasible method is using ?residential estate + building number or house number? to specify a particular address.

Correspond to the social networks, if given all the community information, we can also use the ?community ID + node ID? to specify a particular node. Here, we only consider the non-overlapping communities. So, we can use a 2-level description to describe both a node and the community the node belongs to.

In order to effectively dig out the community structure in big data social network, inspired by the literature [22] [31] [32], we use the information compression theoretical model [33] to encode the network into binary code. According to the Shannon?s first theorem (Shannon?s Source Coding Theorem[34]), combining with the 2-level description described previously, the community mining issue in network is transformed into the optimal encoding issue in the information theory field, which will be detailed in Part A of Section IV.

Taking into account the data size and complexity in the big data social networks, we use MapReduce parallel computing framework to reduce the computational overhead and improve efficiency which will be detailed in the Part B of  Section IV.



IV. ALGORITHM FRAMEWORK  A. An information compression based community mining model To illustrate the idea of information compression model,  a simple weighted social network with 10 nodes was given in Fig.3 as the starting point. Fig.3 (b) and Fig.3 (c) show two different binary encoding schemes for the given social network in Figure 3 (a).

(a) Social network of 10 nodes     (b) Encoding network with fixed length coding     (c)  Encoding network with variable length coding  Figure 2.  Two different binary encoding schemes for the given social network  Fig.3(b) shows that, if encoding the given network in Fig.3(a) with fixed length coding, we will need about 10 ??log210? = 40 bits. According to the links between the nodes and the weight of the links in the Fig.3(a), assuming a random walk starting from the node 1 (Corresponding to the node with codeword ?0000? in Fig.3(b)) with 16 steps as following: 1?3?4?1? 2?4?5?6?3?4?6?7?10 ?9?8?9, we will need a total of 16 ??log210? = 64 bits to describe the walk and ?log210? = 4 bits to describe each step in the walk.

In Fig.3(c), in order to highlight the topology of the social network, especially the community structure, we use the 2-level compression coding to describe all the nodes in the network, of which the first level is the coding of the community and the second level is the coding of node in the community. Because the use of the 2-level description, nodes sharing the same codeword in different communities is allowed. Huffman coding algorithm [36] is an entropy encoding algorithm used for lossless data compression, and is an optimal pre?x-free binary code with minimum expected codeword length. If we consider S to be a random variable that takes a value is   with probability  ( )ip s  , then the entropy is:  ( ) ( ) ( )21 1log  J  j j j  H S entropy p s P s=  = = ??  (1)  The average codeword length of the Huffman coding is:  ( ) ( ) ( )  J  j j j  mean L p s L s =  = ??   (2) According to the Shannon source coding theorem [34]  and the Kraft's inequality [37], we know:     Corollary 1  Consider a coding from a length n vector of source symbols 1 2 n( , ,..., s )S s s= , to a binary codeword with length ( )mean L  . Then the average codeword length per source symbol for an optimal prefix- free code satisfy:  ( ) ( ) ( ) 1H S mean L H S? ? +   (3) Expression (3) provides us with the necessary apparatus  to see that in Huffman illustration, the average number of bits needed to describe a single step in the random walk is bounded below by the entropy H(S), where P(S) is the distribution of visit frequencies to the nodes on the network.

From the Fig 3.(c), we know that the random walk of 16 steps similar to Fig 3(b) will need a mount of 40 bits, and that is to say the average information needed to describe each step in the random walk is 2.5 bits.

The comparison observation of the Fig.3(b) and Fig.3(c) shows that if encoding a given social network according to the 2-layers Huffman coding shown in Fig.3(c), then when random walking in the social network, coding scheme with smaller average description length for each step (or the amount of the information needed to describe each step) corresponds to the sharper and clearer social community division. In other word, if dividing the social network into social communities with an appropriate scheme, then pressing ahead with the 2-layer Huffman coding will effectively reduce the average amount of information needed to describe each step in the random walk [31].

For a given social network G, a given community division mode M and the number of communities m, minimum average amount of information L(M) needed to describe each step in a random walking on G is given by (4).

( ) ( ) ( )  m i i  i  L M qH Q p H P =  = +?   (4) According to the random walk theory, random walk  moves around within the same community with a certain probability ?, and also jumps to other communities with a certain probability (1- ?). In (4), q represents the jump probability of the random walk processing between nodes in different communities in social networks G; and H(Q) represents the information entropy of the community coding (i.e., the access probability of the coded first layer ? the social community itself but not the nodes in the community).

ip  represents the probability of random walk accessing the i-th node in the community iC ; H(Pi) expresses the information entropy of the accessing probability of the nodes in the i-th social community - jC when carrying out a random walk; m corresponds to the number of the current social network division (i.e., the current number of social communities). In (4), the first term represents the average amount of information needed to code the social communities in G with Huffman coding (the first layer coding). The second term presents the average amount of information needed to code a node in a social community with Huffman coding (the second layer coding). From (4),  we get that the smaller the L(M), the more accurate the community division is. So, the problem of social network community division in big data social network G is transformed into the problem of how to find the optimal 2- layer Huffman coding *M   scheme. The objective function will be minimizing the average amount of information to describe each step in the random walk:  ( ){ } f min L M=   (5) Assuming that iq  represents the jumping out probability  of a node from iC   to another node in another community  jC  in one step in the random walk process, and pa represents the probability of accessing the node a, then we  get  m i  i  q q =  =?   and i  i i a  a C  p p q ?  = +?   where 1 i m? ? corresponding to the first term and the second term in  (4) respectively.

Assuming that parameter ? indicates the random jumping probability (i.e., the next node followed by the node A in the random walk process is not the neighbor node of A, but a random one and typically ? = 0.15), then we can get that the probability qi of a node in community iC  jumping to another node in another community jC  is:  ( ) ,  ? 1 ? i i i  i i a a ab  a C a C b C  n nq p p w n ? ? ? ?= + ?? ?   (6)  In (6), iq  is calculated in two parts, the first term represents that the node b   who will be accessed next to the current node a  does not belong to the same community as the node a  but a different community; the second term presents that node b  is selected from the neighbor nodes of node a , and the probability of node b  be chosen is proportional to abw  which is the weight of the edge between the node a and node b .

According to the Shannon source coding theorem [34], the information entropy H(Q) of coding the social communities in the social network G and the information entropy of the nodes in the i-th social community ( )iH P can be expressed as:  ( )  1 1  log i im  m mj j i  j j  q qH Q q q=  = =  = ?? ? ?  (7)     ( ) log  log  i i  i i i  i i i  i i b bb C b C  a a i i  a C b bb C b C  q qH P q p q p  p p q p q p  ? ?  ? ? ?  = ? ? + +  + +  ? ?  ? ? ?  (8)     Wherein (7), the first term represents the information entropy of the next node to be accessed does not belong to the same community as the current node in the random walk process; the second term represents the situation of two nodes belonging to the same community.

Suppose 1M  is the social community division mode of social network G at some time, then merging any two communities ? sC  and tC into a new one and expressing the new division mode by 2M . Synthesizing (4), (6), (7), the decrease amount of the objective function caused by merging  sC  and tC  is: ( ) ( )  ( ) ( ) ( ) ( )  ( ) ( ) ( )  1 2  , , 1 1  1 1  , ,  , ,  ( )  log  log  2 log log log  log    m mi s t s t i s t i i  m m s t i i  i i  s t s t s s t t  s t s t a aa Cs Ct a Cs Ct  L M L M L M  q q q q q q  q q q q  q q q q q q  q p q p  = =  = =  ? ? ? ?  ? = ?  ?= + ? ? +?? ?? ? ? ??  ? ? ?  ?+ + +?  ? ?  ? ?  ? ? ( ) ( ) ( ) ( )  log  log  s s a aa Cs a Cs  t t a aa Ct a Ct  q p q p  q p q p  ? ?  ? ?  ? + +  ?? + + ?  ? ? ? ?  (9)  From (5) and (9) we can get that the decrease amount of the objective function resulting from merging communities is only related to the node accessing probability pa and the current division mode M of the social network G. The probability of nodes accessing in the random walk can be calculated before starting the social network division process, and seeking the optimal social network division  *M  with (5) can be achieved through parallel iterative search method.

B. MapReduce-based social community parallel computing model Parallel computing the accessing probability of all the  nodes in the random walk, it would be easily archived by means of the PageRank algorithm [38] and (10):  ( ) { } 1 1  ji  i i n  i  pp n n?  ? ? ?  = ? + ? ?? (10)  Where ji{n }  represents the set of nodes pointing to the node i  , and ip  represents the accessing probability of the node i , and typically ? =0.15 as described above.

M1 M2 M3 Mk  R1 R2 R3 Rr  k mppers read data in parallel   Figure 3.  Schematic diagram of parallel computing for big data social  networks  Fig.4 shows the schematic diagram of calculating nodes accessing probability ap  in the big data social network G by means of the MapReduce framework. The process can be divided into the following four stages: Step 1. The Map phase, accessing probability ip of node i  is mapped to the nodes pointed by the node i , and the intermediate results outputs in <Key, Value> pairs, where  the key is the node id, and the value corresponds to i  i  p n  .

Step 2. The Combine phase, the purpose is to collect the values of the same key into a list structure and merge the <Key, Value> into new <Key, List (Value)>.

Step 3. The Reduce phase, With the input <Key, List(Value)>, calculating the accessing probability of each node according to (10).

Step 4. Iteration process, continue the iterative process from step 1 to step 3 until nodes accessing probability constringency is archived.

More details about the sub algorithm ParP for the calculation of nodes accessing probability are shown in the Fig.5.

From the discussion in Section IV, we know that the decrease amount of the objective function resulting from merging communities is only related to the node accessing probability ap  and the current division mode M of the social network G. In the case of all the node accessing probability being calculated, the core idea of parallel searching social community in big data social networks is shown in Fig.6.

Figure 4.  Parallel computing process of accessing probability ap of nodes  Step 1. As the majority of the big data social network topology data is stored in the adjacent table format on the MapReduce distributed file system, such as HDFS (Hadoop distributed file system), and the links starting from the same node are stored continuously, so we use the similar to the ?order of arrival? data division means to split the network data, and to ensure that all the links from the same node are divided into the same data split. The process corresponds to the stage P1 in Fig.6.

_________________________________________________ Input: Big data social network G; Output: Nodes accessing probability vector [p1,p2?,pn].

1. Initial the probability to be pi=1/n 2. Perform the following iterative process: map and reduce, until the algorithm archive constringency //Map phase 1) Calculate the probability component pi/ni of node i  mapping to the nodes pointed by node i. and get different <nodeID, probability component> combinations.

2) Merge the probability component with the same nodeID, and get <nodeID, List(probability component)> as the intermediate result.

//Reduce phase 1) Collect the <nodeID, List(probability component)> from  the Map phase and merge them to get  the final <nodeID, List(probability component)> .

2) Calculae the accessing probability of all nodes with (10).

_________________________________________________  Figure 5.  Description of the sub algorithm ParP  Step 2. Nodes processing the map task read the k data splits from the phase P1, and shuffling the data according to the reduce function, corresponding to the stage P2.

Step 3. When the shuffle process finished, nodes processing the reduce task will pull the result from all the nodes running the map tasks by means of polling. Then, the nodes processing the reduce tasks will carry out the iterative calculation to get the corresponding social community division and the social network description. The process corresponds to the stage P3 in Fig.6.

Step 4. The converging node is responsible for pulling all the results from the nodes running the reduce tasks and merging them to get the final social community division information and finally outputting the results. The process corresponds to the stage P4 in Fig.6.

_________________________________________________ Input: Big data social network G; Nodes accessing probability vector [p1,p2?,pn] Output: Social community division {C1,C2,?.Cm} //Map phase 1. k  map tasks read data records from the k data splits of G  , and transform the record into <Key, Value> format, and combine the <Key, Value> into r  parts according to the reduce function.

//Reduce phase 2. r  reduce tasks parallel search the data file 'G G? got from the map phase to discover the social communities, and send the social community description and social community division information to the merging node. The search process is as follows:  1) Each node in 'G  be treated as a social community.

2) Randomly select a node in 'G , denoted as node a  . Search all the neighbors of node a  to find the node that can maximize the Equation (9) and then combine them to generate a new community.

3) Randomly select a next node which has not accessed in current round iteration, repeat the process 2).

4) Repeat the process 2) ~ 3), until all nodes traversal is completed, end the current round of iteration.

5) Treat each community obtained in the last round of iteration as a ?super-node?, repeat the process 1) ~4) until convergence.

3. The merging node collect all the social community information from the r  reduce tasks to generate the  final result and output it.

?????????????????????????  Figure 6.  Description of the sub algorithm ParS     Fig.7 shows the description of the sub algorithm ParS corresponding to the description in Fig.6. Some links are discarded during the map phase because of the division.

However, the loss could be negligible in the real world. The reason is as follows: Firstly, the total number of nodes |V| in the majority of big data social networks G is very large, but the number of links |E| between nodes is less (because of the sparse characteristic of the adjacency matrix), and the number of reduce tasks r  in the MapReduce program in the real world is far smaller than the node size, that means that  | |r V<< is often valid. Secondly, the vast majority of social networks tend to use the breadth-first strategy to store the nodes, and the strategy makes the storage of the nodes belonging to the same social community with the locality features. For these two reasons, splitting the big data social network with the ParS sub algorithm is still able to guarantee the quality of the final social network division. On the point, the accuracy of ParS will be verified in the experimental section.

?????????????????????????????  Input: Big data social network G; Output: Social community division {C1,C2,?.Cm} 1. Execute the sub-algorithm ParP to parallel calculate the  nodes accessing probability vector [p1,p2,?,pn].

2. Execute the sub-algorithm ParS to find the social  communities in G.

????????????????????????  Figure 7.  Complete process of the InfoMR.

Finally, Figure 8 shows the complete description of MapReduce-based social community mining algorithm ? InfoMR on big data social networks.



V. TIME COMPLEXITY ANALYSIS From the discussion in section 4, we know the algorithm  can be separate into two stages ? nodes accessing probability vector calculation (Fig.5) and random access iterative clustering (Fig.6), therefore the time complexity analysis can be conducted into two parts.

Supposing that there are k1 map tasks and r1 reduce tasks in the first stage. The InfoMR calculates the accessing probability of all nodes random walking on the social network by means of cyclic iteration. During the process of each round iteration, when executing the map task, each node need to calculate the probability contribution to its neighbors, and the sum of the neighbors of all the nodes is 2m, so the complexity in this step is O(m). Taking into account that the work is done in parallel by k1 map tasks, so the time complexity for each map task will be O (m/k1).

When executing the reduce tasks, all the Values in <Key, List(Value)> should be accumulated to generate the new <Key, Value>. In the case of r1 reduce tasks completing the n task in parallel, the time complexity for each reduce task will be O(n/r1). Generally, we can set the iteration termination condition to be that the number of iteration round archiving constant C, and the map tasks and reduce tasks process  requires incessant iteration until satisfying the condition. So the time complexity in the first stage will be O(m/k1 + n/r1).

In the second stage, InforMR uses random access iterative clustering method to minimize the objective function. During each round in the iteration of the algorithm (shown in Fig.7), for the current accessing node, it always traverse all of its neighbor nodes, and selects the node that minimizing the objective function to merge. Obviously, during any round of iteration, the combination frequency described above depends on the number of links 'm  in the sub network 'G . So the time complexity for each round of iteration in the random accessing iterative clustering is O( 'm ). Assuming the number of the round of the iteration is h and the number of the reduce tasks is r2, then the time complexity during the second stage ? clustering calculation will be ( ' )O m h? . During the experiments for this paper, we found that log 'h n? , 2' /m m r?  and 2' /n n r?  were valid. So the time complexity of the InfoMR during the second stage will be O((m/r2).log(n/r2)).

Above all, we can see that the time complexity of the InfoMR will be O(m/k1+n/r1+(m/r2)log(n/r2)). As found from the results of the experiments, we found that most of the time cost by the InfoMR was spent on the second stage.

Thus the time complexity of the algorithm can be written as O(m/r2)log(n/r2)).



VI. EXPERIMENTAL ANALYSIS The experimental platform is a cluster connected by the  Gigabit Ethernet internally, including 20 PC (Inter i7 quad- core processor, 2.8GHz, memory 4GB) with 64-bit CentOS 6.0 OS and Hadoop-1.1.1 environment.

A. Accuracy of the algorithm The InfoMR algorithm proposed in this paper was  compared with the Fast GN algorithm [39] and the PDST algorithm [40]. Fast GN algorithm takes the ?modularity? as the objective function and uses the heap data structure to carry out low-up greedy search; PDST algorithm uses iterative calculation the get the spanning tree to mine the social community in the social networks. We take the NMI [39] which was usually used for precision contrast to evaluate the calculation accuracy of the algorithms, the equation is shown as below.

,2 log  log log  ij iji j  i j  ji i ji j  N N N  N N NMI  NNN N N N    ? ? ?? ?  ?=   + ? ?? ?  ?  ?  ?  ? ? (11)  In order to test the accuracy of the algorithm, we use the LFR [41] high-quality data generating program which is widely accepted. Parameters are described as follows: m represents the number of links; k represents average node degree; maxk represents the maximum node degree; u represents the ratio of links in all the links starting from a node A that the nodes on the end side of the link starting from A do not belong to the same community as the node of the starting side (in another word, u represents the ratio of     neighbor nodes of node A that do not belong to the same community as the node A), larger u corresponds to vaguer community structure; minc represents the number of nodes in the smallest community; maxc represents the number of nodes in the largest community. We generate 4  data sets in this test, and the configuration is shown in Tab.1.  Because the data set in the accuracy test is small, we set 2 to be the number of reduce tasks during the InfoMR tests.

Table 1. Data sets generated by the LFR with different parameters Dataset M N K Minc maxc maxk  5000 (S) 41125 5000 20 10 40 40 5000 (B) 45722 5000 20 20 80 80 50000 (S) 832980 50000 40 10 80 80 50000 (B) 882453 50000 40 10 160 160  Fig.9 shows the result of experiments on the four data set, the NMI value changes corresponding to different u value. From Fig.9 (a) ~ Fig.9 (d) we can see, accuracy of the Fast GN algorithm decreases rapidly when network community structure becoming dim (u value increases). This is because there exists a ?resolution? restriction in the objective function of the ?modularity? based greedy searching algorithm, and the algorithm tends to discover communities with similar size. The PDST algorithm still maintain a high accuracy when u upto 0.6. When the size of the community is small ( Fig.9(a) and Fig.9(c) ), the calculation accuracy will be less affected by the blurriness of the community, and vice versa such as in Fig.9(b) and Fig.9(d). This is because the PDST uses the spanning tree calculated by means of multiple round iteration to distinguish the social community structure, and the cohesion within the community is expressed by the number of the spanning trees.

So, when the social community size is small, PDST algorithm is conducive to generate more spanning trees that can cover all the internal nodes of the community, which makes it easier and more effective to discover the social community structure. Further, the InfoMR has archived better clustering precision both in the four different networks. This indicates that using the information compression method and encoding the probability model established by random walk is able to highlight the community structure in the network and effectively help to discover the community structure.

u  .2 .3 .4 .5 .6 .7 .8  N M  I  0.0  .2  .4  .6  .8  1.0  Fast GN PDST InfoMR  u .2 .3 .4 .5 .6 .7 .8  N M  I  0.0  .2  .4  .6  .8  1.0  Fast GN PDST InforMR   (a) 5 000 (small)                      (b) 5000 (big)  u .2 .3 .4 .5 .6 .7 .8  N M  I  0.0  .2  .4  .6  .8  1.0  Fast GN PDST InfoMR  u .2 .3 .4 .5 .6 .7 .8  N M  I  0.0  .2  .4  .6  .8  1.0  Fast GN PDST InfoMR   (c) 50 000 (small)                     (d) 50 000 (big)  Figure 8.  Accurcy comparison of different algorithems on 4 data sets  B. Algorithm performance We use 4 different data set - LiveJounrnal, ASKitter, D1  and D2 to test the scalability of InforMR. LiveJournal1 is an online dating blog network of the friends relationship.

ASkitter2 is a network that describes the topology of Internet.

Data set D1 and D2 is a simulative network generated by LFR [41]. Table 2 shows the details of the 4 data sets. We found most of the time consumed by the cyclic iterative clustering section, the scalability testing mainly studies the effect of the number of reducers (or the number of reduce tasks) to the algorithm performance in the second stage of InforMR.

From Fig.10(a)~Fig.10(d) we can conclude: in the case of constant data size, the running time and the number of reduce is linear approximation, which is the running time reduced with the addition of the reduce number. When reduce number scale is small, the running time is affect by the number of reduce significantly. When the number of reduce reach the ?Critical point? (Fig10(a) LiveJournal, reduce number at 140 and fig10(d) D2, reduce number at 70), InfoMR?s running time is less affected by the changes of reduce number and shows ?Long Tail Effect? to some extend.

The reason is: the cost of Mapreduce is basic fixed, When a small number of reduce causes longer running time, Mapreduce?s start time accounting a smaller proportion of the total running time. When reduce number increased and the total running time is decreased, the proportion of the start time is not negligible. It makes InfoMR performs ?Long Tail Effect? at different dataset.

Although the data size has a decisive influence on the computation time (Fig10.(c)D1 and Fig10.(d)D2, at the same reduce number, the running time is different), the severity of different reduce tasks also affect the overall performance of the algorithm. Fig.10(b) and Fig.10(c) shows although ASkitter and D1 has several times difference on Nodes and Edges, its running time shows much less different at the same number of reduce. This phenomenon is due to the characteristics of the data set itself caused the reduce task uneven. Further, we statistical found data set ASkitter certain degree of nodes reach 6,000 or more, but at D1, the degree of   1 http://snap.stanford.edu/data/com-LiveJournal.html 2 http://snap.stanford.edu/data/as-skitter.html     nodes distributed in a relatively narrow range for limited by LFR simulation program. That is because Askitter data distributed unevenly, the largest division of run time run a larger proportion of the total time, even approximately equal the total time. It caused different performance that InfoMR  works at ASkitter and D1. By the simulated data sets and real data sets experiments show that InfoMR has good scalability and can well suited on big data social network community mining scene.

Table2   Data size of different DS Dataset Nodes Edges  LiveJournal 3,997,962 34,681,189 ASkitter 1,696,415 11,095,298  D1 5,000,000 100,950,931 D2 10,000,000 199,805,136    Number of reduce tasks 100 110 120 130 140 150 160 170        Number of reduce tasks 15 20 25 30 35 40 45 50        (a) LiveJournal data set                        (b) ASkitter data set  Number of reduce tasks 20 25 30 35 40 45 50         Number of reduce tasks 30 40 50 60 70 80 90 100     (c) D1 data set                          (d) D2 data set  C. Conclusion Social community mining has become an important  research field in social networks. Because traditional algorithms on social community mining cannot effectively adapt to the current big data social network scenarios, we proposed a MapReduce-based parallel calculation method to mine social community structure in the big data social networks. Theoretical analysis and related experiments verified the validity of the proposed work in this paper, and it will possess some reference meaning for social network analysis and social community mining with the big data techniques.

