The mining of fuzzy gradient in the case of materialized cube

Abstract  The mining of cube gradient is a new research  problem in the data mining community. It is an  extension of the traditional association rules mining in  the data cube and its purpose is to mine the changes of  measures in a multidimensional space. However,  previous works explore for cube gradients with raw  number, and the semantics of the changes of cube  gradients, such as high or low, are not clear and those  are not easy to be understood for the decision makers.

In this paper, the fuzzy gradient mining problem with  fuzzy constraint sets is presented. The mining  algorithm, MFG, for the problem in the case of  materialized cube is presented. In MFG, the condensed  cube technology is used to reduce the searching space  of the mining algorithm. The performance tests based  on the synthesized data sets show the mining algorithm  is effective and scalable with the fuzzy constraint sets.

1. Introduction  The problem of mining changes of measures in a  multidimensional space was first proposed by  Imielinski, et al. as a cubegrade problem [1], which can  be viewed as a generalization of association rules [2] in  data cubes [3]. Typically, cubegrades can express the  following kinds of questions on the data:  Q1: How is the average age of buyers of salsa affected  by buying soda as well? Example answer: Drops by  10% from 26 to 24.

Q2: How is the average amount of milk bought affected  by customer age among buyers of cereals? Example  answer: Raise by 20% for customers younger than 40  and drops by 5% among customers older than 40.

The constrained cube gradient mining [4] represents  a confined but interesting version of the cubegrade  problem. Its purpose is to extract the pairs of gradient-  probe cell characteristics associated with big changes  in measure from a data cube.

The previous works explore for cube gradients with  raw number, for example, in the above question Q2, the  changes of the average amount are 20% and 5%  respectively, whereas the semantics of the changes,  high or low, are not clear and those are not easy to be  understood for the decision makers. Fuzzy technology  provides a useful method for describing the interface  between human conceptual categories and data.

In this paper, the fuzzy gradient mining problem  with the fuzzy constraint sets is presented. The fuzzy  constraint sets and the semantics of changes of cube  gradient are given in the linguistic formats and those  are easy to be understood for the decision makers. Best  of our knowledge, there are fewer papers on the  problem.

In the data warehouse environment, in order to  enhance the response time of OLAP analysis, there are  always lots of materialized cubes that have been  computed in advance. The mining algorithm, MFG  (short for the mining of fuzzy gradient), for the  problem in the case of materialized cube is presented.

In MFG, the condensed cube technology is used to  reduce the searching space of the mining algorithm.

The condensed cube [5] is a novel and efficient data  organization approach. The basic spirit of the  condensed cube is to condense a number of cells of a  cube into a cell, that is, a single tuple. Thus a  condensed cube can reduce dramatically the size of a  data cube itself. The performance tests based on the  synthesized data sets show the mining algorithm MFG  is effective and scalable with the fuzzy constraint sets.

2. Preliminary technologies  2.1 Fuzzy set     The concept of a fuzzy set extends the notion of a  regular crisp set in order to express classes with ill-  defined boundaries, corresponding in particular to  linguistic values such as "tall", "young", "important"  etc. Within this framework, there is a gradual rather  than sharp transition between nonmembership and full  membership. A degree of membership is associated  with every element x of the universal set X. It takes its  value in the interval [0,1] instead of the pair {0,1}.

Such a membership assigning function ?A: X?[0,1] is called a membership function and the set defined by it  is a fuzzy set.

The concept of ??-cut? of a fuzzy set means a subset made of those elements whose membership is  over or equal to ?: (A?={x?X|?A(x)? ? }). A fuzzy predicate expresses the degree to which the arguments  satisfy the predicate.

2.2 An overview of condensed cube  The CUBE BY operator[3] is a multidimensional  extension of relational GROUP BY operator. While the  semantics of the CUBE BY operator is to partition a  relation into groups based on the values of the  attributes specified in the CUBE BY operator and then  apply aggregations functions to each of such groups,  the CUBE BY operator computes GROUP BY  corresponding to all possible combinations of attributes  in the CUBE BY operator. In general, a CUBE BY  operator on n attributes computes 2n GROUP BYs, or  cuboids. The grouping attributes are called dimensions  and the aggregated attributes are called measures. A  tuple with dimension attributes and measure attributes  in a data cube is called a cell. While the aggregation is  applied to groups of relation table tuples obtained by  partitioning the relation table on the cuboid attributes,  there exist such partitions that contain only one tuple  that is named as a single tuple.

Given a single tuple r(r(a1),..,r(an)), where r(ak)  denotes the value of dimension ak (1?k?n) and its single dimensions SD={ai,..,aj} (1?i?j?n) or its SDSET, the complete set of cells condensed by the  single tuple r is denoted as ExpandSet(r) and it can be  computed by the following Expand operator:  (1) Expand(r, SD)=r? such that m(r?)=m(r),  r?(ak)=r(ak) for ak?SD and r?(ak)=* for ak?SD, where m(r?) and m(r) denote the measure of r? and r  respectively.

(2) Expand(r, SDSET)={ r?| r?=Expand(r, SDi),  SDi?SDSET}.

For example, given a single tuple  r=(A=2,B=3,C=1,M=60) and its single dimensions  SD={A}, ExpandSet(r)={(2, *, *, 60), (2, 3, *, 60), (2,  *, 1, 60), (2, 3, 1, 60)}. In general, for a given single  tuple r(r(a1),..,r(an)) and its SD={ai,..,aj} (1?i?j?n), there are the number of 2n-j cells in ExpandSet(r), in  other words, the 2n-j cells are condensed into the single  tuple r. Note that all the cells in ExpandSet(r) have the  same aggregation value m(r) because all of them are  only aggregated from the same single tuple. According  to the Expand principles, we also have two important  properties on a single tuple r:  (1) The non-* dimension values of the cells in  ExpandSet(r) are all derived from the single tuple r and  equal to the corresponding dimensions values of r.

(2) All of cells in ExpandSet(r) share the same non-*  dimension values on SD and the cells that have the  same non-* dimension values on SD are contained in  the set ExpandSet(r).

In a condensed cube, we only need to physically  store the single tuple together with an extra field to  store the single dimensions information of the single  tuple. The cells can be expressed by the single tuple are  not stored physically. When needed, these cells can be  generated through the Expand principles of the single  tuple. The SD fields of these non-single tuples are  equal to ? in a condensed cube. These non-single tuples can be viewed as the general cells in a general  data cube since they don?t condense any cells.

3. Problem descriptions  3.1 The similar relationship of cells  Given two distinct cells c1 and c2 of a data cube D  with n dimensions, c1 is an ancestor of c2 and c2 is a  descendant of c1 iff on every dimension attributes,  either c1 and c2 share the same value, or c1 has value  ?*?, where ?*? indicates ?all?; c1 is a sibling of c2, and  vice versa, iff c1 and c2 have identical values in all  dimensions except one dimension in which neither has  value *. The single tuple has not descendant cells  because each dimension has the non-* value, i.e., the  specific value. For simplicity, we sometimes say c1 is  similar to c2 if c1 is a descendant, an ancestor or a  sibling of c2.

Example 1 Suppose that the data cube D has three  dimensions A, B, C and one measure M and there are  three cells, c1=(A=4, B=*, C=*, M=90), c2=(A=4, B=5,  C=1, M=70) and c3=(A=4, B=5, C=2, M=80). Then the  cell c1 is an ancestor of c2 and c3, the cell c2 is a sibling  of c3.

3.2 Fuzzy constraint sets     A significance constraint Csig is usually defined as  conditions on measure attributes. A probe constraint  Cprb is usually defined as conditions on dimension  attributes and is used to select a set of user-desired  cells. A cell c is significant iff Csig(c)=true, and a cell c  is a probe cell iff c is significant and Cprb(c)=true. The  complete set of probe cells is denoted as P. The  significant cell that may have gradient relationship with  a probe cell is a gradient cell. A gradient constraint (i.e.

gradient relationship) has the form Cgrad(cg, cp) ?g(cg, cp), where g is a gradient function. In this paper, the  gradient function form is defined as ?m(cg)-m(cp)?,  where m(c) is a measure value for a cell c. A gradient  cell cg is interesting with respect to a probe cell cp?P iff cg is significant, cg and cp satisfy similar relationship  and Cgrad(cg, cp)=true. Considering a quantitative  attribute of these constraints, say x, it is possible to  define some fuzzy sets for x, with a membership  function per fuzzy set, such that each value of x  qualifies to be in one or more of these fuzzy set.

Example 2 Suppose that the given fuzzy constraint  sets Cprb?(A is low, B=*, C=*), Csig?(M is expensive), Cgrad(cg, cp)?((m(cg)-m(cp)) is huge), where A, B, C are the dimensions and M is the measure of data cube. The  corresponding membership functions in these  constraints are defined as following:  Then the three cells in example 1, that is, c1, c2 and c3 are significant and probe cells. The cell c1 is interesting with the cell c2. According to the Expand operator, it is known that if a single tuple c is significant, that is Csig(c)=true, the cells in ExpandSet(c) are also significant because they have the same aggregate value with c.

3.3 The fuzzy cube gradient mining definition  Formally, given a data cube D and the fuzzy  constraint sets Csig, Cprb, Cgrad, the fuzzy cube gradient  mining is to find all the interesting gradient-probe cell  pairs (cg, cp). It is worth mentioned that the significance  constraint is assumed to be anti-monotonic in our  problem. Anti-montonicity is very useful for the  pruning of searching cube. Some methods [6] for  deriving weaker anti-monotonic constraints from non-  anti-monotonic constraints are discussed.

4. The algorithms for our problem  Essentially, the mining of cube gradient in the case  of materialized cube is equivalent to the searching of  gradient-probe cell pairs from a given materialized  cube. In this section, the algorithm MFG for our  problem is presented. In MFG, the condensed cube  data is used to store the cells.

4.1 The description of MFG algorithm  The detailed description of MFG algorithm is shown  in figure 1. The inputs of the algorithm are the given  data cube D and the fuzzy constraint sets that include  the given constraint conditions itself and the given  membership functions of the quantitative attributes of  the constraints. The outputs are the interesting gradient-  probe cell pairs. In detail, each cell pair has the  following format:  If cg={x1, x2, ?, xt, mg1, mg2, ?mgq } with S={fg1, fg2, ?, fgq} then cp={y1, y2, ?, yt, mp1, mp2, ?mpq }  with P={fy1, fy2, ?, fyt} and S={fp1, fp2, ?, fpq}  changes mdelt={m1, m2, ?mq} with M={fm1, fm2, ?,  fmq}, where x1/y1, x2/y2,?, and xt/yt denote the  dimension values of cg and cp, t denotes the number of  the dimensions of the cells, mg1/mp1, mg2/mp2,  ?mgq/mpq denote the measure values of cg and cp, q  denotes the number of the measures of the cells, fg1/fp1,  fg2/fg1, ?, fgq/fpq denote the membership function  values of measures of the cells, fy1, fy2, ?, fyt denote  the membership function values of dimensions of the  probe cell, m1, m2, ?mq denote the gradient function  values, and fm1, fm2, ?, fmq denote the membership  function values of gradient function values.

In our study, the probe cells in P are assumed to be  stored in the ascending order according to the  measures. Thus, if the measure of one probe cell cp can  not satisfy the constraint Cgrad(cg,cp), then all the probe  cells following it will not satisfy the gradient  constraints function and hence can be pruned from P.

In the case that the set P is very large, the hash table  can be used to fast access P. Although the gradient  function is assumed as the form ?m(cg)-m(cp)?, our  algorithm is still applicable to the other form gradient  functions by modifying the computation part of the  gradient function.

low(x)=  -0.02x+1 0?x?50  0   x>50  expensive(x)=  0   0<x?25  1   x>50  0.04x-1  25<x?50  huge(x)= 0.1x   0<x?10  1  x>10  0  x?0     Figure 1. The description of MFG algorithm  4.2 The partial expansion technique  In order to produce all the interesting gradient-probe cell pairs, either cp or cg is a single tuple, we should expand the single tuples. While expanding cg or cp, we observe that it is not necessary to compute all the cells condensed into cg or cp. In detail, the cells in ExpandSets that are not meaningful (i.e, similar) are not computed. For example, suppose that cp and cg have four dimensions A, B, C and D, cp=(1, 2, 3, 3), cg=(2, 3, 2, 3). The single dimensions SDs of cp , cg are both {A}. The dimension values of the cells of ExpandSet(cp) and ExpandSet(cg) are shown in the two rectangle frames in figure 2 according to the Expand principles.

Figure 2. The partial expansion of two single tuples  We scan and compare the values of dimension A, B,  C, and D in the two rectangle frames according to the  order of dimension lexicography. The value of  dimension A in ExpandSet(cp) is equal to 1, whereas  the corresponding dimension value in ExpandSet(cg) is  equal to 2. Then the cells in ExpandSet(cg) and  ExpandSet(cp) have different non-* value on dimension  A. Thus, there is only one possibly meaningful  relationship between the cells in ExpandSet(cp) and  ExpandSet(cg), that is, the sibling relationship.

According to the definition of sibling, the cells in  ExpandSet(cp) and ExpandSet(cg) should share the  same values on the remaining dimensions. Thus, the  meaningful dimension values of the cells in  ExpandSet(cp) are as follow: A=1, B=*, C=* and D=3  or D=*, that is, ExpandSet(cp) only contains two cells  (1,*,*,3) and (1,*,*,*). ExpandSet(cp) should contain  23=8 cells if the single tuple cp takes the complete  expansion. Similarly, ExpandSet(cg) should contain  two cells (2,*,*, 3) and (2, *, *, *). The similar  gradient-probe cell pairs are ((1,*,*,3), (2,*,*, 3)) and  ((1,*,*, *), (2,*,*, *)).

The above method is named as partial expansion  technique in this paper. The main spirits of partial  expansion technique is to recognize the possibly  meaningful relationship between the cells in  ExpandSet(cp) and ExpandSet(cg) and then utilize the  relationship to determine the meaningful dimension  values in ExpanSets and hence reduce the size of  ExpandSets. The key of partial expansion is to  recognize the possibly meaningful relationship. The  possibly meaningful relationship can always be  recognized through the different dimension values of  the corresponding dimensions. In general, if the  different dimension values in ExpandSets are non-*  value, then the meaningful relationship is sibling and if  one dimension value is * value and the other is non-*,  then the meaningful relationship is  ancestor/descendant. The description of the procedure  of partial expansion is given in figure 3.

Algorithm 1: MFG  Input: (1) A condensed cube D, (2) Csig, Cprb, Cgrad.

Output: The set of interesting gradient-probe cell  pairs (cg, cp)  Method:  1 P=?; S=?;     //initialization of P and S sets 2 while i<|D| {// |D| denotes the number of cell of D  3 read(c);  4 if Cprb(c)=true {  //c is a probe cell  5 for j=1 to |S|   //compared with the cells of S  6   if(Cgrad(S[j], c))=true //S[j]denotes the j-th cell  7      if S[j] and c are non-single tuple  8         if S[j] is similar to c output (S[j], c);  9      else call Partial_expansion(S[j], c);  10  else break;  11 for k=1 to |P| {     // compared with the cells of P  12   if(Cgrad(c, P[k]))=true  13      if P[k] and c are non-single tuple  14          if P[k] is similar to c output (c, P[k]);  15      else call Partial_expansion(c, P[k]);  16   else break;  17   if(Cgrad(P[k], c))=true  18     if P[k] and c are non-single tuple  19         if P[k] is similar to c output (P[k], c);  20     else call Partial_expansion(P[k], c); }  21     add c to P; }  22 else if(Csig(c)=true) {  23   for m=1 to |P|  24     if(Cgrad(c, P[m]))=true  25        if P[m] and c are non-single tuple  26            if P[m] is similar to c output (P[m], c);  27        else call Partial_expansion(P[m], c);  28     else break;  29   add c to S; }  30 i++; }   //process the next cell  ExpandSet(cp): A=1,B=2 or *,C=3 or *,D= 3 or *  ExpandSet(cg): A=2,B=3 or *,C=2 or *,D= 3 or *     Figure 3. The description of Partital_expansation  algorithm  5. The algorithm performance tests  In this section, we present our experimental results  on the performance (in terms of time) of MFG  algorithm. All experiments are conducted on a PC  platform with an Intel Pentium  500M CPU, 218M  RAM and Windows 2000 OS. All experiments are  performed using synthetic (algorithmically generated)  datasets. The dataset includes 1,024 tuples following  the uniform distribution. The number of dimension is  set to 6 and the cardinality of all attributes is set to  1000. The aggregate function used in the cube  computation algorithm is SUM function.

The materialized condensed cube generated from  the synthetic dataset by the BU-BST algorithm includes  30,557 tuples whereas the generally materialized cube  (i.e. generated by the BUC algorithm) includes 61,114  tuples. So the searching space of the problem is  dramatically reduced. The scalability of the algorithm  with the probe constraint sets, significant constraint  sets and gradient constraint sets are tested and the  results show that the algorithm is effective and scalable  with the different constraint sets. The runtime of the  algorithm are varied from 40s to 600s with different  constraint sets. Due to the limit of space, the results are  not given here.

6. Conclusions  In this paper, the problem of fuzzy gradient mining  in the case of materialized cube and the mining  algorithm MFG are presented. The performance studies  of the algorithm based on the synthetic dataset show  the algorithm is effective and scalable with different  constraint sets. Further enhancing the performance of  MFG algorithm is an interesting work such as the study  of parallel mining algorithm.

Acknowledgements The paper is supported by  National Natural Science Foundation of China  (No.60205007), Natural Science Foundation of  Guangdong Province (No.031558, No.04300462),  Research Foundation of National Science and  Technology Plan Project (No.2004BA721A02),  Research Foundation of Science and Technology Plan  Project in Guangdong Province (No.2003C50118),  Research Foundation of Science and Technology Plan  Project in Guangzhou City (No.2002Z3-E0017) and  Youth Research Foundation of School of Information  Science & Technology at Sun Yat-sen University  (No.350416).

7. References  [1] T.Imielinski, L.Khachiyan, and A.Abdulghani,  ?Cubegrades: Generalization Association Rules?, Tech. Rep.,  Dept Computer Science, Rutgers University, Aug. 2000.

[2] R.Agrawal, T.Imielinski, and A.Swami, ?Mining  Association Rules between Sets of Items in Large Databases?,   Management of Data (SIGMOD?93), ACM Press, 1993,  pp.207~216.

[3] J.Gray, A.Bosworth, A.Layman, and H.Pirahesh, ?Data  cube: A relational aggregation operator generalizing group-  by, cross-tab, and sub-total?, Proceedings of International  Conference on Data Engineering (ICDE?96), IEEE  Computer Society Press, 1996, pp. 152~159.

[4] G.Dong, J.Han, J.Lam, J.Pei and K.Wang, ?Mining  Multi-Dimensional Constrained Gradients in Data Cubes?,   Data Bases (VLDB?01), Morgan Kaufmann, 2001, pp.

321~330.

[5] W.Wang, J.Feng, H.Lu and X.Y.Jeffrey, ?Condensed  Cube: An Effective Approach to Reducing Data Cube Size?,   Engineering (ICDE?02), IEEE Computer Society Press,  2002, pp.155~165.

[6] J.Han, J.Pei, G.Dong and K.Wang, ?Efficient  computation of iceberg cubes with complex measures?,   Management of Data (SIGMOD?01), ACM Press, 2001,  pp.1~12.

Algorithm 2: Partital_expansation  Input: The gradient cell cg, the probe cell cp, Csig,  Cprb, and Cgrad Output: The interesting gradient-probe cell pairs  between cg and cp.

