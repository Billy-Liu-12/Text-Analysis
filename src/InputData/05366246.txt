Influence of Sample Size on Prediction of Animal  Phenotype Value Using Back-Propagation Artificial

Abstract?Although linear multivariate approaches used to analyze large genetic data sets did not allow a large part of the total variance to be explained, strong distortions with nonlinear data sets, horseshoe effects had always been found. Artificial neural networks could gather their knowledge by detecting the patterns and relationships in data and learn through experience, and could perform well for optimization and prediction in complex non-linear systems. Artificial neural networks have been widely used in many life areas, but have not been used to predict the genomic breeding values or animal phenotypes. In this paper, Back-Propagation artificial neural network with Variable Hidden Neurons was used to predict the genomic breeding values. The results showed that artificial neural network could predict the animal genotype value, whatever there were interaction effect or not between gene loci. The sample size for training artificial neural network model could affect the training speed obviously, the training speed were obviously slowed along with enlargement of number of hidden neurons. A good structure of Back-Propagation artificial neural network needs a big sample for training its parameters. In some what, the sample size for training prediction model probably was not an important factor for prediction stability of artificial neural network; but large sample trained neural network model was very useful for training a Back-Propagation artificial neural network model with a small prediction error.

Keywords-Genomic breeding value; molecular marker; artificial neural networks; learning rate

I.  INTRODUCTION Genetic marker?s availability allowed large-scale surveys  of genetic diversity to be carried out in various species. Such projects have provided large data sets that give access to a detailed knowledge of the genetic structure of populations.

Most analyses have made use of methods based on population genetic models. Recently, a new technology called genomic selection is revolutionizing animal breeding. Linear multivariate approaches used to analyze large genetic data sets did not allow a large part of the total variance or inertia to be explained by the first main principal components. However, strong distortions with nonlinear data sets, horseshoe effects had been found due to unimodal response curves in principal components analysis, arch effects, outliers, missing data, etc.

for the .complex relation between traditional animal breeding and genomic architecture[1,2].

An artificial neural network (ANN) is a parallel adaptive system, which can gather knowledge by detecting the patterns  and relationships in data and learn through experience, and can perform well for optimization and prediction in complex non- linear systems. Once the neural network is trained and tested it can be given new input information to predict the outputs.

ANN can not only perform well for optimization in complex non-linear systems, but can also adjust the state of the network by the application of new sets of data. Artificial neural networks have been widely used in the areas of population genetics, evaluation of the contribution of repopulation to biodiversity, genetic analysis of populations and classification of individuals based on genotypic data, protein secondary structure prediction, recognition of signal peptide cleavage sites, gene recognition, design of a genome-wide siRNA library, classification and diagnostic prediction of cancers using gene expression profiling, prediction of specific class I MHC binding peptide sequences[3-6], etc. However, artificial neural networks have not been used to calculate genomic breeding values. In this paper, Back-Propagation (BP) artificial neural network would be used to predict the genomic breeding values, and the influence of sample size on prediction of animal phenotype value would be also discussed.



II. MATHEMATICAL MODEL  A. Quantitative trait genetic model Suppose that a quantitative trait is controlled by  N=n1+n2+?ni+?+nk gene loci in the N gene loci, there are k interaction groups, in the I th groups, there are ni gene loci. If there are two alleles in each locus, there would be a(a+1)/2 kinds of band electropherogram states in the jth gene loci in the I th groups, where a is the number of allele?s bands. If the N gene loci were expressed with mathematical element A, B, ?.  a term in graph theory, then a quantitative trait controlled by N=n1+n2+?ni+?+nk gene loci can be described with a global graph including nk subgraph in figure 1.

Figure 1 The graph of quantitative trait controlled by N gene loci     B. BP Neural Networks model Back propagation (BP) neural networks used in our  research are composed of one input layer for collecting inputs of molecular marker materials, one output layer for collecting the predictions of quantitative traits and one hidden layers for performing a weighted sum of the inputs and passing the resulting value through a non-linear function to the output layer. Individual weights are progressively adapted, using for instance a back-propagation algorithm, to minimize the difference between calculated and expected outputs; the weights assuring the best results then being used to test and compare the performance of the neural networks, which allow the networks to learn nonlinear and linear relationships between input and output vectors, in our research which is the relation between the gene loci and quantitative trait phenotype value.



III. MATERIALS PREPARATION  A. Marker Materials Suppose that a quantitative trait is controlled by N gene  loci. Although it is possible that there are more than two alleles in each locus, for the convenience of simulation, we still think that there are only two alleles in our research. These alleles are marked with M1,m1 ;M2, m2 ;?;Mi, mi ;?;MN, mN units respectively. Then, the molecular marker electropherogram has three states: mm, Mm and MM, which were quantified with -1, 0 and 1 respectively. To an individual, we can sample a series of data from -1, 0, 1 to represent the marker states of individuals.

In our simulation, we assumed that a quantitative trait were controlled by 9 gene loci, which were divided into 3 interaction groups with 2, 3, 4 gene loci respectively. Then the first group has 9 genotypes, the second group has 27 genotypes, the third group has 81 genotypes, altogether there are 39 genotypes. All of the marker genotypes were used in our simulation; part of combinations between different interaction groups was sampled randomly.

B. Phenotype materials For the convenience sake of simulation, the genotype  value of every interaction group (Egroup) were randomly sampled from 0 to 1, as no interactions between different interaction group, the phenotype value P could be calculated by the following formulas:  ? =  = kn   Egroup i  iP               (1)  where Egroupi is the genotype value of the ith interaction group.



IV. SIMULATION AND PREDICTION OF ARTIFICIAL NEURAL NETWORKS  A. Parameters of Artificial Neural Networks Structure In order to build the non-linear continuous functions  expressing the interdependency between the collected marker  data and the phenotype value, a series of artificial neural networks were built, trained, cross-validated and tested. All the networks were built using only one input layer, one hidden layer and one output layer.

The input animal databases including marker materials of samples with 30, 60 and 90 animals were produced as described above, and a sample including 30 animal data were randomly selected to test the precise of prediction. All connection weights were randomized before beginning a training phase?  and the learning rate was 0.05. To avoid overtraining, the first training phase was stopped when the minimum squared error (Mse)<10-6 or the training epochs>103, and the second training phase was stopped when the minimum squared error (Mse)<10-10 or the training epochs>106  B. Simulation and prediction of Artificial Neural Networks The Back-Propagation algorithm is sensitive to the  number of neurons in their hidden layers. Too few neurons can lead to underfitting. Too many neurons can contribute to overfitting, in which all training points are well fit, but the fitting curve takes wild oscillations between these points. One method for improving network generalization is to use a network that is just large enough to provide an adequate fit.

The larger a network you use, the more complex the functions the network can create [8]. If we use a small enough network, it will not have enough power to overfit the data.

Unfortunately, it is difficult to know beforehand how large a network should be for a specific application.

In our manuscript, the number of input neurons was the number of gene locus N, the number of output neurons was the number of quantitative traits, while the number of hidden neurons on estimating the phenotype value was defined as an adjustable number, and the initial number of hidden neurons was 2 times of the number of gene locus N. For discussing the influence of hidden neurons number and sample size, we use a series of networks, whose hidden neuron number is 2-10 times of input neuron number. All the training process was performed by the Matlab 7. 1 software.



V. RESULTS  A. Influence of sample size on calculating time   Figure2. Influence of learning rate and neuron number on calculating time.

The controlled error is 10-6 (figure 1A) and 10-10(figure 2B), and the max number of training epochs are 103(figure 2A) and 106(figure 2B). Solid line is the training speed of sample size 30. Dashed line is the training speed of sample size 30. Solid line is the training speed of sample size 60. Dotted line is the training speed of sample size 90. Circle ?o? is the minimum in a line, the minimum time point are (20, 0.1805), (24, 0.2608) and (33, 0.5065)    in figure 2A, and the minimum time point are (20, 0.1942), (24, 0.2797) and (33, 0.5249) in figure 2B  After trained, simulated and predicted with a series of hidden neurons, we found that the sample size could affect the training speed obviously, the training speed were substantially slowed along with enlargement of number of hidden neurons, but there were not obviously exchange along with the exchange of the number of training epochs and the training error tolerance.

B. Influence of sample size on Euclidean distance   Figure3.  Influence of sample size on Euclidean distance  The solid line is the training speed of sample size 30 (figure 3A and figure 3D). The dashed line is the training speed of sample size 60(figure 3B and figure 3D). Dotted line is the training speed of sample size 90(figure 3C and figure 3D). Circle ?o? is the minimum in a line, the minimum Euclidean distance point are (23, 0.72612) in figure 3A, (22, 1.0983) in figure 3B and (89, 1.0464) in figure 3C  In order to investigate the influence of sample size on the prediction of artificial neural networks, the Euclidean distance was used to weigh the deviation of predicted phenotype value from observed phenotype value of n animals. After the ANN achieving a steady state through training, the average Euclidean distance between observed values and the values acquired from the neural network were shown in figure3.  From the figure3, we found that Euclidean distance had not become smaller, and stability and prediction accuracy were not substantially improved along with the enlargement of sample size n, the minimum Euclidean distance was not improved along with the enlargement of sample size n, and the optimum points were different to different training sample. These suggested that the sample size for training prediction model was not an important factor, perhaps other factors are more important for training a good prediction ANN model, for example, and the uniformity of sample probably be an important factor for training a good ANN model.

C. The influence of sample size on Mse The typical performance function that is used for training  feedforward neural networks is the mean sum of squares of the network errors (Mse). If we calculated it with estimated phenotype and observed value, which can reflect the estimated  stability of Back-Propagation artificial neural network, the smaller Mse reflects a better stability; a bigger Mse reflects a bad stability of Back-Propagation artificial neural network.

Only all of the estimated phenotype is close to observed value, can the Mse be small. In addition to expressing the prediction stability of artificial neural networks, the Mse of different sample was calculated to investigate the ANN?s stability for predicting the animal phenotype.

Figure4.  Influence of sample size on Mse  The solid line is the training speed of sample size 30 (figure 4A and figure 4D). The dashed line is the training speed of sample size 60(figure 4B and figure4D). Dotted line is the training speed of sample size 90(figure 4C and figure 4D). Circle ?o? is the minimum in a line, the minimum Mse point are (23, 0.1054) in figure 4A, (22, 0.2416) in figure 4B and (89, 0.2184) in figure 4C.

The result was shown in figure 4, we found that Mse of different sample size almost has no change along with the enlargement of sample size, but the optimum structure of Back-Propagation artificial neural network  with minimum Mse was different, and the optimum Back-Propagation artificial neural network  trained with big sample size was not batter than the optimum Back-Propagation artificial neural network  trained with small samples, and along with the enlargement of sample size, the number of hidden neurons become bigger. These supposed that a good structure of Back- Propagation artificial neural network need a big sample for training its parameters, in some what, the sample could not affect the ANN?s stability for predicting the animal phenotype.

D. The influence of sample size on maximum error between simulation value and truth  The difference between estimated value and truth reflect the maximum error, if the maximum error is very large, although the stability of Back-Propagation artificial neural network is high, the precise of prediction is not high either.

From the figure 6, we could find that the maximum error of different sample size almost has no difference between sample size 30 and 60, but the minimum maximum error of sample size 90 is very small, and different networks need deferent number of hidden neurons for training the Back-Propagation    artificial neural network. The change trend of maximum error along with the increasing of hidden neurons was the same among different Back-Propagation artificial neural network trained by different sample size. These supposed that large sample trained Back-Propagation artificial neural network could get very good prediction ANN model.

Figure5.  Influence of sample size on maximum error  The solid line is the training speed of sample size 30 (figure 5A and figure 5D). The dashed line is the training speed of sample size 60(figure 5B and figure 5D). Dotted line is the training speed of sample size 90(figure 5C and figure 5D). Circle ?o? is the minimum in a line, the minimum point of maximum error are (44, 0.0610) in figure 5A, (67, 0.0722) in figure 5B and (60, 0.0036) in figure 5C.



VI. DISCUSSION Artificial neural network (ANN) is a parallel adaptive  system and therefore requires training. Back-propagation (BP) algorithm is a powerful method of supervised learning [7,8].

Unlike the methods of regression, could perform well for optimization in complex non-linear systems, and adjust the state of the network by the application of new sets of data.

ANN had successfully been used in biological sequence analysis for purposes as diverse as protein secondary structure prediction, recognition of signal peptide cleavage sites, gene recognition, etc [3-6]. In the past, the idea of using DNA markers to improve the rate of genetic gain in animals had been around for decades, but the adoption of marker assisted selection had been limited until very recently a new technology called genomic selection was proposed. The genomic selection revolution began with 2 developments. The first was the publication by the Bovine Genome Sequencing and Analysis Consortium describing the sequence, Annotation, and comparative analysis of the genome marks a major milestone in animal genetics [2, 9]. The second development was the demonstration that it was possible to make very accurate selection decisions when breeding values were predicted from dense marker data alone, using a method termed genomic selection [2]. However, the genome is very complex, a fascinating linkage between traditional animal breeding and genomic architecture suggests that perhaps the  function relation was not enough for describing the relation between markers and phenotypes, and the genetic diversity should be carefully monitored as genomic selection for quantitative traits as a routine technology for animal genetic improvement [2]. In our research, we found that the non-linear prediction ability of ANN just provided an tool for describing the relation between markers and phenotypes, artificial neural networks could gather knowledge by detecting the relations between molecular marker polymorphism and phenotype value, and could predict the animal genotype value, whatever there were interaction effect or not between gene loci, which was very convenience for animal breeding and production.

From our result, we could also find that the sample size could affect the training speed obviously, the training speed were substantially slowed along with enlargement of number of hidden neurons. A good structure of Back-Propagation artificial neural network need a big sample for training its parameters, in some what, the sample could not affect the ANN?s stability for predicting the animal phenotype. The sample size for training prediction model probably was not an important factor; perhaps other factors were more important for training a good prediction ANN model, for example, the uniformity of sample probably was an important factor for training a good ANN model. However, to the enlargement of sample size, it was not that there was any use for training a good prediction model. Our result showed that the maximum error of different sample size almost had no difference between sample size 30 and 60, but the minimum maximum error of sample size 90 was very small. These supposed that large sample trained Back-Propagation artificial neural network was very useful for training a prediction ANN model.

