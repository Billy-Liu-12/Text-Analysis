Hybrid Search Based Association Rule Mining

Abstract   Association rule mining algorithms provide different search strategies to discover the frequent itemsets; however the problem of large search space is still hard to handle. The present paper suggests a novel hybrid search technique based on reducing the search space, I/O, and CPU times. This can improve the performance up to several orders of magnitude compared to APRIORI algorithm.

Keywords - Data Mining, Association Rules, Reduced Search Space, Hybrid Search   1. Introduction   Association rule mining (ARM) is a two-step process: (1) Finding the frequent itemsets: The search space is  divided into levels where each level contains itemsets of the same length.

(2) Generating their strong association rules [1, 2, 3, 4, 5, 6, 7, and 8].

The main challenging problem for improving the performance of ARM is its huge search space, so its performance can be significantly improved if this search space was reduced before applying ARM algorithms [9]. This reduction will enable ARM algorithms to apply more efficient searching techniques. This paper presents a technique to reduce search space and apply more efficient searching technique. To assess the applicability and usefulness of the proposed technique, an implementation was developed based on Bondon?s implementation for the APRIORI algorithm [10].

Section 2 presents the algorithm?s research  methodology and problem definitions, section 3 evaluates the algorithm performance, and section 4 gives the conclusion of the research.

2. Methodology   This section introduces the current research methodology. It starts with the searching approaches and presents some definitions and lemmas used to create the proposed ?Hybrid? technique. In the last subsection the proposed technique is described.

2.1 Searching approaches   There are two common properties in ARM: (1) If I is a frequent itemset and J ?  I, Then J is also  a frequent itemset.

(2) If I is an infrequent itemset and I ? J, Then J is  also an infrequent itemset.

Based on either of the used properties, the breadth first based searching approaches, used by ARM algorithms to discover the frequent itemsets, are divided into bottom-up, top-down, and hybrid approaches [11].

In the bottom-up approach as shown in Figure 1, ARM algorithms start with the first level. The initial candidate set consists of all the individual items. After the first database scan, the frequent itemsets of length one, are found. Using the frequent itemsets in level one, the candidate set for level two is created based on property (2). The operations of scanning database, discovering frequent itemsets from the candidate set, and generating the new candidate set for the next level are repeated until examining all levels or generating empty candidate set.

In top-down approach as shown in Figure 1, the last level is the first one to be examined. The initial candidate set contains the itemset of length n. After database scanning and candidate set is generated for level n-1 using the infrequent itemset in level n based on the property (1), next, the algorithm is continuously navigating down, and in every iteration, the candidate     set for level K is generated using infrequent itemsets in level K+1.

The Pincer-search algorithm [6] for discovering maximal frequent itemsets is an example of a hybrid approach. In Pincer-search, both approaches bottom-up and top-down are merged producing a bidirectional based algorithm. The result of one direction is used to reduce the number of candidate itemsets in the other direction.

The bottom-up approach provides a relatively poor performance if at least one frequent itemset is long, since it requires a number of database scans equal to the length of the longest frequent itemset before discovering it. On the other hand, the top-down approach has similar drawbacks, as it requires repeated database scanning in case the database has at least one short infrequent itemset. Despite the Pincer-search provides hybrid search technique, it still inherits partially both approaches drawbacks, because it still starts from both the first and last levels.

The main limitation of using an intermediate level in the complete search space as a starting navigation level is the huge number of generated candidate itemsets especially for the databases with large number of individual items.

The proposed reduced search space algorithm overcomes this limitation and allows the use of intermediate level as a starting level, as shown in Figure 2.

Fig.1: Navigation directions   2.2 Basic Definitions and Lemmas  This section of the research represents some basic definitions and lemmas.

Definition 1:?L(I) is the support of itemset I at level L. ?L(I) is the probability of transactions of length L to contain the itemset I.

Definition 2: PI is the support distribution of itemset I over the search space levels, and it is defined as PI = [?1(I) ?2(I).. ?n(I)], where n is the number of items, and equals to the maximum number of levels in the search space.

Definition 3: ?L(I) is the accumulative support of itemset I, at level L. ? L(I) is the probability of transactions of length greater than or equal L, to  contain the itemset I. ?L(I) = ? =  LMax  Lk  _  ?k(I) where Max_L  equal to the maximum transaction size. If the maximum transaction size is unknown, Max_L will equal to the number of the items.

Definition 4: accI is the accumulative distribution of itemset I over the search space levels. accI = [?1(I) ?2(I)??n(I)], where n is the number of items, and equals to the maximum number of levels in the search space.

Definition 5: ACCK is the accumulative distribution of all k-itemsets over the search space levels. K is the order of the distribution.

ACCK =  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  nacc  acc acc  .

.

Where ACCK [i][j] = ?j(i)   Lemma 1: For all itemsets I of length L, ?L(I) = ? (I) Lemma 2: For all itemsets A of length L, where I ?A, ?L(I) ? ? (A)   Definition 6:?` (A) is the estimated value of support of A. ?`(A)= Min  AI ? ( ?L(I)) Where, L is the length of A.

The length of itemsets I, equals to the order of the accumulative distribution that is used in estimated value calculation.

Lemma 3: For all itemsets A of length L, ? (A)? ?` (A).

Definition 7: Itemset I is a frequent itemset at level K, iff ?k(I) ? min_support.

Lemma 4: If itemset I is an infrequent itemset at level K then all itemsets of a length greater than or equal to K and contains I, are infrequent.

Afdf Afdf   Bottom up  Top down     2.3 Illustrative Example  Consider the dataset shown in Figure 2.a, there are  five individual items {a, b, c, d, e}, and 6 transactions; ACC2, accumulative distribution of order two, is the accumulative distribution of 2-itemsets over the search space levels. Figures 2 b, c and d show only three layers of ACC2 for levels 2, 3, and 4. ACC2 is presented as three dimensional array. The first and the second dimensions are items indices which construct 2- itemsets, while the third one is the level index.From Figure 2.c the frequent 2-itemsets at level 3 are{a,d} , {a,e} , {b,e} , and {d,e} assuming the minimum support count is 2 transactions. From these frequent 2- itemsets and using the concept in lemma 4, we can generate candidate itemsets at level 3 which is {a,d,e}.

In contrast, the number of itemsets at level 3 is 10 itemsets as shown in Figure 1, without using ACC2, all of the 10 itemsets can be considered as candidate itemsets. As a result, ACC2 and Lemma 4 allow selecting an intermediate level as a starting navigation level.

Afdf Afdf afdf  TID  Items  1  a, d, e  2  c  3  a, b, d, e  4  a, d, e  5  b, c, e  6  b, e  (a) Example database.

afdf   a b  c  d  e  a  3  1  0  3  3  b  0  3  1  1  3  c  0  0  1  0  1  d  0  0  0  3  3  e  0  0  0  0  5  (b) Accumulative supports at level 2  afdf  a b c d e  a 3  1  0  3  3  b  0  2  1  1  2  c 0  0  1  0  1  d 0  0  0  3  3  e 0  0  0  0  4  (c) Accumulative supports at level 3.

a b c d e  a 1  1  0  1  1  b 0  1  0  1  1  c 0  0  0  0  0  d 0  0  0  1  1  e 0  0  0  0  1  (d) Accumulative supports at level 4.

Fig.2: ACC2 accumulative distribution of order 2   2.4. The Hybrid Algorithm  The following steps summarize the proposed  algorithm:  (1) Generate the accumulative distribution of 2- itemsets count over levels (ACC2).

(2) Using ACC2 to select an intermediate level as start navigation level.

(3)  Use the proposed pre-prune, join, and post-prune methods to candidate itemsets for the start navigation level.

(4) Scan the database to evaluate the candidate itemsets and discover the frequent ones.

(5) Use the discovered frequent itemsets in step 4 and ACC2 as input for pre-prune, join, and post-prune methods to generate candidate itemsets for all levels greater than the start navigation level.

(6) Use the discovered frequent itemsets in step 4 and ACC2 as input for modified post-prune to generate candidate itemsets for levels less than the start navigation level. In modified post-prune, additional constrain is added for removing from the candidate set any itemset that is a subset from any frequent itemset in start navigation level.

(7) Scan the database to evaluate the generated candidate itemsets in steps 5 and 6.

(8) Union the discovered frequent itemsets in step 7 and step 4 to generate all frequent itemsets.

The proposed algorithm uses join and post-prune, methods which are similar to those used in Apriori algorithm [2] while the proposed pre-prune and the modified post-prune methods are shown in Figures 3 and 4 respectively.

Pre-prune is done in two steps; in the first step, the accumulative distribution is examined using the min_support constrain to get frequent 2-itemset at level k (definition 6), in the second step, the (k-1)-itemsets that will be joined to generate k-itemsets are pruned, by comparing the frequent 2-itemsets at level k with those at level k-1 to obtain the 2-itemsets that are frequent at level k-1 and infrequent at level k. These 2-itemsets are denoted as delta itemsets, then all the (k-1)-itemsets that represent supersets of any of the delta itemsets are removed or labeled as not-joinable (lemma 4).

In the modified post-prune as shown in Figure 4, additional constrain is added to prune the candidate set by removing any itemset that is a subset from any frequent itemset in the starting navigation level. Since the subsets of frequent itemsets are also frequent (property 1) and there is no need to reexamine them.

Procedure to pre-prune based on ACC2  Input: itemsets of CK, k-level candidate itemsets and ACC2 Output: new pruned CK  1. delta _itemsets := all 2-itemsets which were frequent in ACC2 level k-1 and became infrequent in level k  2. for all 2-itemsets ID in delta_itemsets 3. for all itemsets IR in Ck 4.   if ID is subset of IR 5.   delete IR from Ck 6. return Ck  Fig. 3: Pre-prune method.

Procedure:modified-post-prune  Input: itemsets of CK, k-level candidate itemsets and frequent itemssets at start navigation level ( LSL ) Output: new pruned CK  1. for all itemsets IC in Ck 2. for all itemsets IF in LSL 3. if IC is subset of IF 4.   delete IC from Ck 5. return Ck  Fig. 4: Modified post-prune method.

3. Performance Evaluation   The performance of the proposed algorithm is compared to the APRIORI performance with different synthetic datasets that varies in the number of items, the average transaction size, and the number of transactions. Performance evaluation experiments were performed on the synthetic databases, generated using IBM synthetic data generator [12]. These synthetic databases use the following parameters: |D| is the number of transaction, |N| is the Number of items, |T| is the average size of transactions, and |I| is the average size of maximal frequent itemsets. For example, N1000.T10.I4.D100K specifies that the number of items is 1000, the average size of transactions is 10, the average size of maximal frequent itemsets is 4, and the database size is 100,000 transactions.

Fig. 5: Change in no. of candidates.

The results of datasets T8I4, T10I4, and T10I6 for N1000L1000D100k are shown in Figures 5 and 6. In these experiments, the proposed algorithm     produced fewer candidates in most cases. It is shown also that, the proposed algorithm has shorter execution time.

Figure 5 presents the reduction of the number of candidate itemsets using the Hybrid algorithm compared to APRIORI, while Figure 6 presents the change in execution time with different minimum supports for both algorithms.

Fig. 6: Change in execution time.

4. Conclusion   The present paper introduces a hybrid search technique which improves the performance of the ARM algorithms up to several orders of magnitude compared to APRIORI in most cases. The reduction in number of database scans, and candidate itemsets greatly reduced the required I/O and CPU times can face the main challenges of the ARM algorithms.

5. References   [1] R. Agrawal, T. Imielinski, and A. Swami, ?Mining  association rules between sets of items in large  databases,? Proceedings of ACM SIGMOD, 1993, pp.

207?216.

[2] R. Agrawal, and R. Srikant, ?Fast algorithms for mining  association rules,? Proceedings of International Conference on Very Large Data Bases, 1994, pp. 487? 499.

[3] J.S. Park, M.S. Chen, and P.S. Yu, ?An Effective hash-  based algorithm for mining association rules,? Proceedings of ACM SIGMOD, 1995, pp. 175?186.

[4] S.A. Ozel, and H.A. Guvenir, ?An algorithm for mining  association rules using perfect hashing and database pruning,? Springer 10th Turkish Symposium on Artificial Intelligence and Neural Networks, Gazimagusa, Berlin, Germany, 2001, pp. 257-264.

[5] A. Savasere, E. Omiecinski, and S. Navathe, ?An  efficient algorithm for mining association rules in large databases? Proceedings of the 21st International Conference on Very Large Data Bases,1995, pp. 432- 444.

[6] Dao-I Lin, and Z. M. Kedem, ?Pincer-Search: an efficient  algorithm for discovering the maximum frequent set,? IEEE Transaction on Knowledge and Data Engineering, vol. 14, no. 3, 2002, pp. 553-566.

[7] M.J. Zaki, ?Scalable algorithm for association mining,?  in: IEEE Transaction on Knowledge and Data Engineering, vol. 12, no. 3, pp. 372-390, May/Jun 2000.

[8] D. Burdick, M. Calimlim, and J. Gehrke, ?Mafia: A  maximal frequent itemset algorithm for transactional database,? IEEE Proceedings of the 17th International Conference on Data Engineering, 2001, pp. 443-452.

[9] A. M. Ghanem, B. Tawfik, and M. I. Owis,?Reduced  Search Space Based Association Rule Mining Algorithm,? 4th Cairo International Biomedical Engineering Conf. , Dec.,2008.

[10] Ferenc Bodon, ?Surprising results of trie-based FIM  algorithms,? IEEE ICDM Workshop on Frequent Itemset Mining Implementations (FIMI'04), vol. 90, Brighton, UK, 1. November 2004.

[11] A. Ceglar and J. F. Roddick, ?Association mining,?  ACM Computing Surveys Journal, vol. 38, no. 2, Article 5, July 2006.

[12] R. Agrwawal, A. Arning, T. Bollinger, M. Mehta, J.

Shafer, and R. Srikant, ?The quest data mining system,? ACM SIGKDD proceedings of 2nd data mining, 1996.

