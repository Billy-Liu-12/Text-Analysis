An Effective Algorithm for Mining Quantitative  Association Rules Based on High Dimension Cluster

Abstract? Mining association rules plays an essential role in data mining tasks. Many algorithms have been proposed for mining Boolean association rules, but they cannot deal with quantitative and categorical data directly. Although we can transform quantitative attributes into intervals and applying Boolean algorithms to the intervals. But this approach is not effective and is difficult to scale up for high-dimensional cases.

An efficient algorithm, DBSMiner (Density Based Sub-space Miner), is proposed by using the notion of "density- connected" to cluster the high density sub-space of quantitative attributes and gravitation between grid / cluster to deal with the low density cells which may be missed by the previous algorithms, DBSMiner not only can solve the problems of previous approaches, but also can scale up well for high-dimensional cases. Evaluations on DBSMiner have been performed using the car and the shuttle databases maintained at the UCI Machine Learning Repository.

The results indicate that DBSMiner is effective and can scale up quite linearly with an increasing number of attributes.

Keywords- Data mining; Quantitative association rules;Density based Algorithms;High Dimension Cluster

I. INTRODUCTION Frequent itemsets[1] play an essential role in many data  mining tasks that try to find interesting patterns from databases, such as association rules[2], correlations[3], classifiers[4], clusters[5] and many more of which the mining of association rules is one of the most popular problems. The problem of Association Rule Mining (ARM) in large transactional databases was introduced in [1]. A typical example of association rule mining is market basket analysis.

This process analyzes customer buying habits by finding associations between the different items that customers place in their "shopping baskets". Inspired by work [1], several fast algorithms based on the level-wise Apriori framework[1]and partitioning [8] are proposed to remedy the performance bottleneck of Apriori. In addition, several novel mining techniques, such as HASH algorithms [9], sample algorithm [6] and other techniques [7, 10], also received much attention lately.

Most of the prevalent approaches assume that the transactions only carry Boolean information and ignore the valuable knowledge inherent in the quantities of the items. To find the association rules, the Boolean approaches assume that all we need to know is whether an item is contained in a transaction or not. Thus, Boolean association rules have the  advantages that they are easy to interpret. However, in a real database, attributes can be quantitative and the corresponding domains can have multiple values or a continuous range of values, for examples, age and salary. By considering this type of attributes, association rule like this one, (30  age  39) and (50000  salary  79999)  (1 cars 3), will be desirable.

To find these rules, we can use ARCS [14] (Association Rule Clustering System) which borrows ideas from image- processing to handle the problem. For more details, see in [14].

The ARCS indicate a way to handle the problem m but it just uses an equip-width or equip-depth binning method. It can?t deal with the high dimension dataset. To settle this hard conundrum, a new density and grid based cluster algorithm was proposed. It partitions the search space into cells, and then using a density based cluster algorithm to cluster the association rules directly in the cells.

The rest of the paper is organized as follows. In Section II, we introduce some definitions of the quantitative association rule mining problems. In Section III, we present our approach and our relative algorithms. In Section IV, we give the experimental results and our analysis. Finally, in Section V, we give our conclusions and the future work.



II. PROBLEM DESCRIPTIONS  A. Boolean association rule Let I= {i1,i2,?,im}, be a set of different items, and D be a  database of transactions, where each transaction T is a set of items such that T ? I. The number of the transactions in D is denoted by |D|, associated with each transaction is a unique identifier, called its TID. We call X ? I a pattern (or an itemset). The count of X is the number of transactions in D that contain X, which is labeled as count(X), and let supp(X) be the percentage of transactions in D that contain X.

Definition 1. Given a transaction database D and a minimum support threshold s (or the minimum support count ), if supp(X) s (or count(X) ), we say that X is a frequent itemset, called FI for short, and we denote the set of all frequent patterns by FS.

Given sets of items T. A transaction database D over T, and minimal support and confidence thresholds s and ? , the association rule mining is to find all the frequent itemsets with support s and confidence? .

Sponsored by Natural Science Foundation of Shaanxi Province ( 2005F13); the Research Fund by Education Department of Shaanxi Province ( 06JK248); Biography GUO Yunkai(1978-),male, graduate student, main research in data mining.E-mail: gyunkai@tom.com  YANG Junrui(1961-), male, born in Xi?an of Shaanxi, professor, main research interests include artificial intelligence, knowledge system, data mining etc.

B. Quantitative Association Rules We now give a formal statement of the problem of mining  Quantitative Association Rules and introduce some terminology.

We use a simple method to treat categorical and quantitative attributes uniformly. For categorical attributes, the values of the attribute are mapped to a set of consecutive integers. For quantitative attributes that are not partitioned into intervals, the values are mapped to consecutive integers .These mappings let us treat a database record as a set of (attribute, integer value) pairs, without loss of generality.

Now, let I = {i1, i2,... ,im} be a set of literals, called attributes. Let P denote the set of positive integers. Let Iv denote the set I?P. A pair (x,v) denotes the attribute as, with the associated value v. Let IR denote the set{<x,l,u>?I?P?P | l u, if x is quantitative; l=u ,if x is categorical }. Thus, a triple {x, l, u}? IR denotes either a quantitative attribute x with a value in the interval [l,u], or a categorical attribute x with a value l . We will refer to this triple as an item. For any X ?IR, let attribute(X) denote the set {x|<x,l,u>?X}.

A quantitative association rule is an implication of the form X Y where X?I,Y?I ,X Y=? , and no item in Y is an ancestor of any item in X. The rule X Y holds in the transaction set Y with confidence c if c% of transactions in D that support X also support Y. The rule X Y has support s in the transaction set D if s% of transactions in D support X Y.

C. Problem Decomposition We solve the problem of discovering quantitative asso-  ciation rules in five steps: 1. Determine the number of partitions for each quantitative  attribute. (See Section 3.) 2. For ordered categorical attributes, map the values of the  attribute to a set of consecutive integers. For quantitative attributes that are not partitioned into intervals, the values are mapped into consecutive integers. In this case, the algorithm only sees values for quantitative attributes. That these values may represent intervals are transparent to the algorithm.

3. Apply a cluster algorithm which is fit for high dimension data to the data produced by the step 2. In this paper, we introduced a new algorithm CBSD (See Section 3) to cluster the data with all attributes. By clustering all attributes together, the relations among all attributes are considered, and the clusters may be more meaningful.

4. Clustering the association rules.

5. Use the frequent itemsets to generate association rules.

The general idea is that if, say, ABC and AB are frequent itemsets, then we can determine if the rule AB => C holds by computing the ratio conf = support(ABC)/support(AB). If conf >s, then the rule holds.



III. HIGH DIMENSION CLUSTER ALGORITHM  A. Relative work and definitions Clustering is one of the primary data mining tools which  help a user understand the natural grouping of attributes in a data set. Large amount of data are being collected in many  business and scientific domains which is being subjected to data analysis for finding interesting and previously unknown patterns. Clustering techniques for large scale data containing a large number of attributes are gaining importance to make the task of knowledge discovery and data mining from various databases much more efficient. The density-grid-based cluster is a new method to avoid the defect of the distance-based algorithm which only finds the "round" cluster. First, divide search space into grids, then for each sub-space, using a density-based algorithm to cluster the sets in it. The classical methods such as CLIQUE [11] and WAVE-CLUSTER [12] are widely used in many fields. This paper is based on the density and grid method on the basis of the physics of gravity inspired a sort of space in the density of the clustering algorithm. Based on the density and grid method, and inspired by the gravity of physics, we put forward a novel clustering algorithm based on sorted density sub-space.

B. Definitions Let A= {A1, A2,?,Ad } be asset if bounded, totally ordered  domain. Let S=A1?A2???Ad be a d-dimensional numerical space, where A1,?,Ad are attributes (dimensions) of S. The input is a set of d-dimensional records. The space S is partitioned into a grid of non-overlapping rectangular units.

The units are obtained by partitioning every dimension into intervals of equal Length in div, which is given as an input parameter. A cell, c, is the intersection of one interval from each dimension, having a form {c1,?,cd}, where ci = [li, ui) is the interval in the partitioning of Ai. A record r= (r1, ?,rd) is contained in the cell c, if  li ?  ri ?  ui, for all ci. A cell c is dense if the fraction of the total data points contained in the cell is greater than a user-defined threshold. We define ? as the threshold, which is traction of the total number of records present in the dataset.

The clusters are unions of connected high density cells.

Two k-dimensional cells are connected if they have a common face or if there is another cell which is connected to both of them.

Definition 2.density datum :dim the minimum density threshold   used to specify the density of the subspace.

Definition 3.dense grid: the grid which density is great than dim.

Definition 4.uncertain grid: the grid?s density is less than dim, but not zero.

Definition 5.center of mass (jth dimension)    K  n  i ij i  j n  i i  N X C  N  =  =  ? =  ,where K is the coefficient,Ni is the number in  ith grid,Xij is the ith grid?s coefficient in jth dimension.

Definition 6.gravitation between grid and cluster:   M N F K  r ?= ,where K is the coefficient is the number of the  clusters is the number in grid, r is the distance between grid and the center of mass .

Definition 7.gravitation between clusters:      1 2N N F K  r ?= ,where K is the coefficient,N1 N2 are the  numbers in these grids,r is the distance between the centers of the two grids .

Definition 8 .minimum force f Given the coefficient K, the minimum gravitation between grids or grids to cluster.

Definition 9.an object p is directly direct-density-reachable from object q if p and q is the high density grid and with respect that they are adjacent or the gravitation.

Definition 10.density-connected: if there exists a link p1,p2,?,pn,p1=q,pn=p, for each pi ? D,(1 i n),pi+1 and pi is direct-density-reachable then p  is density-connected to q .

C. algorithms CBSD (clustering Based on Sorted Dense unit)  The cluster is processed as follows After the space partitioned; we consider that the subspace sub-space with the densest density is the core of cluster. So the algorithm CBSD first sort all the sub-space which density greater than the minimum threshold ? in descending order and add it into  list L. Fetch the first node P of L, labeled its cluster number 1, then search all the nodes which are direct-density-reachable or density-connected to P, add these node to the cluster. If L is not empty, then fetch the first node of L, its cluster number incremented by 1, deal it by the same method until L is empty.

From the above analysis, we can give the final algorithm as below.

Algorithm CBSD (clustering Based on Sorted Dense unit) INPUT (MinDen , f) (1) Using a grid structure to quantize the object space into a finite number of cells, in this section, assume the number as k (2) Delete all the blank cells and such cell which has low density and has no high- density neighbor nearby. If a cell has low density but the attraction force between its high-density neighbors is greater than the minimum force f, then label it as cluster?able.

(3) cluster_no = 0;  // no is the cluster number (4)  CS = ?; //CS are the cluster sets (5)  Divide each attribute into t equal parts, initialize S; (6) Scan the database DB, for each trans (7) if(trans Si) Si.density ++; (8) for all Si (9)    {  if(Si.density >= MinDen) (10)      S = S  Si;    } (11) Sort( S);/*sort in the descending order*/ (12) Insert S into a list L ; (13) While (L ?) (14)  {  cluster_no ++; (15)     C no = ?; (16) get the first node P in L and set P.cluster-no = no; add P into C no; (17) repeat = true; (18)  While (repeat && L ?) (19)   {   repeat = false; (20)  Search all the node N in L which it can directly  density-reachable from P or it is N?s density- connected, then add it into C no;  (21)    if(in 20 found a node that can add to C no ) (22)    repeat = true; } (23)   CS = CS  C no ; //add C no into  the cluster sets ; (24)      } //end while(L ?) (25) End  The process (2) is used to prune the blank cells and the isolated cells which have no value to the cluster. But if a cell is at the edge of a high density and its density is low, for common method, this cell maybe missed, that?s why the grid- based method has a low precision. So in order to avoid such defects, we labeled it cluster-able if it has the short distance to its high-density neighbor.

Figure .1 2?dimension space examples  Example 1. Consider a 2-dimension dataset as shown in Fig.1. The div is 6 and dim is 4. All the no-empty cells are labeled. The dense grids are 2(5), 4(5), 5(4), 6(8), 7(6), 8(5), 10(4), and 11(4). The uncertain grids are 1(3), 3(3), and 9(1).

First we sort the dense grids and inserted into list L, then L is {6(8), 7(6), 2(5), 4(5), 8(5), 5(4), 10(4), and 11(4)}. The first node in L is 6(8), fetch it from L, set its cluster number =1.

Then search all 6?s direct-density-reachable nodes and density-connected nodes. Cell 5 and 2 are clustered into the first cluster. Cell 1(3) is not a dense grid, but from the graph we can see that it is verily near the cluster 1. For common method, it maybe missed just for its low density. To avoid this defect, we take its true distance between the cell and its dense density neighbor, if the gravitation between them is greater than the minimum gravitation f, we can say the cell is part of the cluster. By this method, we may encounter anther problem, take cell 3 for example. The cell 2 and 4 has the same density, which cluster does cell 3 to join? To this problem, we just stored it in a variable, and after all the dense cells clustered, and then using the formula in definition 6 to compute this cell?s gravitation to all the clusters? mass center, the cluster that has the maximal gravitation is the cluster to join.



IV. EXPERIMENTAL RESULTS All our experiments were performed on 2.1G Hz AMD  Athlon 64 X2 Dual Core 4000+ with 896MB of memory, running at Windows XP SP2. The codes of DBSMiner algorithms were complied using Microsoft Visual C++ 6.0.

To evaluate the effectiveness of DBSMiner, we applied it to the Statlog (Shuttle) data set and the Car data set maintained at the UCI Machine Learning Repository [13].

A. The Shuttle Data Set The shuttle dataset has 43,500 transactions and contains 9  attributes all of which are numerical. The first one being time.

The rest are the classes which have been coded as follows: 1 Rad Flow     2 Fpv Close 3 Fpv Open     4 High  5 Bypass 6 Bpv Close       7 Bpv Open For shuttle,  div =16, dim = 2 , and the result is as fallows: (1) [-437, 602] ? [29, 113] ? [-2044, 3743] ? [-188, 436] ? [- 13839, 13148] ? [-26, 75] ? [-353, 270] ? [-356, 266] [1, 5]  support = 99.905747 % (2)[1029, 1149] ? [76, 80] ? [0, 0] ? [20, 34] ? [0, 0] ? [20, 23] ? [46, 55] ? [24, 34] [6, 6] support = 0.004598 % (3)[-1521, -1163] ? [105, 106] ? [-1, 0] ? [36, 36] ? [0, 13] ? [68, 69] ? [69, 69] ? [0, 0] [1, 1] support = 0.00459% (4)[-4184, -3700] ? [106, 106] ? [-1, 40] ? [34, 34] ? [-10, - 2] ? [69, 69] ? [72, 72] ? [4, 4] [7, 7] support = 0.00459 % B. The Car Data Set  The car dataset has 1728 transactions and 6 categorical attributes. The attributes? information is shown as follows: buying: vhigh, high, med, low.

maint: vhigh, high, med, low.

doors: 2, 3, 4, 5more.                    persons: 2, 4, more.

lug_boot: small, med, big.            safety: low, med, high.

The destination class values are: unacc, acc, good, vgood , indicate the customer?s decision of buying a car under a given condition.

For car data set , div = 16, dim = 2 , the results are as fallows: (1) buying =[high] ? maint = [vhigh, low] ? doors = [2, 5more] ? persons=[2, more] ? lug_boot = [small, med] ? safety = [high, low]  class = [unacc, acc] support = 24.24 % (2)buying = [vhigh] ? maint = [vhigh, low] ? doors = [2, 5more] ? persons=[2, more] ?  lug_boot = [small, med] ? safety = [high, low] class = [unacc, acc] support =24.18 % (3) buying = [med, low] ? maint = [vhigh, low] ? doors = [2, 5more] ? persons = [2, more] ? lug_boot = [small, med] ? safety = [high, low] class =  [unacc, vgood] support = 48.43  % (4) buying = [med, low] ? maint= [vhigh] ? doors= [2, 3] ? persons= [2] ? lug_boot=[small, big] ?  safety=[high] class = [unacc] support = 0.23 % The first two rules state that one is unlikely to buy a car if the car price is high and the luggage boot is small or medium.

C. Efficiency To estimate DBSMiner?s efficiency, we set div = 9 and the  number of total transactions is 1000,000. We increase number of records each step by 100,000, Fig.2 shows the result.

Figure.2  Runtime of DBSMiner  In Fig.2, we can see that the speed of DBSMiner  can scale up quite linearly with the increasing number of transactions numbers.

CONCLUSIONS In this paper, we introduce a novel algorithm called  DBSMiner which employs an efficient high dimension cluster algorithm CBSD to deal with the high dimension dataset  and then mining the association rules . DBSMiner has several unique features: it is able to (i) dealing with the high dimension dataset and can deal with low density sub-space which may be missed by previous algorithms and (ii) only need to check the neighbor cell, needn?t scan the whole space.

Experiments showed that DBSMiner is able to discover meaningful and interesting quantitative association rules which many existing data mining algorithms may missed in their mining process.

