EDOS: Employing Mini-DB for High Semantic Object Store

Abstract?Storage management server, compatible with de- coupled data and meta data fashion, is being employed frantically to build large-scale distributed storage system for performance and capacity. To design this hot commodity on flexibly managing the extracted data with little meta data but extended attributes has become a big challenge. This paper breaks a new way to object orient store and implement the dedicated prototype, called EDOS. We reexamine several new requirements and prior works, and employ Mini-DB as the back-end (like DBFS) to guarantee the scalability and durability for EDOS. We design three kinds of object locators and multi-indices to improve retrieval performance and absorb random I/O, utilize a swap mechanism between internal and external objects for tunable throughput, which nested beneath the generic key-value database schema and benefited from memory pool technique. The replication component in Mini-DB helps to build the multi nodes in the distributed environment.

It is easy to build up the object-based distributed file system by EDOS with ACID transaction semantics and high reliability.

The experimental results show that our kernel-level implemen- tation of EDOS performed better than the other existences in practice.

Keywords-database; object store; file system; search;

I. INTRODUCTION The nowadays distributed storage systems trend to sep-  arate the metadata and data operations by offloading the data store service from the storage system to obtain a significant, positive impact on the scalability in the data path. The most immediate effect of this separation is: the data store is just responsible for storage management and high I/O throughput, while do not need to maintain any file- level metadata services. Object storage as a representative design in this fashion has many advantages such as security, data sharing, intelligence, etc. to be the future architecture for distributed storage system. However, there are a small body of work on building a dedicated object-based storage manager.

This paper introduce EDOS, a new paradigm for object-  based storage. With EDOS, the developers can easily build their object-based distributed system in scalable, flexible and reliable fashion. EDOS prefers to use a database-like storage schema rather than age-old hierarchical structure.

Our specific hypothesis that a database-like infrastructure  is more feasible and suitable for the object-based storage system is motivated by several observations: the first one is that an object is a logical structured collection of data,  descriptive attributes, and security policies. A database-like infrastructure provide a well-defined schema that can be used to store the objects as well as reflect the relationships between these properties [1]. Furthermore, each object server involves tremendous number of objects and organizes them by flat labels instead of directory-oriented abstraction [2], [3]. A database-like infrastructure with a powerful data- retrieval capability can quickly locate data, attributes and any other properties that the objects consists of, e.g google bigtable [4]. The third one, since a database-like infrastruc- ture has more knowledge about the data it stores, the storage manager can accurately understand the format of objects, and thus make more effective optimizing policies to improve the performance of overall system. The last observation is that storage manager requires to maintain the integrity and consistency of the objects. A database-like infrastructure can conveniently implement transactional object service to guarantee the reliability.

A key challenge of building a database-like object storage  manager is performance, in many aspects. The first one is derived from the weakness of ?database?: it easily suffers the poor throughput. The second one, since object can sense its internal organization, large size object easily incurs extra overhead to seek the specific data positions. We leverage Hash-table abstraction for structured attribute data and B- tree for object data respectively. In addition, we build multiple indices to feasibly locate the relevant elements in an object, for example, users only need to get the relevant small part of the whole data, while transferring and loading entire object may suffer high cost and risk for such big unit.



II. DISCUSSION & MOTIVATION  A. Object-based characters: Object can be used to store different data structures such as files, database tables, medi- cal images, or multimedia, and their associated attributes [5], [6]. Unlike traditional chunk that has no knowledge about its fixed size data (treated as BLOB [7], [8] data), object is of variable size and can sense its data organization.

Consequently, the object commonly suffers heterogeneous workloads varied from small, random access for partial data sections to extremely large, sequential access for entire object data. Then again, the attributes, which is regarded as the description of corresponding object characters, are   DOI 10.1109/NAS.2010.35     structured data and mainly accessed by storage or data mining applications.

B. Storage managers: Traditional data management so-  lutions include file system and full-fledged database system.

File system provides high performance but have many dis- advantages for achieving better performance and flexibility to build a object-based storage manager: these systems must reorganize large number of flat labelled objects as a hierarchical directory for preventing them in a single directory. These systems must integrate extra service to coordinate the operations of dependent attribute and object data. Moreover, the file-based design scarcely serve both structured attribute and unstructured object data well, as the local storage requirements of them are significant different.

For example, the major access operations on the former are random query insert, while the latter often suffers high I/O streaming access. At last, an object manager is proposed as a intelligent autonomy unit, yet these designs typically treat the attributes as unstructured file data, and thus make it difficult to mine and exploit knowledge for supporting decision-making. On the other hand, database provides a convenient, high-level interface and powerful data-retrieval capability, but they do not measure up to the performance requirements of large-scale storage application.

In a nutshell, the storage requirements of object-based  storage deviate from traditional workloads [9], and thus either file system or database management fails to meet these requirements well. This observation motivates us make our research & development investments on these requirements and propose EDOS, which combines the advantages of these two typical storage abstraction so that it is very compliant with object-based storage system.

The reminder of this paper is organized as follows: In  section 3, we describe the general design and characters of EDOS, mainly expound the provenance of our idea, and in section 4, we describe our measurement methodology and results. The relevant research work is presented in Section 5, We suggest some future work and conclude in Section 6.



III. DESIGN AND IMPLEMENTATION  Fig. 1 shows the overview of the prototype architecture about EDOS. The Mini-DB holds the internal objects and communicate with the local file system by the way of swap [10]. The object is accessed by object locator. To absorb the random I/O attacks the performance and provide a mechanism for efficiently maintaining information about storage that can be re-used after the object data contained thereon is no longer needed is the storage usage issue.

We also leverage the object index to store lists of reusable blocks. Specifically, the object index is used to store both used block entries and reusable block entries.

Figure 1. Architecture of EDOS  A. Object Store Schema  We designed the schema for object store to be simple and relatively general, so that similar implementations could be undertaken on top of similar systems. We took a chunk- based approach, rather than storing each object data as one Binary Large Object (BLOB), The free-space table, freespace.db, manages free extents  of the partition. Initially free space database has one record whose data is just one big extent which means a whole partition. When a change in file size happens this database will update its records.

Extents table, the external object consists of the unique  extent, which size is not fixed. The extents database, ex- tents.db, maps file offset to physical blocks address of the extent including the file data. As this database corresponds to each file, its life time is also same with that of a file. The exact database name is identified with an object ID. This database file is only dynamically removable while all other databases are going on with the file system.

The Object tables, map unique object identifiers to object  data indexed by a balanced tree which may be sorted, and it is possible to influence data layout policies by modifying the identifier assignment and sort function. We assign the identifier by the pair of (PID,UID), PID denotes partition ID, UID is the user object ID which is randomly created. The actual data associated with the object is also stored in the object tables which have different page size respectively. For the given object of variable size, the segmentation job from object to several page of unequal different size is running background. We first sort the tree by the object identifier and then by the page size. This guarantee the pages belong to an object contiguous layout.

The data store of files will be called a view. The reason  not to use search or query, but use filter instead, is because     Database Key Value Index Type Layout Internal Object.db Object Entry Chunk Data B+-Tree Private Partition External Object.db Alias File Descriptor HashTable Decoupled with Internal Table Attribute.db (ObjectID,AttrbuteID) Attribute Value HashTable Cluster with Internal Object Index.db ID Object Locator HashTable In Memory  Free Space.db Free Extent NO. Free Space Start Address B+-Tree Boot Partition Log.db Transaction ID Log Content Queue Private Disk  Table I Database schema of object store.

search or query sound too much ?single shot?, though the terms are almost analogous. It is not that all files should be stored as one large file, but more that closely related files should be treated as one unit.

B. Min-DB in Kernel  Our Mini-DB integrates transactional hash, queue, bal- anced tree to store data following the above hybrid schemas.

The application native data format is proposed to fit static and predictable object access patterns in our workloads, which indicate little relational structure inherently. We im- plement three kinds of object locators to track the Mini-DB in memory, disk or both, and deliver robust data storage features, such as ACID transactions for strong semantic like [11], direct Mini-db based recovery, high concurrency for share-all context, and single-master replication for high availability. These enable our object store to scale massively with lower latency and more reliable.

Rather than storing each object as one binary large object  (BLOB), which will damage the performance issue. Two different types (internal and external) of objects are defined respectively. Internal objects are stored in the Mini-DB with copy semantics and participation in the transactional model, so that all the properties of Mini-DB objects also pertain to internal objects. External objects with reference semantics similar to shared files in a file system are large extra data stored in operating system files outside of the Mini-DB.

In this case, all clients effectively perform operations on a single copy of an external object, as opposed to being supplied their own separate copies as the internal. To client?s perspective, whether a particular object is stored inside or outside the Mini-DB is largely transparent.

For higher performance, we hand-tailor and port Berkeley  DB running in kernel model, called Kernel BDB Subsys- tem(KBDBS), with the advantage of minimizing data copies and context switches. To address the non-trivial problem, we use kernel functions instead of system calls to handle shared memory and ensure coherency firstly. Shared memory had to be carefully reexamined in the context of the kernel address space, the only one address space. Secondly, KBDBS re- quires some form of self-blocking mutual exclusion. Thirdly, we make efforts involved in porting to kernel (test bed platform), which is chosen as our experimental OSD plat-  form. At last, wrapper functions for routines like operation on above schemas and integrity ensuring with concurrent accesses to the Mini-DB by more than one process or kernel thread are isolate with other OS code.

C. Extensible Hash for Attributes  Most attributes tend to follow a write-once read-many paradigm and are relatively fixed in size, structured and are used more or less for the duration of the transaction.

So the workload is characterized as small request and intensive random I/O, and differs from objects. To address the assumptions, we elaborate the extensible hash storage schema to support unconstrained size of attribute value, fast look-up and in-place overwrite, and small space overhead.

However, hashing is intuitively simple as long as you find  a good hash function, while hashing is a trade of speed and space. i.e. Using more space to speed up searching. Dynamic hashing is to solve this space problem: do not allocate huge space for small number of data. Instead the space is allocate when needed. We keep a hash array in your main memory and the array stores the pointers to bucket in disk where your data is stored in those buckets. We define two types of length bench when the bucket is full but the local depth is smaller than global depth. Split bucket and re-distribute old records into two bucket, then re-try inserting. To protect against corruption due to potentially simultaneous access by multiple processes, all accesses to an attribute (setting, retrieving, or enumerating) are wrapped with flock system calls. In our system, we adopt this extensible hash table in mini-db for attribute storage. Other implementation includes liner table and B-Tree to address this problem. Liner table is inefficient and does not scale well to large numbers of attributes specially in retrieval. B-Tree undertakes more overhead than extensible for most attributes of the fixed size.

D. Object Locators  To address the security and performance for object store, the data structure of locator is used to indicate to which area the corresponding object belongs, and it is stored within the cell of table rather than the external, because transaction routines can be used on it. The security rules used to govern access to the locators of the objects just as the real data, but with different storage schema and decoupled data path. In     addition, three types of locators are proposed respectively by durable and enable-trigger characters, and stored as arrays of static length of values, so that they can be sent back and forth between the clients and the server in the platform- independent model. Next, we will introduce the design of the tree kinds of locators in detail.

NEME ODLIO IMLIO LEO length  ? ? ? version  ? ? ? flag  ? ? ? byte wd  ? ? ? object id  ? ? ? object num  ? ? file id  ?  Table II Three kinds of Locators  Tab. II shows the three kinds locators in our system. In locator for internal objects on disk(ODLIO), the length field has a fixed value for internal objects, the version value is incremented when updating the locator, flag field indicates objects? state such as NULL or initialized, and object id as the key to index columns that contain objects. The in- memory locator (IMLIO) generated in dynamic memory for an object contains all of the fields of ODLIO except the additional ?object-number? field, indicating the size of the chunk the object was divided. Lastly, both on-disk and in- memory locators for external objects (LEO) incrementally include ?fileid? field, which stores an unique identifier supplied by the operating system how the file is opened.

In general, the design of the locators help servers not  maintain state information to indicate which locators have been supplied to each client by the dynamic versions. The privilege check is performed and results of the privilege check are buffered for the first access time, since an object that has been accessed once is much more likely to be accessed again. Consequently, the amount of server-side re- sources consumed by locators is reduced, particularly when clients perform operations on a relatively small percentage of the objects. The operation on external objects depends on the fd(file operation handler), which indicates where the external object resides and is kept by KBDBS by the mapping table of alias-to-fd. Once the client has received the external object locators, the server retrieves the fd-alias from the locator and inspects the alias-to-fd mapping to determine the relevant file containing the external object. With the file descriptor, the server can access the external object through the operating system simply.

E. Multi-Indices for objects  A storage object is a logical collection of bytes of variable size and can be used to store entire data structures, such as files, database tables, medical images, or multimedia. Storing  such kind of data as ?BLOB? affects the performance. To address this problem, we employ multi-indices as well- known methods to absorb more random object I/O.

In simple case, by using secondary index include one en-  try in the form (objectid, addr) for each object in the column, the table scan could be avoided by directly traversing the index to locate the entry with objectid, and then following the ?addr? pointer in that index entry to the beginning of the data for object. Unfortunately, a traditional index could find the start address quickly, though the act of scanning from the beginning of the object data to the specified starting position is inefficient and time consuming, even if accesses a little portion of large data. So the combination of the objectid and a pointer number is used to generate the key of the index entry, since the index is allowed to store index entries for any given object across more than one leaf node of the object indices.

Instead of storing the first entry in the index itself, the first  entry contains up to N pointers to indicate the location of the first N chunks of object data, where N is determined by the maximum size of the entries. This scheme will increases efficiency both in the common case that an object is accessed at near the beginning of the object data and other parts of the data which denotes the hot one. Some control information in the first index entry, such as the current version and total size of an object, avoids accessing the object index. Both object data and the first index entry for a object is optionally stored in the table with the locators. To avoid the time and space overhead associated with manipulating object chunks attached with holes within index when object is copied or otherwise manipulated, we only performed on object chunks identified by non-null pointers.

F. Space Allocation Objects that fall below a certain size threshold are stored  within Mini-DB, conversely split into external objects stored outsides. According to incremental or decremental object update, this procedure is dynamical adjustment by objects migration and chunks swapping. The ?chunk size? is speci- fied for an object column and indicates the unit of directly accessible portions (?object chunks?) of the object data. In KBDBS, the unit of locking operation is object chunk, so bigger chunk size harm concurrency because less data is unavailable to other users when the chunk is locked.

The KBDBS employs disk devices to be allocated for  database usage and specifies the individual chunk size for each table respectively. And the disk blocks within each chunk are contiguous, the chunk blocks belong to any given object do not have to be contiguous. The free space table ensure that they are initially contiguous by two type sorted indices, free extents size ASC and chunk number DESC respectively. The indices could guarantee quick time to find the suitable free extent for new allocation and the more higher cluster ratio between usable chunks.

The pre-allocation used to leave a portion of tables pace or index empty and available to store newly added data, and ensuring a proper amount of free space for KBDBS provides the several benefits: Incremental updates are faster when free space is available and properly clustered; Variable- length and altered objects have room to expand, potentially reducing the number of relocated objects; More object data on physically continuous chunks do a favour in read-ahead operations with optimal performance for the majority of queries access data randomly.

G. Consistency  In semantic storage systems, it is important to be able to supply data items as they existed at a particular point in time.

Our system employed the version-based consistency policy to guarantee the client be able to obtain the non-current state object in specified time. To address ?fine-grained? consistency for concurrent access by multiple versions of internal objects which denote the hot data with requirement of transaction scenarios, we supply the particular ?snapshot? rules to ensure only the appropriate visioned object sent to the client. For example, the version field of each in- memory locator indicates the snapshot being used by the client that requested the object locator, it allows the client to operate on a separate copy of the object that was made at the snapshot time. Specially, when performing any modifications on portion of data, the client locks the associated row to prevent other clients from updating to the object before its modification commits, meanwhile only the locator used to perform an modification is updated with the new snapshot identifier.

Object versioning is performed by making a copy of  a chunk of object data before it is modified. subsequent operations that access that object chunk through the current version of the object index will be directed to the most recently modified version of that object chunk. Besides, we specify an attribute ?PCTVERSION? derived from database, which determines the percent of all (maximum quantity) used object data space that can be occupied by old versions of object data chunks. As soon as old versions of object data chunks start to occupy more than the PCTVERSION amount of used object space, the server tries to reclaim and reuse the storage containing old versions of object data chunks. Thus, the server must keep track of which blocks are reusable.



IV. PERFORMANCE OF THE EDOS PROTOTYPE  To compare with some general-purpose file sytems, we want to have the insight on the difference between traditional file system and object store. Besides, Ext3 is used by Lustre for object based storage and has the same disk layout as Ext2 but adds a journal for reliability. XFS, JFS and ReiserFS are modern file systems with high-performance by using B-trees and extent-based allocation techniques.

EDOS also need to be compared with the prior object- based storage techniques such as EOBFS and OSDFS. We define ?EOBFS? as such kind of object store that use B+- tree to index and variable size of extents to allocation like the ?OBFS?[2] which is the storage manager in Ceph[12].

?OSDFS? denotes the concept of simulator for object store, in which the object is a logical collection of several files supported by the native file systems, so that the main job for ?OSDFS? is to maintain the mapping between objects and files. In order to allow for a fair comparison, the two method are implemented in kernel model as EDOS.

Then, for attribute storage we compared our dynamic hash  method to linear table and B-Tree method, the evaluation metrics includes the retrieval, update response time and space utility cost.

A. Experimental Setup A. Test bed: Our EDOS ran on embedded devices reference  to Intel IOP348 I/O Processors platform, with 2GB memory and eight (#:Seagate ST3160815AS Ultra320 SCSI) disks of 160GB in size and configured as RAID0. They ran the Linux operating system of a patched 2.6.18 kernel version.

Network Interface is Intel Pro PCI-X Gigabit Ethernet NIC.

Our Client acted as the common PC machines, the per-  formance parameter as follows: B. Benchmark Workload: The workload presented to the  object store is quite different from that of general-purpose file systems, even the standard and private benchmark for object store is hard to be available. To solve this problem, synthetic workloads composed of create/write, read and delete operations in ratios learnt from the workload model of large scale distributed file systems. These allowed us to examine the performance of the object store on the expected workload.

The distributed file system workloads are inherently dy-  namic, with significant variation in data and metadata access as active applications and data sets change over time as our target workload. So we choose LLNL 1 trace to synthesis workloads as our benchmark workload for the comparable test. The LLNL trace we download indicates that file size scales up from 0B to 2GB, the total number of files reach 900 million and they are 33TB space, in which dominated files distributed between 512KB 16MB, even accurately two thirds of all between 2MB and 8MB. The disk space mainly dominated by files of the size from 2MB to 1GB.

As the statistics table III shows, benchmark I is a read-  intensive workload in which reads account for 70% of all requests and the total size of the read requests is around 22.6GB. The writes, and deletes account for 15.6% and 14.4% of the requests. Contrarily, benchmark II is likely the somewhat write-intensive workload, reads account for 29% of the requests, and writes and deletes account for 36.2% and 34.8%.

1Lawrence Livermore National Laboratory, site: https://www.llnl.gov/             Total-Throughput Read-Throughput Write-Throughput  Th ro  ug hp  ut (M  B /s  )  (a) BenchMark1 Result  EDOS EBOFS OSDFS  XFS JFS  ReiserFS  EXT3 EXT2         Total-Throughput Read-Throughput Write-Throughput  Th ro  ug hp  ut (M  B /s  )  (b) BenchMark2 Result  EDOS EBOFS OSDFS  XFS JFS  ReiserFS  EXT3 EXT2  0.1     1 2 5 10 20 25 30 40 50  E la  ps ed  ti m  e (m  ill is  ec on  d)  # of Attributes  (c) Comparing cumulative attributes reading and writing times  B+-Tree-write B+-Tree-read  Linear-write  Linear-read ex-Hash-write ex-Hash-read            0 1 2 3 4 5 6 7 8 9 10 A  ve ra  ge T  hr ou  gh pu  t ( M  B/ S)  OPT# (10^4)  (d) Long-term performance testing  EDOS EBOFS OSDFS  XFS JFS  ReiserFS  EXT3  Figure 2. EDOS performance using the two benchmarks  Operation Benchmark I Benchmark II (Object#, Bytes) (Object#, Bytes)  Read (71038, 22.6G) (29613, 9.6G) Create/Write (15790, 5.1G) (36826, 11.5G) Delete (14240, 4.6G) (35297, 11G) Sum (101068, 32.3G) (101736, 32.1G )  Table III TRACE DISTRIBUTION  B. Read/Write Test  We take several groups of test to evaluate the average performance of EDOS on the above benchmarks. The main type data type in object store are objects and attributes, so the evaluation focuses on object and attributes separately.

Considering the long-term operation on file system, the per- formance may decline for the fragment, thus we also adopt long-time span and operation intensive trace to compare our phototype with other solutions.

1). For Objects: Figure 2(a) shows that average throughput  of object operations on Benchmark I and figure 2(b) denotes the test on Benchmark II. As we can see, the storage systems adopt the variable size allocation beats a lot with what based on fixed size allocation such as Ext2/3.

It is the nearly the same read performance between  EBOFS, XFS and JFS, because they are employ the similar index and allocation method. EBOFS write perform better than the two because it segment the partition into several independent regions, building multiple B+-trees associated  with these areas. Besides, the adaptive granularity of allo- cation leads a better write performance. The underlying file system backing end for OSDFS is XFS in our testing, so it is performance dependent on the native file system, but the one more catching layer helps to performance a little improvement.

EDOS exhibits the best read performance, almost more  than 10% better than EBOFS, because the multiple indices and the locators to make it more sensitive to its data and no need to maintain the directory-based hierarchy which cause the loss of performance. For example, Ext2/3 use linear table to organize directory structure.

2). For Attributes: The test measured the time required to  write several string-valued attributes in succession followed by the time to read all of the attributes written , in which each string-valued attribute was given a random string varying uniformly in size between 1 and 1024 bytes. Although we anticipate the common scenario to associate more than ten attributes with an object, we repeated this test for attribute counts of up to 2000 attributes per object to stress the scalability of the linear design.

The result was shown in figure 2(c) and tells us the  comparing cumulative attributes reading and writing times for three kinds method to store attributes. Linear table has lower disk space and access time costs compared to the B+Tree indexing table in KBDBS, however the overhead of extensible hash table is the fewest. Because the nature of balance of B-Tree make extra cost when reorganization and an average space inefficiency make B-Tree the worst per-           1 2 4 8 16  Th ro  ug hp  ut (M  B /s  )  Number of I/O threads  Random Read Performance Under Different Backend  Rawdisk EXT2 EXT3  XFS      1 2 4 8 16  Th ro  ug hp  ut (M  B /s  )  Number of I/O threads  Sequential Read Performance Under Different Backend  Rawdisk EXT2 EXT3 XFS     1 2 4 8 16  M ax  im um  L at  en cy  T im  e( m  s)  Number of I/O threads  Maxmum Latency Time Under Different Backend  RAWDISK EXT2 EXT3 XFS         1 2 4 8 16  Th ro  ug hp  ut (M  B /s  )  Number of I/O threads  Random Write Performance Under Different Backend  Rawdisk EXT2 EXT3  XFS       1 2 4 8 16  Th ro  ug hp  ut (M  B /s  )  Number of I/O threads  Sequential Write Performance Under Different Backend  Rawdisk EXT2 EXT3  XFS  0.2  0.4  0.6  0.8   1.2  1.4  1.6  1.8  1 2 4 8 16  A ve  ra ge  L at  en cy  T im  e( m  s)  Number of I/O threads  Average Latency Time Under Different Backend  RAWDISK EXT2 EXT3 XFS  Figure 3. Performance of different storage back-end for EDOS  formance. Our extensible hash operates several times faster than linear table because of its weak retrieval capability.

3). Hybrid Stress Testing: Lastly, we compared these file  system performance with several solution on object store including EDOS on the long term span and verified the durable performance, since durability and slow degradation is the key to build powerful service. Based on the Benchmark II in table III, we enlarged the trace samples to long term emulation. Figure 2(d) shows that the average throughput of each solution degrades by the escaped time. EXT2/3 adopt the allocation with fixed length, which are more sensitive to the space continuity, and degrades more obviously than other does. The Reusable blocks and the swap mechanism make it resilient to space management. EDOS realized allocation with variable size by the attribute (PCTVERSION).

C. Storage Back-end There will have several problems, with respect to whether  using a raw disk directly or native storage abstraction.

To verify this impact on performance , we replace the underlying ?open?, ?read?, ?write? and ?lseek? interface calls to work on a raw partition for raw disk I/O emulation.

We leverage Tiobench2, a portable, threaded file system  benchmark program supporting pthreads, to test the con- currency I/O performance for the various back-end storage media. Before testing, a object-based file system client needs to be created, which kept to the VFS-style architecture and was used to wrap the local file operations by OSD protocol. Each thread running on the client then creates a file to measure read and write operations across the network.

Figure 3 shows the average read and write throughput at sequential or random offsets inside of files and raw block devices, as well as average and maximum access latencies.

2http://sourceforge.net/projects/tiobench  The result in figure 3 shows that EXT2 is the best perform- ing Linux file system for transaction process applications, but it may lead to data corruption because EXT2 lacks ordered data mode. For that reason we recommend using EXT3, as it both performs well and supports ordered data mode. The measurement was performed for XFS as well, it has problems with applications which repeatedly extend files, and that is a common usage pattern in KBDBS.

D. Chunk size In KBDBS, the different tables could be assigned page  size(for objects we call it chunk size). It is the basic access unit and decides the granularity of concurrency, since the bigger chunk size the fewer chunks, in which the more data will be locked. However, space allocator guarantee the continuity inside the chunk, so that big chunk leads to high throughput usually. Some evidences can be found in figure 4.

To access small chunk is possible to create more random I/O, which dose harm to the performance a lot. However, when chunk size up to 2MB, it is not so much enhancement; 16MB is the preference choice for our testing workload, and up to the threshold, the throughput starts to drop down. Intuitively, the performance will be dominated by how long it takes to get the data to or from the disks. Given the request unit, the small disk I/O will be easy to be accomplished with more request circle; the large chunk will mean that most I/Os get serviced by a single disk I/Os. However, We leverage the KBDB?s own strategies to gather I/Os to minimize I/O overhead. In that case we need to know what the database is actually doing to choose the right chunk size.

E. Cost of Operation Replication The prototype uses a replication degree of one and strict  replication, the cost of replication is expressed by the cost of transporting the operation to replicate to the succeeding            0 1 2 4 8 16 32 64 128       A ve  ra ge  T hr  ou gh  pu t (  M B/  S)  MINI-DB Chunk Size ( x 1KB)  0 0.5 1 2 4 6        53.555 5657.556   Figure 4. Chunk size in MINI-DB impact the performance  Replication Average response time Comparison Disabled 10.7 ms 100% Enabled 21.2 ms 198%  Table IV Replication of EDOS  node and the time it takes the succeeding node to execute the operation and reply.In addition, replication comes with the cost of using extra storage. In our prototype, there are two copies of each record; the primary and the replica. The actual cost of replication compared to the time it takes to execute on the primary node can be greatly affected by two factors: 1)The work load on the succeeding node. 2)The network latency/bandwidth according to the size of operation data.

If the network latency between the primary and replica  node is on the order of hundreds of milliseconds, or even seconds, the cost of replication is very high. An application deployed on the Internet would have to relax the response time bounds. The network latency between the nodes in our test system is in the range 0.1 0.3 milliseconds shared with each host of the same performance. Test results for two equal test runs are show in Table IV below, where replication was disabled in the first run and enabled in the second. 100000 put-operations were executed for each run. The size of the successor list was set to two.

As can be seen, we experienced nearly a twofold increase  in response times when replication was enabled. This seems reasonable, as the request must be carried out at two nodes instead of one. The extra usage of CPU resources related to the replication should be low, as the prototype simply sends the request to the succeeding node, executes the operation locally and then fetches the result from the succeeding node.



V. RELATED WORK Database File Systems: Before Gnome Storage, WinFS,  and now Apple Spotlight, IBM?s AS/400 is based on an object-oriented database filesystem which is implemented at  the firmware level rather than at the OS-level. The PICK OS was an even earlier example of a ?database filesystem?, it has an SQL-like language to query databases and create reports.

KBDBFS and I3FS of FSL lab employed in-kernel database to support extra characters wrapped standard file system operations, the useful extensions include extended attributes, ACLs, a new operation to retrieve file, integrity checker and intrusion detection file system [13], [14], [15], [16]. DBFS is a block-structured embeddable file system developed on top of the Berkeley DB (BDB) with an interface largely consistent with the normal POSIX file system interface by FUSE.The Mac has used a database to store files and their attendant meta-data since HFS was introduced. It uses B- trees idea of having a resource fork to all files, a little name badge for every file that tells type information. Contrary to HFS, BeOS [17] allows for creation of your own objects and in the file system. Close, but replace ?filename? with ?attributes?. Besides, some file systems are built on the top of very famous open source database, such as PqsqlFS from inverted file system, mysqlFS, and tabFS uses SQLlite as its storage end.

Object-based File System: With the advent of OSD  (Object Storage Device) [18], IBM released OSDFS [19], it is an extension to Ext2 that works with an OSD. The Panfs integrates an object-based clustered architecture to orchestrate file activity across the Storage Cluster and manage system performance. Solaris OSD Project consists of the Solaris SCSA-compliant device drivers and related software to provide both initiator and target support for storage devices that adhere to the OSD protocol. Intel?s Open Storage Toolkit contains reference implementations of OSD, and comes up with whole suite containing OSDFS, OSD initiator, OSD Target simulator, which DISC-OSD suite was built based on. PDL has a OSD implementation of its own called the ursa minor. They are integrating it with their pnfs implementation. Ceph [12], is a distributed file system for petabyte scale files, in which intelligent OSDs manage data replication, failure detection, and data migration during failure recovery or system expansion.



VI. CONCLUSION AND FUTURE WORK With the rethinking of object-based characters and storage  managers? technique clues, the novel search based archi- tecture for object store (EDOS) has been analyzed and proposed, which employs a stable Mini-DB core supporting transaction and replication. The EDOS makes it easy to ensure the reliability and extensibility for object store, and innovative features can be rapidly prototyped, and stronger storage semantics can be explored. Our experiments verified our hypothesis on the issue.

The competition for iSCSI is Object-based storage. This  competition will become more intense in the near future with the advent of the high performance interconnection and transport technique, such as Remote Direct Memory     Access(RDMA) [20]. We will target the new condition to rethink the storage management again.

