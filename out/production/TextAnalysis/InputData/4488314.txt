Usage-Based Positive and Negative Verification of User Interface Structure

Abstract   The new approach to structure verification of the web  hyperlinks is presented in the paper. It utilizes several  types of patterns extracted from the web usage data: posi-  tive and negative association rules, positive sequential  patterns as well as the new kind of patterns ? sequential  patterns with negative conclusions. All they enable to  appraise the usefulness of hyperlinks in both the positive  and negative way. Based on this knowledge, the content  managers can adequately promote the trustworthy hyper-  links and remove the negatively verified ones.

1. Introduction   The structure of user interface in the web-based infor-  mation systems is expressed by the set of hyperlinks used  by users to move from one page to another. Hyperlinks are  mostly created by content managers according to their  knowledge and experience. On the other hand, users navi-  gate though the site and leave fingerprints of their activi-  ties in the form of records in logs. We can make use of  this usage data to verify usefulness of hyperlinks. It means  that some patterns extracted from data about user behavior  can be utilized to either acknowledge or deny the need of  the existence of hyperlinks. This knowledge can be used  by content managers to eliminate some ineffective hyper-  links or move the useful ones to the more prominent place  in the web page.

2. Related Work   Web site navigational structure can be assessed upon  several data sources: web usage, web content, knowledge  of the web manager as well as the project fixed require-  ments and its general aims. This assessment may include  many different factors. Almost every part of a web portal  design may be assessed. There are many approaches to  improve site structure usability; the basic one is link vali-  dation. Currently, every web design application has the  ability to validate the hyperlink destination but the prob-  lem of missing links still occurs. Its innovative mobile  agent solution, which can be used even with very limited  access to the Internet connection, was proposed in [7].

The content-based navigational connections are derived  from the similarity between textual content of web pages.

The content in such approach is usually described with  descriptors i.e. terms, which are expected to be the most  informative and distinctive. One of the best known meas-  ures for descriptor selection is term frequency - inverse  document frequency measure (tf-idf) [13, 21]. Terms,  which occur relatively frequent in one document (tf), but  rarely in the rest of the set (idf), are more likely to be  relevant to the topic of the document. Terms that appear  on many pages are not useful in distinguishing between a  relevant page and irrelevant ones. The inverted document  frequency idf reduces the influence of these terms. More-  over, terms with the too low or too high idf, as the bad  content descriptors, can be excluded from further process-  ing [13]. Besides, the importance of terms that occur in  some specific parts of the HTML content like title, de-  scription and keywords can be increased [12]. A content-  based system links and recommends pages similar to the  just viewed one. It is kind of item-to-item correlation [18].

Baraglia and Silvestri utilized their own measure of  web page usability. It is based on the analysis of web logs  and is independent from the content of pages. The strength  of the correlation between two pages is symmetric and it  is, in its idea, similar to confidence function in association  rules, see sec. 4.2. The main difference is that the authors  used in the denominator the greater values from two: the  number of user sessions containing the first web page and  the number of sessions with the second page [5].

Proposals of new hyperlinks are the other aspect of  navigation structure improvement. One of such methods  exploits case based reasoning CBR as a possibility for the  automatic generation of hyperlinks for hypertexts as ex-  tension of traditional textual methods [10].

The other method of hyperlink assessment is querying  visitors using forms [17]. However, it is very difficult to  evaluate the replies as users tend to present subjective  opinions. Moreover, a site designer is not able to compare  the results to a model site as one does not exist.

The next method is a statistical log analysis. It may de-  liver information about most common path, average ses-   DOI 10.1109/ICAS.2008.15       sion length, pages where visitors leave the site, etc. An  example of improvements based on this approach has  been presented in [24].

There are also some mathematical methods to assess  the navigation structure of an information system, e.g. the  technique that utilizes the complexity of graph represent-  ing the site [27]. The most reliable graph measure appears  to be a number of independent paths in the graph.

Srikant and Yang proposed algorithms to discover  wrong locations of web pages in the hierarchical structure  of the site based on the backtrack analysis of navigational  paths extracted from web logs [23].

In many cases an automated assessment is the best way  to discover incorrect site structure. Based on the archive  of navigational patterns some automatic simplifications of  path existing in the system can be recommended [20, 23].

Typical association rules and their indirect version were  used for creation of recommendation ranking list and as  the minor importance research for the assessment of hy-  perlinks. Conducted experiments revealed that about a  half of all hyperlinks can be confirmed by typical associa-  tion rules while almost 90% by indirect ones [14].

Spiliopoulou and Pohle have defined the success of a  site as the efficiency of its component pages in attracting  the users to exploit the supported services and buy the  offered goods, especially in e-commerce sites. For this  purpose, they proposed three basic measures: the contact  efficiency, relative contact efficiency and conversion  efficiency of a page. All of them are evaluated with statis-  tical analysis of data about page requests and both cus-  tomer and non-customer user sessions extracted from web  logs. Finally, they recommended pages that needed to be  improved [22].

Cowderoy analyzed the web site complexity from the  developer perspective and compared it to the typical met-  rics useful for software development projects [9].

Effectiveness of the web site can be also studied from  the organizational point of view as the measure for quality  of services provided especially by the state dependent  agencies [26].

The idea of both negative and positive verification of  hyperlinks based on association rules derived from web  logs has been proposed for the first time in [15] and then  examined in [16]. Positive and negative patterns including  simplified, 2-page sequential patterns extracted from us-  age data have been utilized for filtering of content-based  recommendation list in [11].

3. Usage Data   Every http server is able to record and retain all incom-  ing http requests in its log files. A typical request consists  of information about the user (IP address, user agent, user  identifier, and user password), the requested resource  (URL) and the request itself (date and time, version of the  protocol, http method, code, and size of the returned re-  source) as well as the navigation (referer field). At anony-  mous access user identifier and user password are left  empty. Here we have the content of the example log: 1. 156.17.3.118 - - [14/Jan/2007:00:00:49 +0100] "GET /ebip/rss.php  HTTP/1.0" 200 4228 "-" "MagpieRSS/0.72  (+http://magpierss.sf.net)"  2. 90.156.41.179 - - [14/Jan/2007:00:01:02 +0100] "GET /doktoranci/ HTTP/1.1" 200 3983 "http://www.pwr.wroc.pl/" "Mozilla/5.0 (Win-  dows; U; Windows NT 5.1; en-US; rv:1.8.0.9) Gecko/20061206  Firefox/1.5.0.9"  3. 90.156.41.179 - - [14/Jan/2007:00:01:04 +0100] "GET /doktoranci/zapisy/ HTTP/1.1" 302 5 "http://www.pwr-  old.wroc.pl/doktoranci/" "Mozilla/5.0 (Windows; U; Windows NT  5.1; en-US; rv:1.8.0.9) Gecko/20061206 Firefox/1.5.0.9"     Figure 1. Positive and negative verification of  user interface structure based on usage patterns  Most web sites in the Internet provide anonymous ac-  cess to their pages. For that reason, sessions and paths, i.e.

usage data, have to be extracted from the cleansed set of  HTTP requests stored in the log files using some rules [8].

First, requests from web crawlers (robots), usually sent by  search engines, have to be removed based either on textual  analysis of the user agent field or on the entire set analysis  [25]. Next, only meaningful resources are left, i.e. those  from the white list of resource types: html, pdf, doc, etc.

Definition 1. Let P={p1, p2, ..., pN} be a finite set of  web pages in a single web site. The set si, called a session,  is a tuple si=<si + ,si  - >, where si  +????P is the set of distinct       pages visited during the ith users? visit in the web site and  si - is the compliment of si  + in P, i.e. si  - =P\si  + . Let S be a  multiset of all available user sessions. Only sessions with  card(si + )>1 are taken into consideration.

Definition 2. The path hi=<pi1, pi2, ?, iim  p > is the se-  quence of consecutive pages requested (visited) during the  ith users? visit i.e. user session si; where mi is the number  of requests in session si. Only paths with mi>1 are taken  into consideration. Pages are ordered by increasing re-  quest time, i.e. page pik+1 has been requested just after  page pik, for all natural k<mi. Let H be the multiset of all  available paths.

In other words, a session is the partition of set P, i.e.

si +?si  - =P and si  +?si - =?. Since si  + is the set of all distinct  pages visited during the ith session and each page from si +  may occur in the path hi many times then mi?card(si + ).

Note that repetitions are allowed within both S and H,  i.e. there may exist many sessions with the same content  pages and many paths that would contain the same pages  in the same order.

The simple but quite effective session and path extrac-  tion consists in matching IP address and user agent fields  from the cleansed log data set, i.e. all request that came  from the same IP address and user agent with the idle time  between two following requests of less than 30 minutes  (tk+1?tk<30min) [8, 6] are treated as one user session.

4. Verification of the Web-based User Inter-  face Structure   4.1. The General Concept of Verification   The usage data ? log files are processed to obtain ses-  sions (unordered set of visited pages) and navigational  paths, Fig. 1. Next, the positive and negative association  rules are extracted from sessions. Simultaneously, paths  are used to discover positive sequential patterns and the  new kind of negative patterns ? sequential patterns with  negative conclusions. Since all the patterns operate on sets  of items, they need to be aggregated to provide single  values for pairs of pages. These aggregated values are  used to verify hyperlinks extracted from the content of the  web site in the either positive or negative way. Positive  verification combines positive association rules and se-  quential patterns whereas negative association rules and  sequential patterns with negative conclusions are used to  discover useless hyperlinks that can later be removed by  content managers.

4.2. Positive and Negative Association Rules Ex-  tracted from Usage Data   A positive association rule is an implication of the form  X?Y, where X?P, Y?P, X?Y=0. A rule X?Y means that  if the page set X occurs in some user sessions then the  page set Y co-occurs in these sessions. In other words,  there are N + >0 user sessions si=<si  + ,si  - > in S for which  X?Y?si + .

Each rule has two associated measures that denote its  significance and strength, called support and confidence  respectively. The support sup a (X?Y) of the positive rule  X?Y in set S specifies the popularity of the rule among all  sessions, i.e. in the entire set S. It is described with the  following formula:  sup a (X?Y)=  { }( ) )(  :,  Scard  sYXSssscard iiii +?+ ???=  . (1)  The confidence conf a  (X?Y) of a positive rule X?Y in  set S is:  conf a (X?Y) =  { }( ) { }( )+?+  +?+  ??=  ???=  iiii  iiii  sXSssscard  sYXSssscard  :,  :, . (2)  Another type of associations are negative rules [1, 3,  28, 29]. A confined negative association rule is a negative  implication of the form X?~Y, where X?P, Y?P, X?Y=0.

A confined negative association indicates the negative  relationship between X and Y in user sessions, i.e. there  are N ? >0 user sessions si=<si  + ,si  - > in S that contain set X  but do not contain set Y, i.e. X?si + and Y?si  - . The support  sup a (X?~Y) of a confined negative rule X?~Y in set S is:  sup a (X?~Y)=  { }( ) )(  :,  Scard  sYsXSssscard iiiii ?+?+ ????=  .

(3)  The confidence conf a  (X?~Y) of a confined negative  rule X?~Y in set S is:  conf a (X?~Y)=  { }( ) { }( )+?+  ?+?+  ??=  ????=  iiii  iiiii  sXSssscard  sYsXSssscard  :,  :, .

(4)  As typically in association rules mining, only positive  and negative rules with support and confidence that ex-  ceed two minimum thresholds minsup a and minconf  a ,  respectively, are considered.

There are several algorithms that extract both positive  and negative association rules [1, 3, 28, 29] and some  others designed for only positive rules extraction like  apriori, Eclat, FP-growth.

4.3. Aggregation of Association Rules   In general, rules of the form X?Y or X?~Y operate on  sets of elements, i.e. both X and Y can consist of many  web pages. Moreover, there may be many rules X?Y for a  pair of pages pi and pj that pi?Xi and pj?Y. To utilize       association rules to assess pairs of user interface connec-  tions (hyperlinks) we would need a single measure for a  pair of pages pi and pj. Thus, based on the confidence as  usability measure, an integration mechanism has been  introduced. It joins into one value average confidence  conf ass-avg  (pi?pj) all positive association rules X?Y con-  taining pi on their left side (pi?X) and pj on the right side  (pj?Y), for every pair pi and pj, as follows:  conf ass-avg  (pi?pj)=  ( )  { }( )YpXpYXcard  YXconf  ji  YpYXpX  a  ji  ???  ?? ?? ,:  :,: . (5)  Similarly, separately for every pair pi and pj, we con-  sider all negative association rules X?~Y for which pi?X,  pj?Y. Consequently, we obtain one average confidence value conf  ass-avg (pi?~pj):  conf ass-avg  (pi?~pj)=  ( )  { }( )YpXpYXcard  YXconf  ji  YpYXpX ji  ???  ?? ?? ,:~  ~ :,:  .

(6)  Note that for each pair pi, pj, only one of three mutually  exclusive cases is possible:  1. There exists at least one positive association rule X?Y, pi?X, pj?Y which support and confidence ex- ceed minsup  a and minconf  a , respectively, and as a re-  sult conf ass-avg  (pi?pj) ? minconf a and the relation  from pi to pj is positive.

2. There exists at least one negative association rule X?~Y, pi?X, pj?Y which support and confidence ex- ceed minsup  a and minconf  a , respectively, and as a re-  sult conf ass-avg  (pi?~pj) ? minconf a and the relation  from pi to pj is negative.

3. There is neither positive X?Y nor negative associa- tion rule X?~Y such that pi?X, pj?Y or these rules do not exceed minimum thresholds. Consequently,  conf ass-avg  (pi?pj) = conf ass-avg  (pi?~pj) = 0, and we  cannot say anything about relation from pi to pj.

4.4. Positive Sequential Patterns based on Usage  Data   Sequential patterns are sequences of items (pages) fre-  quently visited one after another by users within their  navigational paths (see definition 2). To define frequent  sequences we need to specify the concept of sequences  inclusion.

Definition 3. A sequence hi=<pi1,pi2,?, iim  p > contains  another sequence qj=<pj1,pj2,?,pjn>, if there exist n inte-  gers k1<k2<?<kn, called index K of qj in hi, such that  1ik p =pj1,  2ik p =pj2, ?,  nik p =pjn. Sequence qj is also called  the subsequence of hi. Item nik  p  is called the end or the  last item of qj in hi with respect to index K whereas its  position in hi, that means kn, is called the end position and  denoted by k end  .

Each sequence hi may contain up to ( i m  2 ?mi?1) differ-  ent sequences qj that consist of at least two items-pages.

Note that shorter sequences (1-item) are completely use-  less in the structure verification.

Let us consider an example. A navigational path  h=<p4,p6,p1,p6> means that first the user visited page p4,  next page p6, p1, and they finished on page p4. This se-  quence contains the following 2-, 3- and 4-item subse-  quences: <p4,p6>, <p4,p1>, <p4,p6,p1>, <p4,p1,p6>,  <p4,p6,p6>, <p4,p6,p1,p6>, <p6,p1>, <p6,p6>, <p6,p1,p6>,  <p1,p6>. Their number ? 10 is less than the maximum (11)  due to repetition of p6. Note that subsequence <p4,p6>  have two separate indexes K1 and K2 for the given se-  quence h: K1=(1,2) and K2=(1,4); the end positions are  k1 end  =2 and k2 end  =4, respectively. The index denotes posi-  tions of the subsequence?s items within the given se-  quence.

Definition 4. Support sup q+  (q) of sequence q in H (see  definition 2) is the number of all source sequences (paths)  from H that contain subsequence q. Support can be ex-  pressed either as an integer or the percentage of the H?s  quantity:  sup q+  (q)= { }( )  )(  ofesubsequencais:  Hcard  hqHhcard ? . (7)  If sequence q is contained frequently enough in the source  sequences from H, i.e. sup q+  (q)?minsupq+ then such se- quence q is called a positive sequential pattern in H.

There are several algorithms to mine positive sequen-  tial patterns like AprioriSome, AprioriAll [2], PrefixSpan  [19] or SPAM [4]. Any of them can be used to extract  positive sequential patterns.

4.5. Sequential Patterns with the Negative Con-  clusions   Having positive sequential patterns specified, we can  define an auxiliary compliment and finally a new pattern  in data mining: sequential patterns with the negative con-  clusions.

Definition 5. A complement C(q,h,K) of the subse-  quence q in sequence h with respect to index K is the set  of all items of h that follows the last item of subsequence  q in h. The most numerous complement of subsequence q  in h is called the maximum complement of q in h and  denoted with C max  (q,h).

For the example navigational path h=<p4,p6,p1,p6,p1,p6>  and subsequence q=<p4,p6>, we have three end positions  k1 end  =2, k2 end  =4, and k3 end  =6 for three corresponding in-  dexes K1, K2, and K3, respectively. Hence, complement  C1(q,h,K1)=C2(q,h,K2)={p1,p6} and C3(q,h,K3)=?. The complement is not a multiset so it does not allow repeti-       tions. The first two complements are the greatest so they  are simultaneously the maximum complement  C1(q,h,K1)=C2(q,h,K2)=C max  (q,h). In general, the maxi-  mum complement corresponds to index K with the small-  est value of end position k1 end  =2.

Note that for the given sequence h and its subsequence  q, their maximum compliment contains all their comple-  ments: C1?C max  , C2?C max  , and C3?C max  . Based on this  feature we can easily prove that if page p does not belong  to maximum complement C max  (q,h), then page p also does  not belong to its subsets, i.e. to any of the complements  Ci(q,h,Ki).

Now, we can try to discover new patterns for each posi-  tive sequential pattern, namely sequential patterns with  negative conclusion using all its maximum compliments.

Definition 6. A sequential pattern s - (q?~X) with the  negative conclusion in path multiset H is the implication  q?~X. It denotes that if sequence q occurs in the source  paths hi then set X does not intersect any of the comple-  ments in these paths hi, i.e. there some paths hi?H: q is the  subsequence of hi and X?C max  (q,hi)=?.

In other words, if the user visits sequence q of web  pages they usually do not visit any of the pages from set

X. It simultaneously means that set X occurs in maximum  complement of sequence q in paths h from H very rarely.

Term rarely denotes here that such case is valid for many  source paths.

Similarly to association rules, each sequential pattern  s - (q?~X) with the negative conclusion possesses two  measures: support sup s- (q?~X) and confidence  conf s-  (q?~X).

sup s- (q?~X)=  ( ){ }( ) )(  , ofsubseq.ais: max  Hcard  XhqChqHhcard ?=???  (8)    conf s-  (q?~X)=  ( ){ }( ) { }( )hqHhcard  XhqChqHhcard  ofesubsequencais:  , ofsubseq.ais: max  ?  ?=??? (9)  Note that conf s-  (q?~X) = sup s-  (q?~X) / sup q+  (q).

Only patterns s - (q?~X) that exceed minimum thresh-  olds are really considered, i.e. sup s- (q?~X)?minsups- and  conf s-  (q?~X)?minconf s-.

A sequential pattern sup  s- (q?~X) with the negative  conclusion, which has 1-page left side q, is simply equiva-  lent to a negative association rule q?~X, see sec. 4.2.

Sequential patterns with negative conclusion can be  discovered from the previously obtained set of positive  sequential patterns. First, the frequent set of maximum  complement is extracted from source paths that contain  each such positive sequential pattern. Pages from domain  P (see definition 1) that do not belong to any complement  of these paths automatically become members of the nega-  tive pattern conclusion X. All other pages that frequent  maximum complement are treated as candidate members  for conclusion set X. These candidates and their combina-  tions Xi that exceed minsup s-  and minconf s-  thresholds  form sequential patterns s - (q?~Xi) with negative conclu-  sions. The process is repeated for each positive sequential  pattern q.

4.6. Aggregation of Sequential Patterns   Similarly to association rules, eq. (5) and (6), we can  calculate average support sup s+avg  (pi,pj) of positive se-  quential patterns, for each pair of web pages pi and pj.

Separately, average confidence conf s-avg  (pi?~pj) is evalu-  ated for sequential patterns with the negative conclusions.

4.7. Verification Functions   Aggregated confidences conf ass-avg  (pi?pj) of positive  association rules and aggregated support of positive se-  quential patterns sup s+avg  (pi,pj) are utilized to evaluate  positive verification function verif +  (pi?pj) that corre-  sponds to the connection from page pi to pj:  verif +  (pi?pj)=?* conf ass-avg  (pi?pj)+?*sup s+avg  (pi,pj) (10)  where ? and ? are constants that help to balance influence of association rules and sequential patterns.

Similarly, the negative verification function  verif - (pi?pj) is calculated based on average negative  confidences of association rules conf ass-avg  (pi?~pj) and  sequential patterns with negative conclusions conf s-  avg (pi?~pj), separately for each connection from pi to pj:  verif - (pi?pj) = ?*conf  ass-avg (pi?~pj) +  ?*conf s-avg(pi?~pj).

(11)  where ? and ? are adjustment constants.

Based on the values of verif  + (pi?pj) and verif  - (pi?pj)  we are able to verify usefulness of links between pages.

The high value of verif +  (pi?pj) positively supports the  existence of the hyperlink from pi to pj. Moreover, due to  association rule contribution, the verification can even  suggest new connections between pages.

On the other hand, a significant value of verif - (pi?pj)  can be an important sign for removal of the hyperlink  from pi to pj. Since all components of verification func-  tions are calculated from the usage data, i.e. http requests  (navigational sessions and paths) then the entire verifica-  tion process is based on historical user behaviors.

8. Conclusions and Future Work   Log files of the http servers contain data that can be  used to discover patterns corresponding to behavior of  web site users. Positive and negative association rules  extracted from log data reflect typical usage patterns but  they do not respect the navigational order whereas sequen-       tial patterns strongly depend on the sequence of naviga-  tion. Association rules and sequential patterns comple-  ment one another; by making use of the aggregated ver-  sions of them we obtain the comprehensive and com-  pressed view onto how users utilize the structure of the  web site, i.e. connections between pages.

The new pattern type ? sequential patterns with nega-  tive conclusions provide new knowledge that enables to  verify the existing hyperlinks in the negative way. As a  result, the content manager of the web site can remove  useless hyperlinks and simplify the structure of the user  interface.

Future work will focus on the new algorithms that  would help to mine both association rules and sequential  patterns in the effective way.

