

One Double-Reduct Approach to Get Key Rules and the Experiment in Prison Computer Information Security    Lv Hanfei Department of Information management  ZheJiang Vocational Police Academy Hangzhou, China lhf_b@sina.com    Abstract ? We introduce one Double-Reduct approach to get key  rules of data set in this paper. Adopting rule usefulness as a measure of key rules, our approach greatly reduces the rule numbers comparing to the traditional method. Through an experimental study of computer information security in prison, this approach is proved to be feasible and effective.

Key Words?Double-Reduct; Key Rules; Information Security.



I.  INTRODUCTION  In 1980s Pawlak first proposed rough sets theory [1].

Rough set theory had been applied to rule discovery, attribute selection and knowledge discovery in the databases such as decision make, data mining and machine learning. The condition attributes and decision attributes build up the decision table. One of the most important concepts in rough set theory is reduct. A reduct, subset of condition attributes, is used to represent the whole data set. Reduct generation is usually designed to extract key condition attributes from the decision table. The process of getting the key rules will become easier and more effective by considering fewer condition attributes.

However, this method is not widely used for the generation of association rules. Association rule algorithm has one main problem that there are usually too many rules generated, and it will take plenty of time to process a lot of rules. During the data pre-processing process, we can use some rough set method to find the redundant attributes.

Association rules generation will become faster and more effective after removing the redundant attributes. Klemettinen [2] first introduced the rule templates concept. Pre-defined rule templates are useful to generate association rules. And these rules will be applied for decision making and auto recommender systems [3], [4].

We explain how the rough sets theory is very useful to generate key association rules. It?s our interest to know how to apply these rules in decision-making process. Moreover, we try to find such rules which have, in a subsequent part, the decision attributes which are useful for decision-making. We use rule usefulness measure [10] based on rough sets to evaluate the effectiveness of the association rules. This approach is useful not only for decision-making process but also for recommender system applications.

In Section 2 we discuss the background of association rules algorithm and the definition of rule usefulness. We describe our Double-Reduct model to get key rules in Section 3. And in Section 4 we describe the applications of Double- Reduct method to prison computer security data set. Finally in  Section 5 we conclude our work by discussing how to expand the model application area in the future.



II. DOUBLE-REDUCT  A. Purpose During rule generation process we often analyse plenty of rules generated from the data set. But only some of them are useful and interesting for us. It is a challenge to find the useful and interesting rules among all the available rules for many researchers. Klemettinen [2] raised one method to help finding rules. This method ranks the rules by ?rule interestingness measure?. Rules with lower measures are regarded less interesting. The rule interestingness measure from different sources is popular to extract interesting rules. Another way is to rank the rules by ?rule usefulness measures? [10]. We want to give one method to get the key rules in an easy and effective way, and we rank the rules by ?rule usefulness measure?. Thus we introduce the Double-Reduct approach for the purpose of selecting the key rules quickly in this paper.

B. The Concept of Association Rules and Rule Usefulness Measure  Experts often use an association rule algorithm to select patterns from business transactions. For example, by analysing business transaction records from the market, with the help of association rules algorithm we can find all kinds of shopping behaviours in market basket analysis. Then we can use the association rules collected to explain such behaviours, and it is helpful to increase the sale of the market by placing related goods in the right place.

In general a rule of the form ?  ? ?  is called an association rule [5] . Here?  and ?  are different item sets and they do not share common items. The association rule ? ? ?  holds in the transaction set L with confidence c, c =  || ||  ? ?? ?  , if the condition c% of transactions in L that  contain ?  also contain ?  is true. The rule ?  ? ? has  support s, s = ||  || L  ?? ?  , if the condition that s% of  transactions in L contain ?? ?  is true. We define? as antecedent and define ? as consequent here.

The parameter confidence means a ratio of the number of transactions that the antecedent and the consequent appear at the same time to the number of transactions the antecedent appears independent. In the transaction set we often use  2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops  DOI 10.1109/IPDPSW.2012.262   2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum  DOI 10.1109/IPDPSW.2012.262   2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum  DOI 10.1109/IPDPSW.2012.262     support to compute how often the antecedent and the consequent appear at the same time. One challenge of association rule algorithm is that there are so many rules generated. As we all know, it is very difficult things to analyse all the rules. To reduce the rule numbers generated the rule interestingness measures have been introduced [5].

In this paper we adopt the method Rule Usefulness Measure [10] to evaluate the usefulness of rules. The rule usefulness definition is stated below. For convenience we use RU to replace Rule Usefulness Measure in this paper.[10] Definition 1. If one rule is generated more frequently from different rule sets, this rule is more useful than any other rules.

We define rule usefulness measure as below [10], Definition 2.If one rule appeared k times from different rules sets, and reduct sets number is n.

RU  = n k                                                                    (1)  First we collect the original data set and construct the original decision table T = (U, C, D). Here in this table U = {u0, u1, ... , um?1} is the items set; C = {c0, c1, ..., cp?1} is the condition attributes set; D is the decision attributes set. In this paper we only consider one decision attribute in decision table and it is the most common situation in our experiment. Then we do the first reduct and generate a set of rules. It is represented by RC = {Rule0, Rule1, ..., Rulen?1}. The next step is to construct the new decision table as below.

A new rule decision table is constructed and we use Am?(n+1) to represent it . The row consists of each item from the original decision table(u0, u1, ..., um?1 is the row), and Rule0,Rule1, ...,Rulen?1 build up the columns of this new decision table. And the decision attribute of new decision table is transplanted from the original decision table. Just to check each rule and find the corresponding decision attribute value is essential for us. We checked the original items one by one. If the antecedent or the consequent of a rule does not appear, we say one rule is not fit for the item. If the antecedent and the consequent of a rule appear at the same time, we say one rule is fit for the item. In our opinion, it means whether a rule can correctly interpret the item or not.

Definition 3. Double-Reduct Rule Data Set. In decision table we do the first reduct to get the rules, then we use rules to rebuild the decision table and do the second reduct.



III.  THE DOUBLE-REDUCT APPROACH TO GET KEY RULES  The flow process chart of Double-Reduct Approach is shown in Figure 1. And we describe the approach by analysing how to get key rules from the original data set of prison computer information security.

A. The Double-Reduct Model  We describe the general model of our experiment and explain how we can get the key rules step by step.

Fig. 1 Double-Reduct Approach Model.

B. The method to get the key rules The first step is data pre-processing. The data instances contain missing attribute values and the inconsistent data instances are processed during this period. When two or more data instances contain the same condition attribute values but different decision attribute values, we say that inconsistency exists in a decision table. We must remove these data instances first. We choose some data instances whose condition attribute values are the same but decision attribute values are different. Such data instances are removed during the first step. There are many discretization algorithms. For instance, equal frequency binning or entropy algorithm [8], are also used during this period. Finally at the end of data pre- processing step we generate core attributes. In rough set theory we must clear the inconsistencies in the data set first to generate the core.

The second step is to generate various multiple reducts.

There are different algorithms which provide generation of multiple reducts. One is ROSETTA?s genetic algorithm. It can generate multiple reducts.  RSES [9] is another one. It is a genetic algorithm for user defined number of reducts generation. It is widely used for for larger data sets to generate representative reducts.

The third step is generating rules. We generated multiple  Original Data Set  The First Reduct  Rules Generate  Construct New Data Set Table  The Second Reduct  Get Key Rules  Result Evaluation     reducts first, then we use the condition attributes in the reduct and the decision attributes as the input to generate rules.

Klemettinen introduced the rule templates concept [2]. Pre- defined rule templates are useful for generating perfect association rules to be used in decision-making and recommendation systems [3], [4]. We use rule templates in this step. Depending on various applications and the desired results, rule templates should be defined for expected types of rules and for subsumed rules prior to the rule generation and are applied when the rule is in generating period.

For instance, given decision attribute D, and condition attributes B1, B2, ? , Bn, the following template defines that at the consequent of a rule only decision attributes can appear, and B1, B2, ?, Bn lead to a decision of D. We can express it as follow:  B1; B2; ? ; Bn ?D.

One advantage of defining templates is to remove  subsumed rules. Given rule B1; B2 ?D, we can remove the following rules B1; B2; B3 ?D and B1; B2; B4 ?D.

Obviously they are subsumed. We can use the rules only with decision attributes on the consequent part for some decisions.

These rule templates were used in our applications.

After the rule generations multiple rule sets are therefore generated for multiple reducts. By counting the rule frequencies appearing across all the rule sets, Rule Usefulness Measures are further computed for each generated rule. Rules are ranked with each of their usefulness measures.

The last step is to evaluate the results. Core attributes play a very useful role for evaluating the ranked rules. Rules with usefulness 1.00 indicate that they contain all of the core attributes. Rules that include more core attributes are more useful than rules that include fewer or none core attributes.

Among all the condition attributes core attributes are the most representative, and more useful rules contain these more representative attributes, which are the core attributes.

Therefore we can evaluate the ranked rules with their rule usefulness by checking for the presence of the core attributes in the rules [10].

C. Complex Analyses We can calculate the time complexity for the Double-  Reduct approach. In the dataset there are X data instances, and there are Y attributes for each data instance. X? is the number of distinct values in the discernibility matrix [1]. The matrix consists of attributes for calculating the core and the reduct, and for the data set t is the number of multiple reducts. We can analyse the time complexity in the worst case.

It takes O(XY!) [5] to generate the apriori association rules, and it takes O(X?2) [6] to generate the first reducts. The core generation takes O(XY) [7]. Then it takes O(XY!) to generate second rule sets for multiple reducts. It takes O(k log k) to calculate the rule usefulness for the total rules i generated by the multiple rule sets. Therefore the time complexity of Double-Reduct method is limited by O(X?2 + XY + XY! + k log k) ? O(XY!) in the worst case.



IV. EXPERIMENT OF PRISON COMPUTER INFORMATION SECURITY  A. Prison Computer Information Security Data Set From one jail we got 42 effective items for prison computer information security. Then we construct the original data set which contains eight condition attributes and one decision attribute. The prison computer information security is the decision attribute. And it is represented by d here. For d, {0, 1} means {Computer is not safe, Computer is safe}. We use Information Security Evaluation Software Tools to get the information security value of each prison computer. And we use {b1, b2, b3, b4, b5, b6, b7, b8} to stand for {Operation System, CPU, Memory, USB Permission Option, Kill Virus On time Option, Past time after Used, Password Option, Connect to Internet Option }.

{0, 1} means {Window Series(include Window2000, Window NT, Window XP, window 7, etc), Unix Series(include Solaris, AIX, Linux, etc)} for b1.

{0, 1} means {CPU is AMD series, CPU is Intel series} for b2.

{0, 1} means {Memory is larger or equal to 1G, Memory is less than 1G} for b3.

{0, 1} means {USB port is permit to use, USB port is forbidden to use} for b4.

{0, 1} means {Kill virus on time, Do not kill virus on time} For b5.

{0, 1, 2} means {Past time after used is less than one year, Past time after used is between one year and three years, Past time after used is more than three years} for b6.

{0, 1} means {No password or password is weak, password is rough} for b7.

{0, 1} means {Connect to Internet, Not connect to Internet} for b8.

The constructed original data set of prison computer security is in Table I below.

TABLE I  PRISONER COMPUTER INFORMATION SECURITY ORIGIANAL DATA SET b1 b2 b3 b4 b5 b6 b7 b8 d 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 0 2 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 2 1 0 1 ? ? ? ? ? ? ? ? ? 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1  We check all of the data set and find no inconsistent or incomplete data items here. Then we collect 42 effective data records totally. We adopt Johnson?s Reduct generation algorithm 2 in ROSETTA [8] to generate reduct on the new decision table here.

TABLE II  PRISON COMPUTER INFORMATION SECURITY  REDUCT DATA SET No Reduct Set 1 { b1, b5, b6, b7}     2 { b1, b2, b5, b7} 3 { b1, b4, b5, b7} 4 { b1, b2, b3, b4, b7, b8}  There are 4 reduct data set in Table II.  We adopt apriori algorithm to generate 19 rules with support = 1% and confidence = 100%, and they are shown in Table III.

TABLE III  RULES  OF PRISON COMPUTER INFORMATION SECURITY No Rules 0 b1=0 ? b4=0 ? b8=0 ?d =0.

1 b1=0 ? b5=1 ?d=0.

2 b1=0 ? b6=2 ?d=0.

3 b2=0 ?d=0.

4 b3=1 ?d=0.

5 b4=0 ? b5=0 ? b8=0 ?d=0.

6 b4=0 ? b6=2 ?d=0.

7 b5=1 ? b6=2 ?d=0.

8 b7=0 ?d=0.

9 b1=1 ?d=1.

10 b2=1 ? b4=0 ? b5=0 ?d=1.

11 b2=1 ? b5=0 ? b6=2 ?d=1.

12 b4=1 ? b5=1?d=1.

13 b4=1 ? b6=2 ?d=1.

14 b4=1 ? b7=1 ?d=1.

15 b4=0 ? b5=0 ? b6=1 ?d=1.

16 b5=0 ? b7=1 ?d=1.

17 b6=0 ?d=1.

18 b8=1 ?d=1.

In the discussion with prison management policeman, we are informed that they appreciate our work to get key rules for prison computer security. Then they can do better in improving the prison computer security level. Before the collaboration with us, they have many years of management experience. But some experiences are not suitable for new computers. Sometimes the experiences are not effective and they do not know how to improve the computer security level better.

TABLE IV Key Rules of Second Reduct  No. in Table IV  Key Rules Got From Second Reduct Rule Usefulness  9 b1=1 ?d=1. 1.0 16 b5=0 ? b7=1 ?d=1. 0.75  For instance, we analyses the 9th rule b1=1 ? d=1. It means if the prison computer use the operation system of Unix series, the information security of the computer is well.

Then we analyses another 16th rule b5=0 ? b7=1 ? d=1. It means if we kill the virus on time and use the rough password, the information security of the computer is well.

B. Differences between Double-Reduct approach and traditional reduct method  The Double-Reduct algorithm is a new method to get the key rules. It is different from the traditional reduct method in the following ways.

First, the traditional reduct method first constructs the discern matrix and get the core. It only needs to do one reduct to get the rules. However, it generates plenty of rules and it can not get the key rules in one time. For our method, after one reduct, we select the useful rules to build the dataset and do the second reduct. And the Double-Reduct approach can greatly reduce the calculation and get the key rules quickly.

Second, the Rule Usefulness Measure is an objective measure [10]. Adopting rule usefulness as a measure of key rules, our approach is easy and objective.

However, we found a limitation for the Double-Reduct method. If only one reduct exists for a data set, the Double- Reduct method returns all the rules with the usefulness of 1.00 [10]. The result is the same as traditional rule generation for the data set itself..



V. CONCLUSION We presented one Double-Reduct method, and it is a direct and effective approach to getting key rules for prison computer information security. After the first reduct and getting the rules, we look at the useful rules as condition attribute and construct another rules data set as new decision table. Then the second reduct is done to get key rules from the new decision table.

We can combine other method such as interesting measures and usefulness measures with the double-reduct method to evaluate the key rules. We want to use our approach to the process of large rules data set and evaluate the effect in the future.

