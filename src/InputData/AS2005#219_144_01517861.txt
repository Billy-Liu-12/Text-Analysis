Mining Interesting Topics for Web Information Gathering and Web

Abstract  The quality of discovery patterns is crucial for  building satisfactory systems of Web text mining. It is no  doubt that we can find numerous frequent patterns from  Web documents. However, there are many meaningless  frequent patterns. This paper presents a novel method to  improve the quality of discovered patterns. It generalizes  discovered patterns into interesting topics in order to  acquire the necessary useful information. The  experimental results also verify the proposed method is  promising.

1. Introduction  The motivation arises while we determine the  interesting and useful Web pages or text documents to a  specified topic for information gathering [8]. It is easier  for users to answer which of Web pages or documents are  relevant to the specified topic rather than describe what  the specified topic they want.

It is definitely that we can obtain numerous  discovered patterns from Web documents for answering  what users want using association mining technique [12].

However, the very difficult problem is to determine  useful and interesting patterns from numerous discovered  patterns in order to improve the performance of using  discovered knowledge.

The concept of maximal patterns [4] [6] has been  presented to extend association mining for text mining,  which defined useful patterns according to their  frequencies in a category rather than in the entire data set.

Also the concept of closed sequential patterns was  presented [11] for association rule mining and has been  used in text mining [12]. To compare to the frequent  patterns, maximal and closed patterns carry more  meaning than frequent patterns, but they are too abstract  for text mining.

A group of patterns may be interesting if they come  from the same relevant document. This observation  encourages us to find some abstractions to replace groups  of patterns. In this paper, we present the concept of  interesting subtopics which consist of specificities and  sets of term weights. We also constrain subtopics?  termsets to be closed-maximal patterns which are closed  patterns in a category (e.g, a document) rather than the  entire data set.

We introduce a composition operation to merge  subtopics if they are in a common category or have the  same termset.  At last, we present a method to determine  interesting subtopics according to top-K negative  documents.

2. Definitions  Let D be a training set of documents, which consists  of a set of positive documents, D+; and a set of negative  documents, D-, where each document is a set of terms  (may include duplicate terms). Let VD = {t1, t2, ?, tn} be a  set of selected terms for all documents in D.

A set of terms X is referred to as a termset if X VD.

Let X be a termset, we use [X] to denote the covering set  of X, which includes all positive documents d such that X  d, i.e., [X] = {d | d  D+, X d}.

Given a termset X, its occurrence frequency is the  number of positive documents that contain the termset,  that is |[X]|. A termset X is called frequent pattern if |[X]|  min_sup, a minimum support.

Let X be a frequent pattern in d. We call it closed-  maximal pattern if not  any frequent patterns X1 in d  such that:  X X1 d  V D.

We use the concept of exhaustivity (Exh in short) to  assess relevance for all positive documents d, which  describes the extent to which d discusses a specified  topic.  It is measured using graded scales:  Irrelevant - Exh(d) = 0: d does not contain any  information about the specified topic.

Marginally relevant - Exh(d) = 1: d discusses a few  aspects of the specified topic or mentions the topic  but only in passing.

Fairly relevant - Exh(d) = 2: d discusses some of the  sub-themes or viewpoints for the specified topic.

Highly relevant - Exh(d) = 3: d discusses all or most  of the sub-themes or viewpoints for the specified  topic.

According to the above graded scales, we have D+ =  {d | d D, Exh(d)>0}. Let  = {(t1, f1), (t2, f2), ?,(tr, fr)}  be a term number pairs. We call  a subtopic if it satisfies:  i) termset( ) = {t |  (t, f) }, which is a closed-  maximal pattern;  ii)  d D+ such that termset( ) = d  VD and  ={(t,  Exh(d) f) |  f is the number of occurrences of t in d  for all t termset( )}.

We also use the concept of specificity (Spe in short) to  assess relevance of subtopics, which describes the extent  to which the subtopic focuses on the specified topic. We  let Spe( ) = Exh( ) for all subtopics in the beginning.

A subtopic 1 equals to a subtopic 2 if and only if  termset( 1)=termset( 2). A subtopic is uniquely  determined by its termset, that is a distinguish feature to  clustering methods.  Two subtopics may be composed if  they have the same termset or they are in the same  category (e.g., they are the title and the abstract of an  article, respectively). In this paper, we use a composition  operation, , to generate new subtopics.

Let 1 and 2 are two subtopics. We call 1 2 the  composition of 1 and 2 which satisfies:  }),()),()((  ))()((|),{(  }),(,),(|),{(     fttermsettermset  termsettermsettft  ftftfft  (1)  Spe( 1 2) = Spe( 1) + Spe( 2).

A subtopic 1 is called a supper topic of a subtopic 2 if  and only if termset( 1) termset( 2) and 2 is not a  composition of 1. We can say 2 is-a 1 or 2 is derived  from 1 if 1 is a supper topic of 2.

3. Mining Algorithm  The details of finding the subtopics can be found in  Algorithm 3.1. In this algorithm, step (1) initialises the set  of subtopics and the set of possible closed-maximal  patterns C. Step (2) finds the set of all poosible closed-  maximal patterns X such that |[X]| 1. In step (3), all  subtopics are found and step (4) returns the result.

We assume the basic operation in Algorithm 3.1 is the  comparison between two terms and the terms in VD are  sorted. The time complexity of step (2) is O(m(slogn +  slogs)) = O(mslogns), where n=|VD| is the number of  selected terms,  m=|D+| is the number of positive  documents and s is the average size of documents. The  time complexity of step (3) is O(mslogs + ms) =  O(mslogs), where determining of maximal patterns takes  O(mslogs). Therefore the time complexity of this  algorithm is O(mslogns).

Algorithm 3.1(D, VD, min_sup): Discovery of  subtopics.

Input: D, VD and minimum support min_sup.

Output: , the set of subtopics.

Method:  (1) = empty, C = empty;  // C includes all possible  // closed-maximal patterns X such that |[X]| 1  (2) for (each d D+) {  pd =  d;  for (each t d and t VD)  pd =  pd  - {t};  Sort pd ;  C = C  {pd}; }  (3) for (each pd C and |[pd]|  min_sup)  { // we view pd as a non-empty array  =empty, i = 1;  while (i size(pd)) {  j = i, f = 1;  while ( j size(pd) and pd[j] = pd[i])  { j = j + 1, f = f + 1; }  =  {(pd[i], Exh(d) * f)}, i = j; }  if (  empty)  {  Spe( ) = Exh(d), =  { }; } }  (4) return ;  Table 1 depicts a set of positive documents for a  specified topic ?Economic espionage?. We let VD = {bill,  econom, espionag, german, man, us, vw} in this example.

Table 1. An example for D+  Document Terms Exh  d1 german vw  1  d2 us us econom espionag  1  d3 bill econom espionage us 1  d4 bill econom espionage us 1  d5 espionag german man vw  1  d6 espionag german german man vw  1  We can discover 6 subtopics from Table 1 using  Algorithm 3.1, where min_sup=1, i.e.,  = { 1, 2, 3, 4,  5, 6} (see Table 2).

Table 2. Discovered subtopics  Name Spe Subtopic  1 1 {(german,1), (vw,1) }  2 1 {(econom,1), (espionag,1), (us,2), }  3 1 {(bill,1), (econom,1), (espionag,1), (us,1)}  4 1 {(bill,1), (econom,1), (espionag,1), (us,1)}  5 1 {(espionag,1), (german,1), (man,1), (vw,1)}  6 1 {(espionag,1), (german,2), (man,1), (vw,1)}      Also, we should compose all pairs of subtopics with  the same termset using Equation (1). For example, we let  7 = 3 4 = {(bill,2), (econom,2), (espionag,2), (us,2)}  since termset( 3) = termset( 4) = {bill, econom, espionag,  us}; and 8 = 5 6 = {(espionag,2), (german,3), (man,2), (vw,2)}.

Finial we can update   and let  = { 1, 2, 7, 8} be  the discovered subtopics, where 3, 4, 5 and 6 are  removed since they are redundant subtopics after  finishing the composition operation. Table 3 illustrates  the set of discovered subtopics.

Table 3. Discovered subtopics  Name Spe Discovered Subtopic  1 1 {(german,1), (vw,1) }  2 1 {(econom,1), (espionag,1), (us,2), }  7 2 {(bill,2), (econom,2), (espionag,2), (us,2)}  8 2 {(espionag,2), (german,3), (man,2), (vw,2)}  4. Interesting Topics  Indeed not all discovered subtopics are interesting  subtopics for the specified topic. In this section, we  illustrate how to determine interesting subtopics.

We cannot determine meaningless discovered  subtopics using only positive documents. However, the  consequential result of using discovered subtopics is that  some irrelevant documents may be marked in relevance.

That guides us to find interesting subtopics through  tracing errors made by using the discovered subtopics.

Given a Web page p, search engines often use a  formula to calculate a weight for it according to the  behind knowledge about a query.   For example, we use  the following formula to evaluate a weight for p:  DVpt vt  vSpepweight  ),(,  )()(    (2)  The difficult problem is that some negative  documents in D- may obtain larger weights than some  positive documents? since not all discovered subtopics  are interesting subtopics.

For example, given the following negative document:  nd = {german, focus, vw, unveils, new, passat, says,  sales}, we have weight(nd) = weight(d1) since nd  V D =  d1  V D, where d1 is a positive document (see Table 1).

Therefore we do not know which one is relevant.

For example, we have the following relations  between 1 and other discovered subtopics, negative  document nd and positive document d1: 8 is-a 1, d1 is-a  1 and nd is-a 1. The relations indicate that 1 is not an  interesting subtopic since it derives both positive and  negative documents.

Let ND be the set of top-K negative documents in D-.

We call a subtopic interesting subtopic if not nd  ND such that termset(  ) nd.

5. Evaluations  In this section, we use an experiment to evaluate the  proposed method. We use a standard data collection,  TREC2002 filtering track data collection (Text REtrieval  Conference, see http://trec.nist.gov/), which included  Reuters Corpus articles from 1996-08-20 to 1997-08-19.

In the experiment, we test 20 topics: R101, 102, ?, to  R120, which include about ten thousands different XML  documents.

The proposed testing is compared to closed-maximal  pattern mining model that discards information about the  frequency of the terms in the document. We use both top  25 precision and breakeven point which are two methods  used in Web mining for testing effectiveness. The greater  both the top 25 precision and the breakeven point, the  more effective the model is.

The selected terms in VD were chosen by removing  stop words, stemming the documents using the porter  stemming algorithm and then taking the top 150 terms  chosen by tf*idf weights. We view each positive article as  three documents: headline, title and body and use the  following settings:  Exh(headline) = 3  Exh(title) = 2  Exh(body) = 1  We also let min_sup be 1 and top-5 negative  documents were used to generate ND.

Given a new incoming document, d, we extend  Equation (2) for both interesting topics model and  maximal patterns model as follows:  ),()()( dtermtermweightdweight DVterm  i  where otherwise  yxif yx   ),( , i = 1 (denotes interesting  subtopics) or 2 (denotes closed-maximal patterns), and  )(1 termweight wterm  wSpe ),(,  )(  )(2 termweight wterm  Spe ),(,  )(  for all term VD.

Figure 1 shows the difference of interesting subtopics  and closed-maximal patterns in Break Even Point and  Top 25 Precision for the 20 topics. The positive values  (the bars above the horizontal axis) mean the interesting  topics performed better than maximal patterns. The  negative values (the bars below the horizontal axis) mean  the closed-maximal patterns performed better than  interesting topics. It is no less impressed by the  performance of the interesting topics since both top 25  precision and breakeven points gain a significant  increase.

-0.15  -0.1  -0.05   0.05  0.1  0.15  0.2  0.25  0.3  0.35            Break Even Point  Top25 Precision  Figure 1. Difference between models  Another advantage of using interesting topics is that  we can use reshuffle operation (see [9]) to remove  uncertainties from some interesting topics. For each nd  ND, the reshuffle operation first determines a set of  offenders from , which satisfies  nd = {  | termset( ) nd }.

It also determines the offering of the offenders using the  following equation:  offering: nd  [0,1],  ndtermvterm  voffering  ,),(  )(  In addition, it shifts ( -1)/  of the offering from  termset( ) nd to termset( ) - nd, where  = 2 in this  experiment.

On average using both the interesting topics and the  reshuffle operation increased the Break Even Point and  Top 25 Precision by 13.9% and 10.2%, respectively.

6. Related Work and Conclusions  Web information gathering (IG) systems tend to find  useful information from the huge size of Web related data  sources to meet their user information needs. The key  issue regarding the effectiveness of IG is acquiring of  user profiles [8]. It is also a fundamental issue in Web  personalization [3].

In this paper, we use interesting topics to describe user  profiles.  We try to derive meaning from user feedback.

Association mining has been used in Web text mining for  such purpose for association discovery, trends discovery,  event discovery, and text classification [5] [7].

To compare with IR-based models, data mining-based  Web text mining models do not use term independent  assumption [1].  Also, Web mining models try to discover  some unexpected useful data [10] [2]. The disadvantage  of using association mining for interesting topics is that  there exits many noise patterns. The noise patterns make  the performance of text mining systems ineffectively [9]  [12]. On the other hand, clustering analysis can reduce the  number of patterns by creating a hierarchical structure;  however, it is difficult to explicitly interpret the relation  between a node and its parents using user acceptable  concepts. Another drawback is that the coupling between  levels is fastening.

To overcome the above disadvantages, in this paper  we present a new text mining technique to find interesting  topics. To compare with clustering analysis, the proposed  approach can easily show the meaning of discovered  subtopics. To compare with association mining and  maximal patterns, our approach can obtain just right  knowledge from text documents.

We have verified that the new technique not only gains  a better performance on both precision and recall, it also  decreases the burden of on-line training since the  proposed approach only needs relevant data and part of  irrelevant data in the training set.

