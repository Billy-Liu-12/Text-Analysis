A SAT-Based Approach for Enumerating Interesting Patterns from Uncertain Data

Abstract?Discovering useful patterns plays an essential role in data management and data mining. Frequent itemset mining in uncertain transaction databases semantically and computation- ally differs from traditional techniques applied on (standard) precise transaction databases. Uncertain transaction databases consist of sets of existentially uncertain items. The uncertainty of items in transactions makes traditional techniques inappli- cable. Recent works propose interesting SAT-based encodings for the problem of discovering frequent itemsets in deterministic transaction databases. Our aim in this work is to extend the SAT-based encoding of frequent itemset mining to uncertain databases. Then, we propose a novel declarative mining frame- work for extracting uncertain frequent patterns from uncertain transaction databases. It makes an original use of constraints relaxation to obtain upper bounds to the expected support of frequent patterns; while guaranteeing the enumeration of all frequent itemsets with no false negatives. We experimentally evaluated our approach. The experimental results on real and synthetic data sets demonstrate the effectiveness of our proposal in mining frequent patterns.

Index Terms?Uncertain Transaction Database, Frequent Item- set Mining Problem, Propositional Satisfiability.



I. INTRODUCTION  Pattern mining constitutes a well established data mining research issue. The most important and representative problem is frequent itemset mining (FIM, for short), which consists in finding all sets of items that appear in a data set with frequency no less than a user-specified minimum support threshold.

Since the emergence of the first algorithm of Agrawal [1] for extracting frequent itemsets, FIM algorithms were constantly developed, while trying to get better performance. These algorithms can be classified into two main categories: The level-wise and the pattern-growth like approaches. The first one is based on the generate-and-test framework such that Apriori [1]. While the second category is based on the divide- and-conquer framework such as FP-growth [2], and H-Mine [3] (see [4] for an overview).

These existing proposals deal with certain transaction datasets, that is, each item occurring in a transaction is known with certitude. Unfortunately, when dealing with real-world  applications, the massive amounts of data are tainted with imperfection and uncertainty. This is the case for data collected from experimental measurements susceptible to noise. For instance, the locations of objects obtained through GPS or RFID, the data collected from sensors, and the statistical com- putations are inherently noisy. Discovering interesting patterns from this kind of datasets is more difficult than mining from traditional transaction datasets Consequently, the problem of finding itemsets on uncertain transaction datasets has gained a tremendous interest among data mining community. Indeed, several approaches and algorithms have been proposed (e.g.

[5], [6], [7]). More details are provided in the related works section.

The goal of the above mentioned data mining methods is to generate patterns satisfying some specified constraints.

Indeed, numerous properties (e.g., frequency, maximality, anti- monotony) have been studied and implemented in different data mining systems. Unfortunatly, the existing methods have the drawback of being less flexible when dealing with new constraints. More precisely, new kind of data and/or con- straints characterizing interesting patterns continuously emerge from applications or users. Usually, such modifications in the problem specification leads inevitably to new implementations from scratch. Such lack of flexibility and declarativity is identified as one of the main drawback of classical data mining approaches.

Recently, a new declarative data mining research trend has been initiated by Luc De Raedt et al. in [8]. So, in addition to the level-wise and the pattern-growth approaches there have been other studies which employ a declarative approach. In this kind of approaches, the data mining task is modeled as a constraint network or propositional formula whose models corresponds to the patterns of interest. Indeed, a large body of work has sought to solve the pattern explosion problem by mining under constraints. In fact, in [8] De Raedt et al. addressed this problem by initiating a research trend on constraint programming based pattern mining. The authors proposed a framework for itemset mining offering a   DOI 10.1109/ICTAI.2016.44    DOI 10.1109/ICTAI.2016.44    DOI 10.1109/ICTAI.2016.44     declarative and flexible representation model and benefiting from several generic and efficient constraint programming solving techniques. In the same vein, several contributions have been proposed in order to address other data mining settings using propositional satisfiability (SAT) (e.g., [8], [9], [10], [11], [12]).

Today, SAT has gained a considerable audience with the advent of a new generation of solvers able to solve large instances encoding real-world problems with millions of vari- ables and clauses. In fact, these solvers represent important low-level building blocks for many important fields, e.g., SAT modulo theory, Quantified Boolean formulas, Pseudo Boolean, argumentation theory, etc. Additionally to the tra- ditional applications of SAT to hardware and software formal verification, SAT technology has been applied in different new fields such as planning, bioinformatics, cryptography and more recently data mining. In most of these applications, we are usually interested in determining the satisfiability of a given propositional formula (decision problem), or in finding an optimal solution such as in Max-SAT (Maximum Satisfiability). However, in data mining, we mainly deal with the enumeration of all the models of the formula. Notice that all existing declarative approaches focus on computing pattern from certain datasets. In this work, we will exploit the progress of SAT solvers by providing SAT based encoding for discovering relevant patterns over uncertain data. To the best of our knowledge, this paper presents the first attempt to discover frequent uncertain patterns via SAT solvers.

The remainder of the paper is organized as follows. The next section presents related works. Section III introduces some basic definitions about FIM over uncertain data and the boolean satisfiability problem. Section IV is devoted to our novel SAT-based framework for discovering interesting patterns. Section V provides a relaxation of our proposed framework. Section VI presents and discusses the results of our experimental evaluation on real and synthetic uncertain datasets. Finally, we conclude the work in Section VII with some further perspectives.



II. RELATED WORK  Mining frequent itemsets is an important problem in data mining. It is involved for example in the association rules derivation process. When dealing with FIM from uncertain data, studies are mainly based on the extension of classical itemset mining approaches. Consequently, one can also dis- tinguish two kind of approaches: the level-wise approach and the pattern-growth approach.

A. Level-wise approach  There have been many studies dealing with the FIM problem under uncertain datasets which employ the level- wise approach. More precisely, the algorithm U-apriori [5] was proposed to retrieve frequent itemsets from uncertain datasets. It was based on the Apriori algorithm [1]. In fact, the basic idea of this approach is to generate at each iteration the candidate itemsets of size k + 1 from those of size k.

Such set of candidates is then used to extract all the frequent patterns. Indeed, an itemset is considered as frequent if its expected support number exceeds the predefined minimum support. This process stops when there is no new candidate.

However, this algorithm has several drawbacks. That is, the generation and the processing of the excessive candidate itemsets require multiple scans of the database. Besides, the other critical shortcoming of this approach is that it needs generating candidates. So, for datasets with a high number of large transactions and small minimum expected support, the number of candidates might be huge in practice.

In order to overcome this problem, approximation tech- niques for efficient itemset mining were employed in the context of uncertain databases. For instance, in [6] the au- thors proposed the use of an upper bound of the expected support of an itemset to prune the search space. Moreover, another algorithm, named MBP, aims to reduce the number of candidates itemsets by using the Poisson distribution in order to capture the estimated expected support of an itemset [13].

However, the number of candidates might be huge and hence constitutes the bottleneck of the mining process. Recently, another technique [14] was proposed for further improving the efficiency of the MBP algorithm by adopting the maximum average probability of an item as a weight value, in order to decrease the estimate of the expected support of a higher-order itemset.

B. Pattern-growth approach  The main idea of pattern-growth algorithms comes from the well-known FP-growth method [2], designed to handle certrain data. The first algorithm that employs this method for mining uncertain data is called UF-growth [15]. This algorithm requires two scans. Firstly, it constructs an UF-tree relying on the following rules: (1) itemsets are ordered according to their (accumulated) expected support numbers and added to the UF- tree; (2) Itemsets share the same path when the corresponding items and their expected support are the same. Secondly, the algorithm finds all frequent itemsets from the proposed UF- tree using the pattern growth-algorithm. Unfortunately, the processing in uncertain data is different from that in certain ones, since each item is described by an existential probability.

Consequently, the UF-tree requires a huge memory cost and computational time to process a large amount of data. Hence, the resulting UF-tree is not compact as the original FP-tree.

In addition, approximate methods were proposed in an attempt to make the tree more compact. In [7], the authors improved the UF-growth algorithm by reducing the size of the UF-tree. More precisely, they consider that the items with the same k-digit value after the decimal point possess the same probability. However, the main shortcoming of this improved algorithm is that some of the frequent itemsets might be lost.

Furthermore, the UFP-growth algorithm was introduced in [16]. It groups similar nodes (with the same item x and similar existential probability values) into a cluster. However, depend- ing on the clustering parameter, the corresponding UFP-tree may be as large as the UF-tree. In [17] the authors proposed     a more compact tree structure, named PUF-tree, to capture uncertain data. The obtained PUF-tree is then used to find frequent itemsets. The algorithm uses the PUF-tree to obtain upper bounds to the expected support of frequent patterns. In [18], the authors proposed a tight upper bound to the expected support for uncertain frequent pattern mining, using a more compact tree structure called TPC-tree. As substitutes to trees, hyper-structures were used by the UH-Mine [19], which is reported in AT-Mine [20].

Please note that the above described methods employ the expectation of the support of an itemset to measure whether this itemset is frequent. This definition refers to the expected support-based frequent itemset. Another semantic explanation have been used to provide information about the frequency of an itemset. This latter refers to the probabilistic frequent itemset (see [21] for more details).

In this work, we concentrate on the first kind of approaches, namely expected support-based frequent itemset mining algo- rithms.



III. PRELIMINARIES  In this section, we provide some background information about frequent itemsets mining of uncertain data (e.g., existen- tial probability, expected support), and we then present some preliminaries about propositional satisfiability problems (e.g., CNF formula, model, SAT problem.

A. Frequent Itemset Mining over uncertain transaction databases  In this subsection, we briefly recalls some notions about excepted support-based frequent itemsets over uncertain data.

Let ? be a set of items. An uncertain transaction database is a finite set of n uncertain transactions D = {T1, T2, ..., Tn}.

Each uncertain transaction Ti (1 ? i ? n) is defined as a couple (tid, I) where tid is the transaction identifier and I ? ? is an itemset represented as {a1 (p1), a2 (p2), ..., am (pm)}, s.t. pj (1 ? j ? m) is the existential probability of the item aj . Intuitively, pj denotes the possibility of item aj appearing in the tid tuple, with value 0 < pj ? 1 1. We assume that items? existential probabilities in transactions are determined through independent observations. The attribute tid refers to a unique itemset.

We say that a transaction (tid, I) supports an itemset J if J ? I.

Moreover, the cover of an itemset I in an uncertain transaction database D is the set of transactions in D supporting I:  C(I,D) = {(tid,J ) ? D | I ? J } In the sequel, we adopt common definitions widely used  by the data mining community.

Definition 1: Let D = {T1, T2, ..., Tn} be an uncertain transaction database. Then, the probability of an item aj  1If an item a has an existential probability of zero, it does not appear in the transaction.

(1 ? j ? m) in a transaction Ti (1 ? i ? n), denoted by p(aj , Ti), is defined as:  p(aj , Ti) = pji (1)  Example 1: Let us consider an example from the super- market domain. The goal is to find sets of products that are purchased by a large group of customers. This information can be useful to identify the purchase behaviour of the customer to progress the business. The transaction database is shown in Table I below, where each item represents a product that can be bought by customers. We will use this database a running example throughout the paper.

From Table I, the probability of the item {milk} in the transaction T3 is p(milk, T3) = 0.3.

Based on the above definition, the existential probability of a pattern I in an uncertain transaction Ti can be seen as the product of the corresponding existential probability values of every item a within I, and is defined formally as:  Definition 2: Let D = {T1, T2, ..., Tn} be an uncertain transaction database. Then, the existential probability of an itemset I in Ti (1 ? i ? n), denoted by p(I, Ti), is defined as:  p(I, Ti) = ?  aj?I,I?Ti pji (2)  Example 2: Let us consider Example 1. We can see that the probability of the itemset {milk, bread} in the transaction T3 is p({milk, bread}, T3) = p(milk, T3) ? p(bread, T3) = 0.3? 0.4 = 0.12.

Definition 3: Let D = {T1, T2, ..., Tn} be an uncertain transaction database. Then, the expected support number of an itemset I in D, denoted by ExpSN, is defined as:  ExpSN(I,D) = ?  Ti?D p(I, Ti) (3)  Example 3: The expected support number of the itemset {milk, bread} in the dataset depicted in Table I is ExpSN(I,D) = p({milk, bread}, T1) + p({milk, bread}, T2) + p({milk, bread}, T3) + p({milk, bread}, T5) = 0.3 + 0.56 + 0.12 + 0.1 = 1.08.

Let D be an uncertain transaction database over ? and ? a minimum support threshold. The problem of mining frequent itemset over D can now be defined as finding:  FIM(D, ?) = {I ? ? | ExpSN(I,D) ? ?} (4) That is, the set FIM(D, ?) contains the set of frequent  uncertain itemsets with an expected support number larger than ?.

Example 4: Let us consider again Example 1. Assume that ? is set to 0.4. Let us take the itemset {milk, bread}.

Its ExpSN is ExpSN({milk, bread},D) = 1.08, which is     TID Customer Transactions T1 A milk (0.6) butter (0.3) juice (0.3) bread (0.5) T2 B milk (0.7) game (0.25) bread (0.8) T3 B milk (0.3) juice (0.8) bread (0.4) egg (0.2) T4 C juice (0.7) bread (0.2) egg (0.3) T5 D milk (0.5) bread (0.2) shoes (0.3)  TABLE I UNCERTAIN TRANSACTION DATABASE  larger than ?. The itemset {milk, bread} is thus frequent.

Contrary, the itemset {egg, bread} is considered not frequent as ExpSN({egg, bread},D) = 0.8 + 0.6 = 0.14 is smaller than ?.

B. Boolean Satisfiability Problem  Let us first introduce the propositional satisfiability prob- lem (SAT) and some necessary notations. We consider the conjunctive normal form (CNF) representation for the propo- sitional formulas. A CNF formula ? is a conjunction (?) of clauses, where a clause is a disjunction (?) of literals. A literal is a positive (l) or negated ?l propositional variable. The two literals l and ?l are said complementary. A CNF formula can also be seen as a set of clauses, and a clause as a set of literals.

Let us mention that any propositional formula can be translated to CNF using linear Tseitin?s encoding [22]. Further, we denote by V ar(?) the set of propositional variables occurring in ?.

A Boolean interpretation B of a propositional formula ? is a function which associates a value B(l) ? {0, 1} (0 corresponds to false and 1 to true) to the propositional variables l ? V ar(?). It is extended to the CNF formulas as usual. A model of a formula ? is a Boolean interpretation B that satisfies the formula, i.e., B(?) = 1. We note M(?) the set of models of ?. SAT problem consists in deciding if a given propositional formula admits a model or not. This well known NP-Complete problem has seen spectacular progress these recent years. Thus, efficient SAT solvers can now handle CNF formulas encoding industrial problems up to millions of variables and clauses. Consequently, providing SAT encoding for a given problem allows us to benefit from this progress.

The DPLL procedure [23] is the most popular search method for solving SAT problems. This procedure explores the space of partial assignments in a systematic way that eliminates the ones that lead to a contradiction, and extends incrementally the promising ones. A partial assignment can be extended by selecting an unassigned variable and trying in turn the two possible assignments (True, False) to that variable (branching), thus creating two new partial assignments. A reduced formula is created in each case; a True assignment to li eliminates all clauses that contain the literal li (these clauses are satisfied) and all appearances of ?li from the remaining clauses. Literals occurring in clauses of size one, called unit- literals, are propagated (assigned to True) systematically at each node of the search tree. A False assignment has the sym- metric effect. The same procedure is then applied recursively to each reduced formula.



IV. A SAT ENCODING OF FREQUENT ITEMSET MINING In this section, we present our SAT-based approach for  the problem of mining itemset over uncertain databases. Our framework builds on an existing method which showed the feasibility of propositional satisfiability for pattern mining over precise databases [11].

Without loss of generality, we fix an uncertain transaction database D = {(1, I1), . . . , (n, In)} and a minimal support threshold ?. Our SAT encoding of itemset mining that we consider is based on the use of propositional variables to represent the items and the transaction identifiers in D. More precisely, for each item a (resp. transaction identifier i), we associate a propositional variable, denoted la (resp. qi). These propositional variables are used in 0/1 linear inequalities to capture all possible itemsets and their covers. More formally, given a Boolean interpretation B, the candidate itemset and its cover are expressed as {a ? ? | B(la) = 1} and {i ? N | B(qi) = 1}, respectively.

We now introduce our SAT-based encoding using the propositional variables described previously. The first propo- sitional formula that we describe allows us to obtain the cover of the candidate itemset:  n?  i=1  (?qi ? ?  a??\Ii la) (5)  That is, this propositional formula expresses that qi is true if and only if the candidate itemset is supported by the ith transaction. In other words, the candidate itemset is not supported by the ith transaction (i.e., qi is false), when there exists an item a (i.e., la is true) that does not belong to the transaction (i.e., a ? ? \ Ii).

Let us now give the formula expressing that the support of the candidate itemset has to be larger than the specified threshold ?.

n?  i=1  ?  a?Ti (p(a, Ti)? la ? qi + ?la ? qi) ? ? (6)  In the sequel, EFIM(D, ?) will denote the encoding that corresponds to the conjunction of the two formul? (5) and (6). Then, the following property holds:  Property 1: Let D = {T1, T2, ..., Tn} be an uncertain transaction database. Then, B is a model of EFIM(D, ?) iff I = {a ? ? | B(la) = 1} is a frequent itemset where C(I,D) = {i ? N | B(qi) = 1}.

Example 5: Let us consider the uncertain transaction database of Table I. The Problem that encoding the     enumeration of frequent itemsets with a ?=0.4 can be written as follows:  ?q1 ? (lgame ? legg ? lshoes), ?q2 ? (lbutter ? ljuice ? legg ? lshoes), ?q3 ? (lbutter ? lgame ? legg ? lshoes), ?q4 ? (lmilk ? lbutter ? lgame ? lshoes), ?q5 ? (lbutter ? ljuice ? lgame ? legg), ??5i=1  ? a?Ti(p(a, Ti)? la ? qi + ?la ? qi) ? 0.4  Let us stress that when the existential probability value of each item in D is equal to 1 (i.e., p(a, Ti) = 1, ?i (1 ? i ? n)), then  ? a?Ti(p(a, Ti)? la ? qi + ?la ? qi) =  ? a?Ti(1?  la?qi+?la?qi). As la+?la = 1, ?  a?Ti(p(a, Ti)? la?qi+?la? qi) = qi holds. This corresponds exactly to the boolean encoding proposed in deterministic case, i.e.,  ?n i=1 qi ? ?  [11].



V. RELAXATION BASED COMPUTATION OF FREQUENT ITEMSETS  In this section, we address the challenging issue of man- aging the expected support condition through a logical ap- proach based on constraints. Indeed, we have shown in the previous section that such constraint can be expressed by the inequality  ?n i=1  ? a?Ti(p(a, Ti) ? la ? qi + ?la ? qi) ? ?.

However, constraint-based approaches involve usually linear constraints. In fact, the first linear encoding of general 0/1 linear inequalities to CNF have been proposed by J. P. Warners in [24]. Several approaches have been introduced to deal with the issue of finding an efficient encoding of such constraints (e.g. [25], [26], [27]) as a CNF formula. Efficiency refers to both the compactness of the representation (size of the CNF formula) and to the ability to achieve the same level of constraint propagation (generalized arc consistency) on the CNF formula.

Unfortunately, the translation of Inequality (6) into a lin- ear one is clearly intractable. This bottleneck makes our constraint-based mining approach unfeasible in the general case.

To tackle this subtle issue, one way consists to weaken the original constraint p(I, Ti) in order to obtain a less restrictive variant. Indeed, a relaxation of ExpSN(I,D) could be performed, so that when satisfied, the itemset I is frequent.

Please notice that this relaxation allows a necessary but not sufficient condition for an effective mining of frequent itemsets over uncertain transaction databases.

Now, we formally present our constraint-based mining ap- proach for relaxing the expected support of a given itemset.

To do so, let us give firstly some useful notations.

Definition 4: Let D = {T1, T2, ..., Tn} be an uncertain transaction database and I an itemset comprising k items (i.e., a k-itemset) s.t. I ? Ti (1 ? i ? n). We define the maximum existential probability of I in Ti (1 ? i ? n) as:  pmax(I, Ti) = pmax(k, Ti) = max{ ?  a?J , J?Ti p(a, Ti) | |J | = k}  (7) Intuitively, for a given k-itemset I, pmax(I, Ti) is defined as  the product of the k highest existential probability values in the transaction Ti where J ? Ti. Notice that pmax(I, Ti) provides us with an important property of covering the existential probabilities of all possible patterns containing k items in Ti as stated in the following proposition.

Proposition 1: Let D = {T1, T2, ..., Tn} be an uncertain transaction database. Let I be a k-itemset s.t. I ? Ti (1 ? i ? n). Then, for all k-itemset J where J ? Ti, we have:  p(J , Ti) ? pmax(I, Ti)  Example 6: In Table I, the maximum existential probability of the itemset {milk, butter} in the transaction T1 is:  pmax({milk, butter}, T1) = 0.6? 0.5 = 0.3  Let us point out that the new relaxed constraint is expressed in the light of an upper bound of existential probabilities values.

Since the expected support number associeted to an itemset I is the sum of all existential probabilitis of I over all the transactions containing I, the relaxed support number of a given itemset can then be defined as follows.

Definition 5: Let D = {T1, T2, ..., Tn} be an uncertain transaction database and I an itemset in D. Then, the relaxed expected support number of I, is defined by:  R ExpSN(I,D) = ?  Ti?D pmax(I, Ti) (8)  It is easy to show the following result.

Proposition 2: Let D = {T1, T2, ..., Tn} be an uncertain transaction database and I an itemset in D. Then, the follow- ing inequality holds:  R ExpSN(I,D) ? ExpSN(I,D) Based on Definition 5, for any k-itemset I = {a1, . . . , ak}  R ExpSN(I,D) can be considered as an upper bound to the expected support of I, i.e., pmax(I, Ti) ? p(I, Ti) and ExpSN(I,D) ? R ExpSN(I,D). So, if R ExpSN(I,D) ? ?, then I can not be frequent. Conversely, if I is frequent, then R ExpSN(I,D) must be greater than or equal to ?. Thus, our approximation gives us a safe condition w.r.t. R ExpSN(I,D) and ?. This condition, which allows us to obtain an upper bound of the expected support for each pattern, could be safely applied for all frequent uncertain patterns. Consequently, this can lead to the generation of a complete set of frequent itemsets from the original transaction database without any false negatives.

Now, let us illustrate the behaviour of the SAT solver to handle our mining approach. Indeed, the proposed method     relies on DPLL SAT solver for model enumeration. To im- prove the efficiency of the computation, we launch the solver iteratively according to the cardinality k (i.e., k-itemsets for k ? 1). Based on the following inequality, we guarantee that the pattern of size k is potentially frequent.

Fk = ( ?  a?? la = k) ? (  ?  Ti?D pmax(k, Ti)? qi ? ?) (9)  Our iterative based approach for mining frequent itemset is depicted by Algorithm 1. It takes as input an uncertain transaction database, and returns all the frequent itemset as output. As our Boolean formula encodes the FIM problem, each time a model is found, an itemset is extracted. Indeed, Algorithm 1 starts by generating a Boolean formula G that allows us to obtain the cover of the candidate itemset (line 1). The SAT encoding of the formula G is described by the formula (5). The SAT enumerator is then launched on the simplified formula (Fk ? G) to search for all models. More precisely, the SAT solver enumerates in each iteration, the set of models S ? for the (?a?? la = k) (line 6). Then, the obtained models S ? are added to the set S in order to keep the whole set of extracted itemsets (line 7). If a k-itemset X is not frequent, then all its supersets Y (i.e. X ? Y ) are not frequent. Thus, the enumeration procedure stops when no model is generated for the fixed value k (line 8).

Algorithm 1: Iterative SAT-based Itemsets Enumeration Input: An Uncertain Transaction Database D Output: A Set of all frequent itemsets S  1 G ? SATEncodingTable(D); 2 S ? ?; S ? ? ?; /* Boolean models */ 3 k ? 0; /* size of itemsets */ 4 repeat 5 k ? k + 1; 6 S ? ? enumModels(Fk ? G); 7 S ? S ? S ?; 8 until (S ? = ?); 9 return Set of all frequent itemsets (S);

VI. EXPERIMENTS In this section, we evaluate the performance of our algo-  rithm empirically. The primary goal is to assess the declara- tivity and the effectiveness of our proposed method. The exper- iments are performed on a PC with an Intel Core i7 processor, with 8 GB RAM main memory, and the Precise Pangolin x86 64 OS. Moreover, our implementation of Algorithm 1 was written in C.

Since real uncertain data sets are not available, in our experiments, we carried out an empirical evaluation of our algorithm using a variety of real-life certain datasets taken from the FIMI repository 2 (i.e., mushroom, soybean, pri- mary tumor, vote, chess, etc.), and we assigned an (randomly  2FIMI: http://fimi.ua.ac.be/data/  generated) existential probability value to each item in every transaction in these datasets. Notice that assigning probability to deterministic database in order to generate meaningful uncertain test data is widely accepted by the data mining community. More details of the characteristics of the above datasets are listed in Table II. For each instance, we mention the number of transactions (=transaction), the number of items (=items), and the length of each transaction.

Dataset ?= Transactions ?= Items Length mushroom 8124 120 21  anneal 812 93 42 zoo 1 101 36 16  soybean 650 50 16 vote 435 48 16  primary tumor 336 31 15 tic-tac-toe 958 27 9  TABLE II DATASETS SUMMARY  Results for a representative set of datasets are shown in Fig- ure 1 (log scale). The other instances present similar behaviour.

We evaluate the run-time of our algorithm on different datasets by varying the value of minsup. Notice that the reported results are based on the average of multiple runs for each case. Figure 1 presents the run-time evolution against the minsup value.

It is worthwhile to notice that our method required shorter run-times for most datasets. Here, when minsup decrease we observe that the run-time goes up. The interesting observation is that our method gives satisfactory results using a flexible approach that allows us to add other constraints without new implementation from scratch.

To better highlight the overall performances, we also mea- sured the number of false positives (i.e., infrequent patterns) generated by our algorithm. We present results (in percentage) using one minsup value for each dataset. As shown in Table 1, our algorithm generated fewer false positives which do not exceed 30% for most instances, except mushroom and anneal that possess less interesting values of false positives. This can be explained by the large length of transactions in these datasets. We support our explanation by noticing that SAT encodings of these databases are large enough to explain the complexity of the proposed method.

Dataset minsup % False Positive anneal 20 56.31  mushroom 20 42.94 zoo 1 0.1 30.29  tic-tac-toe 0.1 4.84 vote 0.1 31.50  soybean 0.1 30.52 primary tumor 0.1 25.08  TABLE III THE AVERAGE OF FALSE POSITIVES FOR DIFFERENT DATASETS  To objectively evaluate our approach, we prune the potential infrequent patterns. We test thus the impact of the value of k              2  4  6  8  10  12  14  16  18  20  tim e  (s ec  on ds  )  expected support  mushroom         0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  tim e  (s ec  on ds  )  expected support  vote         0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1  tim e  (s ec  on ds  )  expected support  soybean   0.5   1.5   2.5   3.5   4.5   0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1 tim  e (s  ec on  ds )  expected support  primary-tumor  Fig. 1. Results on a sample of datasets by varying the support value             2  3  4  5  6  7  % F  al se  P os  iti ve  k  mushroom           2  4  6  8  10  12  14  % F  al se  P os  iti ve  k  vote          2  4  6  8  10  12  14  % F  al se  P os  iti ve  k  soybean           2  4  6  8  10  12  % F  al se  P os  iti ve  k  primary-tumor  Fig. 2. Results on a sample of datasets by varying k  on the mining process. The parameter k controls the size of the itemsets we process in each run. We fix the expected support number of the different datasets (e.g., 10 for the mushroom dataset and 0.1 for the other datasets), and vary the lenght of  the itemset. The results are shown in Figure 2. As we can see, a small k leads to few false-positive itemsets. On the other hand, If k is large enough, our algorithm is prone to a high rate of false-positives.

So we can summarize the results of this section by saying that our approach:  ? allows us to cut as much useless itemsets by filtering out the infrequent pattern efficiently.

? guarantees to find all frequent patterns with no false negatives.

? is solver-independent, such that the appropriate SAT solving method can be used for the mining task.



VII. CONCLUSION In this paper, we proposed a constraint model for mining  frequent itemsets from uncertain data sets. Our method is based on iterative SAT algorithm for the enumeration of fre- quent pattern. Since managing the expected support condition can lead to an exponential formulation, our approach uses constraints relaxation to obtain upper bounds to the expected support of frequent patterns. More precisely, these bounds are computed based on the highest existential probability; this relaxation guarantees to find all frequent patterns with no false negatives. Experimental results show the benefits of our relaxed expected support in uncertain FIM.

As future work, we aim to exploit the heuristics of SAT solvers for the FIM problem to even improve the performance of our approach. For a better relaxation and in order to reduce the number of false positives, we also aim to partition the transactions to tighten upper bounds to the expected support.

In order to more improve the effectiveness of our framework, we intend to compare it to state-of-the-art algorithms.

