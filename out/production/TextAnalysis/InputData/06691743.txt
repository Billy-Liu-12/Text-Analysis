Evaluating Parallel Logistic Regression Models

Abstract?Logistic regression (LR) has been widely used in applications of machine learning, thanks to its linear model.

However, when the size of training data is very large, even such a linear model can consume excessive memory and computation time. To tackle both resource and computation scalability in a big-data setting, we evaluate and compare different approaches in distributed platform, parallel algorithm, and sublinear approximation. Our empirical study provides design guidelines for choosing the most effective combination for the performance requirement of a given application.

Keywords-Logistic Regression Model; Parallel Computing; Sublinear Method; Big Data;

I. INTRODUCTION  The logistic regression model [13] plays a pivotal role in machine learning tasks. The model is suited for classifi- cation problems and is supported by a substantial body of statistical theories. A binary classification problem modeled by LR can be easily extended to a multi-class classification problem. We focus our study on the binary LR model in this paper (and the conclusions can be extended to a multi-class setting).

In recent years, many modern datasets have grown dras- tically in both data volume and data dimensionality. Large data volume and high data dimensionality bring both re- source and computational challenges to machine learning algorithms. For example, a social networking site such as Facebook consists of tens of millions of records, each is composed of hundreds of attributes. For text and multimedia categorization, we usually have to deal with billion-scale datasets in a feature space of thousands of dimensions. In this work, we evaluate approaches to scale up LR in a big-data setting. The approaches we evaluate include three aspects: 1) distributed platform, 2) parallel algorithm, and 3) sublinear approximation.

In the platform aspect, we compare two well known distributed systems: Hadoop [22] and Spark [23]. Hadoop employs HDFS [2] and MapReduce [8]. Spark promotes the efficiency of iterative algorithms and also supports HDFS.

In the parallel algorithm aspect, we compare the sequential optimization algorithms: stochastic gradient descent [24], which can obtain optimal generalization guarantees with a small number of passes over the data. The algorithm can easily be parallelized on either Hadoop or Spark, and it  can also be used in an online setting, at which a data instance is seen only once in a streaming fashion. Finally, to further speed up computation, we compare aforementioned platform/algorithm combinations with our previously pro- posed sublinear algorithms [20]. These sublinear algorithms access a single feature of a feature vector instead of all features at each iteration to reduce computation time. Our evaluation and comparisons provide insights to facilitate an application designer for selecting a solution that best meets the performance requirement of the machine-learning task at hand.



II. RELATED WORK  We present related work in the two aspects: platform and algorithm.

A. Computing Platforms  The two distributed platforms that we work with have their own advantages in the implementation of machine learning algorithms. The Hadoop [22] platform allows for distributed processing of large datasets across clusters of computers us- ing simple programming models. It utilizes MapReduce [8] as its computational paradigm, which is easily parallelized.

Additionally, Hadoop provides a Distributed File System (HDFS). Both MapReduce and HDFS are designed to handle node failures in an automatic way, allowing Hadoop to support large clusters that are built on commodity hardware.

Spark [23] is a cluster computing system that aims to make data analysis fast. It supports the in-memory cluster computing. A job can load data into memory and query it repeatedly by creating and caching resilient distributed datasets (RDDs). Moreover, RDDs achieve fault tolerance through lineage: information about how RDDs are derived from other RDDs is stored in reliable storage, thus making RDDs easy to be rebuilt if a certain partition is lost. Spark can execute up to be two orders faster than Hadoop for iterative algorithms.

Table I compares different features between Hadoop and Spark platforms. Spark supports two types of operations on RDDs: Actions and Transformations (As and Ts). Actions include functions like count, collect and save. They usually return a result from input RDDs. Transformations include functions like map, filter and join. They normally build new       RDDs from other RDDs, which comply with the lineage rule. In the table, we use ?NFS? to denote ?Normal File System?.

Table I PLATFORM COMPARISON: HADOOP VS. SPARK  Hadoop Spark Computational Paradigm MapReduce As and Ts File System Supported HDFS HDFS and NFS  Design Concept Key-value Pairs RDD Fault Tolerance Technique Redundancy Lineage  It is worth mentioning that Apache Mahout [19] runs on Hadoop and is a scalable machine learning library.

Mahout integrates many important algorithms for clustering, classification and collaborative filtering in its package. It is highly optimized so that Mahout also achieves good performance for non-distributed algorithms. We use Mahout as the baseline in our experiments.

B. Sublinear Methods  Sublinear methods have recently been proposed by several researchers. Clarkson et al. [6] first presented the solution of approximation algorithms in sublinear time. The algorithm employs a novel sampling technique along with a new multiplicative update procedure. Hazan et al. [15] exploited the approach to speed up linear SVMs. Cotter et al. [7] extended the method to kernelized SVMs. In [14], Hazan et al. applied the sublinear approximation approach to solve linear regression with penalties. Garber and Hazan [11] also developed sublinear method with Semi-Definite Program- ming (SDP). Peng et al. [20] utilized the method in the LR model with penalties and developed sequential sublinear algorithms for both ?1-penalty and ?2-penalty.

C. Other Related Work  LR is fairly straightforward to be parallelized. Paralleliza- tion efforts on several machine learning algorithms such as SVMs [4], LDA [17], Spectral Clustering [5], and fre- quent itemset mining [16] require much more sophisticated sharding work to divide a computational task evenly onto distributed subtasks. For a comprehensive survey, please consult [3] and [1].



III. LOGISTIC REGRESSION MODEL AND SEQUENTIAL SUBLINEAR ALGORITHM  A. Logistic Regression Model  In this paper, we mainly concern the binary classification problem. We define the training dataset as X = {(xi, yi) : i = 1, . . . , n}, where xi ? Rd are input samples and yi ? {?1, 1} are the corresponding labels. For simplici- ty, we will use the notation X = [x1,x2, . . . ,xn]T and y = (y1, y2, . . . , yn)  T to represent the training dataset in this paper. To fit in the logistic regression model, the expected value of yi is given by  P (yi|xi) = 1 1 + exp(?yi(xTi w + b))  ? gi(yi),  where w = (w1, . . . , wd)T ? Rd is a regression vector and b ? R is an offset term.

The learning process aims to derive w and b by solving an optimization problem. It is a common practice to add penalties to LR model in order to avoid overfitting and make the optimization result more practical. Typically, for ?2-penalty, we solve the following optimization problem:  max w,b  { F (w, b|X )? ?  ?w?22  } . (1)  For ?1-penalty, we optimize the following criterion: max w,b  { F (w, b|X )? ??w?1  } . (2)  Here, we define F (w, b|X ) = ?ni=1 log gi(yi) in both (1) and (2). To be brief, we omit the derivation here.

B. Sequential Sublinear Algorithm  We use the following notations to define sequential sub- linear algorithm for penalized logistic regression. Function clip (?) is a projection function defined as clip (a, b) ? max (min (a, b) ,?b) , a, b ? R. Function sgn (?) is the sign function; namely, sgn (?) ? {?1, 0, 1}. g (?) is the logistic function g (x) = 1/(1 + e?x).

In Algorithm 1, we give the sequential sublinear approx- imation procedure for logistic regression. The algorithm is from our previous work [20]. Mathematical symbols we use here comply with definitions in Section III-A. Parameter ? controls learning rate, T determines iteration number, and ? defines approximation level. Details of the parameter initialization step are documented in [20].

Algorithm 1: Sub-Linear Logistic Regression (SLLR)  1: Input parameters: ?, ? or ?,X, Y 2: Initialize parameters: T, ?,u0,w1,q1, b1 3: Iterations: t = 1 ? T 4: pt ? qt/?qt?1 5: Choose it ? i with probability p(i) 6: coef ? yitg  (?yit ( wtTxit + bt  ))  7: ut ? ut?1 + coef? 2T  xit 8: ?t ? argmax???  ( ptT ?  ) , if input ? for ?2-penalty  9: ut ? soft-thresholding operations, if input ? for ?1-penalty 10: wt ? ut/max {1, ?ut?2} 11: bt ? sgn  ( ptTy  )  12: Choose jt ? j with probability wt (j)2/?wt?22 13: Iterations: i = 1 ? n 14: ? ? xi (jt) ?wt?22/wt (jt) + ?t (i) + yibt 15: ?? ? clip (?, 1/?) 16: qt+1 (i)? pt (i)  ( 1? ??? + ?2??2)  17: Output: w? = 1 T  ? t wt, b? =  T  ? t bt  Each iteration of the SLLR algorithm has two phases: stochastic primal update and stochastic dual update. Steps from line 4 to line 11 consist of the primal part while the dual part is composed of steps from line 12 to line 16. We give the sublinear algorithm in a unified way for ?2-penalty and ?1-penalty. If we are dealing with ?2-penalty, we ignore line 9 and accomplish the computation in line 8 by a simple     greedy algorithm. Here, ? represents a Euclidean space with conditions ? = {? ? Rn | ?i, 0 ? ?i ? 2, ???1 ? ?n}. If we are faced with ?1-penalty, we ignore line 8 and expand the procedure of line 9 as the following:  Procedure: Line 9 in Algorithm 1  Iterations: j = 1 ? d if uprevt (j) > 0 and ut (j) > 0  ut (j) = max (ut (j)? ?, 0) if uprevt (j) < 0 and ut (j) < 0  ut (j) = min (ut (j) + ?, 0) uprevt+1 ? ut  In this sequential mode, each iteration takes O(n + d) time, which is sublinear to the dataset size.



IV. PARALLEL SUBLINEAR LOGISTIC REGRESSION  In this section, we describe our parallel sublinear al- gorithms implemented on Hadoop and Spark, respectively.

We also formally introduce the traditional parallel gradient algorithm used in Spark and the online stochastic gradient descent method used in Mahout. In all pseudo-code, we follow symbol notations defined in Section III-A.

A. Parallel Sublinear Algorithms on Hadoop  We develop an algorithm to achieve sublinear performance for learning logistic regression parameters using the archi- tecture of MapReduce. The pseudo-code of Algorithm 2, together with auxiliary procedures, explains the critical parts of this algorithm.

Algorithm 2: PSUBPLR-MR  1: Input parameters: ?, ? or ?,X, Y, n, d 2: Initialize parameters: T, ?,u0,w1,q1, b1 3: Iterations: t = 1 ? T 4: wt ? storeInHdfsFile(?hdfs://paraw?).addToDistributedCache() 5: pt ? storeInHdfsFile(?hdfs://parap?).addToDistributedCache() 6: conf primal ? new Configuration() 7: job primal ? new MapReduce-Job(conf primal) 8: conf primal.passParameters(T, n, d, bt) 9: job primal.setInputPath(?...?) 10: job primal.setOutputPath(?tmp/primalt?) 11: job primal.run() 12: (wt+1, bt+1)?Primal-Update(wt, bt) 13: Choose jt ? j with probability wt+1 (j)2/?wt+1?22 14: wt+1 ? storeInHdfsFile(?hdfs://paraw?).addToDistributedCache() 15: conf dual ? new Configuration() 16: job dual ? new MapReduce-Job(conf dual) 17: conf dual.passParameters(d, jt, bt+1, ?) 18: job primal.setInputPath(?...?) 19: job dual.setOutputPath(?tmp/dualt?) 20: job dual.run() 21: pt+1 ?Dual-Update(pt) 22: Output: w? = 1  T  ? t wt, b? =  T  ? t bt  This parallel design generally follows the framework of sequential sublinear algorithm. It has two computational components in each iteration: the primal update part from lines 4 to 12 and the dual update part from lines 13 to 21.

Procedure: Primal-Map(inputfile)  1: Configuration.getParameters(T, n, d, bt) 2: wt ? readCachedHdfsFile(?paraw?) 3: pt ? readCachedHdfsFile(?parap?) 4: it ? parseRowIndx(inputfile) 5: xit ? parseRowVector(inputfile) 6: yit ? parseRowLabel(inputfile) 7: r ? random(seed) 8: if pt(it) > rn 9: tmp coef = pt(it)yitg  (?yit ( wtTxit + bt  ))  10: else 11: tmp coef = 0 12: Iterations: j = 1 ? d 13: Set key ? j 14: Set value? tmp coef?  2T xit (j)  15: Output(key, value)  Procedure: Primal-Reduce(key in, value in)  1: key out? key in 2: value out??for same key in value in 3: Output(key out, value out)  In the primal update part, there is a parallel implementa- tion segment from lines 4 to 11, and also an unavoidable sequential segment as on line 12. In the dual part, it is the same situation that steps from lines 14 to 20 employ parallel implementation, whereas the sequential bottleneck is from lines 13 to 21. As the primal part and the dual part do not exhibit any inter-dependency, they can be executed simultaneously in each computation iteration. This halves the computation time. This framework is shown in Fig. 1.

In the parallelization segment of the primal mapreduce job, we process all data instances in parallel. In the primal update step, we compute gradients from ?almost? all data instances and make the weighted average value according to vector p as the output gradient for update. The details of this algorithm design is shown in Procedure Primal-Map and Procedure Primal-Reduce. Here, we employ a randomization strategy when we compute those ?almost? all gradients.

From lines 7 to 8 in Procedure Primal-Map, all data instances are computed if r = 0. As the expectation value for pt(it) is 1n , we normally set r to range between zero to one.

Two more issues must be taken care of by the algorithm.

The first issue is parameter passing. It is critical to choose an efficient way to pass the updated parameters between iterations and even between different MapReduce jobs. The design of Hadoop requires passing parameters between  Figure 1. Parallel implementation flow chart for PSUBPLR-MR     Procedure: Primal-Update(wt, bt)  1: ?wt ? readFromHdfsFile(?tmp/primalt?) 2: wt+1 ? wt +?wt 3: wt+1 ? wt+1/max {1, ?wt+1?2} 4: bt+1 ? sgn  ( ptTy  )  Procedure: Dual-Map(inputfile)  1: Configuration.getParameters(d, jt, bt+1, ?) 2: wt+1 ? readCachedHdfsFile(?paraw?) 3: it ? parseRowIndx(inputfile) 4: xit ? parseRowVector(inputfile) 5: yit ? parseRowLabel(inputfile) 6: ? ? xit (jt) ?wt+1?22/wt+1 (jt) + yitbt+1 7: ?? ? clip (?, 1/?) 8: res? 1? ??? + ?2??2 9: key ? it 10: value? res 11: Output(key, value)  computation iterations through files. This IO step between iterations is clearly a bottleneck.

The second issue is that datasets are generally sparse when the data dimension is high. This characteristic makes us focus on dealing with data sparsity issue in our code.

Instead of naively writing the simple code in data intensive situations, we only store pairs of index and value in a data vector. This sparse format brings us great memory-use efficiency improvement.

B. Parallel Sublinear algorithms on Spark  The algorithm to solve sublinear learning for penalized logistic regression in Spark is shown below. In the pseudo- code of Algorithm 3, the procedure for Primal-Update and the procedure for Dual-Map are the same as those in Algorithm PSUBPLR-MR.

This parallel design is very similar to that of Algorithm PSUBPLR-MR. The most important difference is the cache() operation in line 3. To make it work in Spark, we follow the rule to construct an RDD for each data instance. Also to cater for data sparsity, the design is that every data value correspond to its individual index. And the index is also involved in the computation along with the value. We omit the changes for ?2-penalty and ?1-penalty here to make the algorithm easier to be understood.

In parallel mode, the primal update contains the update of wt, which takes O(n) time. The dual update contains the ?2-sampling process for the choice of jt in O(d) time, and the update of p in O(1) time. Altogether, each iteration takes O(n + d) time. Compared to the analysis of sequen- tial algorithm, parallelization does not necessarily change computational complexity. Theoretically, parallel sublinear algorithm can be two times faster than the sequential version as the time for both update procedures in each iteration is reduced to O(1). Moreover, by starting two separate MapReduce jobs in one iteration simultaneously, the running time can be reduced to O(max{n, d}).

Procedure: Dual-Update(pt)  1: var? readFromHdfsFile(?tmp/dualt?) 2: Iterations: j = 1 ? n 3: pt+1(j)? pt(j) ? var(j)  Algorithm 3: PSUBPLR-SPARK  1: Input parameters: ?, ? or ?,X, Y, n, d 2: Initialize parameters: T, ?,u0,w1,q1, b1 3: points ? spark.textFile(inputfile).map(parsePoint()).cache() 4: Iterations: t = 1 ? T 5: gradient ? points.map(( 1  1+e?y((w T t x)+b)  ? 1) ? y ? p[index]) .reduce(sum())  6: (wt+1, bt+1)?Primal-Update(wt, bt) 7: Choose jt ? j with probability wt+1 (j)2/?wt+1?22 8: pAdjust ? points.map(MW-Update()).reduce(copy()) 9: pt+1 ?Dual-Update(pt) 10: Output(w, b)  C. Parallel Gradient Descent in Spark  The parallel gradient descent method to solve LR in Spark is shown below.

Algorithm 4: PGDPLR-SPARK  1: Input parameters: ?, ? or ?,X, Y, n, d 2: Initialize parameters: T, ?,u0,w1,q1, b1 3: points ? spark.textFile(inputfile).map(parsePoint()).cache() 4: Iterations: t = 1 ? T 5: gradient ? points.map(( 1  1+e?y((w T t x)+b)  ? 1) ? y) .reduce(sum())  6: wt+1 = wt ? gradient ? x 7: b = b? gradient 8: Output(w, b)  The pseudo-code of Algorithm 4 shows that the algorithm is naturally parallelizable. We can take in all data in the same iteration and just compute the gradient in a MapReduce fashion. As for the cache() operation and RDD design for data sparsity, it is the same with Algorithm PSUBPLR- SPARK.

D. Online Stochastic Gradient Descent in Mahout  Though SGD is an inherently sequential algorithm, it is blazingly fast. Thus Mahout?s implementation can handle training sets of tens of millions of instances. The SGD system in Mahout is an online learning algorithm, implying that we can learn models in an incremental fashion instead of traditional batching. In addition, we can halt training when a model reaches target performance. Because the SGD algorithms need feature vectors of fixed length and it is very costly to build a dictionary ahead of time, most SGD applications use an encoding system to derive hashed feature vectors. We can create a RandomAccessSparseVector, and then use various feature encoders to progressively add features to this vector. In our implementation, we use Ran- domAccessSparseVector for data sparsity and the function call by OnlineLogisticRegression to train the LR classifier.

We also perform cross validation. However, to maintain consistency with other methods for a fair comparison, we write our own code to ensure the same cross validation scheme is applied to all algorithms being evaluated.



V. EXPERIMENTAL SETUP  This section presents the details of the dataset information and testing environment of our experiments.

A. Dataset Information  We choose five open datasets to run all six test programs.

Details are shown in Table II. Different from the simulated 2d dataset, the other four datasets are all sparse. We split each dataset into the training set and the testing set. We randomly repeat such split 20 times and our analysis is based on the average performance of 20 repetitions. In Table II, Density is computed as  Density(Dataset) = # of Nonzeros  # of all entries ,  which stems from [21]. Balance describes the binary distri- bution of labels. We compute it as following:  Balance(Dataset) = # of Positive Instances  # of Negative Instances .

These five datasets are carefully selected with an incremen- tal trend in size. The simulated 2d dataset represents toy data situations and serves for the initial test of correctness of classifiers. The 20NewsGroup dataset is best-known for the test of LR model, which has a balanced distribution between positive and negative data instances. It also shows the application towards topic classification. The Gisette dataset [12] is relatively larger, and feature vectors are less sparse. The ECUESpam dataset [9] is selected due to its imbalanced distribution between positive and negative data instances. It has a higher data dimensionality than the Gisette dataset. However, because of data sparsity, it has fewer nonzero values involved in the computation. Finally, the URL-Reputation dataset [18] contains millions of data instances and features. The raw data are stored in the SVM- light format, which has the volume of more than 2GB. It can be seen as a representative of massive datasets in the sense that it exceeds the scalability of Liblinear, which will be shown below.

B. Testing Environment  There are all together 6 test programs for comparison.

Online stochastic gradient descent method is run on Mahout in the sequential mode. Liblinear [10] is also a baseline test program we choose. It is sequential and outperforms many other programs for LR on a single machine. SLLR performs the sequential sublinear algorithm on a single machine.

PSUBPLR-MR performs the parallel sublinear algorithm on Hadoop. PGDPLR-SPARK is the test program for the  parallel gradient descent run on Spark. PSUBPLR-SPARK implements the parallel sublinear algorithm run on Spark.

We run our test programs on a six-node cluster, confiured as shown in Table III. This computing cluster is considered small in size. Nevertheless, past works (documented in [3] and [1]) in parallelization show that this configuration suffices to tell the trend of scalability. Our future work will use a larger cluster to run on much larger scale datasets in production settings to validate our conjecture.

Table III CLUSTER INFORMATION  CPU Model Intel Xeon E5-1410: 2.80GHz Number of node 6  Number of CPU per node 4 Cores, 8 Threads RAM per node 16G Disk per node 4T HDD  Interconnection Method Gigabyte Ethernet

VI. EXPERIMENTAL RESULTS  This section conducts analysis on experimental results.

We report and analyze results in 1) accuracy, 2) efficiency, 3) scalability and 4) robustness.

A. Results on Precision  The results of six test programs achieved on five datasets are shown in Table IV. These are the average results of  Table IV ACCURACY RESULTS. THE MEANINGS OF ABBREVIATIONS ARE AS FOLLOWS: 20-N-G, 20 NEWS GROUP; URL-R, URL-REPUTATION.

20-N-G Gisette ECUESpam URL-R Mahout 71.3% 91.5% 85.2% 91.5%  Liblinear 92.0% 97.4% 97.1% ? SLLR 91.5% 94.8% 92.3% 94.2%  PSUBPLR-MR 90.5% 94.6 91.7% 93.8% PGDPLR-SPARK 92.0% 97.0% 93.7% 96.0%  PSUBPLR-SPARK 90.5% 95.8% 91.7% 94.0%  running cross validation. Note that Liblinear cannot be implemented with the full URL-Reputation dataset on our machines due to memory limitation. In Figures 2 (a)-(d), we show the test error as a function of iteration number on each dataset for all six test programs.

B. Results on Running Time  The running time of six test programs used on five datasets is shown in Table V. The reported running time is the average of our corss-validation executions. Fig. 3 reports the same results graphically.

Observations  From the above results, we reach the following conclu- sions.

1) Liblinear performs best. It fully utilizes memory, and the single machine implementation does not require any communication between machines. However, its scalability is limited by the memory size of the single     Table II DATASETS  Name Dimension # of instances Density # of nonzero values Balance # of training # of testing 2d 2 200 1.0 400 1.000 ? ?  20NewsGroup 16428 1988 7.384? 10?3 238511 1.006 1800 188 Gisette 5000 7000 0.12998 4549319 1.000 6000 1000  ECUESpam 100249 10687 2.563? 10?3 2746159 5.882 9000 1687 URL-Reputation 3231961 2376130 3.608? 10?5 277058644 0.500 2356130 20000  2 4 6 8 100.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  Iteration Number  Te st  E rr  or Mahout Liblinear SLLR PSUBPLR?MR PGDPLR?SPARK PSUBPLR?SPARK  2 4 6 8 10 0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  0.18  Iteration Number  T e  s t  E r r o  r      Mahout  Liblinear  SLLR  PSUBPLR?MR  PGDPLR?SPARK  PSUBPLR?SPARK  10 20 30 40 50  0.05  0.1  0.15  0.2  0.25  0.3  Iteration Number  T e  s t  E r r o  r      Mahout  Liblinear  SLLR  PSUBPLR?MR  PGDPLR?SPARK  PSUBPLR?SPARK  10 20 30 40 50 0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  0.18  Iteration Number  T e  s t  E r r o  r      Mahout  Liblinear  SLLR  PSUBPLR?MR  PGDPLR?SPARK  PSUBPLR?SPARK  (a) 20NewsGroup. (b) Gisette. (c) ECUESpam. (d) URL-Reputation.

Figure 2. Test error, as a function of iteration number.

Table V RUNNING TIME. THE MEANINGS OF ABBREVIATIONS ARE AS FOLLOWS:  20-N-G, 20 NEWS GROUP; URL-R, URL-REPUTATION.

20-N-G Gisette ECUESpam URL-R  Mahout 9.83s 131.8s 96s 10100s Liblinear 0.79s 2.4s 13s ?  SLLR 20.05s 130.5s 1028s 3248s PSUBPLR-MR 1360.85s 3687.9s 11478s 16098s  PGDPLR-SPARK 10.52s 99.2s 924s 3615s PSUBPLR-SPARK 8.57s 89.1s 796s 2918s  2d 20NewsGroup Gisette ECUESpam URL?Reputation           Algorithms  R u  n n  in g  T im  e /s      Mahout  Liblinear  SLLR  PSUBPLR?MR  PGDPLR?SPARK  PSUBPLR?SPARK  Figure 3. Running time.

machine that it runs on. Nevertheless, if the dataset can be fully loaded into the memory of a single machine, this efficient and direct sequential way of implementation is highly recommended for both fast speed and high accuracy.

2) Mahout?s precision is not good, especially on those datasets where positive and negative instances are imbalanced. However, it is a representative example of sequential algorithm that can train massive data in acceptable time. We monitor the memory when running Mahout and find that its memory use is much  lower than Liblinear. This is the advantage brought by both online algorithm and hashing operations on features. Therefore, Mahout offers good scalability guarantee. When only single machine with limited memory is available, sequential online training like Mahout is recommended.

3) The precision of all sublinear methods is acceptable.

The developed parallel sublinear algorithm only has a small drop in precision.

4) Hadoop system suffers from a drawback for running LR optimization methods. Its cluster programming model is based on an acyclic data flow from a stable storage to a stable storage. Though it has the benefits of deciding where to run tasks and can automatically recover from failures in runtime, the acyclic data flow is inefficient for iterative algorithms. All six test programs involve a number of iterations, thus making PSUBPLR-MR perform poorly, worse than sequential algorithms. When we study the details of Hadoop implementation, we find that the starting time of MapReduce job in PSUBPLR-MR is about 20s, including time for task configuration and parameter passing. This running time overhead is not negligible, especially when a dataset is relatively small. We also identify that the running of ?Primal-Map? dominates one iteration time (more than 66%). It is the same situation for PSUBPLR-SPARK.

5) Spark employs the ?all-in-memory? strategy, and it constructs RDD on demand. We implement PGDPLR- SPARK and PSUBPLR-SPARK in the normal file system instead of HDFS. Current results on Spark show that it is much more efficient than Hadoop, and performs better than Mahout. As a distributed platform, we recommend Spark to process massive     data. Further, we recommend to choose PSUBPLR- SPARK for shorter running time in exchange for a slight precision degradation. As the Spark platform is still under development, we expect that our running time results for PGDPLR-SPARK and PSUBPLR- SPARK can still be improved.

6) Another interesting point we would like to raise is about dataset size versus sparsity. When comparing ECUESpam dataset and GISETTE dataset, the for- mer has higher data dimensionality but it is more sparse. Comparing the data point between Figures 4 (b) and (c), we can find that algorithms on ECUES- pam dataset enjoy shorter running time per iteration than on GISETTE dataset as ECUESpam has fewer nonzero values involved in the computation. However, algorithms on ECUESpam dataset require more iter- ations than on GISETTE dataset, which is because of higher data dimensionality of ECUESpam dataset.

This high dimensionality even causes ECUESpam to take longer execution time in total. In summary, to get a general sense of running time for a dataset, we must consider both data volume and sparsity.

C. Results on Cluster Size  In Figures 4 (a)-(d), we show the running time as a function of number of nodes. It is evident, as prior work (e.g., [4]), has pointed out, that due to IO and communi- cation overheads, the benefit of adding more nodes would eventually be wahed out. Though we have only employed six nodes in this study, the figures already indicate speedup slows down when more nodes are used. Therefore, to deal with big data, future work should pay more attention on reducing IO and communication overheads.

D. Fault Tolerance  There are both system level techniques and algorithm level techniques to provide fault tolerance in parallel computation.

Results are shown in Table VI. Here, randomization is  Table VI FAULT TOLERANCE ANALYSIS  System Level Algorithm Level PSUBPLR-MR ? ?  PGDPLR-SPARK ? ? PSUBPLR-SPARK ? ?  employed in all sublinear methods, thus providing algorithm level fault tolerance for PSUBPLR-MR and PSUBPLR- SPARK. Hadoop and Spark can both provide system level fault tolerance, but in different ways. Hadoop employs HDFS, whereas Spark has RDDs. Of all six test programs, PSUBPLR-MR and PSUBPLR-SPARK are fault-tolerant in both system level and algorithm level.

Fig. 5 shows the iteration time as a function of percentage of failed maps on URL-Reputation dataset for all three  PSUBPLR?MR PGDPLR?SPARK PSUBPLR?SPARK           Number of Failed Maps  It e r a ti  o n  T im  e (a  ft e r f  a il u  r e s )/  s      1%  5%  10%  15%  30%  Figure 5. Iteration time, as a function of percentage of failed maps, on URL-Reputation Dataset, run on 6 nodes  parallel test programs running on six nodes. The number of iterations of PSUBPLR-MR remains unaffected when maps fail. Both PGDPLR-SPARK and PSUBPLR-SPARK increase number of iterations when maps fail, because of RDD reconstruction. However, the increase is not significant.

In general, Hadoop spends more overhead than Spark to support fault tolerance, and hence Hadoop enjoys less impact during failure recovery.



VII. CONCLUSION  In this paper we analyzed three optimization approaches along with two computing platforms to train the LR model on large-scale, high-dimensional datasets for classification.

Based on extensive experiments, we summarized key fea- tures of each algorithm implemented on Hadoop and Spark.

We can conclude that sequential algorithms with memory intensive operations like Liblinear can perform very well if datasets can fit in memory. For massive datasets, if limited by machine resources, Mahout with its online algorithm is a good choice with a slightly lower precision. If machine resources are abundant, as Spark outperforms Hadoop for LR model training, we recommend choosing between par- allel sublinear method and parallel gradient descent method (both on Spark) to trade off between speedup and precision.

Though we used only a six-node cluster to conduct experiments, our conclusions are expected to hold for larger datasets and more computing nodes. We will validate this conjecture when we evaluate these algorithms in a produc- tion setting with a substantial larger cluster to deal with much larger application datasets.

