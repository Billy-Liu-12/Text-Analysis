A Method of Improvement and Optimization on  Association Rules Apriori Algorithm

Abstract?The efficiency of mining association rules is an important field of Knowledge Discovery in Databases. The algorithm Apriori is a classical algorithm in mining association rules. A novel procedure was proposed to delete many transactions which need not be scanned repeatedly. The procedure described in this paper reduced the number of database passes to extract frequent itemsets. A  method was showed to reduce the number of candidate itemsets by optimizing the join procedure of frequent itemsets. To this end, the IApriori algorithm for mining frequent itemsets, which is the improvement algorithm of Apriori, is designed in this article. By a number  of experiments, the proposed algorithm outperforms the Apriori algorithm in computational time. The simulation results of knowledge acquisition for fault diagnosis also show the validity of IApriori algorithm.

Keywords?data mining, association rules, Apriori algorithm, frequent itemsets  Apriori   Apriori  Apriori   [1-3]  2 Apriori  Agrawal Apriori [4]    973 2002CB3122000 863 2003AA412010   Proceedings of the 6th World Congress on Intelligent Control and Automation, June 21 - 23, 2006, Dalian, China    Apriori  2.1 item itemset  K C K K  L K I I1 I2 I3 nI nI item  D T1 T2 T3 mT  mT transaction  mT I? A B? A I? B I? A B = ?  1 2 1 2... ...c dA A A B B B? ( )1,2,...,cA c n= ( )1, 2,...,dB d n= I  A B? D support D A B  support A B? support A B? ( )P A B A B? D confidence D A B A confidence A B? ( )P A B  D  minSupport minConfidence  3 Apriori Apriori  C 1 minSupport L 1  L K C K 1 C K 1 2 L K  C K 1 minSupport L K 1  L K 1   L K C K 1  Apriori [5-9]  D T  10 C K  C K T T 11  C K T C K minSupport  T C K 1 T  10 11 Apriori IApriori  D T item K 1 C K C  K 1 T 11 Apriori  D 9 2  D C1 L1 TID Items ItemSet Support ItemSet Support T1 I1,I2,I5 {I1} 4 {I1} 4 T2 I1,I4 {I2} 5 {I2} 5 T3 I3,I5 {I3} 5 {I3} 5 T4 I2,I4 {I4} 4 {I4} 4 T5 I3,I4 {I5} 4 {I5} 4 T6 I2,I3 T7 I4,I5 C2 T8 I1,I2,I3,I5 ItemSet Support T9 I1,I2,I3 {I1,I2}  {I1,I3} L2 {I1,I4}  ItemSet Support D {I1,I5} {I1,I2} TID Items {I2,I3} {I1,I3} T1 I1,I2,I5 {I2,I4} {I1,I5} T3 I3,I5 {I2,I5} {I2,I3} T6 I2,I3 {I3,I4} {I2,I5} T8 I1,I2,I3,I5 {I3,I5} {I3,I5} T9 I1,I2,I3 {I4,I5}  C3 ItemSet Support D  {I1,I2,I3} TID Items L3 {I1,I2,I5} T1 I1,I2,I5 ItemSet Support {I1,I3,I5} T8 I1,I2,I3,I5 {I1,I2,I3} {I2,I3,I5} T9 I1,I2,I3 {I1,I2,I5}  1 IApriori     1 IApriori D C 3  T3 T6 item 3 T3 T6 D T1 T8 T9  11 C 3 L 3 D T1 T3 T6 T8 T9  IApriori  IApriori C K  K L K C K 1 L K  K 1 K C K 1  Apriori 2 L 1 I1 I2 I3 I4 I5 L 1 C 2 Apriori 5 5 25  IApriori 4 3 2 1 10 L 2 I1 I2 I1 I3 I1 I5 I2 I3  I2 I5 I3 I5 L 2 C 3 Apriori 6 6 36 IApriori 5 4 3 2 1 15  D item ASCII C K  IApriori C 1  minSupport L 1 K 1  L K C K 1 M L K L K N L K N 1  N 1 2 3 M 1 K  D C K 1  T item K 1 T  T C K minSupport T  minSupport  L K 1 L K 1 K  K 1  Input:dataset,minSupport,minConfidence Output:L(K) Method: // InstantiateGlobals(); // C(1) L(1) BuildL1AndC1(); // C(K) L(K)(K>=2) AprioriGenerate(L); HasInFrequentSubset(subset,L); // GenerateAssociationRules();  // void AprioriGenerate(L itemSet) { // for (int m3 = 0; m3 <= TidtotalRows; m3++) { int m5 = 0; for (int m2 = 0; m2 < C[K].cnum; m2++) { // T K 1 if ((K+1) > Tid[m3].tnum) { // T break; } for (int m4 = m5; m4 < Tid[m3].tnum; m4++) { // if(C[K].value[m2] == Tid[m3].value[m4]) { count = count + 1; // m5 = m4 + 1; break; } else     count = count; } } // for (int m6 = 0; m6 <= CtotalRows; m6++) { for (int m7 = 0; m7 <= TidtotalRows; m7++) { // C(K) T  minSupport if (C[K].count < minSupport) and (C[K].cnum  == Tid[m7].tnum) { // T Tid[m7].tnum = 0; } } } } }  4 IApriori IApriori  11 DDApriori Apriori  2 3 Windows XP Professional CPU  Intel pentium 1.6GHZ DDR 256M C Visual C  6.0  12 Census Database  IApriori Apriori    5 IApriori  1 13      TID Items 100 101,1,4,5 200 101,15,17 300 101,4,5 400 101,17,28,29 500 101,4,5 600 101,17,28 700 101,5,17,24 800 101,4,5,15 900 101,4,17,24 1000 101,5,17,24  8800 108,16,38,39 8900 108,20,39,40 9000 108,10,16,20,40  3 60      4 => 101 confidence = 100% 5 => 101 confidence = 100% 17 => 101 confidence = 100% 4 AND 5 => 101 confidence = 100% 11 => 102 confidence = 70% 14 => 102 confidence = 100% 18 => 102 confidence = 93.33% 11 AND 14 => 102 confidence = 100% 11 AND 18 => 102 confidence = 100% 14 AND 18 => 102 confidence = 100% 18 AND 6 => 102 confidence = 100% 11 AND 18 AND 6 => 102 confidence = 100% 10 => 104 confidence = 60% 10 AND 20 => 104 confidence = 63.63% 12 => 105 confidence = 100% 19 => 105 confidence = 80% 32 => 105 confidence = 100% 12 AND 32 => 105 confidence = 100% 8 => 106 confidence = 83.33% 26 => 106 confidence = 100% 27 => 106 confidence = 100% 7 => 107 confidence = 100% 13 AND 7 => 107 confidence = 100% 13 AND 23 => 107 confidence = 69.23% 23 AND 7 => 107 confidence = 100% 23 AND 25 => 107 confidence = 100% 13 AND 23 AND 7 => 107 confidence = 100% 39 => 108 confidence = 100%   Apriori  Apriori Apriori  IApriori Apriori  [1]  . [M], : , 2002:  100~128.

[2]  , . [M]. :  , 2003, 152~157.

[3]  , . [M], :  , 2003.

[4]  Agrawal R, Imielinski T, Swami A. Mining association rules  between sets of items in large databases. In: Bunemuu P, Jajodia S eds. Proceedings of the 1993 ACM SIGMOD Conference on  Management of Data. New York, NY: ACM Press, 1993. pp.

207~216.

[5]  . Apriori [J].

, 2003, 19(3): 196~198.

[6]  Agrawal R, Skikant R. Fast algorithms for mining association rules in large databases[C]. In Proceeding of the 20th  Internationa1 Conference on Very Large DataBases, Santiago, Chile, 1994. pp. 487~499.

[7]  , . Apriori [J].

, 2002, 28(7): 104~105.

[8]  , . [J].

( ), 2001, 19(3): 22~26.

[9]  . Apriori [J].

, 2003, 29(19): 83~84.

[10] , , . [J].

, 1999, 4(2): 36~38.

[11] . Apriori [J].

, 2004, 21(11): 82~84.

[12] ftp://ftp.ics.uci.edu/pub/machine-learning-databases [13] , . [J].

, 2004, 11(5): 406~408.

