A New Algorithm for Frequent Itemset Generation in Non - Binary Search  Space

Abstract Association rule induction is a powerful data mining method, used to analyze the regularities in data trends by finding the frequent itemset and association between items or set of items. Several research attempts have been done for the purpose based on the binary data space. In this paper an algorithm has been proposed for the same purpose but based on the non- binary data space. The algorithm is capable to generate the frequent itemset more close to the real life situations as it consider the Strength of Presence of each items implicitly. Also the algorithm can be directly applicable to the real time data repository for finding the frequent itemset.

Keywords: Frequent itemset, Association rule, Data mining, Apriori algorithm.

1. Introduction  In recent days, the nature of database applications and automated data collection tools lead to tremendous amounts of data stored in operational databases, data warehouses, and other information repositories. These large amounts of data are worthless unless they become knowledge or information using the concept of Knowledge Discovery in Databases (KDD) [1, 2]. Data mining is a concept used hugely now days for the purpose of data explosion and knowledge discovery from the large dataset. Further, the concept of Data mining has recently attracted considerable attention from database practitioners and researchers because of its applicability in many areas such as decision support, market strategy and financial forecasts.

One of the common data mining techniques, Association rule induction is a powerful method used to find regularities in data trends [3]. It has two  different phases; first is to find the frequent itemsets and the second is to find the rules from these itemsets.

An association rule expresses an association between items or sets of items. Finding the above association rule is valuable for cross ? marketing analysis, attached mailing applications, store layout, and customer segmentation based on buying patterns etc.

The algorithms for mining frequent patterns can be classified into two approaches, namely, candidate generation and pattern-growth. The representative algorithm of the first approach is the Apriori algorithm and Apriori like algorithms [4]. Apriori algorithm traverses the Boolean search space in a pure breadth- first manner and finds support information by explicitly generating and counting each node. Another approach is FP-growth algorithm [5]. FP-growth algorithm uses prefix-tree structure to mine frequent itemsets without generating candidates and scans the database only twice. Some other approach like Dynamic Itemset Counting (DIC) [6] algorithm works on the principle of early candidate generation. Recently many other algorithms were developed for mining frequent patterns [7, 8, 9]. But most of them are an enhancement of Apriori Algorithm or FP-growth algorithm and use the technique for mining frequent itemsets in binary search space.

Though wide variety of approaches for mining frequent itemset have been proposed but, none of these solutions have clean bounds for mining frequent items from non-binary data sets. Binary dataset representation gives information about an item being present or not in the search space, but does not provide any information about the strength of its presence which can be more effective in drawing association rules close to real life situations. As an example, 100 units of Item X, 10 units of item Y and 100 units of Item X, 1000 units of item Y could not be differentiated in Binary Search Space. Hence, it cannot model real time situations as it cannot provide any clue   DOI 10.1109/ITNG.2009.36    DOI 10.1109/ITNG.2009.36     regarding the strength of presence of items being processed.

The algorithm proposed in this paper, can mine frequent itemset and can draw association rules from non-binary datasets. The proposed algorithm can perform the mining task on non-binary search space and can yield effective set of association rules directly from the real time data repository. Further, the Strength of Presence may provide new incites for generating most accurate frequent itemset, which are close to real life situation. The rest of this paper is organized as follows. To make the paper self sustained, an overview of Apriori Algorithm is discussed in Section 2. In Section 3 we present the proposed NB Frequent Itemset algorithm with detail example. Conclusion and future works are mentioned in Section 4.

2. Overview of Apriori Algorithm  Apriori is the first algorithm that pioneered the use of support-based pruning to exponential growth of candidate itemsets [4] with systematic control. The data is assumed to be represented as binary dataset, where each row corresponds to a transaction and each column corresponds to an item. If any item belongs to a particular transaction, then its corresponding entry is equal to 1 otherwise, it is denoted as 0. The support for a set of items is the number of transactions in which all the attribute values is 1.

The Apriori principle states, If an itemset is frequent, then all of its subsets must also be frequent.

The algorithm is based on the above principle and it iterates over two phases, the phase of candidate generation and the phase of verification, at each level.

Since, Apriori is a level-wise algorithm and it generates frequent itemsets one level at-a-time, from itemsets of level-1 to the longest frequent itemsets. At each level, new candidate itemsets are created by using frequent itemsets discovered at the previous level. At each level, the transaction database is scanned once to determine the actual support count of every candidate itemset. Also, Pruning in Apriori algorithm is essential for the removal of unwanted itemsets and is of prime importance in binary search space.

3. Proposed NB Frequent Itemset Algorithm  Let P = {p1, p2 . . . pm } be the set of items. Let T = {t1, t2 . . . tn} be the set of transactions where each transaction ti is a set of items such that ti ?T. Each transaction ti contains a sub-set of items that has been processed and can be represented as a vector,  mimiii pwpwpwt ?......??? 2211 +++= .................. ( 1 )   Where, wij implies Weight or Strength of Presence of item pj, which has been processed in transaction ti. Also, weight is non-binary real value where the item has processed and is zero where it has not processed.

Now, at level k, each itemset can be considered as vector which is a combination of k number of items from any transaction and can be expressed as a vector ISK,   kikiik pwpwpwSI ?......??? 2211 +++= , where k < m                              .................. ( 2 )    Mining Frequent Itemsets over non-binary search space presents the interesting new challenges over traditional mining in binary search space. Firstly, the non-binary search space requires new approaches to calculate support and it has to be dynamic in nature as we are dealing with real time dataset. Secondly, pruning cannot be applied to non-binary dataset as it may eliminate candidate itemset which at higher level may become frequent. For example, let the itemset {sugar, milk} is frequent, {milk, tea} is also frequent but {sugar, tea} is not frequent. Now, if we prune this itemset, {sugar, milk, tea} will be considered as a non- frequent itemset as one of its subset is not frequent.

But in real time, it can be a frequent itemset with the associated strength of presence of items. Hence, we need to consider separate mechanism for support calculation and candidate generation at each level to mine the association rules from non-binary search space.

3.1. Support Calculation for Non-binary Search Space  The non-binary weight is used to quantify the Strength of Presence of items in transaction vector as well as in Itemset vector in some level. An example of non-binary data set is shown in Table 1, where p1, p2, p3 and p4 are the items; t1, t2 ,t3 ,t4 and t5 are the transactions on the items and the corresponding cell value is representing the respective wij.

The support calculation for the proposed work is divided into two parts, (i) the Dynamic Support (ML) for level L itemsets is calculated using two factors, firstly the average of scalar distances of the Itemset Vectors from the origin in the L-dimensional space, which corresponds to the level L and  secondly standard deviation (SD) of all such distances. The dynamic support needs to be calculated for all possible     itemset vectors of corresponding level. For a specific itemset of some level, dynamic support will yield the set of transactions which have interested strength of presence of the items corresponding to that level (ii) the Static Support is the user defined threshold value which is minimum count of transactions that should be supported by any itemset at any level after calculating dynamic support. The sequence of dynamic support and static support calculation will be used to generate the Frequent Itemset in some given level.

Table 1. Non-Binary Data Set T p1 p2 p3 P4 t1 10 5 3 8 t2 20 10 6 7 t3 30 15 9 11 t4 40 20 12 23 t5 50 25 15 35   Let, the distance distL[i] is distance of itemset vector ISL correspond to ith transactions and is calculated by using Equation-3 considering the origin as a reference.

Also let SD is standard deviation of all such distances.

Now the dynamic support ML is the range defined by the boundary [average scalar distance - SD, average scalar distance+SD]. All transactions involving with the itemset ISL and having interested strength of presence of the items corresponding to level L should be within the range defined by ML. Further, ML can be represented as Equation-4 and given as follows,    1 ......][ iLiiL wwwidist +++=   .......... ( 3 )  SDnidistM n  i LL ?= ?  =  /)][(  .................... ( 4 )   Table 2. Distance distL[i] calculation correspond to  level 3 T p1 p2 P3 distL[i]  T1 10 5 3 11.576 T2 20 10 6 23.152 T3 30 15 9 34.727 T4 40 20 12 46.303 T5 50 25 15 57.879   An example of distance distL[i] calculation correspond to level 3 of Table ? 1 dataset, has been shown in Table ? 2. The SD of all distances is equal to 16.371. The Dynamic Support at Level 3 for the itemset {p1, p2, p3} is M3 and is the range [18.356, 51.097] using Equation 4. Now, the count of the number of transactions, which have their distL[i] within ML,  has been shown in Table ? 3.

Table 3. Dynamic Support applied to Table 2  Itemset Count Corresponding  transactions and Weights of Supported Itemset  {p1, p2, p3}  3 t2={20,10,6}, t3={30,15,9}, t4={40,20,12}   Now if Static Support is 3, then we can conclude that the Itemset {p1, p2, p3} is Frequent Itemset, which has interested strength of presence of the corresponding items in the non-binary search space.

3.2. Candidate Generation  The candidates generated at any level L cannot be restricted to combining the items of the frequent itemsets obtained at the previous level. Unlike binary search space, in non-binary search space an itemset obtained may surprisingly be frequent even if its subsets are not frequent. Hence, a new approach is required to generate candidate itemsets.

Initially, in level 1, all the items in the non-binary data set will be treated as individual candidate. So each candidate itemset in level 1 have cardinality one. In order to generate the candidate itemsets at level 2, all possible distinct unordered combination of items with cardinality two need to be considered, where the items belong to set P.  Subsequently, to generate candidates at each higher level, all possible distinct unordered combination of items with cardinality equal to the level need to be considered. Usually, the candidate itemset of level k are generated from the candidate itemset of level k-1.

3.3 The Algorithm  The NB Frequent Itemset algorithm basically uses two functions, (i) Candidate_Itemsets, to generate candidate itemsets at each level and (ii) Check_Dynamic_Support, used to calculate the dynamic support for any itemset corresponding to some level. The algorithm proceeds from level 1 to level N, where N is the total number of items in the non-binary search space.

The main function NB_Frequent_Itemset is used to generate the frequent itemsets at each level from the non-binary search space using the functions Candidate_Itemset and Check_Dynamic_Support.

//k is level identification number, Ck will store all possible itemsets at level k and Lk will store the frequent itemsets at level k.

Function NB_Frequent_Itemset()     Initialize: k:=1, n= Number of Items, C1 = all the Level 1 itemsets, Static Support; Compute the dynamic support on each itemset of   C1 and use the static support determine L1; L1 := {All frequent itemsets of Level 1}; k :=2; while(k  ? n) begin Ck  = Candiate_Itemsets() with the given Ck-1; for each Itemset x belongs to level k do  Check_Dynamic_Support(x, k) and then check x using static supports;  Lk := {All Frequent Itemsets of Level k}; k =: k+1; end Answer := ?k Lk  End Function  The function Candidate_Itemset() is used to generate all possible unique candidate itemsets at each level. To generate itemsets at any level k, it uses the candidate itemsets generated at level k-1.

Function Candidate_itemsets() Ck  = ?;  for all itemsets l1 ? Ck-1 do for all itemsets l2  ? Ck-1 do  if  l1[1]=l2[2] ? l1[2]=l2[3] ? ... ? l1[k-1] <l2[k- 1]  then c =  l1[1],l2[2] ..., l1[k-1]=l2[k-1] Ck = Ck   ? {c}  End Function  The function Check_dyanmic_support() takes an itemset as input and produces the corresponding distance, mean and standard deviation.

Function Check_Dynamic_Support (itemset x, Level L) // ML will yield the range of dynamic support using //  Equation (4)   Compute distL[i] for each transaction i corresponding to itemset x at level L; Compute the Standard deviation SD of all distL[i] corresponding to itemset x; Compute ML at level L; for all i ? dist[i] do check if dist[i] is in the range of ML;  End Function  3.4. Example   As proposed in the algorithm, mining non-binary search space with the concept of weight or strength of presence is useful to generate the most accurate frequent itemsets from the given dataset and directly applicable to the real time data repository. For example, an Enterprise may map Product as an Item and a purchase session by some customer as a transaction to generate the frequent itemset using the proposed algorithm. Now in the non-binary search space each transaction represents the set of weight values of the respective products purchased. The non- binary data set can be represented using the transaction-product matrix [Table-4].

Let, the Static Support is set to 6. Now, at Level ? 1, the algorithm will read the dataset to compute the dynamic support [Table ? 5] and then use the static support to determine the set of frequent itemset corresponding to the Level ? 1 and which is, L1 := {{p3}, {p4}, {p5}};   Table 4. Non-Binary Dataset  T p1 P2 p3 p4 p5 T1 10.05 5.00 399 2.90 10.10 T2 22.00 377.00 26.00 40.00 15.00 t3 30.90 0.00 91.00 6.99 11.00 t4 41.00 2.00 0.00 8.00 8.00 t5 50.57 254.00 150.00 0.00 10.75 t6 11.15 78.66 23.75 90.87 11.05 T7 1.00 0.00 0.00 429.33 17.00   Table 5. Candidate Itemsets of Level 1  Itemset Dynamic Support Count {p1} [7.033, 40.587] 4 {p2} [0, 243.551] 5 {p3} [0, 231.307] 6 {p4} [0, 241.111] 6 {p5} [6.476, 17.21] 7   Table 6. Candidate Itemsets of Level 2  Itemset Dynamic support Count {p1,p2} [0, 136.68] 5 {p1,p3} [0, 127.486] 6 {p1,p4} [10.752, 190.164] 6 {p1,p5} [0, 120.576] 5 {p2,p3} [5.372, 179.592] 2 {p2,p4} [9.461, 26.191] 2 {p2,p5} [13.584,167.536] 5 {p3,p4} [0, 128.205] 4 {p3,p5} [0, 121.158] 6 {p4,p5} [0, 120.732] 6   Now, at Level ? 2, the function NB_candidate_itemsets() is called and candidates itemsets are generated. Next dataset is read to compute the dynamic support [Table-6] and then the static support is used to determine the set of frequent itemset     corresponding to the Level ? 2. The candidate item set at Level ? 2 is, C2:{{p1,p2}, {p1,p3}, {p1,p4}, {p1,p5}, {p2,p3}, {p2,p4}, {p2,p5}, {p3,p4}, {p3,p5}, {p4,p5}}  After eliminating the elements with static count we have the frequent itemset at level ? 2 as,  L2 := {{p1,p3},{p1,p4}{p3,p5},{p4,p5}}  The algorithm works in the same fashion for Level - 3, Level -4 and so on till the level identification is equal to the number of products and will generate frequent itemsets corresponding to each level as,  L3 := {{p1,p3,p5}, {p1,p4,p5}}; L4 := ?; L5 := ?.

Since, after the iteration equal to the number of products in the dataset. the algorithm stops. Hence the frequent itemsets obtained for the entire dataset is,  L := L1 ?  L2 ?  L3 ?  L4 ?  L5 := {{p3}, {p4}, {p5}, {p1,p3}, {p1,p4}, {p3,p5}, {p4,p5}, {p1,p3,p5}, {p1,p4,p5}}  3.5. Discussions  The NB Frequent Itemset algorithm essentially removes the restriction of mining data associated with binary dataset. Also it is interesting to see that the in example, using the non-binary dataspace, though the itemset {p1} is not frequent but several other itemset with the item p1 is frequent. This is possible as in the proposed algorithm we are considering the Strength of Presence of each item to generate the more frequent itemset. As the proposed algorithm is using non-binary search space, it can be directly applicable to the data repository for the purpose of association rule mining.

Also the mechanism will be effective to yield most accurate frequent itemset and association rules.  The algorithm will be executed till the number of iterations equal to the number of items in the dataset. However, the time complexity will be O(n2m), where m is the number of items and n is the number of transactions in the given non-binary dataset.

4. Conclusions and Future Research  In real life data analysis, predicting the frequent itemset with quantitative measure is a major concern, which can be the first step towards mining the association rules with broader objectives i.e. rules with quantitative measures of items. The proposed NB Frequent Itemset algorithm in this paper focuses upon  mining association rules from non-binary dataset, with the consideration of said quantitative measure. The consideration of Strength of Presence or Weight of each item while generating the frequent itemset is not only effective  to yield most accurate association rules but also advantageous as it can be directly applicable to the real time data repository. The methodology used for this algorithm, also lead to a better way of calculating Support.

Further studies will concentrate on testing the algorithm on real environment in order to prove its practical utility and to extending the algorithm for non- binary dataset related to hierarchical items.

5. References  [1] Piatestsky-Shapiro G., ?Knowledge discovery in databases?, AAI/MIT Press, 1991.

[2] Fayyad UM, Piatetsky-Shapiro G, Smyth P, Uthurusamy R., ?Advances in knowledge discovery and data mining?, AAAI Press, 1996.

[3] Agrawal R, Imielinski T, Swami A., ?Mining association rules between sets of items in large databases?, Intl. Conf. on Management of Data (Proc. ACM SIGMOD), Washington, DC,  1993, pp. 207-216.

[4] R. Agrawal, R. Srikant, ?Fast algorithms for mining association rules?, Proceedings of the 20th Very Large DataBases Conference (VLDB?94), Santiago, Chile, 1994, pp. 487? 499.

[5] J. Han, J. Pei, Y. Yin, ?Mining frequent patterns without candidate generation?,  Proceedings of the 2000 ACM New York, NY, USA, 2000, pp. 1?12.

[6] S. Brin, R. Motawani, J.D. Ullman and S. Tsur , ?Dynamic Item set counting and implication rules for market basket data?  in Proc. of the ACM SIGMOD International Conference On Management of Data, 1997, pp. 255 ? 264.

[7] Jie Dong, Min Han, ?BitTableFI: An efficient mining frequent itemsets algorithm?, Elsevier Science Publishers B.

V., Amsterdam, The Netherlands,  2007, pp.  329-335.

[8] Lei Ji, Baowen Zhang, Jianhua Li, ?A New Improvement on Apriori Algorithm?, Intl. Conf. on Computational Intelligence and Security, 2006, pp. 840-844.

[9] Cristian Aflori ,Mitica Craus, ?Grid implementation of the Apriori algorithm?, Elsevier Science Ltd., Oxford, UK, 2007,  pp. 295-300.

