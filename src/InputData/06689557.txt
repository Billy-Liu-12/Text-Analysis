aK-Mode: New Algorithm to Cluster OLAP  Requirement Schemas

ABSTRACT? Different methods are proposed to generate the schema of data mart. We propose a new one that starts by clustering the OLAP requirements that are represented as schemas, then, it generates the schemas of the data mart one for each cluster, and finally it validates them. In this work, we will focus on the first step and we will propose a new algorithm ?aK- Mode? to cluster our schemas. Indeed, many techniques exist, they are applied to numerical and categorical data, but they cannot be used in our case since we have to take into consideration the semantic aspect while making the comparison. The new algorithm extends the k-mode by modifying the dissimilarity measure.

Keywords ? ak-mode; Clustering; Simple Matching; OLAP Requirement schema; Data Mart Schema.



I. INTRODUCTION The data mart ?is defined as a flexible set of data, ideally  based on the most atomic (granular) data possible to extract from an operational source, and presented in a symmetric (dimensional) model that is most resilient with unexpected user queries? [25]. Different approaches and methods have been proposed to ensure its design. Our main goal is defined in the same area where we propose the construction of the schema of the data mart from the OLAP requirements that are presented as schemas.

In order to realize our purpose, we should take into consideration three steps which are: clustering OLAP requirements schemas (ORS)s, generating the schema of data mart one for each cluster, and validating each data mart schema.

In this work, we will focus on the first step and we will propose a new algorithm ?aK-Mode? that can be applied to cluster the ORSs.

Data Mining proposes different techniques to deal with the data such as: prediction, Association rules, etc.  As we seek to organize the ORSs into groups by maximizing their similarity into one group and minimizing it between groups; we propose the use of ?Clustering?. It is unsupervised classification of patterns; it involves dividing a set of data points into non- overlapping groups, or cluster of points [32]. The objects within one cluster are more similar to those of another one, so the clustering aims at maximizing the homogeneity within the same group, it is used to group the different schemas resulting from the process of transforming the requirements, into clusters according to their departments or business functions.

Clustering was used at the beginning with the numerical data proposing then the following algorithms: K-means [9], FFT algorithm [6], PAM [13], CLARA [34], CLARANS [24], etc.

With the emergence of categorical data and their use in real databases, it becomes important to look for new algorithms to cluster this kind of data, so, many new ones have been  proposed, such as: K-Mode [17], ROCK [29], QROCK [15], CACTUS [26], COOLCAT [4], CLICK [18], LIMBO [21], MULIC[3], etc.

The algorithms use different distances/measures, and this is the cause of their evolution since those applied to numerical data cannot deal with categorical data.

In our work, we will cluster schemas. This kind of data is different since it provides additional information. We have to take this into consideration when we compare because it can influence the result of the clustering. For example, several words can denote the same thing, so if we use the traditional measures, the result of our comparison will not reflect the reality since the traditional version of k-mode ignores this point.

So, to overcome this problem, we propose an extension of k- mode which is ?aK-Mode? that modifies the Simple Matching (SM) dissimilarity measure.

The outline of this paper is as follows: in section 2, we detail some works mixing the OLAP and the Data Mining technologies. In the next section, we propose the use of multidimensional table to express the OLAP requirements. The information is visualized using an intermediate schema. Section 4 clarifies some concepts that are used next. In section 5, we define the aK-mode algorithm, we argue this choice and we explain the different modifications. We finish this work with the conclusion.



II. THE STATE OF THE ART In this section we list the various works mixing the OLAP  and the Data Mining technologies to take advantages of the both.

In [23], Ben Messaoud et al. propose OpAC (Operator for Aggregation by Clustering) which is considered as a new operator for multidimensional on-line analysis. It uses the agglomerative hierarchical clustering to achieve a semantic aggregation on the attributes of a data cube dimension. The authors propose taking advantage from both the OLAP and the Data Mining to get at the end an analysis process that provides the exploration, explication and prediction capabilities. Another data mining system DBMiner was presented in [10]. This latter integrates different data mining functions such as characterization, comparison, association, classification, prediction and clustering, as well as it integrates database and OLAP technologies. It has the advantage to mine various kind of knowledge at multiple levels of abstraction from different databases also from data warehouse. It offers SQL-like data mining query language, DMQL and a graphical user interface to facilitate the interactive mining. In the same context, Goil, in [28], proposes PARSIMONY which is a parallel and scalable  CoDIT'13     OLAP and Data Mining framework for large datasets. The system sparses the datasets using chunks so the data can be stored either as a dense block or as sparse representation. There is also, iDiff  [31] which is an operator used to automate the manual discovery processes using mining technology that can play a fitting role in providing the state of these products. iDiff returns summarized reasons for drops or increases observed at an aggregated level, and this can be done in a single step.

In [22], Chen et al. propose a scalable DW and OLAP-based engine for analyzing web log records. The proposed framework supports the typical OLAP operation and DM operations such as extended multilevel and multidimensional association rules. The OLAP server is used as a computation engine to support DM operations.

The data mining can be applied to detect the outliers. In this field, the [30] implements OLAP-outlier-based data association method as the result of the combination of OLAP and Data Mining. The proposed method integrates both outlier detection concept in DM and ideas from OLAP field and it is used to solve the data association problem.  It is used also to discover causal relations among heterogeneous databases as presented in [14] that propose a computer software agent. This agent combines Data Warehouse, OLAP and KDD functionalities in order to support the kno wledge discovery tasks. The solution consists on developing IIMiner (Integrated Interactive data Miner) that is used to provide convenient ways for the user to interact with the KDD processes using OLAP and DW techniques. In [27], Goil and Choudhary present a parallel multidimensional framework for large data sets in OLAP. This framework has been integrated with DM of association rules to facilitate handling a large number of dimensions and large datasets.



III. OLAP REQUIREMENTS The requirement plays a crucial role in the DW process  design; it can cause the failure of the whole project if it is faulty.

Despite its importance not much attention has been paid to this phase causing the fail of 85% of the DW projects to meet business objects, 40% of the DW projects never develop [11].

This phase is used to specify ?what data should be available and how it should be organized as well as what queries are of interest? [8] and it is serves to extract the important elements related to the multidimensional schema (facts, measures, dimensions, hierarchies).

In order to facilitate the specification of requirements, we propose the use of multidimensional table that helps, next, to generate the schemas.

A. The Multidimensional Table  Different works in the literature (such as [16], [7]) propos the use of n-dimensional table (called also multidimensional table) as a way to express the needs of the decision makers.

Since the decision makers may not be computer scientists, i.e.

they can find difficulties to express their needs using the SQL queries (especially when using the GROUP BY and/or HAVING clauses), we propose, as solution, the use of n- dimensional table.

It is a tabular representation that can show the fact of the decision maker. This fact and its measures can be analyzed according to dimensions and their granularity levels.

This choice is done because:  ? It is easy to be used by a non computer scientist user.

? It allows seeing values of certain attributes as a function  of others.

? The representation is close to decision makers? vision of  data [7] The Fig. 1 shows the model that we adapt to present our MT that has the following structure:  ? ?DomaineName?: The domain of analysis.

? ?FactName?: The fact corresponding to the analyzed  subject.

? ?Measure?: The measures which are defined through an  aggregation function ?f?: {f1(m1),?, fn(mn)} ? ?AF?: The function that used to calculate the measure.

? ?Dimension?: The dimensions related to the subject of  analysis.

? ?P?: The parametrs.

? ?Hierarchy?: the hierarchy associates the different levels  to their linked dimension instance.

Fig. 1. The model of multidimensional table  The Fig. 2 presents an example of MT where we have three dimensions ?Customer?, ?Supplier? and ?ListBook?, one fact ?Sale? and the measure ?Benefit? that will be calculated using the ?Sum? function. Concerning the dimension ?Customer? we take into consideration two levels ?CustomerID? and ?Quantity?.

Fig. 2. Example of Multidimensional Table (MT)  B. The schema of the OLAP requirement Using the database and the multidimensional table, we propose the visualization of the intermediate schema. This latter allows the user to validate him/her-self his/her requirements.

Applying this to our example, we get the Fig.3 which is the intermediate schema corresponding to the MT of the Fig.2.

CoDIT'13     Customer CustomerID Quantity  Supplier SupplierID Qty  Sale Benefit  ListBook ListID Cost     Fig. 3. The intermediate schema corresponding to the MT (Fig.2)  Since a schema ?Sch? is complex in term of composition, comparing its whole structure at once is not an adequate solution, so we propose its decomposition into categories which are: fact table (F), set of measures (M), set of dimension tables (D), set of attributes (A), set of hierarchies (H), and set of parameters (P).

? The Fact table corresponds to the subject of analysis. It is defined by a tuple (FN, MF{}) with FN represents the name of the fact and MF{m1,m2,m3,m4,?} corresponds to the set of measures related to the fact F.

? The Dimension tables represent the axis of analysis.

Each one is composed by (DN, A{}, HD{}) with DN corresponds to the dimension name, A {a1,a2,a3,a4,?} presents the set of attributes describing the current dimension D, and HD{h1D, h2D, h3D, h4D,?}is a set of ordered hierarchies. Each hierarchy has a tuple (HN, P{}) with HN is the name of the current hierarchy and P{p1, p2, p3, p4,?} is a set of ordered parameters.



IV. DEGREE OF SIMILARITY (DESIM) Let Sch1 and Sch2 be two schemas belonging to the same  cluster (e.g. they belong to the same domain).

Let Ci be the categories of elements existing in the schema.

Ci = {fact, dimension, measure, attribute, parameter, hierarchy} ? ei ? Sch1, ? ej ? Sch2, so that ei and ej belong to the same category Ci.

When we calculate the similarity between the elements of the two schemas, we should take into consideration the following points:  ? The identical: the case where we use the same elements name in the two schemas.

DeId (e1, e2) =1 if e1 and e2 are identical and 0 else.

? The synonymous:  it is the case where we use two different names that have the same meaning.

DeSy (e1, e2) = 1 if e1 and e2 are synonymous, and 0 else.

? The typos: it is the case where the user makes mistakes when writing the name of the element.

In this case, we calculate the degree of error. If it is low, we are in the case of typing error. If it is high we are in the case of two different words. In the following we only take into consideration the first case.

DeTy (e1, e2) =1 if e1 and e2 are the same with the existence of typing error.

? The post-and pre- fixe: it is the case where we use post- fixes or pre-fixes to design the same thing.

DePost (e1, e2) = 1 if one the two elements is the post- fixe of the other, and 0 else.

DePre (e1, e2) = 1 if one of the elements is the pre-fixe of the other, and 0 else.

The degree of similarity between ei and ej (DeSim(ei, ej)) is measured by the numeric value in [0, 1], it is the average of previous degrees, and it is calculated as following formula (1):   DeSim(ei, ej): Sch1 x Sch2 ? [0, 1]  DeSim(e1, e2) =[DeId (e1, e2) + DeSy (e1, e2) + DeTy (e1, e2)+ [DePost (e1, e2) or DePre (e1, e2)]] / 4         (1)   Two schemas are considered ?similar? if they have the highest degree of similarity (DeSim).



V. THE AK-MODE ALGORITHM In this section, we will introduce the aK-Mode which is an  extension of the k-mode algorithm. We will start by arguing our choice, then, we will detail our algorithm.

A. The reasons behind extending the k-mode algorithm  The Data Mining (DM) is ?the analysis of (often large) observational data sets to find unsuspected relationships and to summarize the data in novel ways that are both understandable and useful to the data owner? [5].

Many techniques and algorithms are proposed such as: clustering, classification, prediction, etc.

In our case, we propose the use of clustering because it is the process of partitioning a given population of events or items into sets of similar elements, so that items within a cluster have high similarity in comparison to one another, but are very dissimilar to items in other clusters [1].

Clustering can be applied to various type of data: continuous numerical variable,[19], binary variables [12], categorical variables [19]. We are interested on categorical data. Some of proposed algorithms are: K-MODE [17], ROCK [29], QROCK [15], CACTUS [26], COOLCAT [4], CLICK [18], LIMBO [21], MULIC [3], etc.

To choose the appropriate algorithm, we compare them in term of ?time complexity? as presented in Table1.

TABLE I. CLUSTER ALGORITHMS FOR CATEGORICAL DATA   Algorithm Complexity Coefficient K-MODE O (n) Simple Matching  ROCK O(kn2) Links QROCK O(n2) Threshold CACTUS Scalable Support  COOLCAT O (n2) Entropy CLICK Scalable Co-occurrence LIMBO O (nLogn) Information Bottleneck MULIC O (n2) Hamming measure   We can notice that the k-mode has O(n) which is the lowest complexity, but it cannot deal with our data because it does not take into consideration the semantic aspect of the elements, so, we extend it and we propose the aK-Mode.

CoDIT'13      B. The differences between k-mode and ak-mode  The k-mode algorithm is used mainly to cluster categorical data. In this work, we are manipulating schemas. With this kind of data, it is not possible to use the previous k-mode, so we propose an extension which is ?aK-Mode?.

The new algorithm takes the following points into consideration:  ? ?k? the number of clusters: one of the challenges of k- mode related to the construct of clusters is the estimation of the appropriate ?k? [5]. Different solutions have been proposed such as the use of validity measures [2], or the use of gap statistic [5].

Since we are looking for generating the schemas of data mart, and we know that one data mart corresponds to a particular area, one of the ideas is to choose ?k? as the number of existing domains. So we can be sure that the existing schemas in one cluster belong to one domain.

? The distance: the k-mode uses the simple matching dissimilarity measure that takes 0 or 1. The main idea of this measure is the use of the relative frequencies of the cluster modes [33], [20].

In our case, the previous measure cannot response to our needs. To compare the schemas, we have to take into consideration their semantic. So, as consequence we must modify the simple matching algorithm.

C. The algorithm  The algorithm of aK-mode is described as follows: a) Define the ?k? number of existing domains b) Select ?k? initial modes.

c) Allocate an object to the cluster whose mode is the nearest to the cluster, using the following formula (2):  d (A, B) = ? ?(ai, bi) where ?(ai, bi) = 0 if ai = bi and ?(ai, bi) =1  if ai ? bi          (2)  Update the mode of the cluster after each allocation.

d) After all objects have been allocated to the respective cluster, retest the objects with new modes and update the clusters.

e) Repeat steps (b) and (c) until there is no change in clusters.

D. The extension of Simple Matching  In this part we present the new measure.

The extension of Simple Matching: The simple matching coefficient as applied to categorical data is calculated using the formula (2). As this coefficient cannot be used with the schemas, we propose the following algorithm (Fig. 4), it takes as input the ?Mode? and the ?ORS? to give as result the coefficient of the new simple matching ?CoefSM?.

Input: ORS, Mode Output: CoefSM Begin  CoefF   = SimilarityFunctionF (ORS, Mode) CoefM = SimilarityFunctionM (ORS, Mode) ListSimDim[]= ListSimilarDimension(ORS, Mode) CoefD  = SimilarityFunctionD (ListSimDim[]) CoefA  = SimilarityFunctionA (ListSimDim[]) CoefH = SimilarityFunctionH (ListSimDim[]) CoefP   = SimilarityFunctionP (ListSimDim[])  CoefSM=[(MaxD ? CoefD )/MaxD]+(MaxM ? CoefM) /MaxM] + ( MaxH ?  CoefH ) /MaxH + ( MaxF ?  CoefF ) /MaxF + ( MaxA ?  CoefA ) /MaxA + ( MaxP ?  CoefP ) /MaxP ]  End   Fig. 4. The ?Simple Matching? algorithm  With:  ? CoefD: calculates the number of similar dimensions using ?SimilarityFunctionD?.

? CoefM: calculates the number of similar measures using ?SimilarityFunctionM?  ? CoefH: calculates the number of similar hierarchies using ?SimilarityFunctionH?  ? CoefF: calculates the number of similar facts using ?SimilarityFunctionF?  ? CoefA: calculates the number of similar attributes using ?SimilarityFunctionA?  ? CoefP: calculates the number of similar parameters using ?SimilarityFunctionP?  The ORS of any user contains one fact table, and for each schema the set of existing measures belongs to the same fact.

That facilitates the comparison of those two elements. As consequence the two functions ?SimilarityFunctionF? (Fig.5) and ?SimilarityFunctionM? (Fig.6) take as input the ORS and the mode to calculate directly the similarity of facts and the measures.

Input: ORS, Mode Ouput: coefF Begin int total=0;  If (ORS.Fact.equals (Mode.Fact)          total ++ If (Synonym (ORS.Fact, Mode.Fact)== true)   total ++ If (Typos(ORS.Fact, Mode.Fact) ==true)     total ++ If (PostFixe(ORS.Fact, Mode.Fact) ==true or PreFixe (ORS.Fact,Mode.Fact) ==true)) total ++  coefF = total /4; End   Fig. 5. ?Similarity FunctionF? algorithm   CoDIT'13     Input: ORS, Mode Ouput: coefM Begin int total=0;  For (each ORS.Measure) For (each Mode. Measure) If (ORS. Measure.equals (Mode. Measure) total ++ If (Synonym(ORS.Measure, Mode.Measure) == true) total ++ If (Typos(ORS. Measure, Mode. Measure) ==true) total ++ If (PostFixe(ORS. Measure, Mode. Measure)==true or PreFixe (ORS. Measure, Mode. Measure) ==true)) total ++ End For End For coefM = total /4; End   Fig. 6. ?Similarity FunctionM? algorithm  Concerning the dimensions, each schema has a list of dimensions. We must start by determining the pairs of similar dimensions using ?ListSimilarDimension? (Fig.7) function that stores them into table ?ListSimDim[]?. Using this latter, we can calculate coefficients using the following functions: ?SimilarityFunctionD? (Fig. 8), ?SimilarityFunctionA?, ?SimilarityFunctionH?, and ?SimilarityFunctionP?  Input: ORS, Mode Ouput: ListSimDim[] Begin int i = 0 For (each ORS.Dimension) For (each Mode. Dimension) If (ORS. Dimension.equals (Mode. Dimension)  or (Synonym (ORS. Dimension, Mode. Dimension) == true) or Typos(ORS. Measure, Mode. Measure) ==true or (PostFixe(ORS.Measure, Mode.Measure) ==true or PreFixe (ORS. Measure, Mode. Measure) ==true))  ListSimDim[i]=ORS.Dimension, Mode.Dimension i++ End if End For End For End  Fig. 7. ?ListSimilarDimension? algorithm  Input: ListSimDim[] Ouput: coefD Begin int total =0 For (each ListSimDim[i])  If(ListSimDim[i].ORS.Dimension.equals (ListSimDim[i].Mode.Dimension) total ++ If (Synonym (ListSimDim[i].ORS.Dimension, ListSimDim[i].Mode.Dimension) == true) total ++ If(Typos(ListSimDim[i].ORS.Dimension, ListSimDim[i].Mode.Dimension) ==true) total ++ If (PostFixe (ListSimDim[i].ORS.Dimension, ListSimDim[i].Mode.Dimension)==true or PreFixe (ListSimDim[i].ORS.Dimension, ListSimDim[i].Mode.Dimension) ==true)) total ++  End For End For coefD = total /4; End  Fig. 8. ?Similarity FunctionM? algorithm  To define the ?coefSM?, we need also to calculate ?MaxD? (corresponding the maximum number of the existing dimensions), ?MaxM? (corresponding to the maximum number of the existing measure), ?MaxH? (corresponding to the maximum number of the existing hierarchies), ?MaxF? (corresponding to the maximum number of the existing facts), ?MaxA? (corresponding to the maximum number of the existing attributes), and ?MaxP? (corresponding to the maximum number of the existing parameters).

E. The update of the Mode Concerning the update of the mode, we use the same method as defined in the k-mode which is based on the frequency of the elements.



VI. CONCLUSION In our work we proposed aK-Mode which is an extension  of k-mode used to cluster the schemas extracted from the OLAP requirements. The proposed algorithm integrates the equivalence, the synonymous, the case of error, the post- and pre-fixes nomination, to take into consideration the semantic aspect when comparing the different schemas.

The goal behind this work is to get a set of clusters. Each one contains schemas belonging to the same domain. This facilitates the construction of data mart schemas that will be used next to build the schema of the data warehouse.

As perspective, we propose the use of ?union-based algorithm? instead of ?frequency- based algorithm? to improve the update of the ?Mode?, we propose also the techniques of matching and mapping to ensure the fusion of different schemas existing in one cluster to get the corresponding data mart schema.

