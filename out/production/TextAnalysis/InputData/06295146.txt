Analysis of Association Rule Extraction between  Rough Set and Concept Lattice

Abstract?The model of concept lattice has strong ability of knowledge representation and knowledge discovery. Rough set theory based on the attribute reduction method often inevitably cuts out some useful information. Concept lattice, by contrast, has the relative completeness in association rule mining, and is user-friendly to find interesting information. So it can improve the mining efficiency. Based on the summaries of several typical attribute reduction algorithms, the thesis extracts association rules from the decision table, and shows that concept lattice can better realize the intuitive visualization in the process of association rule mining.

Keywords-rough set; concept lattice; attribute reduction; association rule

I.  INTRODUCTION  Rough set and concept lattice are two new branches of mathematics produced in the early 1980s. As two important tools of knowledge discovery, rough set and concept lattice have the common background and objective, but different characteristics.

Rough set determines the object sets kind by the binary relation between attributes and objects. The attribute reduction is to delete the uncorrelated or unimportant attribute in keeping the same classification ability, and then makes sure the rest of the attributes can completely determine knowledge discovery. However, it has been proved that asking for the minimum reduction is a NP-hard problem [2]. In order to find the minimum reduction, people have given a lot of attribute reduction methods, including two different defining views of algebra and information. The two methods have been proved to be completely equivalent in the document [1], but usually the way in information will be more intuitive and efficient than algebra expresses.

Heuristic reduction algorithm is to use the heuristic information to reduce the cost of space and time, and then get the approximate optimal solution. The typically algorithms include the heuristic reduction based on the attribute significance proposed by Pawlak [2~3], and the MIBARK algorithm based on mutual information proposed by Miao Duo-Qian [4]. Wang Guo-Yin [5] gave two attribute reduction algorithms based on conditional information entropy? CEBARKCC and CEBARKNC. The principle of MIBARK and CEBARKCC is to seek core attributes firstly, and then  reduce the non-core attributes. CEBARKNC does not need to seek core attributes, but gradually delete attribute reductions based on the original condition attribute set. Skoworon [6] proposed the method of discernibility matrix. This algorithm is more complete than heuristic algorithms in theory, and it can easily obtain an accurate result. But the question is that it is only suitable for small data sets for its heavy workload on the construction.

Concept lattice sets up its hierarchical structure based on the binary relation between attribute and object. Each node of the concept lattice corresponds a formal concept, which includes extension (namely collections of all objects) and connotation (which are the collections of properties). The theories of concept lattice and rough set are to classify based on data tables. The difference is that the rough set classifies the data table by equivalence relation, but the concept lattice is combined with the order theory, especially the complete lattice theory, to discuss the concept hierarchy. Besides, the Hasse diagram of concept lattice can reflects the concept relation between generalization and specialization visually, and is convenient to realize the visualization of knowledge. In the paper [7], the discernibility matrix is used in the attribute reduction of formal context, and constructs the reduction concept lattice directly. Although this method can reduce the calculated amount, some key information will be cut out and go against the discovery of knowledge.

The thesis firstly processes a decision table with several classic algorithms of attribute reduction, and then applies value reduction methods [8-10] widely used to extract association rules.

On the other hand, we use the quantitative extended concept lattice [11~13] to extract association rules. After comparing the process and consequence of the two models, the advantage of concept lattice model in association rules mining will be further discussed.



II. CORRELATION  THEORIES  A. Rough Set Define 1 (Decision Table DT) We call the quaternion  ),,,( fVDCUDT ?=  a decision table, and U = {x1, x2 ...

xn} is the limited object sets, called the domain of discourse.

}{ CC ?= ?? is conditional attribute set. }{ DddD ?= is decision attribute set. And ?????= DCDC ,,? .

Computer Science & Education (ICCSE 2012) July 14-17, 2012. Melbourne, Australia   ThP4.20    )( DCVV ?? ??= ?? is the value field of function f.

=f },:{ DCVUff ???? ???? means the  information function of decision table.

Definition 2 (Equivalence Class) (U, R) is an approximate space. U is object set, and R is the equivalence relation in the U. The equivalence class produced by (U, R) is  [ ] [ ] }),({},{/ RxxxxRxxRU jijRiiRi ?=?= .

Definition 3 (The Core of Knowledge ) Given a knowledge base K = (U, S) and its an equivalence relation race P, all necessary relationships in set P is called the core of P, recorded as CORE (P). PR ?? , If IND (P) = IND (P-{R}), we call R is unnecessary in P, whereas it is necessary.

Definition 4 (The reduction of Knowledge) Give a knowledge base K = (U, S) and two equivalence relation races P and Q.

PQ ? . If (1) Q is independent, (2) IND (Q) =IND (P), we call Q is a reduction of P, recorded as RED (P).

B. Concept Lattice Definition 5 (Formal Context) We call (U, A, I) for a formal context, of which U = {x1, x2... xn} is object set, and each xi (i ? n) is a object. A = {a1, a2... am} is attribute set, each aj (j ? m) called an attribute. I is the binary relationship between U and A. AUI ?? , if Iax ?),( , x has the attribute of a, recorded as xla.

Definition 6 (Concept) For a form background (U, A, I), we define the operation in the object set UX ? and attribute set  AB ? . },,{ xlaXxAaaX ???=? and  },,{ xlaBaUxxB ???=?  represent collections of objects in B and collections of attributes in X. If the dualistic group (X, B) content BXBX == **, , we call the (X, B) is a concept, of which X is extension of the concept and B is connotation of the concept.

Definition 7 (Concept Lattice) ),(and),( 2211 BXBX are two concepts of a form background. If 21 XX ?  (the same as 12 BB ? ), we call ),( 11 BX the sub-concept of ),( 22 BX , and ),( 22 BX is the super-concept of ),( 11 BX , recorded as ),( 11 BX ? ),( 22 BX . The partial ordered sets of all the concepts in (U, A, I) note for ),,( IAUL , which is called concept lattice. In fact, concept lattice is a complete lattice, recorded as )),,,(( ?IAUL .

C. Typical Reduction Algorithms of Decision Table At present, typical reduction algorithms of decision table  include blind deleting, the method of Skoworon discernibility matrix, the method of Pawlak significance of attributes and the reduction algorithm based on mutual information.

In the algorithm of blind deleting, if the attribute  i? satisfies the formula )()( )(}){( DposDpos CINDCIND i =? ? , it  means the attribute i? is unnecessary in A. We need to delete the column of i? , and merge the repeat lines. This algorithm can get a relative reduction, but usually could not get a satisfactory result for deleting some useful information.

The attribute reduction algorithm based on Skoworon discernibility matrix [6] can easily and intuitively get the attribute core of decision table. But it always leads to big expense in time and space in the process of constructing the matrix. In addition, a large number of repeat elements will take up a lot of internal memory. Chang Liyun [14] put forward an improved algorithm based on the discernibility matrix, which can effectively reduce the combinational numbers of attributes by the core attributes and the sets without core attributes. An improved algorithm and a kind of incremental updating algorithm were proposed in the document [15] and [16], and they both improved the efficiency of the attribute reduction. But, all these methods can't improve the complicacy of discernibility matrix radically. With regard to the instability of inconsistent decision table, document [17] gave a definition of improved discernibility matrix, and it also proved the algorithm is suitable for both compatible decision table and inconsistent decision table.

Pawlak algorithm calculates the relative significance based on positive field in proper order, and puts it into the core set one by one to get a reduction. Based on the mutual information, the attribute reduction algorithm [4] takes the mutual information as the terminal condition. According to the significance of attribute, it gradually selects the most important attributes to add to the relative core set.

Based on the definitions of mutual information between condition attribute and decision attribute, the MIBARK algorithm uses the attribute core set as the origin, and gradually adds the condition attributes in a bottom-up way.

The document [18] pointed out CEBARKNC algorithm maybe still have redundant attributes, so it gave an improved algorithm. In allusion to the heavy calculated quantities of the significance in the MIBARK algorithm, document [19] used the frequency of attributes as the heuristic information to get an improvement. Document [20] gave another definition based on the average decision strength and the entropy of the decision, and proved their consistency.

The document [21] put forward a recursive algorithm, by using the heuristic function as the measurement criteria of selecting the condition attribute, and defining the concepts of hierarchical structure and approximation. Document [22] introduced the concept of decision-making strength. It improved the algorithm's completeness through improving the expression of the attribute significance. But no matter how to improve, heuristic algorithm still relies on the approximate optimal solution based on some typical algorithms, and it has no substantial changes.

ThP4.20

III. REALIZATION AND ANALYSIS OF THE ATTRIBUTE REDUCTION  A. Example and Attribute Reduction Results Based on Rough Set Give a decision table ),,,( fVDCUDT ?= . C = {a,  b, c, d, e} is the condition attributes, D= {f} is decision attribute.

Tab.1   DT= ),,,( fVDCU ?            Merge the same lines in Tab.1.Then express them by the equivalence class. The simplified decision table is Tab.2.

With the method of blind deleting, we can get a relative attribute reduction B = {c, d} of the decision table. With the algorithm of discernibility matrix, and two kinds of heuristic attribute reduction algorithms, we can get two reduction sets- {a, c, d} and {c, d, e}. The results show that the algorithms based on rough set theory are the same basically. In particular, the algorithm of blind deleting always deletes condition attributes iteratively, and merges the repeat line until get a reduction. So this method will not necessarily get a satisfactory result.

B. Analysis and Contrast on Algorithm  Set A as the number of attribute, and U as the number of record. So the time complexity of the blind deleting  algorithm is )2( 2UO A ? .

The time complexity of the discernibility matrix algorithm,  has been analyzed in the document [23], the result  is )2( 2UO A ? . In the decision table, when there are lots of condition attributes and records, the solution of the space and time complexity increases greatly, so the efficiency would be significantly reduced. In the reduction algorithm of Pawlak based on attribute significance, the number of cycle is A  at  most, the worst complexity is )lg( 2 UUAO . The time complexity of reduction algorithm based on mutual  information is )( 22 UAO .

In fact, the heuristic algorithms based on rough set theory,  are only suboptimal methods. They need adjust constantly by attribute significance and comentropy, and it still can not solve the problem fundamentally. Although the attribute reduction algorithm based on discernibility matrix has a complete theory, the combination will grow in a "burst" type with the increase of the attributes.

C. Attribute Reduction Algorithm of Discernibility Matrix Based on Concept Lattice We can see the formal context (U, A, I) as a special kind  of decision table or information system. And the decision table under the context of multiple-valued can also be turn into the two-value context. The concept lattice showed in Fig.1 is from Tab.1.

According to the thought of discernibility matrix reduction of document [24~25], we can calculate the final result of disjunctive normal form:  =L )( FCf ?? = (a ? b ? c ? d ? e ? f).

So the final result is consistent with the original attribute  set, namely D= {a, b, c, d, e, f}.

The concept lattice after attribute reduction refers to Fig.1.

The attribute reduction result of concept lattice is often  more complete than rough set. This is because before the attribute reduction, the algorithms based on rough set theory usually delete some redundant data, which always contain important knowledge information. Besides, the heuristic algorithms tend to give up the less important attributes, which may be very important to the classification accuracy.

And the attribute reduction method based on discernibility matrix, is suitable only for compatibility and complete decision table, otherwise the algorithm could lead to wrong results.

Fig.1   The Hasse diagram of concept lattice according to Tab.1    Tab.1   DT= ),,,( fVDCU ?    Tab.2   Simplified Decision Table     ThP4.20

IV. ASSOCIATION RULE MINING ANALYSIS ON THE ATTRIBUTE REDUCTION OF ROUGH SET AND CONCEPT LATTICE  Association rule is the logic formula like BA? . The decision-makers are not always interested in all rules, so we can mine the interesting rules through the measure methods of support and confidence. The support and confidence are defined as follows:  )()( BAPBASupport ?=?                    (1)  )( )()()(  ASupport BASupportABPBAConfidence ?==? (2)  A. Association Rules Mining based on Attribute Reduction of Rough Set We removed relative unnecessary attributes in the  decision table by the attribute reduction above. And we can get the minimal results which has the accordant classification ability with the original decision table by the reduction of attribute value.

The decision tables after attribute reduction refer to the Tab.3 and Tab.4.

Reduct the value in two decision tables respectively, and then get the corresponding core value tables as follows.

From Tab.5, we can get three decision rules.

Rule 1: )1,()1,())0,()0,(( fdca ??? .

Rule 2: )0,()1,()1,( fca ?? .

Rule 3: )0,()0,( fd ? From Tab.6, we can get four decision rules.

Rule 4: )1,()0,( fe ? .

Rule 5: )1,()0,( dc ? )1,( f? .

Rule 6: )0,()1,()1,( fec ?? .

Rule 7: )0,()0,( fd ? .

The supports from rule 1 to rule 7 are as follows:  61616161316131 . And they have the same confidence of 1.

B. Analysis on the Results of Rules Extracting from Quantitative Extended Concept Lattice The method put forward in the document [11~13] is how  to extract association rules from the quantitative extended concept lattice. The quantitative extended concept lattice is to quantize extension of concepts, and ignore the specific information of the extension. In this way, it can realize the visualization of association rules greatly, and is convenient for users to find out the interested rules. When the support is less than the threshold, we delete the rules directly from concept lattice. So this process is also called structuring pruning concept lattice. The attributes after reduction can be transformed into quantitative extended concept lattice, as shown in Fig.2.

The rule after reduction based on the rough set is: )1,()1,())0,()0,(( fdca ??? . And in the quantitative  extended concept lattice, the corresponding rule 5 is fcd ? , which is extracted from the concept (5, cd) and concept (3, cdf). The supports of the two concepts are 93and95 . And the confidence of the rule 5 is 53 , which can be obtained from the Hasse diagram directly. Also, we can get the implication relation of fd ? , that is the rules of 3 and 7, and its confidence is 64 .

The confidence of the rule adfe? in the concept lattice is 61 , so it contains the rule 4 of )1,()0,( fe ? obviously.

The confidence of dfc?  is 73 , and the confidence of adfe?  is 61 , so we can get the rule of ce df? , and its  confidence is 1416173 =? , furthermore, we can get the contained rule 6 of )0,()1,()1,( fec ?? .

Similarly, we can push out the rule of fad ? , and its confidence is 92 , including the rule 1 of )1,()1,()0,( fda ?? .

The confidence of fac? is 18910 , and it contains the rule 2 of )0,()1,()1,( fca ?? .

Thus, each extracted rule based on the rough set theory, can be expressed by the quantitative extended concept lattice   Fig.2 The Hasse diagram of quantitative extended concept lattice according  to Fig.1  Tab.3 Decision table of {a, c, d}           Tab.4 Decision table of {c, d, e}    Tab.5 Core value of Tab.3                    Tab.6 Core value of Tab. 4     ThP4.20    directly and simply. The radix of extension in the concept is the number of the same intension in the relational database.

The confidence of the association rules mining results, are just to show whether or not the rules exist. So it does not express the confidence exactly.

In the theories of rough set and concept lattice, association rules are described through the relationship among attribute sets (concept connotation), and they reflect the inclusion relation of the object sets (concept extension). The less the attributes in the reduction set, the less the rules they derive. And the example proved this point. Besides, the attribute reduction method based on rough set, merges the repeat lines firstly, and leads the loss of much object information in the process of association rule mining.

Relatively, concept lattice, including all the extension information, can ensure the relative integrity of association rule mining. And its mining process is multi-dimensional and multi-attribute [13].

In addition, the Hasse diagram of concept lattice is helpfully to realize knowledge diffusion and establishment of the transparent high-level knowledge. So it can reflect the reality objectively. The association rule mining based on quantitative extended concept lattice is visually and simply so as to reduce the blindness and mechanical, and improve the mining efficiency.



V. CONCLUSIONS Compared with rough set theory, concept lattice can  effectively avoid the loss in process of attribute reduction. And it can also ensure the completeness [26] of the association rule mining by extracting rules multi-dimensionally and multi- attribute. The concept lattice sets up kinds of connections and explanations among knowledge by Hasse diagram visually, so as to help to improve the efficiency of rule extraction and discover knowledge.

