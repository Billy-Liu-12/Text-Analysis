Correlation Hiding by Independence Masking

Abstract Extracting useful correlation from a dataset has  been extensively studied. In this paper, we deal with the opposite,  namely, a problem we call correlation hiding (CH), which is  fundamental in numerous applications that need to disseminate  data containing sensitive information. In this problem, we are  given a relational table T whose attributes can be classified into  three disjoint sets A, B, and C. The objective is to distort some  values in T so that A becomes independent from B, and yet,  their correlation with C is preserved as much as possible. CH  is different from all the problems studied previously in the area  of data privacy, in that CH demands complete elimination of the  correlation between two sets of attributes, whereas the previous  research focuses on partial elimination up to a certain level. A  new operator called independence masking is proposed to solve  the CH problem. Implementations of the operator with good  worst case guarantees are described in the full version of this  short note.



I. INTRODUCTION  Correlation elimination is fundamental in applications that  need to disseminate data containing sensitive information.

Assume, for example, that the census bureau has produced  a table T with attributes A = {Race}, B = {Income}, and C =  {Investment-expense, Food-expense, Entertainment-expense},  which needs to be put online to allow the public to study  the spending behavior of various ethnic groups and income  classes. Doing so, however, will also reveal the correlation  between Race and Income, which should be avoided, because  such correlation may lead to sensitive debates such as how  much wealthier an ethnic group is than another. In other  words, the government would like to release T in such a way  that hides the correlation between Race and Income, and yet  preserves all the other correlations.

The problem cannot be settled by giving away two tables: (i)  T1 that has only A and C, and (ii) T2 with only B and C. This  is because their equi-join T1 ?? T2 may restore a significant  portion of the original T . The worst case is that no two tuples  in T have the same values on C, allowing the equi-join to  rebuild T precisely.

A good solution in the above scenario should fulfill two  requirements. First, it must totally destroy the dependence    between Race and Income. That is, the Income distribution of  any specific race, such as Caucasian, should look exactly the  same as that of any other race, such as Asian. Second, it needs  to do so by distorting as little information in T as possible.

Otherwise, the resulting table would not allow researchers to  perform meaningful data mining, thus defeating the objective  of publication.

This paper deals with a general version of the problem  described earlier. Assume that we are given a table T whose  attributes have been classified into three disjoint subsets A, B,  and C. The goal is to compute another table T ? where  the set A of attributes is independent from the set B of  attributes;  a large number of values of T are retained.

We refer to the above problem as correlation hiding (CH),  to which we are not aware of any adequate solution. It is  opposite to the classic topic of correlation extraction in data  mining. The closest existing works are found in the areas of  privacy preserving data publication (PPDP), and association  rule hiding (ASH). However, as elaborated in Section II, CH is  fundamentally different from both PPDP and ASH, in that CH  demands complete elimination of some designated correlation,  whereas PPDP and ASH focus on partial elimination up to a  certain level. Such a difference renders the solutions of PPDP  and ASH inapplicable to CH.

In this short note, we propose an operator called indepen-  dence masking (I-masking) to carry out correlation hiding.

The operator works by masking some values in T in order  to make the sets of attributes A and B appear mutually  independent. The goal is to minimize the number of values  masked. In the full version of this short paper, we show that  the problem is NP-hard, and describe several fast approximate  algorithms with good approximation guarantees. The full paper  also contains experiments that verify the practical effectiveness  of I-masking in the context of association rule mining.



II. RELATED WORK  In this section, we review two problems that are similar to  the problem of correlation hiding (CH) studied in our paper,  and explain why the existing solutions are not applicable to  CH.

Privacy preserving data publication (PPDP). In recent years,  PPDP has received considerable attention from the database    and data mining communities (see the recent work [1] and  the references therein). Given a table T containing sensitive  information, PPDP aims at computing an anonymized version  T ? that satisfies a privacy constraint, such as k-anonymity  [3], [4] or ?-diversity [2], pre-determined by the administrator.

PPDP is similar to CH in the sense that both of them need to  distort the correlation between two sets of attributes A and B  in the original table T .

The key difference between PPDP and CH is the degree of  distortion. Specifically, the goal of PPDP is to distort just to  the level required by the underlying privacy constraint. Any  1 Chinese University of Hong Kong  2 Simon Fraser University  3 Nanyang Technological University  4 Hong Kong University of Science and Technology  978-1-4244-5446-4/10/$26.00  2010 IEEE ICDE Conference 2010964  additional distortion should be avoided because, in PPDP, the  remaining (un-distorted) correlation of A and B is meant to be  released, as it is the target of research studies. In contrast, CH  aims at complete removal of the correlation between A and B,  namely, they must appear totally independent from each other  in the published T ?.

In fact, CH even differs from PPDP in their fundamental  rationales. Specifically, the motivation of PPDP is to conceal  the details of a distribution, but preserve its big picture. The  reason is that statistical analysis does not require fine details  anyway; hence, even with some details removed, the resulting  dataset can still be useful for statistical studies. In contrast,  the distribution targeted by CH must be fully destroyed, and  is not meant to be analyzed at all.

Association rule hiding (ASH). Let S be a set of transactions,  each of which is a set of items from a discrete domain such  as products sold by a supermarket. Denote by R the set of  association rules that can be mined from S. Assume that there  is a subset R? of R that is sensitive, and should not be known  by the public. The objective of ASH [5] is to modify S to  another dataset S? such that the (insensitive) association rules  in R ? R? can still be discovered from S?, but those in R?

cannot.

Unlike ASH that deals with transactions, CH is concerned  with relational tables. Even if one regards a relational table as a  set of transactions (by treating each cell as an item, and each    tuple an item set), ASH solutions still cannot be applied to  CH due to two reasons. First, ASH demands two parameters,  namely support and confidence, to define association rules.

It is unclear how these parameters would fit in CH. Second,  similar to PPDP, an ASH algorithm does not fully eliminate  the data correlation, because some (insensitive) association  rules must remain discoverable. CH, as mentioned before,  demands complete removal of the correlation between two  sets of attributes. In fact, it appears rather difficult to even  cast CH as an ASH problem. The reason is that association  rules are essentially a specific type of correlations, whereas  the objective of CH is to eliminate any correlation in general.



III. PROBLEM DEFINITION  In this section, we will formalize the problem of correlation  hiding (CH). Let T be a relational table whose attributes have  been classified into three disjoint subsets A, B, and C. The  correlation between attribute sets A and B is sensitive, and  must be fully concealed. C, on the other hand, involves all the  remaining attributes of T that do not belong to A and B. CH  aims at converting T to another dataset T ? such that

I. A and B are independent in T ?, and

II. T ? preserves the other correlations in T as much as  possible.

We refer to T as the source table, and T ? as the sanitized  table.

Note that the two requirements are not equally important:  requirement I has a higher priority. Specifically, if T ? has  not destroyed the correlation of A and B, it is always a poor  solution no matter how well it retains the other correlations  in T . This is due to the protective nature of the applications  modeled by CH. For instance, consider the motivating example  in Section I where A = {Race}, B = {Income}, and C =  {Investment-expense, Food-expense, Entertainment-expense}.

As long as the government is not convinced that the correlation  between Race and Income has disappeared, it would prefer to  keep the data away from public access.

Requirement I can be described in a more rigorous manner  as follows. Let a be any value in the domain of T ?.A, and  similarly, b be any value in the domain of T ?.B. Note that in  case A (B) has multiple attributes, a (b) is a multidimensional  vector. Denote by Pr[a] (or Pr[b]) the percentage of tuples in  T ? whose values of A (B) are equal to a (b). Likewise, denote    by Pr[a, b] as the percentage of tuples in T ? carrying both a  and b simultaneously. Then, the independence in requirement  I can be specified as  Pr[a, b] = Pr[a]  Pr[b]. (1)  As an immediate corollary, one can easily verify that if the  above equation holds, then any subset of A is also independent  from any subset of B.

The other correlations in requirement II can be specialized  into 5 concrete types:  1. Correlation A: the correlation among the attributes in A.

2. Correlation B: the correlation among the attributes in B.

3. Correlation C: the correlation among the attributes in C.

4. Correlation AC: the correlation between the attributes of  A and those of C.

5. Correlation BC: the correlation between the attributes of  B and those of C.

Apparently, it is straightforward to preserve correlation C  because the attributes in C do not need to be touched in  correlation hiding at all. Among the other types of correlations,  correlations AC and BC are of higher importance. This is  because in practice C is typically a set of measures, such that  the goal of scientific studies is to find out how those measures  are influenced by the attributes in A and B, respectively.

CH is a general problem that may be attacked using different  methodologies. This is analogous to the opposite problem of  correlation extraction, which can be performed by clustering,  association rule mining, classification with decision trees, etc.

In the next section, we describe a feasible methodology to  perform CH.



IV. INDEPENDENCE MASKING  We present an operator called independence masking (I-  masking) to carry out CH. The operator takes an integer  parameter u, whose effect will be explained later. Denote by  n the cardinality of the source table T . In the sequel, we will  assume that n is a multiple of u. In case it is not, we randomly  remove at most u ? 1 tuples from T to make the property  hold. As will be clear later, u is typically by far smaller than   BA  tuple ID Age Occupation Income  1 30~50 CEO 25k  2 30~50 Salesman 4k    3 30~50 Prof 10k  4 > 50 Prof 20k  5 30~50 Prof 11k  6 < 30 Prof 6k  7 < 30 Manager 18k  8 < 30 Manager 13k  9 < 30 Manager 7k  10 30~50 Prof 15k  11 30~50 Prof 9k  12 30~50 Prof 6k  tuple ID Age Occupation Income  1 30~50 CEO 3  2 30~50 Salesman 1  3 30~50 Prof 2  4 > 50 Prof 3  5 30~50 Prof 2  6 < 30 Prof 1  7 < 30 Manager 3  8 < 30 Manager 2  9 < 30 Manager 1  10 30~50 Prof 3  11 30~50 Prof 2  12 30~50 Prof 1  BA BA  tuple ID Age Occupation Income  1 30~50 * 3  2 30~50 * 1  3 30~50 * 2  4 * Prof 3  5 * Prof 2  6 * Prof 1  7 < 30 Manager 3  8 < 30 Manager 2  9 < 30 Manager 1  10 30~50 Prof 3  11 30~50 Prof 2  12 30~50 Prof 1  (a) (b) (c)  Fig. 1. Example 1 (B has only one attribute). (a) shows the source table T . (b) shows the  intermediate table T ? after replacing each Income value with a  cluster label. (c) shows the final sanitized table T ?.

n; hence, removing at most u ? 1 tuples will not lose much    information.

To facilitate discussion, let us assume, without loss of  generality, that A has d attributes A1, A2, ..., Ad. Denote by  dom(Ai) the domain of Ai. I-masking outputs a sanitized table  T ? that has as many tuples as T . Furthermore, the attributes  of T ? can also be classified into disjoint subsets A, B, C such  that  A has d attributes corresponding to those in T . Specif-  ically, the domain of each attribute in A augments the  domain of the corresponding attribute of T by a special  symbol ?. Formally, dom(T ?.Ai) = dom(T .Ai)? {?}  for all 1 ? i ? d.

B has a single attribute whose domain is the set of  integers from 1 to u. It relates to the set B of attributes  in T such that each integer of T ?.B represents a cluster  of T in its subspace B.

C is exactly the same as the C of T . Namely, I-masking  touches only A and B, and leaves C intact.

More precisely, I-masking has three steps detailed as fol-  lows.

1. Partition T based on its attribute set B into u clusters  such that every cluster has an equal number of tuples.

Label the clusters as 1, 2, ..., u, respectively.

Once this is done, the values of T .B are no longer needed:  we will be concerned only with the clusters. Hence, let  us replace the B values of each tuple with the label of the  cluster it belongs to. This creates an intermediate table  T ?, whose A and C are the same as T , but its B has an  integer domain of [1, u].

2. Mask some A values of T ? as ? so that A and B are  independent from each other. Let the resulting table be  T ?.

3. Return T ?, as well as the u clusters of B values obtained  earlier in the first step.

Next we illustrate the above steps with two examples. Since  I-masking does not alter the attributes in C, we will omit  them in our examples but their presence should be implicitly  understood.

Example 1. Consider that T is the table in Figure 1a, where A  = {Age, Occupation} and B = {Income}. Note that 30?50  is a raw value of Age (as opposed to a generalized value  as one would find in k-anonymization [3], [4]). Assume u    equals 3. Step 1 of I-masking creates u equal-sized clusters  out of the values in T .B. Here, B has a single numeric  attribute. So the clustering results in three intervals: [4k, 7k],  [9k, 13k], and [15k, 25k], each of which covers the Income  of exactly 4 tuples. Label these intervals (i.e., clusters) as 1,  2, 3, respectively. Replacing each Income value with the label  of the interval it falls in gives the intermediate table T ? in  Figure 1b.

Step 2 of I-masking hides some values of A with the symbol  ?, until A and B have become independent. The resulting  table T ? is shown in Figure 1c, where 6 ? are used. To see  the independence of A and B, notice that Equation 1 holds for  any values a and b in the domains of A and B in Figure 1c,  respectively. For example, let a = (*, Prof) and b = 1. Then,  we have Pr(a, b) = 1/12, which is indeed the product of  Pr(a) = 3/12 and Pr(b) = 4/12.

Finally, I-masking returns the table in Figure 1c, together  with the Income values in clusters, namely, cluster 1 = {4k,  6k, 6k, 7k}, cluster 2 = {9k, 10k, 11k, 13k}, cluster 3 = {15k,  13k, 20k, 25k}.

In Example 1, B involves only a single attribute. Next, we  will see another example where B has multiple attributes.

Example 2. Let T be the table in Figure 2a, where A =  {Race} and B = {Income, Saving}. Assume u is 2. For  clarity, we denote the B values of each tuple as a 2D point  in Figure 2b. As before, Step 1 of I-masking clusters these  points into u clusters, as illustrated in Figure 2b. Figure 2c  shows the intermediate table T ? after replacing the B values  with cluster labels. Step 2 of I-masking generates the sanitized   Race Income Saving  r1 point 1 (see right)  r1 point 2  r1 point 3  r1 point 4  r1 point 5  r1 point 6  r2 point 7  r2 point 8  r2 point 9  r2 point 10  r2 point 11    r2 point 12  A B  Income  Saving  cluster 1          cluster 2     A B  Race Cluster  r1 1  r1 1  r1 1  r1 1  r1 2  r1 2  r2 2  r2 2  r2 2  r2 2  r2 1  r2 1  A B  Race Cluster  r1 1  r1 1  * 1  * 1  r1 2  r1 2  r2 2  r2 2  * 2    * 2  r2 1  r2 1  (a) (b) (c) (d)  Fig. 2. Example 2 (B has multiple attributes). (a) shows the source table T . (b) illustrates the  clustering on T .B. (c) gives the intermediate table T ?, and  (d) the sanitized table T ?.

table T ? as in Figure 2d, where the two attributes have become  independent. This T ? is returned, together with the points in  Figure 2b (i.e., the B values of T ) in their respective clusters.

Although in the previous examples B contains only numeric  attributes, I-masking also works even if some or all the  attributes in B are categorical. The only thing we require on  B is the ability to cluster its values. This can be achieved  by many algorithms, such as k-means (with straightforward  adaptation to ensure all clusters are equally large), that perform  clustering in the metric space. These algorithms are applicable  as long as one can supply a distance function to calculate the  similarity of two objects, which is easy to formulate in most  applications.

Quality of correlation preserving. It is clear that I-masking  completely retains Correlations B and C (see the classification  of correlations in Section III), because the values of those  attributes are returned directly. The introduction of ?, appar-  ently, loses a part of Correlations A and AC. Furthermore,  the choice of u also affects Correlation BC. To see this, recall  that, in T ?, I-masking essentially presents the projection of  T on B in the form of u clusters; hence, a larger u captures  Correlation BC better.

It would be nice if we could put an upper bound on how  much of Correlations A, B, and AC is lost. This, unfortu-  nately, is not possible because CH has a compulsory goal  eliminating the correlation between A and B (see requirement  I in Section III). In the worst case A and B can be extremely  related such that removing their correlation would necessitate  masking all values. It thus follows that I-masking is a best-  effort process. Namely, we should reduce the information  loss as much as possible, on condition that requirement I  is satisfied. Put differently, given a specific value of u, we  would like to generate T ? using the smallest number of ?.

Following this rationale, I-masking can be cast as the following  optimization problem.

PROBLEM 1: Let T ? be a table with d + 1 attributes.

Among these attributes, there is one, denoted as B, whose  domain consists of integers from 1 to u. Denote by A as the  set of all other d attributes in T ?.

T ? has the property that exactly 1/u of its tuples carry 1,  2, ..., u as their B values, respectively. Let T ? be a table that  is identical to T ? except that some values in the attributes  of A have been masked by ?. T ? is said to be independent  if its A and B are independent from each other. The goal is  to find an independent T ? containing the smallest number of  stars.



V. WHAT IS IN THE FULL PAPER  The full paper contains several formal results on Problem 1.

Specifically, the paper first shows that the problem is NP-  hard, and then, gives two fast approximate algorithms with  approximation ratios u ? 1 and d, respectively. Furthermore,  the full version also includes experimentation to demonstrate  the effectiveness of I-masking in association rule mining.

ACKNOWLEDGEMENTS  This work is supported by Grants GRF 4161/07 and GRF  4173/08 from HKRGC.

