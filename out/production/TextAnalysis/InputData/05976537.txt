2011 3rd Conference on Data Mining and Optimization (DMO)   28-29 June 2011, Selangor, Malaysia

Abstract? It is a difficult task to set rare association rules to handle unpredictable items since approaches such as apriori algorithm and frequent pattern-growth, a single minimum support application based suffers from low or high minimum support. If minimum support is set high to cover the rarely appearing items it will miss the frequent patterns involving rare items since rare items fail to satisfy high minimum support. In the literature, an effort has been made to extract rare association rules with multiple minimum supports. In this paper, we explore the probability and propose multiple minsup based apriori-like approach called Probability Apriori Multiple Minimum Support (PAMMS) to efficiently discover rare association rules. Experimental results show that the proposed approach is efficient.

Keywords: knowledge discovery, frequent-pattern, rare association rules

I.  INTRODUCTION Association rule mining is an important data mining  technique to discover interesting associations among the entities (or items) in a dataset.  Since the introduction of association rules in [1], mining of association rules has been extensively studied in the literature [2,5,10,12].

The basic model of association rule is as follows. Let I={i1, i2----in} be a set of items. Let T be a set of transactions (data set), where each transaction t is a set of items such that t

I. An itemset X is a set of items {i1,i2,---ik},1?k?n, such that X   I. The itemset containing k number of items is called k-item set. An association rule is an implication of the form, A=> B, where A   I, B   I and A?B=? [1].

Definition 1: The support of A=>B, denoted as (A U B) = ?(A U B)/?T?, where f(AUB) refers to frequency of AUB and  ?T? is the transactional database size.

Definition 2: The confidence of A=>B, denoted as C(A=>B)=S(A U B)/S(B). The term minsup indicates the user specified minimum support and the term minconf indicates the user specified minimum confidence.

Given T, the problem of mining association rules is to discover all rules that satisfy minsup and minconf constraints. An itemset that satisfies minsup constraint is called frequent itemset or frequent pattern. Given a transaction dataset consisting of set of items, mining association rules generally involve two steps:  (i) Discovering all frequent patterns and (ii) Generating all strong association rules from the set  of frequent patterns.

A rare association rule is an association rule applicable to  rare items. Rare association rules can provide useful knowledge. However, it is difficult to mine rare association rules because single min_sup based association rule (or frequent pattern) mining approaches like Apriori and Frequent Pattern-growth (FP-growth) suffer from the dilemma called ?rare item problem?. The ?rare item problem? can be described as follows. If min_sup is set too high, we miss the frequent patterns involving rare items because rare items fail to satisfy high min_sup. To find frequent patterns consisting of both frequent and rare items, we have to set min_sup very low. However, this may cause combinatorial explosion and produce too many frequent patterns. In addition, some of the uninteresting patterns can be generated as frequent patterns.

Definition 3: Let A = A1, A2, ? , An be a set of all events. If  P(Ai1Ai2 ? Aik)= P(Ai1)P(Ai2)? P(Aik)   (1)   For any k (1<k?n), any 1 ? i1< i2<?< ik?n, the formula (1) is right, then A1, A2, ?, An are inter-independent events.

Candidate frequent itemsets refer to the itemsets whose probability is larger than the user sets; they may be the last frequent itemset or not.

A. Apriori Algorithm Example 1: Consider a Transactional dataset T, shown in the Table 1. The set of items I = {I1, I2, I3, I4, I5, I6}.  Total number of transactions in T is 10.  A set of items is to form a pattern. The support of these item or a patterns is the number of times which occurred in the database against the total number of transactions in the database which implies the percentage support of a pattern.

Support (item/pattern) = Number of times it has occurred / Total Number of Transactions.

TABLE I.  TRANSACTION TABLE  BILLS  ITEMS B1 I1, I2, I3  B2 I4, I5      B3 I2, I3, I5  B4 I4, I5  B5 I6, I2, I3  B6 I1, I2, I3, I5  B7 I6, I5  B8 I2, I3, I5  B9 I5  B10 I1, I2, I3, I5   Candidate_1_items comprise all distinct items in the transactional database with their corresponding supports.

Considered minimum support is 2, which have to satisfy support of frequent items, or patterns than it will be placed in the frequent_1_itemset, see the table 2.

TABLE II. FREQUENT_1_ITEMSET Items COUNT  I4  2  I6  2  I1  3  I2 6  I3  6  I5 8   Candidate_2_itemset is to be generated by clubbing two frequent_1_items and also calculated corresponding support of a pattern. Frequent_2_itemset contains the patterns which satisfy minimum support.  Candidate_n_itemset?s are generated by combining frequent_n-1_itemset and then satisfy the minimum support, which is placed in frequent_n_itemsets. The remaining steps will be explained in the above procedure.  See the table 3.

TABLE III. FREQUENT_4_ITEMS Food Items COUNT I1, I2, I3, I5 2   There is only one Frequent_4_item set so the next candidate_5_item sets will not be generated. Association rules are derived from these patterns. The confidence of a rule is the items occurred in the database to the left side (condition on rule). Rules, which satisfy both minimum support and minimum confidence is a frequent pattern, and such rules are considered as strong rule.  If the minimum confidence is 70% then 3 rules will be generated. To improve the performance of mining frequent patterns consisting of both frequent and rare items, efforts have been made to discover the frequent patterns using? multiple min_sup frameworks?.

B. Apriori with multiple minimum supports Example 2: Consider a Transactional dataset T, shown in Table 1. Candidate_1_items contains all distinct items in the  transactional database with their corresponding supports.

Considered minimum support is 2 which have to satisfy support of frequent items or patterns than it will be placed in the frequent_1_itemset. User defined inputs are least support, Support Difference and Maximum Item Pattern Difference. Let least support (LS) = 2, support difference (SD) =2, and maximum item pattern difference (MIPD) =4, where LS refers to user-specified ?least support? and SD refers to support difference. SD can be either user-specified or derived using equation 2.

SD = ?(1 - ?)    (2) where, ? represents the parameter like mean, median of the item supports and ?  [0,1].

Each item in the transaction dataset is specified with a support constraint called minimum item support (MIS). A pattern is defined as frequent, if its support is greater than or equal to the minimal MIS value among all its items. MIS values are assigned based on their corresponding support values so it is compared with frequent items, rare items are specified with relatively lower MIS values. MIS values are generated using following equation (3) [9].

MIS (Item) = Count (Item) ? SD    when (Count (Item) - SD) > LS = LS  otherwise (3)  TABLE IV: FREQUENT_1_ITEMSET Items COUNT MIS  I4  2 2  I6  2 2  I1  3 2  I2 6 4  I3  6 4  I5 8 6   Item Pattern Difference (IPD) is to differentiate the pattern to item, the main logic here is filtering out the uninteresting patterns, which distinguishes between the support of a pattern and the support of the maximal frequent item in that pattern. See the table 4. Maximum Item Pattern Difference (MIPD) is then entered by user depends on the requirement and IPD value must be less than or equal to the MIPD.

IPD <= MIPD and Support(pattern) >=  Min_MIS  Candidate_2_itemset is to be generated by clubbing two frequent_1_items. Frequent_2_itemset contains the patterns which satisfy minimum support.

TABLE V. FREQUENT_2_ITEMSETS Items COUNT I1, I2 3  I1, I3 3  I2, I3 6  I2, I5 4  I3, I5 4         TABLE VI. FREQUENT_3_ITEMS Items COUNT  I1, I2, I3 3  I2, I3, I5 4   The remaining steps will be the same as above procedure.

According to multiple minimum support basis the support of pattern must be greater than minimum MIS value among all items in the pattern. Pattern {I4, I6} of these support count is 2 and minimum MIS value is 2 but it is not selected for next generation of subsequent patterns. See table 5 and 6.

IPD{I4,I5}=MAX{MIS(I4),MIS(I5} ? count{I4,I5} IPD {I4, I5} = MAX{2,7} ? 2 = 7-2 =5 User-entered MIPD value is 4, IPD value always less than or equal to MIPD value but here it is not satisfying thus it is not selected for generating next patterns. There is only one Frequent_4_item set so the next candidate_5_item sets cannot be generated. If the minimum confidence is 70% then 5 rules will be generated.

The rest of the paper is organized as follows. Section II  discusses the related work. Section III explains the solution framework used for generation of rare association rules.

Section IV gives some reflections about the observed results and rare association rules are generated. Finally, section V concludes with mention of the future likely enhancements of the system.



II. RELATED WORK To find rules that involve both frequent and rare items,  min_sup has to be set very low. If the min_sup is very low it generates huge number of frequent items and if min_sup is set high we may miss some interesting items. This paper proposes a novel technique to solve this problem. The technique allows the user to specify multiple minimum support to reflect the nature of the items and their varied frequencies in the database [2]. An Efficient Implementation of Pattern Growth Approach? described the implementation techniques of an adaptive pattern growth algorithm, called AFOPT, which demonstrated good performance on all tested datasets and also extended the algorithm to mine closed and maximal frequent item sets [6]. Multiple minimum support approach uses the notion ?item-to-pattern difference? to efficiently discover rare association rules. This model still extracts uninteresting rules if the items? frequencies in a dataset vary widely. The proposed approach assigns appropriate min_sup values for frequent as well as rare items based on their item supports and thus reduces both ?rule missing? and ?rule explosion? problems [9,13].

A model-based frequency constraint was developed as an  alternative to a single, user-specified minimum support. This constraint utilizes knowledge of the process generating transaction data by applying a simple stochastic mixture model, which allows for transaction data?s typically highly  skewed item frequency distribution. A user-specified precision threshold is used together with the model to find local frequency thresholds for groups of item sets. This approach shows that the new constraint provides improvements over a single minimum [3]. Based on association analysis, an improved algorithm of apriori is developed. The main ideas of the algorithm is to count the probability of each item and used to make the next generation of candidate and frequent items without scanning the data base every time [4]. Custom-built apriori algorithm has been proposed to find the effective pattern analysis [14].Association rule mining among infrequent items and a general survey and a comparison of apriori and FP growth with different data sets has been discussed [7,8].



III. PROPOSED APPROACH The proposed algorithm generalizes the apriori algorithm  for finding the frequent itemsets. We call the new algorithm Probability Apriori Multiple Minimum Support (PAMMS).

Let P1, P2?Pn are the independent probability of items A1, A2?An. The probability for any two item Ak, Am(Pk<Pm) both appeared in one transaction is Pkm. Count the probability of each attribute item (A1, A2,?Am) of a DB by scanning the DB first time. The probability of any two items Ak and Am appeared synchronously in one record is Pkm. If Ak and Am is total correlation, then the Pkm is the minimum of the Pk and Pm. If Ak and Am is total independent, then the Pkm is Pk *Pm , so we can estimate using equation (4):   Pkm = (a * min(Pk, Pm) + b * Pk * Pm)  (4)  a, b are constant parameters, which is used to represent  the association between the items in pattern.

We can use the method, which described above to find out all the frequent itemsets without scanning DB so many times. Count the support of the frequent itemsets by scanning the DB another time. Output the association rules from the frequent itemsets [4,11].

Example 3: Consider a Transactional dataset T, shown in Table1. Table 7 contains frequent items in first column and second column represents the number of times the item has occurred in the database. User defined inputs are least support (0.2), Support Difference (0.1) and Maximum Item Pattern Difference (0.4). COUNT= No of Times occurred in Database / Total No of Transactions. Each item in the transaction dataset is specified with a support constraint called minimum item support (MIS).

TABLE VII. CANDIDATE_1_ITEMS Items COUNT MIS  I4  0.22222 0.122  I6  0.22222 0.122  I1  0.33334 0.222  I2 0.66667 0.555      I3  0.66667 0.555  I5 0.88889 0.777   Candidate_1_items contains all distinct items in the transactional database with their corresponding supports.

Considered minimum support is 2 which have to satisfy support of frequent items or patterns then it will be placed in the frequent_1_itemset. See the table 8.

TABLE VIII. FREQUENT_1_ITEMSET Items COUNT  I4  0.22222  I6  0.22222  I1  0.33334  I2 0.66667  I3  0.66667  I5 0.88889   a, b are constant parameters, which is used to represent  the association between the items in pattern.

a = No of items / No of  Transactions, b = 1 ? a, a = 6 / 10 =0.666667, b=0.3333333. Probability concept is used to find the support count of a pattern, which, approximates the value between minimum and maximum.

TABLE IX. CANDIDATE_2_ITEMSETS  Items COUNT MIN_MIS IPD I4, I6 0.136409 0.122 0.063  I4, I1 0.144000 0.122 0.156  I4, I2 0.168000 0.122 0.432  I4, I3 0.168000 0.122 0.432  I4, I5 0.184000 0.122 0.616  I6, I1 0.144000 0.122 0.156  I6, I2 0.168000 0.122 0.432  I6, I3 0.168000 0.122 0.432  I6, I5 0.184000 0.122 0.616  I1, I2 0.252 0.200 0.348  I1, I3 0.252 0.200 0.348  I1, I5 0.276 0.200 0.524  I2, I3 0.504 0.500 0.096  I2, I5 0.552 0.500 0.247  I3, I5 0.552 0.500 0.247   Candidate_2_itemsets is to be generated by joining two frequent_1_items and calculated corresponding support of a pattern calculated, see the table 9. Frequent_2_itemset contains the patterns which satisfy minimum support.

Candidate_n_itemset?s are generated by combining frequent_n-1_itemset and then satisfies the minimum MIS value and IPD value, which is placed in frequent_n_itemsets, see the table 10. The remaining steps are the same as in above procedure. In the first column the items are marked with bold, it represents pattern is selected for next generation  of patterns and second column count is marked with red and bold color, which represents support count is not greater than minimum support. In the table 1.4 pattern {I4,I6} is not satisfied to enter in to the frequent_2_items because Min_support = 1.804  but support count of these pattern is 1.364. According to probability basis the support of pattern is must be greater than minimum support value. Thus pattern {I4,I6} is not selected for generating next patterns. Also pattern {I1,I5} of  these support count is 0.276 and minimum MIS value is 1.222, it is not selected for next generation of patterns.

IPD {I1,I5} = MAX{0.2222,0.7777} ? 0.276 = 0.7777-0.276 =0.501  Here user entered MIPD value is 0.4, IPD value always less than or equal to MIPD value but here it is not satisfying thus it is not selected for generating next patterns.

TABLE X. FREQUENT_2_ITEMSETS Food Items COUNT  I1, I2 0.252  I1, I3 0.252  I2, I3 0.504  I2, I5 0.552  I3, I5 0.552   TABLE XI. FREQUENT_4_ITEMS  A. Food Items COUNT  I1,I2,I3 0.21168   There is only one Frequent_4_item set, and so the next candidate_5_item sets will not be generated, See the table 11.

Algorithm (PAMMS): 1. Begin; 2. Generate candidate 1-itemset C1;  C1= item    // finding unique items in the data and assigning to candidate 1_item  3. Calculate support S, for itemsets in C1.

A. Transactions=Calculate number of transactions B. count(c1)=frequency count of item   // number  of times item occurred in the data base C. Support(c1)=count(c1) / Transactions;  //Support  refers to probability support 4. a and b are parameters         // Used to calculate  prob_support of item/pattern a= No of items / No of Transactions; b=1-a;  5. MIS=calculate-MIS(c1,Support); 6. MIPD= User input // Maximum item pattern  difference.

7. Max_value = max(Support) ? MIPD 8. If Max_value  > min_mis than  min_sup1=Max_value; Else min_sup=min_mis;      9. L1= { <i>  | i ? c1,  Support (i) ? min_mis(i) };    // L1 refers frequent_1_itemset  10. L1= sort(L1,MIS); 11. for k=2; Lk-1??;k++  do       // repeats till no  frequent item occurs Ck=candidate-gen(Lk-1); Ct=subset(Ck,t);  for each candidate c ? Ct do temp_prob(j)= Support (c) if min_mis >MIS(c) than  min_min=MIS(c) j++;  end for Support (ck) = a * min(temp_prob) +  ( b * min(temp_prob) * max(temp_prob)); Max_value=max(temp_prob);  if max_value >= min_mis  than min_sup=max_value  else min_sup= min_mis IPD=max_value ? prob_sup(ck);  Lk ={  c ? C | IPD<=MIPD & Support(ck) ? min_mis & Support(ck)>=min_sup| for all i ? c) }  end for 12. Answer=Uk Lk; 13. min_conf=User input    // enter minimum  confidence for generating rules 14. Generate Association Rules.

15. End  calculate-MIS (c1 : Candidate_1_itemset,prob_sup : support of items)  SD: support difference (User input), LS: least support among all items Set Max_value=0   a. for i = 1; i ? | C1 |; ++i do  if max_value > Support(i)  max_value= Support(i);  M(i) = Support(i) ? SD(n); if M(i) < LS then  MIS(i) = LS;  else  MIS(i) = M(i); end if end for  b. return MIS;

IV. EXPERIMENTAL RESULTS In this section, we present the experimental results  conducted on chess and mushroom datasets. The chess dataset (335K) contains 3169 transactions and 75 items. The mushroom dataset (553K) contains the 8124 transactions, and 119 items. Both of the datasets are available at Frequent Itemset Mining (FIMI) repository [15]. For experimental purpose we have compared all the algorithms, which are Apriori, Probability Apriori, Apriori with Multiple Minimum Supports and Probability Apriori with Multiple Minimum Support (PAMMS) with chess and mushroom datasets. Here 1st column represents the type of algorithm like Apriori, PA,  AMMS or PAMMS and second column represents least support (LS=10%), Maximum Item Pattern Difference (MIPD=50%) and Support Difference (SD=10, 15, 20, 25 and 30). Third column represents the number of frequent items generated at specific minimum support for AMMS or  TABLE XII. COMPARING PERFORMANCE RESULTS ON APRIORI, PROBABILITY APRIORI, AMMS AND PAMMS USING CHESS DATASET (335K). THE TERMS AMMS, PAMMS ARE USED AS ACRONYMS FOR APRIORI WITH MULTIPLE MINIMUM SUPPORT AND PROBABILITY APRIORI WITH MULTIPLE MINIMUM SUPPORT   PAMMS algorithms. Fourth column indicates the  execution time for generating frequent items. Fifth column represents number of rules generated at different confidence levels. According to above results AMMS algorithm generates mined frequent items and it takes more time for generating frequent items as compared with Probability Apriori Algorithm. Thus PAMMS Algorithm is used multiple minimum support and probability concept to reduce the execution time and also generates mined results. See table 12 and 13.

TABLE XIII. COMPARING PERFORMANCE RESULTS ON APRIORI, PROBABILITY APRIORI, AMMS AND PAMMS USING MUSHROOM?S DATASET (553K)  Algorithm LS=10 %  MIPD =50%  SD  # Frequ  ent Items  Execution Time(Sec)  # Rules  Conf =70%  Conf= 80%  Conf= 90 %  Apriori  >1149  >4659.172 >27594 >2734  >1401  Probability Apriori  >1117  >236.109 >2254 >2254 >2195  PAMMS 30 3412 21.969 41237 40829 32620 AMMS 30 1209 1307.313 1055 806 775  PAMMS 25 1536 9.750 11996 11901 9774 AMMS 25 1133 1236.484 931 682 682  PAMMS 20 1410 8.031 10479 10389 8356 AMMS 20 1019 896.907 744 527 527  PAMMS 15 535 5.657 2166 2133 1875 AMMS 15 896 811.375 372 248 248  PAMMS 10 451 4.235 1638 1629 1392 AMMS 10 786 694.469 1150 871 812   Algorithm  LS=10 %  MIPD =50%  SD  # Frequen t Items  Execution Time (Sec)  # Rules  Conf= 70%  Conf= 80%  Conf= 90%  Apriori  >2735 >2310 >1652 >1324 >668  Probability Apriori  >4387 >21.282 >4355 >4342 >4235  PAMMS 30 1702 9.985 356 343 241 AMMS 30 659 680.953 393 393 393  PAMMS 25 182 6.219 155 118 112 AMMS 25 659 675.968 393 393 393  PAMMS 20 122 5.906 51 51 51 AMMS 20 483 558.610 243 243 243  PAMMS 15 116 5.798 36 36 36 AMMS 15 388 505.407 183 183 183  PAMMS 10 98 5.453 51 51 48 AMMS 10 385 496.828 183 183 183       The number of frequent items generated by AMMS algorithm is more with same support count used for PAMMS algorithm.

With the same support count but with different support difference (SD) given to the PAMMS algorithm we can see slight increase in the frequent items. This proves that the performance of the PAMMS algorithm takes less time for execution because it uses the multiple minimum supports, so it dynamically generates the support for each pattern, thus it reduce the uninteresting patterns and also it scans the database only once.



V. CONCLUSION AND FUTURE WORK In order to test our experiment, an improved Apriori  approach with Probability Apriori Multiple Minimum Support has been developed for improving the execution time and minimum support. Evaluation of the performance of proposed model is done by conducting experiments on both chess and musroom datasets. The result shows that probability apriori with multiple minimum supports algorithm improves the performance over the existing approaches.

Many directions for future enhancements are open.

Among them, we can mention:  1.  The testing of the proposed work on a grid computing environment and  2.  An appropriate method for assigning confidence values in a dynamic manner to generate rare association rules.

