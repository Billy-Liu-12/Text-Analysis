Analyzing Time-Series Data by Fuzzy Data-Mining Technique

Abstract-Time series analysis has always been an important and interesting research field due to its frequent appearance in different applications. In this paper, we attempt to use the data mining technique to analyze time series. Many previous studies on data mining have focused on handling binary-valued data. Time series data, however, are usually quantitative values. We thus extend our previous fuzzy mining approach for handling time-series data to find linguistic association rules. The proposed approach first uses a sliding window to generate continues subsequences from a given time series and then analyzes the fuzzy itemsets from these subsequences. Appropriate post-processing is then performed to remove redundant patterns. Experiments are also made to show the performance of the proposed mining algorithm. Since the final results are represented by linguistic rules, they wir be friendlier to human than quantitative representation.

Index Terms-time series, data mining, fuzzy set, association rule

I. INTRODUCTION T IME series analysis has always been an important and  interesting research field due to its frequent appearance in different applications. Some domains such as bioinformatics [2, 13], medical treatment [27] and finance [7] especially emphasize it for making good prediction and decision. A time series is usually composed of lots of data points, each of which represents a value at a certain time. In the past, many approaches based on regression [22], neural networks [16, 25, 31] and other mathematical models [10] were proposed to analyze the time series. Recently, the data mining technique has also been used in analyzing time series. For example, Das et al.

proposed a mining algorithm to discovery basic shapes (patterns) from time series [12]. The rule format found out was "IfA occurs, then B occurs within time T', where A and B were patterns and T was a time duration. In addition, Yuan et al.

proposed a quantitative movement-pattern mining algorithm  Chun-Hao Chen is with the Department of Computer Science and Information Engineering, National Cheng-Kung University, Tainan, 701, Taiwan. (e-mail: chchen@idb.csie.ncku.edu.tw).

Tzung-Pei Hong is with the Department of Computer Science and Infornation Engineering, National University of Kaohsiung, Kaohsiung, 811, Taiwan. (corresponding author; phone: +886+7+5919031; fax: +886+7+5919049; e-mail: tphong(nuk.edu.tw).

Vincent S. Tseng is with the Department of Computer Science and Information Engineering, National Cheng-Kung University, Tainan, 701, Taiwan. (e-mail: tsengsm(mail.ncku.edu.tw).

for time series [30]. The rule mined out was represented as "If the rate of exchange of some stock rises 15%, then its closing price may rise 10%".

Many previous studies on data mining have focused on  handling binary-valued data. Time series data, however, are usually quantitative values, so designing a sophisticated data-mining algorithm able to deal with this type of data presents a challenge to workers in this research field.

Recently, fuzzy set theory [33] has been used more and more frequently in intelligent systems because of its simplicity and similarity to human reasoning [20]. The theory has been applied in fields such as manufacturing, engineering, diagnosis, economics, among others [15, 20, 24, 29]. Several fuzzy learning algorithms for inducing rules from given sets of data have been designed and used to good effect with specific domains. Hong et al. proposed a fuzzy mining approach [16] to find fuzzy interesting itemsets and fuzzy association rules from quantitative data. The mining results obtained could be smooth due to the fuzzy membership characteristics.

In this paper, we thus extend our previous approach [16] and propose a fuzzy mining algorithm for time series to find linguistic association rules. The proposed approach first uses a sliding window to generate continues subsequences from a given time series and then analyzes the fuzzy itemsets from these subsequences. Appropriate post-processing is also performed to remove redundant patterns. Since the final results are represented by linguistic rules, they will be friendlier to human than quantitative representation.



II. REViEW OF RELATED MNING APPROACHES Data mining is most commonly used in attempts to induce  association rules from transaction data [6]. The goal is to discover important associations among items' such that the presence of some items in a transaction will imply the presence of some other items. To achieve this purpose, Agrawal and his co-workers proposed several mining algorithms based on the concept of large itemsets to find association rules in transaction data [3-6]. They divided the mining process into two phases. In the first phase, if the number of an itemset appearing in the transactions was larger than a pre-defined threshold value (called minimum support), the itemset was considered a large itemset. In the second phase, association rules were induced from the large itemsets found in the first phase. All possible association combinations for each large itemset were formed, and those with calculated confidence values larger than a  0-7803-9017-2/05/$20.00 C2005 IEEE 112    predefined threshold (called minimum confidence) were output as association rules.

Das et al. proposed a mining algorithm for time-series  analysis [12]. Their approach composed of two phases: time-series discretization and association-rule generation. In the discretization phase, the approach used a clustering method to find basic shapes from time series, and then transformed the time series into a discretized series based on the basic shapes found. In the second phase, an Apriori-like method was used to generate association rules. The rules found out in the above way were different from traditional association rules. The rule format was "IfA occurs, then B occurs within time r", which meant the occurrence of B was followed by A within T time units. In addition, Yuan et al. proposed a mining algorithm for discovering quantitative movement pattern from time series. A rule mined out might be like this: "If the rate of exchange of some stock rises 15%, then its closing price may rise 10%".

As to fuzzy data mining, Hong et al. proposed several fuzzy  mining algorithms to mine linguistic association rules from quantitative data [16, 19, 23]. They transformed each quantitative item into a fuzzy set and used fuzzy operations to find fuzzy rules. Their proposed algorithm focused on transaction data. Cai et al. proposed a weighted mining approach to reflect different importance to different items [8].

Each item was attached a numerical weight given by users.

Weighted supports and weighted confidences were then defined to determine interesting association rules. Yue et al.

then extended their concepts to fuzzy item vectors [32].

In handling time-series data, Song et al. proposed a fuzzy stochastic fuzzy time series and its models by assuming its values are fuzzy sets [26]. Chen et al. proposed a two-factor time-variant fuzzy time series model to deal with forecasting problems [9]. Au and Chan proposed a fuzzy mining approach to find fuzzy rules for time series classification [1]. Watanabe exploited the Takagi-Sugeno model to build a time-series model [28]. Thus, in this paper we proposed the fuzzy association rules mining algorithm that to mine rules among data points in the given window size.



III. MINING Fuzzy ASSOCIATIoN RULES FOR TIME SERIES In this section, the proposed fuzzy mining algorithm  integrates the fuzzy sets, the Apriori mining algorithm, and the time-series concepts to find out appropriate linguistic association rules is described below.

Theproposedfuzy time-series mining algorithm: INPUT: A time series S with k data points, a set of h  membership functions for data values, a predefined minimum support x; a predefined minimum confidence 2, and a sliding-window size w.

OUTPUT: A set of fuzzy association rules with confidence values from S.

STEP 1: Transform the time series S into a set of subsequences W(S) according to the sliding-window size w. That is, W(S) = {s, s, = (d4, d.+. ... , d = 1 to k-w+13, where dp is the value of the p-th data point in S.

STEP 2: Transform thej-th (i = 1 to w) quantitative value Vpj in each subsequence sp(p = 1 to k-w+ 1) into a fuzzy set fpj represented as:  (pLL + fpj2 + + fpjh Rjl Rj2 Rjh)  using the given membership functions, where Rjl is the l-th fuzzy region of the j-th data point in each subsequence, h is the number of fuzzy memberships, and fpj is vpj's fuzzy membership value in region Rjl.

Each Rjl is called a fuzzy item.

STEP 3: Calculate the scalar cardinality of each fuzzy item Rjl as:  k-w+1  count1,= Efpjl p=1  STEP 4: Collect each fuzzy item to form the candidate l-itemsets C1.

STEP 5: Check whether the support value (= countjl / (k-w+1)) ofeach Rjl (p <j <p+w-1 and I < I < h) in C1 is larger than or equal to the predefined minimum support value a If Rjl satisfies the above condition, put it in the set of large 1-itemsets (L1). That is:  LI = {Rjl countjl a,; <j<p+w-1 and -I <I<h}.

STEP 6: IF L1 is not null, then do the next step; otherwise, exit the algorithm.

STEP 7: Set r = 1, where r is used to represent the number of fuzzy items in the current itemsets to be processed.

STEP 8: Join the large r-itemsets Lr to generate the candidate (r+1)-itemsets Cr+i in a way similar to that in the apriori algorithm [6] except that two items generated from the same order of data points in subsequences cannot simultaneously exist in an itemset in Cr+i.

Restated, the algorithm first joins Lr and Lr under the condition that r-1 items in the two itemsets are the same and the other one is different. It then keeps in Cr+j the itemsets which have all their sub-itemsets of r items existing in Lr and do not have any two items Rjp and Rjq ofthe same j.

STEP 9: Do the following substeps for each newly formed (r+l)-itemset I with fuzzy items (II, I2, ..., Ir+j) in Cr+j: (a) Calculate the fuzzy value ofI in each subsequence  sp as  fis =f PA f PA ...Af* , wherefP is the membership value of fuzzy item Ij in sp. If the minimum operator is used for the intersection, then:  r+1  f-SP= Minfijp (b) Calculate the count ofI in all the subsequences as:  k-w+1  count,= ZfIp p=1  (c) Ifthe support (= count1! (k-w+1)) ofIis larger than or equal to the predefined minimum support     valuea; put it in Lr+-.

STEP 10: IF Lr+j is null, then do the next step; otherwise, set r =  r+ 1 and repeat STEPs 8 to 10.

STEP 11: Shift each large itemsets (II, I2, ..., Iq), q . 2, into (II',  I2 , ..., Iq'), such that the fuzzy region Rjl in I, will become RI, in II' and a fuzzy region Ri, in the other items becomes R(i-+j,t, where Rj1 is the l-th fuzzy region of thej-th data point in each subsequence.

STEP 12: Remove redundant large itemsets from the results after STEP 11.

STEP 13: Construct the association rules for each large q-itemset I (after STEP 12) with items (II, I2, ..., Iq), q . 2, using the following substeps: (a) Form each possible association rule as follows:  IIA...AIk IAIk+lA...AAIq -* Ik,1kl toq.

(b) Calculate the confidence values of all association  rules by the following formula: k-w+1  p=1 k-w+lE(ffsPA ...A fstP, f4P A ... fA ) p=l  STEP 14: Output the association rules with confidence values larger than or equal to the predefined confidence threshold A.

Note that in STEP 2, the transformation from data values to fuzzy sets in each subsequence can be simplified by removing the first fuzzy set from the previous subsequence and add the fuzzy set derived from the last data point in the current one.



IV. AN EXAMPLE In this section, a simple example is given to show how the  proposed algorithm can generate fuzzy association rules from the given time series. Assume the data points in the time series are given as shown in Table I.

TABLE I THE TIME SERIES USED IN THIS EXAMPLE  1, 6, 9, 8, 4, 3, 7, 9, 6, 4, 2, 4, 7, 8, 3  The time series in Table I contains 15 data points. Each data point represents a value at a certain time. For example, the second data point in the time series means the value obtained at time 2 is 6.

Assume the fuzzy membership functions for the data values  are defined as shown in Fig. 1. There are three fuzzy membership functions, Low, Middle and High, used in this example.

For the time series given in Table I, the proposed mining algorithm proceeds as follows.

STEP 1: The given time series is first transformed into a set  of subsequences according to the predefined window size.

Assume the given window size is 5. There are totally 11 (=15-5+1) subsequences obtained from the time series. The results are shown in Table II.

STEP 2: The data values in each subsequence are then  transformed into fuzzy sets according to the membership fimctions given in Fig. 1. Take the first value v11 (=1) in the subsequence sj as an example. The value "1" is converted into the fuzzy set, where Ai.term is a fuzzy region of the i-th data in the subsequences and is called a fuzzy item. This step is repeated for the other data points and subsequences.

Membership value Low Middle High   0 2 5 6 9 data value  Fig. 1. The membership fimctions used in this example  STEP 3: The scalar cardinality of each fuzzy item is calculated as its count value. Take the fuzzy item A .Low as an example. Its scalar cardinality = (1 + 0 + 0+ 0 + 0.33+ 0.67 + 0 +0 +0 +0.33 +1) = 3.33. This step is repeated for the other fuzzy items.

STEP 4: All the fuzzy items are collected as the candidate  l-itemsets.

TABLE II  THE SUBSEQUENCES OBTAINED FROM THE GIVEN TIME SERIES FORW = 5  SI (1,6,9,8,4) s7 (7,9,6,4,2) 52 (6,9,8,4,3) S8 (9,6,4,2,4) S3 (9,8,4,3,7) S9 (6,4,2,4,7) S4 (8,4, 3, 7, 9) Sio (4, 2,4, 7, 8) S5 (4, 3, 7, 9, 6) SI] (2,4, 7, 8, 3) s6 (3, 7, 9, 6, 4)  STEP 5: For each fuzzy item, its count is checked against the predefined minimum support value a. Assume in this example, ais set at 30%. Since the support values ofAl.Low, A,.Middle, A2.Middle, A3.Middle, A3.High, A4.Middle, A5.Low, and A5.Middle, are all larger than 30%, these items are thus put in LI (Table III).

TABLE III THE SET OF LARGE 1-ITEMSETS Li FOR THIS EXAMPLE  A1.Low 0.303 A3.High 0.303 A1.Middle 0.425 A4.Middle 0.485 A2.Middle 0.485 A5.Low 0.303 A3.Middle 0.455 A5.Middle 0.485  STEP 6: Since LI is not null, the next step is then done.

STEP 7: Set r=1, where r is the number of fuzzy items in the  current itemsets to be processed.

STEP 8: In this step, the candidate set Cr+l is generated from  Lr. C2 is then first generated from LI as follows: (A1.Low, A2.Middle), (A1.Low, A3.Middle), (A,.Low, A3.High), (A1.Low, A4.Middle), (A,.Low, A5.Low), (A,.Low, A5.Middle), (Aj.Middle, A2.Middle), (Al.Middle, A3.Middle), (Al.Middle,     A3.High), (A1.Middle, A4.Middle), (Ai.Middle, As.Low), (A1.Middle, A5.Middle), (A2.Middle, A3.Middle), (A2.Middle, A3.High), (A2.Middle, A4.Middle), (A2.Middle, As.Low), (A2.Middle, As.Middle), (A3.Middle, A4.Middle), (A3.Middle, As.Low), (A3.Middle, As.Middle), (A3.High, A4.Middle), (A3.High, As.Low), (A3.High, As.Middle), (A4.Middle, As.Low), and (A4.Middle, As.Middle). Note that no two fuzzy items with the same Ai are put in a candidate 2-itemset.

STEP 9: The following substeps are done for each newly  formed candidate itemset.

(a) The fizzy membership value ofeach candidate itemset in  each subsequence is calculated. Here, assume the minimum operator is used for the intersection. Take (Aj.Low, A2.Middle) as an example. The derived membership value for this candidate 2-itemset in sp is calculated as: min(I.0, 0.67)=0.67. The results for the other subsequences are shown in Table IV.

TABLE IV THE MEMBERSHIP VALUES FORA1.LOWn A2.MIDDLE  1 1 1 D__1.0 2 0 0 0.0 3 0 0.33 0.0 4 0 0.67 0.0 5 0.33 0.33 0.33 6 0.67 0.67 0.67 7 0 0 0.0 8 0 1 0.0 9 0 0.67 0.0 10 0.33 0 0.0 11 1 0.67 0.67  The results for the other 2-itemsets can be derived in a similar way.

(b) The scalar cardinality (count) ofeach candidate 2-itemset in the time series is then calculated in the same way.

(c) The supports of the above candidate itemsets are then calculated and compared with the predefined minimum support (30%). In this example, only the two itemsets, (A1.Middler'A4.Middle) and (A2.MiddlerA5.Middle), satisfy this condition. They are thus kept in L2.

STEP 10: Since L2 is not null in the example, r = r + 1 = 2.

STEPs 7 to 9 are then repeated to find L3. C3 is first generated from L2. In this example, no candidate 3-itemsets can be formed. L3 is thus an empty set. STEP 11 then begins.

STEP 11: The large 2-itemset (A2.Middle n As.Middle) is  then shifted into (AI.Middle r) A4.Middle) since its first region occurs in A2.

STEP 12: The two large itemsets (A1.Middle n A4.Middle)  and (A1.Middle r) A4.Middle) are the same and only one ofthem is kept.

STEP 13: The association rules for each large itemset are  then constructed by the following substeps.

(a) All the possible association rules are first formed from  the large itemsets. In this example, only the large  2-irmeset (A,.Middle n A4.Middle) exists. The following two possible association rules are then formed:  1. IfAI = Middle, then A4 = Middle; 2. IfA4 = Middle, then AI = Middle;  (b) The confidence values ofthe above association rules are then calculated. Take the first association rule as an example. The fuzzy counts ofA .Middle, and A4.Middle are calculated as 4.67 and 3.34.

The confidence of the association rule " IfA1 = Middle, then A4 = Middle" is then calculated as:   (A.Middle n A4 Middle 3.34 p= = - = 0.d715  (AI.Middle ) 4.67 p =1  Results for the other rule is shown below.

" If A4 = Middle, then Al = Middle " with a confidence of  0.626.

STEP 14: The confidence values of the above association  rules are then compared with the predefined confidence threshold 2. Assume A is set at 0.65. The following rule is thus output to users:  1. IfAI = Middle, then A4 = Middle, with a confidence factor of 0.72.

This rule can be easily explained as if the value of a data point is middle, then the value of a data point after three time units will be also middle with a high probability. The above rule can thus be used as the meta-knowledge concerning the given time series.



V. EXPERIMENTAL RESULTS AND DIscussIoNs In this section, the experiments made to show the  performances ofthe proposed method are described. They were implemented in Visual C++ 6.0 at a personal computer with Intel Pentium IV 3.00GHz and 512MB RAM. The dataset consisted of the first 100 stock prices in Japan Nikkei 225 market [1 1] from June, 30, 1989. The sliding-window size is set at5.

Experiments were first made to show the relationships between numbers of association rules and minimum support values along with different minimum confidence values.

Results are shown in Fig. 2.

From Fig. 2, it is easily seen that the numbers of association  rules decreased along with the increase in minimum support values. Also, the curve of numbers of association rules with larger minimum confidence values was smoother than that of those with smaller minimum confidence values, meaning that the minimum support value had a large effect on the number of association rules derived from small minimum confidence values.

The relationship between numbers of association rules and minimum confidence values along with various minimum support values is shown in Fig. 3.

From Fig. 3, it is easily seen that the numbers of association  rules decreased along with an increase in minimum confidence    values. The curve of numbers of association rules with larger minimum support values was smoother than that for smaller minimum support values, meaning that the minimum confidence value had a larger effect on the number of association rules when smaller minimum support values were used. All of the various curves approximate to 0 as the minimum confidence value approached 1.

I 30  IE 20   0.1 0.15 0.2 0.25 0.3  - coif=0 3 - wnf=0.5 * coi60.7- conflJ.9 - confl _  Fig. 2. The relationship between numbers ofrules and minimum supports.

ofnumbers ofrules are not high.

At last, experiments were made to compare the numbers of  rules generated with and without Step 12 of removing redundant large itemsets. The results are shown in Fig. 5. From Fig. 5, it can be easily observed that removing redundant large itemsets during the mining process has its efficacy. Without this step, too many redundant rules may be generated and may make users confused.

to 200  0 50  5.9 1  With 9emoVinzRaeJan-.Step --- Wi&tht RnUtRMddantStep  C.3 0.5 0,7 Confidence  Fig. 5. The numbers of rules between with and without removing redundant step  -- sup=0.1 nsup=].15 -sup-0.20 -r- sLp=0.25 x -op=0.30 Fig. 3. The relationship between numbers of rules and minimum confidences.

Fig. 4. The relationship between numbers ofrules and minimum confidences among different sliding-windows.

Fig. 4 shows numbers of rules along with different sliding-window sizes, when the minimum support is set at 20%.

As we expect, when the size of sliding-windows increases, the number of rules also increases. However, the increasing ratios

VI. CONCLUSION AND FuTuRE WORKS In this paper, we have attempted to use the data mining  technique to analyze time series. We proposed a fuzzy time-series mining algorithm that integrates the fuzzy sets, the Apriori mining algorithm, and the time-series concepts to find out appropriate linguistic association rules. The proposed approach first uses a sliding window to generate continues subsequences from a given time series and then analyzes the fuzzy itemsets from these subsequences. Appropriate post-processing is then performed to remove redundant patterms. Experiments are also made to show the relationships between numbers of association rules, minimum supports and minimum confidences. Since the final results are represented by linguistic rules, they will be more friendly to human than quantitative representation. The rules thus mined exhibit quantitative regularity in time series and can be used to provide some suggestions to appropriate supervisors. They can be used in two ways, prediction and post-analysis. For instance, ifa rule like "IfA1 is Low and A3 is Middle, then A5 is High " is mined, it can be used to predict the behavior ofA5 fromA, and A3. On the contrary, if a rule like "IfAl is Low and A5 is High, then A3 is Middle " is mined, it can be used for post-analysis. The proposed approach thus provides another altemative to analyze time series.

Although the proposed method works for time series, it is just a beginning. There is still much work to be done in this field. Our method assumes that the membership functions are known in advance. In [17, 18], we proposed some fuzzy leaming methods to automatically derive the membership functions. In the future, we will attempt to dynamically adjust the membership functions in the proposed mining algorithm to avoid the bottleneck of membership-function acquisition. We will also continuously attempt to enhance the mining algorithm for more complex problems.

