Mining Approximate Frequency Itemsets over Data Streams based on D-Hash  Table

Abstract?Frequent itemsets (or frequent pattern) mining, which is the basic step during data stream mining, has been paid more and more attention by researchers. Because of the uncertainties and continuities of data stream, the time- efficiency and space-efficiency of many mining algorithms are unaccepted. In this paper, hashed table is introduced to represent the synoptic data structure. By this way, the memory footprints in Lossy Counting algorithms can be reduced. In addition, the algorithm of frequent itemsets mining based on D-Hashed Table (MFS-HT for short) is proposed to obtain the items whose frequency count exceeded a user-specified threshold in data streams. Comparing with Lossy Counting and a similar algorithm called Mining Frequent Item sets over data Streams by Matrix (MISM for short), the experiment result shows that MFS-HT is more effective both in time and space efficiency.

Keywords-data stream; frequent itemset; D-HT; MFS-HT

I. INTRODUCTION With the gradual popularization of Internet applications,  more and more computer data analysis involved data stream.

For example, Web server log, data feeds from sensor networks monitoring, network traffic monitoring data, stocks exchange data, click stream, weather and environmental testing and so on.

The concept of data stream, different papers have similar definitions [1-3]. In short, data stream is a series of unpredictable transactions whose continuously arrive.

Compared with datasets stored in databases, data stream is real-time, continuous, orderly, infinite, and the searching tasks in data stream must be correct real-time, usually only one or very few times scans are allowed. Therefore, it is very difficult to implement frequent itemsets mining in data streams efficiently.



II. RELATED WORK C. Giannella, Jiawei Han and Jian Pei presented FP-  stream algorithm [4] based on FP-growth, and it was effective when the average length of transaction and itemsets in data streams was short. On the contrary, its performance would descend rapidly. Charikar M and K Chen, represented a algorithm called  Count Sketch[5] which has O k ?2? log N ??  Space complexity, could computes the frequent itemsets those frequency counting exceed  1 k 1?  with probability at least 1 ? . Literature [6] represented ?groupTest?, further reduce the complexity of space to  O k log k log 1 ?? log M . The literature [7] represented ?Lossy Counting? based on sampling. It was an effective solution to mining frequent 1 - set of issues. And in the same literature, the expand algorithm to mining multi- itemsets was also represented. The literature [8] introduced partial data stream to estimate the frequent itemsets based on the theory ?Hoeffding Bound?. The literature [9] made a sliding window in the mining frequent itemsets.

In this paper, an improved data Stream frequent itemsets mining algorithm, called MFS-HT (Mining Frequent Itemsets over Data Streams by Hashed Table) is proposed.

MFS-HT introduces hash and sampling technique, based on Lossy Counting [7] (LC for short) which is an effective approximate frequent itemsets mining algorithm using sampling. In order to reducing space complexity and the time of data-processing, a special hash table is used to replace the summary memory data structure D-tree in LC. Our contributions are:  ? Reduce the memory cost; ? Simplify the projection relation of logical data  structure and physical data structure; ? Introducing new mining strategy, reducing the  complexity of time The experiment result showed that our algorithm is more  effective both in time and space efficiency than the two algorithms mentioned above.



III. PROBLEM DEFINITION Let I I , I  , ? , I  be a set of items which likely  appear in the data streams. Let T TID, x , x , ? , x , xI 1 i n be a set of transaction items, in while n is the size of the transaction set. An itemset is a non-empty set of items. An itemset with size k is described as k-itemset. A transaction data stream TDS T , T , ? , T is a continuous sequence of transactions, where N is the TID of latest incoming transaction TN.

Definition I (Frequent Itemsets) Define that the support of an itemset X over TDS, denoted as sup(X), is the number of transaction in TDS containing X as a subset. An itemset X I  is called a frequent itemset only if  sup X s ?|TDS|, where s is a user defined minimum support threshold  DOI 10.1109/SNPD.2009.29    DOI 10.1109/SNPD.2009.29     in the range of 0, 1  and |TDS| is the number of transactions in current data streams.

In fact, it?s unrealistic to store all the data into limited main memory or even in secondary storage. And it is very difficult to compute the precise frequency of an itemset, since the current frequent itemset maybe infrequent in a short time. Therefore, while mining in data streams, we usually mine approximate frequent itemsets as alternative.

Definition II (Approximate Frequent Itemsets) an itemset X Imeans a frequent itemset only if  sup X s ? ?|TDS|, where s is a user-defined minimum support threshold in the range of 0, 1 ,  ? 0, 1  is an error parameter user- defined such that  ? s , and |TDS| is the number of transactions in current data streams.



IV. THE LOSSY COUNTING ALGORITHM Let?s describe the original extend LC first [7]. Constant  N denotes the current length of the stream. The two parameters user-defined: minimum support threshold s and the probability of error . Divide the incoming data stream into buckets, each bucket has the same size  ? 1?  of transactions Bid denotes the unique integer label called bucket id for each bucket and starts count from 1. B denotes the current bucket id. In the algorithm, instead of processing one by one, the transactions will be processed in batch when the main memory has had been full with transactions. ? denotes the current number of buckets in main memory (see more details in [7]).

The data structure D is a set of entries of summary data set in main memory.  The entries are form of  I , f, ? , where in {} is an itemset, f is an integer represents its frequency counting in current data stream, and ?  is the maximum possible error in f.

Initially D is empty. After main memory is full of transactions, the batch process starts and D is updated as follows:  1) Update Operation: For each  entry itemset, f, ?   in D, update the frequency  f by counting the occurrences in current batch transactions. If the entry being updated satisfies entry f B  then delete it from D.

2) Insert Operation: If an itemset whose frequency counting satisfies f ??  and there is no entry in D, then creates a new entry itemset, f, ? B , and insert into D.

In the next section, we describe our implements of D, reconstructed the original data structure D in LC in order to improve the efficiency of insert, update and search operations.



V. THE ALGORITHM MFS-HT PROPOSED In this section, the algorithm MFS-HT is proposed. The  algorithm uses the mechanism of error control used in LC to control and prune the approximate frequent itemsets. Our implement of D structure is described firstly.

A.  D-HT Benefiting from the original LC, it?s easy to see that the  efficiency of algorithm depends on, to a great extent, the efficiency of searching and updating the data structure of D.

LC maintains the entries with tree structure. Although tree is effective to search, but the maintenance of the relationship between tree nodes and preservation of these relations result in great time and space cost and cannot be ignored in data streams mining.

For cutting down the time and space consuming, a new structure ? D-Hash Table (D-HT for short) is introduced.

Hash table, or a hash map, is a data structure that associates keys with values. Hash tables support the efficient searching, inserting and deleting of elements in constant time on average (O 1  in the number of accessed memory cells).

Algorithm: f denotes a hash function; ADDRESS denotes the start address of data stream stored area in main memory; L denotes the allocated memory maximum capacity of entries, to make sure that the results within the range of restrictions. LIST which initially is empty denotes the maximum itemset in the data stream, and it means that all transactions must be the subset of LIST, or equal to LIST.

Etc. 2-itemset mining task, initially, D-HT is empty.

During batch processing, whenever a transaction is in processing, first get all the 2-itemset of current transaction, and for each 2-itemset, perform two step of search operation.

1) Search LIST to find whether there is an entry contains the 2-itemset in D-HT. If it is NOT, then joins the 2-itemset with the maximum itemset LIST and go to step 2); otherwise, go to step 3).

2) Insert Operation. Create a new  entry 2itemset, 1, B ? , and find an empty memory location to store. Go to step 4).

3) Update Operation. Search D-HT to locate the entry that contains the itemset and should be updated. The locate function depend on the hash function. Etc. a 2-itemset I I , call the hash function f i, j , and the returned integer named OFFSET, where denotes the offset from ADDRESS, in size of entry. And then access the memory to find out whether there is already an entry in store.

4) If the batch processing is not finished, then go to step 1), else the construction of D-HT is finished, prune current D-HT, and remove the entries those satisfy following inequality entry f ? N B.  Simple Example  A D-HT generated from a data stream which contains nine transactions (Table 1 below, as same as transactions in [14]), is shown in Figure 1. Based on the assumption that the size of bucket at this time is much larger than 16, the current bucket id is 1. Using a simple hash function like following:  A ?5  1 key i ? 37 j ? 7 17 f i, j L ? key ? A key ? A  Where i, j  represents the label of the 2-itemset, its value can be an integer which denotes the sequence index of each item in LIST, or some kind of data with a clear semantic, String etc. In this example, it is just integer. L is the capacity     of entries in allocated memory area. The result value of function will be the offset from base memory address in main memory (the offset is in size of each entry or in an element).

The chosen of hash function should follow two principles:  1. Low collusion probability; 2. The result value must fall on a certain interval. L as  mentioned above etc.

Follow the algorithm in 5.1, the D-HT will be like figure  1.When get the same result value of hash function, the entry will be stored in the first empty location in a higher address.

The situation is called collusion, which will be discussed in section 7.

Figure I. D-HT  TABLE I. DATA STREAM OF TRANSACTION  TID Itemset T100 I1, I2, I3 T200 I2, I4 T300 I2, I3 T400 I1, I2, I4 T500 I1, I3 T600 I2, I3 T700 I1, I3 T800 I1, I2, I3, I5 T900 I1, I2, I3  ? ?.

Data From [14] section 5.2  C. Mining with MFS-HT D-HT stored all frequent itemsets, their frequency  counting and the potential frequent itemsets in data stream.

Clearly, after the D-HT structure is constructed completely, the data stream mining task changes into the problem that list the entries in D-HT whose frequency are over user defined support threshold s and output them. The specific mining process is described as follows:  (1) To deal with 1-itemset. Treat the 1-itemset most like 2-itemset in our algorithm, expand the item to insert and update the entry. For example 1-itemset  I , treat it as .

Access the entry in D-HT using mapping function ,  . If the frequency of entry estimated is not less than , it means that entry itemset is frequent 1-itemset.

(2) Then traverse all 2-items subset of LIST. Treat as a starting point, for each subset  of LIST, get the corresponding , , ?  in D-HT. If  satisfies , it means that is a frequent 2-itemset.

(3) When exists a frequent 2-itemset  ,  in known itemsets, its frequency is not less than s  (recorded as  , , ditto). If also exists  ,  and  , , satisfies  ,  and  f I ,I s , then 3-itemset , ,  is a frequent 3-itemset whose frequency exceeds minimum support threshold. If  , ,  ,, ? ,  , ,  f I ,I s , and these itemsets can compose a n-itemset which exists in data stream transactions, then n-itemset , , ? , ,  is a candidate frequent n- itemset.

After the processing described above, all candidate frequent itemsets can be mined out from data stream.

Algorithm 1: MFS-HT Input: 1) The user defined minimum support threshold s; 2) The user defined maximum error probability ; 3) [Hash function f?] (Optional)  Output:  Set of frequent itemsets SET FUNCTION f  //default implements of hash function SET    f  f || f? FUNCTION MFSHT  FOR  i  1 to n IF DHT f II, II   s  SET  SET  II; ENDIF FOR j i 1 to n  IF DHT f I , I   s SET SET  I , I ENDIF DFS DHT f I , I ENDFOR ENDFOR ENDFUNCTION  FUNCTION  DFS entry    T      IF entry f  s   T  T  last element of entry itemset   DFS DHT f I , I   IF DHT f I , I f  s          SET  SET  T  I          RETURN SET           ENDIF    ENDIF ENDFUNCTION Using Algorithm 1 to mine frequent itemsets in D-HT  that be shown in Figure 1, and request 2 as minimum support threshold. The result is shown as following: the candidate frequent 1-itemsets are  I , I , I , I , I ; the candidate frequent 2-itemsets are I , I , I , I , I , I , I , I , I , I , I , I . And the candidate frequent 3-itemsets are I , I , I , I , I , I .



VI. EXPERIMENTS In this section, we build up a simple data stream  environment to simulate a real-time data analysis system. We carried out experiments on two DELL OPTIPLEX GX520 PCs, with configuration of 2.80 GHz Pentium 4 CPU, 1 G RAM, 80G hard drive, Linux Kernel Version 2.6.11. One is acting server and the other is client. The Experiment selected LC [7] and MISM [10] algorithm as a comparison in this paper. And Java program language was used to implement the two algorithms. Algorithm required hash function selected MD5 (Message Digest Algorithm 5) algorithm.

Each experiment ordinal distributes 128M; 32M RAM as D- HT storage area and fixes the size of entry in 512 byte.

Intercept last 18 bits and 16 bits of the 128 bits MD5 generated numerical as offset from the base address of storage area in main memory.

In our experiment, the same date streams with the size of 1 million are used those mentioned in literature [7]. One has an average transaction size of 10 with average large itemset size of 4. The other has average transaction size 15 and average large itemset size 6. The names of the datasets are also following names in [7] T10.I4.1000K and T.15.I6.1000K, where the three numbers denote the average transaction size (T), the average large itemset size (I) and the number of transaction respectively. Items were drawn from a universe of  10K unique items. In our experiments, we always fix 0.1s  (one-tenth of s). The server stores transactions of two data stream in oracle 8i database, and sends all data to client in 10s.

A.  Experimental Results (a) Figures for Data Set T10.I4.1000K   Figure II. Times Taken Comparison for Data Set T10.I4.1000K with  128M of Buffer Size   Figure III. Times Taken Comparison for Data Set T10.I4.1000K with 32M  of Buffer Size  (b) Figures for Data Set T.15.I6.1000K   Figure IV. Times Taken Comparison for Data Set T.15.I6.1000K with  128M of Buffer Size   Figure V. Times Taken Comparison for Data Set T.15.I6.1000K with 32M  of Buffer Size        0.001 0.003 0.005 0.007 0.009  Varying Support Threshold s Buffer 128M  Lossy Counting MISM MFS-HT  Ti m  es ta  ke n  in se  co nd  s  Support Threshold s      0.001 0.003 0.005 0.007 0.009  Varying Support Threshold s Buffer 32M  Lossy Counting MISM MFS-HT  Ti m  es ta  ke n  in   se co  nd s  Support Threshold s         0.001 0.003 0.005 0.007 0.009  Varying Support Threshold s Buffer 128M  Lossy Counting MISM MFS-HT  Ti m  es ta  ke n  in se  co nd  s  Support Threshold s   0.001 0.003 0.005 0.007 0.009  Varying Support Threshold s Buffer 32M  Lossy Counting MISM MFS-HT  Ti m  es ta  ke n  in   se co  nd s  Support Threshold s     B. Analysis For data set T10.I4.1000K which has shooter average  length of transactions and smaller set of unique items, it?s obviously shown in Figure 6.1-6.2 that the MFS-HT algorithm is more effective than LC and MISM. MFS-HT algorithm?s smaller space complexity and fast hash memory access strategy has brought considerable improvement in time complexity. Comparing with LC, MFS-HT don?t have to maintain the complexity of tree structure, and the memory access strategy is effective and simple -- use hash function for 1-1 mapping, does not need or rarely need times in iterate entries. For Comparing with MISM, MFS-HT also has an advantage in performance. Especially in the case of a smaller buffer (Figure 6.2), it can be seen that MFS-HT and LC?s performance are superior to MISM, and they have a more smooth performance curve. MISM using 2-dimensions linear matrix maintain the data structure, inserting and deleting entries needs to move a lot of entries to retain the relationship of matrix?s horizontal, vertical coordinates to the specific 2-itemset [10]. Therefore, along with the requested support threshold s changed, the MISM?s performance curve has evident wobble. Because of the arrival order of transactions in random, the algorithm will perform with great uncertainties.

For data set T.15.I6.1000K which has longer average length of transactions and bigger set of unique items, the performance of three algorithms have different degrees of decline. The three algorithms all use known 2-itemsets to find multi-itemsets. Joining and checking the validity of joined multi-itemsets are all CPU burst operations. Along with the increasing of average length of transactions and size of the unique items? set in data stream, computing multi- itemsets? time complexity will be O N , where N denotes the number of 2-itemsets. Compared with other algorithms, MFS-HT still has advantages of lower space complexity and effective memory access strategy.



VII. FURTHER DISCUSSING  A. Hash Collision In V we discussed a bit rough the strategy of MFS-HT  algorithm solving the hash collision problem. In computer science, a hash collision or hash clash is a situation that occurs when two distinct inputs into a hash function produce identical outputs. In other words, for a given hash function H, and a set of values {x, y} (x y), if H(x) = H(y), then we conclusion that hash function H has occurred a hash collision for x and y.

Our algorithm uses hash to buildup memory storage. The probability of hash collision will directly impact the performance. But there many excellent hash functions with very low probability of hash collision can be used in our algorithm. Like MD5 in our experiments, it is sufficient to guarantee the necessary strong collision resistance in applications. Although MD5 is also can be created artificial collision, more details were in [11], [12] and [13]. Robert J and Jenkins Jr. also proposed a hash function [15] for table lookup using 32-bit or 64-bit arithmetic. These hashes are  fast and have low collision probability. In literal [15] also give a framework to evaluating hash functions by funnel [16]. The details of evaluating of hash functions are more areas of cryptography, here is no longer too much to start.

More reliable hash function is certainly will bring more time complexity. The trade-off principle is more depends on L. If L is large, a simpler hash function can be chosen; otherwise, the additional time cost cause by hash collision cannot be ignored. But as the development of the hardware of computers, the memory can be cheaper than before, L will be very large.

B. Future Work The usage of hash function in our algorithm is designed  to be pluggable. We are trying to introducing multi-hash functions cooperated together, reducing the hash collision and enhancing the performance.



VIII. CONCLUSIONS In this paper, we try to improve the performance of LC  by changing the key data structure in memory, reducing the memory storage cost and access time cost, to enhance the efficiency of the algorithm. An algorithm based on LC algorithm does determine mechanism for frequent itemsets and error control mechanism in data stream mining was proposed. And the implement of key data structure D by introducing Hash Table named D-HT to solve the efficiency problem of data accessing and addressing was described also. The experiment result showed that our algorithm is more effective both in time and space than the two algorithms mentioned above. Along with increasing of the average length of transactions and set of unique items in data stream, there will be certainly lose in performance. But it still very effective in non-dense data stream mining and also very effective than some algorithms in mining dense data streams.

