TWO REVISED ALGORITHMS BASED ON APRIORI FOR MINING  ASSOCIATION RULES

Abstract: Association rule mining is concerned with the discovery  of interesting association relationships hidden in databases.

Traditional algorithms are only considering the constraints of minimum support and minimum confidence. However, sometimes it is essential to find stronger association rules for decision makers possessing inadequate resources, and sometimes less strong rules are needed. In this paper, we propose two revised algorithms based on Apriori considering the constraints of three factors: minimum support, minimum confidence and minimum interest. In order to reduce the times of scanning a database, we adopt a matrix structure in our algorithms.

Keywords:  Data mining; Association rule; Matrix  1. Introduction  Mining association rules from a transaction database has been studied extensively. The problem was first introduced by Agrawal et al.[1,2],who defined it as finding all rules from transaction data with the constraints of minimum support and minimum confidence. An association rule is an implication of the form A?B, where A and B are frequent itemsets in a transaction database and A ? B= .Such a rule reveals that transactions in the database containing items in A tend to contain items in B, and the probability, measured as the fraction of the transactions containing A also containing B, is called the confidence of the rule. The support of the rule is the fraction of the transactions that contain all items in both A and B. Agrawal et al.

decomposed the mining process into two phases. In the first phase, candidate itemsets are generated and counted by scanning the transactions. If the number of an itemset in the transactions is larger than or equal to the predefined value of minimum support, the itemset is considered as a frequent itemset. The process is repeated to find all frequent itemsets.

In the second phase, all associations rules satisfying the  predefined value of minimum confidence are generated from the frequent itemsets found in the first phase.

?  Due to the great success of the algorithm above, varieties of algorithms for mining association rules have been proposed. Srikant and Agrawal proposed a method to mine association rules from those with quantitative and categorical attributes [3].Brin et al. studied the correlations of association rules [4]. Dong and Li tried to efficiently mine long patterns from database [5]. With the rapid development of data mining techniques and tools, many researchers focus on finding alternative patterns.

Padmanabhan and Tuzhilin discussed unexpected patterns [6]. Suzuki, Liu et al., Hwang et al. and Hussain worked on exceptional patterns [7-10]. Wu et al. and Yuan et al.

studied negative association respectively [11,12]. Also, quite a few researchers proposed fuzzy mining algorithms based on fuzzy set theory in recent years [13-16].

The rest of this paper is organized as follows. In Section 2, we present the formal definition of the promblem . In Section 3, we proposed two revised algorithms based on Apriori considering the constraints of minimum supports, minimum confidence and minimum interest of association rules. In Section 4, we illustrate our two algorithms using a concrete example. Finally, we conclude this paper with a summary in Section 5.

2. The problem definition  In this section, we introduce the basic concept of strong, stronger and less strong association rules and the function interest.

The problem of mining association rules is defined as follows . Let be a set of literals called items. Let  be a set of transactions where any transaction is a set of items such that .Each transaction is associated with a unique identifier TID. A transaction,  { miiiI ,,, 21= } D  DT ? IT ?  DT ?  contains a set of items or an       itemset, , if . An association rule is an expression of the form , where , , and  IX ? TX ? YX ? IX ? IY ?  ?=?YX .The rule holds in D with support, sup , and confidence, conf  = .

YX ? )()( YXPYX ?=? X( ?  )Y )  }  } kII ,1 D   } }  )  }  |( XYP Traditionally, only two constraints (minimum support  and minimum confidence) are used to prune the k-itemsets generated in the process of mining association rules.

However, there are two exceptions traditional algorithms can not satisfy decision maker?s needs.

a. For decision makers possessing inadequate resources, frequently it is essential to find stronger association rules than usual .

b. For those possessing abundant resources, less strong rules than usual are often needed in order to provide more useful association rules .Because there could also exist useful association rules in the infrequent itemsets besides frequent itemsets in a database.

Definition 1.

Let mini_sup and mini_conf denote the predefined minimum support and minimum confidence, respectively.

An association rule is strong if sup( ) mini_sup and conf( ) mini_conf.

YX ? YX ? ? YX ? ?  Then, the mining of association rules with traditional methods is concerned with finding all the strong rules in a database.

To search for stronger and less strong association rules efficiently from a database, we could integrate the constraint of the function interest introduced by Wu et al. in [11] into the traditional support-confidence framework.

According to probability theory, X1 and X2 are independent if . So the rule  is less interesting if the value of sup is very small.

)()()( 2121 XPXPXXP ??  21 XX ? )( 21 XX ? )sup()sup( 21 XX?  Definition 2.

interest .If interest min_inter, where min_inter is a predefined value, then itemset  is referred to as a potentially interesting itemset.

),( 21 XX ??= )sup( 21 XX )sup( 1X )sup( 2X ?),( 21 XX  21 XX ?  Definition 3.

interest  2.If interest min_inter, where min_inter is a predefined value, then itemset  is referred to as a potentially interesting itemset.

???= )sup(),,( 11 nn XXXX )sup( 1X ),sup( nX ?n ?),,( 1 nXX  ?1X  nX? Definition 4.

Let mini_inter denote the predefined minimum interest mini_inter. An association rule is stronger if  sup( ) mini_sup , conf( ) mini_conf  and interest( ) mini_inter.

YX ?  YX ? ? YX ? ? YX ? ?  Definition 5.

An association rule is less strong if sup( ) mini_sup , conf( ) mini_conf or sup( )>0 , conf( ) mini_conf and interest( ) mini_inter .

YX ? YX ? ? YX ? ? YX ? YX ? ?  YX ? ?  3. Algorithms for mining stronger and less strong association rules  In the following, we propose two revised algorithms based on Apriori considering the constraints of support, confidence and interest.

While Apriori must scan a database many times to find all the large items, our algorithms scan a database only once by adopting a matrix structure. It refers to [17] Definition 6.

{ miiiI ,,, 21= be a set of literals. Let  be a set of transactions in a database, , in  which  D  n Tnjjjj tttV ),( 21=  ??  ? ? ?  ?  ? =  Tii  Tii t  j  j ij ,0  ,1 , ; Then  could be  written as .

nj ,2,1= D  ),,( 21 mVVVD = Theorem 1.

Let { } is the 1-itemset of , then the support count of in ,  ,,1, mkI k = D { }kI D  ? =  = n  i ijk tIcount   )( , . mk ,1=  Theorem 2.

Let { is a k-itemset of , then the support count of in ,  kII ,1 D { }  ?? ==  = k  j ij  n  i k tIIcount  1 )( , . ?k  Proof .  According to Apriori, only when all single items of { appear in the ith transaction Ti, the value of support count of  increases by one. According to Definition 2, when  appears in Ti, then  =1, or else =0. So only when all appear in Ti,  increases by one. Thus, the  support count of {  count  =  kII ,1 { kII ,1  )1( kjI j =  ijt ijt kII1  ? =  = k  j ijt   ,1 1( kIIcount  kII ,1 )( 1 kII       ?? ==  k  j ij  n  i  t  .

Our algorithms are based on Apriori, so we can utilize two facts from it.

Fact 1.

Let Ck is the candidate k-itemset, a k-itemset ,  , then .

kCc ?  csLs k ??? ? ,1 kLc ? Fact 2.

l is a large itemset, if min_conf, then the rule  is interesting.

,, ??? sls  ?)(/)( scountlcount )( sls ??  3.1. Algorithm for mining stronger association rules (AMS for short)  Input: minimum support mini_sup; minimum confidence mini_conf; minimum interest mini_inter  Output: association rules Step 1.  Scan the database, construct a matrix, and initialize it according to Definition 6. For example, if I2 is an item in the first transaction T1, that is 12 TI ? , then  , if not, then  .In this way, the value of each item in the matrix must be either 0 or 1.

112 =t 012 =t  Step 2. Calculate the support count of each 1-item Ij, , ),1( mj = according to theorem 1. Then, generate large 1-itemset . { }mIIL ,,11 = Step 3. By joining  to , we get candidate k-itemset C  1?kL 1?kL )2( ?k k , calculate the support count and interest of  each k-item according to Theorem 2 and Definition 3, respectively. For each ( is a candidate k-itemset), if  mini_sup, or interest ( ) < mini_inter, then delete from according to Fact 1.Then, generate large k-itemset . Repeat Step 3 until large k-itemset .

kCc ? kC <)sup(c c  c kC  kL ?=kL Step 4. Calculate confidence of association rules generated from . Then, find out all the interesting rules according to Fact 2.

1?kL   3.2. Algorithm for mining less strong association  rules(AMLS for short)  Input: minimum support mini_sup; minimum confidence  mini_conf; minimum interest mini_inter Output: association rules Step 1.  Scan the database, construct a matrix, and initialize it according to Definition 6.In this way, the value of each item in the matrix must be either 0 or 1.

Step 2.  Calculate the support count of each 1-item  Ij, , ),1( mj = according to Theorem 1. Then, generate large 1-itemset . { }mIIL ,,11 = Step 3.  By joining  to , we get candidate k-itemset C  1?kL 1?kL )2( ?k k , calculate the support count and  interest of each k-item according to Theorem 2 and Definition 3, respectively. For each ( is a candidate k-itemset), if mini_sup and interest ( ) < mini_inter, then according to Fact 1, delete from .

If sup( )=0, according to Definition 5, also delete c from . Then, generate large k-itemset . Repeat Step 3 until large k-itemset .

kCc ? kC <)sup(c c  c kC c  kC kL ?=kL  Step 4.  Calculate confidence of association rules generated from . Then, find out all the interesting rules according to Fact 2.

1?kL  4. A concrete example  In this section, a concrete example about the supermarket shopping-basket analysis is presented to illustrate our algorithms.

Table 1. A set of transactions from a database   TID Items Bought  T1 I1,I2,I5 T2 I2,I4,I5 T3 I2,I3 T4 I1,I2,I4 T5 I1,I3 T6 I2,I3 T7 I1,I3 T8 I1,I2,I3,I5 T9 I1,I2,I3   A. According to the algorithm for mining stronger  association rules, we proceed the 4 steps in the following:  Step 1.  According to Definition 6, from Table 1 it is easy  to find , Tjjj ttV ),( 91= ? ? ?  ? ?  = TiIj TiIj  tij ,0 ,1  , j=1,2,3,4,5;  then       ( )  ? ? ? ? ? ? ? ? ? ? ? ?  ?  ?  ? ? ? ? ? ? ? ? ? ? ? ?  ?  ?  ==   ,,,, 54321 VVVVVD   Step 2. According to Theorem 1, the support count of each 1-item is calculated as follows:  ,6)1(  1 ==?  =i itIcount ,7)2(   2 ==?  =i itIcount  ,6)3(  3 ==?  =i itIcount ,2)4(   4 ==?  =i itIcount  3)5(  5 ==?  =i itIcount .

Then, we obtain large 1-itemset . { }543211 IIIIIL = Step 3.  Based on , by taking joining step of Apriori, we get candidate 2-itemset  .

1L {=2C ),31(),21( IIII  ),32(),51(),41( IIIIII ),42( II ),52( II ),53(),43( IIII })54( II  According to Theorem 1, the support count of each 2-item in C2 can be calculated as follows:.

.

,4)21(  21 ==?  =i ii ttIIcount ,4)31(   31 ==?  =i ii ttIIcount  ,1)41(  41 ==?  =i ii ttIIcount ,2)51(   51 ==?  =i ii ttIIcount  ,4)32(  32 ==?  =i ii ttIIcount ,2)42(   42 ==?  =i ii ttIIcount  ,3)52(  52 ==?  =i ii ttIIcount ,0)43(   43 ==?  =i ii ttIIcount  ,1)53(  53 ==?  =i ii ttIIcount 1)54(   54 ==?  =i ii ttIIcount  According to Definition 2, the interest of each 2-item in C2 can be calculated. For example interest (I1I2)  )2sup()1sup()21sup( IIII ?= = 9/79/69/4 ?? =6/81.Th e supports and interests of all the 2-items are listed in Table 2.

In table 2, the 2-items whose support  mini_sup  and interest  mini_sup are listed as follows: ( ), ( ), ( ).So, the large 2-itemset  .By taking joining step, we can get candidate 3-itemset C  ?  ? 21II  32 II )}  52 II =2L ( ) ( ) ({ 52,32,21 IIIIII  3 ( ){ }532 III= . ) is not included in ,so delete from .Then, C  53( II  2L ( )532 III 3C 3= . ? Step 4. Calculate the confidence of the candidate rules. For example, the confidence of  is calculated as follows: . Confidences of all the candidate rules are listed in Table 3.

21 II ? 6.03/2)1(/)21( >=IcountIIcount   Table 2.Minimum support and minimum interest of candidate 2-items   ( Ii Ij ) sup ( Ii Ij )  mini_sup (2/9)  interest ( Ii Ij )  mini_inter (0.05)  (I1 I2) 4/9 > 6/81 > (I1 I3) 4/9 > 0 < (I1I4) 1/9 < 3/81 < (I1I5) 2/9 = 0 < (I2I3) 4/9 > 6/81 > (I2I4) 2/9 = 4/81 < (I2I5) 3/9 > 6/81 > (I3I4) 0 < 12/81 > (I3I5) 1/9 < 9/81 > (I4I5) 0 < 6/81 >   Table 3.Confidence of candidate rules based on AMS   Candidate rules Confidence  21 II ?  2/3 12 II ?  4/7 32 II ?  4/7 23 II ?  2/3 52 II ?  3/7 25 II ?  1   Suppose the predefined minimum confidence (mini_conf) is 0.6. Then, according to the calculation above, we derive the association rules as follows:  ,21 II ? 25,23 IIII ?? .

B. According to the algorithm for mining less strong association rules, the result of the first two steps is the same as that of AMS, so we only proceed Step 3 and Step 4 as follows.

Step 3. In Table 2, the 2-items whose support mini_sup or interest  mini_sup are listed as follows:   . However, sup =0, sup =0, so  ? ?  ( ) ( ) ( ) ( ) ( ) ( ) ( ),43,52,42,32,51,31,21 IIIIIIIIIIIIII ( ),53II ( 54II ) ) )( 43II ( 54II       delete , .Then, the large 2-itemset .

Based on L  ( )43II ( 54II ) )53,52,42,32,51,31,212 IIIIIIIIIIIIIIL =  )}  } }  ( ) ( ) ( ) ( ) ( ) ( ) ({ } 2, we can obtain candidate 3-itemset:  . ( ) ( ) ( ) ({ 532,531,521,3213 IIIIIIIIIIIIC = According to Theorem 2 and Definition 3, the  supports and interests of all 3-items are calculated respectively, and all of them are listed in Table 4.

According to Table 4, the 3-items whose support mini_sup or interest  mini_inter are listed as follows:  .So, the large 3-itemset .By taking joining step,  we get candidate 4-itemset C  ? ?  )532(),521(),321( IIIIIIIII =3L { )532(),521(),321( IIIIIIIII  4= .

However, is not included in ,so delete  from .Then .

{ )5321( IIII )531( III 3L  )5321( IIII 4C ?=4C   Table 4.Minimum support and minimum interest of candidate 3-items   (IiIjIk) sup (IiIjIk)  mini_sup (2/9)  Inter (IiIjIk)  mini_inter (0.05)  (I1I2I3) 2/9 = 10/81 > (I1I2I5) 2/9 = 4/81 < (I1I3I5) 1/9 < 3/81 < (I2I3I5) 1/9 < 5/81 >   Table 5.Confidence of candidate rules based on AMLS   Candidate rules Confidence  321 III ??  1/3 312 III ??  2/7 213 III ??  1/3 321 III ??  1/2 231 III ??  1/2 132 III ??  1/2 521 III ??  1/3 512 III ??  2/7 215 III ??  2/3 521 III ??  1/2 251 III ??  1 152 III ??  2/3 532 III ??  1/7 523 III ??  1/6 325 III ??  1/3 532 III ??  1/4 352 III ??  1/3 253 III ??  1    Step 4. Since the predefined mini_conf is 0.6, according to the calculation in Table 5, we derive the rules as follows:  , , ,  215 III ?? 251 III ?? 52 III ?? 253 III ??1 C. According to the classic Apriori, the association rules generated from it are described as follows: I5 ? 1I ? ,2I ,251 III ?? ,152 III ?? ,21 II ?  25,23 IIII ?? Compared with the classic Apriori, AMS generates  stronger association rules: .

Obtaining such information, decision makers possessing inadequate resources, e.g. human resources, could put more resources into such items as above to earn more. On the other hand, AMLS generates less strong association rules compared with those by Apriori. For example,  suggests that shoppers who purchase items I3,I5 are also likely to purchase items I2 at the same time.

However, the rule can't be discovered in Apriori. With AMLS, it is quite promising to produce rare but informative item rules besides rules generated by frequent items. This allows decision makers with abundant resources utilize their resources efficiently.

,23,21 IIII ?? 25 II ?  253 III ??  5. Conclusions  In this paper, we investigated the problem of mining stronger and less strong association rules from a database.

The classic Apriori is only considering the constraints of minimum support and minimum confidence. With the function interest, we presented two algorithms, AMS and AMLS, considering the constraints of three factors: minimum support, minimum confidence and minimum interest. In order to reduce the times of scanning the database, we adopt a matrix structure in our algorithms.

With a concrete example, we compare the different results of Apriori, AMS and AMLS and analyze their implication in reality. By comparison, AMS and AMLS could better satisfy the needs of different decision makers.

Acknowledgements  The work was partly supported by the National Natural Science Foundation of China (70671004, 70401006, and 70521001), Beijing Natural Science Foundation (9073018) Program for New Century Excellent Talents in University (NCET-06-0172) and A Foundation for the Author of National Excellent Doctoral Dissertation of PR China (200782).

