The Fast Itemset Miner: A Detailed Analysis of the Candidate Generation Stages

Abstract? Association Rule Mining (ARM) represents an important tool for Data Mining techniques. A key step in ARM is to determine the frequent itemsets present within the analyzed data. Recent algorithms addressing this problem focus on identifying frequent itemsets without candidate generation steps. This paper details a new algorithm ? Fast Itemset Miner (FIM for short) ? that still relies on candidate generation stages, but has a different approach from the standard Apriori algorithm. This approach focuses on increasing size of the interval that candidates belong to, rather than increasing the size of the candidates by one with each corresponding iteration.

By comparing the Apriori and FIM candidate generation stages, we show that this second approach uses a faster and more efficient method of determining candidate itemsets. Moreover, such an approach also favors a better support counting method, which greatly impacts on the overall time response of the FIM algorithm.

Index Terms? Data Mining, Association Rule Discovery, Frequent Itemset Mining, Candidate Based Algorithms

I. INTRODUCTION  Association Rule Mining (ARM) refers the problem of identifying all relations between the elements of a given data set so that the presence of a given element/set of elements implies the presence of another element/set of elements [1], [2], [3], [4]. This process usually involves two distinct stages: the first one involves determining all frequent itemsets present within the collection of transactions, while the second stage aims to find strong association rules based on the previously identified itemsets [1], [5]. The first stage is generally considered to be the most important one with respect to the overall efficiency of the ARM algorithms.

The first notable algorithm to efficiently solve the issue of frequent itemset mining is the Apriori algorithm (1994) [6], [1], [5]. The algorithm performs multiple scans over the target transaction database as follows. The first such scan determines the frequent items with respect to the minimum support limit given by the user. All subsequent scans involve two stages: the first one ? the so called join stage ? is to identify all possible k-itemset candidates based on the frequent (k?1)-itemsets found in the corresponding previous scan; the support for each candidate is then computed in the second stage and all candidates that do not meet the minimum support value are removed ? this stage is known as the pruning stage. The key aspect of this algorithm is  A. Archip is with The Faculty of Automatic Control and Com- puter Engineering, The ?Gheorghe Asachi? Technical University of Ias?i alexandru.archip@cs.tuiasi.ro  M. Craus is with The Faculty of Automatic Control and Com- puter Engineering, The ?Gheorghe Asachi? Technical University of Ias?i craus@tuiasi.ro  the efficient generation of candidates. An optimal approach must consider the Apriori Property [6]: If an itemset is frequent, then all of its subsets must also be frequent. Should this restriction be met, Agrawal et al. state that the Apriori algorithm achieves a linear time complexity with respect to the size of the transaction database [6].

Further analyses performed using the Apriori algorithm have shown that its main advantage of counting the support value only for the determined candidates could also prove to be its greatest weakness. The amount of candidate itemsets that is determined in the join step could be quite large and this could negatively influence the overall performance [7], [4]. Moreover, in such cases, methods that offer an efficient candidate selection may consume considerable more resources [1], [5].

Taking this into consideration, ulterior attempts to develop better and faster algorithms follow two different approaches.

The first one is to completely eliminate the candidate item- sets. One such remarkable algorithm is FP-Growth (first presented in [7] and revised in [8]). The algorithm builds prefix trees from the target transaction database by perform- ing exactly two scans of the database: one to find the frequent items and one to actually build the prefix tree. In order to determine all frequent itemsets, the prefix tree is traversed in a bottom-up manner [7], [8]. A much more recent algorithm ? called And Code (AC for short) ? is presented in [4].

The main idea behind AC is to binary code itemsets and transactions.

The second line of approaches to the itemset mining prob- lem has been to improve the standard candidate generation techniques for the Apriori algorithm. Attempts have been made to reduce the overall number of I/O operations involved [9] or to use different data structures in order to represent the itemsets and the candidates [10].

It can be noticed that almost no effort had been directed to investigating other means of generating candidate itemsets.

The present paper details a new method for identifying valid candidates for a relatively new algorithm ? Fast Itemset Miner (FIM for short) [3], [2]. In our previous work (papers [2] and [3]) little or no detail is given for the candidate generation stages used in FIM. The main goal of this paper is to show how simple and efficient is this new method of determining valid candidates. Section II describes the base sequential algorithm and the simple candidate generation stages for this algorithm. This section also includes a proof of correctness for this particular method of identifying can- didate itemsets. Section III includes a presentation of the generalized form for both the algorithm and the candidate    generation and a few details regarding the parallel version of the algorithm. Section IV will present a first set of comparative time results between FIM and Apriori, while the last section of the paper is dedicated to some final remarks and conclusions, as well as a few details on how this new algorithm could evolve.



II. THE BASE FIM ALGORITHM AND CANDIDATE GENERATION METHOD  The fundamental idea behind FIM is to determine new frequent itemsets by gradually increasing the size of the interval that includes the items contained within an itemset [2]. Let n be the total number of frequent items. The algo- rithm begins with n intervals, each such interval including a single frequent item. At a given stage k, new candidates will be generated from items belonging to the interval [i, i+ k] (having i ? {1,2, ...,n? k}), while for the subsequent stage k+1 new candidates will be determined by combining items belonging to [i, i+k+1] (having i? {1,2, ...,n?k?1}). The following notations will be used [3], [2]:  Fi, j?the set of all frequent itemsets that include items belonging to interval [i, j]  Ci, j?the set of all candidates that include items belonging to interval [i, j]  (1)  In [2] it is shown that new frequent itemsets in Fi, j contain both items i and j. As a direct consequence, the new candidates for interval [i, j] must contain both items i and j. Therefore, new candidates will be generated according to equation (2) [2], [3].

Ci, j = {X ?Y |(X ? Fi, j?1? i? X)?(Y ? Fi+1, j? j ? X)} (2)  The base FIM algorithm is given in Algorithm 1 [2], [3].

Algorithm 1 works as follows. A preprocessing stage is  required to determine all frequent items, trim transactions by discarding non-frequent items and sort the new trans- action dataset descending with respect to an items support threshold. Assuming n frequent items, in the first stage of the algorithm, new candidates will be generated based on the following subsets: {1,2},{2,3}, ...,{n? 1,n}. For the next stage, candidates will be determined from items belonging to {1,2,3},{2,3,4}, ...,{n?2,n?1,n}. This same procedure continues until, finally, new candidate itemsets will include items from the interval {1,2, ...,n}. One of the key advantages of this approach is that the FIM algorithm does not need to scan the whole transaction database in order to determine the support for any given valid candidate. In order to determine the support for all candidates included in Ci, j, the algorithm must scan only the transactions that include item i.

Since the FIM algorithm is a candidate based approach to the frequent itemset mining problem, a key issue that needs to be solved is to efficiently determine the new candidates for a given stage (line 4 in Algorithm 1). In order to better  Algorithm 1 Fast Itemset Miner Base Algorithm Require: Fi, j = set of frequent itemsets belonging to [i, j] Require: D = transaction set, t ? D  1: for k = 1 to n?1 do 2: for all i, j : i ? 1, ...,n? k; j = i+ k do 3: Fi, j? Fi, j?1?Fi+1, j 4: Ci, j = {X ?Y |(X ? Fi, j?1? i ? X)? (Y ? Fi+1, j? j ?  X)} 5: for all transactions t ? Di do 6: {Di denotes the subset of transactions in D that  include frequent item i} 7: Ct ? subset(Ci, j, t) {determine all candidates in  Ci, j that are include in transaction t} 8: for all candidates c ?Ct do 9: c.support++  10: end for 11: end for 12: Fi, j = Fi, j ?{c ?Ci, j|c.support ? minsupp} 13: Fk? Fk ?Fi, j 14: end for 15: end for  understand the solution we submit for this step of the base FIM algorithm, the following notations will be used for an itemset X ? Fi, j:  the maximal itemset N such that N ? X and N?{i}= N?{ j}= ? ? the core itemset of X  (3)  su f f ix(X) = X \{i} ? the suffix itemset of X pre f ix(X) = X \{ j} ? the prefix itemset of X .

(4)  Also, our solution relies on the following lemma, which is a direct consequence of the Apriori Property [6].

Lemma 1: Any given itemset X is not frequent if its core itemset (equation (3)) is not frequent.

Using Lemma 1 and considering line 4 in Algorithm 1, we can say that a candidate itemset is valid if its core is a frequent set. Moreover, each subset of a valid candidate T ? Ci, j that does not simultaneously contain items i and j must be a frequent itemset. The new method we use to identify valid candidates is based on the following theoretical result.

Lemma 2: Let T be a candidate itemset in Ci, j. T is a valid candidate if and only if it is obtained through joining two frequent itemsets X ? Fi, j?1 and Y ? Fi+1, j that simultaneously meet the following conditions:  i ? X s?i j ? Y (5) su f f ix(X) = pre f ix(Y ) (6)  Proof: Lets assume that there is a valid candidate T ? Ci, j that may be obtained from joining itemset X ? Fi, j?1 and Y ? Fi+1, j such that:  i ? X s?i j ? Y (7) su f f ix(X) 6= pre f ix(Y ) (8)  and T = X ?Y (9)    According to equation (3), the core itemset of T is:  NT = T \{i, j} (10)  According to equations (10), (1) and (8), candidate T may be written as:  T = {i}?NT ?{ j} (11)  If T is a valid candidate, then is core NT must be a frequent itemset (Lemma 1). Therefore, considering Lemma 1 and the Apriori Property, it results that:  itemset X ? = {i}?NT is frequent (12) itemset Y ? = NT ?{ j} is frequent (13)  Considering equations (12), (13) and (1), it results that:  X ? ? Fi, j?1 (14) Y ? ? Fi+1, j (15)  and that: T = X ??Y ? (16)  Furthermore, considering equations (12), (13) and (3) it results that:  su f f ix(X ?) = X ? \{i}= NT = pre f ix(Y ?) = Y ? \{ j} (17)  Equation (17) contradicts the assumption of equation (8).

Reciprocal, let X ? Fi, j?1, i ? X and Y ? Fi+1, j, j ? Y that  conform to equation (6). Let T =X?Y ?Ci, j be the candidate itemset that may be obtained by joining itemsets X and Y .

T may be written as:  T = {i}? su f f ix(X)?{ j}= {i}? pre f ix(Y )?{ j} (18)  According to equations (3) and (4), the core itemset of T is NT :  NT = su f f ix(X) = pre f ix(Y ) (19)  Since X and Y are frequent itemsets, then all their subsets are frequent itemsets (Apriori Principle [6]). This implies that itemset NT is frequent. Therefore, all subsets of candidate T that do not simultaneously include items i and j are frequent.

Therefore T is a valid candidate.

Algorithm 2 depicts the new base method for generating all candidate itmesets in Ci, j for the FIM base algorithm.

Algorithm 2 The candidate generation method for the FIM base algorithm Require: Fi, j?1, Fi+1, j  1: for all itemset X ? Fi, j?1 do 2: for all itemset Y ? Fi+1, j do 3: if su f f ix(X) = pre f ix(Y ) then 4: candidate ? X ?Y 5: add candidate ??n Ci, j 6: end if 7: end for 8: end for  In order to compare this new method against the standard Apiori method, we must first analyze the basic AprioriGen method given in Algorithm 3.

Algorithm 3 The standard Apriori candidate generation method Require: Lk?1 {the set of all frequent k?1-itemsets}  1: for all pairs (s.a, s.b) ? Lk?1?Lk?1 such that a < b do 2: candidate = s.a.b; 3: if all k?1 subsets of candidate are frequent then 4: add candidate to candidate list 5: end if 6: end for  Lets assume that the current stage for the Apriori algorithm is stage k = 5, i.e. all valid candidates of size k = 5 must be determined. Lets assume the target candidate is {1,2,3,4,5}.

According to Algorithm 3, this candidate is obtained by joining itemsets {1,2,3,4} and {1,2,3,5}. In order to vali- date this candidate, all its k = 4 subsets must be generated and all of them must be frequent 4? itemsets. Identifying all 4? itemsets include in {1,2,3,4,5} requires 5 steps.

Assuming n4 to be the total number of frequent 4? itemsets, we can state that validating candidate {1,2,3,4,5} requires (5 ? logn4) steps to complete.

When analyzing our method for generating the same can- didate (Algorithm 2), it can be observed that no supplemental steps are required for validating candidate {1,2,3,4,5}. This result is proven to be correct and is enforced by Lemma 2.

This example clearly outlines the simplicity of our method when compared against the standard Apriori approach.



III. THE GENERAL FIM ALGORITHM AND CANDIDATE GENERATION METHOD  In the previous section, the base FIM algorithm has been briefly presented. As it can be observed from Algorithm 1, the simple model assumes that initial intervals include only a single frequent item. If we consider n to be the total number of frequent items, it is clear that FIM requires n stages to completely determine all frequent itemsets [2], [3]. This implies a total of n partial scans over the transaction database.

The general model of the FIM algorithm aims to reduce this number of scans. In order to achieve this goal, it is necessary to group the frequent items in m larger intervals (obviously, m < n). The first stage of the generalized FIM algorithm implies applying Algorithm 1 over each interval of frequent items Ii (1? i?m) in order to indentify all frequent itemsets that belong to the target interval. The following notations will be used [3]:  FIi,I j?the set of all frequent itemsets in Ii, j = Ii? Ii+1? ...? I j  CIi,I j?the set of all candidate itemsets in Ii, j (20)  In [3] it is shown that a frequent itemset X ? FIi,I j must simultaneously include itemsets belonging to Ii and I j. There- fore, new candidates belonging to CIi,I j must simultaneously include itemsets belonging to Ii and I j. This implies that new candidates are determined according to (21) (a general    approach to equation (2)):  CIi,I j = {X ?Y |(X ? FIi,I j?1 ? Ii?X 6= ?)? (Y ? FIi+1,I j ? I j ?Y 6= ?)}  (21)  The main difference from the base model is that the general FIM algorithm will consider for each stage a new interval Ii (1 ? i ? m) rather than a new frequent item i (1? i? n). Algorithm 4 presents this general approach.

Algorithm 4 The general model for the FIM algorithm Require: FIi,Ii = {set of all frequent itemsets in Ii} Require: Di = set of transactions grouped by intervals  (subsets of the original D transaction list that include items for interval Ii)  1: for k = 1 to m?1 do 2: for all Ii : i ? 1, ...,m? k do 3: I j = Ii+k 4: FIi,I j ? FIi,I j?1 ?FIi+1,I j 5: CIi,I j = {X ? Y |(X ? FIi,I j?1 ? Ii ? X 6= ?) ? (Y ?  FIi+1,I j ? I j ?Y 6= ?)} 6: for all transactions t ? Di do 7: Ct ? subset(CIi,I j , t) {determine candidates in  CIi,I j included in t} 8: for all candidates c ?Ct do 9: c.support++  10: end for 11: end for 12: FIi,I j = FIi,I j ?{c ?CIi,I j |c.support ? minsupp} 13: end for 14: end for  In order to efficiently generate all valid candidates for a single stage, we must adapt Algorithm 2. For a given itemset (either frequent or candidate) X ? FIi,I j , the following notations will be used (the general form of equation (4)):  su f f ixG(X) = X \{k|k ? Ii} pre f ixG(X) = X \{k?|k? ? I j}.

(22)  Using equation (22), the corresponding method for de- termining valid candidates represents a generalization of Algorithm 2 and is presented in Algorithm 5.

Algorithm 5 Generalized approach to the candidate genera- tion stage for Algorithm 4 Require: FIi,I j?1 , FIi+1,I j  1: for all itemset X ? FIi,I j?1 do 2: for all itemset Y ? FIi+1,I j do 3: if su f f ixG(X) = pre f ixG(Y ) then 4: candidate ? X ?Y 5: add candidate in CIi,I j 6: end if 7: end for 8: end for  Lemma 3 proves that Algorithm 5 generates all possible valid candidates.

Lemma 3: Let T be a candidate itemset in CIi,I j . T is a valid candidate if and only if it is obtained through joining two frequent itemsets X ? FIi,I j?1 and Y ? FIi+1,I j that simultaneously meet the following conditions:  X \ su f f ixG(X)? Ii (23) Y \ pre f ixG(Y )? I j (24)  su f f ixG(X) = pre f ixG(Y ) (25) Lemma 3 represents the general form for Lemma 2,  therefore its proof is similar.

The parallel model for the general FIM algorithm is  straight forward since processing a set of intervals may be performed independently once the required values have been transfered accordingly. The initial number of intervals is equal to the number of parallel working units (either processes or threads, depending on the underlying parallel architecture). Each working unit would apply Algorithm 1 for its given interval. After local processing is finished, it can easily be observed the the for all loop on line 2 in Algorithm 4 may be run in parallel. A detailed presentation of this parallel model is given in [3]. The important thing to notice is that this approach does not affect at all the candidate generation method presented in Algorithm 5. Once each active working unit receives its corresponding new data, no supplemental communications are required in order to determine all possible candidates and all computation is performed locally. This is a major advantage for this new algorithm over almost all parallelizations of the Apriori based approaches.



IV. PRELIMINARY TEST RESULTS  A preliminary series of tests have been performed in order to compare the overall performance of the FIM algorithm against the Apriori standard algorithm. For both algorithms a binary approach was used in order to code transactions and itemsets. Both algorithms were run on a synthetic dataset (T10I4D100K), having 100000 transactions and 1000 individual items. For the FIM algorithm, the number of intervals has been modified such that both Algorithm 1 and Algorithm 4 could be tested (with their corresponding candidate generation methods depicted in this paper). Table I presents the results we obtained for a minimum support limit of 500 transactions (resulting in a total of 569 frequent items).

The first important note is that FIM outperformed Apriori every time, even for the last test for FIM (1 interval with 569 items). This last test is extremely relevant, since in this particular case the FIM algorithm needed to fully scan the transaction database for each stage. Since the standard Apriori algorithm behaves in a similar manner, this last test outlines the efficiency of the candidate generation methods that have been presented in this paper (Algorithms 2 and 5): a difference of 800+ seconds. The second important note is that the minimum execution time for FIM is 517.19 seconds (60 intervals, with 9/10 items per interval ? even split) ? more than 5 times faster than Apriori. This result is explained    TABLE I FIM VS. APRIORI - TRANSACTION SET T10I4D100K, MINIMUM SUPPORT  THRESHOLD 500 TRANSACTIONS  Apriori Algorithm Frequent items Time (s)  569 2845.20 FIM Algorithm  Interval count number of frequent items per interval Time (s) 569 1 2029.24 300 2 999.91 200 3 730.19 150 4 642.68 100 6 539.54 90 7 533.90 80 8 524.44 70 9 520.34 60 10 517.19 50 12 556.67 40 15 581.98 30 19 590.57 20 29 634.84 10 57 1025.86 1 569 2026.04  by both our methods of generating candidates and by the reduction in database scans explained in [3].



V. CONCLUSIONS AND FURTHER WORK  This paper presented new and efficient means of deter- mining candidate itemsets for a particular algorithm ? The Fast Itemset Miner (FIM). These methods exploit both the Apriori Property and the particularities of FIM algorithm to avoid any unnecessary candidate validation steps and to avoid generating too many candidates. Also, these methods do not impose any supplemental restrictions on the parallelization of the base and general FIM algorithms. Although limited tests have been performed until now, their results underline the potential of this novel approach to the frequent itemset mining problem.

Our immediate priority is to properly test this new ap- proach, not only against Apriori, but also against faster algo- rithms such as FP-Growth. Favorable results would underline the fact that candidate based approaches should still be con- sidered in addressing the frequent itemset mining problem.

If properly handled, such approaches do have the advantage of eliminating unnecessary test cases. A second line of tests must address the parallel FIM algorithm. This new algorithm implies a simple, straight forward parallelization that may be easily adapted to either shared memory parallel machines, either message passing ones.

