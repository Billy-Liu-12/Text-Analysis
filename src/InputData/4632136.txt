

Abstract-In this paper, we describe an approach for au- tomatically finding the prototypic navigation behavior of web users. User access logs are examined in order to extract the most significant user navigation access pattern. Such approach gives us an efficient way to better understand the way users are acting, and leads us to improve the structure of websites for improving navigation. Another interesting application deals with recommender systems where the system can suggest to users links that could interest them. A main issue of navigation pattern discovering deals with the way to decide when two traces are identical or not, even when they present some little difference. In this paper we describe the overall characteristics of our similarity function, and we will introduce some of our results.



I. INTRODUCTION  The World Wide Web provides a rich environment for users to retrieve information. Day after day, Internet is growing up at an incredible speed. Information becomes so huge and so difficult to be found that users can easily be lost in this vast source of information even with the assistance of search engine. Websites are also growing up more and more and become by themselves a large source of information that is in most cases inaccessible for all users and must be organized.

Web access logs conta~ a lot of information that allows one to understand users' needs, to study and to observe the effectiveness of the structure of the website, etc... Many approaches have been developed in the web usage mining domain concerning these problems, some of them considers that a sequence ofpages is only a set of pages and don't refer to the order of pages, and others don't take the order of pages in consideration. Faced to this situation we investigated this domain in order to propose an approach that takes into account the problems of other approaches and gives a good solution for users.



II. WEB USAGE MINING  Web usage mining, also known as web log mining, aims to discover interesting and frequent user access patterns from web browsing data stored in the log files of web/proxy servers or browsers. Many works of web usage mining have been developed to help users in their search for infonnation. Some of these works are focused in creating recommender systems [1] that would help people make decisions in a complex inform~tion space, where the volume of information available for them is huge. A recommender system relies on a prediction model to find out the interests of users and then make recom- mendations. In general, the recommender systems' behavior  consist ofpredicting user's next actions based on their previous one. Another application of web usage mining is caching and prefetching of web pages. Most users perceived that latency is a major problem in today's World Wide Web. Many factors contribute to this problem, including transmission latency, DNS name server lookups, TCP connection establishment, and start of session delays at HTTP servers [2]. Conventional web caching techniques [3] attempt to address part of this problem by temporarily storing recently accessed web content close to the user, on the client device or on a proxy server.

These techniques work well when the content is reused several times, potentially by several users. However, caching may not reduce latency when there is poor locality of reference and  . access to dynamic and personalized content. A complementary  . approach to reducing latency is to effectively predict user access behavior and to use this knowledge to prefetch content close to the user. In general, prefetching applications consist ofpredicting the future access of a web user and then prefetch its content in order to resolve latency problems [4]. Web usage mining has many applications, such as improving search engines [5] and for allowing customized browsing in websites [6], [7] etc... .

In all these applications the goal is the development of an effective prediction algorithm. The advantage of the prediction algorithm is that it will help web users find what they look for and not to be lost in these huge sources of information.

The major classes of web usage mining applications are based on the discovery of users' navigation patterns. The general technique used in order to find navigation patterns are statistical analysis, association rules, clustering, classifi- cation, and sequential pattern mining. In many works these methodologies are combined together in order to find users' navigation patterns.

? Statistical Analysis: Statistical approaches are the most common methods for extracting knowledge about visitors to a Web site. By analyzing the session file, one can perform different kinds of descriptive statistical analy- sis (frequency, mean, median, etc... ). Typical extracted statistics include the most frequently accessed pages, average page viewing time and average navigation path length.

? Assoc~ation rules: Association rules generation [8] can be used to relate pages that are often referenced together in a single server session. In the context of Web Usage Mining, association rules refer to sets of pages that are     accessed together with a support value exceeding some specified threshold. These pages may not be directly connected to one another via hyperlinks.

? Clustering: is the process of organizing objects into groups whose members are similar in some way. A cluster is a collection of objects which are "similar" between them and are "dissimilar" to the objects belonging to other clusters. In the Web Usage Mining domain, there are three kinds of interesting clusters to be discovered: User clusters, session clusters and page clusters. Clus- tering users tends to establish groups of user exhibiting similar browsing patterns. Clustering sessions tends to establish groups of user sessions in which the users have similar access patterns. Clustering pages tends to establish a group of pages according to their content.

? Classification: is the task of mapping data items into one of several predefined classes [9]. In the web domain, Classification is used to develop user profiles that belong to a predefined class or group which is characterized by a set of features or properties.

? Sequential pattern: The problem of discovering sequential patterns [10], [11] deal with finding inter-transaction pat- terns such that the presence of a set of items is followed by another item in the time-stamp ordered transaction set.

The discovery of sequential patterns in Web server access logs allows Web-based organizations to predict user visit patterns and help in targeting advertising aimed at groups of users based on these patterns.

The various existing approaches in the field of Web Usage Mining have many advantages and disadvantages, and in the majority of the cases they are very dependent on application type. As an example PageGather [12] is an algorithm which processes the access log file us~g statistical methods in order to find collections ofrelated pages at a web site. Then it applies cluster mining to the data and produces candidate index-pages as output. PageGather consists of performing 5 basic steps:  1) Process the access log into visits.

2) Compute the co-occurrence frequencies between pages  and create a similarity matrix.

3) Create the graph corresponding to the matrix, and  find maximal cliques (or connected components) in the graph.

4) Rank the clusters found, and choose which to output.

5) For each cluster, create a' web page consisting of links  to the documents in the cluster, and present it to the webmaster for evaluation.

Based on the PageGather algorithm, index pages are created for easier navigation.

However PageGather assumes that a visitor can easily navigate a lengthy list of shortcuts, and thus provide perhaps dozens of suggested links.

A draw back for PageGather is that it relies on the human web masters to determine the appropriateness of the generated index pages in a final check. This will be so difficult to realize in the case of sites that have so many pages to be  indexed.

Another appoach, named WebWatcher [13] which is a virtual agent that helps user to reach a particular target page or goal, it offers suggestions about where to go next.

The WebWatcher acts as a guide, offering suggestions based on its knowledge of each user's interests. It learns to suggest appropriate links for users by leaming from previous tours, in which user-selected links are identified by adding the keywords chosen by the user at the beginning of tour.

A second learning strategy involves reinforcement, in which the agent reinforces a given link using words encountered in pages downstream of it. A third approach to learning uses these two approaches together which give best results. User must communicate with WebWatcher in order to tell if user goal is reached or not etc...

Letizia [14] is another agent-based approach, it is a client side browsing assistant that accompanies the user while browsing. It is located on user's single machine. No goal is predefined in advance by users.

Letizia uses the past behaviour of the user to anticipate a rough approximation of its interests. It looks for more documents that are related to the users' interest or might be relevant to future request when user is not browsing or reading documents.

By referring to the advantages and disadvantages of the different methods and approaches, we conceived our approach based on a similarity function.

In this paper, we present our novel approach for building a similarity function that finds a set of interesting navigation patterns called prototype. Those prototypes could be used to build an optimized cartography for the website, to find the most popular navigation pattern, to predict future page request by users, etc...

Our similarity function uses a sequential finding algorithm and an n-gram prediction model to construct a list of popular sequence prototypes.



III. EXTRACTING NAVIGATION PATTERNS  By referring to many approaches and by studying the needs of different web usage mining applications, we drew certain constraints which have guided us in our navigation pattern searching approach. Extracting navigation patterns implies a phase of regrouping of user's sessions according to their proximity. This regrouping implies the search, for a given sequence, the more similar sequences to this one. One of the characteristics of our work is to assimilate sequences which are not absolutely identical. What could be regarded as negligible errors in the course of a web surfing sessions is not taken into account, in order to differentiate these erros from web surfing sessions which are considerable different.

A. Similarity Function Definition  The constitution of groups which are used as support together with the definition of a prototypic course is based     Fig. 2. Output sample of the similarity function  the filtered sequences coming from the access log files of the site. The procedure of extraction of the patterns produces as output, groups where each one is related to a prototypic representative sequence in which all the members of the group were considered to be similar to it (Figure 2)..

<:re3uka1:-  ;;I!r:~~:~~f~:l" it.~""5" ....:..th;~""l 10 12 14" ,llu.ni::;:"4" n~1:.''''''S9.38'' e>.:<.ld"...."1.87"  <Eel.lm~JiC~ i;:j","34" \i,hl~;:,111 1012 1415" dOl"'''90.47" ..~ -:sc:qucncf i,r--="57" ..?.;!u!:':::"'12910 12 14" :.im",?'S7.23" :-...

"sequellte ;,"?"230" "';0<"""1 10 12 1412" ;.;r"m"90.47" .'> <"'/groupe> ':grcupe l!hE""2" ld="W' ':,:.,t'l{.=r::"l 0 12 14 J5" ~:.(ltioi="8" Tl'K'l\.="S9.32" C<..Jrf="6.2T' 'P'l'<:!,I-;I'lc~n3":' - -.;;equ<" .." id=-14" ",i'!<~"l0 1214 151r .;im~"90.47"t> <r.eq!J.ence jJ:::"34" \'I;jt;r......."l 10 12 14 15" ~iT1p ..90.47" ..'".> -:scqucnce ~p."38" ..?"iue:::"2 10 12 14 IS" :.in1","90.47" .:;..

<te ...

1) Searching: The searching phase of the similarity func- tion consists of finding and calculating the similarity between all the sequences received at the entry. This stage of research uses the following parameters: Coverage, Error, ErrorRate, seqMargin.

? Coverage: Indicates the minimal number of common elements between the two sequences. It is a percentual value which is by default equal to 75%.

Example: Given seql = PI P2 P3 P4, seq2 = PI P2 P3 and coverage = 90%. It is necessary that 3.60 ~4 pages ordered as in seq1 appear in seq2 in order to consider  Groups Bod prototypes  Fig. 1. Similarity Function Sturcture  Similar sequences.

having differenllenglh  V:<:.:rlfkJll!'inand containing different terminal Conlirrnlng {he  seauences I simlarity (N-GramProd_n Model) ~ ~;;;;::'~r;"lIarity  J ""lY-;og !/  Sirrllar ,/ Keep every sequences sequence in lhe containing !.he groups where tt same terminal has maximum SeQuences similarity  set of sessiOtl (S8ssion file)  request. For this reason each step (visited page) is important and is represented in our model of.information retrieval.

Another thing can also be considered as novelty in our approach, it is the capability of considering some pages as visited by error. And this can be done without referring to the time of viewing pages and without having any information about the content of pages etc...

B. Implementation of the function  One can divide the procedure carried out by the function of similarity into three phases: Searching, verification, and filtering (Figure 1). The similarity function receives as input  on a similarity function which provides a rate of similarity for a pair of given sessions. The extraction of the sessions and then the courses are done in a very traditional way using server access log files of a given Web site.

This similarity function takes into account a certain number of factors in order to produce a value of resemblance. Some of these important factors are:  ? Order of the pages in the sequence: We consider that it is an essential factor for revealing two similar navigation behaviors. It considers the order of appearance of the pages in the session. For example: the sequence "PI P2 P3 P4" will be considered as different from the sequence "PI P3 P2 P4", even if the same pages were visited in these two sequences.

? Error tolerance: It's the capacity to neglect pages which can be regarded as "visited by mistake". As example, is it necessary to consider that both sequences seqI: PI P2 P6 P3 P4 and seq2: Do PI P2 P3 P4 correspond or not to different sequences? The answer that we propose is a gradual answer where such a difference will be or not neglected according to a parameter of the function and some other characteristics like the number of pages relatively different from the length of sequences.

? Comparison sequences of different length: Even if the sequences present different lengths it can be useful to determine that certain sequences are in fact only sub sequences of others which are larger but whose is iden- tical. With this intention we used an n-gam prediction model, which after having noted the similarity on the first elements, will check if the sequences could be identical.

It consists of a means for determining if it is very probable or not that the shortest sequence, if it were to be prolonged, its in conformity with the longest sequence.

For example: given seql: PI P2 P3 and seq2: PI P2 P3 P4, we will consider that the sequence PI P2 P3 is similar to the sequence PI P2 P3 P4 if, according to the other sessions, the page P4 is very likely to be found after a sequence PI P2 P3.This model of prediction is based on the "n_gram model" see [15].

The set of these factors mentioned above present the novelty of our approach.For example one of our method's advantages is that it conserves the integrity of the used analysed se- quences.

Prototypes that represent clusters must be necessarily exis- tent sequences "navigation sessions". Statistical approaches or other data mining approaches don't take into account this constraint because it doesn't present an interest for them. They search for any type of sequences (sub sequences, elements of sequences, etc... ) in order to find relation between pages, users, and etc...

In our case, this choice can be explained by our aim for " rebuilding the cognitive process of users while they are searching for information ". However in this perspective we consider that each step of information retrieval is important because it could modify knowledge of the user either his     The results obtained by this procedure are prototypes which represent the most significant courses, both in quantity as in quality, of the file log. Currently, we are examining the possibility of developing a recommender system where the prototypes will constitute our behavior database. We can use any prediction method based on probabilities and/or on a prediction model, for example n-gram for exploiting this database in order to predict the future choices of the uSer and assist him (Figure 4).

94 roll S  3 => 15=> 22 => 1  . , .

14 'ou S  Fig. 3.. Output Group number  3 => 15=> 22 => 1

IV. RESULTS  As we already explained, the results of the similarity func- tion are prototypes which represent the most frequent courses.

This list of prototypic courses will be regarded as a mini log file where we can apply any statistical method to obtain, for example the most visited pages in the site etc... According to our test on the log files of the site of our laboratory (LSIS) and on the website of the MIAGE speciality, we obtained meanful significant prototypic courses. For example, in the case of the site of the LSIS laboratory the most frequent course is P3 => Pl5 => P22 => PI which means (/ => Equipes => membres => Fiche). This is consistent within the context ofa laboratory site , since normally the web surfers use this site to search for informations about the members of research teams of the laboratory (publications, etc.) Returning to the groups obtained, we noted that the number ofgroups changes between one log and another but preserving certain common prototype groups Figure (3). This number tends to become constant when the number of sessions increases.

them to be similar. And in this case we consider that seq2 is not similar to seq1 because there exist less than 4 elements of seql in seq2 (only 3 elements PI P2 P3).

? Error: This parameter indicates the maximum number of pages visited by mistake that appear between two suc- cessive pages of the sequence. By default this parameter is equal to 1. Example: If ErroFI and we compare seqI = PI P2 P3 with seq2= PI P7P2 P3 or seq3= PI P7 P2 P8 P3 we will obtain a positive result in both cases. On the other hand a sequence such as seq4= PI P7 P9 P2 P3 will give a negative result.

? ErrorRate: This parameter is a rate used to take into account the increase of error with the length of the sequence. It is a percentual value that equals by default to 15%. This parameter is in direct is related to the number of common elements between the 2 sequences.

As example, given the two sequences: Seql: PI P2 P3 and Seq2: PI PIO P2 P6 P3. In this case we will reject the similarity. In effect, 2 errors for 3 common pages provide an error rate higher than 15%.

? SeqMargin: This parameter enables us to take account of the difference in length between the sequences to be compared (Le. The number ofelements which appear and which are neither errors nor elements of the sequence).

This parameter is an integer value which is by default equal to 1, and will be multiplied by the maximum allowed number of errors (in relation to the error rate).

For example, ifSeqMargin=1 and ErroFI, let us compare the two sequences seql: PI P2 P3 and seq2: PIO PI P6 P2 P3 PI2 P13. The sub-sequence PI P2 P3 (3 elements) is included in seq2 with the page P6 as error. The 3 other remaining elements PIO, PI2 and P13 are neither the pages visited by error nor the pages of the sequence in common with seqI. We calculate the number of elements of the sequence in the margin. (That is, the elements different than the elements in common with seqI, and different from the error elements). Nb of Element Al- lowed in the margin=A * B A= SeqMargin B=Maximum number oferror allowed = (C * D)/l 00 With C = Number of common element found D = ErrorRate let's suppose it's equal to 25 This gives: Nb of Element Allowed in the margin = I * B B = (3 * 25) / 100 = 0.6 ~ 1 So, the number of elements allowed in the margin = I *1 = I. However, there are 3 elements (p1O, P12, and P13), and thus we consider that seq2 is not similar to seqI.

Fig. 4. Recommender System  2) Verification: The verification phase of the similarity function consists of verifying the results obtained from the first phase (Searching) using the n-gram prediction model.

3) Filtering: The filtering phase is the last phase of the similarity function, it consists of a filtering of the results after the two first phases have ended (searching and verification).

We tested the effectiveness and stability of our parameters on different log files. By varying the values of different parameters we found that: The number of result groups decrease with the increase of the value of the coverage (Figure 5), which verifies the correct functionality of this parameter, as? the increase in value of this parameter augments the discriminative nature of the     Fig. 5. Variation Of Coverage Parameter  comparison between two sequences.

Similarly, other parameters were checked and tested in the same way.



V. ApPLICATION  A. User navigation simulation  The results obtained from our model are represented as a list of sequences. Every sequence represents the most interesting user navigation behavior. These sequences will be the core of the simulator, and will be used in order to predict the navigation of new users. The simulator (Figure 6) takes as input either one or a sequence ofpages, and produce as output a list ofpages that will be represented as a navigation behavior.

Sf!flttet~:3; I  . . I  l"-?"~"'?"".~,~==,,=~=_====~~=~=,=-=.~"j Fig. 6. Simulator Interface  1) Functionality: In order to study the navigation behavior of users, we built a simulator that reproduces the same navi- gation behavior of the users according to the results obtained from the above mentioned approach. The simulator doesn't take into account the time of navigation, it is only interested about the order of appearance of the pages. The aim of the simulation is to study how users are acting, in order to provide them better navigation environment. Also, the simulator can be used or extended in order to provide the best path for attaining a destination page.

In conclusion the core of the simulator is a list of prototypes obtained from the similarity function. Every prototype is characterized by a number (its presence) for example: The sequence 1 5 7 21 has a presence = 13:7 The sequence 1 5 7 21 24 has a presence = 136?7 The sequence 1 5 7 5 7 has a presence = 1~7 The sequence 1 26 8 has a presence = 1~7 etc...

As mentioned before, the simulator will take as input a page or a sequence ofpages so for example, if the simulator receives as input the page pI, then all the sequences that start with pI are analyzed and used in order to generate a probabilistic model. This probabilistic model will be executed and will give as result the next page that the user will visit after pI, and so on.

2) Pobabilistic Model: The probabilistic model is generated from a given list of sequences, this model will be used to obtain what page will appear after these pages. We can represent this model as a two dimension array (Figure 7).

Fig. 7. Probabilistic model  This two dimension array indicate that there is, for example, a 20% of possibility that users tend to visit P2 after PI, 10% tend to visit P3 after PI, 40% tend to visit P5, etc...

According to these values we dynamically assign a range of numbers for each page. This range of numbers will be chosen in relation to their possibility values. Finally, a random number will be generated and which, according to its value, the correspondent page will be chosen and sent to the output of the simulator. Then, the same process will be done, but this time for other sequences and another type of page.



VI. RELATED WORK  Understanding the behaviour of computer users is an active research area. This technique allows us to find out how the users are acting, under which conditions, and what they 'are asking for. Many methods have been developed in this research area, mainly for extracting the behaviour of web users.

As a result of our comparisons between our approach and the rest we have found out that: Our approach has the following advantages:  ? It extracts the most frequent user behaviour, and at the same time it groups user sessions in clusters.

? It can be used to study the behaviour of users running any computer applications, apart from web navigation.

? It gives us the possibility to understand what the users are actually doing in terms of errors. It can show and help us detect errors in the users' activity, referring neither to the term of time (navigation time, etc...) nor to the content of the navigation information.

As a possible disadvantage our approach presents the following:  ? The results can only be real complete sequences. It cannot detect patterns if they do not appear at least once together as a sequence (without errors of other elements). Thus, it is not the most appropriate method for finding out individual page frequencies.

Our method can be considered as a sequential pattern algorithm and as a clustering algorithm.

One of the famous sequential pattern algorithms is AprioriAll. This algorithm is highly effective, and gives good results because it' eventually extracts all the frequent sequences. There may exist the possibility that it returns a sequence that does not really exist as a complete sequence.

This will not be useful for studying the users' behaviour in a computer application. Nevertheless, the result can be useful for finding which items or pages where accessed altogether.

In [16] a method to classify web site users is proposed.

Each user session is stored in a vector that contains the number of visits for each page, and an algorithm is used to find similar vectors, in order to cluster them. The clusters obtained with this method do not take into account the order of the pages in each sequence.

There are also other clustering algorithms based on neural network algorithms, but the disadvantage they present is that it is not possible to verify the results or to understand clearly the results.



VII. CONCLUSIONS  Although there exist many tools for static analysis of Web site visits (the most visited page, average time of visit, etc.), there is, however less tools which are interested in kinemat- ics of navigation, which are useful for helping web users.

Moreover existing approaches which are primarily statistical or neuronal are very often dedicated to a given application and do not make it possible to explain the results obtained and thus to include/understand the noted behaviors. We proposed a similarity function in order to extract, the pattern ofnavigation by clarifying the criteria of clustering and session selection.

One of the characteristics of our approach is that it offers a certain tolerance to errors in navigation. Indeed, we make the assumption that a web surfer can look for his way on. the site and consequently borrow alternatives which do not call into question its total course. So as not to exclude this course from a group tendency, it is necessary that the similarity func- tion is able to assimilate courses which seem different. The obtained results are satisfactory. Tested on several sites and starting from various log files, they reveal coherent practices of navigation, compared to the contents of the site and to statistics. The current prospects relate to the development of recommendation tools, the production of models of operators  of the type "Virtual Net surfer" for testing of sites and the retro-design of these same sites.

