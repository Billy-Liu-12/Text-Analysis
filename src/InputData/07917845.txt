VeinDeep: Smartphone Unlock using Vein Patterns Henry Zhong, Salil S. Kanhere, and Chun Tung Chou

Abstract?This paper presents VeinDeep, a system for using vein patterns to secure smartphones from opportunistic access, e.g. a device left unattended. VeinDeep takes advantage of infrared depth sensors, which at the time of writing have recently started to appear in smartphones for 3D indoor mapping and localisation. We find these sensors can be re-purposed to capture images of the unique vein patterns on the back of each person?s hand. We simulate a depth sensor equipped smartphone by developing VeinDeep on a low power Compute Stick. We use Kinect V2 depth sensor to collect 240 recordings from 40 hands belonging to 20 test subjects. Then use this data to compare VeinDeep to one older but popular vein pattern recognition algorithm which uses Hausdorff distance and one recently developed algorithm which uses Kernel distance. We achieve a precision of 0.98, compared to 0.9 for Kernel distance and 0.5 for Hausdorff distance when recall is approximately at 0.83. In addition VeinDeep does all this while taking an average of 6 MiB of memory and 466 milliseconds per comparison. This is an average of 1/6 run time, 2/3 the memory of Hausdorff distance and 1/3 the run time, 1/2 the memory of Kernel distance.



I. INTRODUCTION  The pervasive smartphone has been integrated into many facets of daily life. The portability, connectivity and on-board sensors have enabled a variety of uses in addition to com- munication. Many applications involve processing potentially sensitive data on the smartphone, examples include social networking [1], health monitoring [2] and banking [3]. In response, several methods have been developed to secure the contents of smartphones from opportunistic access, e.g. a device left unattended.

Authentication on smartphones are divided into implicit and explicit methods [4]. Implicit methods learn from the user?s past behaviour by collecting sensor or metadata to determine if it matches against current behaviour [5][6]. However, there is a vulnerable interval of time before suspicious behaviour triggers a lockdown. Explicit methods do not have this weak- ness and identify the user based on the user supplying a specific piece of information. PINs, passwords and pattern unlock screens are explicit methods but open to shoulder- surfing [7] and user chosen passwords tend to be predicable [8]. Biometrics such as face and fingerprint recognition are resistant to shoulder-surfing and cannot be easily guessed like passwords. However, fingerprints can be left on the very device it is meant to secure [9] and faces can be recorded whenever the user is visible in public [10].

In this paper we propose a system called VeinDeep. A system to secure a smartphone from opportunistic access by identifying the owner based on vein patterns. Vein patterns  Fig. 1: VeinDeep is designed to unlock an infrared depth sensor equipped smartphones using vein patterns. Then the user takes an infrared image and depth map of the back of their hand. This is used to extract vein patterns which identify the user.

have the advantage of not leaving imprints on surfaces like fingerprints and veins lay underneath the skin [11], so are not easily recordable when in public like a face. Our system uses an infrared (IR) depth sensor to record vein pattern images. Veins are visible under IR illumination because IR light penetrates biological tissue, but blood attenuates IR light differently from other tissue [12]. Since this operates outside of the visible spectrum, it works irrespective of skin colour.

This method has become feasible on smartphones at the time of writing due to the recent integration of IR depth sensors into smartphones [13][14]. These sensors are active IR devices which take 3D photographs [15]. This is used to facilitate accurate 3D indoor mapping and localisation [16]. We find it is feasible to re-purposed them for vein pattern recognition.

The use case of VeinDeep is depicted in Figure 1. The user is expected to take an IR image and depth map of the back of their hand (hand dorsum) while making a fist. The fist gesture helps make veins more visible by forcing veins closer to the surface. The user can use their left or right hand. The hand used to unlock the device must be the same one registered with the device, as vein patterns are unique to each person and each hand [17][18].

Vein patterns are extracted from the IR image using an adaptive threshold filter. The depth map is used for background removal and to correct perspective distortion. Perspective dis- tortion makes regions of the hand dorsum tilting away from the sensor appear smaller. Given vein pattern images, VeinDeep      uses the location where veins intersect with each row in the image and the row index to compare similarity. A change in the number of vein intersections indicates where veins start, end or cross. This discriminating feature is compared using a similarity function.

The following are the main contributions of this paper.

? We present VeinDeep, an authentication system based on  vein pattern recognition. As far as we are aware, this is the first instance where depth sensors have been re- purposed for this task.

? We tested with 240 vein patterns, taken from 40 different hands, provided by 20 participants. We achieve a pre- cision of 0.98, with a recall of 0.83. We compare the result against one popular existing vein pattern recogni- tion algorithm, Hausdorff distance [19] and one recently developed algorithm, Kernel distance [20]. When recall is approximately 0.83, we find Kernel distance achieves a precision of 0.9 and Hausdorff distance achieves 0.5 which is lower than the 0.98 of VeinDeep.

? We implement VeinDeep as a proof of concept system.

We use a low powered Compute Stick with a Kinect V2 depth sensor. This simulates a depth sensor equipped smartphone, as at the time of writing such devices were not readily available.

? We demonstrate the lightweight nature of VeinDeep by showing that it takes an average of 466 milliseconds and 6 MiB of memory to compare vein patterns on the Compute Stick. This is 1/6 the run time, uses 2/3 the memory of Hausdorff distance and is 1/3 the run time, uses 1/2 the memory of Kernel distance.

The remainder of this paper is organised into the following sections. Section II presents the related work. Section III presents the way the system works. Section IV presents the results of the experiments. Section V ends with the discussion and conclusion.



II. RELATED WORK One of the earliest studies of vein pattern recognition  was conducted by Fujitsu Laboratories at the beginning of the 2000s. This study [18] involved 70000 participants aged between 5-85. According to their research, vein patterns were unique to each individual participant. They achieve a false pos- itive rate of 0.00008%, but their methods remain unpublished.

Many publications appeared after the early study. For this section we restrict the review to a selection of papers which use images of the palm or hand dorsum veins. While finger vein pattern recognition exists, solutions require a finger to be placed and scanned inside a receptacle. So it is unlikely to be used to secure a mobile device in its current form.

Research on vein pattern recognition mainly use IR cameras under IR illumination to acquire vein pattern images. They can be broadly categorised into two classes, those based on key points or a similarity score. Key point methods attempt to match corresponding points between reference and test vein patterns. One popular implementation is by Ladoux et al. [21].

An image is captured of the palm, lit by IR emitters able to  deeply penetrate tissue. A threshold filter is used to extract vein patterns and box filter to remove noise. SIFT features are used to identify points in regions with high contrast changes.

Another implementation is by Kumar et al. [17]. An IR image is captured of the hand dorsum when the subject makes a fist.

A Laplacian of Gaussian filter is used to extract vein patterns, a line thinning algorithm is applied and small isolated clusters of veins are removed to clean the image. Key points are based on vein endings and crossings. Ding et al. [11] uses an IR image of the hand dorsum laid flat with a mix of techniques from the first two papers. The patterns are extracted using a threshold filter and key points are based on vein endings and crossings.

VeinDeep also makes use of the location of key points.

However, there are far fewer key points representing vein crossings and endings in our vein images. The is because IR depth sensors produce lower resolution images and have less powerful IR emitters compared to the equipment used in the aforementioned papers. However, it has access to depth map which is used to aid in background removal and reduce perspective distortion. VeinDeep also makes use of similar filters and images are taken of the hand dorsum in the same manner as the Kumar et al. paper to improve vein visibility.

Methods based on a similarity score measure similarity in shape between two vein patterns. Wang et al. uses thermal [19] and IR [22] images of the hand dorsum. The vein patterns are extracted using a threshold filter and cleaned using a median filter. The Hausdorff distance is computed as a measure of similarity. Finally, their method [23] was modified to make it rotation invariant by aligning against the webbing between fingers. Zhang et al. [20] takes IR images of the hand dorsum.

A threshold filter extracts the vein pattern images. The Kernel distance function measures similarity between vein patterns.

Both methods use the coordinates of every pixel to compute the score.

VeinDeep compares reference and test points using a sim- ilarity function which is implemented in the same manner as Zhang?s Kernel distance function. However, VeinDeep is different in that it does not use the location of every pixel in the vein pattern image. This is explained in more detail in Section III. We later test the features of VeinDeep against Hausdorff and Kernel distance in Section IV.



III. SYSTEM DESIGN  The algorithm workflow of VeinDeep is depicted in Figure 2, 5 and 7. As the figures suggest, VeinDeep consists of 3 major components. After the image is recorded, vein patterns are taken from the IR image using an adaptive threshold filter. This finds local intensity changes as veins appear darker than surrounding tissue in the IR image. The background is removed using a depth threshold as we assume the hand is closer to the sensor than the background. We correct the vein pattern image for perspective distortion by using the depth map to rotate the pixels in 3D. An example of the output from 3 participants is shown in Figure 3.

Fig. 2: The algorithm workflow from raw infrared image and depth map to vein pattern.

Fig. 3: Examples of vein patterns from 3 different participants.

The second step is to find the key points in the vein pattern.

We do this by finding the locations where veins intersect with each row in the vein pattern image. This feature is important for the following reasons. It is discriminating because it reveals the location of vein endings and crossings. The relative position of these points are resolution independent. The vein pattern image appears larger when taken closer to the sensor.

The final step is to compare a points in reference and test vein patterns. We compute a similarity score. This is implemented in same manner as Zhang?s Kernel distance function. However, our input is the previously found key points. This is important because Zhang uses every pixel instead of key points. This makes our comparison much faster.

One of the challenges is a sequence of vein patterns recorded from the same source may have subtle differences.

Henceforth we refer to this as jitter. An example of jitter is shown in Figure 4. Some sources of jitter are sensor errors.

Changes in distances between hand and sensor. Changes in perspective distortion caused by the hand dorsum being tilted differently in each image. Much of the implementation and a vast majority of system parameters are chosen to deal with this problem. The implementation details and the way we choose parameters are described in the following sections.

A. Extract Vein Pattern  The workflow for extracting the vein pattern is depicted in the 1st column from the left of Figure 2. The first step is to crop the region of interest (ROI) from both the IR image and depth map. The vein patterns are computed by applying an adaptive threshold filter on the ROI. The filter reveals the veins  Fig. 4: The series of vein pattern images were taken in succession from the same the right hand of a single person.

However there are small differences in each extracted vein pattern image. We refer to this as Jitter.

which appear darker than surrounding tissue in the IR image.

The background is removed using an image mask computed from the depth map. We first define some parameters and heuristics for parameter choice before the detailed explanation.

? u1, u2, v1, v2 represent the rows and columns of the bounding box of the ROI. We choose the values so the bounding box is large enough to enclose the hand dorsum.

? I , D, i(u, v), d(u, v) represent the IR image and depth map from the ROI. I is the IR image, i(u, v) holds the IR pixel intensity value at coordinates (u, v). D is the depth map, d(u, v) hold the depth pixel value at coordinates (u, v).

? M , m(u, v), z1, z2 are used for background subtraction.

M is binary image mask consisting of binary pixels m(u, v). z1, z2 are thresholds used to determine the value of each m(u, v). We choose z1 to be equal a value which lets the hand be close enough to fill the ROI or the closest operating range of the sensor to maximise resolution of the recorded data. z2 the range for which data recorded beyond this point is too far to be useful.

? N , n(u, v), a, b are used for background subtraction. N is binary image mask consisting of binary pixels n(u, v).

N is equal M scaled down in width by a and height by b. a, b are chosen to be the largest value which removes the outline of the hands in the IR image.

? O(u, v, o) is a function and not part of the images shown in Figure 2. It is used as part of an adaptive threshold      filter. It computes the mean value of a block of o ? o pixels in width and height, centred on i(u, v). o is chosen to be larger than the width of a vein in pixels, but smaller than the typical distance between veins.

? J , j(u, v) represent the vein patterns. J is a binary image consisting of binary pixels j(u, v). If a pixel value is 1 it is considered vein structure, 0 represents the background.

We extract I and D from the ROI of the raw input bounded by rows u1, u2 and columns v1, v2. A mask M is created from D using a threshold filter. We choose to use a threshold filter because we have made the assumption that objects within a certain distance to the sensor belong to the hand of the user and everything else belongs in the background. The following is the definition for the threshold filter.

m(u, v) =  { 1 if z1 > d(u, v) ? z2  (1)  A secondary mask is created from M called N . N is M scaled down in width by a and height by b, using nearest neighbour interpolation. A smaller mask is used to remove the outline of the hand in background subtraction. A 4-connected components algorithm is used to clean N . All but the largest component is removed and replaced with zero value pixels.

The background is removed and vein patterns are extracted from I using an adaptive threshold filter. The filter takes in N and O(u, v, o) and produces result J . The filter is defined in the following formula.

j(u, v) =  ????? 1 if i(u, v) > O(u, v, o)  and n(u, v) = 1  (2)  B. Calculate Derivatives and Angles  The workflow for calculating derivatives and angles is depicted in the 2nd column from the left of Figure 2. J is corrected for perspective distortion caused by the surface of the hand dorsum being tilted in relation to the image plane. We use 3 derivatives to compute the angle of tilt based on the 3D depth map at each pixel. Then use them to approximate the average angles of tilt on 3 axes for the entire hand dorsum surface.

We define some variables before the detailed explanation.

? ?(u, v), ?(u, v), ?(u, v) are the angles along the 3 axes  at coordinates (u, v). This is computed from the 3D depth map.

? ?mean, ?mean, ?mean are the mean values of all the angles. E.g. ?mean is the mean value of all the computed ?(u, v).

? ?ref , ?ref , ?ref are the reference angles. We want to rotate the vein patterns in 3D until the average angles on 3 axes equal these values. ?ref , ?ref , ?ref are fixed constants for all implementations.

? ?, ?, ? are the differences between the mean and refer- ence angles. E.g. ? = ?ref ? ?mean.

? ?max, ?max, ?max are the maximum values allowed for ?, ?, ?. If exceeded, the input is rejected. ?max, ?max,  ?max are chosen to be some value less than 90, where it is possible to recover from jitter.

The 3 angles are calculated at locations where n(u, v) = 1.

For ?(u, v) these are calculated at locations where m(u, v) = 1. The angles are approximated using the following formulas [24].

?d(u, v)  ?u =   (d(u? 1, v)? d(u+ 1, v)) (3)  ?d(u, v)  ?v =   (d(u, v ? 1)? d(u, v + 1)) (4)  ?m(u, v)  ?u =   (m(u? 1, v)?m(u+ 1, v)) (5)  ?m(u, v)  ?v =   (m(u, v ? 1)?m(u, v + 1)) (6)  ?(u, v) = arctan( ?d(u, v)  ?u /1) (7)  ?(u, v) = arctan( ?d(u, v)  ?v /1) (8)  ?(u, v) = arctan( ?m(u, v)  ?v / ?m(u, v)  ?v ) (9)  We take all the ?(u, v) that have been computed and find the mean value. We call this ?mean. The same is applied to ?(u, v), ?(u, v) giving ?mean, ?mean respectively.

?mean, ?mean, ?mean are angles which represent the average tilt on 3 axes. We want to align J against 3 reference angles which we call ?ref , ?ref , ?ref . To do this we compute the difference between mean and reference angles, which are called ?, ?, ?. These are defined as follows.

? = ?ref ? ?mean (10) ? = ?ref ? ?mean (11) ? = ?ref ? ?mean (12)  A high level of tilt can obfuscate the vein pattern image.

A combination of perspective distortion and shadows cast by knuckles and depressions on the hand dorsum surface mean the vein pattern can become irrecoverable. Since it is possible to detect the tilt, we limit the largest absolute value of ?, ?, ?.

If these limits are exceeded, the input is rejected. We call these thresholds ?max, ?max, ?max.

C. Rotate to Reduce Distortion  We use the 3 previously computed angles to rotate the vein pattern image J in 3D and reduce perspective distortion. The algorithm workflow is depicted in the 2nd last column from the right of Figure 2. The properties of projective geometry [25] mean we have to convert each pixel with coordinates (u, v) into a different coordinate system, perform rotations to then convert it back. We first define some variables.

? (x, y) are the coordinates along the horizontal and vertical  axes of a pixel in the new coordinate system used to rotate the vein pattern image J .

? f is the focal length of the sensor. This is a fixed value based on the hardware.

? w, h are width and height of the region of interest. w = u2? u1, h = v2? v1.

Fig. 5: The algorithm workflow from vein pattern to sequence of points.

? H1, H2, H3 are the rotation matrices used to rotate the pixels in the vein pattern image.

The relationship between the two coordinate systems is the following.

d(u, v)  ??uv  ?? = ??f 0 bw/2c0 f bh/2c 0 0 1  ???? xy d(u, v)  ?? (13) Once converted to the new coordinate system the pixels in  J can be rotated using the following 3 rotation matrices.

H1 =  ??1 0 00 cos(?) sin(?) 0 ? sin(?) cos(?)  ?? (14) H2 =  ??cos(?) 0 ? sin(?)0 1 0 sin(?) 0 cos(?)  ?? (15) H3 =  ?? cos(?) sin(?) 0? sin(?) cos(?) 0 0 0 1  ?? (16) The matrices are applied to (x, y, d(u, v)). The relationship  in Equation 13 is used to transform it back into (u, v) coordinate system.

H1H2H3  ?? xy d(u, v)  ?? (17) D. Clean Image  After J is corrected for distortion, we get the corrected result which is cleaned-up. The clean-up is depicted in the last column from the right of Figure 2. We define some variables for this final step.

? K, k(u, v) represent the final vein pattern image after  perspective distortion correction and clean-up. k(u, v) is the binary pixel value at coordinates (u, v).

? g, any segment containing less than g non-zero pixels is removed. g is chosen to be large enough so that the majority of segments containing noise are removed.

A cleaning step is performed to finalise K. First a 4- connected components algorithm is run. All segments smaller than g pixels are removed. Small gaps are then filled. For every k(u, v) = 0 which has 2 neighbours that are non-zero then it is replaced with k(u, v) = 1. Finally, white space is trimmed.

We find the bounding box which encloses all non-zero pixels  Fig. 6: The effect of changing pre-processing parameters are shown in this image.

and remove rows and columns outside this bounding box. This is the last step in pre-processing.

E. Effect of Changing Pre-processing Parameters  We have provided a visual guide in Figure 6, on the effect of changing several pre-processing parameter values. We determined the optimal values for each parameter empirically.

However, once set these values stay the same for any user so long as the sensor remains the same.

They are grouped into 3 groups. Parameters a, b crops the vein pattern. o is the size of the adaptive filter. A smaller o preserves more detail, but introduces noise in the vein pattern image. A larger o leads to the removal of thinner veins. g is used to remove segments consisting of less than g pixels. A smaller g preserves detail but leaves in noise.

F. Convert Vein Pattern to Sequence  We convert the final binary vein pattern image into a sequence of points. The algorithm workflow is depicted in Figure 5. The points present the locations where veins intersect with each row in the vein pattern image. We want to find these points because they describe the shape of the vein pattern.

Finding these key points is similar to applying a line thinning algorithm. However, our approach is faster as it only requires one pass compared to the multiple passes needed for line thinning. We use one pass as we are not trying to preserve connectivity after thinning.

We define the sequence of points using the following variables.

? S, s(j) represent the sequence of points computed from a vein pattern image. s(j) represent the jth data point in S.

The number of vein intersections is estimated by the number of transitions from a zero value to non-zero value pixel in each row. The result is S = s(1), s(2), . . . , s(jmax) and s(j) = (j, v). v is the position of the point on the vertical axis, j is the row index.

Fig. 7: We compare the similarity of sequences of key points from 2 vein patterns. First by computing the Euclidean distance between key points in both sequences. Then the Euclidean distance is used to compute a similarity score.

G. Compare Sequences  Our similarity function is the Gaussian function. First we compute the Euclidean distance between each point in the reference and test sequences. The distance values are used to compute the final similarity score. The algorithm workflow is depicted in Figure 7. We define some variables to describe this process in detail.

? S is the sequence computed from reference vein pattern.

T is the sequence computed from the test vein pattern.

s ? S, t ? T .

? ||s ? t|| is the Euclidean distance between points s and t.

? KF (s, t, ?) = exp(? ||s?t||  ?2 ) is the Gaussian function.

? is the kernel width. It penalises mis-alignment between key points in reference and test images. The optimal value must be empirically determined.

The similarity score is calculated using the following for- mula. ?  s?S  ? s??S  KF (s, s?, ?)+? t?T  ? t??T  KF (t, t?, ?)?  2? ? s?S  ? t?T  KF (s, t, ?)  (18)

IV. EXPERIMENTS  This section outlines the experiments and implementation details of a proof of concept VeinDeep system. We collected data from several subjects and use this data to benchmark VeinDeep against Hausdorff and Kernel distance. We present benchmarks showing the matching accuracy, mean matching time and memory consumption. Then we test the effect of changing the kernel distance.

A. System Setup  We implemented VeinDeep on two different platforms: a Windows PC and Windows Computer Stick. The system spec- ifications are shown in Table I. As IR depth sensor equipped smartphones were not yet readily available at the time of writing, we simulate the device using a Compute Stick and Kinect V2 depth sensor. The Compute Stick represents the  Platform PC Compute Stick CPU 2.6 Ghz i5 3320M 1.33 Ghz Atom Z3735F Cores 2 4 RAM 8 GiB 2 GiB OS Windows 10 64 bit Windows 10 32 bit  TABLE I: The test platforms.

Fig. 8: The system used a Kinect V2.

kind of performance found in low power embedded devices like smartphones. The Kinect V2 was setup on a tripod point- ing down as shown in Figure 8. The system was developed in C++, using the OpenCV 3.1 library and Visual C++ 2015 compiler.

B. Data Collection  Data was recorded from a total of 20 participants. The par- ticipants included 12 males and 8 females. 17 were graduate students aged between 20 to 35 years. One individual aged 18, 2 aged between 50 - 60. The recordings were conducted in an office environment. 6 instances were recorded per hand, per participant. The right and left hands have different vein patterns so each participant?s hands were treated as a separate subject. This gave a total of 240 recorded instances from 40 different hands. Each recorded IR image and depth map is 512 ? 424 pixels in resolution. Our ROI is a centre portion 80? 80 pixels in resolution.

We have the participant make a fist for each recording.

This gesture forces the veins on the hand dorsum closer to the surface making them more visible. The IR illumination on a depth sensor is weak, this gesture helps improve data collection. We did not choose to use the palm as thicker tissue in the palm obstructs the vein patterns.

Each recording was made with the hand dorsum approxi- mately in the centre of the ROI, between 50 to 65 cm from the sensor. We had the participant retract their hand in between each recording. This procedure introduces some jitter, in order to create a data set with height and tilt variations.

The system parameter values were set using the heuristics from Section III. The parameter values are shown in Table

II. The f parameter is the focal length of the Kinect V2 lens which is 3.657 mm [15]. All tests were performed using these values unless otherwise stated.

VeinDeep Parameter Value u1, u2, v1, v2, z1, z2 216, 296, 172, 252, 500, 650 a, b 0.7, 0.8 o, g 11, 9 ?ref , ?ref , ?ref 90, 90, 180 ?max, ?max, ?max 25, 25, 35 f 3.657 ? 5  TABLE II: System parameter values.

C. Kernel and Hausdorff Distance  In this section we describe Kernel and Hausdorff distance.

Zhang?s Kernel distance is implemented in the same way as our similarity function. Shown in Formula 18. However, Zhang used all pixels instead of key points. So we used all pixels while testing Kernel distance.

Hausdorff distance does not have configurable parameters.

For every point k1 in reference pattern K1 we find the shortest Euclidean distance to some point k2 in test pattern K2. This gives a set of distances equal in number to non-zero value pixels in K1. We then find the maximum value in this set.

The exact definition is the following formula.

max k1?K1  ( min k2?K2  (||k1? k2||)) (19)  D. Matching Accuracy  Figure 9 shows the precision / recall values. Table III shows the confusion matrices when precision is at approximately 0.98. VeinDeep edges out Kernel distance in precision. Vein- Deep achieves a precision of 0.98 with a recall of 0.83. While Kernel distance has a precision of 0.90 at a similar recall value.

Higher precision mean lower false positives at the expense of greater false negatives. This is good for authentication systems as a false positive allows in an intruder which is disastrous.

While a false negative is only a temporary inconvenience.

For testing, we had 6 recordings for each hand. We ran- domly picked one to use as a reference vein pattern, the 5 others are used as positive samples, the remaining 234 recordings from other subjects are used as negative samples.

We do this for all 40 hands. For all 3 algorithms we pre-process the vein patterns using the procedure depicted in Figure 2.

Then we pass the cleaned vein pattern images through the 3 algorithms to get similarity scores. The Kernel distance function worked best with a kernel width of 1. While VeinDeep worked best with a kernel width of 5.

To generate Figure 9, we ran the 3 algorithms using the procedure described previously. On each iteration we recorded the similarity score. If this score was below a threshold we considered it a positive. If it matched the ground truth then it was considered a true positive, otherwise a false positive. If we were above the threshold then the result is considered a negative. If it matched the ground truth it was considered a true negative, otherwise a false negative. We adjust the thresholds to get our precision/recall curves. The threshold values are 0 - 350 for VeinDeep, 0 - 15 for Hausdorff and 0 - 2000 for Kernel Distance.

Fig. 9: The precision / recall results of vein pattern matching algorithms. VeinDeep with a precision of 0.98, a recall of 0.83, edges out Kernel distance which has a precision of 0.90 at a similar recall level.

VeinDeep Truth Observed Positive Negative Positive 166 3 Negative 34 9357 Kernel Distance Truth Observed Positive Negative Positive 173 21 Negative 27 9339 Hausdorff Distance Truth Observed Positive Negative Positive 1706 196 Negative 4 7654  TABLE III: The confusion matrices when precision is at approximately 0.98.

We believe VeinDeep performed best for the following reason. Kernel functions apply a penalty when a pixel in the reference image has no matching pixel close to the corresponding location in the test image. The input vein pattern images are often not perfectly aligned because of jitter. Key point selection in VeinDeep mean there are fewer potential points to penalise compared to Kernel distance.

E. Matching Speed  Figure 10 shows the mean time to process and match vein patterns on PC and Compute Stick platforms. We wanted to test if there was any unacceptable lag in response time.

We started to measure the time taken after reading the test image into memory and stopped once the similarity scores were computed. VeinDeep is the fastest by far taking 466 milliseconds on the Compute Stick. This is under 1/3 the time of Hausdorff distance and under 1/6 the time of Kernel distance.

The results coincide with a cursory analysis of the asymp- totic upper bound. We exclude the pre-processing step as all      3 algorithms use the same procedure. We first define nz(K1) as the number of non-zero pixels in K1, P (K1) the number of key points found in K1. If we are given a reference vein pattern image K1 and test pattern K2. Then assume mathematical operations such as exp(? ||k1?k2||   ?2 ) are O(1).

We get the following upper bounds.

Kernel distance consists of 3 parts as shown in Formula 18.

All 3 parts require a quadratic number of steps. The following expression is the complexity.

O(nz(K1)2 + nz(K2)2 + (nz(K1)? nz(K2))) (20)  Hausdorff distance consists of 2 parts as shown in Formula 19. The inner expression require nz(K2) steps. The outer expression requires nz(K1) repetitions of the inner loop. The total complexity is the following.

O(nz(K1)? nz(K2)) (21)  VeinDeep has a quadratic asymptotic upper bound like Kernel distance due to implementation similarities. However, input length is based on P (K1) and P (K2). The following expression is the complexity.

O(P (K1)2 + P (K2)2 + (P (K1)? P (K2))) (22)  We know that P (K1) ? nz(K1) and P (K2) ? nz(K2).

However, under most circumstances the number of non-zero pixels vastly exceeds the number of key points. So under most circumstances VeinDeep is faster.

F. Memory Use  Figure 11 shows the mean memory usage to process and match vein patterns on the Compute Stick platform. We wanted to know if the memory usage fit within our embedded system. We recorded the peak memory usage while processing each test image and computed the mean value. VeinDeep used 6 MiB, Kernel distance used 9 MiB and Hausdorff used 14 MiB. All 3 algorithms used a small portion of total available memory.

G. Changing the Kernel Width  All parameters except one in VeinDeep deal with pre- processing of the vein pattern image. Figure 12 shows the precision / recall values after of changing ?, the kernel width.

As previously explained, a smaller kernel width leads to greater penalties when a pixel in the reference image does not have a corresponding pixel in the same location in the test image. We can see choosing anything outside a narrow niche led to non-optimal results. Optimal values for this parameter must be determined empirically.



V. DISCUSSION AND CONCLUSION  This paper has presented VeinDeep to secure smartphones from opportunistic access. VeinDeep re-purposes the IR depth sensor to record vein patterns from the back of a person?s hand, in order to authenticate them. We build a proof of concept on a low power Compute Stick and incorporated a Kinect V2 as our depth sensor. We tested our system with 240 vein  Fig. 10: The mean run time to compare a pair of vein pattern images.

Fig. 11: The mean memory usage to compare a pair of vein pattern images.

pattern images take from 40 hands, belonging to 20 people.

We achieved a matching precision of 0.98 and recall of 0.83, while taking an average of 6 MiB and 466 milliseconds per match. This is better than Hausdorff distance an older vein matching algorithm and Kernel distance a newer algorithm, which had precisions of 0.5 and 0.9 respectively at similar recall levels. VeinDeep does this while taking 1/6 run time, 2/3 the memory of Hausdorff distance and 1/3 the run time, 1/2 the memory of Kernel distance. We believe our prototype is sufficient to demonstrate the viability of VeinDeep.

Aside from authentication VeinDeep can be used to per-      Fig. 12: The precision / recall values of VeinDeep after chang- ing ?. ? is the kernel width, which controls the penalty when a point in the reference image does not closely correspond with a point in the test image. Changing this value outside a narrow niche led to non-optimal results. It is the only parameter in VeinDeep which is not related to pre-processing of the vein pattern image.

sonalise devices. In a multi-user system custom setting can be retrieved for every user, such as custom ring tones and wallpapers.

There are also several potential ways of improving Vein- Deep for future consideration. One possible improvement is to parallelise the algorithm. Most of the time is spent on image filters which loop through the input. Each iteration does not require write access to shared resources. Therefore it can greatly benefit from parallelism.

We are greatly limited by our sensor resolution. An im- proved sensor could give us the opportunity to test under more difficult scenarios. We had participants retract their hand between each data recording in order to introduce some jitter.

We are able to detect any extreme tilt of the hand dorsum and reject such edge cases. A better sensor could allow recovery of clean vein pattern images even if the hand is extremely tilted in relation to the sensor plane.

Another task for future consideration is to test the system on actual hardware. It allows testing of power consumption which is not possible on a simulated system.

Our proposal for using vein patterns is targeted at thwarting opportunistic access to a device. We do not believe it will stand up to dedicated attackers. We would like to try and break vein pattern recognition in future work.

