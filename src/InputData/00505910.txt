Data mining: machine learning, statistics, and databases

Abstract Knowledge discovery in databases and data min-  ing aim a t  semiautomatic tools for the analysis of large data sets. We give an overview of the area and present some of the research issues, especially from the database angle.

1 Introduction Knowledge discovery in databases (KDD), often  called data mining, aims at the discovery of useful information from large collections of data. The dis- covered knowledge can be rules describing properties of the data, frequently occurring patterns, clusterings of the objects in the database, etc. Data mining has in the 1990?s emerged as visible research and develop- ment area; both in industry and in science there seems to be a lack of methods for efficient analysis of large data sets.

Current technology makes it fairly easy to collect data, but data analysis tends to be slow and expen- sive. There is a suspicion that there might be nuggets of useful information hiding in the masses of unan- alyzed or underanalyzed data, and therefore semiau- tomatic methods for locating interesting information from data would be useful. There are several success- ful applications of data mining. See [9] for a recent overview of the area.

This paper gives a short discussion of some of the database-oriented research issues in knowledge discov- ery. We start in Section 2 by briefly discussing the KDD process, basic data mining techniques, and list- ing some prominent applications. Section 3 discusses the role of machine learning and statistics in KDD and data mining. In Section 4 we move to the role of databases in knowledge discovery by looking a t  a simple example of data mining, namely the problem of discovering association rules. We present a simple algorithm for this task, and show in Section 5 how the same ideas can be used for other types of data, too.

Section 6 moves to a generic data mining algorithm, and discusses some of the architectural issues in data mining systems. Based on these ideas, we consider in  *Part of this work was done while the author was visiting the Max Planck Institut fiir Informatik in Saarbriicken, Ger- many. Work supported by the Academy of Finland and by the Alexander von Humboldt Stiftung.

Section 7 the possibilities of specifying KDD tasks in a high level language and compiling these specifica- tions to efficient discovery algorithms. Section 8 con- siders the possibilities of representing large data sets by smaller condensed representations, and Section 9 is a short conclusion.

2 The KDD process The goal of knowledge discovery is to  obtain useful  knowledge from large collections of data. Such a task is inherently interactive and iterative: one cannot ex- pect to obtain useful knowledge simply by pushing a lot of data to a black box. The user of a KDD sys- tem has to have a solid understanding of the domain in order to select the right subsets of data, suitable classes of patterns, and good criteria for interesting- ness of the patterns. Thus KDD systems should be seen as a interactive tools, not as automatic analysis systems.

Discovering knowledge from data should therefore be seen as a process containing several steps:  1. understanding the domain;  2. preparing the data set;  3. discovering patterns (data mining),  4. postprocessing of discovered patterns, and  5 .  putting the results into use.

(See [8] for a slightly different process model and ex- cellent discussion.)  Understanding the domain of the data is naturally a prerequisite for extracting anything useful: the user of a KDD system has to have some sort of under- standing about the application area before any valu- able information can be obtained. On the other hand, if very good human experts exist for a domain, it can be hard for semiautomatic tools to obtain any novel information. This can be the case in fairly stable do- mains, where the humans have had time to achieve expertise even in the details of the data. A possi- ble example occurs in some areas of retailing, where the products and customer profiles can stay about the same for longer periods of time. The easiest appli- cation areas for KDD seem to  be ones where general  0-8186-7264-1196 $05.00 0 1996 IEEE    human experts can be found but the actual microlevel properties of the data are changing. This seems to be the case in, e.g., telecommunications, where the oper- ators of the networks have a fairly good overview of the systems characteristics, but changes and updates in equipment and software mean that actual expertise in the details of the data is more difficult to obtain.

Preparation of the data set involves selection of the data sources, integration of heterogeneous data, clean- ing the data from errors, assessing noise, dealing with missing values, etc. This step can easily take up to 80 % of the time needed for the whole KDD process; this is not surprising, since the difficulties in data integra- tion are well known.

The pattern discovery plhase in KDD is the step where the interesting and frequently occurring pat- terns are discovered from the data. In this paper we follow the terminology introduced in [8]: data mining refers to the pattern discovery part of knowledge dis- covery. Elsewhere, especially in industry, data mining is often used as a synonym for KDD.

The data mining step can use various techniques from statistics and machine learning, such as rule learning, decision tree induction, clustering, inductive logic programming, etc. The emphasis in data min- ing research is mostly on efficient discovery of fairly simple patterns.

The KDD process does not stop when patterns have been discovered. The user has to be able to under- stand what has been discovered, to view the data and patterns simultaneously, contrast the discovered pat- terns with background knowledge, etc. Postprocessing of discovered knowledge involves steps such as further selection or ordering of patterns, visualization, etc.

Some approaches to KDD methodology put a heavy emphasis on postprocessing.

The KDD process is necessarily iterative: the re- sults of a data mining step can show that some changes should be made to the data set formation step, post- processing of patterns can cause the user to look for some slightly modified types of patterns, etc. Efficient support for such iteration is one important develop- ment topic in KDD.

Prominent applications of KDD include health care data, financial applications, and scientific data [26, 191. One of the more spectacular applications is the SKICAT system [7], which operates on 3 terabytes of image data, producing a classification of approxi- mately 2 billion sky objects into a few classes. The task is obviously impossible to  do manually. Using example classifications provided by the users, the sys- tem learns classification rules that are able to do the categorization accurately and fast.

In industry, the success of KDD is partly related to the rise of the concepts of data warehousing and on-line analytical processing (OLAP). These strate- gies for the storage and processing of the accumulated data in an organization have become popular in recent years. KDD and data mining can be viewed as ways of realizing some of the goals of data warehousing and OLAP.

3 Data mining, machine learning, and statistics  Data mining combines methods and tools from a t least three areas: machine learning, statistics, and databases. Indeed, one can sometimes hear the fol- lowing comments.

0 Data mining is just machine learning!

0 Data mining is just statistics!

0 What does data mining have to do with  In this section we discuss briefly the first two points; the rest of the paper is devoted to a discussion on the third point.

The close links between machine learning, statis- tics, and data mining are fairly obvious. All three ar- eas aim at locating interesting regularities, patterns, or concepts from empirical data. The exact relation- ships of these areas have been subject to some debate.

Machine learning methods form the core of data mining: decision tree learning or rule induction is one of the main components of several data mining algo- rithms. There are also some differences, however.

The emphasis on the process of knowledge discov- ery is one; large parts of machine learning literature concentrate on just the learning or induction step, al- though exceptions of course exist.

The next difference concerns the relative roles of concepts and data. It seems to me that most ma- chine learning research assumes there is something to be learned, i.e., that there is an underlying interesting concept or mechanism that produces the data. The data can be corrupted by noise, errors, etc., but still the idea is that there is an interesting concept a t  the bottom. In knowledge discovery, on the other hand, the data is the primary thing, and one does not neces- sarily assume that there would be any sensible struc- ture behind the data. For example, in analyzing retail sales data, the data is what it is, and the users are not interested in obtaining a full understanding of it; useful nuggets of information are sufficient. Of course, this difference is not absolute.

A third difference is related to the goals. KDD sys- tems typically have fairly modest aims, in terms of the complexity of the obtained knowledge. Whereas parts of machine learning research aim a t  learning things that are difficult for humans to do, most of the work in KDD aims at finding knowledge that a competent data analyst would in principle be able to find, if he/she had the time. This distinction is particularly evident when one compares the area of machine discovery to knowledge discovery.

An often cited difference between KDD and ma- chine learning is the amount of data. Traditionally, machine learning research has concentrated on looking a t  data sets containing hundreds or thousands of ex- amples, while KDD applications consider larger data sets. I?m not sure how significant this distinction is, however: some machine learning work has been done on huge data sets, and KDD methods can be a use- ful even on small data collections. Furthermore, the  databases?

essential source of complexity in data mining is typi- cally not the number of objects in the database, but rather the number of attributes: the number of pos- sible patterns typically grows at least exponentially in the number of attributes. This growth is the real source ofdifficulty, not the number of objects in the database.

Summarizing, machine learning is a t  the core of KDD, but there are differences between the areas.

In statistics the term data mining has been used for a long time, often in slightly derogatory fashion, as re- ferring to  data analysis without clearly formulated hy- potheses. A more fashionable term is ezploratory data analysis (EDA) [29], which stressed the supremacy of data as guiding the analysis process. KDD and EDA have very similar aims and methods.

According to the interesting statistical perspective on KDD by Elder and Pregibon [6], the focus of statis- tics has gradually moved from model estimation to model selection. Instead of looking for the parameter values that make a model fit the data well, also the model structure is part of the search process. This trend fits the goals of KDD nicely: one does not want to fix the model structure in advance. The recent ad- vances in, say Markov chain Monte Carlo (MCMC) methods, make it possible to consider far larger model spaces than previously. In addition to these tech- niques, the KDD community has lots to learn from statistics] e.g., in the handling of uncertainty.

The main differercz between KDD and statistics is perhaps in the extensive use of machine learning meth- ods in KDD, in the volume of data, and in the role of computational complexity issues in KDD. For ex- ample, even MCMC methods have difficulties in han- dling tens of thousands of parameter values; some sort of combinatorial preprocessing is needed to make the model selection task tractable. It seems that such combinations of methods can be useful: combinato- rial techniques are used to prune the search space, and statistical methods are used to explore the remaining parts in great detail.

atabases and data mining What is the role of databases in data mining?

Database management systems are systems especially developed for the storage and flexible retrieval of large masses of structured data, so in principle they should be suited for KDD, including data mining.

In the remaining sections of this paper we dis- cuss some issues that are connected with the use of database management systems in KDD applications.

We introduce the issues by considering first a simple data mining problem, and then generalizing it.

The problem we consider is finding association rules from binary data. Assume we have a set R = {AI,. . ., A p }  of binary attributes, i.e., the domain of each A, is (0, l}. A relation r = { t l ,  . . . , in} over the schema R is a matrix with columns R and n rows, each row being a vector of length p of 0?s and 1?s.

An association rule [l] about T is an expression of the form X 3 B ,  where X C R and B E R \ X. The intuitive meaning of the rule is that if a row of the  matrix T has a 1 in each column of X ,  then the row tends to have a 1 also in column B.

Examples of data where association rules might be applicable include the following.

A student database at a university: rows corre- spond to students, columns to courses, and a 1 in entry (s ,  c )  indicates that the student s has taken course c.

m Data collected from bar-code readers in super- markets: columns correspond to products, and each row corresponds to the set of items pur- chased a t  one time.

m A database of publications: the rows and columns both correspond to publications, and ( p , p ? )  = 1 means that publication p refers to publication p? .

A set of measurements about the behavior of a bunch of systems, say exchanges in a telephone network. The columns correspond to the presence or absence of certain conditions, and each row correspond to a measurement: if entry (m, c )  is 1, then a t  measurement m condition c was present.

Given W C R, we denote by s (W,  r )  the frequency of W in T :  the fraction of rows of r that have a 1 in each column of W .  The frequency of the rule X =+ B in T is defined to be s ( X  U { B } ,  r ) ,  and the confidence of the rule is ;i(X U {B}, r ) / s ( X ,  r ) .

In discovery of association rules, the task is to find all rules X + B such that the frequency of the rule is at least a given threshold U and the confidence of the rule is at least another threshold 8. In large retailing applications the number of rows might be 10? or even l o 7 ,  and the number of columns around 5000. The fre- quency threshold U typically is around lo-? - The confidence threshold 8 can be anything from 0 to 1. From such a database one might obtain hundreds or thousands of association rules. (Of course, one has to be careful in assigning any statistical significance to findings obtained from such methods.)  Note that there is no predefined limit on the num- ber of attributes of the left-hand side X of an associa- tion rule X 3 B;  this is important so that unexpected associations are not ruled out before the processing starts. It also means that the search space of the rules has exponential size in the number of attributes of the input relation. Handling this requires some care for the algorithms, but there is a simple way of pruning the search space.

We call a subset X C R frequent in r ,  if s ( X ,  r )  2 U.  Once all frequent sets of r are known, finding the association rules is easy. Namely, for each frequent set X and each B E X verify whether the rule X\{B}  j B has sufficiently high confidence.

How can one find all frequent sets X ?  This can be done in a multitude of ways [I, 2 ,  12, 14, 281. A typical approach [I, 21 is to use that fact that all subsets of a frequent set are also frequent.

First find all frequent sets of size 1 by reading the data once and recording the number of times each at- tribute A occurs. Then form candidate sets of size 2 by     taking all pairs ( B ,  C} of attributes such that { B }  and {C} both are frequent. The frequency of the candidate sets is again evaluated against the database. Once fre- quent sets of size 2 are known, candidate sets of size 3 can be formed; these are sets { B ,  C, D }  such that { B ,  C}, { B ,  D} ,  and (C, D} are all frequent. This process is continued until no more candidate sets can be formed.

C := {{A}I I A E R}; 3 := 0; while C # 0 do  As an algorithm, the process is as follows.

3? := the sets X E C that are frequent; add F? to 3; C := new candidate sets generated from F?;  od;  The algorithm has to read the database a t  most K+ 1 times, where K is the size ofthe largest frequent set.

In the applications, K is small, typically at most 10, so the number of passes through1 the data is reasonable.

(Note that there are at least frequent sets; thus, if K is much larger than 10, the output of the frequent set algorithm gets quite big. There still might be only few interesting association rules, however.)  A modification of the above method is obtained by computing for each frequent set X the subrela- tion rx  C T consisting of those TOWS t E r such that t [A]  = 1 for all A E X .  Then it is easy to see that for example T { A , B , C )  = ?- {A$)  n r { B , c } .  Thus the re- lation r x  for a set X of size IC can be obtained from the relations rxt and r x ? ,  where X? = X \ { A }  and X? = X \ { B }  for some A ,  B E X with A # B. This method has the advantage that rows that do not con- tribute to any frequent set willl not be inspected more than once. For comparisons of the two approaches, see [ 2 ,  14, 281.

The algorithms described above work quite nicely on large input relations. Their running time is ap- proximately O I N F ) ,  where N = n p  is the size of the input and F is the size of the output (the sum of the sizes of the frequent sets).? This is nearly linear, and the algorithms seem to scale nicely to tens of millions of examples. The only case when they fail is when the output is too large, i.e., there are too many fre- quent sets, but in that case the whole task is not very sensible.

The methods for finding frequent sets are simple: they are based on one nice but simple observation (subsets of frequent sets must be frequent), and use straightforward implementation techniques.

A naive implementation of the algorithms on top of a relational database system would be easy: we need to pose to the database management system queries of the form ?What is s ( { A l , .  . . , Ak}, r)??, or in SQL  select count(*) from r t where t [A l ]  = 1 and . . .  aind t[Ak] = 1  ?This is actuallynot quite true: the running time is O ( N F ? ) , where F? is the sum of the sizes of the frequent sets and the sets in the candidate collection C during the operation of the algorithm [24].

The number of such queries can be large: if there are thousands of frequent sets, there will be thousands of queries. The overhead in performing the queries on an ordinary dbms would probably be prohibitive.

The customized algorithms described above are able to evaluate masses of such queries reasonably ef- ficiently, for several reasons. First, all the queries are very simple, and have the same general form; thus there is no need to compile each query individually.

Second, the algorithms that make repeated passes through the data evaluate a large collection of queries during a single pass. Third, the algorithm that build the relations r x  for frequent sets X use the results of previous queries to avoid looking a t  the whole data for each query.

5 Application: finding episodes from sequences  The basic ideas of the algorithm for finding associ- ation rules are fairly widely applicable. In this section we describe an application of the same ideas to tRe problem of finding repeated episodes in sequences of events.

Consider a sequence of events ( e , t ) ,  where e is the type of the event and t is the time when the event occurred. Such data is routinely collected in, e.g., telecommunication networks, process monitoring, quality control, user interface studies, and epidemi- ology. There is an extensive statistical literature on how such data can be analyzed, but most methods are suited only for small numbers of event types.

For example, a telecommunications network alarm database is used to collect all the notifications about abnormal situations in the n t . v x k .  V - 2  number of event types is around 200, and there are 1000-10000 alarms per day [13].

As a first step in analyzing such data, one can try to find which event types occur frequently close together.

Denoting by E the set of all event types, an epzsode cp is a partially ordered set of elements from E.  An episode might, e.g., state that events of type A and B occur (in either order) before an event of type C.

Given an alarm sequence r = ( e l , t I ) ,  . . . , ( e n , & ) ,  a slzce rt of r of width W consists of those events ( e x ,  t z ) of r such that t 5 t ,  5 t + W .  An episode cp OCCILTS in r t ,  if there are events in T t  corresponding to the event types of p and they occur in an order respecting the partial order of p. An episode is frequent, if it occurs in sufficiently many slices of the original sequence.

How to find all episodes from a long sequence of events? Using the same idea as before, we first locate frequent episodes of size 1, then use these to generate candidate episodes of size 2, verify these against the data, generate candidate episodes of size 3, etc. The algorithm can be further improved by using incremen- tal recognition of episodes; see [25]  for details, and [22 ] for extensions. The results are good: the algorithms are efficient, and using them one can find easily com- prehensible results about the combinations of event types that occur together.

6 A generic data mining algorithm and architecture  A fairly large class of data mining tasks can be de- scribed as the search for interesting and frequently oc- curring patterns from the data. That is, we are given a class P of patterns or sentences that describe proper- ties of the data, and we can specify whether a pattern p E P occurs frequently enough and is otherwise in- teresting. That is, the generic data mining task is to find the set  P I ( d , P )  = { p  E P 1 p occurs sufficiently often in d and p is interesting}.

This point of view has either implicitly or explicitly been used in discovering integrity constraints from databases, in inductive logic programming, and in ma- chine learning [4, 5 ,  18, 19, 201; some theoretical results can be found in [24], and a suggested logical formalism in [IT].

For association rules, the pattern class is the set of all rules of the form X + B ,  and a rule is interesting if its confidence is sufficiently high. For finding episodes, the patterns are the episodes and there need not be any interestingness criterion.

The algorithm given above can be generalized to finding P l ( d ,  P) as follows.

C := initial patterns from P; while C # 0 do  for each p E C  3 := 3 U { p  E.C 1 p is sufficiently frequent in d } ; C := new candidates generated from 3;  find the number of occurrences of p in d ;  od; output interesting patterns from 3;  In addition to discovering association rules or fre- quent episodes, istantiations of this algorithm include finding keys of relations [21], hill-climbing searches for best descriptions [15, 191, etc. In hill-climbing, the set C will contain only the neighbors of the current ?most interesting? pattern. For other strategies, a possible problem with this algorithm is that the test for inter- estingness is applied only a t  the end. If there are lots of frequent but uninteresting patterns, the algorithm will use a lot of time inspecting these.

The generic algorithm suggests a data mining sys- tem architecture consisting of a discovery module and a database management system. The discovery mod- ule sends queries to the database, and the database answers. The queries are typically of the form ?How many objects in the database match p?,  where p is a possibly interesting pattern; the database answers by giving the count.

If implemented naively, this architecture leads to slow operations. To achieve anything resembling the efficiency of tailored solutions, the database manage- ment system should be able to utilize the strong simi- larities between the queries generated by the discovery module.

The view of data mining as locating frequently occurring and interesting patterns from data sug- gests that data mining can benefit from the exten- sive research done in the area of combinatorial pattern matching; see, e.g., [IO].

7 Towards higher-level data mining Currently, data mining research and development  consists mainly of isolated applications. Among oth- ers, Imielinski [16] has eloquently argued that data mining is today a t  the same state as data manage- ment was in the 1960?s: then all data management applications were ad hoc; only the advent of the re- lational model and powerful query languages made it possible to develop applications fast e Consequently, data mining would need a similar theoretical frame- work.

The approach of computing P l ( d ,  P) presented in the previous section provides one possible framework.

A data mining task could be given by specifying the class P and the interestingness predicate. That is, the user of a KDD system makes a pattern query. Mim- icking the behavior of an ordinary dbms, the KDD system compiles and optimizes the pattern query, and executes it by searching for the patterns that satisfy the user?s specifications and have sufficient frequency in the data.

As an example, consider the simple case of mining for association rules in a course enrollment database.

The user might say that he/she is interested only in rules that have the ?Data Management? course on the left-hand side. This restriction can be utilized in the algorithm for finding frequent sets: only candidate sets that contain ?Data Management? need to be consid- ered.

Developing methods for such pattern queries and their optimization is currently one of the most inter- esting research topics in data mining. So far, even the simple techniques such as the above have not been suf- ficiently studied. It is not clear how far one can get by using such methods, but the possible payoff is large.

In addition to developing query processing strate- gies for data mining applications, changes in the un- derlying storage model can also have a strong effect on the performance. A very interesting experiment in this direction is the work on the Monet database server developed a t  CWI in the Netherlands by Martin Ker- sten and others [15, 31. The Monet system is based on the vertical partitioning of the relations: a relation with k attributes is decomposed into IC relations, each with two attributes: the OID and one of the origi- nal attributes. The system is built on the extensive use of main memory, has an extensible set of basic operations, and supports shared-memory parallelism.

Experiments with Monet on data mining applications have produced quite good results [15, 141.

8 Condensed representations We remarked in Section 2 that KDD is an iterative  process. Once a data mining algorithm has been used to discover potentially interesting patterns, the user often wants to view these patterns in different ways, have a look a t  the actual data, visualize the patterns,     etc. A typical phenomenon is also that some pattern p looks interesting, and the user wants to  evaluate other patterns that closely resemble. p .  In implementing such queries, caching of previous results is obviously useful.

Still, having to go back to the original data each time the user wants some more information seems some- what wastful. Similarly, in the generic data mining algorithm presented in Section 6 the frequency and interestingness of each pattern are verified against the database. It would be faster to  look at some sort of short representation of the data.

Given a structure d E ID, and a class of patterns P ,  a condensed representation for d and P is a data structure that makes it possible to  answer queries of the form ?HOW many times does p E P occur in d? approximately correctly and more efficiently than by looking a t  d itself.

A simple example of a condensed representation is obtained by taking a sample from the data: by count- ing the occurrences of the pattern in the sample, one gets an approximation of the number of occurrences in the original data. Another, less obvious example is given by the collection of frequent sets of a 0-1 val- ued relation [23]: the collection of frequent sets can be used to give approximate answers to  arbitrary boolean queries about the data, even though the frequent sets represent only conjunctive concepts. The data cube [I13 can also be viewed as a condensed representation for a class of queries. Similarly, in computational ge- ometry the notion of an &-approximation [27] is closely related.

Developing condensed representations for various classes of patterns seems a promising way of improving the effectiveness of data mining algorithms. Whether this approach is generally useful is still open.

9 Concluding remarks Data mining and knowledge discovery are currently  quite popular, and there is lots of hype around. As the area is partly a mixture of techniques from different fields, there is also the danger of reinventing old (bad) solutions.

We summarize some of the database related re- search directions.

1. Development of pattern query specification lan- guages and techniques for optimizing pattern queries.

2. Condensed representations for various classes of pat terns.

3. Caching strategies for processing strongly related queries.

4. Combinations of data mining and statistical tech-  5. Using background knowledge (e.g., metadata) in  6. Tools for selecting, grouping, and visualizing dis-  niques.

the KDD process.

covered knowledge.

Acknowledgements Discussions with Tomasz Imielinski, Willi Kloes-  gen, Arno Siebes, and Stefan Wrobel have been most useful. Pirjo Ronkainen and Hannu Toivonen provided comments on an earlier version.

