Efficient Probabilistic Skyline Query Processing in MapReduce

Abstract?As a popular parallel programming model, how to process probabilistic skyline query over uncertain data in MapReduce framework is becoming an urgent problem to be resolved. In MapReduce framework, implementing prob- abilistic skyline query is nontrivial since the probabilistic skyline query is not decomposable. Therefore, in this paper, we propose a filter-refine two phases approach in MapRe- duce that translates the probabilistic skyline query into two decomposable computations for obtaining the final results.

Firstly, we describe the whole processing procedure of filter- refine, and then propose an efficient probabilistic skyline query processing algorithm in MapReduce. Furthermore, to reduce the computation and communication cost, we develop the optimized probabilistic skyline query processing algorithm to prune the unpromising data both in filter and refine phases.

Finally, we conduct extensive experiments on synthetic data to verify the effectiveness and efficiency of the proposed filter- refine approach with various experimental settings.

Keywords-probabilistic skyline; MapReduce; uncertain data

I. INTRODUCTION  Nowadays, with the rapid development of data collection and data processing, the uncertain data are inherently exist in many applications by the factors like data incompleteness and randomness, privacy protection, imprecise measuring equipments, and delayed data updates, etc. Data manage- ment over uncertain data is becoming a hot topic in both industry and academic communities. A considerable quantity of research achievements have been proposed to model the uncertain data and answer many types of queries, such as top-k query [1], [2], skyline query [3], [4], [5], and so on.

Although probabilistic skyline query over uncertain data has gained many achievements [3], [4], [6], the existing research achievements mainly focus on the centralized en- vironments. When processing the numerous data, it requires distributed approach to process the probabilistic skyline query over uncertain data. Meanwhile, probabilistic skyline query is not decomposable, since the union of the results in each subset is not the answers over the entire uncertain database (to illustrate in Section II-A). Therefore, we cannot simply divide the large scale datasets into small subsets, and then compute the union of the returned probabilistic skyline query answers of each subset. As the celebrated parallel programming model and cloud computing architecture, how to process the probabilistic skyline query over uncertain data in MapReduce [7] is an urgent problem to be resolved.

In this paper, we propose a two-phases approach, named  filter-refine, to translate the non-decomposable probabilistic skyline query into two decomposable problems and process the probabilistic skyline query in MapReduce framework. In the first phase, named filter, we first obtain the Candidate Set and the Affect Set (to define in Section II) by one pass MapReduce computation. In the second phase, named refine, we can identify the final results of probabilistic skyline query from the Candidate Set according to the Affect Set through the MapReduce computations. Furthermore, to lower the computation and communication cost, the optimized prob- abilistic skyline query processing algorithm is presented to prune more unpromising uncertain data both in filter and refine phases of the filter-refine approach. In summary, the contributions of this paper can be summarized as follows:  ? A two-phases approach, named filter-refine, is proposed to resolve the problem of probabilistic skyline query.

? The probabilistic skyline query algorithm and its opti- mized algorithm are proposed in our filter-refine based on MapReduce framework.

? We conduct extensive experiments to verify the effec- tiveness and efficiency of the filter-refine approach.

The rest of this paper is organized as follows. In Section II, our filter-refine approach and the probabilistic skyline query algorithm in MapReduce framework are proposed.

The optimized probabilistic skyline query algorithm is de- veloped in Section III. The experimental results are reported in Section IV. Section V briefly reviews the related work.

Finally, we conclude this paper in Section VI.



II. PROBABILISTIC SKYLINE QUERY PROCESSING  In this section, the problem statement is first illustrated in Section II-A. Then, the preliminaries are given in Section II-B. Our filter-refine approach and the proposed probabilis- tic skyline query algorithm are presented in Section II-C.

A. Problem Statement  Uncertain data model and possible world semantics.

Suppose an uncertain database D is composed of a set of n tuples ti (1 ? i ? n). Each tuple ti has the existence probability P (ti). In addition, based on the uncertain data model [8], a possible world W can be instantiated by taking a set of tuples from the uncertain model. Each tuple can choose its existential status independent of the other tuples, not considering any mutually exclusive rule. The possible world probability of W is calculated by Equation 1.

2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.35     P (W ) = ?  t?W  p(t)? ?  t??W  (1? p(t)) (1)  If we define the set of all possible worlds as ?, then the value of  ? W?? P (W ) = 1. When an uncertain database  has two uncertain tuples, the number of the possible world instance is 22 = 4, and then n uncertain tuples is 2n. There- fore, the number of possible world instances is much larger than the number of uncertain tuples. So, it is unpractical to enumerate and compute the whole possible world instances.

For probabilistic skyline query as defined in [9], the Skyline Probability is computed by Definition 1 and the Threshold Skyline is described in Definition 2.

Definition 1: (Skyline Probability) Given an uncertain database D, the Skyline Probability of an uncertain tuple t equals to the product of its existence probability (P (t)) and the nonexistence probability of uncertain tuple t?, (1-P (t?)), that t? dominates t (t? ? t), which can be written as follows:  PSKY (t) = p(t)? ?  t??D,t??t  (1 ? p(t?)) (2)  Definition 2: (Threshold Skyline) Given an uncertain database D and a threshold p, the results of Threshold Skyline in D is the uncertain tuples with Skyline Probability are larger than p. (p=0.3 of the examples in this paper).

PSKY (D) = {t|PSKY (t) > p} (3)  X  Y  t12 (9,6,0.3)  t11 (8,7,0.6)  t10 (7.5,8,0.7)  t9 (6.5,7,0.4)  t1 (1,9,0.7)  t2 (2,4,0.6)  t3 (4,7,0.8)  t4 (5,0.5,0.3) t5 (1.5,1,0.2) t7 (4,1.5,0.5)  t6 (3.5,6,0.5)  t8 (6,2.5,0.8)   Figure 1: An Example of Uncertain Database D  Figure 1 shows the uncertain database D to be used in our paper (i.e. D1 ?D2 ?D3), with the uncertain database D1, containing the tuples t1, t2, t3, t4; the uncertain database D2, containing the tuples t5, t6, t7, t8; the uncertain database D3, containing the tuples t9, t10, t11, t12. For example, the Skyline Probability of t2 equals to the product of its existence probability 0.6 and the nonexistence probability of t5 dominating t2, equivalently 0.8?(1-0.2)=0.64.

After introducing the above definitions, whether an un- certain tuple is the final probabilistic skyline query result is dependent on all the uncertain tuples that dominate itself. In addition, a query operator op is decomposable [10], when it can be computed by another operator op? as follows:  op(?ni=1Di) = op ?(?ni=1op(Di)). Unfortunately, the prob-  abilistic skyline query is not a decomposable problem ac- cording to Definition 1 and the above condition. Consider the uncertain databases D1 and D2 in Figure 1. Now, we want to gain the probabilistic skyline query results over the union of D1 and D2. It is known that PSKY (D1) is {t1, t2, t3}, while PSKY (D2) is {t6, t7, t8}. The probabilistic skyline query results on the union of PSKY (D1) and PSKY (D2) are shown in Figure 2(a), {t1, t2, t7, t8}. The probabilistic skyline query results in D1 ? D2 are shown in Figure 2(b), {t1, t2, t7}. Obviously, PSKY (D1) ? PSKY (D2) is not equal to PSKY (D1 ? D2). The above example further demonstrates that the probabilistic skyline query is not a decomposable problem. We cannot divide the whole database into several subsets and combine the results of each subset as the final results of probabilistic skyline query.

X  Y  t1(1,9,0.7)  t2 (2,4,0.6)  t3(4,7,0.8)  t7 (4,1.5,0.5)  t6 (3.5,6,0.5)  t8 (6,2.5,0.8)   (a) PSKY (D1) ? PSKY (D2) X  Y  t1 (1,9,0.7)  t2 (2,4,0.6)  t3 (4,7,0.8)  t4 (5,0.5,0.3) t5 (1.5,1,0.2) t7 (4,1.5,0.5)  t6 (3.5,6,0.5)  t8 (6,2.5,0.8)   (b) PSKY (D1 ?D2)  Figure 2: An Example of Non-decomposable Problem  Therefore, it is very difficult to process the probabilistic skyline query over large scale datasets in MapReduce. In MapReduce, the original dataset is divided into independent splits processed by the Mappers respectively. So, in each Mapper, it can only compute the local Skyline Probability of each uncertain tuple in its split. It cannot obtain the dominance information of the other Mappers. Just as the above example, although the Reducers can combine the intermediate results of all Mappers, we still cannot gain the final correct Threshold Skyline answers. It is necessary to compute the nonexistence probability of all the uncertain tuples dominating the given tuple so as to identify whether this tuple is the answer or not. So, only one pass MapReduce computation cannot obtain the final results. Then, we pro- pose a two-phases approach, named filter-refine, to process the probabilistic skyline query in MapReduce.

B. Preliminaries  Although the probabilistic skyline query is not decompos- able and is hard to be processed in MapReduce framework, there are still many attributes and characteristics that can be used to realize the probabilistic skyline query. We consider the whole uncertain database D is composed of the uncertain subset D1, D2,...,Dm, (D = ?mi=1Di). During the computa- tion course, according to Definition 1 and Definition 2, when an uncertain tuple ti satisfies  ? t? i ?Di,t  ?  i ?ti  (1-P (t?i)) ? p, it     must be irrelative to the final results shown in Theorem 1.

Furthermore, if these uncertain tuples are deleted, the final results cannot be affected shown in Theorem 2.

Theorem 1: If an uncertain tuple ti in Di satisfies:? t? i ?Di,t  ?  i ?ti  (1-P (t?i)) ? p, it must not be the final prob- abilistic skyline query result in Di and even in D.

Proof: Suppose the uncertain tuple ti is the final result of Di, we have the following equation:  PSKY (ti) = P (ti)? ?  t? i ?Di,t  ?  i ?ti  (1? P (t?i)) > p (4)  Since 0<P (ti)<1, so ?  t? i ?Di,t  ?  i ?ti  (1-P (t?i)) > p in Equa- tion 4. It is contradictory to the hypothesis, so ti must not be the final result in Di and even in D.

Theorem 2: Deleting the uncertain tuple ti, which satis- fies  ? t? i ?Di,t  ?  i ?t(1-P (t  ? i)) ? p, can not affect the computa-  tion of the final results.

Proof: Suppose the uncertain tuple t?i is the final result,  there are three cases to be considered.

1. If t?i ? ti, then the computation of t  ? i is irrelative to ti.

2. If t?i ? ti or ti ? t ? i, then the computation of t  ? i is  irrelative to ti.

3. If ti ? t?i, then t  ? i must not be the final result  according to ?  t? i ?Di,t  ?  i ?t(1-P (t  ? i)) ? p, which contradicts  to the hypothesis.

Therefore, deleting the uncertain tuple ti in Di cannot  affect the computation of the final results. We define this kind of uncertain tuples as the Irrelative Set in Definition 3.

Definition 3: (Irrelative Set) The local Irrelative Set of Di (SIi) contains all the uncertain tuples which can meet the following condition:  ? t? i ?Di,t  ?  i ?ti  (1-P (t?i)) ? p.

Furthermore, except the uncertain tuples in SIi of each  Di, all the remained ones are relative to the computation of the final results. It is easy to obtain the uncertain tuples meeting Definition 2 in each Di. We call these uncertain tuples as the Candidate Set of Di as shown in Definition 4.

Definition 4: (Candidate Set) The local Candidate Set of Di (SCi) contains all the uncertain tuples with Skyline Probability P (ti)?  ? t? i ?Di,t  ?  i ?ti  (1-P (t?i)) > p, ti ? Di.

The Candidate Set of the union of each Candidate Set  of Di forms the global Candidate Set of D (SC ) after the relevant filtering. Theorem 3 demonstrates that all the final results of D are contained in the global Candidate Set.

Theorem 3: The global Candidate Set must contain all the final probabilistic skyline results in D.

Proof: In each split Di (1?i?m), the local SCi contains all the uncertain tuples which PSKY (ti) > p, ti ? Di. We only delete the uncertain tuples in each SIi. For D=D1 ? D2 ? ... ?Dm, all the final results are in the SC of D.

Last, except the uncertain tuples in SCi and SIi, there are still the uncertain tuples meeting the following conditions: (a) P (ti) ? p, ti ? Di, (b)  ? t? i ?Di,t  ?  i ?ti  (1-P (t?i) > p.

Although the number of this kind of uncertain tuples is relatively small to SCi and SIi, we cannot delete these  uncertain tuples because they can affect the computation of the uncertain tuples in SC as shown in Theorem 4.

Theorem 4: The uncertain tuple ti satisfying (a) P (ti) ? p, ti ? Di, (b)  ? t? i ?Di,t  ?  i ?ti  (1-P (t?i) > p can not be deleted, because it can affect the computation in SC of D.

Proof: Suppose that t?i is in SCi of Di, there are three cases to be considered.

1. If t?i ? ti, then the computation of t ? i is irrelative to ti.

2. If t?i ? ti or ti ? t ? i, then the computation of t  ? i is  irrelative to ti.

3. If ti ? t?i and ti is deleted, then the PSKY (t  ? i) lacks  the term of 1-P (ti), so the PSKY of t?i is affected by ti.

Then, we define these uncertain tuples as the Affect Set of  each Di in Definition 5. The union of each Affect Set of Di forms the global Affect Set of D (SA) after relevant filtering.

Definition 5: (Affect Set) The local Affect Set of Di (SAi) contains all the uncertain tuples meeting: (a) P (ti) ? p, ti ? Di, (b)  ? t? i ?Di,t  ?  i ?t(1-P (t  ? i)) > p.

C. The General Framework  In this section, our filter-refine is proposed for processing the probabilistic skyline query over large scale datasets. In the filter-refine approach, we translate the probabilistic sky- line query into two decomposable computations for returning the final results. Figure 3 shows the whole processing course of the filter-refine. The subscripts F and R stand for the filter and refine phases MapReduce computation, respectively. The main processing course is that filter phase generates the global SC and SA, and then refine phase gains the final results, which to be illustrated in detail.

Filter Phase The original uncertain database D is divided into several splits, such as D1, D2,...,Dm in Figure 3. In Map phase, each Mapper processes its own split and generates its own SCi and SAi, simultaneously deleting the uncertain tuples in SIi by any centralized algorithm. After that, each Mapper sends its local SCi and SAi to the Reducers as its intermediate results. In Reduce phase, one Reducer combines all the local SCi of each split and generates the global SC using the same algorithm as the Mappers. The other Reducer combines all the local SAi of each split and computes the global SA by the same algorithm as the Mappers too. By deleting the uncertain tuples satisfying the definition of Irrelative Set, the global SC and SA are obtained after further purifying of the whole local SCi and SAi of all Mappers. The global SC and SA are written into two independent files respectively to be processed further. In addition, the filter phase gains the global SC and ensures that all the uncertain tuples dominating the final results cannot be deleted in Theorem 5. The global SC not only contains all the final results, but also the other uncertain tuples which are not the final results. So, we still need the following refine phase to compute the final results.

Theorem 5: If t is the final result, t? ? t, that t? must be in the global SC or SA.

Proof: Let DOM(t) means all the uncertain tuples dominated by t. Let A=  ? t???DOM(t?),t???t(1-P (t  ??)) and B=  ? t???DOM(t),t???t(1-P (t  ??)). If t? ? t, then: A ? B. In A, if t? is deleted, then  ? t???DOM(t?),t???t(1-P (t  ??))< p. In B, t is the final result, then P (t) ?  ? t???DOM(t),t???t(1-  P (t??))? p, and then ?  t???DOM(t),t???t(1-p(t ??))? P (t) ??  t???DOM(t),t???t(1-p(t ??))? p, so the value of B? p. They  are mutual contradictory. This situation cannot happen.

The MapReduce computation of our filter phase is shown  in Algorithm 1. In the Initialize method of Mapper (Lines 1?5), we initialize two sets, which are used to store the uncertain tuples in local SCi and SAi and also set the threshold. In the Map method of Mapper (Lines 6?12), we calculate the local SCi and SAi in each split. In the Close method of Mapper (Lines 13?17), the intermediate results are emitted by the Mapper. In the Reduce method of Reducer, the global SC and SA are calculated (Lines 18?25).

Algorithm 1 filter Phase 1: class MAPPER 2: method INITIALIZE() 3: setC=new CandidateSet(); 4: setA=new AffectSet(); 5: set.setQ(q); 6: method MAP(Object key, Text value) 7: for each Si in CandidateSet do 8: p=new Point(p); 9: refresh pSkyline of Si;  10: for each Si in AffectSet do 11: p=new Point(p); 12: refresh pSkyline of Si; 13: method CLOSE() 14: for each Si in CandidateSet do 15: EMIT(one, new Text(p.toString())); 16: for each Si in AffectSet do 17: EMIT(two, new Text(p.toString())); 18: class REDUCER 19: method REDUCE(IntWritable key, IterableText values) 20: for i=1 to m do; 21: Combine CandidateSet of split i; 22: Calculate global CandidateSet; 23: for i=1 to m do; 24: Combine AffectSet of split i; 25: Calculate global AffectSet;  In the following, a concrete example is taken to illustrate the processing course of filter phase as shown in Figure 4. The uncertain database is D in Figure 1, composing of the uncertain databases D1, D2 and D3. For convenience, we suppose that the uncertain database D is divided into three splits processed by three Mappers, D1, D2 and D3, respectively. After processing D1, the first Mapper generates its local SC1, {t1, t2, t3} and its local SA1, {t4}.

After all the Mappers complete, the local SCi of each split is sent to the same Reducer, while the local SAi of each split is sent to the other Reducer. For example, the input of the Reducer processing SC is the uncertain tuples {t1, t2, t3, t6, t7, t8, t9, t10, T11}. Using any centralized probabilistic skyline algorithm, for the uncertain tuple t3,?  t??SC1?SC2?SC3,t??t3 (1-P (t?))=(1-P (t2))?(1-P (t6))?(1-  P (t7))=(1-0.6)?(1-0.5)?(1-0.5)=0.1 is already smaller than the threshold, so all the uncertain tuples dominated by t3 must be in the Irrelative Set. The output of global SC is only five uncertain tuples, {t1, t2, t6, t7, t8}. The same course is set to the computation of global SA, {t4, t5, t12}.

Refine Phase After identifying the global SC and SA, the final probabilistic skyline query results are generated in refine phase. Because the global SC contains all the final results and some tuples that are not still be deleted, we first fetch the global SC file and store the file into each Mapper of refine phase. Meanwhile, according to Theorem 5, all the uncertain tuples dominating the global SC are in the global SC and SA. So the output in filter phase, the global SC and SA, are used to be the input of refine phase. In this case, the global SC and SA are divided into several splits. In each Mapper, it stores the global SC and checks the dominance relationship between the global SC and its own split. If an uncertain tuple t? in the input split Di of one Mapper dominates another uncertain tuple t in the global SC , the relative computation  ? t??Di,t??t  (1-P (t?)) of t should be refreshed. Then, in Reduce phase, the Reducer combines the products of the nonexistence probability of tuples in all Mappers dominating t in the global SC , and then multiplies the existence probability P (t) of t. Furthermore, the Reducer calculates the Skyline Probability of each uncertain tuple in the global SC and returns the final results.

Algorithm 2 shows the processing course of refine phase.

In the Initialize method of Mapper (Lines 1?5), we mainly fetch the global SC file and store it into each Mapper. In the Map method of Mapper (Lines 6?9), for each uncertain tuple in each split, we compare the dominance relationship between the input tuples and each tuple in global SC , and then refresh the tuples in global SC . In the Close method of Mapper (Lines 10?11), the global SC refreshed is emitted in each Mapper. In the Reduce method of Reducer (Lines 12?18), for each ti in global SC , the Reducer multiplies its own existence probability and gains the final results.

Following the above example in filter phase, Figure 5 shows the computation course of refine phase. The input of refine phase is the global SC and SA which is divided into two splits. First, the global SC file is simply broadcast- ed to each Mapper, containing <t1,1>, <t2,1>, <t6,1>, <t7,1>, <t8,1>. The tuple <t1,1> means that the existence probability of (1,9) is 0.7, and 1 means the product of nonexistence probability of the uncertain tuples dominating t1. For example, in the first Mapper, the uncertain tuple <t8,1> is dominated by {t4, t5, t7} so the uncertain tuple     D1  Dm  SC  SA  D2  SCm,SAm  SC2,SA2  SC1,SA1  SA1,SA2,?,SAm  SC1,SC2,?,SCm  INPUTF MAPF SHUFFLEF REDUCEF OUTPUTF  PSKY(D)  (SC+SA)r  (SC+SA)2  (SC+SA)1  PSKY(SC)  INPUTR MAPR SHUFFLER REDUCER OUTPUTR  ... ... ...

Figure 3: The General filter-refine Approach  INPUTF MAPF SHUFFLEF REDUCEF OUTPUTF  SC1: {t1, t2, t3} SA1: {t4}  SC1+SC2+SC3: {t1, t2, t3, t6, t7, t8, t9, t10, t11}  D1  D2  D3  SC:{t1,t2,t6,t7,t8}  SC2: {t6, t7, t8} SA2: {t5}  SC3: {t9, t10, t11} SA3: {t12}  SA1+SA2+SA3:{t4, t5, t12} SA: {t4, t5, t12}  Figure 4: filter phase in MapReduce  Algorithm 2 refine Phase 1: class MAPPER 2: method INITIALIZE() 3: set=new CandidateSet(); 4: set.setQ(q); 5: cacheFiles=getLocalCacheFiles(); 6: method MAP(Object key, Text value) 7: for each Si in global CandidateSet do 8: p=new Point(p); 9: refresh the tuples in global CandidateSet;  10: method CLOSE() 11: EMIT(newText(p.getKey()),newText(p.toString())); 12: class REDUCER 13: method REDUCE(Text key, Iterable<Text> values) 14: Double sum=1.0; 15: for each Si in global CandidateSet do; 16: Calculate the pSkyline of Si 17: If pSkyline of Si > q; 18: EMIT(new Text(),new Text(sum+s));  <t8,1> should be refreshed to <t8,(1-0.3)?(1-0.2)?(1- 0.5)>. The same course is applied to the second Mapper.

For each tuple in the global SC , the Reducer combines the products of nonexistence probability and multiplies its own existence probability. For example, the Skyline Probability of the uncertain tuple <t8,1> equals to 0.8?0.28?1=0.224, so it is not the final result.

INPUTR MAPR SHUFFLER REDUCER OUTPUTR  < t1, 1 > < t2, 0.8 > < t6, 0.8 > < t7, 0.8 > < t8, 0.28 >  {t4, t5, t6, t7}  {t1, t2, t8, t12}  < t1, 1 > < t2, 1 >  < t6, 0.4 > < t7, 1 > < t8, 1 >  t1: 0.7*1*1=0.7 t2: 0.6*0.8*1=0.48  t6: 0.5*0.8*0.4=0.16 t7: 0.5*0.8*1=0.4  t8: 0.8*0.28=0.224  {t1, t2, t7}  Figure 5: refine phase in MapReduce  In short, our filter-refine approach can identify the final results of probabilistic skyline query. In the above example, we suppose that all the local Candidate Set can be processed  in one Reducer. When the size of all local Candidate Set is so large that one Reducer cannot process, we use a group of Reducers to generate the global Candidate Set. Although this kind of global Candidate Set may not be the optimal global Candidate Set, it does not affect the computation of the final results. We can use effective method to estimate the appropriate number of Reducers, and then generate the global Candidate Set. The processing course of the Affect Set is the same as the Candidate Set. Simultaneously, in the above example, we also suppose the global SC can be processed in one Mapper in refine phase, so there is only one pass MapReduce computation in refine phase to obtain the final results. When the size of the global SC is so large that it cannot be processed by one Mapper in refine phase, we can divide the global SC into several parts, and then adopt multi-passes MapReduce computations in refine phase to generate the final probabilistic skyline query answers.

In addition, in filter phase, each Mapper generates its own SC and SA. They don?t have the distributions of uncertain tuples in the other Mappers. If adopting adaptive communication mechanisms to make the Mappers obtain the processing information of the others to prune some unpromising uncertain tuples, the computation and commu- nication cost can be considerably decreased. Therefore, it is important to propose effective and efficient optimized algo- rithm to enhance the performance of filter-refine processing the probabilistic skyline query.



III. OPTIMIZED PROBABILISTIC SKYLINE QUERY  A. ComMapReduce Framework  ComMapReduce is an optimized MapReduce framework with lightweight communication mechanisms [11]. A new node, named the Coordinator node, is added to store and generate the shared information, which can effectively filter the considerable unpromising intermediate data to be trans- ferred in the shuffle phase. Several communication strategies are proposed to illustrate how to communicate and obtain the shared information in ComMapReduce.

B. Probabilistic Skyline Query in ComMapReduce  For remarkably improving the performance of filter-refine, the ComMapReduce framework can be used both in filter phase and refine phase to prune the unpromising uncertain     tuples. Algorithm 3 briefly introduces the processing pro- cedure of generating the shared information in filter and refine phases. After each Mapper completes, it computes the most optimal one according to the evaluation criterion of each application as its local shared information sending to the Coordinator node (Lines 1?4). Then, after receiving the shared information of Mappers (Lines 5?6), the Coordinator node can generate the relative optimized shared information (Lines 7?8). The Coordinator node sends the current global shared information to the Mappers (Lines 9?10). So, the Mappers can filter some unpromising intermediate results avoiding to be processed by the Reducers. The Mapper and Reducer algorithms are the same as the ones in MapReduce framework, so we don?t present the algorithms again in this section. Then, the optimizations of filter phase and refine phase in ComMapReduce will be illustrated in the following.

Algorithm 3 Coordinator in ComMapReduce 1: class COORDINATOR 2: method INITIALIZE() 3: InfoType sharedinformation; 4: set=new sharedinformationSet(); 5: method RECEIVE() 6: Receive the local sharedinformation; 7: method FIND() 8: Calculate the global sharedinformation; 9: method SEND()  10: Send the global sharedinformation to the Mappers;  Filter Phase In the local SCi of each split in filter phase, there are some uncertain tuples which are not the final results but cannot be still deleted. In ComMapReduce, identifying the shared information is important to enhance the perfor- mance of each application. According to the characteristics of probabilistic skyline query, the following equation can be used to generate the shared information in ComMapReduce framework. We suppose that (t1, t2, ..., td) of tuple t and (t?1, t  ? 2, ..., t  ? d) of tuple t  ? are the corresponding coordinates of t and t?. If the probability density function of the uncertain tuple in D is f(t)=f(t1, t2, ..., td). The dominance ability of uncertain tuple t? is determined by Equation 5, where R={t|t1 ? t?1, t2 ? t  ? 2, ..., td ? t  ? d}. The larger of Equation  5 it computes, the better dominance ability it becomes. The approximate distribution of f(t) can be easily obtained by random sampling or collecting histograms.

C(t?) = P (t?)?  ? R f(t1, t2, ..., td)dt1dt2 ...dtd?  R dt1dt2 ...dtd  (5)  In the following, the same example as Section II is adopted to illustrate the processing procedure of filter phase by Eager Communication Strategy of ComMapReduce. Just as this example, after the first Mapper completes, it chooses its local shared information according to Equation 5, t2, and sends it to the Coordinator node. The Coordinator node  sets t2 as the global shared information and sends it to the first Mapper. When the second Mapper completes, it sends its local shared information, t7, to the Coordinator node. After receiving t2 and t7, the Coordinator node sets t2 as the global shared information again and sends it to the second Mapper according to Equation 5. When the third Mapper completes, it sends t9 to the Coordinator node as its shared information. After the Coordinator node receives t9 of the third Mapper, it compares t9 to the current global shared information t2 and then sets t2 as the final shared information according to Equation 5 too. When the third Mapper receives t2 from the Coordinator node, (1-P(t2))?(1-P(t9))=0.24 is already smaller than p, so t10 and t11 that dominated by t9 can be deleted. Thus, adopt- ing efficient communication strategies of ComMapReduce framework, we can prune more unpromising uncertain tuples and reduce the communication and computation cost.

Refine Phase In refine phase, each Mapper compares the dominance relationship between each uncertain tuple of its split and the global SC . In that case, in one Mapper, if the nonexistence probability of uncertain tuple t in the global SC is already equal or less than the threshold, then t must not be the final result and t can be sent to the Coordinator node as the shared information of this Mapper. Therefore, the Coordinator node can send the shared information to the other Mappers as the global shared information, then it is unnecessary to compute the uncertain tuple t in the other Mappers. As shown in Figure 7, following the above example in filter phase of ComMapReduce, in the first Mapper, the uncertain tuple t8 is dominated by the uncertain tuples {t4, t5, t7}. So the nonexistence probability of t8 equals to (1-0.3)?(1-0.2)?(1-0.5)=0.28, which is smaller than the threshold. Since t8 must not the final result, the first Mapper sends t8 to the Coordinator node as its shared information. After the Coordinator node receives t8, it sets t8 as the global shared information and sends it to the second Mapper. After receiving t8, the second Mapper directly deletes t8. Therefore, whether filter phase or refine phase, the performance of probabilistic skyline query can be improved remarkably in ComMapReduce framework.



IV. EXPERIMENTAL EVALUATION  A. Experimental Setup  All our experiments are run on a cluster with 9 computers which are connected in a high speed Gigabit network.

Each computer has an Intel Quad Core 2.66GHZ CPU, 4GB memory and CentOS Linux 5.6. One computer is set as the Master node and the Coordinator node, and the others are set as the Slave nodes. We use Hadoop version 0.20.2 and compile the source codes under JDK 1.6.

Table I summarizes the parameters used in our experimental evaluation, along with their ranges and default values shown in bold. The experimental data are synthetic data and the probability is generated randomly. The performance of our     INPUTF MAPF SHUFFLEF REDUCEF OUTPUTF  SC1: {t1, t2, t3} SA1: {t4}  SC1+SC2+SC3: {t1, t2, t3, t6, t7, t8, t9}  D1  D2  D3  SC:{t1,t2,t6,t7,t8}  SC2: {t6, t7, t8} SA2: {t5}  SC3: {t9, t10, t11} SA3: {t12}  SA1+SA2+SA3:{t4, t5, t12} SA: {t4, t5, t12}  Coordinator:{t2}  {t2} {t7} {t9}  Figure 6: filter phase in ComMapReduce  INPUTR MAPR SHUFFLER REDUCER OUTPUTR  < t1, 1 > < t2, 0.8 > < t6, 0.8 > < t7, 0.8 > < t8, 0.28 >  {t4, t5, t6, t7}  {t1, t2, t8, t12}  < t1, 1 > < t2, 1 >  < t6, 0.4 > < t7, 1 > < t8, 1 >  t1: 0.7*1*1=0.7 t2: 0.6*0.8*1=0.48  t6: 0.5*0.8*0.4=0.16 t7: 0.5*0.8*1=0.4  {t1, t2, t7}  Coordinator :{t8}  {t8} {t8}  Figure 7: refine phase in ComMapReduce  filter-refine approach is evaluated in details with various experimental settings, containing the probabilistic skyline query in MapReduce (PSMR), and its improved version, probabilistic skyline query in ComMapReduce (PSCMR).

Since the data volume of the global SC is not especially large in our experiments (7.78M-19.85M), it can be processed in one Mapper in refine phase. So, it is only one pass MapReduce computation to gain the final results in refine phase. Simultaneously, the global Candidate Set and the Affect Set can be generated by only one Reducer respectively.

Parameter Range and Default  Data Records (100M) 0.2(0.842G),0.4(1.62G),0.6(2.43G),0.8(3.23G),1.0(4.04G) Dimensionality 2, 3, 4, 5  Threshold 0.3, 0.4, 0.5, 0.6 Number of Slave Nodes 1, 2, 4, 8  Table I: Experimental Parameters  B. Experimental Results  First, we investigate the influence of changing the Data Records in Figure 8. With the increasing of data records, the running time of PSMR and PSCMR all increase. The running time of PSCMR is shorter than PSMR. With the increasing of data records, there are more data to be processed, so the running time of PSMR and PSCMR all increase in two distributions. In PSCMR, adopting the Eager Communica- tion Strategy to early prune more unpromising uncertain tuples in filter phase, so the performance of PSCMR is better than PSMR. In Anti-correlated distribution, because the data are skewed to the final results and the computation cost increase accordingly, the running time of PSMR and PSCMR are longer than the Independent distribution. However, the performance of PSCMR is still optimal to PSMR.

Second, we evaluate the performance of changing the Dimensionality. As shown in Figure 9, with the increasing of dimensionality, the running time of PSMR and PSCMR all rise largely. When increasing the dimensionality, the compu- tation cost of identifying dominance relationship increases accordingly, so the running time also increases sharply. In PSCMR, the running time is shorter than PSMR by using efficient communication strategy.

Third, we test the influence of changing the threshold as shown in Figure 10. The running time of PSMR and PSCMR  decreases as increasing the threshold. PSCMR is better than PSMR. The reason is that if the threshold is larger, then the number of uncertain tuples in the Irrelative Set of each split will increase. So the number of uncertain tuples in global SC and SA decreases. Consequently, the running time decreases with the increasing of the threshold.

0.2  0.4  0.6  0.8  1  R un  ni ng  T im  e (s  )  Data Records (100M)  PSMR PSCMR  (a) Independent Distribution        0.2  0.4  0.6  0.8  1  R un  ni ng  T im  e (s  )  Data Records (100M)  PSMR PSCMR  (b) Anti-correlated Distribution  Figure 8: Performance of Changing Data Records         2 3 4 5  R un  ni ng  T im  e (s  )  Dimensionality  PSMR PSCMR  (a) Independent Distribution         2  3  4  5  R un  ni ng  T im  e (s  )  Dimensionality  PSMR PSCMR  (b) Anti-correlated Distribution  Figure 9: Performance of Changing Dimensionality        0.3 0.4 0.5 0.6  R un  ni ng  T im  e (s  )  Threshold  PSMR PSCMR  (a) Independent Distribution        0.3 0.4 0.5 0.6  R un  ni ng  T im  e (s  )  Threshold  PSMR PSCMR  (b) Anti-correlated Distribution  Figure 10: Performance of Changing Threshold  Then, we evaluate the performance of changing the num- ber of slave nodes. As shown in Figure 11, the running time of PSMR and PSCMR all decrease by increasing the number of slave nodes. Because the system parallelism is strengthened, so the running time decreases when processing the same data volume. PSCMR can effectively filter numer- ous unqualified uncertain tuples by efficient communication mechanisms, so its performance is optimal to PSMR.

Finally, we investigate the size of the global SC and SA of PSMR and PSCMR in Figure 12 to evaluate the Filter Capacity of our algorithms. In the two distributions, the size of the global SC and SA of each algorithm is much smaller than the original database. The size of the global SC and SA in PSCMR is much less than PSMR. The reason is that PSCMR adopts efficient communication strategy to filter numerous unpromising uncertain tuples.

1 2 4 8  R un  ni ng  T im  e (s  )  Slave Nodes  PSMR PSCMR  (a) Independent Distribution        1 2 4 8  R un  ni ng  T im  e (s  )  Slave Nodes  PSMR PSCMR  (b) Anti-correlated Distribution  Figure 11: Performance of Changing Number of Nodes       0.2 0.4 0.6 0.8 1.0  M ap  pe r  O ut  R ec  or ds  (  )  Data Records (100M)  PSMR PSCMR  (a) Independent Distribution       0.2 0.4 0.6 0.8 1.0  M ap  O ut  pu t R  ec or  ds (   )  Data Records (100M)  PSMR PSCMR  (b) Anti-correlated Distribution  Figure 12: Performance of Filter Capacity

V. RELATED WORK  A. Probabilistic Skyline Query Processing  Probabilistic skyline over uncertain data is first proposed by Pei et. al [3] in 2007. They tackle the problem of skyline analysis over uncertain data and propose two meth- ods of computing the results. The reverse skyline query over uncertain data is researched by Lian and Chen [6] in both monochromatic and dichromatic cases. The problem of computing skyline in sliding windows over an uncertain data stream under given probability thresholds is investigated in paper [4]. Paper [12] combines the feature of top-k objects to model the problem of top-k skyline objects against uncertain data. Ding et. al [13] propose the notation of distributed sky- line queries over uncertain data and two efficient algorithms to retrieve the qualified skylines from distributed local sites.

B. Query Processing in MapReduce  Google?s MapReduce [7] was first proposed in 2004 for massive parallel data analysis in shared-nothing clusters. We only illustrate some works about the query processing in MapReduce. HaLoop [14] is an improvement of MapRe- duce supporting iterative applications, which improves their efficiency by making the task scheduler loop-aware and by adding various caching mechanisms. HadoopDB [15] is a hybrid architecture of MapReduce and relational database, which tries to combine the advantages of both. Paper [16] investigates how to perform kNN join using MapReduce.



VI. CONCLUSIONS  In this paper, we study the problem of processing prob- abilistic skyline query over massive data in MapReduce framework and propose filter-refine two phases approach. In filter phase, the global Candidate Set and the Affect Set are generated by one MapReduce computation. In refine phase, the final probabilistic skyline query answers are returned.

To reduce the computation and communication cost, the optimized algorithm is developed to prune the unpromising uncertain tuples. Numerous experiments demonstrate that the proposed algorithms are efficient and effective. Studying other query processing applications over uncertain data in MapReduce framework is an important task for future work.

