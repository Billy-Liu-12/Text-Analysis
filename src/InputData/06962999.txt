* Corresponding author: Hong Wang.

Abstract  Mining frequent itemsets from traditional database is an  important research topic in data mining and researchers achieved  tremendous progress in this field. However, with the emergence of  new applications, the traditional way of mining frequent itemsets is  not available in uncertain environment. In the past ten years,  researchers proposed different solutions in extending the  conventional techniques into uncertainty environment. In this paper,  we review the previous algorithms based on the two definitions of  frequent itemsets, then we improve the traditional classic algorithm  for mining frequent itemsets in uncertain databases under the  definition of frequent probability. Finally, we tested our algorithm  on a number of uncertain data sets. The experiments on both  sythetic and real data have shown that the new adaptation of classic  algorithm is efficient and gain better results on accuracy.

Keywords frequent itemsets mining, uncertain database;  algorithm, data mining  1 INTRODUCTION  Frequent itemset mining is a popular and important step to  analyze data in a broad range of applications. In the past twenty  years, researchers have made tremendous achievements in  developing efficient and scalable algorithms for frequent itemsets  mining(FIM) in precise and certain databases. The three most  classic FIM algorithms are Apriori [1], FP-growth [2] and Eclat  [3].

However, in many emerging applications, real data is typically  subject to noise and measurement error, thus the existence of an  itemset in a transaction is usually captured by probability, which  poses a number of unique challenges on exploiting efficient  algorithms for mining frequent itemsets in uncertain environment.

In the case of uncertain databases, we do not know whether a  transaction contains an itemset with certainty in real world, so the  Possible World Semantics(PWS) is often used to interpret the  uncertain databases. Each possible world w is associated with a  probability of world exists P(w), thus the database generates a set  of possible worlds under the PWS, where each world is defined by  a fixed set of transactions. The sum of possible world probabilities  is one, and the number of possible worlds is exponentially large.

Based on the possible worlds semantics, two different  definitions on frequent itemsets are presented in uncertain  databases: expected support-based frequent itemsets[3][4] and  probabilistic frequent itemset[5][6]. Most previous studies have  been developed under the two definitions respectively[7][8].

However, Y. Tong[9] verified that the two definitions have a rather  close connection. And in present research of frequent itemsets  mining, the novel definition of frequent probability is used more  widely.

In this paper, we reviewed the present status of frequent  itemsets mining in uncertain database communities, and propose a  new adaptation of classic algorithm to mine frequent itemsets in  uncertain databases, which combine the merits of both exact and  approximate probabilistic frequent itemsets mining algorithms.

Finally we analyze its performance and test it over well known  benchmark data sets.

The rest of the paper is organized as follows. In section 2, we  introduce the problem definition in uncertain database. Then we  review the novel developments in section 3 and present our  improvement in section 4. In section 5, the results of experiments  are shown and we draw our conclusion in section 6.

2 PROBLEM DEFINITIONS  In traditional databases, most of the previous studies on frequent  itemsets mining assume a data model in which each transaction  contains complete and doubtless items. However, in emerging  applications, an uncertain database is formed with uncertain  transactions of extential uncertain items. Therefore, it is nature for  the researchers to introduce a probability to describe the existence    of an item in a transaction (Table.1), which is the attribute  uncertainty model usually adopted for the field of frequent itemsets  mining in uncertain environment. In the attribute uncertainty model,  each transaction consists of attributes (items) with values carrying  uncertain information. As shown in Table 1, this is just the  extended version in traditional databases.

Since the transactions or items are probabilistic in nature, it is  impossible to count the frequency of itemsets deterministically.

Chui et.al[10] proposed the expected support of itemsets in their  U-Apriori algorithm to depict the degree of frequency under  uncertian environments.

Definition 1.(expected support-based frequent itemset) Given  an uncertain transaction database with N transactions. the expected  support of an itemset X is defined as the sum of expected  probabilities of presence of X in each of the transactions in the  database. With a minimum expected support ratio, min_esup, an  itemset X is an expected support-based frequent itemset if and only  if esup(X) ? N?min_ esup.

However, considering an itemset frequent only by its expected  support has a major drawback[11]. Uncertain transaction database  naturally involve uncertainty on support of an itemset, which is  important when evaluating a frequent itemset. However, this  information is forfeited when using the expected support approach.

Therefore, the confidence of an itemset is frequent is introduced to  interpret uncertain datasets. Thus a probabilistic way is defined as  the measurement to evaluate the uncertain data.

Definition 2. (Probabilistic Frequent Itemset) Given an  uncertain transaction database with N transactions, a minimum  expected support ratio, min_sup, and a probabilitic frequent  threshold min_prob. An itemset X is considered a probabilistic  frequent itemset if its frequent probability is larger than the  probabilistic frequent threshold[12]:  Pr(X)=Pr{sup(X)?N?min_sup}>min_prob  By experimental study and theoretical analysis, Yongxin  Tong[9] presents that there is some relationships between the two  definitions on frequent itemsets. Therefore, some researchers have  proposed that the more efficient algorithms can be gained to mine  probabilistic frequent itemsets by employing appropriate expected  support-based frequent itemset mining algorithms. Provided that  the variances of support of each itemset are calculated as well as  expected support on each itemset, the expected support-based FIM  algorithms can be used to mine probabilistic frequent itemsets as  well.

Table 1.Attribute Uncertain Data Sets  TID Transaction itemset  T1 A(0.6) B(0.5) C(0.3) D(0.5)  T2 A(0.7) D(0.8) E(0.25)  T3 A(0.3) C(0.8) D(0.4)  T4 C(0.7) D(0.3) E(0.2)  T5 A(0.5) C(0.7) E(0.3)  3 FIMALGORITHMS IN UNCERTAIN DATABASES  3.1 Extension of Classic Algorithms into Uncertain Case  Consider three classic frequent itemsets mining algorithms  (Apriori, FP-growth, and H-mine) and their adaptions. Although  Aproiri[1] algorithm is slower than the other two algorithms in  deterministic databases, with the well-known downward closure  property, UApriori[10] algorithm performs very well among the  three algorithms and is usually the fastest one in dense uncertain  databases with high min_esup. However, FP-growth algorithm[14]  does not show similar efficient behavior in the uncertain cases.

There are some possible reasons: Firstly, the probabilities of items  in uncertain database make the database sparse due to fewer shared  nodes and paths. Secondly, in the uncertain cases, the compression  of UFP-tree[10] is substantially reduced because it is hard to take  the advantage of shared prefix path in FP-tree. Therefore, as the  support threshold goes down, the UFP-trees constructed become  large and too many candidate itemsets generated, which leads to  sharp increase of memory usage. So the FP-tree structure does not  extend well in the sparse uncertain databases.

Based on the divide-and-conquer framework and the  depth-first search strategy, UH-mine[15] is quite suitable for sparse  uncertain databases. As an extension from classical algorithm in  deterministic case, UH-mine algorithm fails to compress the data  structure and use the dynamic frequency order sub-structure, so it  is faster in building head tables of all levels than building all  conditional subtrees. Therefore UH-mine always outperforms other  algorithms in uncertain sparse databases with low min_sup.

3.2 Exact FIMAlgorithms in Uncertain Databases  Based on the connections of the two definitions on frequent  itemsets over uncertain databases, most of existing algorithms can  be divided into two categories: the exact frequent itemsets mining  algorithms and the probabilistic frequent itemsets mining  algorithms. Besides the three most representative exact expected  support-based frequent itemset mining algorithms reviewed in  subsection 3.1, there are other two exact probabilistic frequent  algorithms.

Since the support of an itemset is a random variable following  the Poisson Binomial distribution, [16] designed a dynamic  programming-based algorithm (DP) to compute frequent  probability. With the recursive relationship, frequent probability is  deduced as one subtracts the probability computed from the  corresponding cumulative distribution function of the support. So  the time complexity of DP for each itemset is reduced to  O(N2?min_esup). Another divide-and-conquer algorithm was also  presented in [11] to calculate frequent probability exactly. Unlike  DP algorithm, DC algorithm calls itself recursively to divide an  uncertain database until only one transaction left, then it records  the probability distribution of support of the itemset in transaction.

At last, DC algorithm obtains the probability distribution in  conquer part.

3.3 Approximate FIMAlgorithms in Uncertain Databases  There are two kinds of approximate frequent itemsets mining  algorithms in uncertain databases: the expected support-based  frequent algorithms and probabilistic frequent algorithms. In reality,  there are three probabilistic frequent algorithms that are more  important in uncertain databases.

Taking into account that the support of an itemset can be  regarded as a random variable following Poisson Binomial  distribution, the probabilistic frequent algorithms are designed  based on the Poisson distribution or Normal distribution  respectively when uncertain databases are large enough.

In Poisson-distribution-based UApriori algorithm[17], the  frequent probability of an itemset is rewritten by the cumulative  distribution function (CDF) of the Poisson distribution, in which  parameter ? is the expectation and variance of random variable.

Following the monotonic of CDF with respect to ? , PDUApriori  computes the corresponding ? of the given min_prob and calls  UApriori to find the results.

According to the Lyapunov Central Limity, the Normal  distribution-based approximate probabilistic frequent itemset  mining algorithm, NDUApriori[18], rewrites the frequent  probability of itemset with standard Normal distribution. Then  NDUApriori algorithm employs the Apriori framework and uses  the cumulative distribution function of standard Normal  distribution to calculate the frequent probability. Different from  PDUApriori, NDUApriori algorithm returns frequent probabilities  of all probabilistic frequent itemsets. However, it is impractical to  apply NDUApriori on very large sparse uncertain databases since it  employs the Apriori framework.

Summarizing the achievement and the shortage of the previous  research, Yongxin Tong in [9] integrates the framework of  UH-Mine and the Normal distribution approximation and proposes  NDUH-Mine algorithm, which achieves a win-win partnership in  sparse uncertain databases. In particular, by comparison of  algorithm framework and approximation methods, some important  conclusions are also drawn in this paper:  NDUApriori is the fastest algorithm in large enough dense  uncertain database, while NDUH-Mine requires reasonable  memory space and scales well in very large sparse uncertain  databases.

The Normal distribution-based approximation algorithms  build a bridge between the expected support-based frequent  itemsets and the probabilistic frequent itemsets.

Approximate probabilistic frequent itemset mining  algorithms usually outperforms any existing exact  probabilistic frequent itemset mining algorithms in the  algorithm efficiency and the memory cost.

4 THE NEWADAPTION OF CLASSIC FIMALGORITHM  Summarizing the achievement and the shortage of the previous  research, approximate probabilistic frequent algorithms is  considered as one of promising approaches that is proved to have  the same efficiency as expected support-based algorithms. It  guarantees to return frequent probabilities of all probabilistic  frequent itemsets with high confidence, as long as the databases  underlying is large enough. However, there is something worthy of  further consideration.

4.1 A data-related issue  Nowadays, a lot of algorithms and techniques are proposed for  finding itemsets from transactional databases, in which some are  more efficient for dense datasets, while others are more suitable for  sparse datasets. So there is no single algorithm that is the most  efficient for both sparse datasets and dense ones[13]. While in  uncertain cases, the databases are often considered as the sparse  ones, i.e., only a small number of items have a nonzero probability  of appearing in a given transaction in uncertain databases.

Definition 3. A database is dense if the frequent items in it  have high relative support. A database is sparnse if the frequent  items in it have low relative support. The relative support can be  computed as the ratio of absolute support against number of  transactions(or frequent-item projections) in the database[16].

In general, when the relative support of frequent items is equal    to or higher than 10%, the database is regarded as dense. On the  other hand, when the relative support of frequent items is low, such  as far below 1%, it is sparse. However, if the relative support is  between the value of 10% and 1%, the result will depend on the  size of the practical database.

In the case of sparse uncertain databases with a low min_sup  given, although a large amount of frequent items are generated at  the first scan of the database, the number of k-frequent itemsets  found in the following steps may drop dramatically after several  recursive operations. Furthermore, once a little large min_sup is  given, the trend of declining in the number of frequent items will  reduce more sharply, which means that it may be impossible to  produce a large enough candidate frequent dataset to find (k+1)  frequent itemsets with approximate frequent itemsets mining  algorithms according to Central Limit Theorem (Shown in Fig.1  [7] ).

Even in the case of dense uncertain databases, the same  problem maybe occurs. For example, in a dense database following  Zipf distirbution, which is generally used to describe the online  sales on the Internet, although large enough frequent items are  produced at the very beginning, there will be a much small size of  frequent itemsets generated for the following mining of (k+1)  frequent itemsets. In this case, the condition of Lyapunov Central  Limit Theory no longer holds, and those approximate probabilistic  frequent algorithms would have difficulty to accomplish the  effective mining.

Figure 1. Varying Support & Frequent Probability[7]  Therefore, considering the whole process of frequent itemsets  mining, we propose an adaptive solution of mining frequent  itemsets from uncertain databases, which is divided into the  approximate mining phase and the exact mining phase.

4.2 AnAdaption of Classic FIMAlgorithm  There are three steps in the new algorithm.

Firstly, based on the definition of probabilistic frequent itemsets,  we calculate the frequentness probability in a systematic manner  thanks to its anti-monotonic property. With a large enough  uncertain database UDB is given, it is sensible for us to choose a  kind of approximate probabilistic frequent itemsets mining  algorithms, which avoids the operations of expanding the database  into the exponential number of possible worlds. In other words,  under the definition of mining probabilistic frequent itemsets,  NDUApriori algorithm is applied in large enough dense uncertain  databases, while NDUH-Mine algorithm is prefer in large enough  sparse uncertain databases. We calculate the variance of each  itemset when obtain the expected support of each itemset, then the  frequent probability is gained by the standard normal distribution  formula:  m in_ sup 0 .5 sup (X )P r(X ) ( ) (X )  N e Var  In this way, we calculate the frequent itemsets such as  2-frequent itemsets and 3-frequent itemsets.

In the second step, we weigh the number of frequent itemsets  based on the conditions of the special form of the Central Limit  Theorem as well as the memory usage, and switch to exact  probabilistic frequent algorithm adaptively if necessary. Otherwise,  we continue the approximate probabilistic frequent algorithms, and  test again after every two rounds of mining, until the frequent  itemsets database is not large enough to satisfy the Central Limit  Theorem.

At last, since the approximate probabilistic frequent algorithms  is not suitable in the following process, we prefer the dynamic  programming algorithm to mine remaining (k+1)-frequent itemsets  in the following process. That is, we produce the exact frequent  probability recursively with the formula below: ))(1()()()()( 1,1,1, jjijjiji TXPrXPrTXPrXPrXPr  Where Pr ? 0, j=1 0 ? j ?|T|, Pr?i,j=1 i>j  Furthermore, some pruning strategies such as Chernoff  bound-based pruning and anti-monotonic property are introduced  to reduce the running time.

In the next section, we design some experiments to show the  performance of the adaptive algorithm in large uncertain data,  which confirms our goal.

5 EXPERIMENTS  In this section, we introduce the results of preliminary    experiments conducted. The experiments run on a PC with a 2.5  GHz Intel(R) core(TM) i5-2520M processor and 4 Gbytes of RAM.

The operating system was Window 7. All algorithms were  implemented and compiled using Microsoft?s Visual C++ 2010.

5.1 Results on Experimental Databases  Experimental data sets comes from the well-known Frequent  Item set Mining Data Set Repositiory[19]. Among the cases that  data sets follow the Gaussian distribution, we conduct our  experiments in dense uncertain databases as well as sparse ones. In  the dense dataset with high mean and low variance, namely  Connect with the mean (0.95) and the variance (0.05), we run our  adaptive algorithm with NDUApriori algorithm switching to DP  algorithm. While in the sparse dataset with high mean and low  variance, namely Gazelle with the mean (0.95) and the variance  (0.05), we run our hybrid algorithm with NDUH-Mine algorithm  switching to DP algorithm. The preliminary results show that our  hybrid algorithm is efficient (Shown in Fig.2) and more accuracy  (Shown in Table.1).

Figure 2. Results in Dataset of Connect  5.2 Results on Historical Prescriptions Database  In the TCM research, commercial software such as SPSS  Clementine is prevailing for data mining, in which the classic  Apriori algorithm is used. As a comparison, we ran our new  algorithm on PC with the same environment shown above.

Meanwhile, the same different min_sup values are selected in the  two algorithms according to the experience on the syndrome and  clinical diagnosis.

Table 2.Accuracy in Dataset of Gazelle  Min_sup NDUH-Mine NewAdaption 0.001 0.89 0.93 0.003 0.91 0.95 0.005 0.95 0.97 0.01 0.96 0.98 0.03 0.98 1  Without any doubt, the adaptation of classic algorithm wins  with overwhelming advantages in the performance of running time  and accuracy. In shorter time, it successfully analyzed 13868  prescriptions, the frequency of each herb and association rules  among the herbs were computed, core combinations and new  prescriptions were mined out of the database. Taking into account  potential error and loss in the historical prescriptions, it is difficult  for existing traditional data tools such as SPSS Clementine to find  all the really existed combination principles and core combinations  of herds in prescriptions because of their limitations on component  organization. In order to make a remedy on this defect, we  introduced different weights on the potential frequent items in the  transaction database which may be lost due to unknown reasons.

The weights determined by frequent itemsets mined and the  experience gained in clinical diagnosis. By relaxing the support  definition, we use the new algorithm for a probabilistic item set  mining in incomplete transaction database of prescriptions. The  adaptation of classic algorithm demonstrated its efficiency and  effectiveness in the application of prescriptions analysis of TCM.

6 CONCLUSIONS AND FUTURE STUDY  Mining frequent item sets is an important problem in data  mining, and is also the first step of deriving association rules.

Hence, many efficient frequent itemsets mining algorithms have  been proposed. These algorithms work well for databases with  precise values, but may not be efficient in mining probabilistic data.

In this paper, we compared the present algorithms for extracting  frequent itemsets from uncertain databases. And then we proposed  an adaptation of classic algorithms, which mines approximate  frequent itemsets when the database is large enough, while exerts  an exact mining process adaptively as the candidate itemsets under  consideration are not large enough. Although our algorithm is  developed on the traditional framework, they can be suitable for  supporting other improved algorithms for handing uncertain data.

In the futures, we will do more efforts on the research of  mining frequent itemsets in uncertain databases. With further  experiments, we will explore the characters of specific uncertain    databases such as Chinese medicine prescriptions database, and  improve frequent itemset mining in imperfect database, thus utilize  and develop the previous studies into emerging applications.

ACKNOWLEDGEMENT  We are grateful for the support of the Natural Science  Foundation of China (61373149) and the Shandong Provincial  Project for Science and Technology Development  (2012GGB01058).

