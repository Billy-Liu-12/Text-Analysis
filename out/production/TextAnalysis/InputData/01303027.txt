Parallel Mining of Association Rules from Text Databases on a Cluster of Workstations

Abstract  In this paper, we propose a new algorithm named Parallel Multipass with Inverted Hashing and Pruning (PMIHP) for mining association rules between words in text databases.

The characteristics of text databases are quite different from those of retail transaction databases, and existing mining al- gorithms cannot handle text databases efficiently because of the large number of itemsets (i.e., sets of words) that need to be counted. The new PMIHP algorithm is a par- allel version of our Multipass with Inverted Hashing and Pruning (MIHP) algorithm [13], which was shown to be quite efficient than other existing algorithms in the con- text of mining text databases. The PMIHP algorithm re- duces the overhead of communication between miners run- ning on different processors because they are mining local databases asynchronously and prune the global candidates by using the Inverted Hashing and Pruning technique. Com- pared with the well-known Count Distribution algorithm [2], PMIHP demonstrates superior performance character- istics for mining association rules in large text databases, and when the minimum support level is low, its speedup is superlinear as the number of processors increases. These experiments were performed on a cluster of Linux worksta- tions using a collection of Wall Street Journal articles.

Key words: Parallel association rule mining, text retrieval, multipass, inverted hashing and pruning, cluster computing, scalability.

1 Introduction  Mining association rules in transaction databases has been demonstrated to be useful and technically feasible in several application areas [3], particularly in retail sales. Let  ?This research was supported in part by LexisNexis, NCR, and Wright Brothers Institute (WBI).

I = {i1, i2, . . . , im} be a set of items. Let D be a set of transactions, where each transaction T contains a set of items. An association rule is an implication of the form X ? Y , where X ? I, Y ? I, and X ? Y = ?. The association ruleX ? Y holds in the database D with confi- dence c if c% of transactions in D that containX also con- tain Y . The association rule X ? Y has support s if s% of transactions in D containX?Y . Mining association rules is to find all association rules that have support and confidence greater than or equal to the user-specified minimum support (called minsup) and minimum confidence (called minconf), respectively [1]. For example, beer and disposable diapers are items, and beer? diapers is an association rule mined from the database if the co-occurrence rate of beer and dis- posable diapers (in the same transaction) is not less than minsup and the occurrence rate of diapers in the transac- tions containing beer is not less than minconf.

The first step in the discovery of association rules is to find each set of items (called itemset) that have co- occurrence rate above the minimum support. An itemset with at least the minimum support is called a large item- set or a frequent itemset. In this paper, the term frequent itemset will be used. The size of an itemset represents the number of items contained in the itemset, and an itemset containing k items will be called a k-itemset. For exam- ple, {beer, diapers} can be a frequent 2-itemset. Finding all frequent itemsets is a very resource consuming task and has received a considerable amount of research effort in re- cent years. The second step of forming the association rules from the frequent itemsets is straightforward as described in [1]: For every frequent itemset f , find all non-empty sub- sets of f . For every such subset a, generate a rule of the form a? (f ? a) if the ratio of support(f ) to support(a) is at least minconf.

The association rules mined from point-of-sale (POS) transaction databases can be used to predict the purchase behavior of customers. In the case of text databases, there are several uses of mined association rules between sets of words. They can be used for building a statistical thesaurus.

0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    Consider the case that we have an association rule B ? C, whereB andC are words. A search for documents contain- ingC can be expanded by includingB. This expansion will allow for finding documents using C that do not contain C as a term. A closely related use is Latent Semantic Index- ing, where documents are considered close to each other if they share a sufficient number of associations [9]. La- tent Semantic Indexing can be used to retrieve documents that do not have any terms in common with the original text search expression by adding documents to the query result set that are close to the documents in the original query re- sult set. Frequent itemsets mined from a text database may be useful in the task of document ranking.

The word frequency distribution of a text database can be very different from the item frequency distribution of a sales transaction database. Additionally, the number of unique words in a text database is significantly larger than the number of unique items in a transaction database. Fi- nally, the number of unique words in a typical document is much larger than the number of unique items in a transac- tion. These differences make the existing algorithms, such as Apriori [1], Direct Hashing and Pruning (DHP) [14] and FP-Growth [10], ineffective in mining association rules in the text databases.

Considering the large number of candidate itemsets to be processed from very large text databases, parallel as- sociation rule mining is quite essential. In this paper, we propose a new parallel association rule mining algorithm named Parallel Multipass with Inverted Hashing and Prun- ing (PMIHP). PMIHP is a parallel version of our sequential Multipass with Inverted Hashing and Pruning (MIHP) al- gorithm [13], which was shown to be effective for mining association rules in text databases, especially when the min- imum support level is low.

We implemented the proposed PMIHP on a cluster of Linux workstations, and analyzed its performance using a collection of Wall Street Journal articles in the 1996 Text Research Collection (TREC) [16]. The new algorithm demonstrates a superlinear speedup as the number of pro- cessors increases when the minimum support level is low.

PMIHP is also shown to be much faster than the Count Dis- tribution algorithm [2], which is a parallel version of the Apriori algorithm.

2 Parallel Multipass with Inverted Hashing and Pruning (PMIHP) Algorithm  The new Parallel Multipass with Inverted Hashing and Pruning (PMIHP) algorithm is a parallel version of the sequential Multipass with Inverted Hashing and Pruning (MIHP) algorithm. The MIHP algorithm is based on the Multipass approach [11] and the Inverted Hashing and Pruning (IHP) [12] that we proposed.

In PMIHP, the Multipass approach reduces the required memory space at each processor by partitioning the frequent items and processing each partition separately. Thus, the number of candidate itemsets to be processed is limited at each instance. The Inverted Hashing and Pruning is used to prune the local and global candidate itemsets at each pro- cessing node, and it also allows each processing node to determine the other peer processing nodes to poll in order to collect the local support counts of each global candidate itemset.

The Multipass approach and the Inverted Hashing and Pruning are described in Section 2.1 and Section 2.2, re- spectively, and the pseudo-code of the sequential MIHP al- gorithm is given in Section 2.3.

2.1 Multipass Approach  The Multipass algorithm partitions the frequent items, and thus partition the candidate itemsets. The partition size is selected to be small enough to fit in the available memory of the processing node. Each partition is then processed separately. Each partition of items contains a fraction of the set of all items in the database, so that the memory space required for counting the occurrences of the sets of items within a partition will be much less than the case of counting the occurrences of the sets of all the items in the database.

The Apriori algorithm [1] is modified to be the Multipass-Apriori algorithm as follows:  1. Count the occurrences of each item in the database to find the frequent 1-itemsets.

2. Partition the frequent 1-itemsets into p partitions, P1, P2, . . . , Pp.

3. Use Apriori algorithm to find all the frequent itemsets whose member items are in each partition, in the order of Pp, Pp?1, . . . , P1, by scanning the database. When partition Pp is processed, we can find all the frequent itemsets whose member items are in Pp. When the next partition Pp?1 is processed, we can find all the frequent itemsets whose member items are in Pp?1 and Pp. This is because, when Pp?1 is processed, the items in Pp?1 are extended with the frequent itemsets we found from Pp and then counted. This procedure is continued until P1 is processed.

Assume, without loss of generality, that the frequent 1- itemsets (or, simply items) are ordered lexically. The fre- quent items are partitioned into p partitions, P1, P2, . . . , Pp, such that for every i < j, every item a ? Pi is less than every item b ? Pj . Thus, the itemsets under considera- tion for some partition Pi have a particular range of item prefixes. Notice that if the partitions have the same num- ber of items, the potential number of itemsets that will be  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    formed by extending a lexically lower ordered partition will be larger than the potential number of itemsets from a lexi- cally higher ordered partition.

Since the frequent items are ordered lexically, it is im- portant to process the partitions in sequence from the high- est ordered one to the lowest ordered one. This processing order is required to support the subset-infrequency based pruning of candidate itemsets. Figure 1 shows an example of partitions, where items 0, 1, 2, 3, 4, and 5 are frequent 1-itemsets and are partitioned into P1, P2, and P3.

0 1 2, 3, 4, 5  Itemsets that begin with  Itemsets that begin with   Itemsets that begin with  first  1 are processed second  0 are processed last  2, 3, or 4 are processed  6 frequent items in 3 partitions  P P P 1 2  Figure 1. Partitioning a set of 6 frequent items for Multipass-Apriori  When P3 is being processed, we consider only the can- didate itemsets whose members are in {2, 3, 4, 5}, not in- cluding 0 and 1. Thus, the number of candidates will be smaller compared to the case of considering all the fre- quent 1-itemsets. Similarly, when P2 is processed, item 0 is not considered, and many of the candidate itemsets can be pruned. Therefore, it requires less memory space during the processing as one partition is processed at a time.

Suppose that {2, 3} is the only frequent 2-itemset found as a result of processing the frequent items in partition P3.

When the next partition P2 is being processed, item 1 in P2 is extended with each of the items in P3 first. As a result, we can find some frequent 2-itemsets including item 1. Let?s assume that {1, 2}, {1, 3}, and {1, 5} are found frequent.

Then, {1, 2} and {1, 3} are joined into {1, 2, 3}, and we need to check if {2, 3} is also frequent to determine whether {1, 2, 3} is a candidate 3-itemset or not. Since {2, 3} was found frequent when P3 was processed, {1, 2, 3} becomes a candidate 3-itemset. This explains why we need to process  the last partition of frequent items first. On the other hand, {1, 2, 5} and {1, 3, 5} are not candidate 3-itemsets because {2, 5} and {3, 5} were not found frequent.

In practice, if the estimated number of candidate itemsets to be generated is small after processing a certain number of partitions, then we can merge the remaining partitions into a single partition so that the number of database scans will be reduced.

2.2 Inverted Hashing and Pruning (IHP)  In the Inverted Hashing and Pruning algorithm, a hash table, named TID Hash Table (THT), is created for each item in the database. When an item occurs in a transaction, the transaction identifier (TID) of this transaction is hashed to an entry of the THT of the item, and the entry stores the number of transactions whose TIDs are hashed to that entry. Thus, the THT of each item can be generated as we count the occurrences of each item during the first pass on the database. After the first pass, we can remove the THTs of the items which are not contained in the set of frequent 1-itemsets, and the THTs of the frequent 1-itemsets can be used to prune some of the candidate 2-itemsets. In general, after each pass k ? 1, we can remove the THT of each item that is not a member of any frequent k-itemset, and the remaining THTs can prune some of the candidate (k + 1)- itemsets.

Consider a transaction database with seven items: A, B, C, D, E, F, and G. Figure 2 shows the THTs of these items at the end of the first pass. In our example, each THT has five entries for illustration purpose. Here we can see that item D occurred in five transactions. There were two TIDs hashed to 0, one TID hashed to 1, and two TIDs hashed to 4.

A B C D E F G     4 5                                    Entries  TID Hash Tables  Items  Figure 2. TID Hash Tables at the end of the first pass  If the minimum support count is 7, we can remove the THTs of the items B, D, E, and F as shown in Figure 3.

0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    A B C E F G     4 5                 Entries  TID Hash Tables  Items  D  Figure 3. TID Hash Tables of frequent 1- itemsets  Only the items A, C, and G are frequent and are used to de- termine C2, the set of candidate 2-itemsets. As in the Apri- ori algorithm, {A, C}, {A, G}, and {C, G} are generated as candidate 2-itemsets by pairing the frequent 1-itemsets.

However, in IHP, we can eliminate {A, G} from consider- ation by using the THTs of A and G. Item A occurs in 12 transactions, and item G occurs in 19 transactions. How- ever, according to their THTs, they can co-occur in at most 6 transactions. Item G occurs in 5 transactions whose TIDs are hashed to 0, and item A occurs in no transactions that have such TIDs. Thus, none of those 5 transactions that contain G also contains A. Item A occurs in 3 transactions whose TIDs are hashed to 1, and item G occurs in 6 transac- tions with those TIDs. So, in the set of transactions whose TIDs are hashed to 1, items A and G can co-occur at most 3 times. The other THT entries corresponding to the TID hash values of 2, 3, and 4 can be examined similarly, and we can determine items A and G can co-occur in at most 6 transactions, which is below the minimum support level.

In general, for a candidate k-itemset, we can estimate its maximum possible support count by adding the minimum TID hash count of the k items at each entry of their THTs.

If the maximum possible support count is less than the re- quired minimum support, it is pruned from the candidate set.

2.3 Multipass with Inverted Hashing Pruning (MIHP) Algorithm  The MIHP algorithm is a combination of the Multipass- Apriori algorithm and the Inverted Hashing and Pruning de- scribed above, and the details of MIHP is presented below:  1) Database = set of transactions; 2) Items = set of items; 3) transaction = ?TID, {x | x ? Items}?; 4) Comment: Read the transactions and count the  occurrences of each item and create a TID Hash Table (THT) for each item using a hash function h.

5) foreach transaction t ? Database do begin 6) foreach item x in t do begin 7) x.count+ +; 8) x.THT [(h(t.T ID)] + +; 9) end 10) end 11) Comment: F1 is a set of frequent 1-itemsets.

12) F1 = {x ? Items | x.count/|Database| ? minsup}; 13) Partition F1 into p partitions, P1, P2, . . . , Pp; 14) Comment: Process the partitions in the order of  Pp, Pp?1, . . . , P1.

15) for (m = p;m > 0;m? ?) do begin 16) Comment: Find Fk, the set of frequent  k-itemsets, k ? 2, whose members are in partitions Pm, Pm+1, . . . , Pp.

17) for (k = 2;Fk?1 	= ?; k + +) do begin 18) Comment: Initialize Fk before Pp is processed.

19) ifm = p then Fk = ?; 20) Comment: Ck is the set of candidate  k-itemsets whose members are in Pm, Pm+1, . . . , Pp.

21) Comment: Fk?1 ? Fk?1 is the natural join of Fk?1 and Fk?1 on the first k ? 2 items.

22) Ck = Fk?1 ? Fk?1; 23) foreach k-itemset x ? Ck do 24) if ?y | y = (k ? 1)-subset of x and y 	? Fk?1 25) then remove x from Ck; 26) Comment: Prune the candidate k-itemsets  using the THTs.

27) foreach k-itemset x ? Ck do 28) if GetMaxPossibleCount(x)/|Database| 29) < minsup then remove x from Ck; 30) Comment: Scan the transactions to count  the occurrences of candidate k-itemsets.

31) foreach transaction t ? Database do begin 32) foreach k-itemset x in t do 33) if x ? Ck then x.count+ +; 34) end 35) Fk = Fk ? {x ? Ck | x.count/|Database| ?  minsup}; 36) end 37) end 38) Answer = ?k Fk;  The formation of the set of candidate itemsets can be done effectively when the items in each itemset are stored in a lexical order, and itemsets are also lexically ordered.

As specified in line 22, candidate k-itemsets, for k ? 2, are obtained by performing the natural join operation Fk?1 ? Fk?1 on the first k ? 2 items of the (k ? 1)-itemsets in  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    Fk?1, assuming that the items are lexically ordered in each itemset [1]. For example, if F2 includes {A, B} and {A, C}, then {A, B, C} is a potential candidate 3-itemset. Then the potential candidate k-itemsets are pruned in line 24 by using the property that all the (k ? 1)-subsets of a frequent k-itemset should be frequent (k?1)-itemsets [1]. This is the subset-infrequency based pruning of the candidate itemsets.

Thus, for {A, B, C} to be a candidate 3-itemset, {B, C} also should be a frequent 2-itemset. To count the occurrences of the candidate itemsets efficiently as the transactions are scanned, they can be stored in a hash tree, where the hash value of each item occupies a level in the tree [1].

Here, GetMaxPossibleCount(x) returns the maxi- mum number of transactions that may contain a k-itemset x by using the THTs of the k items in x. Let?s de- note the k items in x by x[1], x[2], . . . , x[k]. Then GetMaxPossibleCount(x) can be defined as follows:  GetMaxPossibleCount(itemset x) begin k = size(x); MaxPossibleCount = 0; for (j = 0; j < size(THT ); j + +) do  MaxPossibleCount +=min(x[1].THT [j], x[2].THT [j], . . . , x[k].THT [j]);  return (MaxPossibleCount); end  size(x) represents the number of items in the itemset x and size(THT ) represents the number of entries in the THT.

For further performance improvement, the MIHP algo- rithm is used together with the transaction trimming and pruning method proposed as a part of the DHP algorithm [14]. The concept of the transaction trimming and prun- ing is as follows: During the kth pass on the database, if an item is not a member of at least k candidate k-itemsets within a transaction, it can be removed from the transaction for the next pass (transaction trimming), because it cannot be a member of a candidate (k + 1)-itemset. Moreover, if a transaction doesn?t have at least k+1 candidate k-itemsets, it can be removed from the database (transaction pruning), because it cannot have a candidate (k+1)-itemset. It is con- venient to use a slightly weaker form of the rule for MIHP.

The transaction trimming rule as stated is only applied to the items that are in the F1 partition being processed. The items that are not in the partition being processed are removed if they are not members of any candidate itemset during the current pass. Also, a transaction is pruned during the kth pass if it does not have at least k candidate k-itemsets, each of which contains one or more items from the current F1 partition being processed. Transaction trimming and prun- ing is synergistic with the candidate pruning by IHP.

2.4 Parallel MIHP (PMIHP) Algorithm  The Parallel MIHP algorithm is based upon the sequen- tial MIHP algorithm. The database is partitioned into al- most equal-sized local databases, one for each processing node, and the MIHP algorithm is parallelized by three ad- ditional actions: exchanging and merging of the local TID Hash Tables (THTs) of items between processing nodes to produce the global TID Hash Tables replicated in the pro- cessing nodes; identifying the global candidate itemsets that are just locally frequent in at least one node, but may have sufficient global support; and determining the global sup- port of the global candidate itemsets and updating the list of global frequent itemsets. The global THT of each item is just a linear cascade of the local THTs of the item, instead of creating a separate global THT whose TID hash count is the sum of the local TID hash counts at each entry.

For an itemset to be globally frequent in the whole database, it must be frequent in at least one local database.

If a local support count of an itemset is above the global minimum support count, then it is recorded as a globally frequent itemset. However, if the local count of an item- set is above the local minimum support count but less than the global minimum support count, then it is recorded as a global candidate itemset.

The major steps of the PMIHP algorithm are as follows:  1. Every processing node counts the occurrences of each item in its local database and builds the local THTs.

2. The local support counts and THTs of items are ex- changed between processing nodes, so that each pro- cessing node can obtain the global support counts of all items. Each processing node then determines the set of globally frequent items and discard the local THTs of globally infrequent items. The received local THTs of the items that are not local to the node are also dis- carded, and each node creates the global THT of each local item by linearly cascading its local THTs created by the processing nodes.

3. Every processing node partitions the frequent 1- itemsets into the same p partitions, P1, P2, . . . , Pp, as in the MIHP algorithm. The number of partitions is determined, such that the candidate itemsets for each partition would be resident in the main memory of pro- cessors. It has been shown that the total execution time is not sensitive to the partition size unless it is too large [11].

4. As in MIHP, the partitions are processed one by one, in the order of Pp, Pp?1, . . . , P1, by all the processing nodes. That means, all the processing nodes discover the globally frequent itemsets whose members are in  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    Pp first, and then discover the globally frequent item- sets whose members are in Pp?1 and Pp, and so on, until partition P1 is processed.

5. For each partition Pi, 1 ? i ? p, each processing node uses MIHP algorithm to find all the locally fre- quent itemsets from its local database. Some of these itemsets may have sufficient local support to be glob- ally frequent as well. The locally frequent itemsets that do not have sufficient local support to be globally fre- quent are global candidate itemsets. The global THTs are used to prune the global candidate itemsets by es- timating their maximum global support counts, as in MIHP. When certain number of global candidate item- sets are accumulated at each node, it requests other processing nodes to provide their local support counts of those global candidate itemsets. The other process- ing nodes to be polled to obtain their local support counts for each global candidate itemset can be easily determined by checking the local THTs contributed by the other processing nodes (in step 1) for the member items of the global candidate itemset. Only the pro- cessing nodes that have a positive TID hash count for the global candidate itemset will be polled.

6. The processing nodes exchange their lists of global fre- quent itemsets after all the partitions are processed.

To perform the communication between processing nodes efficiently, we imposed a logical binary n-cube struc- ture on the processing nodes. Then the processing nodes can exchange and merge the local information through in- creasingly higher dimensional links between them [5]. In the n-cube, there are 2n nodes, and each node has n-bit bi- nary address. Also, each node has n neighbor nodes which are directly linked to that node through n different dimen- sional links. For example, there are 8 nodes in the 3-cube structure, and node (000)2 is directly connected to (001)2, (010)2 and (100)2 through a 1st-dimensional link, a 2nd- dimensional link, and a 3rd-dimensional link, respectively.

Thus, in the n-cube, all the nodes can exchange and merge their local information in n steps, through each of the n dif- ferent dimensional links. When n = 3, the three exchange and merge steps are: step 1: node (??0)2 and node (??1)2 exchange and merge, where ? denotes a don?t-care bit.

step 2: node (?0?)2 and node (?1?)2 exchange and merge.

step 3: node (0??)2 and node (1??)2 exchange and merge.

It is neither necessary nor desirable to obtain the global support counts of the global candidate itemsets at the end of the processing of each partition Pi, 1 ? i ? p. Instead, when certain number of global candidate itemsets are accu- mulated at a node, it can start polling the other processing nodes to obtain their local support counts of those global candidate itemsets.

As in MIHP, transaction trimming and pruning can be used together with PMIHP. However, to provide the local support counts of global candidate itemsets, each process- ing node cannot prune the item that can be a member of some global candidate itemset from the transactions in its local database. Thus, the frequency of polling between processing nodes to obtain the local support counts of the global candidate itemsets must be balanced with the loss in efficiency caused by not trimming and pruning the transac- tions in the local databases.

3 Performance Analysis of Parallel MIHP  Some experimental tests have been done to evaluate the performance of PMIHP and to compare it with that of the Count Distribution (CD) algorithm [2]. The Count Distribu- tion algorithm is a parallel version of the Apriori algorithm.

In Count Distribution, the database is partitioned and dis- tributed over the processing nodes initially. At each pass k, every node collects local counts of the same set of candidate k-itemsets. Then, these counts are merged between pro- cessing nodes, so that each node can identify the frequent k-itemsets and generate the candidate (k + 1)-itemsets for the next pass. To merge the local support counts of the can- didate itemsets, synchronization between nodes is required at every pass, and maintaining the same set of candidate itemsets in all the nodes is redundant.

We implemented PMIHP and Count Distribution on a Linux cluster with 9 nodes connected by a Fast Ethernet switch. Eight of the nodes were used for parallel mining, and one node was used as a management console. Each node has a 800 MHz Pentium processor, 512 Mbytes of memory and a 40 Gbytes disk drive.

Both PMIHP and Count Distribution were implemented in the Java language. The Java virtual machine was the IBM JVM 1.1.8 for the Linux operating system with the just-in- time (JIT) compiler enabled. The interprocess communica- tion was performed using the Java RMI (Remote Method Invocation) protocol.

For all of the tests, the JVM memory for objects was constrained (via the mx parameter) to 416 Mbytes. The ini- tial heap size was set to 416 Mbytes (via the ms parameter) to control the effect of heap growth overhead on the per- formance. The partition size used for the PMIHP was 100 frequent items, and the global TID Hash Table (THT) size was 400 entries for each item. As the global THT of each item is a linear cascade of its local THTs, the local THT size for the 1-node case was 400 entries per item, and was 50 en- tries per item for the 8-node case. It has been shown that the sizes of the partitions and THT are not critical for the over- all performance [12]. The mining algorithms were executed on 1-node, 2-node, 4-node, and 8-node configurations.

We used a 6-month sample of the Wall Street Jour-  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    nal published from April 2, 1990 through September 28, 1990 for the performance comparison between PMIHP and Count Distribution as well as between MIHP and Apriori.

The sample contains 21,703 documents and 116,849 unique words. We varied the minimum support level from 5% to 1.75% to measure its impact on the miners. For PMIHP and Count Distribution, the 6-month sample was sequentially distributed to the processing nodes by assigning the articles of 16 or 17 days to each node.

Figure 4 shows the total execution times of Apriori, FP- Growth and MIHP; and Figure 5 shows the total execution times of PMIHP and Count Distribution when both algo- rithms were run on 8 nodes. The Apriori and Count Dis- tribution algorithms were not able to run within the mem- ory constraint of 416 Mbytes when the minimum support level is below 2%. The memory requirement for the can- didate itemsets is the limiting factor for both Apriori and Count Distribution. MIHP has much better performance than Apriori because MIHP prunes many candidate itemsets by using the Inverted Hashing and Pruning, and processes a limited number of candidate itemsets at a time based on the Multipass approach. More performance analysis result of MIHP is available in [12].

The FP-Growth algorithm performed well at the higher minimum support levels. However, its performance deteri- orated at the 2% minimum support level. The FP-tree be- comes too large when the minimum support level is low, and as a result, the total execution time increases sharply.

Lower minimum support levels were not attempted.

As shown in Figure 5, PMIHP performs significantly better than Count Distribution, and the performance gain increases as the minimum support level decreases. It is im- portant to note that the minimum support levels were se- lected in this test, such that the miners would run within the constraint of the main memory and thus eliminates the effect of paging upon the performance.

Comparing Figures 4 and 5, we can see that the speedup of Count Distribution over Apriori is fairly good. When the minimum support level is 2%, it is about 6, whereas the speedup of PMIHP over MIHP is about 4. However, as we shall see below, the speedup of PMIHP is quite good at a lower minimum support level of 0.15%. Since the amount of computation increases rapidly as the minimum support level decreases, the speedup improvement at low minimum support levels is quite encouraging.

To evaluate the effect of the number of processing nodes on the performance of PMIHP, we used a 8-day sample of the Wall Street Journal, starting from October 1, 1991.

There were 1,427 documents and 31,290 unique words. We used a minimum support count of 2 documents (i.e., mini- mum support of 0.15%) and a stop-word list from Fox [8].

There were 12,828 frequent words. The minimum support count of two documents was selected based upon the result  ? Apriori  ? FP-Growth  ? MIHP  Minimum Support  Ti m  e (s  ec on  ds )  ?  ?  ? ?  ?  ???  ? ?  ???0       5.00% 4.00% 3.00% 2.00% 1.75%  Figure 4. Total execution time to find all fre- quent itemsets (in 21,703 documents)  ? CD  ? PMIHP  Minimum Support  Ti m  e (s  ec on  ds )  ?  ?  ? ?  ?  ?  ???0      5.00% 4.00% 3.00% 2.00% 1.75%  Figure 5. Total execution time to find all fre- quent itemsets (in 21,703 documents, on 8 nodes)  of our experiments showing that low minimum support lev- els are required to use the frequent itemsets for document retrieval and ranking. We did not stem the words, but we monocased the words.

The database was assigned to the processing nodes se- quentially by day. For the 2-node case, one node was as- signed the articles of the first 4 days and another node was assigned the articles of the last 4 days. The assignments for the 4-node and 8-node cases were done in a similar man- ner. These daily collections have a mean of 178, a standard deviation of 22.58, and a median of 174 documents.

Figure 6 shows the total execution time of PMIHP re- quired to find frequent 3-itemsets as the number of process- ing nodes changes, and Figure 7 shows the corresponding speedup over the sequential processing. We can see that the speedup increases as the number of processing nodes increases, and the increasing rate is slightly higher than lin-  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    ear. The speedup is 1.65 for the 2-node system, indicating some degree of parallelization overhead mainly due to the interprocess communication to exchange the support count information. The speedup is 3.76 for the 4-node system, but the increasing rate of the speedup is 2.27 as the number of nodes is doubled from 2 to 4. As the number of nodes is doubled from 4 to 8, the increasing rate of the speedup is higher than linear again, which indicates that PMIHP is quite scalable.

Number of Processors  Ti m  e (s  ec on  ds )       1 2 4 8  Figure 6. Total execution time of PMIHP to find frequent 3-itemsets (in 1,427 documents, min- sup = 0.15%)  Number of Processors  S pe  ed up  ?  ?  ?       2 4 8  Figure 7. Speedup of PMIHP (minsup = 0.15%)  The PMIHP algorithm has two main data mining activi- ties: counting the support of local candidate itemsets in the corresponding local database; and counting the support of global candidate itemsets in multiple local databases. These two activities are interleaved during the mining as the global support counting is invoked when the number of identi- fied global candidate itemsets exceeds a certain number at each processing node, which was set to 20,000 in our ex- periments. To measure this global support counting time,  we reconfigured PMIHP to defer the global support count- ing of the global candidate itemsets at each node and syn- chronized the nodes before the start of the global support counting phase. Figure 8 shows the global support counting time of the mining process with the logest run time among all the mining processes executed on different processing nodes. Moreover, we used the wall clock time to measure this global counting time, hence it is an upper bound of the actual global support counting time of all the mining pro- cesses.

Comparing Figures 6 and 8, we can see that the 2-node case has a much longer global support counting phase than 4-node and 8-node cases. The portion of the global support counting phase for the 2-node case is about 8% of the total execution time, but it is about 4% for the 4-node case, and about 3% for the 8-node case. Thus, the impact of the global support counting time on the overall speedup is very small, and it is reduced further as the number of processing nodes increases.

Number of Processors  Ti m  e (s  ec on  ds )       2 4 8  Figure 8. Global support counting time to find frequent 3-itemsets  Since our processing environment does not provide the statistics for job accounting, exact CPU time measurement was not feasible. So, we measured the average execution of a processing node using the wall clock time. Figure 9 shows the average execution time of a node in the 1-node, 2-node, 4-node, and 8-node configurations. We can see that the 2-node case requires significantly less average execu- tion time per node than the 1-node case; and as the number of processing nodes increases further, the average execu- tion time per node deceases more than linearly. This result is completely consistent with the observed speedup values, and also indicates that increased efficiency is behind the per- formance gain.

Since the identical PMIHP algorithm was executed on all of our system configurations, the differences in execu- tion time must be associated with some workload differ- ences. Figure 10 shows the average number of candidate  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    Number of Processors  Ti m  e (s  ec on  ds )       1 2 4 8  Figure 9. Average execution time per node to find frequent 3-itemsets  2-itemsets processed by each node in our four different sys- tem configurations. Note that the number of candidate 2- itemsets for the 1-node case is approximately the same as the average number of candidate 2-itemsets for the 2-node case. This result is consistent with the observed total and average execution times for the 1-node and 2-node cases.

There is significant reduction in the average number of can- didate 2-itemsets processed for the 4-node and 8-node cases over the 1-node and 2-node cases. This result represents the nonuniform distribution of itemsets over the local databases as well as the effective reduction of the candidate itemsets by the Inverted Hashing and Pruning technique.

N um  be r  of C  an di  da te  -it  em se  ts       M IHP  2-node PM IHP  4-node PM IHP  8-node PM IHP  Figure 10. Average number of candidate 2- itemsets per node  Figure 11 shows the average number of candidate 3- itemsets processed by each node. We included the number of candidate 3-itemsets processed in Apriori to demonstrate the usefulness of the Inverted Hashing and Pruning. The number of candidate 2-itemsets for Apriori was about 82 million, which is why we did not show that in Figure 10.

We can observe the same pattern of reduction in the candi-  dates 3-itemsets as in the candidate 2-itemsets. This reduc- tion in the average number of candidate itemsets processed by each processing node may be the most clear explanation for the high increasing rate of the speedup observed as the number of processing nodes increases.

N um  be r  of C  an di  da te  -it  em se  ts       Apriori  M IHP  2-node PM IHP  4-node PM IHP  8-node PM IHP  Figure 11. Average number of candidate 3- itemsets per node  We also ran a test with a larger database: 8 weeks of the Wall Street Journal, published from January 2, 1991 through February 22, 1991 (February 23rd was a Wall Street Journal holiday). There were 6,170 documents and 64,191 unique words, of which 31,948 were frequent words at the minimum support level of 0.03%, i.e., 2 out of 6,170 doc- uments. The 1-node system required 845,702 seconds to find 1,554,442 frequent 2-itemsets, whereas the 8-node sys- tem required 33,183 seconds. This performance represents a superliner speedup of 25.5 of the 8-node system over the 1-node system. Thus, we can conclude that the performance of PMIHP is quite scalable when the database is large and the minimum support level is low, which is the case of high workload.

The 1-node case generated 16,174,357 candidate 2- itemsets, whereas the 8-node case generated 2,459,629 can- didate 2-itemsets per node on the average. The total num- ber of candidate 2-itemsets counted by the 8 nodes were 19,677,031, which means that only 21.7% of the candidate 2-itemsets were counted at more than one processing node.

This implies that the distribution of words across the 8-week sample of the Wall Street Journal is quite skewed.

In the Count Distribution algorithm, all the nodes count the same set of candidate itemsets in each pass over the database regardless of the distribution of items over the lo- cal databases. On the other hand, in our PMIHP algorithm, not all candidate itemsets are counted at more than one node when the distribution of items over the local databases is not uniform. Obviously, the more skewed the data distri- bution, the better the performance of PMIHP. Cheung et al. [4] proposed several approaches to partition the database  0-7695-2132-0/04/$17.00 (C) 2004 IEEE  Proceedings of the 18th International Parallel and Distributed Processing Symposium (IPDPS?04)    to achieve a high degree of skewness. Text documents, ar- ranged in a chronological order, do appear to have a high degree of skewness, and benefit the PMIHP algorithm.

4 Conclusions  The proposed Parallel Multipass with Inverted Hashing and Pruning (PMIHP) algorithm is a parallel version of our Multipass with Inverted Hashing and Pruning (MIHP) al- gorithm, and it is effective for mining frequent itemsets in large text databases. The Multipass approach reduces the required memory space at each processor by partitioning the frequent items and processing each partition separately.

Thus, the number of candidate itemsets to be processed is limited at each instance. The Inverted Hashing and Prun- ing is used to prune the local and global candidate itemsets at each processing node; and it also allows each processing node to determine the other peer processing nodes to poll in order to collect the local support counts of each global candidate itemset.

PMIHP distributes the workload to multiple processing nodes to reduce the total mining time without incurring much parallelization overhead. The average number of can- didate itemsets to be counted at each processing node is much smaller than the case of sequential mining, while the time for the synchronization between processing nodes to exchange the count information for the global candidate itemsets is very small compared to the total execution time.

PMIHP is able to exploit the natural skewed distribution of words in text databases, and demonstrates a superlinear speedup as the number of processing nodes increases. It has a much better performance than well-known parallel Count Distribution algorithm [2], because the average number of candidate itemsets to be counted at each processing node is much smaller, especially when the minimum support level is low. Overall, the performance of PMIHP is quite scal- able even when the size of the text database is large and the minimum support level is low, which is the case of high workload.

