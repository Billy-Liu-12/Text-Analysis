Objective-Oriented Utility-Based Association Mining

Abstract  The necessity to develop methods for discovering as- sociation patterns to increase business utility of an enter- prise has long been recognized in data mining community.

This requires modeling specific association patterns that are both statistically (based on support and conjidenceJ and semantically (based on objective utility) relating to a given objective that a user wants to achieve or is interested in.

Howevec we notice that no such a general model has been reponed in the literature. Traditional association mining focuses on deriving correlations among a set of items and their association rules like diaper --t beer only tell us that a pattern like {diaper} is statistically related to an item like beer. In this paper, we present a new appmach. called Objective-Oriented utility-based Association (OOA) min- ing, to modeling such association patterns that are explicitly relating to a user's objective and its utility Due to its focus on a useri objective and the use of objective utility as key semantic information to measure the usefulness of associa- tion patterns, OOA mining differs significantly fmm existing appmaches such as the existing constraint-based associa- tion mining. We formally dejine OOA mining and develop an algorithm far mining OOA rules. The algorithm is an enhancement to Apriori with specific mechanisms for han- dling Objective utility. We prove that the utility constraint is neither monotone nor anti-monotone nor succinct nor con- vertible and present a novel pruning strategy based on the utility constraint to improve the efficiency of OOA mining.

0-7695-1754-4102 $17.00 0 2002 IEEE 426  1 Introduction  Association mining is an important problem in data min- ing. Briefly, given a historical dataset of an application, we derive frequent patterns and association rules from the dataset by using some thresholds, such as a minimum sup- port and a minimum confidence. Since Agrawal's pioneer work [l], a lot of research has been conducted on asso- ciation mining. Major achievements include approaches to improving the efficiency of computing the frequent pat- terns from large datasets [Z, 51, approaches to applying con- straints to find more interesting patterns 13, 11, 151. and ap- proaches to eliminating irrelevant association rules by mak- ing use of some interestingness measures 19, 141.

Observe that most existing approaches to association mining are itemset-correlation-oriented in the sense that they aim to find out how a set of items are statistically cor- related by mining association rules of the form  Il, ..., I,,, 4 1,+1(s%,c%) (1)  where s%, the support o f  the rule, is lhe probability of all items 11, ..., I,,,+, occurring together, and c%, the conji- dence OF the rule, is the conditional probability of 1,+1 given the itemset {Il, ..., I,,,}. Both s% and c% are ob- tained simply by counting the frequency of the respective itemsets in a given dataset, and are greater than or equal to the user-specified minimum support and minimum confi- dence. respectively.

Although finding correlations of itemsets like diaper + beer is very important, in many situations people may be more interested in finding out how a set of items support a specific objective Obj that they want to achieve by discov-  mailto:ydshen@ios.ac.cn   ering association rules of the form  I ] ,  ..., I ,  + Obj(s%,c%,u) (2) where s% is the probability that all items 11, ..., I,,, together with Obi hold, c% is the conditional probability of Obi giventheitemset{Il, ..., I,},anduis theufilifyoftherule, showing to what degree the pattern {I1, ..., I,} semanti- cally supports Obj. Due to its focus on an objective and the use of objective utility as key semantic information to measure the usefulness of association patterns, we refer to this new type of association mining as Objective-Oriented urility-based Association (OOA) mining, as opposed to tra- ditional Ifemset-Correlafiou-Oriented Associafion (ICOA) mining.

OOA mining derives patterns that both statistically and semantically support a given objective Obi. Informally, I = {Il, _.., I,} is said to stafisfically suppon Obi if the support s% and confidence c% of the rule (2) are not be- low a user-specified minimum support ms% and a user- specified minimum confidence me%, respectively. And I is said to semanfically suppon Obj if the utility u of the rule (2) is not below a user-specified minimum utility mu.

As a result, all patterns derived in OOA mining must be in- teresting to an enterprise since when employed, they would increase the (expected) utility of the enterprise above the user-specified minimum level (U > mu). Therefore, OOA mining has wide applications in many areas where peo- ple are looking for objective-centered statistical solutions to achieve their goals. For a typical example, in business situ- ations a manager may use OOA mining to discover the best business strategies by specifying hisiher objective as ?high profit and low risk of loss:? Another example is in medi- cal field. A doctor may use OOA mining to find the best treatments for a disease by specifying an objective ?high effectiveness and low side-effects.?  The term utility is commonly used to mean ?the quality of being useful? and utilities are widely used in decision making processes to express user?s preferences over deci- sion objects towards decision objectives [6, 131. In deci- sion theory, we have the well-known equation ?Decision = probability + utility,? which says that a decision object is chosen based on its probability and utility. Since associ- ation mining can be viewed as a special decision problem where decision objects are patterns, we may well have, cor- respondingly, an equation ?Interestingness (of a pattern) = probability + utility.? This equation further justifies the ne- cessity and significance of enhancing traditional probability (support and confidence) based association mining with ob- jective related utilities.

Since utilities are subjective, they can be acquired from domain expertdusers. We would point out, however, that this does not mean we need to acquire a utility for each sin- gle item in a dataset. As we will see in Section 3, it suffices  to obtain utilities only for those items in a dataset which are directly related to the given objective. The population of such objective items would be quite small in practical appli- cations.

In this paper, we systematically study OOA mining. In Section 3, we formally define the concepts of objective, support, confidence, and utility under the frame of OOA mining. In Section 4, we develop an algorithm for mining OOA frequent patterns and rules. The algorithm is based on Apriori. with an enhancement that handles objective utility.

Traditional association mining is NP-hard, but OOA min- ing does not seem to be easier. To improve the efficiency of OOA mining, we will present a novel strategy for prun- ing itemsets based on the support and utility constraints. In Section 5 ,  we present some experimental results.

2 Related Work  The necessity to develop methods for finding specific patterns which can be used to increase business utility has long been recognized by several researchers [7, 10, 141. To the best of our knowledge, however, no work on association mining has been reported in the literature which formally models such patterns that are explicitly relating to a user?s objective and its utility.

Our work is related to but different from existing con- strained association mining. Existing constrained associa- tion mining, typically represented by the work of Bayardo, Agrawal, and Gounopolos [31, Han, Lakshmanan, Ng, Pang and Pei [ l l ,  121, and Srikant, Agrawal and Vu [15], takes the form { ( S  + T)IC} where S and T are sets of items and C is a set of constraints on the selection of S and T. When T is not empty, such kind of association min- ing belongs to ICOA mining because no matter what con- straints C is. it always derives asociation rules of the form I,, ..., I,,, + J1, ..., J,, where both itemsets { I ] ,  ..., I,} and {JI, ..., J,,} satisfy C. Certainly, OOA mining can use constraints, too. Constrained OOA mining takes the form { (S + Obj)lC,bj} where c o b ,  is a set of constraints on the selection of S in terms of the objective Obi. Constrained OOA mining always derives OOA rules.

Another significant difference between existing con- strained association mining and OOA mining is that most exisitng work focuses on SQL-style constraints includ- ing item selection, pattern length, set relations (c, 2.

etc.), ~uxc(S)OU, min(S)Ou, surn(S)Ov, m n t ( S ) O v  and aug(S)Bv, where S is an itemset, U is a real number, and 0 is - < or 2 (see [I21 for a summary of types of constraints dis- cussed in the literature). These constraints fall into one of the following four well-defined categories: monotone, anti- monotone, succinct or convertible. In OOA mining, how- ever, we introduce objective utility as a key constraint. On the one hand, an (arbitrary) objective and its utility are dif-     ficult, if not impossible, to be formulated using SQL-style constraints. On the other hand, the utility constraint is nei- ther monotone nor anti-monotone nor succinct nor convert- ible. Therefore, no existing constrained association min- ing methods are applicable to it. In this work we push the utility constraint deep into OOApriori (a variant of Apriori) to prune candicate patterns in order to efficiently derive all OOA rules.

We would paint out that although business objectives, such as ?high profit and low risk of loss,? can be viewed as constraints, such constraints seem to be at a meta-level w.r.t.

the above mentioned SQL-style constraints. Therefore, spe- cific mechanisms are required to represent and handle them.

The proposed OOA mining may then be the first such mech- anism.

Most recently, Wang, Zhou and Han [I61 and Lin, Yao and Louie [8l suggested adding values to association rules.

The former takes into account the price and quantity of su- permaket sales during association mining, while the latter tries to attach a value to every item in a dataset and use the added values to rank association rules. There are three ma- jor differences between their approaches and ours. First, we do general objective centered mining by explicitly declar- ing a user?s objective and formulating it in a simple, uni- form way (see Section 3). As a result, utilities are assigned only to those items which directly contribute to the objec- tive. Second, we handle both positive and negative utilities, whereas they only consider positive values. Negative util- ity represents punishmenfloss, and it is with negative val- ues that our utility constraints become neither monotone nor anti-monotone nor succinct nor convertible. Third. we push the utility constraints into Apriori and use them to prune candidate itemsets. Neither of the above two approaches addressed this.

Finally, our work is different from existing research on ?interestingness? [9, 141, which focuses on finding ?inter- esting patterns? by matching them against a given set of user?s beliefs. Informally, a derived association rule is con- sidered ?interesting? if it conforms to or conflicts with the user?s beliefs. In contrast, in OOA mining we measure the interestingness of OOA rules in terms of their probabilities as well as their utilities in supparting the user?s objective.

3 Objective, Support, Confidence, and Utility  We assume that readers are familiar with traditional as- sociation rule mining, especially with the widely used Apri- on algorithm [Z]. A dura base or dorase? D B  is associated with a finite set DB,tt of attributes. Each attribute Ai has a finite domain Vi (continuous attributes can be discretized using methods such as that in [41). For each U E V,, A, = U is called an irem. An itemset or a parrern is a set of items.

A k-iremser is an itemset with k items. D B  consists of a fi-  nite set of recorddtransactions built from DBOtt, with each recordbeingaset{Ai = v i ,  ..., A,,, = u,}ofitemswhere Ai # Aj for any i # j .  We use lDBl to denote the to- tal number of records in D B .  Finally, for any itemset I the function count(I, D B )  returns the number of records in D B  that are supersets of I.

An objective describes anything that we want to achieve or we are interested in. In order to discover patterns in a dataset D B  that support our objective Obj, we need first to formulate Obj in terms of items of D B .  This can be done by first partitioning DBatt into two disjoint subsets: DBatt = D B Z y  U DBFtbi where each attribute A E D B Z y  obviously contributes to Obj, whereas each A E DBFzb? does not. For convenience, we refer to attributes in DBZ? as objective anribufes.

Let A be an objective attribute and V its domain. For each U E V ,  A = U is called an objective irem or a class of A. We use class(A) to denote all classes of A. Let ?$3 be a relation symbol such as =, >, <, etc. For each U E V, ARv is called an objecrive relation. An objective can then be represented by a logic formula over objective relations using the connectives A, V or 7.  Formally, we have  Definition 1 An objective Obj over a dataset D B  is a dis- junctive normal form C1 V ... V c,,, (rn 2 1) where each Ci is a conjunction D1 A ... A D,  (n 2 1) with each D j  being an objective relation or the negation of an objective relation.

With an objective Obj as formulated above, we can then evaluate against a dataset how a pattern I = {Il, ..., I,,,} statistically and semantically supports Obj by defining the support, confidence and utility of the corresponding rule 11, ..., lm i Obj. In OOA mining, we say an objective Obj holds in a record T in D B  (or we say r suppons Obj) if Obj is true given T. Furthermore. for any itemset I = {Ti,  ..., Im} we say I U {Obj}  = {Ii, ..., I,,Obj} holds in T if both Obj and all I,s are true in T. We then extend the function cmnt ( I ,  D B )  to cuunt(1 U {Obj} ,  D B )  that returns the number of records in D B  in which I U {Obj} holds.

Definition 2 Let 11,  ..., Im i Obj (s%, c%,u) he an as- sociation rule in OOA mining. Then the support and confi- dence of the rule are respectively given by  count({Ii, ..., l , , , ,Obj},DB) * loo%, lDBl  s% = (3)  Let Obj be an objective and A an objective attribute.

Based on Obj, the classes of A can be subjectively clas- sified into three disjoint groups: dass (A)  = class+(A) U class-(A) U classO(A) where class+(A) consists of all     classes of A that show positive support for Obj, class-(A) of all classes of A that show negative support for Obj, and dasso(A) of all classes of A that show neither pos- itive nor negative support for Obj. Therefore, classes in dass+(A) will bring Obj positive utilities, whereas classes in class-(A) bring negative utilities. We then associate  side-effects which are assigned by experienced domain ex- perts. The doctor then wants to discover from DB, the best treatments with high effectiveness and low side-effects.

Apparently, this is a typical objective-oriented utility-based mining problem.

UA=", (6) c u;(r) = A=u?C,AA="(u.+.) Eclo8s- ( D E )  each class A = v in class+(A) or class-(A) with a utility UA=" (a real number). Since any class in classo(A) can be considered as a special positive class with a utility 0, we can merge classo(A) into the positive group. Therefore, in the sequel we always assume that any class A = v belongs to either class+(A) or class-(A). The groups of positively and negatively supporting classes of a dataset D B  for Obj are then respectively defined as follows: class+(DB) = { A  = v (uA=.)IA E D B z y  and A = v E class+(A)} and class-(DB) = { A  = v (~IA=.)IA E and A = 1) E class-(A)}.

An OOA itemser (or OOA pattern) is a set { A ,  = vl, ..., A,,, = urn} of items with A, E OB::" and A, # A, for any i # j. Let I be an OOA itemset and T a record in D B  with I T .  Let C, be the set of classes in T .  The positive utility u:(I) (resp. negative utility u; ( l ) ) of T for I is the sum of the utilities of all positively (resp.

negatively) supporting classes in C,, given by  u:(r) = c uA=v,  ( 5 ) A="?C.hA="[" ,= . )?e l~~~+ ( D E )  effectiveness I side-effect 5 getting much better 4 very serious 4 eettine better 3 serious vet tolerable  Table 1. A I  9 4  14 5  The positive and negative utility of D B  for I are then  &(I)  = U%), (7)  edical dataset DB1.

3 1 4 1  L -  3 noobviouseffect 2 alittle 2 getting worse 1 normal 1 getting much worse  Table 2. Degrees of the effectiveness and side-effects.

Table 3. Su ports, confidences and utilities.

b j  : (effectiveness>3) A (side-effect<3)  treatment=l+ Obj -1.6 ueatment=2+ Obj 12.5% 50% -1 treatment=3+ Obj 12.5% 66% -0.2 treatment=4+ Obj  18.75% 75% 0.8 treatment=S+ Obj 18.75% 75% 1.2  Note that the last two rules have quite different utilities for the objective, although their support and confidence are the same. Therefore, "treatment=5" should be the best because it has the highest utility in supporting the objective.

4 Mining OOA Rules  4.1 Objective-Oriented Apriori  Definition 4 Let D B  be a dataset and Obj an objective.

Let ms%. mc% and mu be a user-specified minimum sup- port, minimum confidence and minimum utility, respec- tively. Let I = {Il, ..., I,} be an OOA itemset. I is an OOAfrequenr partem/irernser in DE if 5% 2 ms%. Let I be an OOA frequent pattern. I1, ..., I,,, --f Obj (s%, e%, U) is an OOA association rule (OOA rule) if c% 2 mc% and U 2 mu. Here s%, c% and U are as defined in Equations ( 3 ) .  (4) and (9), respectively.

OOA mining is then to derive all OOA N I ~ S  from D E .

We extend Apriori [21 to generating OOA frequent patterns and rules by enhancing it with mechanisms for handling ob- jectives and utilities. For convenience, we refer to the ex- tended algorithm as Objective-Orienred Apriori (OOApri- ori).

For the data structure. we associate each OOA itemset with some necessary data fields to record data like counts and utilities. This is done by organizing an itemset into a structure using pseudo Cf+ language. That is, each OOA itemset I = (4 ,  ..., Im} is internally an instance of the data type ITEMSET defined as follows:  typedef struct { set pattern: //store the pattern {I,, ._.,I,} int countl; //store count(I, DB) int colmtz;/lstoreurunt(IU { O b j } , D B ) float float  U+; //store u+oS(I) (see the formula(7)) U-; //store u&(I) (see the formula (8))  } lTEMSET  We use I .D to refer to the field D of I. I.cmmtl,

I.count2, I.u+ and I.u- are all initialized to 0 when I is  created. Moreover. when no confusion would occur, by I we refer to its pattern I.puttern = { I l ,  ..., I,,,}.

Algorithm 1: Objective-Oriented Apriori.

Input: ms%, mc%, mu. Obj and DB.

Output: F P ,  the set of OOA frequent itemsets, and  function OOApriori(ms%,mc%,mu, Obj,  DB) 1) A R = F P = 0 ; 2) k = 1; 3 )  Ck = {I I I is an OOA 1-itemset in D E } ;  /Part 1: Collect counts and utilities of k-itemsets 4) for each record T in DE 5) 6) 7) I . m n  t ++; 8 ) 9) 10) 11) I . m n t z t + 12) end  13)Lk =0: 14)foreachI = { I l ,  ..., I k }  E c k 15) 16)  AR, the set of OOA rules.

for each k-itemset I E Ck if I g T then begin

I.U+ = I.U+ + u:(I);

I.u- = 1.u- + u;(I); if Obj holds in T then  /Part 2: Check for frequent patterns (Lk) and rules (AR)  ifs% = & I.count 2 ms% then begin LI. = Lk U { I } ;  - I.eo""tz.

17) 1.coun1, ' 18) I.eo"n1, 9  = r.u+-r.u-.

19) 20) A R = A R U { I i  ,..., I k  + O b j ( ~ % , c % , u ) } 21) end  22) if Lh # 0 then begin 23) kt+; 24) 25) goto4) 26) end 27) return F P  = Ui L, and AR end  if c% 2 mc% and u 2 mu then  //Part 3: Generate (k+l)-itemsets  Ck = uprioriGen(Lk-l); /Mew candidate itemsets  In Algorithm 1, for each k 2 1 c k  is used to store can- didate frequent OOA k-itemsets, Lk to store frequent OOA k-itemsets. and AR to store all OOA rules. OOApriori con- sists of three major parts. The first part (lines 4-12) scans the dataset D B  and applies each record in D B  to count- ing the frequency and computing the positive and negative utilities of each candidate itemset in C,. At lines 8 and 9, u t ( 1 )  and u;(I) are as defined in Equations (5) and (6). The second part (lines 13-21) checks the support, confi- dence and utility of each candidate itemset I = {Il, ..., I k } in Ck against the three user-specified minimums ms%.

mc% and mu to see if I is an OOA frequent pattern and I , ,  ..., I k  + Obj is an OOA rule. After all OOA fre- quent k-itemsets and rules have been generated, the third     part (lines 22-26) of OOApriori generates new candidate (k + 1)-itemsets based on Lk by calling the following func- tion aprioriGen().  This function is borrowed from Apriori 121.

function aprioriGen(Lk) 1) Ck+1 = 0; 2) for each pair of itemsets in Lk of the form 3) {Il, ..., I k - 1 ,  I k }  and {Il, ..., I k - 1 ,  b + l } 4) ck+1 = ck+1 U {{I13 .,.>Ik+l}};  / h n e  itemsets 5 )  for each I E Ck+l 6) 7) 8) returnCk+I end  if some k-sub-itemset of I is not in Lk then c k + l  = Ck+l - {I}; //Remove I from Ck+l  After the set c k + l  of new candidate itemsets has been generated, the process goes to the next cycle (line 25) for deriving OOA frequent (k + 1)-itemsets and rules. OOApri- on will continue this way until no new OOA frequent item- sets can be generated (line 22).

Theorem 1 I f I  = {Il, .._, I,,,} is an OOAfrequentpattern and 3 c I with 3 f 0, then 3 is an OOAfrequentpartem.

Theorem 2 OOApriori is sound and complete in the sense that I i s  an OOAfrequenr itemset ifand only $ I  E FP and that 11, ..,, I,,, --f Obj(s%, .%,U) is an OOA d e  if and only if i t  is in AR.

4.2 A Pruning Strategy for Mining OOA Rules  Theorem 2 shows the correctness of applying OOApriori to computing OOA frequent itemsets and rules. In this sec- tion we develop a pruning strategy to improve its efficiency.

Here and throughout, when we say that an OOA itemset I = {Il, ._.i I,,,} passedviolates the confidence or the util- ityconstraint,wemeanthattheOOAruleI,, ..., I ,  + Obj passedviolates the constraint.

Four types of constraints for association mining have been identified in the literature [ I  1, 121. Let C be a con- straint and St and SZ be two arbitrary itemsets. For SI c Ss, C is anti-monotone if SI violating C implies Sz vio- lates C, and C is manotone if SI satisfying C implies Sz satisfies C. If C is succinct then SI and SZ satisfying C implies SI U Sz satisfies C. C is convertible i f  there exists an order R on items such that for any itemset S satisfying C,  every prefix of S w.1.t. R satisfies C.

Theorem 1 assures us that the support constraint for 00.4 frequent patterns is anti-monotone. Therefore, in OOApriori we can safely delete an itemset I from L k  when its support is below the minimum support (see line 15) be- cause no frequent patterns will be built from I. It turns out, however, that neither the confidence nor the utility con- straint for OOA rules is anti-monotone.

Theorem 3 The utility constraint for OOA rules is neither monotone nor anti-monotone nor succinct nor convenible.

The pruning problem is then described as follows: For any itemset I in L k  (see the OOApriori algorithm) that has passed the support constraint but violates either the confidence or the utility constraint, can we delete I from Lk without missing any OOA rules? Without any prun- ing mechanism, OOApriori will generate all OOA frequent items, many of which may produce no OOA rules because of the violation of the confidence or the utility constraint.

Look at the function aprioriGen(Lh) again. Since all (k f 1)-itemsets are composed from the k-itemsets in Lk, we need to keep L b  as small as possible by removing some OOA frequent itemsets from which no OOA rules would be possibly built.

We present a pruning strategy using the support and util- ity constraints. To describe the pruning strategy, we add two more data fields to the internal structure of an OOA itemset I as shown below:  typedef s m c t  { set int c a n t l ;  //store count(I, D B ) int float U+; //store u z B ( l ) float U-; //store u&(J) int a n t : ;  //store IS+( float  pattern; //store the pattern {II, ..., I,,,)  m n t z ;  //store cm~nt(I U {Ob j } ,  D E )  lnu; //store the least negative utility } ITEMSET;  Here, let S be the set of records in D B  in which I U {Obi} holds and S+ be the set of records in S which contain no negative class (i.e., all classes of these records are in class+(DB)) ,  then the first new field count: is used to store IS+( (note that the field covntz stores IS() and the second new field lnu is used to store the least negative util- ity of a record in S - S+, i.e. lnu 5 u;(I )  for any T in s - s+.

Strategy1 Remove any OOA itemset I = { I I ,  ..., I k } fromLk ifI.count: < ms%*(DB( and < mu, where LE- = (ms% * IDB( - I.count:) * I.lnu.

Since I is a frequent OOA itemset, there are at least ms% * (DB(  records in (DBI in which I U {Obi} holds.

When I.count$ < ms% * ( D E ( ,  there a e  at least (ms% t lDB( - I.count:) records in D B  in which I U {Obj} holds that contain negative classes. Therefore, LB- > 0 is the least negative utility of D B  for I and thus is the lower bound of I.u-. As a result, I.u+ - LB- is the upper bound of the utility of D B  for I .  To sum up, this strategy says that an OOA frequent itemset I is removable if the upper bound     of its expected utility is below the minimum utility. The fol- lowing theorem shows that applying this strategy will not miss  any OOA rules.

Theorem4 Letr = {Il, ..., I k }  beanOOAfrequentitem- set. If I.cuunt: < ms% * (DB(  and < mu then there is no OOA itemset J = ( J I ,  ..., J,} 2 I such that .Il, . . . l  J,, -+ Obj is an OOA rule.

It is easy to push Strategy 1 into the OOApriori algo- rithm. This is done by replacing lines 14-21 of Algorithm 1 with the following lines:  14 ) fu reachI={I , ,  ..., I k ) E C k 15) ifs% = 2 ms% then hegin 16) LL- = LL- U { I } ; 17) cryo - 1.count2.

1.eount* 7 18) = I.u+-r.u-.



I.CO""t, 9  19) 20) 20-1) elsebegin 20-2) 20-3) 20-4) 20-5) end 21) end  if c% 2 mc% and U 2 mu then A R =  A R U ( I 1 ,  ..., 4 4 Obj(s%,c%,u)};  L B -  = (ms% * JDBJ - I.count$) * 1 . h ; if I . m n t :  < ms% * lDBl and $$&$  < mu then Lk = LI - (I} //by Strategy 1  The above procedure works as follows: For each candi- date k-itemset in C,, if it passes the support constraint then it is added to LL- (lines 15 and 16). If it also passes both the confidence and the utility constraint, an OOA rule built from I is added to AR (lines 17-20). Otherwise, when I passes the support constraint but violates either the confi- dence or the utility constraint, our pruning strategy is ap- plied (lines 20-1 to 20-5) to remove some OOA frequent itemsets from Lk from which no OOA tules will he pro- duced. The correctness of the OOApriori algorithm en- hanced with the pruning strategy follows immediately from Theorems 2 and 4. That is, 11,  ..., I ,  -i Obj(s%, c%, U) is an OOA rule if and only if it is in AR.

5 Experimental Evaluation  We show the effect of applying our pruning strategy by empirical experiments. We choose the widely used German Credit dataset from the UCI Machine Leam- ing Archive (ftp://ftp.ics.uci.edu/puh Imachine-leaming- databases/statloglgeman/). This dataset consists of loo0 records (each record represents a customer) with 21 at- tributes such as S t a ~ s ,  Duration, Credit-histo?, Purpose, Employment, etc. The last attribute Conclusion classifies a customer as good or badin terms of hidher credits. The rea- son we use this dataset in our experiment is that its attributes  are semantically easy to understand so that we can flexibly create different objectives from them to test our approach.

We build fourdatasets with different sizes from the 1000 records. DS1 consists of 600 records, OS, of 700 records, ..., and OS, of 900 records. The objective attributes are Liable-people, Foreign and Conclusion, and the objective Obj is defined as (Conclusion=good) A (Liable-people=2 V Foreign=no). That is, suppose we are interested in cus- tomers whose credit is good and who either are not foreign workers or have more than one person being liable to pro- vide maintenance for the credit account. A11 the remaining eighteen attributes are treated as non-objective attributes.

The utilities of the major classes of the objective are defined in Table 4 where we normalize the utilities into IO, 1001.

Table 4. Class utilities. I Conclusion [ Foreign [ Liable-people class+(DB) 1 good(70) I no(10) I 2 (20) class-(DB) I bad (70) I yes (IO) I l(20)  Let NI and Nz he the sizes of the two sets of OOA can- didate itemsets generated by OOApriori with and without applying Strategy I ,  respectively. We evaluate the effect of applying Strategy 1 to pruning OOA iternsets by demon- strating its ifemset reducrion mte defined by - . Fig- ure 1 shows our experimental results on the itemset reduc- tion rates where we use different minimum utilities while keeping the minimum support and minimum confidence un- changed. The results strongly demonstrate that applying our pruning strategy can greatly improve the efficiency of the OOApriori algorithm. On average, they pruned 8% - 9% of the candidate itemsets during the mining process. Fig- ure 2 further demonstrates the effectiveness of the pruning strategy, where we use the same minimum confidence and minimum utility while letting the minimum support vary.

1 2 5 10 15  Minimum Wliiies  Figure 1. The itemset reduction rates against minimum utilities.

ftp://ftp.ics.uci.edu/puh   0.50% O.M)% 0.70% 0.80% 0.90%  Minimum Supports  Figure 2. The itemset reduction rates against minimum supports.

6 Conclusions  We have developed a new approach to modeling asso- ciation patterns. OOA mining discovers patterns that are explicitly relating to a given objective that a user wants to achieve or is interested in and its utility. As a result, all OOA rules derived from a dataset by OOA mining are use- ful because applying them would increase business utility of an enterprise. This shows a significant difference from traditional association mining.

We developed an algorithm for mining OOA frequent patterns and rules. The algorithm is an enhancement to Apriori with specific mechanisms for handling objective utility. Since the utility constraint is neither anti-monotone nor monotone nor succinct nor convertible, finding effective pruning strategies is of great significance. We developed a novel pruning strategy for mining OOA rules by combining the support and utility constraints. As far as we can deter- mine, no similar work has been reported in the literature.

Acknowledgement  Yi-Dong Shen is supported in part by Chinese National Natural Science Foundation, Trans-Century Training Pro- gram Foundation for the Talents by the Chinese Ministry of Education, and Foundations from Chinese Academy of Sci- ences. Qiang Yang thanks NSERC and IRIS-I11 program for their support.

