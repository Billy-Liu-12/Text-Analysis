Incremental FP-Growth Mining Strategy for Dynamic  Threshold Value and Database Based on MapReduce

Abstract?With the coming of the Big Data era, data mining has been confronted with new opportunities and challenges.

Some limitations are exposed when traditional association rule mining algorithms are used to deal with large-scale data.

In the Apriori algorithm, scanning the external storage repeatedly leads to high I/O load and brings about low performance. As for FP-Growth algorithm, the effectiveness is limited by internal memory size because mining process is on the base of large tree-form data structure. What?s more, although remarkable achievements have been scored, there are still problems in dynamic scenarios. The paper presents a parallelized incremental FP-Growth mining strategy based on MapReduce, which aims to process large-scale data. The proposed incremental algorithm realizes effective data mining when threshold value and original database change at the same time. This novel algorithm is implemented on Hadoop and shows great advantages according to the experimental results.

Keywords?Association rule mining, FP-Growth algorithm, threshold value, dynamic database, MapReduce.



I.   INTRODUCTION  Big data [1] refers to a collection of datasets which is so huge and complicated that it is infeasible to process by using traditional methods and available technologies. Even if some analytical approach can barely finish the work, it still takes a long time and the outcome might not be satisfactory. Data mining [2], using existing data to analyze the overall trend or predict a problem that may arise in the future, is undoubtedly the core area of big data research.

Association rule mining [3], one kind of data mining algorithms, becomes more and more popular these years. It intends to identify strong rules between no less than two items in database through different measures of interestingness. In a market analysis, association rules like ?the customers who buy beer are likely to get diapers? might be generated according to the processing results. And these rules could be really helpful in making market plans. In addition to this typical application, association rules are also employed in Web usage mining, intrusion detection and continuous production.

After years of study, association rule mining algorithms are well established and effective in general cases. However, when it comes to big data, related algorithms are not mature and need further research. In a practical situation, database is updated periodically and threshold value often changes with needs of mining. It is clearly inefficient that the whole mining process has to be restarted from the beginning every time new data is inserted into database or mining parameter is reset. Furthermore, to deal with the issues resulted from large-scale data, algorithm parallelization has become inevitable.

This paper presents a parallelized incremental FP-Growth (PIFP-Growth) mining strategy. The proposed algorithm successfully solves the incremental issue brought by the dynamic threshold value and database at the same time, which avoids repeated computation. This parallel mining strategy based on MapReduce framework is implemented on Apache Hadoop [13]. The experimental results have proved the effectiveness and advantages of PIFP-Growth.

This paper is organized as follows: Section 2 reviews the related research literature on FP-Growth algorithm and parallelization; Section 3 presents the proposed PIFP-Growth algorithm; Section 4 shows experimental and analysis results.

Section 5 concludes the paper and discusses some future work.



II.   RELATED WORK Association rule mining algorithms can be classified into  two types: Apriori-based algorithms, and tree-based algorithms [5]. Agrawal et al. [4] first put forward the problem of mining the association rules from the customer and transaction databases in 1993 and then proposed the Apriori algorithm in the following year. In 2000, FP-Growth algorithm [4] solved the ?scan? problem by building FP-tree in memory to mine the frequent itemsets.

For the Apriori, there are 2 serious drawbacks: repetitive scanning and excessive candidate itemsets. In fact, the performance could be even worse in reality because of the complex computing environment. By contrast, FP-Growth search for the frequent itemsets in FP-tree, which is a compact data structure built in memory and only needs a 2-time scanning.

Thus, it is obvious that FP-Growth could do much better with a          relatively high-powered computer or in a distributed environment.

In addition to the algorithms mentioned above, incremental mining is another hotspot. Cheung et al. [7] raised FUP and FUP2 in 1996. The association rules would be updated when datasets were added or deleted. The UWEP proposed by Yan et al. [6] was based on the Apriori. The early pruning techniques of UWEP greatly reduced the number of candidate itemsets, thus enhancing the efficiency of FUP. Zhu et al. [8] came up with an effective FP-tree based incremental algorithm, named FIUA and FIUA2. FIUA and FIUA2 could only be applied on the condition of dynamic threshold value and updated database respectively. Both of them failed to take into consideration the fact that several mining conditions were likely to change simultaneously in reality. Furthermore, most tree-based incremental algorithms still had fatal shortcomings like memory dependency. To be specific, the step of producing FP-tree and other temporary trees might cause insufficient memory when processing mass data. Besides, the recursive nature of FP-Growth algorithm also brought lower CPU utilization [9].

Accordingly, researchers at home and abroad have made some achievements in this respect, like parallel FP-tree algorithm based on clustered system [10] and load balanced parallel FP-Growth algorithm [11]. Moreover, MapReduce becomes more widely used to realize parallel algorithm and is a key model of processing large-scale data within a distributed environment. Matteo Riondato et al. [14] proposed PARMA, which could generate approximate association rules in MapReduce. Pradeepa et al. [15] raised a parallelized strategy for the Apriori. In 2008, Li et al. [12], a researcher of Google, proposed the famous PFP (Parallel FP-Growth) algorithm under MapReduce framework. The only drawback was that PFP could not solve the incremental problem.

In view of absolute predominance of MapReduce model in big data area, PIFP-Growth makes further improvements on existing PFP algorithm. The novel algorithm not only solves incremental issue but also avoids memory dependency through parallelization. This algorithm was implemented on Hadoop.

And in the experiment, PIFP-Growth reached higher execution speed than the other two algorithms.



III. PARALLELIED INCREMENTAL FP-GROWTH ALGORITHM  A. Basic FP-Growth Let  1 2{ , ,..., }= nI i i i  be a set of the items, and D  is regarded as a transaction database. A transaction T  containing one or more items is a subset of I , where  iT I?  (1 i n? ? ). Assume that X is an itemset, transaction T  contains X  if and only if X T? . Support number, denoted by ( )count X  , refers to the appearing times of X  in the database D . Correspondingly, support degree is described as the percentage of X  included in D  and symbolized as ( )support X . If support number or support degree of X  is no less than a given minimum support number ,  or minimum support degree s , namely threshold value, X  is called frequent itemset, or large itemset, otherwise non-frequent itemset. The relationship between ( )count X  [3] and  ( )support X is: ( )count X = ( )support X ? D  Where, D  represents the number of the transactions in the database D .

Association rules can be denoted as X Y , where X I? , Y I?  and X Y? = ? . Supposing that s % of the transactions in D  contain X Y? , the support degree equals to s %. And likewise, the confidence degree [4] turns out to be a conditional probability, that is:  ( ) ( )= ?support X Y p X Y ( ) ( | )=confidence X Y p Y X  The algorithm flow can be summarized as follows: Step 1. Mining all the frequent itemsets according to  minimum support degree s , namely threshold value; Step 2. Generating association rules according to the  minimum confidence degree.

Most researches about association rule mining focuse on the  first step, because of its crucial role in the algorithm efficiency.

FP-Growth algorithm adopts divide-and-conquer strategy.

During the second scanning period, transactions are compressed into an FP-tree. Meanwhile, the relationships in transactions among these items remain unchanged in the tree. After that, FP-tree would be decomposed into conditional trees, which are used to mine frequent itemsets.

The essential process of building FP-tree can described as follows:  Firstly, the database should be scanned for one time to produce a support number list of all the items. The second step is to create the root node, denoted by null. The transactions that sorted in descending order with infrequent items removed would be compressed into an FP- tree. Each path on the tree represents a transaction with a node corresponding to an item.

The step of recursive call has FP-tree grow and realize the rule mining. Specific details are presented in [16].

B. PIFP-Growth Assuming that s  is the original minimum support degree,  and s?  denotes new support threshold value. New datasets inserted into database D  is denoted by d , where d  stands for the total number of transactions in d . The updating issue is to mine the frequent itemsets in changed database under support threshold value s? .

The problem can be divided into two parts: updating FP-tree and mining frequent itemsets. In the former part, we firstly decide whether it is necessary to rescan database D or d  to update the FP-tree.

Let L  represents the frequent itemsets, and 1L  be the set of frequent items under the threshold value s . To begin with, let's solve the problem of threshold value and leave the database  intact. When s  changes into s? , we have L?  and 1L ?  instead of L  and 1L  here. It is obvious that if s s?< , frequent itemsets L?         can be easily calculated. Although some original frequent  itemsets may no longer meet new threshold value, 1L ? is included in 1L and no new node needs to be added on the FP-tree.

However, it is relatively complicated, when it comes to s s?> . As mentioned, all the frequent itemsets are mined from FP-tree. If s s?>  and database D  has no change, then 1 1L L ?? , L L?? . So when threshold value becomes smaller, the FP-tree needs to be updated by adding new nodes.

Frequent list offers an easy way to judge whether there is a need to scan database D  again due to recorded support numbers.

Note that in PIFP-Growth algorithm, infrequent items are kept on the list for re-mining.

Following are important properties and theorems, which can be easily derived and proved.

Property 1. Supposing that the frequent pattern tree of threshold value s  and s?  are denoted by ?FP tree  and  '?FP tree  respectively, then all the items in 1L?  correspond to  the leaf nodes or prefix nodes of them, where 1 1 1L L L? ?= ? .

Property 2. Assuming that 1n L= , 1n L ?? = ,  1 1L L?? = ? , then 1L n n? ?= ? .

We can learn that it?s feasible to convert original FP tree?  to new FP tree??  by adding leaf nodes according to Property 1 and 2. Because each item from 1L?  can be mapped to new nodes in FP tree? .

Now let?s take new inserted datasets d  into consideration and keep threshold value s?  unchanged.

Definition 1. If ( ) sDsupport X ?  and ( ) sDdsupport X ? , X is a powerful frequent itemset of D . DPL  and dPL  refer to the powerful frequent itemsets of D  and d respectively.

Theorem 2. If  DdL  represents the frequent item sets of database D d? , then Dd DP dPL L L= ? .

Theorem 3. If X  is a powerful frequent itemset of d , then any subset of X  is a powerful frequent itemset of d . If X  is not a powerful frequent itemset of d , then any superset of X  is not a powerful frequent itemset of d .

On the basis of Definition 1, the database d  only needs to be scanned for one time so as to update the frequent list in D .

And at the same time powerful frequent itemsets of DPL can be obtained as well.

Finding new frequent itemsets of ?D d , requires heavier computation. According to Theorem 2, DdL is the union set of  DPL and dPL . And dPL  can be classified into two kinds: new frequent itemsets in D d? but infrequent in D , new frequent itemsets in d  which haven?t appeared in D . In tree updating part of PIFP-Growth, d  only needs to be scanned for one time in the best case. And compared with traditional algorithm in dynamic scenario, even for the worst case, it saves one scanning time of D , which reduces I/O load to some extent and raise the efficiency. On the other hand, to obtain the powerful frequent  itemsets of d , the thought of Apriori is borrowed to acquire frequent (1 )i item i k? ? ? .

On the basis of the structure of PIFP-Growth algorithm and MapReduce model, four MapReduce phases are used in the whole process. Fig. 1 depicts the 7 stages of PIFP-Growth.

Stage 1: InputSplit, a procedure defined by Google, splits the database into small chunks; these chunks are sent to Mapper and Reducer to finish the parallel counting of support number; the counting results are integrated into a frequent list and items are sorted in descending order; all the items on the frequent list are divided into groups. To give an example, if minimum support number is 3, we have a group list{ {group1: (f:5), (p:4), (e:3) }, {group2: (a:3), (c:3), (b:3) }, where total number of groups can be set manually. Note that: infrequent items including their support number are retained but they are not grouped.

Stage 2: It takes one MapReduce pass to acquire the local frequent itemsets. Mapper reads the group list and assign the related transactions to different computer node according to the group nodes; then each Reducer builds their own local FP-tree; during the recursive progress, the frequent itemsets are be extracted. To continue the previous example, following are some transactions in database.

TABLE I.  EXAMPLE TRANSACTIONS  ID Transaction  001 { f, p, e, a, c, b, m}  002  { f, c, b, y }  003 { f, p, a, c, y, m }  004 { f, p, e, a, b, t }  005 { f, p, e }   Based on PFP algorithm in [12], group 1 only receives  itemsets consist of f, p, e, while group 2 receives the transactions related to the items in both groups. To be specific, Group 1 receives { f, p, e }, { f }, { f, p }, { f, p, e}, { f, p, e}, and group 2 receives { f, p, e, a, c, b }, { f, c, b }, {f, p, a, c }, { f, p, e, a, b }, { f, p, e}. It is unnecessary to process the transactions like { f, p, e, a, c, b } in group 1, because they have already been mined in group 2. Besides, procedures of FP-tree building and mining are similar to traditional FP-Growth, which run on a single computer node.

Stage 3: The integration stage is to combine the local frequent itemsets mined. The frequent itemsets are stored in heap memory and sorted in descending order according to support number; then the intact frequent itemsets of database D  are produced.

Stage 4: During this stage, d is inserted into database and the threshold value is reset as s? . If there exist new items on the frequent list equal or greater than s? , these items have to be grouped as well. In the example, a new group {group3: (m:2), (y:2)}} is added on the group list. Generally speaking, each group has the same number of items to process except the last group.

Database DStage 1  Mapper  Reducer  Group list  Mapper  Reducer  Stage 3 Frequent itemsets of D  Stage 2  Frequent itemsets of D FP-trees of D  New threshold value s New database d  Stage 4  Temporary Group list  Mapper  Reducer  Stage 5  Updated group list  Mapper  Reducer  Stage 7 Updated frequent itemsets  Stage 6   Fig. 1. Flow Chart of PIFP-Growth Algorithm   Stage 5: In this stage, new inserted datasets d  is taken into  consideration. The datasets go through a MapReduce pass like stage 1. Similarly, database d  is divided into tiny parts, and also the itemsets are sorted after parallel counting. After that, frequent list has to be updated. And likewise, new frequent items have to be grouped. Note that: sequence of original frequent items remains unchanged. New items are added to the  tail of list. Some items, becoming infrequent, have no need to be eliminated so as to guarantee the efficiency.

Stage 6: According to the new frequent list, database D and d  are likely to be rescanned. Mapper assigns transactions related to new frequent items to corresponding group; each Reducer updates their own local FP-tree; new frequent items are mined in updated FP-trees. The key step is to eliminate some powerful frequent itemsets to save the mining time. Detailed steps are available in the following algorithm description.

Stage 7: The last step is to integrate all the frequent itemsets from stage 6 as the final result.

Given the limited space available, detailed algorithms, from stage 1 to stage 3, won?t be discussed here. Some key parts of the implemental algorithms are introduced as follows:  Procedure PIFP-Growth ( FP tree? , L, s? , s , d , D )  1 1 1L L L? ?= ? ; // new frequent items of D  and s? if( 1L? ? ? ) update group list; update frequent list; // scan d ;  DPL ; // get powerful frequent itemsets of D and s?  1dPL , 1DPL ; // powerful frequent items of d  and s , D // and s  1 1 1 1DP dPL L L L? ? ?= + ? ; // new frequent items of d  and s?  if( 1L? ? ? ? ) update group list;  if ( 1L? ? ?  and 1L? ? = ? ) // update local FP-trees FP tree?? = conFPtree( 1L? , FP tree? , D );  else if( 1L? ? ?  and 1L? ? ? ? )  FP tree?? = conFPtree( 1 1L L? ? ?? , FP tree? , D d? );  else if( 1L? = ?  and 1L? ? ? ? )  FP tree?? = conFPtree( 1L? ? , FP tree? , d ); else {  DdL L= ; FP tree FP tree?? = ? ; return; }  2 1_ ( )d dPc Apriori gen L= ; // candidate frequent itemsets for (k=2; dkc ? ? ; k++) { k dk kc c L? = ? ;  if( kc? ? ? ) { for each 1 2{ , ,..., }t kc? ? ? ?= ? { calculate support number; }  } { | ( ) '}dPk dkL c c support c s= ? ? ;   ( 1) _ ( )d k dPkc Apriori gen L+ = ; { | ( ) '}k kL c c support c s? ?= ? ? ; // new frequent itemsets  }         Dd DP kL L L?= ? ; }  Procedure conFPtree( L?  ,Tree, database) // update FP-tree  assign transactions T L database?= ? to groups; for each T  in group { // transaction T  f fL L L?? = ? ; // update local frequent list  sort( fL ? ); // descending sort new items nNode T L?= ? ;  insert nNode into Tree; // insert new nodes } }

IV.  EXPERIMENTAL RESULTS  In this section, PIFP-Growth and two main association rule mining algorithm, including Apriori, FP-Growth, were compared and analyzed through experiments.

All the experiments were performed on a 3.10GHz Intel i3-200 dual-core PC with 4GB main memory. The programs are written in Java and implemented on Hadoop.

TABLE II. shows 3 classic datasets that are generally used in test for association rule mining.

TABLE II.  EXPERIMENTAL DATASETS  Dataset Size(MB) Transactions Items Database d  Chess 0.34 3196 75 200  T10I4D1 00K  3.93 100,000 870 8000  kosarak   30.50 990,002 41,270 90000   We divided the existing datasets, denoted by Transactions,  into two parts: the original database D  and new inserted one d, namely database d . The new minimum support degrees are showed below x axis, which are 0.1 greater than the original threshold value. Fig. 2. Experiment of Chess, Fig. 3.

Experiment of T10I4D100K and Fig. 4. Experiment of kosarak give the running time of Apriori, FP-Growth and PIFP-Growth.

We can learn from the results that PIFP-Growth cost the least amount of time, comparing to the other two algorithms.

When data volume is small, differences is not obvious, the trend of three lines goes similarly.

Nevertheless, as the amount of data increases, PIFP-Growth shows great advantage in running time over other two algorithms, especially when threshold value is low.

Fig. 2. Experiment of Chess     Fig. 3. Experiment of T10I4D100K     Fig. 4. Experiment of kosarak

V.   CONCLUSION  Traditional data mining methods and algorithms have limitations when dealing with big data. For instance, Apriori algorithm needs to scan the data from external storage repeatedly so as to obtain the frequent itemsets, which brings heavy I/O load with low performance. Moreover, existing incremental algorithms cannot be applied in situations when both threshold value and database change. This paper presents an incremental FP-Growth mining strategy, which is parallelized under the MapReduce framework. Experimental results indicate that this improved algorithm is effective in reducing time of duplicated work.

In order to achieve higher speedup, more machines can be added into the whole distributed system. And load imbalance issue exists, because the greater group numbers are, the more data corresponding computer nodes have to process. Besides,         the case of deleting datasets from existing database should be taken into the consideration, as well. In future work, further work would be done to optimize the algorithm.

ACKNOWLEDGEMENTS  The research work presented in this paper is partially supported by the Scientific Research Projects of STCSM (Grant No. 13JG0500100), the NSFC (Grant No. 61073090, 61034004 and 61173015), and the Fundamental Research Funds for the Central Universities.

REFERENCE [1] A. Kejariwal, Big Data Challenges: A Program Optimization  2012, pp. 202-207.

[2] Y. Ren, S Lv, and Q.Wang, The design of algorithm for data mining Communication Software and Networks, 2011, pp. 480-483.

[3] M.M. Mazid, A.B.M. Ali, and K. S. Tickle, A comparison between rule based and association rule mining algorithms, Network and System Security, 2009, pp. 452-455.

[4] R. Chang, and Z. Liu, An Improved Apriori Algorithm, International Conference on Electronics and Optoelectronics, 2011, pp. 221-224.

[5] S. Shah, N. C. Chauhan, and S. D. Bhanderi, Incremental Mining of Association Rules: A Survey, International Journal of Computer Science and Information Technologies, vol. 3(3), 2012, pp. 4071-4074.

[6] L. Sun, Y. Cai, J. Li, and J. Lv, An Efficient Algorithm for Updating Association Rules with Incremental Transactions and Minimum Support Changes Simultaneously, Third Global Congress on Intelligent Systems, 2012, pp. 166-171.

[7] S. Li, Y. Zhao, and X. Gu, Application of improved FUP algorithm on hardware product quality analysis system, Journal of Jilin University, vol. 42, 2012, pp. 251-254.

[8] Y. Zhu, and X. Wang, Novel incremental updating algorithm for mining association rules, Computer Engineering, vol. 28, 2012, pp. 25.

[9] A. Heinecke, and D. Pfluger, Emerging architectures enable to boost massively parallel data mining using adaptive sparse grids, International Journal of Parallel Programming, vol. 41, 2013, pp. 357-399.

[10] J.M. Kunkel, Simulating parallel programs on application and system level, Computer Science Research and Development, vol. 28, 2013, pp.

167-174.

[11] Z. Zeng , C. Yang, and Y. Tao, Research of load balance FP-growth algorithm in parallel, Computer Engineering and Applications, vol. 46, 2010, pp. 125-6, 229.

[12] H. Li, Y. Wang, D. Zhang, M. Zhang, and  E. Chang, PFP: Parallel FP-growth for query recommendation, Proceedings of the 2008 ACM Conference on Recommender Systems, 2008, pp. 107-114.

[13] S. Kurazumi, T. Tsumura, S. Saito, and H. Matsuo, Dynamic processing slots scheduling for I/O intensive jobs of Hadoop MapReduce, and Computing, 2012, pp. 288-292.

[14] M. Riondato, J. A. DeBrabant, R. Fonseca, and E. Upfal, PARMA: A Parallel Randomized Algorithm for Approximate Association Rules Mining in MapReduce, Proceedings of the 21st ACM international conference on Information and knowledge management, 2012, pp.

85-94.

[15] A. Pradeepa, and A. S. Thanamani, PARALLELIZED COMPRISING FOR  APRIORI ALGORITHM USING  MAPREDUCE FRAMEWORK, International Journal of Advanced Research in Computer and Communication Engineering, vol. 2(11), 2013, pp.

4365-4368.

[16] JW. Han, J. Pei and YW. Yin, Mining Frequent Patterns without Data, vol. 29(2), 2000, pp. 1-12.

