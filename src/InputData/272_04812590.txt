Mining of Frequent Itemsets from Streams of Uncertain Data

Abstract? Frequent itemset mining plays an essential role in the mining of various patterns and is in demand in many real- life applications. Hence, the mining of frequent itemsets has been the subject of numerous studies since its introduction. Generally, most of these studies find frequent itemsets from traditional transaction databases, in which the contents of each transaction? namely, items?are definitely known and precise. However, there are many real-life situations in which ones are uncertain about the contents of transactions. This calls for the mining of uncertain data. Moreover, due to advances in technology, a flood of precise or uncertain data can be produced in many situations. This calls for the mining of data streams. To deal with these situations, we propose two tree-based mining algorithms to efficiently find frequent itemsets from streams of uncertain data, where each item in the transactions in the streams is associated with an existential probability. Experimental results show the effectiveness of our algorithms in mining frequent itemsets from streams of uncertain data.



I. INTRODUCTION  Frequent itemset mining [2], [12], [13], [15], [18], [19] plays an essential role in the mining of various patterns (e.g., association rules, correlation, sequences, episodes, maximal patterns, closed patterns) and is in demand for many real-life applications. Hence, since its introduction [1], the problem of finding frequent itemsets from databases has been the subject of numerous studies. Many of the algorithms proposed in these studies are Apriori-based. They depend on a generate-and-test paradigm. In other words, they find frequent itemsets from the transaction database by first generating candidates and then checking their support (i.e., their occurrences) against the transaction database.

To improve efficiency of the mining process, Han et al. [10] proposed an alternative framework, namely a tree-based frame- work, for finding frequent itemsets. The algorithm they pro- posed in this framework constructs an extended prefix-tree structure, called Frequent Pattern tree (FP-tree), to capture the contents of the transaction database. Rather than employing the generate-and-test strategy of Apriori-based algorithms, such a tree-based algorithm focuses on frequent pattern growth? which is a restricted test-only approach (i.e., does not generate candidates, and only tests for support).

So far, we have discussed the mining of frequent itemsets from traditional static databases. Over the past decade, the automation of measurements and data collection has produced  tremendously huge amounts of data in many real-life ap- plication areas. The recent development and increasing use of a large number of sensors has added to this situation.

Consequently, these advances in technology have led to a flood of data. We are now drowning in streams of data but starving for knowledge. In order to be able to ?drink from a fire hose? (i.e., to make sense of the streams of data), algorithms for mining these dynamic streams are in demand. This calls for stream mining [6], [8], [9], [16], [20].

When comparing with mining from traditional static databases, mining from data streams is more challenging due to the following properties of data streams:  ? Property 1. Data streams are continuous and unbounded. To find frequent itemsets from data streams, we no longer have the luxury of performing multiple data scans. Once the streams flow through, we lose them. Hence, we need some techniques to capture the important contents of the streams (e.g., to capture recent data because users are usually more interested in recent data than older ones) and ensure that the captured data can fit into memory.

? Property 2. Data in the streams are not necessarily uni- formly distributed; their distributions are usually chang- ing with time. A currently infrequent itemset may become frequent in the future, and vice versa. So, we have to be careful not to prune infrequent itemsets too early; other- wise, we may not be able to get complete information such as frequencies of some itemsets (as it is impossible to recall those pruned itemsets).

To find frequent itemsets from data streams, several stream mining algorithms have been proposed over the past decade.

For example, Giannella et al. [9] proposed a tree-based algo- rithm called FP-streaming for mining streams of precise data.

A common characteristic among the aforementioned Apriori-based and tree-based algorithms is that they handle precise data, such as databases of market basket transactions, Web logs, and click streams. When mining precise data, users definitely know whether an item (or an event) is present in, or is absent from, a transaction in the static databases or dynamic data streams. However, there are situations in which users are uncertain about the presence or absence of some items or events [4], [5], [7], [14], [17], [21]. For example,   DOI 10.1109/ICDE.2009.157    DOI 10.1109/ICDE.2009.157     TABLE I  OUR PROPOSED ALGORITHMS VERSUS THEIR MOST RELEVANT ALGORITHMS  Mining static databases Mining dynamic data streams Apriori-based Tree-based (tree-based)  Mining precise data Apriori [2] FP-growth [10] FP-streaming [9] Mining uncertain data U-Apriori [4], [5] UF-growth [14], [17] Our proposed algorithms  a physician may highly suspect (but cannot guarantee) that a patient suffers from flu. The uncertainty of such suspicion can be expressed in terms of existential probability. So, in this uncertain database of patient records, each transaction ti represents a patient?s visit to a physician?s office. Each item within ti represents a potential disease, and is associated with an existential probability expressing the likelihood of a patient having that disease in ti. For instance, in ti, the patient has an 80% likelihood of having the flu, and a 60% likelihood of having a cold regardless of having the flu or not. With this notion, each item in a transaction ti in traditional databases containing precise data can be viewed as an item with a 100% likelihood of being present in ti.

In addition to the above example, there are many other real-life situations (e.g., environmental surveillance, quantita- tive economics/survey research) in which data are uncertain.

Hence, efficient algorithms for mining uncertain data [3], [4], [5], [11], [14], [17] are in demand. To mine frequent itemsets from uncertain data, Chui et al. [5] proposed an Apriori-based algorithm called U-Apriori. They also introduced a trimming strategy to reduce the number of candidates that need to be counted by U-Apriori. Recently, they [4] further proposed a decremental pruning technique to speed up the mining process.

However, as an Apriori-based algorithm, U-Apriori relies on the candidate generate-and-test paradigm.

Knowing that tree-based algorithms for mining precise data (e.g., FP-growth [10]) are usually faster than their Apriori- based counterparts (e.g., Apriori [2]), we previously proposed a tree-based algorithm called UF-growth [14] for mining uncertain data. Recently, we further proposed two improve- ments [17] to this UF-growth algorithm.

We observed that many existing uncertain data mining al- gorithms (including UF-growth) effectively handles uncertain data but not data streams. Moreover, many existing stream mining algorithms (e.g., FP-streaming) effectively handles data streams but not uncertain data. Hence, some natural questions to ask are: Can we handle streams of uncertain data? Given streams of uncertain data, how can we effectively capture their important contents (as data in these continuous and unbounded streams are not necessarily uniformly distributed, and each item in these uncertain streaming data is associated with its existential probability)? In this paper, we answer these ques- tions by designing and developing two tree-based algorithms for mining frequent itemsets from streams of uncertain data.

The key contributions of this work include the following:  ? the proposal of effective tree structures to capture the important contents of transactions in streams of uncertain data, and  ? the development of two efficient algorithms to mine  frequent itemsets from the transactions captured by these proposed tree structures.

Experimental results in Section V show (i) the effectiveness of our proposed tree structures in capturing essential contents from streams of uncertain data and (ii) the efficiency of our proposed algorithms in mining frequent itemsets from streams of uncertain data. Table I summarizes the salient differences between our proposed work and its most relevant work.

This paper is organized as follows. The next section de- scribes related work and background. In Sections III and IV, we introduce our two algorithms for mining frequent itemsets from streams of uncertain data. Section V shows experimental results. Finally, conclusions are presented in Section VI.



II. RELATED WORK AND BACKGROUND  In this section, we describe some related work and provide some background materials that are relevant to the remainder of this paper.

A. Mining Databases of Uncertain Data  A key difference between precise and uncertain data is that each transaction of the latter contains items and their existential probabilities. The existential probability P (x, ti) of an item x in a transaction ti indicates the likelihood of x being present in ti. Using the ?possible world? interpretation of uncertain data [5], [7], [14], [17], there are two possible worlds for an item x and a transaction ti: (i) the possible world W1 where x ? ti and (ii) the possible world W2 where x ?? ti. Although it is uncertain which of these two worlds be the true world, the probability of W1 be the true world is P (x, ti) and that of W2 is 1 ? P (x, ti).

To a further extent, there are usually more than one transac- tion in a transaction database TDB. For instance, for an item x and a TDB consisting of two transactions t1 and t2, there are four possible worlds: (i) W1 where x is in both t1 and t2, (ii) W2 where x is in t1 but not t2, (iii) W3 where x is in t2 but not t1, and (iv) W4 where x is neither in t1 nor in t2. Let prob(Wj) denote the probability of Wj to be the true world. Then, prob(W1) = P (x, t1) ? P (x, t2), prob(W2) = P (x, t1)?[1?P (x, t2)], prob(W3) = [1?P (x, t1)]?P (x, t2), and prob(W4) = [1 ? P (x, t1)] ? [1 ? P (x, t2)].

Similarly, there are usually more than one domain item in each transaction in a TDB. For instance, for two independent items x, y and a transaction ti, there are also four possible worlds: (i) W1 where both x, y ? ti, (ii) W2 where x ? ti but y ?? ti, (iii) W3 where x ?? ti but y ? ti, and (iv) W4 where both x, y ?? ti. Then, prob(W1) = P (x, ti) ? P (y, ti), prob(W2) = P (x, ti)? [1 ? P (y, ti)], prob(W3) =     [1 ? P (x, ti)] ? P (y, ti), and prob(W4) = [1 ? P (x, ti)] ? [1 ? P (y, ti)].

To generalize, there are many items in each of the n trans- actions in a transaction database TDB (where |TDB | = n).

Hence, the expected support of an itemset X in TDB can be computed by summing the support of X in possible world Wj (while taking in account the probability of Wj to be the true world) over all possible worlds:  expSup(X) = ?  j  [sup(X) in Wj ? prob(Wj)] , (1)  where sup(X) denotes the support of X . The probability of Wj to be the true world, denoted by prob(Wj), can be computed as follows: prob(Wj) =?n  i=1  (? x?ti in WjP (x, ti) ?  ? y ??ti in Wj[1 ? P (y, ti)]  ) .

Note that Equation (1) can be simplified to become the following:  expSup(X) = n?  i=1  (? x?X  P (x, ti)  ) . (2)  With this setting, an itemset X is considered frequent if its expected support equals or exceeds the user-specified support threshold minsup.

Inspired by the key modification made to the Apriori algo- rithm [2] by the U-Apriori algorithm [5] (i.e., incrementing the support values of candidate patterns by their expected support instead of the actual support), we previously proposed the UF-growth algorithm [14], [17] to find frequent itemsets from static databases of uncertain data. The algorithm consists of two key operations: (i) the construction of a UF-tree (in which each node stores an item, its expected support, and the number of occurrences of such expected support for such an item), and (ii) the mining of frequent itemsets from the UF-tree. Let us elaborate. To construct a UF-tree, the UF-growth algorithm scans the database TDB once and accumulates the expected support of each item. It finds all frequent items (i.e., items having expected support ? minsup). It sorts these frequent items in descending order of accumulated expected support.

The algorithm then scans TDB the second time and inserts each transaction of the static database of uncertain data into the UF-tree in a similar fashion as in the construction of an FP-tree except that the new transaction is merged with a child (or descendant) node of the root of the UF-tree (at the highest support level) only if the same item and the same expected support exist in both the transaction and the child (or descendant) nodes.

Once the UF-tree is constructed, the UF-growth algorithm recursively mines frequent patterns from this tree in a similar fashion as in the FP-growth algorithm except for the following:  ? When forming a UF-tree for the projected database of an itemset X , UF-growth keeps track of the expected support of X (instead of the actual support of X).

? When computing the expected support of any extension of an itemset X (say, X?{y} for some item y), UF-growth multiplies the expected support of y in a tree path by the expected support of X (instead of just copies the actual support of X to the actual support of X ? {y}).

While our previously proposed UF-growth algorithm finds frequent itemsets from static databases of uncertain data, it does not handle dynamic data streams.

B. Mining Streams of Precise Data  Recall that (i) streams are continuous as well as unbounded and (ii) data in the streams are not necessarily uniformly distributed. To mine frequent itemsets from streams, Giannella et al. [9] designed the FP-streaming algorithm. Key ideas of their algorithm can be described as follows.

Given an incoming batch of transactions in a data stream, the first step of FP-streaming is to call the FP-growth al- gorithm [10] with a threshold that is lower than the usual minimum support threshold minsup to find ?frequent? itemsets.

Let us call this lower threshold preMinsup. Then, an itemset is ?frequent? if its actual support is no less than preMinsup.

Note that, although we are interested in truly frequent itemsets (i.e., itemsets with actual support ? minsup > preMinsup), FP-streaming uses preMinsup in attempt to avoid pruning an itemset too early. An itemset X having preMinsup ? sup(X) < minsup is currently infrequent but may become frequent later; so, X is not pruned by FP-streaming.

Once FP-growth found the ?frequent? itemsets, the second step of the FP-streaming algorithm is to store and maintain these itemsets in another tree structure called FP-stream. Key differences between an FP-tree and an FP-stream structure include the following. First, each path in an FP-tree represents a transaction, but each path in an FP-stream structure repre- sents a ?frequent? itemset. Second, each node in an FP-tree contains one support value, whereas each node in an FP-stream structure contains a natural or logarithmic tilted-time window table (containing multiple support values, one for each batch of transactions). Since users are often interested in recent data than older data, the FP-stream structure captures only a few recent batches of streaming transactions. As a new batch of transactions flows in, the window slides and the support values of each node shift as well.

It is important to note that, while the FP-streaming algo- rithm finds frequent itemsets from dynamic streams of precise data, it does not handle uncertain data.



III. OUR APPROXIMATE ALGORITHM FOR MINING STREAMS OF UNCERTAIN DATA  Observed from the previous section that, with respect to the stream mining of uncertain data, the aforementioned algorithms fall short in different aspects. For instance, the UF-growth algorithm [14], [17] described in Section II-A deals with (static databases of) uncertain data but not data streams, whereas the FP-streaming algorithm [9] described in Section II-B deals with data streams (of precise data) but not uncertain data.

With this observation in mind, we propose two algorithms in this paper to fill the gap?specifically, we propose two tree- based algorithms for mining frequent itemsets from streams of uncertain data. In this section, we introduce our approximate     algorithm?called UF-streaming?for mining frequent item- sets from streams of uncertain data. This algorithm can be described as follows. When the first batch of transactions in a stream of uncertain data flows in, our proposed algorithm applies UF-growth to this batch. (Recall from Section II-A that UF-growth is an efficient tree-based algorithm for mining frequent itemsets from static databases of uncertain data.) Since data in the streams are not necessarily uniformly dis- tributed, an itemset X that is infrequent in the current batch may be frequent in subsequent batches in the current sliding window (which may make X a frequent itemset in the current window). As data streams are continuous and unbounded, we can no longer go back to the current batch and reconsider X once we moved to subsequent batches. Consequently, if the expected support of X is currently slightly lower than minsup, we better keep X . Otherwise, we may miss X (a possibly frequent itemset). Hence, we apply UF-growth with preMinsup (a threshold that is lower than the usual minimum support threshold minsup) to find ?frequent? patterns. An itemset is ?frequent? if its (expected) support is no less than preMinsup. Note that, although we are interested in truly frequent itemsets (i.e., itemsets having expected support ? minsup > preMinsup), we use preMinsup to avoid pruning an itemset too early. An itemset X having preMinsup ? expSup(X) < minsup is currently infrequent but may become frequent later; so, X is not pruned.

After finding the ?frequent? itemsets, our proposed algo- rithm stores and maintains these itemsets in another tree struc- ture called UF-stream. In our UF-stream structure, each path represents a ?frequent? itemset. Common items in itemsets share the tree path in a similar fashion as in the FP-tree or the UF-tree. Each node in this UF-stream structure contains (i) the item and (ii) a window table (containing a list of w expected support values, one for each batch of transactions).

As users are often interested in recent data than older data, our UF-stream structure focuses on capturing only w most recent batches of transactions in the stream. So, when a new batch of transactions flows in, the window slides and the expected support values of each node in the UF-stream structure shift.

We repeat the above mining process when each of the sub- sequent batches of transactions in the stream of uncertain data arrives. In other words, our algorithm first calls UF-growth to compute ?frequent? patterns from a new batch of streaming uncertain data. Our algorithm then stores those ?frequent? patterns in the UF-stream structure, slides the window, and shifts the w expected support values of each node in the UF-stream structure so as to ensure that it always captures the contents of the w most recent batches of transactions in the stream of uncertain data. To gain a better understanding of this algorithm, let us consider Example 1.

Example 1: Consider the following stream of uncertain data:  Batch Transactions Contents t1 {a:0.9, d:0.8, e:0.7, f :0.2}  first t2 {a:0.9, c:0.7, d:0.7, e:0.6} t3 {b:1.0, c:0.9}  Batch Transactions Contents t4 {b:1.0, c:0.9, d:0.3}  second t5 {a:0.9, d:0.8} t6 {b:1.0, d:0.7, e:0.1} t7 {a:0.9, d:0.8}  third t8 {b:1.0, c:0.9, d:0.3} t9 {a:0.9, d:0.8, e:0.7}  Here, each transaction contains items and their corresponding existential probabilities (e.g., the existential probability of item a in transaction t1 is 0.9).

Let the user-specified support threshold minsup be set to 1.2.

Our proposed UF-streaming algorithm applies UF-growth to the first batch of transactions in the stream of uncertain data us- ing a preMinsup lower than minsup (say, preMinsup=0.9). The UF-growth algorithm constructs a UF-tree as follows. First, it scans the current batch and accumulates the expected support of each item. It finds all ?frequent? items a, b, c, d and e (i.e., the expected support of each of these items ? preMinsup), with their corresponding accumulated expected support of 1.8, 1.0, 1.6, 1.5 and 1.3. Item f having accumulated expected support of 0.2 < preMinsup is removed because it is infrequent in the current batch (the first batch).

When scanning the current batch of transactions, our pro- posed algorithm also inserts each transaction into the UF-tree.

The algorithm first inserts the contents of t1 into the tree, and results in a tree branch ?(a:0.9):1, (d:0.8):1, (e:0.7):1?.

It then inserts the contents of t2 into the UF-tree. Since the expected support of a in t2 is the same as the expected support of a in an existing branch (i.e., the branch for t1), the tree node (a:0.9) can be shared. So, UF-growth increments the occurrence count for the node (a:0.9) to 2, and adds the remainder of t2 as a child of the node (a:0.9):2. As a result, we get the tree branch ?(a:0.9):2, (c:0.7):1, (d:0.7):1, (e:0.6):1?.

Afterwards, UF-growth inserts the contents of t3 as a new branch ?(b:1.0):1, c:0.9):1? because the node (b:1.0):1 cannot be shared with the node (a:0.9):2. Consequently, at the end of the tree construction process, we get the UF-tree shown in Fig. 1(a) capturing the important contents of the first batch of uncertain data.

Once the UF-tree is constructed for the first batch, our proposed algorithm then recursively mines ?frequent? itemsets from this tree with preMinsup as follows. It starts with item e (with expSup({e})=1.3). UF-growth extracts from two tree paths?namely, (i) ?(a:0.9), (c:0.7), (d:0.7)? that occurs once with (e:0.6) and (ii) ?(a:0.9), (d:0.8)? that occurs once with (e:0.7)?and forms the {e}-projected database.

Then, expSup({a, e}) = (1 ? 0.9 ? 0.6) + (1 ? 0.9 ? 0.7) = 1.17 ? preMinsup, and expSup({d, e}) = (1 ? 0.7 ? 0.6) + (1 ? 0.8 ? 0.7) = 0.98 ? preMinsup. Note that these two itemsets {a, e} and {d, e} are ?frequent? (i.e., possibly fre- quent) because minsup > expSup({a, e}), expSup({d, e}) ? preMinsup. Similarly, UF-growth extracts appropriate paths, forms the {d}-projected database, and finds expSup({a, d}) = (1 ? 0.9 ? 0.7) + (1 ? 0.9 ? 0.8) = 1.35. Note that the itemset {a, d} is truly frequent because expSup({a, d}) ? minsup > preMinsup.

As a result, in the first step of our proposed algorithm,     (c:0.9):1  (b:1.0):1(a:0.9):2  (e:0.7):1  (d:0.8):1  (e:0.6):1  (d:0.7):1  (c:0.7):1  (a) The UF-tree for transactions in the first batch  d[1.35,0]  a[1.8,0]  e[1.17,0]  b[1.0,0] c[1.6,0] d[1.5,0] e[1.3,0]  e[0.98,0]  (b) UF-stream for ?frequent? itemsets found in the 1st batch  d[0,1.0]  a[1.8,0.9]  e[1.17,0] e[0.98,0]  e[1.3,0]d[1.5,1.8]c[1.6,0.9]b[1.0,2.0]  d[1.35,0]  (c) UF-stream for ?frequent? itemsets found in the 1st & 2nd batches  d[1.0,0]  a[0.9,1.8]  e[0,0] e[0,0]  e[0,0]d[1.8,1.9]c[0.9,0.9]b[2.0,1.0]  d[0,1.44]  (d) UF-stream for ?frequent? itemsets found in the 2nd & 3rd batches  Fig. 1. The UF-tree and UF-stream structures for Example 1  UF-growth found ?frequent? itemsets {a}, {a, d}, {a, e}, {b}, {c}, {d}, {d, e} and {e} (with their corresponding expected support of 1.8, 1.35, 1.17, 1.0, 1.6, 1.5, 0.98 and 1.3).

Let the window size w=2 batches. In the second step of our proposed algorithm, we store these itemsets in a UF-stream structure as shown in Fig. 1(b). The node a[1.8,0] in this UF-stream structure represents the ?frequent? itemset {a} with an expected support of 1.8 in the first batch of the stream of uncertain data. Similarly, the node d[1.35,0] on the leftmost path ?a[1.8,0], d[1.35,0]? represents the ?frequent? itemset {a, d} with an expected support of 1.35 in the first batch of the stream of uncertain data.

Afterwards, when the second batch of data flows in, our proposed algorithm applies the same mining procedure to this second batch. Specifically, it first constructs a new UF-tree, from which ?frequent? itemsets {a}, {b}, {b, d}, {c} and {d} (with their corresponding expected support values of 0.9, 2.0, 1.0, 0.9 and 1.8) can be found. It then updates the existing UF-stream structure by storing these itemsets in it.

The resulting UF-stream structure, as shown in Fig. 1(c), consists of nine nodes (due to the addition of the node d[0,1.0] representing the new ?frequent? itemset {b, d} that has no expected support in the first batch and an expected support of 1.0 in the second batch).

Similarly, when the third batch flows in, our proposed algorithm first finds ?frequent? itemsets {a}, {a, d}, {b}, {c} and {d} (with their corresponding expected support values of 1.8, 1.44, 1.0, 0.9 and 1.9). It then slides the window (of size w=2 batches) and shifts the expected support values of  each node in the UF-stream structure. The resulting UF-stream structure, as shown in Fig. 1(d), captures the expected support values for ?frequent? itemsets found in the second and third batches. (Note that nodes with zero expected support, such as e[0,0], can be removed.)

IV. OUR EXACT ALGORITHM FOR MINING STREAMS OF UNCERTAIN DATA  In the previous section, we proposed the UF-streaming algo- rithm for mining frequent itemsets from streams of uncertain data. However, there are some potential problems associated with such an approximate algorithm. First, the algorithm calls UF-growth with preMinsup < minsup. As a result, it finds ?frequent? itemsets (i.e., itemsets with expected support ? preMinsup). So, some of these itemsets are not truly frequent (e.g., some may have expected support < minsup). Conse- quently, to find truly frequent itemsets, one needs to apply a post-processing step. Moreover, depending on the value of preMinsup, the algorithm may miss some frequent itemsets (especially, when preMinsup were set too high, say to close to minsup). Like minsup, it is not easy to find an appropriate value for preMinsup. Second, the algorithm requires an extra data structure (the UF-stream structure) to store the mined itemsets. Third, the algorithm uses an ?immediate? mode for mining. As a result, lots of computation could be wasted, especially when many batches flow in before the user requests for the mining results (frequent itemsets). Let us consider an example with a sliding window of size w=2 batches. If the user requests the mining results at the end of the 50th batch, then the algorithm has already computed ?frequent? itemsets for each of the 50 batches, out of which only those from the last two batches are needed for the mining results. Computation on the first 48 batches was wasted.

Hence, in this section, we propose another algorithm? called SUF-growth?that further improves our UF-streaming algorithm (by avoiding the aforementioned potential problems) for mining frequent itemsets from streams of uncertain data.

The following are some advantages for using this SUF-growth algorithm. First, as an exact algorithm, SUF-growth re- turns to the user all and only those truly frequent itemsets (i.e., those with expected support ? minsup). Hence, unlike UF-streaming, the SUF-growth algorithm does not produce false positives or false negatives (with respect to minsup).

As the SUF-growth algorithm uses only minsup (i.e., it does not use preMinsup), we do not need to worry about finding an appropriate value for preMinsup. Second, unlike UF-streaming, our SUF-growth algorithm does not require the UF-stream structure to store the mined itemsets. Third, SUF-growth uses a ?delayed? mode for mining. As a result, unnecessary computation could be reduced. Let us revisit the aforementioned example with a sliding window of size w=2 batches. If the user requests the mining results at the end of the 50th batch, then SUF-growth finds truly frequent itemsets only from the last two batches (i.e., the 49th and 50th batches). In other words, mining was not conducted     on the first 48 batches. Unnecessary computation (on these batches) was avoided.

In the remainder of this section, let us explain how our proposed SUF-growth algorithm find frequent itemsets from streams of uncertain data using a new tree structure called SUF-tree. The key idea of our proposed algorithm can be described as follows. We first construct a SUF-tree, and then extract relevant paths from this SUF-tree (which is a global tree) to recursively form smaller UF-trees for projected databases. Due to the dynamic nature and Property 2 of data streams, expected support of items is continuously affected by the arrival of new batches (and the removal of the contents of older batches). Arranging items in frequency-dependent order in the SUF-tree may lead to swapping?which, in turn, can cause merging and splitting?of tree nodes when the global frequencies of items change. Hence, in the SUF-tree, items are arranged according to some canonical order (e.g., lexicographic order), which can be specified by the user prior to the construction of the SUF-tree or the mining process.

Consequently, the SUF-tree can be constructed using only one scan of the streams of uncertain data, and the resulting SUF-tree captures the contents of the streams. Moreover, the SUF-tree preserves the usual tree properties:  ? The occurrence count of a node is at least as high as the sum of occurrence counts of its children.

? The ordering of items is unaffected by the continuous changes in the expected support values of items.

To record and update the information at each tree node, the SUF-tree keeps a list of occurrence counts (instead of only one occurrence count as in the UF-tree). Each count in this list captures the occurrence of the item in each of the corresponding batch. By doing so, when the window slides (i.e., when new batches arrive and older batches are deleted), information can be updated easily. Specifically, whenever a new batch of transactions flows in, the occurrence count of the node (x:expSup(x)) in the new batch is appended to the list for such a node. In other words, the last entry of the list for such a node then shows the occurrence of such a node in the current batch. Afterwards, when the next batch of transactions flows in, the contents in the list are shifted forward. The last entry shifts and becomes the second-last entry; this leaves room (the last entry) for the newest batch. At the same time, the occurrence count corresponding to the oldest batch in the window is discarded. This has the same effect as deleting from the window those transactions in the oldest batch.

Theoretically, to effectively shift the list of occurrence counts, one may need to traverse all the tree nodes and shift the list of entries in each node. Practically, we do not need to do so. Instead, we use a pointer to indicate the last update of each node. For instance, let us consider a node (x:expSup(x)).

Suppose we are processing the j-th batch of the streaming uncertain data. If the pointer in the node points to the entry representing the (j ? 1)-th batch, then this indicates that the node has just been visited when processing the (j ? 1)-th batch. Otherwise, the pointer points to a much earlier entry  representing the (j ? k)-th batch for some k > 1, which indicates that the node has not been visited since then and the occurrence count of such a node for the entries in-between should be 0 (i.e., the node does not occur in (j ? k + 1)-th, . . ., (j ? 1)-th batches). By doing so, we avoid traversing all the nodes in the SUF-tree.

Next, let us focus on how to perform the actual min- ing. With the SUF-tree, the actual mining of frequent item- sets from streams of uncertain data is ?delayed? until it is needed. In other words, once the SUF-tree is constructed, it is always kept up-to-date when the window slides. Conse- quently, one can mine frequent itemsets from this up-to-dated SUF-tree in a fashion similar to the UF-growth algorithm using an appropriate minsup. More specifically, mining with the SUF-tree employs a divide-and-conquer approach. The algo- rithm forms projected databases (e.g., {e}-projected database, {e, d}-projected database, {e, c}-projected database, etc.) by traversing the paths upwards only. Since items are consistently arranged according to some canonical order, one can guarantee the inclusion of all frequent items using just upward traversals.

There is also no worry about possible omission or doubly- counting of frequent items during the mining process. As the SUF-tree is always kept up-to-date, all frequent itemsets in current streams can be found effectively. To get a better understanding of our proposed algorithm, let us consider the following example.

Example 2: Consider the same stream of uncertain data as shown in Example 1. Let minsup be 1.2 and let the window size w be 2 batches (indicating that only two batches of streaming transactions are kept). Then, when the first two batches of transactions in the data stream flows in, we insert the transactions into the SUF-tree and keep occurrence counts in a list of w entries at each node. Each entry in the list corresponds to a batch. For example, the node (a:0.9)[2,1] in Fig. 2(a) indicates that (a:0.9) occurs twice in the first batch and once in the second batch.

Afterwards, when a subsequent batch (e.g., the third batch) of streaming uncertain data flows in, transactions in the batch are then inserted in the SUF-tree. The list of occurrence counts shifts, the occurrence counts for the oldest (i.e., the first) batch are removed?leaving room for the occurrence counts for the second and the third (i.e., the two newest) batches of transactions. For example, the node (a:0.9)[1,2] in Fig. 2(b) now indicates that the occurrences of (a:0.9) are 1 and 2 respectively in the current window of w=2 batches (i.e., the second and the third batches). Note that those four nodes with lists of zero occurrence counts can be removed.

Once the SUF-tree is constructed, one can easily form an {x}-projected database for some frequent item x. As a concrete example, after the third batch of streaming uncertain data flows in (say, at time T ), we can form the {d}-projected database by traversing relevant tree paths upward from d nodes. The occurrence of any tree path can be computed by summing the occurrence counts in the list for d. Here, the {d}-projected database contains (i) ?(a:0.9)? that occurs     (b:1.0)[1,2]  (d:0.7)[0,1]  (e:0.1)[0,1](d:0.3)[0,1]  (f:0.2)[1,0]  (c:0.9)[1,1]  (a:0.9)[2,1]  (e:0.7)[1,0]  (d:0.8)[1,1]  (e:0.6)[1,0]  (d:0.7)[1,0]  (c:0.7)[1,0]  (a) The SUF-tree for transactions in the 1st & 2nd batches  (b:1.0)[2,1]  (d:0.7)[1,0]  (e:0.1)[1,0](d:0.3)[1,1]  (f:0.2)[0,0]  (c:0.9)[1,1]  (a:0.9)[1,2]  (e:0.7)[0,1]  (d:0.8)[1,2]  (e:0.6)[0,0]  (d:0.7)[0,0]  (c:0.7)[0,0]  (b) The SUF-tree for transactions in the 2nd & 3rd batches  Fig. 2. SUF-trees for Example 2  1+2=3 times with (d:0.8), (ii) ?(b:1.0), (c:0.9)? that occurs 1+1=2 times with (d:0.3), and (iii) ?(b:1.0)? that occurs 1+0=1 times with (d:0.7). Then, our proposed algorithm applies UF-growth to this projected database to find frequent itemsets {d} with expSup({d})=[(1+2)? 0.8] + [(1+1)? 0.3] + [(1+0)? 0.7] = 3.7 and {a, d} with expSup({a, d})=(1+2)? 0.9? 0.8 = 2.16.

It is important to note, in this example, the mining is done only on the second and third batches but not on the first batch. As mining is done only when it is needed, it saves computation.

In addition to saving computation, our SUF-tree used in our proposed SUF-growth algorithm also possesses the following nice properties:  ? The occurrence count of a node is greater than or equal to the sum of occurrence counts of all its children nodes.

? The number of tree nodes in the SUF-tree is bounded above by the number of distinct (item: expected support) pairs.

? The SUF-tree can be applicable for the mining of other patterns (e.g., constrained frequent patterns, closed pat- terns, maximal patterns).

? One can mine frequent itemsets from streams of uncertain data without using the UF-stream structure.



V. EXPERIMENTAL RESULTS  The experimental results cited below are based on datasets generated by the program developed at IBM Almaden Re- search Center [2]. This dataset contains 1M records with an average transaction length of 10 items, and a domain of 1,000 items. We assigned an existential probability from the range (0,1] to each item in each transaction. We set each batch to be 0.1M transactions and the window size to be w=5 batches. In addition to this dataset, we also conducted         0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  R un  tim e  (in s  ec on  ds )  Minimum support threshold (in percentage)  Runtime vs. minsup  UF-streaming SUF-growth  (a) Varying minsup           100  200  300  400  500  600  700  800  900  1000  R un  tim e  (in s  ec on  ds )  Size of database (in thousand transactions)  Runtime vs. #transactions  UF-streaming SUF-growth  (b) Varying the number of transactions  Fig. 3. Experimental results: runtime  the following experiments using some other datasets, including UCI real-life datasets as well as FIMI datasets. The observa- tions or trends were consistent. Hence, for lack of space, we only show below the results on the IBM datasets.

All experiments were run in a time-sharing environment in a 1 GHz machine. The reported figures are based on the average of multiple runs. Runtime includes CPU and I/Os; it includes the time for both tree construction and frequent itemset mining steps. In the experiments, we mainly evaluated the efficiency of the proposed algorithms.

In the first experiment, we tested the effect of minsup (or preMinsup). Theoretically, the runtime decreases when minsup (or preMinsup) increases. Experimental results (as shown in Fig. 3(a)) confirmed that, when minsup increased, fewer itemsets had expected support ? minsup, and thus shorter runtimes were required.

Second, we tested scalability of our proposed algorithms.

Theoretically, both the UF-streaming and SUF-growth algo- rithms are expected to be scalable with respect to the number of transactions. Experimental results (as shown in Fig. 3(b)) showed that runtimes of our proposed algorithms were linear with respect to the number of transactions.

In the third experiment, we tested the effect of the dis- tribution of item existential probability. Theoretically, when items take on many different existential probability values, UF-trees (for the original TDB, and then projected databases for singletons as well as for non-singletons) become larger and times for both UF-tree construction and frequent itemset     mining become longer. On the other hand, when items take on a few unique existential probability values, the runtime becomes shorter. Similar comments are expected to hold for SUF-trees. This was confirmed by experimental results.

In the fourth experiment, we measured the number of nodes in the trees. The results showed that the total number of nodes in the UF-tree is no more than the total number of items in all transactions in the current batch. Similarly, the results also showed that the total number of nodes in the SUF-tree is no more than the total number of items in all transactions in the batches of the current window.

In the fifth experiment, we tested the accuracy of the proposed algorithms. As an approximate algorithm, the UF-streaming algorithm often returned most of the frequent itemsets, occasionally returned a few infrequent itemsets, and occasionally missed a few truly frequent itemsets. In other words, while it often returned most of the frequent itemsets, there is no guarantee that the algorithm is 100% accurate.

In contrast, as an exact algorithm, the SUF-growth algorithm always returned all and only those frequent itemsets. In other words, it did not return any false positives or false negatives.

In the sixth experiment, we measured the efficiency of the proposed algorithms. The results showed that the UF-streaming algorithm responded to the user quicker than the SUF-growth algorithm because the former used the ?im- mediate? mining mode so that it just needed to retrieve relevant paths from the UF-stream structure when the user asked for the mining result. However, the results also showed that the total execution costs for the SUF-growth algorithm was shorter than that for the UF-streaming algorithm, especially when the user requested for the mining results infrequently or near the end of a long data stream. It is because the SUF-growth algorithm used the ?delayed? mining mode so that mining is done only when it is needed. For example, with a stream of 10 batches and w=5, the SUF-growth algorithm only needs to apply the mining process once (to the 6th to the 10th batches), whereas the UF-streaming algorithms applied the mining process six times (to the j-th to the (j+4)-th batches for j=1, . . ., 6).

As ongoing work, we conduct more experiments in studying the effects of the uncertain range (the number of possible worlds), the rate of arrival for streaming uncertain data, as well as various data distributions.



VI. CONCLUSIONS  Frequent itemset mining plays an essential role in the mining of various patterns and is in demand in many real- life applications. Most of the existing algorithms find frequent itemsets from traditional transaction databases consisting of precise data. However, there are many real-life situations in which ones are uncertain about the contents of transactions.

This calls for the mining of uncertain data. Moreover, due to advances in technology, a flood of precise or uncertain data can be produced in many situations. This calls for the mining of data streams. To deal with these situations, we proposed two tree-based mining algorithms?namely (i) the UF-streaming algorithm (which is an approximate algorithm  using an ?immediate? mining mode) and (ii) the SUF-growth algorithms (which is an exact algorithm using a ?delayed? mining mode)?to find frequent itemsets from streams of uncertain data. These two algorithms use the UF-tree, the UF-stream structure, and the SUF-tree to capture important contents of these dynamic streams of uncertain data, where each item is associated with an existential probability. Exper- imental results showed the effectiveness of our algorithms.

