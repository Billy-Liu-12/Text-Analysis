Mining Insightful Classification Rules Directly and Efficiently

ABSTRACT  Classification is one of the important problems in the field of  data mining. Many algorithms have been proposed to solve this  problem and each has its own drawback. This paper discusses  issues about mining classification rules directly and proposes  two algorithms, namely UARC and GARC. These algorithms  use a more suitable association rule mining technique to fmd  insightful and a complete set of rules directly and accurately.

Unlike most other association rule mining algorithms, the  algorithms proposed in the paper can find both frequent k-  itenisct and rules at the same step. After each scan of the  database, only rule itemsets and excluded itemsets arc saved and  used to exclude much more itemsets to generate larger-candidate  itemsets, which will save much computation time and memory.

Upon the information gain criterion, many training cases which  satisfy a special condition can be deleted from database, which  will lead to fewer I/O times for every remaining scan of database.

Finally, a criterion is defnied to terminate the whole mining  process much earlier and at the same time-produce a meaningful  Wle.

1. INTRODUCTION  Classification is one of the important problems in the emerging  field of data mining (also !ao\\n as KDD: Knowledge Discovery  in Database) wluch is aiined at hiding a small set of rules from  training data set with predetermined targets [I]. Many algorithms  such as decision tree, neural network and statistics methods have  been proposed to solve this problem. Compared with other  methods, decision tree classifier is relatively better in speed,  accuracy and understandibility. Many algorithms proposed in the past are only designed for memory-resident data set.

Quinlan's C4.5 or its new version C5.0 is one of such well-  hiowi methods [2]. hi order to deal with the limitation in size of  0-7803-5731-0/99/$10.00 01999 IEEE III-911  data set for decision tree based classification algorithms, many  algoritluns have been put forward in recent years [3][4][5],  which, however, have some drawbacks as follows. First, in order  to find best split, at every node every attribute and every value  group (or interval) should be test. With very large data set, this  may result in'more times of scan of data set and high cost in efficiency. On the other hand, using sampling or partition  techniques may result in low accuracy. Second, the classification  rules correspond to decision trees without pruning and final  generalizing are usually cumbersome, complex and inscrutable.

While pruning and final generating rules from trees need extra  times to scan the data set. Lastly, as for numeric attributes, the  test forni is A d v ,  where A is a attnbute and v is a value. a s form is not always intuitive and sometimes leads to complicate  rules.

Recently, a new algorithm which use association rule mining  technique to fmd classification rules from large amount of data is  discussed by Bing Liu et al.[6]. Association rule mining is  another important technique in data mining, which aims to find  associations among itemsets in transactional database. There are many algorithms proposed in recent years to solve this problem  [7][8][9][10][11], among which Apriori given by Agrawal and  Srikant (1 994) is usually regarded as a typical algorithm. In this  algorithm, association rule mining is separated into two steps.:  first, generating a complete set of frequent itemsets and then  producing association niles from frequent itemsets. Using this  algorithm, Being Liu et al. proposed a algorithm named CBA  (Classification Based on Association) to find classification rules  in two steps which is first generating a' complete set of  association rules called CARS among training data set and then  building a classifier from CARS. As a training data set for  classification usually contains a huge number of association  rules, so it costs much time to find all of these association rules  among which many are not useful, and need a large memory to  store them. Furthermore, selecting a subset of these rules to build    a classifier is also a time consuming task. Therefore, in this  paper, two algorithms are proposed to deal with these problems.

The ideas are as follows: Firstly, in order to prevent from producing a huge number of  candidate itemsets, the traditional association rule algorithm is  modified. The reason why so many candidate itemsets could be  produced is that the minimum support is set to be very low due  to the consideration of a complete set of candidate itemsets.

However, a classification rule is after all a special kind of a  association rule. During the classification nile mining process,  what is important is to f h d  rules that are frequent itemsets and at  the same time have a high confidence instead of frequent  itemsets only with low support. So we combine the two steps of  mining association rules to one and focus on finding and storing  rule itemsets and escluded itemsets (defined in section 2), both  of which are very small sets compared with frequent itemsets.

On the other hand, Apriori algorithm reduces the number of  candidate itemsets by excluding the itemsets that do not satisfy  -minimum support to forni larger itemsets, while in our  algorithms, in addition to this, another constraint is also defined  to rculuce the candidate itemsets further. That is every itemset  that already produced a rule is excluded to make larger candidate  i temsets.

Secondly, in order to save time during the scan of data set, based  on the notion of information gain [2], some cases whch have  produced a rule and contain a value of the first split attribute can  be deleted from the database. Then the generating method of  candidate itemsets changes as well. As a result, the database will  become smaller as the times of scan increases.

Thirdly, in order to improve the efficiency of this algorithm and  at the same time producing a insightful classification rule, a  criterion is defined to terminate the mining process earlier than  the usual association nile mining algorithm.

The rest of this paper is organized as follows. Section 2  describes thc classilication problem using the mining association nile method, and the definition of information gain. Section 3 presents the two algorithms in detail. Section 4 shows some  preliminary experiment results. Finally, section 6 contains our  concludions.

2. PROBLEM STATEMENT  2.1 Association Rule Mining Classification rules can be regarded as a special kind of  association rules within a data set. ?he data set for classification  usually is separated into two groups, one is for training and the other is for testing. Both of them contain a large amount of cases (tuples) consisting of many attributes and one class label. The  task of the classi,fication is to find a set of rules to determine an unseen case's class correctly.

A classification rule is the form of implication like X-C,, where  X is the combination of attribute values, and Ci is one of g class  labels. As for association rules, the rule's form is X-Y, where X  and Y are all combination of attribute values (corresponding to  items in transactional database). So the difference is that Ci is a  special kind of Y. According to this, we can modify the typical  association rule's mining algorithm to mining classification rules  directly and efficiently.

Let D be the data set stored in the database, which contains M cases for training and N cases for testing. Each case is described  by r distinct attributes and one of g class labels. Let A be the set  of r attributes: A= {a,, a?. .. a, } , and G be the set of g class labels:  G={ C, ,C2 ,.._, C,).AnitemofDisoftheform:

I.: a.= V I U' 1 1  where %?A, v;is a value of attnbute a,, i=l ,  2, ..., r, j=1, 2 ,_..,  I\.

Let I be the set of all items: I=fI,j I i=l, 2, ..., r, j=1, 2 ,..., n, }, and Xc_I be a subset of items. Then a candidate itemset is  defined as:  <X, C,> with two numbers: lcount and wcount where lcount is the number of cases contain X and wcount is the  number of cases containing X and labeled Ci. A case d  containing X means Xcd. Another concern of this itemset is its size which is defined as the number of items included in X. So if  X contains !i attribute values, it is called a k-itemset.

The support and confidence of this candidate itemset is defined  as follows: \vwunt  support (< x, Cl >) = ~  confidence (< X, Ci >) = - ID1  \vcount  lcount wliere ID1 is the number of cases in database D. If its support is  greater thrtn a iiiiiumuin support (called minsup) given by user,  then this candidate itemset is a frequent itemset. Furthermore, if  the confidence of tlus frequent itemset is greater than a minimum  confidence (called minconf) given by user, this frequent itemset  is then called a rule itemset and can be output as a rule:  x-c,  m -912    Otherwise, if the support of a candidate itemset is less than  minsup, it is called an excluded itemset that can not be used to produce larger itemsets. Other candidate itemsets except for the  excluded itemsets and rule itemsets can be used to produce  larger itemsets. The reason why rule itemsets are excluded to  produce larger itemsets is that if a rule X-C, holds in D, then X U Y-C, also holds, where Y is another subset of items. So one  such rule can produce many rules like X U  Y-C, since Y can be  any subset of I. However in fact the rule X-C, is the most  meaningful one among those rules.

2.2 Information Gain The information gain criterion is one of those methods [12][13]  used to select best split attributes in decision tree classifiers. We  use it in our algorithm to help decide which training cases can be  deleted from the database.

Suppose a attribute A has n distinct values that partition the data  set T of training cases into subsets T,, T2, ..., T,,. For a data set  ScD, freq(C,,S) stands for the number of cases hi S that belong  to class C,. IS[ denotes tlie number of cases hi data set S. Thai  info(S) is defined as follows to measure the average amount of  information needed to identify the class of a case in S :  where, g is the number of classes.

After data set T is partitioned in accordance with n values of attribute A, the expected information requirement can be defined  as:  ? I T i I  . info A(T) = -c -- x info (Ti) The infomiation gained by partitioning T according to attribute A is delilies as:  i = l  IT1  gain( A)=in fo( T)-in fo,,( T)  Among all attributes in data set T, the best split attribute is the  one that maximizes the information gain.

3. CLASSIFICATION ALGORITHMS  3.1 UARC: Association Rule Based Classification UARC is a direct classification rule?s minhig algorithm, in which a new method is proposed to build candidate itemsets and to  prevcnt both the escludd itemsets and rule itemsets G-om  generating more meaningless candidate itemsets. In addition, a  new criterion is defined to terminate the rule mining process  earlier. The detail is given as follows.

During rule mining phase, after first scan of data set, a record class list called recClass is created initially to contain the class  label attached to each case. The ith entry of the class list corresponds to the ith case in the training database. In the next scan of the database, once a rule is produced, each entry for the  cases containing this rule will be set to a value (assume 0)  different from every class label. Using this list, we can define a  criterion to end the mining process. That is to say, after each  scan of database, if each entry of recClass is the same (or almost  the same, say 90%, which can be given by users according to the  accuracy required and noise contained in this dataset), then stop  further scan of database. If the value of each entry rather than 0  is one of tlie class labels, then a default rule can be produced.

Additionally: alter each scan of database, only rule itemsets,  excluded itemsets ?and information about them such as Icount,  wcount and the corresponding record numbers will be saved.

Other candidate itemsets will be cleared from candidate itemsets  list before the next scan of the database starts. During the next  scan of the database, new candidate itemsets that do not contain  any of excluded itenisets and rule itemsets will be generated for  every cases. Figitre 1 gives the training process of UAKC.

1. initclass(recClass);  2. rule= { 1-itemset cI support(c)>=minsup and confidence(c)>=  minconf) ;  3 .  excluded= { I-itemset cI support(c)<minsup};  4. adjust(recC1ass);  5. for(k=2; finish(recClass)==O && k<=r; k*)  6. hiitcand(cand); currentc=O;  7. for(t=l;t<=M; t t t )  8. C,=gencand(nde,exclude);  9.

IO. addtoand( c,cand);  11. end  12. end  13.

for each itemsct c E C,  R={c E cand I support(c)>=nunsup and confidence(c)>= minconf) ;  14. rule=nile U R;  15.

16. adjust(recClass),  17. end  18. sort(ru1e);  excluded= { c E cand I support(cFniinsup};  III -913    19. default(rule);  20. output(rule); Figure 1 Training process of algorithm UARC.

Lines 1 to 4 present the first scan of the database. It initializes  the value of recClass to be the class label of each case and  produce all 1-itemsets from which rule itemsets and excluded itemsets are selected. According to each rule, its corresponding  cases? class label in recclass will be set to zero (line 4).

Lines 5 to 17 are what should be done in subsequent scan of  database. Dilring each scan, for each case in D, several itemsets  will be generated according to rule itemsets and excluded  itemsets (line 8). Each of these itemsets will be added to  candidate itemset (line 10) and the corresponding lcount, wcount  and each record No. (recno) are saved in structure variable named cand. Then from candidate itemsets cand, nile itemsets  and excluded itemsets are selected, and recClass is adjusted.

After each pass, finish(recC1ass) check if every entry (or almost)  of recClass has the same value, if so, nest scan of database stop.

If scan ternunated, all of itemsets contained in nile \vi11 be sorted  according to its confidence and, support (line 18). Then produce a  default nile according to recClass (line 19), and fuially all of  rules are output (line 20).

3.2 GARC: Gain Based Association Rule Classification When using algorithm UARC, in each pass of the algorithm  every case is read from the database, while in fact some cases  can be deleted from tlie database permanently. However, which  cases can be deleted is a problem of concern that is dealt with  upon the information gain criterion. After tlie first scan of the  database, all of candidate itemsets is saved to variable named  cand. Using each candidate itemset?s lcount and wcount,  infoniiation gain for each attribute -4 that can be used to partition  thc database D into 11 clitasets can he calculatcd as fo l low  ? = -E support ( A  = \ji) x (f: confidence (it) x log, confidence (ik))  i = l  k = I  where Ti corresponds to the data set whose attribute A?s valve  equals v,, g is the numbers of classes, i, stands for itemset <A= vi,  C,>.

With the infoniintiori gain criterion, a best split attribute (called  bestattr) can be selected after frrst scan of database. Then during  the next scan of database, we can delete those cases that produce  rules containing attribute specified by bestattr in the previous scan. This is similar to the tree building process: if a rule that  contains attribute bestattr is produced, there is no need to deal  with its corresponding cases. In addition, there is a little change  in the candidate itemset generating method. For each case, we  can not produce all of its candidate itemsets and can only  produce itemsets include bestattr. The reason is that for the  itemsets?without attnbute bestattr, the lcount and wcount values  can not be counted correctly due to the deletion of some cases  \vliich contribute to the count of lcount and wcount. For itemsets  including attribute bestattr, the values of the attribute bestattr  included in itemsets is different from the values contained in  datasets that.have been deleted, so we can still count these itemsets correctly. This algorithm is shown in figure 2.

1.

2.

3.

4.

5 .

6.

7.

8.

9.

iritclass(recC1ass);  nile= { 1 -itemset clsupport(cp=minsup and confidence(c)>=  minconf } ;  excluded= { 1 -itemset cI support(c)<minsup} ;  adjust( recClass);  bestattr=gain(cand);  getdeleted(delrec, bestattr);  for(k=2; finish(recClass)==O && k = r ;  k++)  initcand(cand); currentc=O;  for(t=l;t<=M; ti+)  10.

11. else  12. C,= gencand(rule, exclude, bestattr);  13.

14. addtocand(c,cand);  15. end  16. end  17.

if ( t E delrec) delete t from database;  for each itemset c E C,  R= {c E cand 1 support(c)>=niinsup and confidence(c)>= iniiicoiif) ;  18. nile=nile U R, 19.

20. adjust(recC1ass);  21. end  22. sort(ni1e);  23. default(ru1e);  24. outpiit(ni1e);  excluded= { c E cand I support(c)<minsup);  Figure 2 Training process of algorithm GARC.

In this algorithm, lines 5, 6, 10, 12 are different from previous  algoritlun UARC. gain(cand) is to select bestattr based 011  III-914    information gain criterion. getdeleted(delrec, bestattr) is to get  the record number (will be saved in delrec) that can be deleted according to attribute bestattr. Line 10 is to delete cases included  in delrec. Gencand(rule, exclude) differs from that in UARC as  stated above.

Data Set pred 1 pred2 pred3 pred4 pred5  4. EXPERIMENTAL RESULTS  c5.0 UARC GARC 0.000 0.000 0.000 0.000 0.000 0.000 0.060 0.015 0.015 0.080 0.030 0.060 0.150 0.115 0.140  This section shows an empirical performance evaluation of  UARC and GARC. The experiment was divided into two parts.

The first part compares UARC, GARC with the typical decision  tree classifier C4.5 in terms of accuracy and the number of rules.

The second part examines the execution time and scalability of  algoritluns UARC and GARC..

In order to accomplish the two parts of evaluation, the data set  used is the synthetic database proposed in [14]. Each tuple in  database has 9 attributes. Ten classification functions were used  to produce data distributions of varying complesitics in tllis paper. We use first 5 functions to produce 5 data sets named predl -predj, among which predj corresponding function 5 is  one of the hardest to characterize mid results in the highest  classification errors.

oredl I 14  4.1 Accuracy and Number of Rules We use a small data set (200 cases) for every function to present  the accuracy and number of rules of three algorithms. Since C5.0  is the new version of C4.5 and much more accurate and faster  than C4.5, in the experiment we compared our algorithms with C5.0. Table 1 shows the classification accuracy of different  algorithm, while table 2 shows the number of rules produced by  every algorithm.

Table1 Accuracy of difrerent algorithms  16 16 pred2 pred3  15 21 15 20 30 30  pred4 pred5  In the experiment, for a categorical attribute, all i,ts possible values are mapped to a set of consecutive positive integers. For a  continuous attribute, its value range is discretized into intervals  and each interval is also mapped to a set of consecutive positive  integers.

It can be seen from table 1 that both UARC and GARC are more  accurate than C5.0 for every data set. Table 2 shows that the  number of rules produced by UARC or GARC is more than that  produced by C5.0, while those rules that are not included in the  result of C5.0 are useful to improve classification accuracy and  are just what (25.0 can not obtain. For example, for dataset pred4,  one of rules found by both UARC and GARC is:  IF commission~40000 and commission<50000 THEN group= 1 This rule is useful and accurate according to function 4, but C5.0  could not found it.

Table 2 also indicates that UARC generates more rules than  GARC for some datasets. The reason is that the set of candidate  itemsets becomes small due to the deletion of some cases, wvl~ch  also lead to the lower accuracy of GARC than UARC.

23 30 26 29 76 38  4.2 Speed And Scalability hi this part, we use different size of data set of function 3 to  examine tlie speed and scalability of our two algorithms along  the number of training cases. Figure 3 illustrates the execution  time used by UARC and GARC as tlie iiumber of training cases increased from 200 to 100000.

As we can see from figure 3, compared to UARC  algorithm, GARC is faster, and the difference behveen UARC  and GARC becomes larger as the number of cases increases. The  result also indicates that both two algorithms achieve near-linear  execution time on disk resident dah.

180 I  1 ; : L L l 3 20 - " 0  0 7 14 21 28 35 42 49 56 63 70 77 84 91 98  Number of examples(in thousand)  Figiue 3 Speed and scalability  IU -915    5. CONCLUSION Both classification rule mining and association rule mining are important problems in the field of data mining. Many kinds of  classification algorithms have been proposed in the past and in  recent years. But each of thein has its drawbacks. In this paper, two algorithms are designed to use the basic concept of  association rules to find classification rules directly and  efficiently. From the empirical evaluation we can see that both two algoritluns achieve better classification accuracy and  produce complete, useful sets of rules. It is also shown that these  two algorilluns both have a good scalability, and GARC has a  betttT execution speed than UARC.

ACKNOWLEDGEMENT This work was partly supported by National Science  Foundation of China (grant No. 69674037 and grant No.

79825 102) and National Defense Foundation of China.

