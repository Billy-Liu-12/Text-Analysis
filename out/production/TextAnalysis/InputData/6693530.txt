Context-Based Event Ontology Extension in Multimedia Applications

Abstract?An approach focused on inferring the finest possible subevent witnessed by individual photographs in a personal photo stream of an event is presented in this work using image metadata (timestamp, location, and camera parameters), information about the user, ontological event model, mobile device connectivity, web services, and external data sources. We introduce Event Ontology Augmentation that is a new technique to obtain, refine, and validate expressive event tags in the context of personal photo annotation. In this technique, an ontological event model is used to describe the vocabulary for a general domain event such as trip. The event-ontology is extended with context-information from heterogeneous data sources. We introduce plausibility measure for ranking and selecting the best possible subevent category related to a group of photos in a photo stream.



I. INTRODUCTION  Modern digital cameras, particularly those on smart phones, record time, GPS location, and camera parameters with the photo?s EXIF header. Photos with such metadata witness events in our lives and there is high-demand for searching through personal photo archives to relive these events. Annotating artifacts with expressive tags supports this demand. Our goal in this work is to bridge the semantic gap which exists between high-level events (which are perceived by people) and photos (which are produced by the machine).

Not all information for inferring events is hardwired to photographs; hence, it must be discovered. Personal photos have become rich sources of information about the events occurring in a user?s life. Events themselves are also key cues to recall personal photos [12] and, therefore, they can be used to create searchable description metadata for them. Events, in general, are structured and their subevents have relatively more expressive power [14]. For instance, the event Giving a Talk is more expressive than its superevent, Professional Trip. In addition, instance events are contextual and should be augmented with context cues (like place, time, weather, participants). This makes instance events more expressive than event types. For example, the instance event Giving a Talk at UCF at most two hours before meeting with Ted on a windy day is far more expressive than the event type Giving a Talk.

We define flexible expressiveness as follows: a) multi-granular conceptual description, which provides conceptual hierarchy in multiple levels using containment event relationships e.g. subevent-of, subClassOf; b) multi-context adaptation of conceptual description, which adapts a concept to multiple contextual descriptions (e.g., event type visit-landmark may have two instances; one instance associated with Forbidden City and the other to Great Wall of China). Currently, photos are not searchable based on expressive subevent tags because manual annotation is a labor-intensive task, and there is no standard mechanism to create and assign these tags to photos  automatically and reliably. Consider the following example: A person takes a photograph at an airport less than 1 hour after his flight arrives. To explain this observation (i.e. the photograph), we first need the background knowledge about the events that generally occur in the domain of a trip.

The corresponding semantics can only come from a domain event-ontology that provides the vocabulary for event/entity and event relationships related to the domain. In general, ontology is a powerful logical framework that is the glue that bonds human understanding of the real world and models of the real world in machines. An event-ontology could support flexible expressiveness. It allows explicit specification of models that could be modified using context information to provide very flexible models for high-level semantics of events. We refer to this modification as Event Ontology Augmentation. It constructs a more robust and refined version of an event-ontology either fully or semi-automatically.

Secondly, given the uncertain metadata of a photo (like GPS that is not always accurate), the event type that the photo witnesses is not decisive; it might either be rent a car, or baggage claim that are two possible conclusions. Because the photo has incomplete information, the derived conclusions are not decisive, but merely possible ? sometimes no single obvious explanation is available, but rather, several competing explanations exist and we must select the best one. In this work, reasoning from a set of incomplete information (or observations) to the most related conclusion out of all possible conclusions (or explanations) is performed through a ranking algorithm that is used in Event Ontology Augmentation.

Problem Formulation: We assume that every input photo has context information (specifically, timestamp, location, and camera parameters) and a user/creator. Each photo belongs to a photo stream P of an event with a basic domain event- ontology O(V,E) whose nodes (V ) are event/entity classes, and edges (E) are event/entity relationships, handcrafted by a group of domain experts. We assume that there is a bucket (B) in which external data sources are represented with a schema.

The sources in B can be queried using the metadata of the input photographs and information about the associated user.

Given P , B, O, and information associated to the user, how does one find the finest possible event tag that can be assigned to a photo or a group of similar photographs in P ?

Solution Strategy: We propose Event Ontology Augmenta- tion technique described as follows: select a relevant domain event ontology O(V,E) through the information related to both the user and P . Using P , B, O, and the user information, infer S that consists of the best relevant subevent categories to P where S ? V . An event category in S is the most plausible one among other competing candidates that have failed to be selected. For each competing candidate si, a plausibility   DOI 10.1109/ICSC.2013.55    DOI 10.1109/ICSC.2013.55     measure mpij is calculated using function f to rank si and indicate how much it is relevant to cj such that cj ? P and cj is a group of similar photographs: f(si, cj) = m  p ij . Next,  augment S using the information from B to obtain expressive event tags T . We define an event tag tei ? T as a subevent of an event that either exists in O, or can be derived from O such that tei is the finest subevent tag that can be assigned to a group of similar photos. Also, if tei is an assignable tag to any photo, and tei does not exist in O, we intend to augment O by adding t  e i to  O using the shortest composition path such that the constraints governing O are preserved. Simply put, the final step is adding T to O by preserving the rules that govern O if T ?? O.

The output is an extension to O that is referred as Or. We argue that Or (see fig 1) can be used for an event recognition task in photo annotation applications. The key insight in our proposed approach is to infer event characteristics from the image metadata (timestamp, location, and camera parameters), information about the user, ontological event model, mobile device connectivity, web services, and external data sources.

We argue that attribute values related to an inferred event need to be obtained, refined, and validated as much as possible to create very expressive and reliable metadata for digital photographs and facilitate image search and retrieval. Fig 4 depicts the processing components of our proposed approach in the context of personal photo annotation.

Several event semantics are utilized in this work like spa- tiotemporal attributes/constraints of events, subevent structure, and spatiotemporal proximity. Unlike machine learning ap- proaches that are limited to the training data set and require an extensive amount of annotation, we propose a technique in which existing knowledge sources are modified and expanded with context information in personal data sources (like Google Calendar, and social interactions), public data sources (like public event and weather directories, geographical and local business databases), and digital media archives (like personal photographs). With this knowledge expansion, new infrastruc- tures are constructed to serve relevant data to communities.

An example result set of our proposed work is depicted in fig 2. Event tags are propagated with event title, place information (like city, category, place name), time, weather, etc. Our proposed technique provides two unique key benefits as follows: 1) An ontological event model is sufficiently flexible to express context attributes for events such that these attributes are not hardwired to the events, but rather they are discovered on the fly. This is a very important feature since it does not limit our approach to a single data set; 2) leveraging context data across multiple sources could facilitate building a consistent, unambiguous knowledge base.

Event ontology augmentation has several challenges: a) we need a language that is characterized by formal semantics and can model different types of entity properties and relationships related to an event domain. OWL is widely used for developing ontologies. However, this language is limited in terms of its capability of describing the semantics of events. A major challenge is to create an extension of OWL and provide the grammar for that extension; b) collecting and combining information from multiple sources is a daunting task. It needs a general mechanism to automatically query sources and represent the output. It also needs a validation mechanism to ensure the coherency of the obtained data; c) currently, publicly available benchmark data sets such as those offered by TRECVid [6] do not suit the purpose of this research. Many deal with activities that are detected using low-level features  Fig. 1: An Example of Augmented Event Ontology.

Fig. 2: Personal Photo Annotation using Event Ontology Augmentation.

in media content such as surveillance video. However, higher- level events have relatively more contextual characteristics; d) according to the useful properties of photoset, relevant event categories in the model must be discovered. So, those properties need to be identified. This paper is organized as follows: in section II, we review the prior art that use context and event models for annotating photographs; in section III and IV, we explain our solution strategy; this is followed by section V that demonstrates our experiments, and section VI which is the conclusion.



II. RELATED WORK  The important role of context in image retrieval is em- phasized in [9]. Context information and ontological event models are used in conjunction by [17], [6]. Cao et al.

present an approach for event recognition in image collections using image timestamp, location, and a compact ontology of events and scenes [4]; this work, does not support subevent structure. Liu et al. reports a framework that converts each event description from existing event directories (like Last.fm) into an event ontology that is a minimal core model for any general event [11]. This approach is not flexible to describe domain events (like trip) and their subevent structure. Paniagua et al. propose an approach that builds an event hierarchy using the contextual information of a photo based on moving away from routine locations, and string analysis of English album titles (annotated by people) for public web albums     in Picasaweb [13]. The limitations of this approach are: 1) human-induced tags are noisy, and 2) subevent relationship is more than just spatiotemporal containment. For instance, albeit a car accident may occur in the spatiotemporal extent of a trip, it is not part of the subevent-structure of the trip.

According to [3], events form a hierarchical narrative structure that is connected by causal, temporal, spatial and subevent relations. If these aspects are carefully modeled, they can be used to create a descriptive knowledge base for interpreting multimedia data. In [15], an image annotation mechanism is proposed that exploits context sources in conjunction with subevent-structure of an event ? this structure is modeled in a domain event ontology. The limitation of this approach is no matter how much an event category is relevant to a group of photos in a photo stream, it is used in photo annotation; as a result, the quality of annotation degrades.



III. EVENT ONTOLOGY AUGMENTATION A photo has incomplete information that can be improved  if combined with the information related to a group of similar photos. In this paper, two images are similar if they belong to the same type of event. Partitioning a photo stream of an event based on the context of its digital photographs can create separate subevent boundaries for its photos [5]. An event is a temporal entity. However, using time as the only dimension in clustering means ignoring other context semantics about events. Better results can be obtained when time and location are used [7]. Also, optical camera parameters in photos provide useful information related to the environment (like outdoor) at which an event occurs [16]. We used an agglomerative clustering that partitions photos hierarchically based on their timestamp, location, and optical camera parameters. We used single linkage clustering and Euclidean distance in our clus- tering technique. However, one can use other approaches and refine the results. Our goal is to derive the best possible subevent category from a set of incomplete observations.

We present the observations with a set of descriptors. Each descriptor is a formula for a photo or a cluster ? here, a cluster consists of a group of contextually similar photos. In this section, we show that it is feasible to go from a set of descriptors D to the best subevent category, when the following conditions are satisfied: (a) the descriptors in D are consistent among themselves, (b) the descriptors in D satisfy subevent categories, (c) axioms of a subevent category are consistently formulated in an event ontology, and (d) the inferred subevent categories are sound and complete.

A. Event Model We use a basic derivation of E* model [8] as our core  event model, to specify the general relationships between events and entities. Specifically, we utilized the relationships subeventOf, which specifies the event structure and event containment. The expression e1 subeventOf e2 indicates that e1 occurs within the spatiotemporal bounds of e2, and e1 is part of the regular structure of e2. Additionally, we used the spatiotemporal relationships like occurs-during and occurs-at to specify the space and time properties of an event.

The time and space model that we used in this work is mostly derived from E* model. The relationship participant is used to describe the presence of a person in an event. We use the relationships co-occurring-with, and co-located-with, spatially-near, temporal-overlap , before, and after to describe the spatiotemporal neighborhood of an event. The relationship  same-as between two events, makes them equivalent entities.

Also, we used several other relationships to describe additional constraints about events (e.g., e1 has-ambient-constraint A, and A has-value indoor). Moreover, to express a certain group of temporal constraints, we utilized some of Linear Temporal Logic, Metric Temporal Logic, and Real-Time Temporal Logic formulas [10], [2]. These formulas are a combination of the classical operators ? (conjunction) , ? (disjunction) , implica- tion (?) , Allen?s calculus [1], ? operator, ? operator, linear constraints, and distance functions; they are used to model complex relative temporal properties. For instance constraint ?[t1,t2](e1 ? ?[t2,t2+1800]e2 ? D?(e2) ? 1800) states that e2 eventually happens within 1800 seconds after e1 and that e2 lasts less than or equal to 1800 seconds. We developed a language L with a syntax and grammar as an extension to OWL to embrace complex temporal formulas. Further, we extended the language to support a combination of classical propositional operators, linear spatial constraints, and spatial distance functions which can not be expressed in OWL; equation feucDist(e1, e2,@ ? 100) shows a relative spatial constraint in L, which states the event e1 occurs at most 100 meters away from the place at which event e2 occurs.

Domain Event Ontology: A domain event ontology provides specialized taxonomy for a certain domain like trip, see fig 3.

The Miscellaneous subevent category in this model is used to annotate the photos that are not matched with any other category. The general vocabulary in a core event model is reused in a domain event ontology. For instance, Parking in fig 3, is a subClassOf of Occurrent (or event) concept in the core event ontology. Also, relationships like subeventOf are reused from the core event ontology. We assume that domain event ontologies are handcrafted by a group of domain experts.

B. Descriptor Representation Model  We represent a descriptor using the schema in script {typed : valued, confidenced : val}, in which typed, valued, and val indicate the type, value,and certainty (between 0 and 1) of the descriptor, respectively. For instance, the descriptor {Flash : ?off ?, confidence : 1.0} for a photo, states that the flash was off when the photo was captured with 100% certainty. Photo and cluster descriptors follow the same representation model, however the rules for computing the value of confidenced are different. We will describe these rules in the following paragraphs. The descriptor model of a cluster includes two fields in addition to that of a photo: plausibility-weight ? 0 , and implausibility-weight < 0. Later, we will explain the usage of these fields. All descriptors are either direct or derived. For photo descriptors, by convention, we assume that a direct descriptor is straightly extracted from the EXIF metadata of a photo, and its confidence is 1, as in the above example. The direct descriptors that we used in this paper are related to time, location, and optical parameters of photos like GPSLatitude ,GPSLongitude , Orientation, Timestamp, and ExposureTime. For a derived descriptor like {sceneType : ?indoor?, confidence : 0.6}, the descriptor value ?indoor? is computed using direct descriptors like Flash, through a sequence of computations that extract information from a bucket of data sources. Some of these descriptors are PlaceCategory1, Distance2, and HoursOfOperation3. The  1The category of the nearest local business to the coordinates of a photo.

2The distance of a local business to the coordinates of a photo.

3The hours during which a local business is open.

confidence score is obtained from the processing unit used to compute the descriptor value ? we developed several information retrieval algorithms for this purpose,in addition to the existing tools in our lab [16]. If a descriptor value is directly extracted from an external data source, confidenced is equal to 1. Direct descriptors of a cluster must represent all photos contained in it; some of these descriptors represent boundingbox, time-interval, and size of the cluster. The confi- dence value for direct descriptors is equal to 1, for instance, in the descriptor {size : 5, confidenced : 1.0} that indicates the number of photos in a cluster, confidenced is equal to 1.

Given a photo pi in a photo stream P , and the cluster c that groups pi with the most similar photos in P , a processing unit produces the descriptors of c using the descriptors of the photos in c, and more importantly, this process is guided by the descriptors of pi. Every photo in c must support every derived descriptor of pi; such cluster is referred as a sound cluster for pi, and the derived descriptors for c are represented by the distinct union of the derived descriptors of the photos in c.

For a derived cluster descriptor d, the value of confidenced is calculated using the formula in equation 1, in which |c| is the size of the cluster, pj is every photo in c that is represented by d, and g(pj , d) gives the confidence value of d in pj . To find a sound cluster for a photo, the hierarchical structure that is produced by the clustering unit, is traversed using depth- first search ? the halting condition for this navigation, if no sound cluster was found, is when current cluster is a leaf node.

confidenced =  |c| ? ?  f(pj , d) (1)  Descriptor Consistency: As we mentioned earlier, consis- tency among a set of descriptors is a mandatory condition to infer the best possible conclusion from it. We make sure that consistency exists among the descriptors of a photo as well as the descriptors of a cluster, using entailment rules described below. (a) vi ? vk: if vi implies vk, then the rules for vk must also be applied to vi. This is referred as transitive entailment rule. For instance, suppose a photo/cluster has the follow- ing description, ?outdoorSeating : true? ; ?sceneType : outdoor?; ?weatherCondition : storm?, which implies that the nearest local business (e.g. restaurant) to the photo/cluster, offers outdoorSeating, and the weather was stormy when the photo(s) were captured. Given the sequence of rules below,  outdoorSeating ? outdoor ? fineWeather, fineWeather ? ?storm  rule 2 is entailed that indicates an inconsistency among the descriptors of a photo/cluster.

outdoorSeating ? outdoor ? ?storm (2) (b) vi ? funcremove(vk): vi implies removing the descriptor vk. This is referred as a deterministic entailment rule.

(c) vi ? vk ? truth value: rules of this type are referred as non-deterministic entailment rules in which the inconsis- tency is expressed by a false truth value e.g. closeShot ? landscape ? false. In that case, further decisions on keeping,modifying, or discarding either of the descriptors vi or vk will be based on the confidence value assigned to each descriptor ? this operation is referred as update, which is executed when an inconsistency occurs between two candidate descriptors. The following rules are used by this process: (a) for two descriptors with the same type, the descriptor with lower confidence score is discarded, (b) for two descriptors  Fig. 3: An event ontology for the domain professional trip.

with different types, the one with lower confidence score gets modified until the descriptors are consistent. The modification is defined as either negation or expansion within the search space. In case of negation, e.g. ?outdoor ? indoor, the con- fidence value for indoor descriptor is calculated by subtracting the confidence value of outdoor descriptor from 1. An example of expansion is increasing a window size to discover more local businesses near a location. To avoid falling inside an infinite loop, we limit the count of negation, and the size of search space during expansion, by a threshold. We assign null to the descriptor that has already reached a threshold and is still inconsistent. null is universally consistent with any descriptor.

The vocabulary that is used to model the descriptors for a photo/cluster is taken from the vocabulary that is specified in the core event model.

C. Bucket of Data Sources We represent each data source with a declarative schema,  by using the vocabulary of the core event model. This schema indicates the type of source output. In addition, it specifies what type of the input attributes a source needs, to deliver the output. Data sources are queried using the SPARQL language4.

The following script shows an example of a SPARQL query that is formed to query a source; var1 is a query variable (output that must be delivered by the source); attr1 is the input attribute of the source; classw indicates a class type, and rela indicates a relationship. The class types and relationships used in such queries are constructed using the vocabulary of the core event model.

SELECT ?var1 FROM < Source URI > WHERE { attr1 core : typeOf classw; var1 core : typeOf classu;  ?var1 core : rela ?x; ?x core : relb ?y;  ?y core : reld attr1. }  The above query is constructed automatically using the schema of data sources, and the available information. Simply put, a source is selected if its input attributes match the available information I . At every iteration, I is incrementally updated with new data that is delivered by a source. The next source is selected if its input attributes are included in I .

4http://www.w3.org/TR/rdf-sparql-query/     This process continues until no more source with matching attributes is left in the bucket B.

D. Event Inference From a set of consistent cluster descriptors, referred as  observations, we developed an algorithm to infer the most plausible subevent category described in a domain event ontology. This algorithm, uses the domain event model, which is a graph; we represent this graph with the notation O(V,E) in which V includes event classes, and E includes event relationships. Traversing the event graph O starts with the root of hierarchical subevent structure specified in the domain event ontology. The algorithm visits event candidates in E through some of the relationships in E like subeventOf, co-occurring-with, co-located-with, spatially-near, temporal- overlap, before, after, and same-as ? these relationships help to reach other event candidates that are in the spatiotemporal neighborhood of an event. An expandable list, referred as Lv , is constructed from E, to maintain the visited event/subevent nodes during an iteration i ? if an event is added to Lv , it cannot be processed again during the extent of i. At the end of each iteration, Lv is cleared. In every iteration, the best subevent category is inferred through a ranking process, from a set of consistent observations. We introduce Measure of Plausibility (mpij) which is used to rank event candidates, and help to find the most plausible subevent category. We compute mpij using two parameters (a) granularity score, and (b) plausibility score. The granularity score (wg) is equivalent to the level of the event in the subevent hierarchy in the domain event ontology. To compute the plausibility score (wAX ), we used ?plausibility-weight? (w+) and ?implausibility-weight? (w?) which are two fields of a cluster descriptor (mentioned earlier). The value of w+ is equal to the confidence value assigned to a descriptor, and the value of w? is equal to ?w+.

If a descriptor could not be mapped to any event constraint, wAX remains unchanged. If a descriptor with w+ = ? satisfies an event constraint, then w+ is added to wAX , otherwise, w? is added to wAX (i.e., wAX = wAX ??). The only exception is for the cluster descriptors time-interval and boundingbox; if either one of these descriptors satisfies an explanation, then w+ = 1; in the opposite case, w? ? ?100 ? when a cluster has no overlap with the spatiotemporal extent of an event si, w? ? ?100 makes si the least plausible candidate in the ranking. According to the formula in III-D, wAX also depends on the fraction of satisfied event constraints; N is the total number of constraints for an event candidate.

wAX =  N  ? wjAX , 1 ? j ? N (3)  Finally, we use the following instructions to compare two event candidates e1 and e2: when e1 is subsumed by e2, mpij for each event candidate is normalized using the formula in equation 4, in which ei ? e1 and ej ? e2, otherwise, ei.m  p ij = ei.wAX . The candidate with the highest m  p ij is the  most plausible subevent category.

ei.m p ij =  ei.wAX max(ei.wAX , ej .wAX)  + ei.wg  max(ei.wg, ej .wg) (4)  When a subevent category is inferred from a set of observa- tions, it will not be considered again as a candidate for the next set of observations. Event inference halts if no more subevent category is left to be inferred from the domain event ontology.

Fig. 4: The Big Picture. Photos and their metadata are stored in photo-base and metadata- base respectively. Using user info, including events? type, time, and space in a user?s calendar, a photo stream is queried, and its metadata is passed to clustering. In descriptor validation, a set of consistent descriptors is obtained from the cluster that best represents an individual photo ? the component event inference uses these descriptors in addition to a domain event ontology that is selected according to user info. Event Ontology Augmentation derives the most relevant subevent categories to the input photo stream, and refines the derived categories by propagating their instances with the information extracted from data sources. The subevent tags are then validated using external sources.

These tags are added to the event ontology (extension) ? the extended event ontology is used in filtering that integrates visual concept verification tool. In this stage, first, irrelevant cluster branches are pruned. Next, for each matched cluster, less relevant photos to a subevent tag are filtered. The output is a set of photos labeled with some tags; these tags are then stored as new metadata for the photos. The remaining photos are tagged as miscellaneous.

E. Refinement, Validation, Extension  The inferred subevent categories E? are refined with the context data extracted from data sources in the bucket B, through the refinement process. First, let us elaborate this process by introducing the notion of seed event, which is an instance of an inferred category in E?, which is not yet augmented with information. An augmented seed-event is an expressive event tag. The seed-event is continuously refined with information from multiple sources.

Our algorithm uses a similar strategy to what we described earlier in subsection III-C. The only difference is that the attributes of a data source at each iteration is supplemented by the user information and the attributes of a seed-event (I) that is represented with the same schema that is described in the event ontology. Given a sequence of input attributes, if a data source returns an output-array of size K, then our algorithm creates K new instances of events with the same type as in the seed-event, and augments them with the information in the output-array. The augmented seed-events are added to I for the next iteration; I is constantly updated until all the event categories in E? are augmented, and/or there is no more data source (in the bucket B) to query. To avoid falling into an infinite loop of querying data sources, we set the following condition: a data source cannot be queried more than once for each seed-event. Moreover, we defined some queries man- ually that are expressed through the relative spatiotemporal relationships in the event ontology, and the augmented seed-     Fig. 5: Filtering Operation.

events; these queries are used to augment the seed-events with relative spatiotemporal properties. When a seed-event gets augmented with information, our technique validates the event tag by using the event constraints, augmented event attributes, and a sequence of entailment rules that specify the cancel status for an event. For instance, if the weather attribute for an event is heavy rain, and the weather constraint fine weather is defined for an event, then the status of the event tag becomes canceled; another example is when the place of occurrence related to the event tag is closed during its time of occurrence. After the validation, event tags are added to the domain event ontology by extending event classes through typeOf relationship. This step produces an augmented event ontology that is the extended version of the prior model, see fig 1.



IV. FILTERING Filtering is a two-step process; during the first step, redun-  dant and irrelevant clusters are pruned from the hierarchical cluster structure which was produced by the clustering com- ponent, see fig 5-step-1. Equation 5 and 6 describe the prune- rule, and match-rule that we use in this step. traverse-rule in equation 7 is used to visit cluster nodes? c implies cluster.

?InsideST (tage , c)? Prune(c). (5) InsideST (c , tage)?Match(c, tage). (6)  InsideST (tage , c) ? hasChild(c)? Trvs(c.child). (7)  The second step filters redundant photos from the matched cluster, see fig 5-step-2. This is accomplished by applying the context and visual constraints of the expressive tag that is matched to the cluster. We used a concept verification tool5 to verify the visual constraints of events using image features. This tool uses pyramids of color histogram and GIST features. Filtering operation is deeply guided by the expressive tags. During this operation, subevent relations are used for navigating the augmented event model. Expressive event tags are stored in metadata-base, as the new metadata for photographs.

5http://socrates.ics.uci.edu/Pictorria/public/demo

V. EXPERIMENTS AND EVALUATIONS We focused on the three domain scenarios vacation, profes-  sional trip, and wedding. Some of the statistics related to these domain models includes the number of event types (i.e., event classes) and relationships; vacation, professional trip, and wedding domains approximately include 16, 24, and 18 event classes respectively. The relationships used in these domain models are mostly derived from the core event ontology which is described in section III-A. We used some of the concepts from currently available models such as Travel.owl6 , and wikipedia description to model the event types for coverage of the vacation life events.

1) Experimental Data set: We crawled Flickr, Picasaweb, and our lab data sets. Based on the assumption that people store their personal photos according to events,we collected the data sets based on time, space, and event types (like travel, conference, meeting, workshop, vacation, and wedding). We developed a Java-based crawler that uses Flickr?s photo search api to download photos. We also used the public service ScraperWiki 7 to develop a crawler to download personal albums from Picasaweb. The crawlers were used to download about 700 albums of the day?s featured photos. In addition, we crawled photo albums uploaded since the year 2010; the reason was that most of the older collections did not contain geo- tagged photos. After 4 months, we collected 84,021 albums (about 6M photos) from which only 570 albums (about 60K photos) had the required EXIF information containing loca- tion, timestamp, and optical camera parameters. We ignored the albums a) smaller than 30 photos, b) with non-English annotations. The average number of photos per album was 105. We used the albums from the most active users based on the amount of user annotation; we ended up with a diverse col- lection of 20 users with heterogeneous photo albums in terms of time period and geographical sparseness. The geographic sparseness of albums ranged from being across continents, to cities of the same country/state. Some of the users return to prior locations, and some do not. Fig 6 sketches the geographic distribution of our data set. We noticed that data sources do not equally support all the geographic regions; for instance,only a small number of data sources supported the data sets captured inside India. The photos for vacation/professional-trip domains have higher temporal and geographical sparseness compared to photos related to wedding domain. The number of albums for vacation domain exceeds the other two.

2) Experimental Set-up: We picked the 4 most active users (based on the amount of user annotation) from our non-lab, downloaded data set, and 2 most active users from our lab data set (based on the number of collections they own). As ground- truth for the lab data set, we asked the owners to annotate the photos using their personal experiences, and an event model that best describes the data set, while providing them with three domain event ontologies (wedding, professional trip, and vacation). For the non-lab data set, the ground truth provides a manual and subjective event labeling done by the very owner of the data set being unaware of the experiments.

Because of the subjective nature of the non-lab data set, the event types that were not contained in the event domain ontology are replaced with event type miscellaneous that is an event type in every domain event ontology in this work. For each experiment, we compute standard information retrieval  6 http://protege.cim3.net/file/pub/ontologies/travel/travel.owl 7https://scraperwiki.com     Fig. 6: Data set geographical distribution. The black bars show the number of albums in each geographic region, and the gray bars show the number of data sources that supported the corresponding geographic region.

measures (precision, recall, and F1-measure), for the event types used in tags. In addition to that, we introduce a measure of correctness for event tags. The score is obtained based on multiple context cues. For instance, label meeting with Tom Johnson at RA Sushi Japanese Restaurant in Broadway, San Diego, during time interval ?blah? in a sunny day, in an outdoor environment, specifies type of the event, its granu- larity in the subevent hierarchy, place, time, and environment condition. We developed an algorithm that evaluates each cue with a number in the range of 0 to 1 as follows: 1) event type: wrong = 0, correct = 1, somehow correct = LpLTP such that Lp is the subevent-granularity level for a predicted tag and LTP is the subevent granularity level for the true-positive tag (the predicted tag is the direct or indirect superevent of the true-positive tag i.e., LpLTP ? 1); 2) place: includes place name, category and geographical region. If the place name is correct, score 1 is assigned and the other attributes will not be checked. Otherwise, 0 is assigned; for the category and/or geographical region if correct, score 1 is assigned, and 0 otherwise. The average of these values represent the score for place; 3) for weather, optical, and visual constraint: wrong=0, correct =1, unsure = 0.5; 4) time interval: if the predicted event tag occurs anytime during the true-positive event tag, 1 is the score, otherwise 0. The average of the above scores represents the correctness measure for a predicted event tag. We introduce average correctness of annotation that is calculated using the formula in equation 8, in which wj is the score for the jth predicted event tag.

correctness =  ?L j=1 wj  L (8)  We also introduced the metric context in equation context = 1 ? Err to measure the average context provided by data sources for annotating a photo stream. In this equation, pa- rameter Err is the average error related to the information provided by data sources used for annotating a photo stream (0 ? Err ? 1); the following guidelines are applied automat- ically, to measure this value: (a) if the information in a data source is related to the domain of a photo stream, but it is irrelevant to the context of the photo stream, assign error-score 1. For instance, data source TripAdvisor returns zero results related to Things-To-Do for the country at which a photo stream is created. Also, if a photo stream for a vacation trip  Fig. 7: Role of context in improving the correctness of event tags.

does not include any picture taken in any landmark location, TripAdvisor does not provide any coverage; (b) assign error- score 0 if the type of a source is relevant as well as its data (i.e. non-empty results); (c) if the data from a relevant source is insufficient for a photo stream, assign error-score 0.5. For instance, only a subset of business venues in a region are listed in data source Yelp; as a result, the data source returns information for less than 30% of the photo stream; (d) finally, for a data source, multiply the error-score by a fraction in which the numerator is the number of photos tagged using this data source, and the denominator is the size of the photo stream. Do this for all the sources and obtain the weighted average of the error-scores. The result is the value for Err.

The implication of our result in fig 7 is as follows: while the correctness of event tags (for a photo stream of an event) peaks with the increase in context, relatively, smaller percentage of photos are tagged using non-miscellaneous events, and larger percentage of photos are tagged using miscellaneous event.

This means if the suitable event type for a group of photos does not exist in an event ontology, the photos are not tagged with an irrelevant non-miscellaneous event; instead, they are tagged with miscellaneous event which means other. The right side of the figure indicates that even though the number of mis- cellaneous and non-miscellaneous event tags does not change, the correctness is still increasing; this means that the tags get more expressive since more context cues are attached to them.

The quality of annotations is increased when more context information is available. This shows that event ontology by itself is not as effective as augmented event ontology. We demonstrate three classes of experiments in table I. This table shows the average values (between 0 to 1) for the measure metrics discussed earlier (precision, recall, F1, correctness).

We use the work proposed in [13] as a baseline. It is based on space and time to detect event boundaries in conjunction with using English album descriptions. This baseline approach, with F1-measure about 0.6 and correctness of almost 0.56, shows promising results, and illustrates that time and space are important parameters to detect event boundaries. On the other hand, the baseline approach is limited to using only spatiotemporal containment for detecting subevent hierarchy, it does not support other types of relationships among events (like co-occurring events, relative temporal relationships) and other semantic knowledge about the structure of events. In addition, it requires human-induced tags which are noisy.

For the second set of experiments, we use an event domain ontology without augmenting it with context information. This approach gives worse results since the context information is disregarded during detecting event boundaries. It provides the F1-measure of almost 0.32 and correctness of 0.13. Our last     Fig. 8: CPU-Time for experimental data sets of the 6 most active users. Each data set is represented by its owner, domain type, source, and size. The domain wed implies wedding domain.

experiment leverages our proposed approach, and achieves F1- measure of about 0.85, and correctness of 0.82. Compared to our baseline approach, we obtain about 26% improvement in the quality of tags which is a very promising result.

3) CPU-Performance: We investigate the running time for event ontology augmentation, and visual concept verification in fig 8, through a two-stage process described below. Fig 8 illustrates the results for data sets of two sources i.e., lab, and non-lab (including Flickr, and Picasaweb), and three event domains.

Stage 1: Intra-domain comparison : In general, we found smaller number of context sources for wedding data sets compared to the other two domains; as a result, the event ontology augmentation process exits relatively faster, and the running time for the concept verification process increases. We observed the correctness of event tags degrades when event ontology augmentation process exists fast. This observation confirms the findings of fig 7.

Stage 2: Intra-source comparison: Within each domain, we compared the cpu-performance among lab and non-lab data sets. We noticed that the augmentation process exits relatively faster for non-lab data sets. The justification for this observation is that we could obtain user-related context like facebook events and check-ins from our lab users (U3, U4), but such information was missing in the case of non-lab data sets. This absence of information impacts wedding data sets the most, since the context information in the wedding scenario largely includes personal information such as guest list, and wedding schedule; such information is not publicly available on public photo sharing websites. In professionalTrip scenario, this impact is smaller than wedding, and larger than vacation; the missing personal information originates from the lack of context information related to personal meetings, and confer- ence schedules. In vacation scenario, data sources are mostly public; only a small portion of context information comes from the user-related context such as flight information,and facebook check-ins; therefore, we did not find a significant change in the cpu-time between lab and non-lab data sets in the vacation domain.

Users U1 U2 U3 U4 U5 U6  baseline[13]  prec 0.65 0.58 0.39 0.53 0.74 0.61 recall 0.89 0.4 0.61 0.64 0.8 0.43 f1 0.75 0.47 0.48 0.6 0.77 0.5 corr 0.63 0.62 0.52 0.62 0.28 0.69  event ontology  prec 0.41 0.17 0.3 0.48 0.12 0.53 recall 0.4 0.2 0.5 0.43 0.24 0.3 f1 0.4 0.18 0.37 0.45 0.16 0.38 corr 0.2 0.08 0.12 0.2 0.03 0.19  proposed  prec 0.74 0.83 0.95 0.92 0.88 0.79 recall 0.91 0.93 0.88 0.7 0.97 0.82 f1 0.81 0.88 0.91 0.79 0.92 0.8 corr 0.8 0.75 0.85 0.79 0.9 0.88  TABLE I: Experimental results for automatic photo annotation for the data sets owned by the 6 most active users.



VI. CONCLUSIONS Our proposed technique addresses a broad range of both  basic and applied research challenges to achieve a powerful event-based system that can adapt to different scenarios and applications such as those in intelligence community, multi- media applications, and emergency response.

