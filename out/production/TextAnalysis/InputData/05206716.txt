Fast Mean Shift by Compact Density Representation

Abstract  The Mean Shift procedure is a well established clustering technique that is widely used in imaging applications such as image and video segmentation, denoising, object track- ing, texture classification, and others. However, the Mean Shift procedure has relatively high time complexity which is superlinear in the number of data points. In this paper we present a novel fast Mean Shift procedure which is based on the random sampling of the Kernel Density Estimate (KDE).

We show theoretically that the resulting reduced KDE is close to the complete data KDE, to within a given accu- racy. Moreover, we prove that the time complexity of the proposed fast Mean Shift procedure based on the reduced KDE is considerably lower than that of the original Mean Shift; the typical gain is of several orders for big data sets.

Experiments show that image and video segmentation re- sults of the proposed fast Mean Shift method are similar to those based on the standard Mean shift procedure. We also present a new application of the Fast Mean Shift method to the efficient construction of graph hierarchies for images; the resulting structure is potentially useful for solving com- puter vision problems which can be posed as graph prob- lems, including stereo, semi-automatic segmentation, and optical flow.

1. Introduction Kernel density estimation and the Mean Shift clustering  procedure are well accepted techniques in the field of com- puter vision, see for example [10, 5] and references therein.

Mean Shift is widely used in many imaging applications such as image and video segmentation [5, 20], denoising [3], object tracking [7], and texture classification [11], inter alia. Roughly speaking, the Mean Shift procedure consists of two steps: (i) the construction of a probability density which reflects the underlying distribution of points in some feature space, and (ii) the mapping of each point to the mode (maximum) of the density which is closest to it.

One of the main difficulties in applying Mean Shift based  clustering to big data sets is its computational complexity which is superlinear in the number of data points. There are several existing techniques which have been developed to increase the speed of Mean Shift. DeMenthon and Megret [8] use an iterative, scale-space like approach to Mean- Shift, with increasing bandwidth. Yang et al. [23] use the Fast Gauss Transform to speed up the sum in the Mean Shift iteration. Guo et al. [13] decompose the Mean Shift sum into a number of local subsets. Paris and Durand [17] use the separability of the multidimensional Gaussian kernel to perform d separate one-dimensional convolutions. Wang et al. [21] use a clever data structure, the dual tree, to speed up Mean Shift. Also somewhat related is the paper by Vevaldi and Soatto on ?Quick Shift? [19].

In this paper, we introduce a new technique, which deals directly with the description or space complexity of the Ker- nel Density Estimate, which is linear in the number of data points. The main focus of this paper is a novel fast Mean Shift procedure which is based on the computation of a pared down Kernel Density Estimate, using sampling tech- niques. We show theoretically that the resulting reduced KDE is close to the complete data KDE, to within a given accuracy. The time complexity of the proposed fast Mean Shift procedure based on the reduced KDE is considerably lower than that of the original Mean Shift; the typical gain is of several orders for big data sets. We verify this large gain experimentally in a number of segmentation experiments.

The method for constructing the more compactly rep- resented KDE is easy to implement; furthermore, the new technique is orthogonal to the existing techniques for speed- ing up Mean Shift, and in most cases, can be implemented along side these older methods for even better performance.

Note that while there has been some work in this regard (compact KDE representation) in the past, it mainly in- volves techniques which rely heavily on neural networks and self-organizing maps [18, 22, 12]. Neural networks lead to a high implementation complexity (as well as other issues) which we wish to avoid in this work.

The remainder of the paper is organized as follows. In Section 2 we first briefly review the KDE framework and the     Mean Shift algorithm for mode finding. Then we propose a sampling method for constructing the compact representa- tion of KDE. Next, based on this compact KDE, we define our Fast Mean Shift procedure, and analyze its computa- tional complexity as compared to the standard Mean Shift.

We also provide a rule for optimal bandwidth selection for the Fast Mean Shift procedure. In Section 3, we demon- strate the performance of the proposed method on the tasks of image and video segmentation, and compare results with standard Mean Shift. In Section 4, we present a new appli- cation of the Fast Mean Shift method for constructing mul- tiscale graph hierarchies for images. Section 5 concludes.

2. Fast Mean Shift In this section, we present the Fast Mean Shift algorithm.

After a brief review of the standard Mean Shift algorithm, we move to a general discussion of compact representations for the kernel density estimate, and prove that a sampling- based scheme gives us such a representation. Given this compact representation, we then propose a fast technique for Mean Shift, and show that its complexity is consider- ably lower than that of the standard Mean Shift. We then examine a soft clustering variant of this algorithm, and con- clude with a discussion of the optimal choice of bandwidth.

2.1. Review of Mean Shift  In this section, we review the ordinary Mean Shift pro- cedure. We begin by briefly defining the Kernel Density Estimate (KDE). Our data is a set of points {xi}ni=1, some- times referred to as feature vectors, living in a Euclidean space: xi ? Rd. In computer vision applications, there is generally one such vector per pixel (or voxel in the three- dimensional case); the vector may be colour, colour aug- mented with position, texture, and so on. The Kernel Den- sity Estimate (KDE) of this data is then taken to be  f(x) = n  n? i=1  KH(x? xi)  This is an estimate of the probability density underlying the points, where the function KH(x ? xi) is essentially a bump centered at xi. More specifically, we takeKH(z) = |H|?1/2K(H?1/2z), where the kernel K is itself a prob- ability density with zero mean, identity covariance, and satisfying lim?x??? ?x?dK(x) = 0. Common choices for K include Gaussian, uniform, and (multidimensional) Epanechnikov kernels. In many cases of interest, H will be diagonal; in general, though, this need not be the case.

The Mean Shift algorithm is essentially a hill-climbing algorithm; that is, starting at any point x, the Mean Shift algorithm is an efficient iteration scheme which brings x up the hill of the KDE, finally stopping at the local maxi- mum (or mode) of the KDE in whose basin of attraction x  lies. To specify the Mean Shift iteration formally, let us sim- plify the form of the kernel, and take the radially symmet- ric form K(x) = ck(?z?2), where k is a one-dimensional profile, such as the one-dimensional Gaussian, uniform, or Epanechnikov profiles, and c is a normalization. Denoting g = k?, then the Mean Shift iteration is given by  x?  ?n i=1 xig  (??x?xi h  ??2)?n i=1 g  (??x?xi h  ??2) ?M(x) (1) Iterating an infinite number of times is guaranteed to bring x to the mode in whose basin of attraction it lies. The ad- vantage of this procedure over ordinary gradient ascent is the lack of need to set a time-step parameter (note no such parameter is present in the above expression); practically, the mean-shift tends to converge in a very small number of steps, typically around 5.

In order to use the Mean Shift algorithm for segmenta- tion or clustering on the set {xi}ni=1, one may run the iter- ations starting at each data point. Denoting by M1(x) = M(x), M2(x) = M(M(x)) and so on, we map each point xi to M?(xi) (though recall that typically, M5(xi) will do the job). Since there are a much smaller number of modes than there are points, this is a form of segmentation or clus- tering, which works very well in practice in a number of applications [20, 6, 14].

2.2. A More Compact KDE: the Problem  Without explicitly computing the complexity of Mean Shift clustering (we put off this exercise until Section 2.5), we note that the running time will be superlinear in n, the number of data points. If we are concerned with large im- ages coming from today?s cameras, n can be easily be in the range 107. Worse yet, we may be interested in using Mean Shift on video or on 3D imagery, such as CT or MRI im- ages; in this case, n of the order of 108 or higher is easily conceivable. It is therefore worthwhile to consider a faster Mean Shift technique. One of the main speed bottlenecks is the description complexity of the KDE, which is O(n); we thus begin by discussing the problem of finding a more compact description for the KDE.

We wish to find a KDE which is close to f(x), but which uses many fewer points to generate it. That is, we wish to solve the following problem:  min {x?j}mj=1, H?  D(f?(?), f(?)) subject to f?(x) = 1 m  m? j=1  KH?(x?x?j)  where D is a distance measure between probability densi- ties and m is a fixed number of points, with m ? n. This optimization seeks to find the data points x?j underlying a second KDE f? which well-approximates the original KDE f , but using many fewer points.

There are several natural choices for the distance mea- sure D, ranging from Lp type distances to more informa- tion theoretic measures such as the Kullback-Leibler diver- gence. In all such cases, the resulting optimization problem is hard to solve globally; this is due the fact that the x?j ap- pear within the kernel K, leading a non-convex optimiza- tion, for which global solutions cannot generally be found efficiently. However, f and f? are more than functions; they are both densities, and this fact gives us a means of finding an efficient solution to the compact representation problem.

2.3. A More Compact KDE via Sampling  Our solution is very simple: we choose the samples x?j by sampling from the distribution given by f(?). Before dis- cussing the logic of this approach, note that such sampling is quite feasible in general, and may be implemented as a three-step procedure:  Theorem 1 For each j = 1, . . . ,m, suppose we construct x?j as follows:  1. choose a random integer rj ? {1, . . . , n};  2. choose a random sample ?j from K(?);  3. set x?j = xrj + H 1/2?j .

Then x?j is a proper sample of f .

Proof: See [9].

As long as one can sample easily from K(?), as is the case for the Gaussian, uniform, and Epanechnikov kernels, then the procedure is very simple.

Now, the main question becomes: if we construct a KDE f? based on the random samples x?j , will it be close to the true KDE f? Of course, the KDE f? itself will be a random variable (i.e. a random function), and thus any result we prove will be of a probabilistic nature. In fact, we have the following result, which guarantees that f and f? are close in expectation, in an L2 sense:  Theorem 2 Let f be KDE with n points, and let f? be a KDE constructed by sampling m times from f , as above, and assume a diagonal bandwidth matrix H? = h?2I. Let the expected squared L2 distance between the two densities be given by J = E[  ? (f(x)? f?(x))2dx]. Then  J ? 4Ah?+A2h?2V + B mh?d  + ABV  mh?d?1 (2)  where A,B, V are constants which do not depend on h? or m.

Proof: See [9].

The meaning of this theorem is straightforward: the two KDEs f and f? will be close (in expectation) if m is large  enough, and if the bandwidth h? is chosen properly1 as a function of m. This is precisely what we want in a compact representation: the description complexity of the KDE has been reduced from O(n) to O(m), but we generate close to the same density, in the expected L2 sense.

We turn to the issue of bandwidth selection in Section 2.7; in the next section, we turn to the central question of how to use our reduced representation KDE in Mean Shift Clustering.

2.4. Fast Mean Shift  How can we incorporate our more compact KDE into the Mean Shift clustering algorithm? We use the following three step procedure:  1. Sampling: Take m samples of the density f to yield {x?j}mj=1. Form the new density f?(x) =?m  j=1Kh?(x, x?j).

2. Mean Shift: Perform Mean Shift on each of the m samples: x?j ? M??(x?j). Here, M? indicates that we use f? (rather than f ) for the Mean Shift.

3. Map Backwards: For each xi, find the closest new sample x?j? . Then xi ? M??(x?j?).

In Step 1, we construct the reduced KDE, and in Step 2, we perform Mean-Shift on this smaller KDE. Given these modes in the reduced KDE, the question is how to map backwards to the original data. We deal with in Step 3, by mapping from each point in the original data (xi) to the closest point in the reduced data (x?j? ), and from there to the mode to which that point flows (M??(x?j?)).

The key speed-up occurs in the second step; instead of using all n samples to compute the Mean Shift, we can use the reduced set of m samples. In the next section, we will quantify the precise theoretical speed-up that this entails; for the moment, note that in a naive implementation it can lead to a speed-up of n/m, which can in pratice be greater than 100, and often considerably larger than that.

2.5. Complexity Analysis  The key aspect in the computation of complexity is the speed of the nearest neighbour search. Note that in each Mean Shift iteration, see Equation (1), one must compute the nearest neighbours out of the n samples xi to the point in question, x. Suppose that we have a data structure which permits nearest neighbour queries in time q(n), and requires a preprocessing time of p(n) to compute; we will look at examples of such structures shortly, but for now, we will  1The question of how precisely to choose the bandwidth is discussed in Section 2.7. For the moment, it is sufficient to note that h? should go to 0 as m goes to ?, but not too fast, i.e., in such as way that mh?d ? ? as m??.

leave them as general. In this case, each Mean Shift iter- ation requires O(q(n)) time to compute; and assuming, as is the case in practice, that O(1) iterations are required for convergence, then the cost of running the algorithm on all n data points (i.e. the overall data reduction algorithm) is then  Torig(n) = O(p(n) + nq(n))  How much faster is our proposed algorithm? Looking back at Section 2.4, we may break down the complexity for each step. Step 1, the sampling step, is O(m). We have already computed the complexity of Step 2, the Mean Shift step; this is simply the above expression, but specified form rather than n samples, i.e. O(p(m)+mq(m)). In Step 3, we must map backwards; that is, for each of the n original sam- ples, we must find the closest amongst the m new samples.

Using our data structure (whose preprocessing time we have already accounted for in Step 2), this requires O(nq(m)).

The total is then  Treduce(n,m) = O(m+ p(m) +mq(m) + nq(m)) = O(p(m) + nq(m))  Comparing this with the expression for Torig(n), and since n = ?(m), we have clearly reduced the complexity. By how much depends on the precise details of the data struc- ture used, and we now attempt to deal with this issue.

In the simplest case, we have no special data structure.

Then p(n) = 0, and the query time is linear in the number of elements q(n) = O(n). In this case, Torig(n) = O(n2), while Treduce(m,n) = O(nm), so the speed-up is a factor of n/m. In practical cases of interest n/m > 100, so this is quite an impressive speed-up.

Now, we may use more complex data structures for search. Voronoi Diagrams are not very useful when the dimension in which the data lives is d > 3 or so, as the preprocessing time is p(n) = O(ndd/2e). Other popular data structures, such as kd-trees, also have space complex- ity exponential in d [2]. There are a variety of approxi- mate nearest-neighbour algorithms, such as those based on locality sensitive hashing [2], which can lead to more effi- cient searches, if we are willing to find neighbours which are close, but not the closest. Indeed, if we choose an approximation factor of c ? 1 ? that is, we find points whose distance from the query are within a factor of c of the closest ? then we will have Torig(n) = O(n1+1/c  ),  and Treduce(n,m) = O(nm1/c ), leading to a speed-up of  O((n/m)1/c ); see for example [2]. Typically, we might  take c = 1.5, leading to a speed-up of about 10 if n/m ? 100. (Note, however, that such data structures often have excellent theoretical properties while being somewhat more difficult to implement in practice, see [2].)  2.6. Variant: Soft Segmentation or Cartoons  We propose the following variant to the Fast Mean Shift clustering algorithm, which leads effectively to a soft seg- mentation. Note that only the third step has changed; we reproduce the first two for clarity.

1. Sampling: Take m samples of the density f to yield {x?j}mj=1. Form the new density f?(x) =?m  j=1Kh?(x, x?j).

2. Mean Shift: Perform Mean Shift on each of the m samples: x?j ? M??(x?j). Here, M? indicates that we use f? (rather than f ) for the Mean Shift.

3. Weighted Map Backwards: For each xi and each x?j , compute a weight between them according to wij ? Kh(xi, xj), such that the weights sum to 1:  ? j wij =  1. Then xi ? ?m  j=1 wijM? ?(x?j).

Such a procedure will yield, instead of a piecewise con- stant segmentation, a piecewise smooth or cartoon-like seg- mentation.2 This is similar in spirit to the Mumford-Shah scheme [16], which seeks a piecewise smooth approxima- tion to the original image. We show examples of both soft and hard segmentation in Section 3.

2.7. The Optimal Bandwidth  Let us return to the expression for the L2 distance be- tween the original KDE f and the reduced KDE f? as given in Equation (2). Suppose we look at the asymptotic case, where m is sufficiently large and h? is sufficiently small; in this case, the upper bound on J is well-approximated by  L = 4Ah?+ B  mh?d  Now, we may try to find the bandwidth3 which minimizes L, takingm fixed; L is convex in h?, and setting its derivative to 0 yields  h??(m) = C1m?  d+1 and L?(m) = C2m?  d+1  where C1 and C2 are (uninteresting) constants. Given the original bandwidth h, this gives us a simple way of choosing h? for the reduced KDE; byy eliminating the constantC1, we get  h? = (n/m)  d+1h  2Note that if the kernel has finite support, then for a particular choice of h it is possible that all weights will be 0. Thus, soft segmentation using a kernel with infinite support, such as a Gaussian, is recommended.

3Note that our choice of bandwidth here is a scalar, yielding a diag- onal covariance. For work which tries to find an adaptive non-diagonal bandwidth matrix from the data, see [4].

Parrot Castle Girl Image Size 844? 636 960? 784 551? 735 Sampling m/n 1024 1024 1024 Speed-up Factor 401 1160 1543  Table 1. Timing results for image segmentation.

3. Segmentation Experiments  3.1. Image Segmentation  In the image segmentation experiment, we compare the performance of the proposed fast Mean Shift method with standard Mean Shift on three images. The feature vectors are taken to be 5-dimensional, and are constructed from the three Lab colour vectors of the individual pixels in the im- age, and of the two corresponding spatial coordinates (x and y). Table 1 summarizes the timing information, and the im- ages themselves are shown in Figure 1. In each example, the sampling factor was taken to n/m = 1024. Our complexity analysis indicates that we ought to expect a speed-up factor of about 1024; in fact, despite using fast nearest-neighbour queries (based on kd-trees, which despite the arguments in [2] tend to work well in low dimensions), the speed-up fac- tor in two of the three cases exceeded this sampling factor, as shown in Table 1.

In Figure 1, we show the results of the standard Mean Shift procedure (second column), the Fast Mean Shift pro- cedure (third column), and the soft segmentation variant of the Fast Mean Shift procedure (fourth column). Clearly, hard segmentation results obtained using the original and the fast Mean Shift methods look quite similar. As ex- pected, the soft segmentation is kind a piecewise smooth cartoon of the original image, with small details removed from the original image. Therefore it may be suitable for noise reduction tasks.

3.2. Varying the Sampling Factor n/m  In order to understand the effect of varying the sampling factor on segmentation, we ran the fast Mean Shift algo- rithm with various sampling factors n/m on the parrot im- age from the previous section. The images are shown in Figure 2. Note that the algorithm does relatively well even up to a sampling factor of n/m = 4, 096, as shown in (d); as the image itself contains a bit more than 5 ? 105 pixels, this corresponds to using only m = 131 randomly selected samples to construct the KDE, which is quite remarkable.

The fast algorithm fails at n/m = 16, 384; this is not sur- prising, as this corresponds to using only 32 randomly se- lected samples to construct the KDE. Indeed, at this high a subsampling rate, the algorithm produces quite different segmentations from one run to the next.

3.3. Video Segmentation  Figure 3 shows results of video segmentation on a 10 frame sequence, for two consecutive frames from a video sequence (see upper row). In this experiment we compared the use of our fast Mean Shift method in two configura- tions. In the first one (the middle row), a frame by frame segmentation was performed. In the second configuration, data from a time window of 10 frames was combined into a single large data set, and used to perform the first two steps (the sampling, and the Mean Shift) in the algorithm of Sec- tion 2.4, using a subsampling factor of 1024. (Note that in this example, it is possible to use a subsampling factor of 5000, though the results are slightly less clean.) Then, the third step (the mapping backwards) from Section 2.4 was applied to each frame. The results of the ?windowed? ver- sion look cleaner; for example, this version does not mis- classify the color of the aircraft, as is the case in the frame- by-frame procedure. Another option is to use the time axis as an additional feature axis, which might be useful for ap- plications such as target tracking. Clearly, the proposed fast method opens up some new possibilities for dealing with large spatio-temporal data sets.

4. Application: Graph Hierarchies In this section, we show how to use the Fast Mean Shift  algorithm in order to construct a hierarchy of graphs corre- sponding to a particular image. Having a multiscale struc- ture on image graphs can potentially be very useful, due to the number of computer vision problems which can be posed as graph problems, and solved using graph algo- rithms, such as graph cuts [15]. Examples of these prob- lems include stereo, semi-automatic segmentation, optical flow, as well as many problems that can be posed as Max- imum a Posteriori estimation over Markov Random Fields.

A multiscale graph hierarchy can be used to help speed up the solution to these problems in the usual multiscale way: the problem is first solved on the coarsest scale; the solution to this problem is then used to initialize the next scale up; and so on. In addition to making the solution faster, such as approach can also lead in some instances to more accurate solutions. See [1] for an example of another sort of graph hierarchy, based on algebraic multigrid techniques.

4.1. A Graph Hierarchy  We define a continuous graph hierarchy using Mean Shift as follows. As before, our data consists of the set of feature vectors {xi}ni=1, where in most applications of interest, each data point corresponds to a pixel in an im- age. Denote by fh(x) the KDE with bandwidth matrix H = h2I, and if x is a mode of fh, let B(x) be the basin of attraction of x, i.e. B(x) = {y ? Rd : M?(y) = x}.

The graph corresponding to bandwidth h is denoted by     (a) (b) (c) (d) Figure 1. Image segmentation example: (a) the original image (b) the result of standard Mean Shift segmentation (c) the result of fast Mean Shift segmentation (d) the soft segmentation variant of the fast Mean Shift. Note similar hard segmentation results in (b) and (c); the proposed method is more than 1000 times faster than the original Mean Shift.

(a) (b) (c) (d) (e) Figure 2. Effect of varying the sampling factor n/m. (a) n/m = 64 (b) n/m = 256 (c) n/m = 1, 024 (d) n/m = 4, 096 (e) n/m = 16, 384.

Gh = (Vh, Eh,Wh), where  Vh = {x ? Rd : x is a mode of fh(?)} Eh = {(u, v) : u, v ? V (h), B(u) ?B(v) 6= ?}  and the edge weights are given by  Wh(u, v) = ?  x?B(u)  ? y?B(v)  Kh(x, y)dxdy  The definition of the vertex set and edge set is quite natu- ral in terms of Mean Shift: the vertex set is just the set of  modes, and the edge set is the set of pairs of modes whose basins of attractions are adjacent in the feature space. The edge weights are also defined in a reasonable fashion, by aggregating the similarity between points in neighbouring basins of attraction, though many other definitions are pos- sible as well. In practice, we will approximate the integral in the edge weight definition by its corresponding sum.

It is clear that this continuous graph hierarchy has the following desirable properties:     Figure 3. Video segmentation example. Upper row: two origi- nal frames from a video sequence. Middle row: the result of the frame-by-frame segmentation using the proposed method. Lower row: the result of the 10-frame window based segmentation using the proposed method.

Theorem 3 Given the graph hierarchy as defined above.

Then the graph corresponding to h = 0 has the property that V0 = {xi}ni=1, while the graph corresponding to h = ? has the property that |V?| = 1.

Proof: See [9].

In other words, we move from a graph whose vertex set comprises the original pixels, to a graph with a single ver- tex. This is very much what we would expect from a multi- scale hierarchy.

Now, to convert from a continuous graph hierarchy to a discrete one, we simply choose a fixed number of band- widths, i.e. G` = Gh` , where the levels run from ` = 0 (finest) to ` = L (coarsest). In particular, in d-dimensional space, it is natural to choose h` ? 2  dh`?1, so that the vol-  ume of coverage doubles at each stage. In this case, we expect L ? log n; using a similar line of argument to that of Section 2.5, we may show that the computing this dis- crete graph hierarchy using ordinary Mean Shift requires  O(Ln2) = O(n2 log n) time. (This assumes a naive near- est neighbour data structure, see Section 2.5 for a more in- depth discussion.)  To use the Fast Mean Shift algorithm, we make two ob- servations. The first is that for the initial level (` = 0) of the hierarchy, we may use m0 ? n samples; this is precisely what we have shown in Sections 2 and 3, from the theo- retical and experimental points of view, respectively. The second observation is that for all subsequent levels, we may use even fewer samples, due to the increasing bandwidths of these levels. We have that h` ? 2  dh`?1, and we may  convert this statement about bandwidths to one about the number of samples to use at each level of the graph, using the result of Section 2.7 for bandwidth selection. In partic- ular, if m` is the number of samples required at level `, this leads to m` ? 2?  d+1 d m`?1, or for d large, m` ? 12m`?1.

Again, following the logic of Section 2.5, we have the com- plexity of the fast version of the discrete graph hierarchy as O( ?L  `=0 n2 ?`m0) = O(nm0).

Note that the speed up factor in computing the graph hi- erarchy is even better than that of Mean Shift itself; it is (n/m) log n, versus n/m for the Mean Shift speed up. In cases of interest where n ? 107 or 108, the log n factor is not insubstantial (i.e. 20? 25).

4.2. An Example  In Figure 4, we show part of a graph hierarchy for an image of a seashore, computed using the fast technique de- scribed above. The four images we show are for bandwidths h = 8, 16, 32, and 64. The number of vertices for these graphs are 266, 59, 11, and 3 respectively. We visualize the graphs by surrounding each contiguous region in white; note, however, that these contiguous regions are not the seg- ments themselves, and often a segment (and hence a vertex) consists of multiple such contiguous regions. This is due to the fact that the graphs are not necessarily planar. This is true in our example, as our feature vector is colour (Lab) rather than texture, without the addition of position; thus, the sandy textured region looks as though it contains many nodes, whereas in fact it contains only a few non-contiguous nodes. Even with this effect, one can see the graph simpli- fication as the bandwidth increases.

5. Conclusions We have presented a fast version of the Mean Shift al-  gorithm, based on computing a pared down KDE using a sampling procedure. We have theoretically demonstrated the closeness of the pared down KDE to the original KDE, as well as the superior complexity of the fast Mean Shift al- gorithm compared to the standard Mean Shift. We have ex- perimentally verified the algorithm?s speed, and shown its potential utility in clustering large data sets, such as video,     (a) (b)  (c) (d) Figure 4. A visualization of the graph hierarchy. (a) h = 8, |V | = 226 (b) h = 16, |V | = 59 (c) h = 32, |V | = 11 (d) h = 64, |V | = 3. See accompanying discussion in text.

and in the computation of complex data structures such as the graph hierarchy presented. It is our hope that the algo- rithm can find immediate application in fast segmentation of existing large data sets, such as medical images.

