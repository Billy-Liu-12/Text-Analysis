Association Rules Mining Based on

Abstract?Association rules mining is an important data mining task.

this paper emphatically analyzes realization skill and defects of  existing algorithms. On the basis, a novel measure, named statistical correlation, which can indicate the correlation degree of multi-items in a rule, is put forward to cut association rules with independent or negative correlation, and its concept, calculating formulas and primary characteristics are defined. In order to conveniently use statistical correlation to cut redundancy rules, a fast association rules mining algorithm, called F-Fminer, is designed with two new data structures UFP-Tree and FP-Forest which use multi-trees  structure to store data. F-Fminer adopts divide and conquer strategy to mine frequent itemsets for every UFP-Tree basing on deepth-first searching. It can be seen from experimentation that the method in this paper has greatly enhanced mining efficiency and reduced a lot of redundant rules than other classical algorithms.

Keywords-association rules; Statistical correlation; UFP-Tree;  FP-Forest

I. INTRODUCTION  Association rules mining is an important problem in the data mining area and has a wide range of applications. It was introduced by Agrawal et al. [1]. A transaction database is a database containing a set of transactions and each transaction is  associated with a transaction id. Let 1 2{ , , , }ND t t t=  be a transaction database and 1 2{ , , , }nI i i i= be the set of items appearing in D, where ( [1, ])it i N?  is a transaction and it I? .

Association rules are those implications that can be depicted  as X Y  , where X I? ,Y I? , X Y? = ? . The support of the rule X Y in D is the ratio of the number of transactions containing X and Y in the transaction sets to the number of all transactions. It is ( ) { : } /support X Y t X Y t D D= ? ? ? .The  confidence of the rule X Y  in the transaction sets is the ratio of the number of transactions including X and Y to the number of those including X. It is written as confidence ( X Y ), that  is to say, ( ) { : } / : ,confidence X Y t X Y t D t X t t D= ? ? ? ? ? .

Mining association rules can be divided into two steps: First, finding all itemsets having the highest supports; Second, generating association rules from frequent itemsets.

Many algorithms have been proposed to mine association rules over the past decade. However, most algorithms basing  on support-confidence structure are likely to appear the following problem: although X Y is a strong rule according with min_supp and min_conf, but X and Y are independent or negative correlation, so many redundant rules are generated.

Furthermore, many association rules mining algorithms remains a time-consuming process.

According to above-mentioned problems, we propose a novel measure statistical correlation to cut independent or negative rules. Then, a new algorithm based on dividing and conquering strategy, F-Fminer, is designed to mine association rules with two new data structure UFP-Tree and FP-Forest.



II. RELATED WORK  In existing association rules algorithms, Apriori[1] and FP-Growth[2] are the most influential mining algorithms.

Aporiori algorithm adopts layer by layer search iteration method to mine association rules, which is frequent k-1 itemset Lk-1 is used to search K itemset Lk. However, it needs to scan database when Lk is found. So Aporiori algorithm will generate a large number of candidate sets. Furthermore, Aporiori algorithm needs frequently scan database. Some researchers successively put forward improved Apriori algorithm, Such as AprioriTid[3], Partition[4]. However, most improved algorithms are still brought many candidate sets.

F-Growth is proposed by Han J et al, which compresses data items into FP-tree, then calls FP-growth method to divide FP-tree into some conditional pattern base. Then frequent itemsets are mined in conditional pattern base by depth-first method. F-Growth algorithm does not generate candidate itemsets, and only scans two times database. But it need excessive CPU consumption and storage cost. Some improved algorithms includes FP-Growth*[5], AFOPT [6].

To solve the existing problem of support-confidence structure, correlation has been adopted as an interesting measure. Xiong et.[7] made a study on the mining of strong correlation item pairs by using Pearson's correlation coefficients. Whereas, when the items and trade numbers are great, the algorithm still needs a mass of calculation. Zengyou H.[8] proposed an improving algorithm, called Tcp, based on the well-known FP-tree data structure.

In addition, other scholars also described their own viewpoints recently. Lee et al.[9] brought forward two new association algorithm including all-confidence and bond by means of pattern-growth technology. Kim et al.[10] put forward the concept of confidence-closed correlated patterns and illustrated the mining algorithm named as CCMine.



III. STATISTICAL CORRELATION  To get the association rules with real correlation, this paper put forward statistical correlation from the view point of statistics to compensate the deficiency of support-confidence.

Statistical correlation is defined as equation (1), which is named as scorrelation.

( ) ( )  ( ) ( ) (1 ( ))  i X Y  i X Y i X Y  D support X Y D support i  scorrelation X Y D support i D support i  ? ?  ? ? ? ?  ? ? ? =  ?  ? ? ?  (1)  The statistical meaning of scorrelation shows that if considering the items in  X?Y  composition a data cube with |X?Y| dimension, which are inter-independent in statistics.

Then, ( )  i X Y  support i ? ? ?  is the ratio of the expected transaction  number of the cube to the whole database. However, as few practical data is independent, therefore, the difference between the number of data points in the cube and expected number is great. If the transaction database is D and data is proportional distribution, then whether a data point is in cube or not can be  regarded as a Bernaulli random variable with  ( ) i X Y  support i ? ? ?  probability. According to central limit theorem, the distribution of point number in cube is approximate to Gaussian distribution on the basis of above-mentioned assumption. Thus,  the expected transaction number in cube is ( ) i X Y  D support i ? ? ?  ,  ( ) (1 ( )) i X Y i X Y  D support i D support i ? ? ? ?  ?? ? is standard deviation.

? Theorem 1  if Scorrelation X?Y 0, it denotes that the items in antecedent X and the consequent Y of an association rule are negative correlation, and the items have a relationship of restricting each other.

? Theorem 2  if Scorrelation X?Y =0, it means that the items in antecedent X and the consequent Y of an association rule are independent, and the items are not mutually influence.

? Theorem 3 if Scorrelation X?Y 0, it represents that the items in antecedent X and the consequent Y of an association rule have some degree correlation, and correlation is more and more strong with the Scorrelation increase.



IV. F-FMINER ALGORITHM DESCRIPTION  In order to conveniently use statistical correlation to cut redundancy rules, this paper puts forward two new data structures, named UFP-Tree and FP-Forest , then F-Fminer, a fast association rules mining algorithm is designed with divide and conquer strategy on deepth-first searching .

A. UFP-Tree and FP-Forest Construction  UFP-Tree is a mutation of FP-Tree. Fig.1 shows UFP- Tree and FP-Forest construction process on the data set showing in Table .

TABLE I. TRANSACTION DATA SET  Tid Itemset Tid Itemset Tid Itemset  100 {I1, I2, I5} 400 {I1, I2, I4} 700 { I1, I3}  200 {I2, I4} 500 { I1, I3} 800 {I1, I2, I3, I5} 300 { I2, I3} 600 { I2, I3} 900 {I1, I2, I3}  At first, frequency 1-itemsets, named L = {I5, I4, I1, I3, I2} are got by one-time scanning database, which are arranged according to support ascending order. It is different with other frequency itemsets mining algorithm, that UFP-Tree doesn?t use item head table. The root of the UFP-Tree is an item in L which can identify this tree, not null, and seeks data with top- down method.  So the length of L is the number of UFP-Tree.

Then, database is scanned second time, frequent itemset tables are obtained for every transaction, which are defined as F. For transaction T100, frequent itemset table which is arranged according to support ascending order, F1= {I5, I1, I2}, and the first itemset I5 is treated as root of UFP-Tree, defined this tree as TI5, the other itemsets I1, I2 are insert TI5. Also, F2= {I4, I2} for transaction T200, the first itemset I4 is the root TI4, the itemset I2 is inserted in TI4. At the same time, one-dimension array ATi is applied to store the counts of other nodes of UFP- Tree. In the same way, multi-trees structure is built when every transaction is operated and FP-Forest is constructed by UFP- Trees for better managing data.

Figure 1. UFT-Trees and FP-Forest construction  The pseudocode of UFP-trees and FP-forest building algorithm is as following.

Algorithm FP-forest( ) Input: transaction database D; Min_support; Output: FP-forest  Scan D, get frequent 1-itensets L; // the length L is n; Build n Trees TIi using the items in L as root Node and n one-dimension Ai ; Scan D for second time  For every transaction t D {Get frequent item table F according the order of L;  T I5  I5:2  T I4  I4:2  T I1  I1:6  T I3  I3:6  T I2  I2:7  I1:2  I3:1 I2:1  I2:1  I1:1  I2:1  I2:2 I3:3  I2:1  I2:2     I4  I1 I3  I2  A I5     I1 I3  I2 A I4    I3  I2  A I1  2I2  A I3     Find the tree TF , which root is the first item of F; Insert other node of F into TF, Ai stores the count of node in tree TF}  B. F-Fminer algorithm realization  According to study the structure of UFP-Tree, a type tree, named single frequent branch, is found that it can get frequent itemsets by easy enumeration.

? Definition 1 single frequent branch. When traversing an UFP-tree in top-down, If the count of a node of UFP-tree is less than the minimum threshold value of support and the count of father node is greater than or equal to the minimum threshold value of support, and every ancestor node only have one child node, then the UFP-tree is defined as single frequent branch.

Due to use multi-trees to store data, it is possible that the items in a UFP-tree do not exist in other UFP-trees. So right shift combination operation is designed to avoid some items loss in frequent itemsets.

? Definition 2 right shift combination operation.  When a UFP-tree has been mined, the branches of UFP-tree are searched, and the first node is found of branches.

Then the branches are inserted some UFP-tree which root node is same with the first node of branches.

The pseudocode of F-Fminer is as following:  Algorithm F-Fminer( )  Input F-Forest structure; Min_support; Min_confidence; Min_scorrelation;  Output: Association rules For each UFP-Tree TIi in FP-Forest  {Scan UFP-Tree TIi; If  TIi is single frequent branch  Get combination of root node and TIi? nodes, output combination as frequent itemsets;  Else find frequent 1-itemsets L If  the length L=1 Get combination of frequent 1-itemsets and root node as frequent itemsets;  Get subsets of frequent itemsets; Calculate Scorrelation and confidence of  generating rules.

If Scorrelation Min_ Scorrelation  confidence Min_confidence Then Output association rules; Else F-Forest(L), build new F-Forest; F-Fminer(new F-Forest); TIi right shift  combination operation; }  Fig.3-5 and Table  shows the mining association rules process on the Ti5. Therein, Min_support is equal to 0.22; Min_confidence is equal to 0.8; Min_scrrelation is equal to 0.2.



V. ALGORITHM CAPABILITY ANALYSIS  In order to test Sorrelation and F-Fminer algorithm capability, experiment adopts the real data. Mushroom, connect and pumsb* data sets come from UCI[11].

T30I1.2D60k data set is generated by IBM synthetic data  generator. Table  shows the characteristic of data. The platform of experiment is an IBM pc with 512 MB RAM and 3.0 GHZ CPU.

Figure 2. Find frequent 1-itemsets   Figure 4. Building new FP-Forest on Ti5  TABLE II.        ASSOCIATION RULES  Trequent  Itemsets  Association  Rules Scorrelation  {I5, I1: 2} I5 I1 0.21  {I5, I1, I2:  2}  I5 I1 I2  I1 I5 I2  I5 I2 I1  0.33  {I5, I2: 2} No rule 0.13  Figure 5. Ti5 right shift  TABLE III. TESTING DATA SET  Data Set  Name  DATA  Item number Record number  Mushroom 119 8124  Connect 129 67557  Pumsb* 7117 49046  T30I1.2D60K 120 60000  A. Running time comparing  Running time is an important parameter of algorithm capability, so this paper selects three classical algorithms Apriori and F-Growth* to compare with F-Fminer algorithm.

Due to above three algorithms basing on different measures, this paper only compares running time of generating frequent itemsets to make these algorithms in the same condition.

30 35 40 45 50      T im  e( s)  M in_support(% )  A priori  FP -grow th*  F-Fm iner  Figure 6. Running time comparing on mushroom set  I5:2  I1:2  I3:1 I2:1  I2:1      I4  I1  I3  I2  A I5   I1 I2   T I2 I1   T I5 I2  I1:2 I2:2  I2:2 2 I2  T I5 I1  I1:6  I3:4 I2:1  I2:2    I3  I2  A I1     4 8 12 16 20       T  im e(  s)  Min_support(%)  Apriori  FP-growth*  F-Fminer  Figure 7. Running time comparing on pumsb* set  45 50 55 60 65 70         T im  e( s)  Min_support(%)  Apriori  FP-growth*  F-Fminer  Figure 8. Running time comparing on connect set  20000 30000 40000 50000 60000       T im  e( s)  D atabase set  Aptiori  FP-growth*  F-Fminer  Figure 9. Running time comparing on T30I1.2D60K  The experimental results in Fig 6-8, shows that the F- Fminer algorithm has better performance than Apriori algorithm and FP-growth* algorithm. Fig 9 shows F-Fminer has better performance than Apriori and FP-growth* with data rapid increase. Therein, the Min-support is equal to 0.45.

B. Association rules number comparing  Generating rules number is also an important parameter of algorithm capability, so this paper compares it among Apriori , FP-growth*, F-Fminer and  F-Fminer without Scorrelation on T30I1.2D60K dataset. Therein, Min-support is equal to 0.45, Min_confidence is equal to 0.7 and Min_scorrelation s equal  to 0.2. The experimental results in Table , shows that the F- Fminer algorithm with Scorrelation has better performance in Generating rules number.

TABLE IV. GENERATING RULE NUMBER COMPARING  Rule Number  Comparing  Algorithm Name  Apriori FP-growth* F-Fminer without  Scorrelation F-Fminer  Rule number 16507 13210 10089 5245

VI. CONCLUSION  In this paper, we have proposed a new association rules mining measure, named Scorrelation, which can enhance the correlation degree of items in association rule and cut negative correlation rules. At the same time, two new data structures, UFP-Tree and FP-Forest, for mine association rules, which store data in multi-trees. It is easy to realize large-scale database storage, and not like a single FP-Tree. Furthermore we have designed a fast mining algorithm, F-Fminer, with divide and conquer strategy. According to the experiments on real large data sets, F-Fminer algorithm has higher efficiency comparing with other algorithms. So the method in this paper is very appropriate to mine association rules in large database.

