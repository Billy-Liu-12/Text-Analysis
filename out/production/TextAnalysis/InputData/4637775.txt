A distributed algorithm of density-based subspace frequent closed itemset mining

Abstract  Large, dense-packed and high-dimensional data mining is one challenge of frequent closed itemset mining for asso- ciation analysis, although frequent closed itemset mining is an efficient approach to reduce the complexity of mining fre- quent itemsets. This paper proposes a distributed algorithm to address the challenge of discovering frequent closed itemsets in large, dense-packed and high-dimensional data.

The algorithm partitions the search space of frequent closed itemsets into independent nonoverlapping subspaces that can be extracted independently to generate frequent closed itemsets. The algorithm can generate frequent closed item- sets according to dense priority: the closed itemset more dense or more frequent will be generated preferentially. The experimental results show the algorithm is efficient to ex- tract frequent closed itemsets in large data.

Keywords: Frequent closed itemset mining, Association analysis, Concept lattice, Partition, Distributed algorithm  1 Introduction  The large, dense-packed and high-dimensional data min- ing is one challenge of frequent closed itemset mining for association analysis, although frequent closed itemset min- ing is an efficient approach to reduce the complexity of min- ing frequent itemsets and some efficient algorithms of fre- quent closed itemset mining have been proposed. When the data is very large, dense-packed and high-dimensional, and the minimum support is small, it is still a hard problem to mine frequent closed itemsets.

In recent years, association analysis [1] has attracted a lot of attention for research and applications. Frequent itemset mining is one sub-problem and the key task of association analysis . Many research works focus on ming frequent itemsets as it is a hard problem when data is large. Some techniques are proposed to reduce the complexity of min-  ing frequent itemsets. The well-known techniques are min- ing maximal frequent itemsets [11, 5] and mining frequent closed itemsets [9].

The problem of finding frequent itemsets from data for association rules can be reduced to finding frequent closed itemsets with closed itemset lattice [8, 10, 13]. And it?s possible to prune the number of rules produced without in- formation loss using closed itemset lattice [8].

Closed itemset lattice is based on concept lattice. The- oretical foundation of concept lattice is derived from the mathematical lattice theory [2, 6] that is a popular mathe- matical structure for modeling conceptual hierarchies. Con- cept lattice can be used to analyze and mine the complex data for such as classification, association rule mining, clus- tering, etc [7]. Furthermore, concept lattice also provides an effective tool of knowledge visualization.

In recent years, extensive studies have proposed some ef- ficient algorithms for mining frequent closed itemsets, such as CLOSE, A-close [9], CLOSET [10], CHARM[13] and CLOSET+ [12], free-sets [4] etc. These algorithms have good performance for sparse data. However, when the data is dense or the size of items is large, the algorithms suffer from the challenge of mining large and high-dimensional data. One efficient solution is to reduce the complexity by partitioning the mining space.

In this paper, we propose a distributed algorithm to discover frequent closed itemsets in large and high- dimensional data. The algorithm is based on the density of items and closed itemsets, and the hierarchical order be- tween the closed itemsets. We propose a simple approach to determine the frequent closed itemsets. The algorithm par- titions the search space of frequent closed itemsets into in- dependent nonoverlapping subspaces that can be extracted independently to generate frequent closed itemsets. The al- gorithm can generate frequent closed itemsets according to dense priority: the closed itemset more dense or more fre- quent will be generated preferentially.

The rest of this paper is organized as follows. The basic concepts of mining frequent closed itemsets are presented   DOI 10.1109/HPCC.2008.147    DOI 10.1109/HPCC.2008.147    DOI 10.1109/HPCC.2008.147     a1 a2 a3 a4 a5 a6 a7 a8 1 ? ? ? 2 ? ? ? ? 3 ? ? ? ? ? 4 ? ? ? ? 5 ? ? ? ? 6 ? ? ? ? ? 7 ? ? ? ? 8 ? ? ? ?  Figure 1. Example of formal context  in the next section. Section 3 analyzes the search space of frequent closed itemsets. The distributed algorithm of min- ing frequent closed itemsets is introduced in section 4. We show the experimental results of the algorithm in section 5.

The paper ends with a short conclusion in section 6.

2 Closed itemset  Definition 2.1 Formal context is defined by a triple (O,A, R), where O and A are two sets, and R is a relation between O and A. The elements of O are called objects or transactions, while the elements of A are called items or attributes.

For example, Figure 1 represents a formal context (O,A, R).

Definition 2.2 Two closure operators are defined as O1 ? O??1 for set O and A1 ? A??1 for set A.

O?1 := {a ? A | oRa for all o ? O1}  A?1 := {o ? O | oRa for all a ? A1}  These two operators are called the Galois connection for (O,A, R). These operators are used to determine a formal concept.

Definition 2.3 A formal concept of (O,A, R) is a pair (O1, A1) with O1 ? O, A1 ? A, O1 = A?1 and A1 = O?1.

O1 is called extent, A1 is called intent.

Definition 2.4 We say that there is a hierarchical order between two formal concepts (O1, A1) and (O2, A2), if O1 ? O2 (or A2 ? A1).

All formal concepts with the hierarchical order of con- cepts form a complete lattice called concept lattice.

Definition 2.5 An itemset C ? A is a closed itemset iff C ?? = C.

a1a2a3a4a5a6a7a8  a1a3a4a5 a1a2a3a4a6  a1a3a4a6  a1a2a4a6  a1a4a6 a1a3a4  a1a2a3  a1a2a3a7a8  a1a2a7a8 a1a2a7  a1a3a7a8  a1a7a8  a1a7 a1a3 a1a4a1a2  a1b b  b b b  b b  b bb b  b b  bb b b b  J JJ      J  JJ       H HHH  HH H HHH  HH HHH  HHH      JJ  JJ  JJ  JJ                  HHHH HH    JJ        ? ?? ? ??  ? ?? ? ??  ?? ??  ?? J JJ  ? ?? ? ??  J JJJ JJ        ?? ?? ??HH  HHHH      Figure 2. Example of closed itemset lattice  Definition 2.6 If C1 and C2 are closed itemsets, C1 ? C2, then we say that there is a hierarchical order between C1 and C2. And C2 is called the sub-closed itemset of C1, or C1 is called the super-closed itemset of C2, if there is no closed itemset C3, C1 ? C3 ? C2.

All closed itemsets with the hierarchical order of closed itemsets form of a complete lattice called closed itemset lattice.

The closed itemset lattice of the formal context of Figure 1 is presented in Figure 2.

Definition 2.7 Given a formal context (O,A, R), C is an itemset, the support or density of C, denoted as support(C), is the number of the transactions of C.

Proposition 2.1 Let an itemset C ? A be a closed itemset, the support of C is support(C) = ||C ?||  3 Analysis of search space for frequent closed itemsets  In this section, we propose an approach to determine a closed itemset in a formal context and then study the hierar- chical order between each closed itemset and its sub-closed itemsets to analyze how to partition the search space of fre- quent closed itemsets.

3.1 How to determine a closed itemset  Definition 3.1 Given a context (O,A, R), an item ai is called maximal item, if ai ? A, for all aj ? A, i 6= j and {ai}? 6? {aj}? .

It is easy to infer the following proposition from the def- inition 3.1.

Proposition 3.1 Given a context (O,A, R), if item ai ? A and ai is a maximal item, and for all aj ? A, i 6= j and {ai}? 6= {aj}?, then {ai} is a closed itemset.

From the proposition 3.1, we have a simple approach to determine a closed itemset in the formal context (O,A, R):  ? Merge the items that contain the same transactions as a single item;  ? Find the maximal items in the merged context;  ? Each maximal item must be a closed itemset.

3.2 How to partition the search space  There are different methods to partition the search space of frequent closed itemsets. We will explore our method by the following objectives:  ? The subspaces are nonoverlapping;  ? The closed itemsets more dense or more frequent can be generated preferentially;  ? The approach to determine a closed itemset can be used in subspaces.

Proposition 3.2 Given a closed itemset Ci of context (O,A, R) and it?s sub-closed itemset Cij where j = 1, 2, 3, ..., Cij?Ci is closed itemset in sub-context (Ci?, A? Ci, R).

Proof : Cij is a sub-closed itemset of closed itemset Ci, then we have Cij ? Ci = (Cij ? Ci)?? in the sub-context (Ci?, A? Ci, R). Thus, Cij ? Ci is a closed itemset in the sub-context (Ci?, A? Ci, R).

Proposition 3.3 All sub-closed itemsets of Ci can be gen- erated from the sub-context (Ci?, A? Ci, R).

Proof : Given a closed itemset Cj of the sub-context (Ci?, A ? Ci, R), (Cj ? Ci) is a sub-closed itemset of Ci.

Thus all sub-closed itemsets can be generated by the closed itemsets of the sub-context (Ci?, A? Ci, R).

From above propositions, we can consider sub-contexts as the subspaces of closed itemsets. The sub-context can be (Ci?, A ? Ci, R), Ci is one super-closed itemset. In each sub-context or subspace, the maximal items or items with high density or support generate frequent closed itemsets.

For example (see figure 3), a2, a3, a4 and a7 are maximal items in the sub-context of a1 and they can generate the closed itemsets: a1a2, a1a3, a1a4 and a1a7.

By this approach, some same closed itemsets will be generated in different sub-contexts as they have different super-closed itemsets. The the sub-contexts are overlap- ping. We can generate the same closed itemsets in only one sub-context and reduce the sub-contexts to nonoverlapping sub-contexts by the following proposition:  a2 a3 a4 a5 a6 a7 a8  1 ? ? 2 ? ? ? 3 ? ? ? ? 4 ? ? ? 5 ? ? ? 6 ? ? ? ? 7 ? ? ? 8 ? ? ?  ??????         J J J  HHHHHH  a1  a1a7 a1a3 a1a2 a1a4  u u u u u  Figure 3. Example of sub-context and sub- closed itemsets of a1  Proposition 3.4 Given two super-closed itemsets Ci and Cj , Ci 6? Cj and Cj 6? Ci. All sub-closed itemsets of Ci and Cj can be generated from the nonoverlapping sub- contexts (Ci?, A?Ci, R) and (Cj ?, A?Cj?Ci?, R), Ci? is noted by all items that is included by Ci in the sub-context (Cj ?, A? Cj , R).

Definition 3.2 Given an itemset ai of context (O,A, R), ({ai}?, A?{ai}??, R) is called projective sub-context of ai.

We analyze the projective sub-contexts of a1a2 and a1a3 (see figure 4). In such sub-contexts of a1a2 and a1a3, a1a2a3 is a common closed itemset for two sub-contexts.

To reduce the redundant closed itemsets, we do not change the projective sub-context of a1a2, but remove a2?= a2 from the projective sub-contexts of a1a3 to reduce the sub- context of a1a3 (see figure 5). Thus, all the closed itemsets that include a2 in the sub-context of a1a3 will not be gener- ated.

The summary of the main idea of partitioning the search space is the reduced projective sub-contexts of high dense closed itemsets are the subspaces of frequent closed item- sets.

4 Distributed algorithm of discovering fre- quent closed itemsets  Given a formal context and minimum of support, we pro- pose a distributed algorithm to mine frequent closed item- sets by following steps:  1) First of all, the algorithm needs to generate the ordered frequent context.

sub-context of a1a2: ({a1a2}?, A? {a1a2}, R)  a3 a4 a5 a6 a7 a8  1 ? 2 ? ? 3 ? ? ? 5 ? ? 6 ? ? ?  ??????         J J J  HHHHHH ??????         HHHHHH  a1  a1a7 a1a3  a1a2 a1a4  a1a2a7 a1a2a3 a1a2a4a6  u u u u u  u u u sub-context of a1a3: ({a1a3}?, A? {a1a3}, R)  a2 a4 a5 a6 a7 a8  3 ? ? ? 4 ? ? 6 ? ? ? 7 ? ? 8 ? ?  ??????         J J J  HHHHHH??????  J J J  HHHHHH  a1  a1a7 a1a3  a1a2 a1a4  a1a3a7a8 a1a2a3  a1a3a4  u u u u u  u u u Figure 4. Example of sub-context and sub- closed itemsets of a1a2 and a1a3  reduced sub-context of a1a3: ({a1a3}?, A? {a1a3} ? {a2}, R)  a4 a5 a6 a7 a8  3 ? ? 4 ? ? 6 ? ? 7 ? ? 8 ? ?  ??????         J J J  HHHHHH??????  HHHHHH  a1  a1a7 a1a3  a1a2 a1a4  a1a3a7a8  a1a3a4  u u u u u  u u Figure 5. Example of reduced sub-context and sub-closed itemsets of a1a3  Definition 4.1 Given the minimum support m, a formal context is called ordered frequent context if we only choose the items which number of transactions is not less than m, and order these items of formal context by number of transactions of each item from the smallest to the biggest one, and the items with the same objects are merged as one item. We note ordered frequent context (O,AC, R)m of the formal context (O,A, R).

Ordered frequent context can count the density or sup- port of each item and facilitate the generation of maximal items and reduced sub-contexts. Ordered frequent context allows to generate frequent closed itemsets according to dense priority: the closed itemset more dense or more fre- quent will be generated preferentially.

2) The second step: from the ordered frequent context, every maximal item forms a frequent closed itemset of the first level.

We have the following proposition by the definition 4.1 and the proposition 3.1.

Proposition 4.1 Each maximal item of the ordered data context (O,AC, R)m is closed itemset.

For example, the frequent closed itemset in first level for the formal context of figure 1 is a1. The density or the sup- port of a1 is 8.

This step is the core of the algorithm. For any sub- context, we only see about the maximal items that can form the frequent closed itemset.

3) The third step: determine the number of partitions ac- cording to the size and dimension of data, or the user?s re- quirement. We can generate more partitions if data is large or high-dimensional. The partitions are independent for dis- tributed mining. The partition is the reduced sub-context of frequent closed itemset. The partitions can be generated from the frequent closed itemsets of first level. From the projective sub-contexts of the frequent closed itemsets of first level, we can generate more partitions if we need.

4)The fourth step: in each partition, if the support of the frequent closed itemset is m, this itemset ends to generate itemsets of next level because the itemsets of next level will not be frequent. Otherwise we will generate the frequent closed itemsets from the reduced sub-context of this item- set. We generate the frequent closed itemsets from a context to all sub-contexts until the closed itemset is not frequent.

5) At the end, we can get all frequent closed itemsets.

For example, given the minimum of support 2, the algo-  rithm generates all frequent closed itemsets (see figure 6) from the formal context of figure 1.

4.1 An example  We search for frequent closed itemsets from the context of figure 1 to illustrate the principle of new algorithm (see     transactional data  ?

generate maximal frequent closed itemsets: a1, a1a2, a1a3, a1a4, a1a7 from contexte (O, AC, R)2 =? 4 dis- tributed partitions of mining frequent closed itemsets  ?????????????)  ? ?  ? ??  @ @ @ @@R  PPPPPPPPPPPPPq  partition1: a1a7  ?

FCIs of parti- tion: a8  partition2: a1a3  ?

FCIs of parti- tion: a7a8, a4, a4a6  partition3: a1a2  ?

FCIs of parti- tion: a3, a7, a7a8, a4a6  partition4: a1a4  ?

FCIs of parti- tion: a6  Figure 7. Extracting the frequent closed itemsets from the context in figure 1  ??????         J J J  HHHHHH     J J J  ??????  J J J  HHHHHH  ??????         HHHHHH         J J J J J?? J  J J  SS            J J  S(8)  S(4) S(5) S(5) S(4)  S(3) S(3)  S(2) S(3) S(2) S(3)  S(2)  S(2)) S(2)  a1  a1a7 a1a3  a1a2 a1a4  a1a7a8 a1a4a6  a1a3a7a8 a1a2a7 a1a2a3  a1a3a4 a1a2a4a6  a1a2a7a8 a1a3a4a6  u u u u uu uu u u u uu u  Figure 6. An example: all frequent closed itemsets (minimum of support is 2; for each node, S means support)  figure 7).

Assuming the minimum support 2, we generate the or-  dered frequent context. The support of a5 is less than min- imum support, so a5 is deleted from the context. We will partition the search space of frequent closed itemset (FCI) into some subspaces.

a1 is maximal item in the ordered frequent context so {a1} is frequent closed itemset. All sub-closed itemsets: a1a2, a1a3, a1a4 and a1a7, can be gen- erated from the sub-context in figure 3. The sub- contexts ({a1a2}?, A?{a1a2}, R), ({a1a3}?, A?{a1a3}? {a2}, R), ({a1a4}?, A ? {a1a4} ? {a2} ? {a3}, R) and ({a1a7}?, A ? {a1a7} ? {a2} ? {a3}, R) are 4 subspaces of mining frequent closed itemsets. The 4 subspaces are independent. We can generate all frequent closed item- sets (FCIs) in each subspace. For exmaple, in sub-contexts ({a1a2}?, A? {a1a2}, R), we can find FCIs: a3, a7, a7a8, a4a6. In whole context, we need to add {a1a2} to each FCI  in sub-context to form the FCI of whole context. There- fore, a1a2a3, a1a2a7, a1a2a7a8, a1a2a4a6 are FCIs of the whole context. As we mine the frequent closed itemsets from the reduced sub-contexts, 7 redundant frequent closed itemsets are avoided to be repeatedly mined in different sub- contexts.

In each subspace, the item which support is less than minimum support should be deleted.

DataSet ID Objects Attributes Closed itemsets  soybean-small d1 47 79 3253 car d2 1728 21 7999 breast-cancer d3 699 110 9860 house-votes-84 d4 435 18 10642 audiology d5 26 110 30401 tic-tac-toe d6 958 29 59503 nursery d7 12960 31 147577 lung-cancer d8 32 228 186092 agaricus- lepiota  d9 8124 124 227594  promoters d10 106 228 304385 soybean-large d11 307 133 806030 dermatogogy d12 366 130 1484088  Table 1. The datasets of real data for experi- ment     5 Experimental results  We have implemented the algorithm in Java to generate frequent closed itemsets. We test the algorithm in some real data and simulation data. We compare the partitioning algo- rithm with some subspaces and non-partitioning algorithm without subspaces. The preliminary experimental results in figure 8 show the efficiency of the algorithm. In the the ex- perimental results, the run time of partitioning algorithm is the total time of all subspaces mining. The experimental results show the partitioning algorithm is much faster than non-partitioning algorithm. The subspaces mining with the partitioning algorithm are independent. We can develop the algorithm in distributed version.

Real data (see table 1) for our experiment comes from machine learning benchmarks: UCI repository [3].

d1 d2 d3 d4 d5 d6 d7 d8 d9 d10 d11 d12  LN (ti  m e(  m illi  se c)  )  Datasets  non-partitioning partitioning  Figure 8. Experimental comparison of parti- tioning algorithm and non-partitioning algo- rithm (minimum support is 90%)  6 Conclusion and further work  One challenge of frequent closed itemset mining is large, dense-packed and high-dimensional data mining. The pro- posed algorithm is based on the density of items and closed itemsets, and the hierarchical order between the closed itemsets in the closed itemset lattice structure. The algo- rithm can partition searching space of frequent closed item- sets into independent reduced subspaces and then mine fre- quent closed itemsets in each subspace. The algorithm is scalable to extract frequent closed itemsets from large and high-dimensional data. The experimental results show the algorithm is efficient to extract frequent closed itemsets in large data.

The future work will focus on comparison the perfor- mance of the algorithm and other frequent itemset mining algorithms, and development of more efficient techniques to analyze huge and heterogeneous distributed data.

