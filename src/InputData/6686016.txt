A MapReduce Based Approach of Scalable Multidimensional Anonymization for Big Data Privacy Preservation on Cloud

Abstract?The massive increase in computing power and data storage capacity provisioned by cloud computing as well as advances in big data mining and analytics have expanded the scope of information available to businesses, government, and individuals by orders of magnitude. Meanwhile, privacy protection is one of most concerned issues in big data and cloud applications, thereby requiring strong preservation of customer privacy and attracting considerable attention from both IT industry and academia. Data anonymization provides an effective way for data privacy preservation, and multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes. However, existing multidimensional anonymization approaches suffer from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud environments. As such, we propose a scalable multidimensional anonymization approach for big data privacy preservation using MapReduce on cloud. In the approach, a highly scalable median-finding algorithm combining the idea of the median of medians and histogram technique is proposed and the recursion granularity is controlled to achieve cost-effectiveness. Corresponding MapReduce jobs are dedicatedly designed, and the experiment evaluations demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional scheme can be improved significantly over existing approaches.

Keywords-big data; cloud computing; privacy preservation; MapReduce; multidimensional anonymization

I. INTRODUCTION Privacy is one of most concerned issues in big data and  cloud computing areas which has attracted considerable attention from IT industry, academia and governments [1, 2].

Big data and cloud computing, two disruptive trends at present, have posed significant challenges on current IT industry and research communities. Big data poses challenges on existing data processing approaches due to its ?3Vs?, i.e., Volume (increasingly enormous amount of data), Velocity (speed of data in and out), and Variety (range of data types and sources). Cloud computing provides powerful and economical computation and storage resources that enable users to handle ever-increasing big data applications such as social networking services and cloud healthcare services. As the analysis of datasets in such big data applications probably provides profound insights into a number of key areas of society including healthcare, medical, government services and the like, the datasets are often  shared or released to third party partners or the public.

Meanwhile, the privacy-sensitive information of data record owners like patients in such datasets often requires strong preservation before the datasets are shared or released.

Data anonymization which is extensively studied and widely adopted [3], provides an effective way for data privacy preservation. It refers to hiding the privacy-sensitive information like identities or sensitive data of record owners like customers or patients. Thus, the privacy of an individual can be effectively preserved while certain aggregate information is exposed to data users for diverse analysis and mining. Multidimensional anonymization scheme is a widely-adopted one among existing anonymization schemes because it incurs low data distortion while it is still globally coded [4]. The scalability problem of this scheme has attracted attention from privacy research communities and several approaches integrating spatial indexing or sampling technologies have been proposed to address it [5, 6].

However, existing approaches suffers from severe scalability or IT cost issues when handling big data due to their incapability of fully leveraging cloud resources or being cost-effectively adapted to cloud computing environments.

Recently, MapReduce [7], a parallel and distributed large-scale data processing paradigm, has been extensively researched and widely adopted for big data applications [8, 9]. Integrated with the infrastructure resources provisioned by cloud systems, MapReduce becomes more powerful, elastic and cost-effective due to salient characteristics of cloud computing, e.g., Amazon Elastic MapReduce service [10]. As such, it is quite promising to leverage MapReduce on cloud to conduct multidimensional anonymization over big data so that the anonymization process can be seamlessly integrated with other data processing or mining tools while high scalability and cost-effectiveness can be gained. As the MapReduce computation paradigm is relatively simple, however, it is still a challenge to design appropriate MapReduce jobs for multidimensional anonymization scheme and therefore intensive research is still required.

In this paper, we propose a MapReduce based approach for multidimensional anonymization over big data, aiming at achieving high scalability and cost-effectiveness via leveraging MapReduce on cloud. Firstly, we investigate the problem of finding the median of a numerical attribute of a large-volume dataset. Finding the median for numerical attributes is a core component for recursively partitioning of datasets in multidimensional anonymization. Then, we   DOI 10.1109/CGC.2013.24    DOI 10.1109/CGC.2013.24    DOI 10.1109/CGC.2013.24     explore the granularity of recursion of multidimensional scheme in MapReduce for cost-effectiveness. A group of MapReduce jobs are deliberately designed, and are coordinated to perform multidimensional anonymization on data sets collaboratively. We evaluate our approach by conducting experiments on real-world data sets.

Experimental results demonstrate that with our approach, the scalability and cost-effectiveness of the multidimensional anonymization scheme can be improved significantly over existing approaches.

The major contributions of our search are three fold.

Firstly, we propose a highly scalable MapReduce algorithm of finding a median for a numerical attribute based on combining the idea of median of medians and histogram.

Secondly, we present a cost-effectiveness aware recursive mechanism for MapReduce based multidimensional scheme by controlling the granularity of recursion according to system status. Lastly, a group of innovative MapReduce jobs are designed to concretely conduct the computation of multidimensional scheme in a highly scalable fashion.

The remainder of this paper is organized as follows. The next section reviews related work about privacy preservation, privacy protection in MapReduce. In Section III, we briefly present some preliminaries for our research and analyze the problems and challenges in existing multidimensional anonymization approaches. Section IV elaborates our approach and presents algorithmic details of MapReduce jobs for multidimensional scheme. We empirically evaluate our approach in Section VI. Finally, we conclude this paper and discuss future work in Section VII.



II. RELATED WORK Privacy preservation on data has been extensively  investigated and fruitful progress has been made by research communities [3]. We briefly review the related work about privacy preservation and MapReduce as follows.

A bulk of privacy models and anonymization approaches have been put forth to preserve the privacy-sensitive information in data sets. k-anonymity [11] and l-diversity [12] are two basic and widely-adopted privacy models to measure the degree of privacy-sensitive information disclosure against record linkage attacks and attribute linkage attacks, respectively. Other privacy models like t- closeness [13] and m-invariance [14] are also proposed for various privacy attack scenarios. Several anonymizing operations are leveraged to anonymize data sets, including suppression [15], generalization [4, 16, 17], anatomization [18], slicing [19], disassociation [20], etc. In this paper, we utilize generalization to anonymize data sets. Roughly, there are four generalization schemes listed as follows [3]. Full- domain scheme [21] makes all domain values in an attribute generalized to the same level of the taxonomy tree. Sub-tree scheme [16] requires that either all child domain values or none of a non-leaf node in a taxonomy tree are generalized.

Multidimensional scheme [4] takes multiple attributes together into account when generalizing domain values. The aforementioned schemes are global recoding, i.e., if a domain value is generalized, all its instances are generalized.

Cell generalization [17], on the contrary, is local recoding  which generalizes identical instances into different levels of domain values.

Our research herein concentrates on the multidimensional anonymization scheme. This scheme incurs low information distortion when anonymizing data while it is still globally recoding, which is quite practical for a wealth of data analytics and mining applications. Thus, this scheme has been extensively explored. LeFevre et al. [4, 5] investigated multidimensional anonymization over large-scale datasets for both general purpose and specific workloads like classification, and leveraged the RainForest decision algorithms [22] and sampling techniques to handle datasets larger than main memory. Iwuchukwu et al. [6] and Mokbel et al. [23] utilized spatial indexing techniques to address the scalability problems of multidimensional anonymization. Pei et al. [24] explored the update of incremental datasets that are anonymized by multidimensional scheme. Although these existing approaches can mitigate scalability problem of multidimensional scheme, they suffer incapability of handling big data anonymization due to their lack of scalability on cloud.

Privacy protection issues related to MapReduce and cloud have begun to draw intensive attention. Puttaswamy et al. [25] described a set of tools called Silverline that can separate all functionally encryptable data from other cloud application data to protect privacy of data. Similarly, Zhang et al. [26] proposed a privacy leakage upper-bound constraint based approach to preserve privacy of multiple data sets by only encrypting part of data sets on cloud. Roy et al. [27] presented a system named Airavat which incorporates mandatory access control with differential privacy. Blass et al. [28] proposed a privacy-preserving scheme named PRISM for the MapReduce framework on cloud to perform parallel word search on over encrypted data sets. Ko et al.[29] proposed the HybrEx MapReduce model to provide a way that sensitive and private data are processed within a private cloud while others can be safely extended to public cloud.

Zhang et al. [30] proposed a system named Sedic which partitions MapReduce computing jobs in terms of the security labels of data they work on and then assigns the computation without sensitive data to a public cloud.

However, the above approaches, frameworks and tools often require participation of human resources. Similar to the research herein, Zhang et al. [31, 32] proposed scalable approaches using MapReduce on cloud to handle data anonymization for the sub-tree anonymization scheme. But the proposed approaches fail to be applied for the multidimensional scheme discussed in this paper.



III. PRELIMINARIES AND PROBLEM ANALYSIS  A. Multidimensional Anonymization Scheme To facilitate subsequent discussion, we briefly introduce  the sub-tree generalization scheme as background knowledge.

Table 1 lists some basic symbols and notations. We assume there is one sensitive value in one record like medical diagnosis, as more sensitive values will not affect our discussion. We use QI as the acronym of quasi-identifier.

The quasi-identifier group is abbreviated as QI-group [33].

Without loss of generality, we adopt k-anonymity [11] as the privacy model in this paper, i.e., for any ??? ? ???, the size of ??	(???) must be zero or at least k, so that a quasi- identifier will not be distinguished from other at least k-1 quasi-identifiers.

Multidimensional anonymization scheme is defined by a single function [3, 4], i.e., : ?1 ? ? ? ? ? ??? , where meanings of symbols are shown in Table 1. This function is used to generalize a data record quasi-identifier ??? =??1, ?2, ? , ? ? to its anonymous quasi-identifier ???? =??1, ?2, ? , ? ?, where ?? , 1 ? ? ?  is the same as ??, or is an ancestor of ?? in ??? for nominal attributes, or is a interval containing ?? for continuous or ordinal attributes.

Multidimensional scheme flexibly allows two quasi- identifiers even having the same value ?? to be independently anonymized into different anonymous quasi-identifiers. In contrast, single-dimensional global recoding in schemes like full-domain or sub-tree is defined by a function for each attribute, i.e., ?: ?? ? ???? , 1 ? ? ?  . Consequently, multidimensional scheme causes less data distortion than the two aforementioned schemes.

The single function defining multidimensional scheme can be constructed by partitioning a dataset into a set of non- overlapping multidimensional regions and mapping the data records in a region to an anonymous quasi-identifier. A region here can be regarded as a QI-group ??	(???). Thus, the resultant anonymous dataset satisfies ?-anonymity if the size of each region is not less than ?. Ideally, the optimal ?- anonymous multidimensional partitioning can partition a dataset into a set of QI-groups that satisfy ? -anonymity, while the data distortion incurred is the lowest with respect to a certain distortion metric. However, the problem of finding the optimal multidimensional partitioning is NP-hard as proved in [4].

Definition 1 (Allowable Split) Consider a dataset ?. For continuous or ordinal attributes, a split to attribute ???? at  value ?? is allowable if and only if  |??? >?? | ? ? and |??? ??? | ? ?, where ??? >?? and ??? ??? are the two sets of data records whose attribute values ?? are larger than ?? or not larger than ?? , respectively. For nominal attribute, a split to attribute ???? at a generalization ???: !"?#?(?) ? ? is allowable if and only if |?$ | ? ?, %$ ? !"?#?(?), where ? is a domain value in ??? , !"?#?(?) is the set of child domain values of ? in the hierarchy, and ?$ denotes the set of data records containing values that can be generalized to $.

Rather than find out the optimal multidimensional partitioning, it is practical and effective for most applications to find sub-optimal solutions according to certain heuristic information. A promising way to partition a dataset recursively if any allowable split exists, producing the minimal multidimensional partitioning. The splitting attributes and points can be chosen greedily according to certain heuristic metrics. This minimal multidimensional partitioning can achieve low data distortion. If all attributes are continuous, the upper bound of QI-group size is 2? & 1 [4], but this fails to hold for nominal attributes.

B. Problem Analysis Existing approaches for multidimensional scheme mainly  leverage the median of values of a continuous or ordinal attribute as the split point for multidimensional partitioning, as median-partitioning is usually a strategy adopted for obtaining uniform occupancy [34]. Intuitively, median- partitioning for continuous or ordinal attributes can produce fairly low data distortion as datasets can be evenly partitioned. However, finding the median often suffers from scalability problems when a dataset is larger than the available memory, which happens frequently in big data applications. The state-of-the-art existing approaches scan the data and count the frequency set of the attribute and then find the median from the frequency set. There are two main shortcomings for this. Since the scanning of the dataset is serial, the heavy I/O will be terribly time-consuming for big data. Another shortcoming is that the number of values in the dataset may be small, which is possible for continuous attribute. This means that the frequency set will still be too large to fit in the main memory. An extreme case is that the attribute values are different from each other, where the frequency set technique will fail to benefit.

Besides, the recursive characteristic of multidimensional partitioning poses a challenge on MapReduce based parallel algorithm design. Normally, the recursive process will terminate when no allowable splits exist. However, to achieve good balance between high efficiency and cost- effectiveness, the recursion granularity should be controlled by determining whether the subsequent computation after a round of recursion is taken by a cluster or just by a compute node. This consideration is reasonable because the scale of sub-problem is becoming smaller and smaller. After a threshold, one computation node is more suitable for the sub- problem from the perspective of cost-effectiveness. As such, it is still challenging to consider scalability, efficiency and IT cost together to address multidimensional anonymization over big data using MapReduce on cloud.

TABLE I. BASIC SYMBOLS AND NOTIONS  Symbol Definition  D A data set containing data records m Number of attributes r A data record, ' ? ? and ' = ??1, ?2, ? , ? , *??,  where ?? ,1 ? ? ? , is an attribute value, and sv is a sensitive value.

Attri The ith attribute of a data record TTi The hierarchy tree of the partially ordered nominal  attribute Attri.

DOMi The set of all domain values in TTi for partially  ordered nominal attribute Attri, or the domain interval for continuous  or ordinal attribute Attri.

Vi The set of domain values of Attri in original data records, i.e., ?? + ???? .

SV The set sensitive values qid A quasi-identifier, ??? = ??1, ?2, ? , ? ?, ?? ????? , 1 ? ? ? .

QID The set of quasi-identifiers, ??? = ????1 , ???2 , ? , ??? ? QIG(qid) Quasi-identifier group containing all records with  the same quasi-identifier qid     ALGORITHM 1. MRMONDRIAN DRIVER.

Input: dataset ?, current quasi-identifier ???.

Output: anonymous data set ??.

1: If no allowable split for ?, return ? as a final QI-group with  anonymous quasi-identifier ???; 2: For continuous or ordinal attributes, find their medians via  MapReduce job: Median-Finding; 3: Compute the coefficient of variation for each attribute via  MapReduce job: CV-Computation; 4: Choose the best splitting dimension ??-?*? with the smallest  coefficient of variation; 5: Partition the dataset ? according to ??-?*? into set {?1, ? , ?? }  via MapReduce job Data-Partitioning, where n = 2 if ??-?*? is continuous or ordinal or n is the number of child values of the generalization if ??-?*? is nominal;  6: Repeat the steps in 1-5 on each dataset in  {?1, ? , ?? };

IV. SCALABLE MULTIDIMENSIONAL ANONYMIZATION USING MAPREDUCE: MRMONDRIAN  In this section, we mainly elaborate scalable MapReduce based multidimensional anonymization approach, named as MRMondrian. IV.A is to present the basic process of multidimensional anonymization scheme and the selection of splitting dimension. The problem of finding the median in a set of values is discussed and corresponding MapReduce algorithms are designed in Section IV.B.

Section IV.C describes the MapReduce jobs for calculating the values of search metric and partitioning datasets. Section

IV.D investigates the recursion granularity controlling for cost-effective multidimensional anonymization.

A. MRMondrian Driver Basically, multidimensional partitioning is a recursive  process, starting on the whole original dataset and terminating when further partitioning on the QI-group will breach privacy requirements. Each round of recursion involves three main steps, namely, finding the best splitting dimension and its corresponding splitting point, splitting the dataset into two or multiple sub-datasets and recursively invoking such a process on the sub-datasets. The first step is the most important one, as it will impact on the data distortion of the resultant anonymous dataset. The second step is relatively simple, but is computation and I/O intensive.

We elaborate each step subsequently and present the driver of the proposed approach MRMondrian.

To guide the selection of the best dimension in the first step, the goodness of a candidate dimension should be measured by a search metric. For continuous or ordinal attributes, [4, 5] suggest that choosing the dimension with the widest (normalized) range of values [34] can obtain approximately uniform partition occupancy which implies low data distortion for multidimensional anonymization.

However, as nominal attributes are also involved in most real-life big data applications, choosing the dimension with producing the best partition occupancy need to consider all types of attributes equally.

We leverage the coefficient of variation [35] of values of an attribute to guide the selection of the best dimension. The coefficient of variation is a normalized measure of dispersion of a probability distribution. The reason why we employ the coefficient of variation rather than the standard deviation to measure the dispersion is that we have to normalize the dispersion of the distribution of each attribute in order to make them comparable with each other.

Let .??? be a random variable indicating the number of an child domain value in a generalization ???, with respect to a data set D . Assume there are ? child domain values, denoted as ?1 , ..., ?? , respectively. Let ?/ represent the number of the occurrence of ?/ in ? , where 1 ? / ? ? .

Then, .??? takes values from {?/  | 1 ? / ? ?} , with the same probability, i.e., 1 ?0 . Based on the distribution of .??? , the expectation of .??? , denoted as 3??? , is calculated by the following formula:3??? = 1? 4 ?/?/ =1 .   (1)  Furthermore, the standard deviation, denoted as 5??? , is derived based on 3???  by  5??? = 61? 4 (?/ &3 ??? )2?/ =1 .  (2) With 3???  and 5??? , the coefficient of variation, denoted  as CV??? , can be obtained by!???? = 5??? 3???0 .   (3) The dimension with the smallest coefficient of variation  can be selected as the best one, i.e., the partitioning on the attribute according to the generalization incurs the least dispersion, which produces the best partition occupancy with high probability. The above metric is quite suitable for comparing nominal attributes with hierarchy, while it suffers incapability for continuous or ordinal attributes. However, the partitioning of a continuous or ordinal attribute can be regarded as partitioning the attribute according to a generalization containing two child values when a splitting point is given. Similar to [4, 5], approximate median of an attribute is leveraged as the splitting point for continuous or ordinal attributes, and finding the approximate median using one-pass MapReduce in a highly scalable fashion will be elaborated in Section IV.B.

Generally speaking, continuous or ordinal attributes will be more preferred than nominal ones as the median based partitioning can result in smaller coefficient of variation due to less dispersion. However, as the median in our approach is approximate, the coefficient of variation is usually larger than 0 and depends on the valuing of the approximate median. Given that nominal attributes are sometime preferred in early phase of the recursive process because it can promote the parallelization of the recursion. Thus, the coefficient of variation of nominal attributes is adjusted by being multiplicated by a parameter 7, 0 < 7 ? 1. In such a way, choosing attributes between continuous/ordinal and nominal attributes can be well balanced.

Once the splitting attributes are identified, partitioning the dataset and recursively repeating the process can be conducted with ease. Algorithm 1 presents the multidimensional anonymization MapReduce driver, i.e., MRMondrian driver. The driver coordinates the macro execution of MapReduce jobs required to conduct the computation in multidimensional anonymization.

Three MapReduce jobs are designed in Algorithm 1,     namely, Median-Finding, CV-Computation and Data- Partitioning, for finding the median of continuous or ordinal attributes, computing coefficient of variation and partition datasets, respectively. The details of these MapReduce jobs will be depicted in following sections.

B. Scalable Median-Finding Using MapReduce According to the problem analysis in Section III.C,  existing approaches suffer severe scalability problem for finding the median of a large-scale dataset. Leveraging the MapReduce paradigm can mitigate such a problem because serial I/O in existing approaches can be parallelized.

However, this by no means solves the scalability issue completely. Although multiple mappers can be launched to read and map data records in a highly scalable and parallel fashion, only one reducer can be launched to aggregate emitted keys and find the median. Due to the characteristic of median, all attribute values should be retained within the reducer, which often results in severe scalability issues.

The purpose of finding the median in multidimensional anonymization is just to find an appropriate splitting point for a continuous or ordinal attribute in order to achieve good partition occupancy. Therefore, it is practical and effective to find an approximate median in a scalable and efficient fashion for most real-life big data applications. We address the scalability problem based on this basic principle.

Specifically, we leverage the idea of the median of medians [36] and the histogram technique together.

The basic process of finding the median of medians consists of three steps. Firstly, a series of data groups are constructed, with a fixed group size, denoted as 9. Then, the median of each group can be identified quickly as 9 is usually small number. Finally, the median of the medians in the last step are figured out. Such a median has a salient feature that it less than ;92 ? /(2 ? 9) of the total values, and greater than another ;92 ? /(2 ? 9) of the total. It is recommend that 9 is valued as 5, i.e., the chosen median lies somewhere between 30% and 70%.

We utilize the median of medians as the split point of a continuous or ordinal attribute. Although this is just an approximation for the real median, we can address the problem of finding the median in a scalable by using MapReduce. The step of finding the median of a fixed group can be parallelized naturally, which is quite suitable for the map phase of a MapReduce job. The computation of finding the median of a fixed group can be conducted in a mapper.

Then, all the medians are medians are shuffled to one reducer. The reducer can find the median of medians. During this round, only 1 90 of total data are shuffled from mappers to the reducer, and ;92 ? /9 of total data will be excluded for the candidates of the real median. This also means that (1 & A9 20 B9 ) of total data are the candidates of the real median.

Since the capability of the reducer is limited, 1 90 of total data probably are larger than its capability. Thus, it is necessary to use multiple rounds of MapReduce job to find  the real median. Let ? denote the amount of total data, and ! denote the capability of the reducer. The round of MapReduce job to find the real median, denoted as ?EF???? , can be bounded by the following lemma.

Lemma 1. ?EF???? ?  #E?2 ?! .

Proof.

As ;92 ?

9 = G  2 , 9 ?* ????,(9 + 1) (2 ? 9), 9 ?* E?? I?? 9 > 1,0 J  9+1 29 is  a decreasing function with respect to 9 and lim9?+K 9+129 = 2, we can derive that  2 ?  ;92 ?

9 ? 23. Let 9?, ? ? 0, be the group  size in the ??" round of the MapReduce job. Let ????*? denote the rest of the ??" round. According the bounds of ;92 ?9 ,? 3?0 ? ????*? ? ? 2?0 . To make the reducer capable of handling median finding without scalability problem, 9? should satisfy 9? ? ?3??! because ??  ??*? 9? ? ! should hold.

When ????*? ? !, i.e., the rest data can be handled in the reducer, the iteration of MapReduce round can terminate.

Hence, if ?2? ? ! holds, the iteration will terminate. It can be derived that the iteration must terminate if ? ? log2 ?! . As a result, the inequality ??? ?  #E?2 ?! holds.

As finding the median of a continuous or ordinal attribute is just a step in the anonymization process, its efficiency is also quite important for the whole approach. Thus, it is practical to design a one-pass MapReduce job which is also scalable and offers good approximation to the real median. If more data are excluded for the median candidates, the approximation to the real median can be improved. Thus, 9 should be small values like 3 or 5. However, the data that the reducer has to handle is still quite huge in such cases. Thus, the scalability problem still exists in a one-pass MapReduce job. To combat this problem, we integrate the histogram technique with the idea of the median of medians.

To decrease the data delivered to the reducer, we put the median of a size-fixed group into a bin and just send the bin statistical information to the reducer. The statistical information of bins can be firstly aggregated in mappers, only aggregate data are emitted to the reducer, thereby offering significantly high scalability. Let M denote the number of bins, and [-?&, -?+] denote the ??" bin, where0 < ? ? M, -?& and -?+ are the lower and upper bounds of the bin, respectively. The reducer can find the bin where the median of medians, where each bin is associated with a value count denoted as !??? . Finally, the value (-?& +  -?+)/2 is chosen as the approximate median of the attribute, i.e., the splitting point is (-?& + -?+)/2.

Based on the analysis and design elaborated above, the Map and Reduce functions of the Median-Finding job are depicted in Algorithm 2. The algorithm finds the medians of all continuous or ordinal attributes simultaneously. Note that we emit bins with the count 1 in Line 3 of the map function.

ALGORITHM 2. MEDIAN-FINDING MAP & REDUCE.

Input: Dataset ?; group size 9; continuous or ordinal dimensions  {??1, ? , ??? }, where ? ? .

Output: Approximate medians {??1 , ? , ???}.

Map: 1: For each 9 records, finding the medians for ??1 , ? , ??? ; 2: Find the bins for the medians, N-?1&, -?1+O, ? , N-??&, -??+O; 3: Emit(1, ?N-?1&, -?1+O , 1?), ..., Emit(?, ?N-??&, -??+O , 1?).

Reduce: 1: For each bin [-?&, -?+], aggregate the second part to obtain !???; 2: Find the bin where the median lies, [-?&, b?+]; 3: Emit(-?&+ -?+2 , (!??? 4 !???M?=10 & 1 M0 ));  ALGORITHM 3. CV-COMPUTATION MAP & REDUCE.

Input: Data record (??' , '), ' ? ?; current quasi-identifier ??? =??1, ?2, ? , ? ?; approximate medians {??1 , ? , ???} for ?  continuous or ordinal attributes.

Output: Coefficients of variation {!?1, ? , !? }.

Map: 1: For each nominal attribute value ?? in ', find its generalization ???? with the parent ?? . Let $? be ?? itself or ?? ?s child that is also ???s ancestor. Emit(i, ?$? , 1?); 2: For each continuous or ordinal attribute value ??: if ?? < ??/ ,  emit(?, ?P-&, ??/ Q, 1?), where -& is the low bound of the attribute with respect to ?; if ?? > ??/ , emit(?, ?P ??/ , -+Q, 1?), where -+ is the upper bound; if ?? = ??/ , emit(?, ?P-&, ??/ Q, 1?) with probability ? and emit(?, ?P ??/ , -+Q, 1?) with probability ?.

Reduce: 1: Aggregate the count of each child value for ordinal attributes, ??/ ,1 ? / ? ?, where ? is the number of child values in the  generalization, or that of two bins for continuous or ordinal attributes, ??0 and ??1;  2: Calculate coefficients of variation for each attribute according to (1), (2) and (3) described in Section IV.A, obtaining {!?1, ? , !? };  3: Emit(?? , !?? );  ALGORITHM 4. DATA-PARTITIONING MAP & REDUCE.

Input: Dataset ?; current quasi-identifier ??? = ??1, ?2, ? , ? ?; the  best splitting dimension ??-?*? .

Output: Datasets {?1, ? , ??}.

Map: 1: Find the generalization ???-?*? for  ??-?*? if it is nominal. Let ??  be the parent and ?? be the value of the current data record '.

Identify the child domain value of ?? , denoted as $? , where $? is either ?? itself or ?? ?s child that is also ???s ancestor. Emit($? , ');  2: If the ??-?*? is continuous or ordinal, let ??/ is the splitting point: If ?? < ??/ , emit(??/&, '); If ?? > ??/ , emit(??/+, '); If ?? = ??/ , emit(??/&, ') and emit(??/+, ') with probability ?, respectively.

Reduce: 3: For each key-value pair, emit(', 9RSS);  The MapReduce paradigm provides combiner mechanism, so that we can aggregate the count of each bin locally in a mapper. In Line 3 of the Reduce function, we also emit !??? 4 !???M?=10 & 1 M0 in order to evaluate the goodness of the selected approximate median.

C. CV-Computation and Data-Partitioning Jobs In order to calculate the coefficient of variation, we need  to count the number of each child domain value of a generalization for nominal attributes, and count the number of values of each split for continuous or ordinal attributes when the splitting point is given. Mappers can count the statistical information independently in a highly scalable way, and reducer can calculate coefficient of variation. The details of the Map and Reduce function of the job CV- Computation are depicted in Algorithm 3.

According to the Algorithm 3, the coefficients of variation of all attributes can be calculated in a MapReduce job in a highly scalable fashion. Note that the combiner mechanism should be exploited to aggregate the statistical information locally in mappers for high scalability and less network traffics. Once the coefficients of variation are calculated, the best dimension can be selected with the  lowest coefficient of variation for multidimensional partitioning. Note that coefficients of variation of nominal attributes can be adjusted by the parameter 7 if necessary.

A MapReduce job Data-Partitioning is designed for conducting multidimensional partitioning. The partitioning process is relatively simple. Details of the Map and Reduce functions of Data-Partitioning are depicted in Algorithm 4.

D. Cost-Effective Recursion Granularity Controlling The recursive characteristic of multidimensional  partitioning can be leveraged to enhance the parallelization of the process, thereby improving the scalability and efficiency. After a dataset is partitioned into a set of sub- datasets, each sub-dataset will be further multidimensional partitioning independently. As a result, we can launch a MapReduce jobs for each sub-dataset, and execute the jobs simultaneously.

One of the hypes for cloud computing is that the computation or storage resources provisioned by cloud are unlimited. Thus, we can launch as many computation nodes as demanded recursively in theory. Let ?(?) denote the execution time of multidimensionally partitioning a dataset with the scale ?. The time required in a round of recursion is$ ? ?, where $ is a constant and ? is the scale. Let MT be the average branch factor for the hierarchy trees for all attributes.

The branch factor of a continuous or ordinal attribute can be regarded as 2, as a binary hierarchy tree can be constructed with all splitting points. Often, the inequality MT ? 2 holds for most cases. Then, ?(?) can be estimated by the following recursive formula in the above settings: ?(?) = ?(?/MT) + $ ? ?.   (4)  In terms of (4), we can derive that ?(?) = ?(?), which will quite efficient for multidimensional partitioning.

However, it is impractical and not cost-effective to launch a number of computation nodes recursively. Let !(?) denote the number of computation nodes, and constant $ be the number of nodes allocated in a MapReduce job. Then, !(?) can be estimated by the following recursive formula:!(?) = MT ? !(?/MT) + $.   (5)  According to (5), we can derive that !(?) = ?(?) , which will be impractical to launch so many nodes and ? is usually quite huge in big data applications. Moreover, it is     unnecessary to recursively launch multiple computation nodes for small granularity of datasets. If the dataset can be handled in the memory of a node, it is better to execute the multidimensional partitioning within one node. As such, we can control the recursion granularity of multidimensional partitioning to make a good balance among scalability, efficiency and IT cost.

There three ways to control the recursion granularity, namely, the number of computation nodes, recursion depth and the size of a partition. These three factors affect each other. Users can specify their control via providing a threshold for any of the three conditions. From the starting point, our approach launches MapReduce jobs in a parallel manner according to the recursion. Once one of the conditions is violated, the subsequent multidimensional anonymization is conducted on the current computation nodes without launching more nodes.



V. EXPERIMENTAL EVALUATION  A. Experiment Settings To evaluate the effectiveness and efficiency of the  proposed approach, we compare it with existing approaches [4, 5]. As the finding the median of an attribute is the core step of the multidimensional anonymization, we just show the experimental results of this step. And more experiments will be conducted in our future work. The execution time of finding the median in our approach is denoted as ??? , and the existing one is ?U. .

Our experiments are conducted in a cloud environment named U-Cloud. U-Cloud is a cloud computing environment at University of Technology Sydney (UTS). The system overview of U-Cloud has been depicted in Fig. 1. We install OpenStack open source cloud environment for global management, resource scheduling and interaction with users.

Further, Hadoop clusters are built based on the OpenStack cloud platform to facilitate large-scale data processing.

All approaches are implemented in Java and standard Hadoop MapReduce API. The Hadoop cluster consists of 6 VMs (1 master and 5 slaves) with type m1.medium which has 2 virtual CPUs and 4 GB Memory. Each round of experiment is repeated 20 times. The mean of the measured results is regarded as the representative. We utilize the Adult dataset and its generated datasets like [31]. The numerical attribute Age is taken to examine the effectiveness and efficiency of our approach. The group size 9 in the median- of-medians approach is set as 5 like [36].

B. Experiment Process and Results We measure the change of execution time ???  and ?U. with respect to the size of dataset. The data size is  denoted as W. The data size varies from 500MB to 5GB, i.e., 500MB ? W ? 5GB , which is big enough to evaluate the effectiveness of our approach in terms of data volume or the number of data records. Fig. 2 demonstrates the change of ??? and ?U. with respect to the data size W.

We can see from Fig. 2 that execution time ??? and ?U.

increase with the growth of data size. But ?U. increases fast with respect to W, while ??? increase more slowly than ?U. .

As a result, the difference between ?U. and ??? is getting larger and larger when the data size increase. Hence, finding an approximate median with our proposed approach is much more scalable and efficient than existing approaches. It can be also seen from Fig.2 that ?U. is less than ??? when W is relatively small. Existing approaches are more efficient than our MapReduce based approach because some extra time cost will be incurred for launching or terminating jobs. But for big data anonymization, such extra time cost becomes minor compared with the scale of the data.

Based on the above experimental results and the fact that finding the median is a core step for multidimensional anonymization, we can conclude that with our approach, the scalability and cost-effectiveness of multidimensional anonymization scheme can be improved significantly over existing approaches.



VI. CONCLUSIONS AND FUTURE WORK In this paper, we have investigated the scalability issue of  multidimensional anonymization over big data on cloud, and proposed a scalable MapReduce based approach. We have examined the scalability issues of finding the median due to its core role in multidimensional partitioning, and have proposed a highly scalable MapReduce based algorithm of finding the median via exploiting the idea of the median of medians and the histogram technique. Coefficient of variation has been introduced as a search metric to guide the selection of splitting dimension for producing good partition occupancy. We have also designed highly scalable MapReduce jobs to calculate coefficients of variation and to partitioning datasets. Recursion granularity of multidimensional partitioning has been studied to gain good balance among cost-effectiveness and efficiency. Lastly, we have conducted experiments on datasets extended from real- life datasets, and the experimental results demonstrate that with our approach, the scalability and cost-effectiveness of multidimensional anonymization scheme can be improved  Figure 1. System overview of U-Cloud. Figure 2. Change of execution time w.r.t. data size W.

significantly over existing approaches.

Privacy concerns of big data on cloud computing have  attracted the attention of researchers in different research communities. But ensuring privacy preservation of large- scale data sets still needs extensive investigation. We will integrate this work into our proposed scalable and cost- effective privacy preserving framework. Based on the contributions herein, we plan to further explore the next step on scalable privacy preservation aware analysis and scheduling on big data.

