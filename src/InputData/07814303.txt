Learning Short Binary Codes for Large-scale Image Retrieval

Abstract? Large-scale visual information retrieval has become an active research area in this big data era. Recently, hashing/binary coding algorithms prove to be effective for scal- able retrieval applications. Most existing hashing methods require relatively long binary codes (i.e., over hundreds of bits, sometimes even thousands of bits) to achieve reasonable retrieval accuracies.

However, for some realistic and unique applications, such as on wearable or mobile devices, only short binary codes can be used for efficient image retrieval due to the limitation of computational resources or bandwidth on these devices. In this paper, we propose a novel unsupervised hashing approach called min-cost ranking (MCR) specifically for learning powerful short binary codes (i.e., usually the code length shorter than 100 b) for scalable image retrieval tasks. By exploring the discriminative ability of each dimension of data, MCR can generate one bit binary code for each dimension and simultaneously rank the discriminative separability of each bit according to the proposed cost function. Only top-ranked bits with minimum cost-values are then selected and grouped together to compose the final salient binary codes. Extensive experimental results on large- scale retrieval demonstrate that MCR can achieve comparative performance as the state-of-the-art hashing algorithms but with significantly shorter codes, leading to much faster large-scale retrieval.

Index Terms? Large-scale retrieval, short binary codes, unsupervised, hashing, min-cost ranking.



I. INTRODUCTION  RECENTLY, the approximate nearest neighbor (ANN)search [1]?[6] has attracted increasing attention for large-scale visual retrieval applications, in which hashing methods are popularly utilized to embed high-dimensional data into a similarity-preserved low-dimensional Hamming space.

Hash codes largely reduce the memory storage requirement and simultaneously expedite computation and search. Existing hashing techniques can be roughly divided into two groups: random projection based methods and learning based methods.

Manuscript received April 26, 2016; revised September 21, 2016 and October 27, 2016; accepted December 19, 2016. Date of publication January 10, 2017; date of current version January 30, 2017. This work was supported by National Natural Science Foundation of China under Grant 61528106. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Guoliang Fan. (Corresponding author: Ling Shao.)  L. Liu and L. Shao are with the College of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing 210044, China, and also with the School of Computing Sciences, University of East Anglia, Anglia NE4 7TJ, U.K. (e-mail: liuli1213@gmail.com; ling.shao@ieee.org).

M. Yu is with the Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne NE1 8ST, U.K. (e-mail: m.y.yu@ieee.org).

Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.

For the random projection based hashing, the most well- known hashing technique that preserves similarity information is probably Locality-Sensitive Hashing (LSH) [7]. LSH sim- ply employs random linear projections (followed by random thresholding) to map close data points in the Euclidean space to similar codes. The kernelized version of LSH, Kernel- ized Locality-Sensitive Hashing (KLSH) [8], has also been proposed and utilized for large-scale image retrieval and classification. To design effective and compact hash codes, a number of projection learning based hashing methods have been introduced. Principled linear projections such as PCA Hashing (PCAH) [9] and its rotational variant [10] have been designed for better quantization. Additionally, another popular technique called Spectral Hashing (SpH) [11] was proposed to preserve the local relationship of data by keeping the neighbors in the input space as neighbors in the Hamming space. After that, researchers utilized anchor graphs to obtain tractable low-rank adjacency matrices for efficient similarity search, termed Anchor Graphs Hashing (AGH) [12]. Beyond those, recently Self-Taught Hashing (STH) [13], Spherical Hashing (SpherH) [14], Iterative Quantization (ITQ) [10], Random Maximum Margin Hashing (RMMH) [15], Dis- crete Graph Hashing(DGH) [16], Latent Structure Preserving Hashing (LSPH) [17] and Compressed Hashing (CH) [18] have also been effectively applied to large-scale information retrieval tasks. More hashing techniques can also be seen in [1], [2], and [19]?[26].

Previous efforts on hashing methods more focus on learning data structure and proposing novel optimization scheme for objective functions. However, less attention is attracted on how to obtain very short (compact) binary codes (less than 100 bits) for some unique but realistic scenarios and applications, such as image retrievals via wearable or mobile devices with limita- tion of computational resources or bandwidth. In other words, notwithstanding the effectiveness of preserving data similarity in the original space by the aforementioned hashing methods, relatively long codes (i.e., over hundreds of bits, sometimes even thousands of bits) are needed to acquire sufficiently discriminative power, otherwise such hashing methods will lead to low actuaries on the searching problems.

For instance, one of the state-of-the-art hashing methods, Iterative Quantization (ITQ)1 [10], cannot reach the satisfac- tory accuracy when the code length is below 100 bits and achieves better performance with the increase of the code length to a certain level as shown in Fig. 1 (a) and (c). In fact,  1To be consistent with original paper [10], PCA is applied prior to ITQ.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Fig. 1. The left subfigure shows the top six retrieved images of four query images, returned by MCR, SpherH, ITQ and RMMH using 20K training images and 16 binary bits on the CIFAR-10 dataset. It is observed that when applying short hash codes (only 16 bits) for image retrieval, the proposed MCR-L1 can outperform other state-of-the-art methods. Images marked by red boxes are incorrect retrieval. Subfigure (a) and (b) illustrates the performance by using Hamming ranking search, and subfigure (c) and (d) illustrates the performance resulted from the hash lookup table search. In detail, the comparison results in subfigure (a) are measured by the average semantic top-200 precision between MCR and ITQ. To achieve the competitive retrieval results with using original 512-d Gist features, MCR can lead to only one-fourth code length of ITQ. The subfigure (b) reveals the retrieval time per query via linear scan on the 60K dataset. Towards the best performance of each method, the black bold line segment indicates the time (seconds) requirement per query via MCR and the blue one indicates the time requirement per query for ITQ. In terms of using the hash lookup table search, subfigure (c) shows the retrieval precision within 2 Hamming distance radius for ITQ and our MCR, and the curves in (c) have the similar tendencies with subfigure (a). In addition, in subfigure (d) the corresponding searching complexity (i.e, possible searching space) of the hash lookup table is explored with the code lengths varying. From all results, it is obviously observed that in terms of the same performance, MCR can achieve large-scale retrieval with less computational complexity than ITQ, due to the shorter code length.

ITQ needs to generate a 320-bit binary code for each sample to achieve the comparative performance compared with original features. For an ordinary workstation with 16GB RAM, if the database contains 1 billion images, it is impossible to load such long binary codes in the memory, since about 37GB2  is required for these 320-bit ITQ codes in total. Moreover, in terms of the online retrieval phase, longer binary codes will also lead to an expensive searching complexity no matter using either the Hamming ranking scheme or the hash lookup table. In detail, from Fig. 1 (b), we can obviously observe that by using Hamming ranking search, the retrieval time will dramatically increase when codes become longer. More importantly, for practical tasks, we always adopt the hash lookup table search instead of Hamming ranking to achieve instant retrieval on large-scale data. However, shorter than 32-bit codes are usually used to establish the hash lookup table, since longer codes will cause a huge searching com- plexity (i.e., huge searching space). For instance, there are around 5 ? 104 possible locations that need to be searched in a lookup table with 320-bit ITQ codes. While, for 80-bit MCR codes, the possible searching locations would be reduced to 3 ? 103 in the hash lookup table as illustrated in Fig. 1 (d). Therefore, it is infeasible to use long binary codes in applications with the instant retrieval requirement and limited computation resources, which motivates us to learn more compact codes (< 100 bits) for large-scale retrieval and still achieve competitive results with state-of-the-art methods.

In this paper, we propose a novel unsupervised hash- ing method called Min-cost Ranking (MCR) to learn very short binary codes (i.e., usually the code length shorter than 100 bits) for large-scale retrieval tasks. To better explore the  2Only about 9GB memory is needed for our proposed MCR but with the same retrieval performance of ITQ as shown in Fig. 1 (a).

discriminative properties for effective binary coding, a two- stage learning procedure is introduced in our MCR. In the first stage, we aim to learn one bit binary code for each dimension of the original features. Considering the intrinsic locality consistency between dimensions, we associate each dimension with its neighboring dimensions to form a linear subspace.

Further to assess the discriminative ability, each dimension is optimized by minimizing the cost function under the maximum margin criterion in their corresponding subspace and finally transformed to one bit via the obtained mapping vector. Similar to SSH [9], for each bit, an individual projection is respectively learned via MCR, instead of learning a whole projection matrix at one time. Different from SSH, learning of each bit in MCR is independent and can be parallel processed, which could be more efficient in the training phase. While, SSH is a sequential learning procedure, in which learning of each bit is dependent on previous ones. In the second stage, similar to [27], we rearrange the cost function value (i.e., MCR cost) of each dimension with the ascending order. Only top- ranked bits are then selected and grouped as the most salient bits for retrieval tasks. The experimental results demonstrate the effectiveness and efficiency of the proposed MCR, which leads to only one-fourth code length of ITQ but achieving competitive results with directly using original 512-d features, as shown in Fig. 1. It is worthwhile to note that our approach has the following advantages:  ? It can reach competitive retrieval results with state-of-the- art hashing methods but with significantly shorter binary codes, which is more feasible for realistic applications with limited computing resources such as wearable or mobile devices.

? Different from some existing hashing techniques [12], [13], [28], once the first stage of bits learning is    LIU et al.: LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1291  accomplished, there is no need to re-train the proposed MCR (but only need to select a certain number of the bits) when generating different lengthed binary codes.

This important property makes MCR more suitable and efficient for applications requiring multiple-length codes.



II. RELATED WORK  In terms of general visual search, there exist some typ- ical techniques. The content based image retrieval (CBIR) methods [29]?[34] are first introduced to achieve the visual indexing based on using the content information (e.g., color, texture and shape) rather than visual annotations or semantics, and suitable for large-scale unsupervised searching. Besides, with the increasing number of Internet users, a large amount of multimodal web data (e.g., images, texts and audio) have been continually generated and stored. Under such circumstances, how to achieve cross-modality similarity search becomes an interesting but challenging problem and has attracted a lot of attention [35]?[38]. Beyond those, recently, web click infor- mation has also been used in image re-ranking, because clicks have been shown to more accurately describe the relevance of retrieved images to search queries. However, a critical problem for click-based methods is the lack of click data, since only a small number of web images have actually been clicked on by users. Inspired by this, some previous methods [39], [40] aim to solve this problem by utilizing the multimodal hypergraph- based learning methods for image click prediction, and apply the obtained click data to achieve more precise image retrieval.

In the rest of this section, we will mainly review some more related hashing techniques and binary bit selection schemes with regards to the large-scale visual retrieval. Principled linear projections like PCA Hashing (PCAH) [9] has been suggested for better quantization rather than random projection hashing. More recently, Spherical Hashing (SpherH) [14] was proposed to map more spatially coherent data points into a binary code compared to hyperplane-based hashing functions.

Meanwhile, the authors also developed a new distance function for binary codes, spherical Hamming distance, to achieve final retrieval tasks. Iterative Quantization (ITQ) [10] was developed for more compact and effective binary coding.

Particularly, a simple and efficient alternating minimization scheme for finding an orthogonal rotation of zero-centered data so as to minimize the quantization error of mapping this data and the vertices of a zero-centered binary hypercube. To obtain high-quality hash codes, researchers developed Discrete Graph Hashing (DGH) [16] to directly solve the binary code without resorting to error-prone spectral relaxation [11] in which binary code is relaxed to be a matrix of reals followed by a thresholding step (threshold is 0). A tractable alternating max- imization algorithm was proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Besides, Random Maximum Margin Hashing (RMMH) [15] introduced a new hash function family that attempts to solve this issue in any kernel space. Rather than boosting the collision probability of close points, RMMH focused on data scattering. By training purely random splits of the data, regardless the closeness of the training samples,  it was indeed possible to generate consistently more indepen- dent hash functions. Due to the great success of deep learning, recently, researchers attempt to simultaneously achieve the discriminative feature embedding and hash code learning with a unified deep model (e.g., deep neural network (DNN) and convolutional neural network (CNN)). Unlike most previous hashing methods which seek a single linear projection to map each sample into a binary vector, deep hashing techniques aim to seek multiple hierarchical non-linear transformations to learn these binary codes, so that the nonlinear relationship of samples can be well exploited and lead to improved perfor- mance in large-scale retrieval. In particular, some representa- tive deep hashing methods can be found in [41]?[46] for either single-view or cross-view retrieval applications. Nevertheless, above methods are all not specifically designed for learning very short binary codes (shorter than 100 bits), so that it is difficult for them to tackle the high-performance searching with limited computation resources.

Inspired by the well-known feature selection problems [27], [47] aiming to select the optimal subset of features from a candidate feature pool, in the binary coding scenario, this issue can be easily transferred to a so-called bit selection problem, which requires to choose a subset of salient bits with the most discriminative power.

In the literature, there are only a few works related to the bit selection problem in large-scale retrieval applications.

Mu et al. [48] proposed a sequential bit selection scheme which can preserve the separability via greedy metric learning. However, in their sequential learning framework, the independence between bits, which is a benefit for the compact hash coding [11], [49], was not explicitly taken into consideration. In addition to [48], another hash bit selection architecture was introduced in [50], which can effectively select good bits from a pool of hash bits generated by different hashing techniques with varied settings and different feature spaces. However, the above two methods are both regarded as the general hash bit selection schemes for any existing hashing techniques. In other words, binary codes generated by other hashing methods are necessarily required in advance before applying the above selection schemes on them. Thus, the final performance through [48] and [50] will highly depend on the quality of hash codes. Different from either purely hashing or binary bit selection schemes, in this paper, MCR is treated as an integrated hashing method to effectively combine unsupervised binary code learning with salient bit selection. In particular, the bit selection criterion in MCR is skillfully formulated as the learning objective during our binary code optimization phase. Thus, the binary coding and bit selection in our framework are highly correlated with each other rather than two interdependent phases similar to previous methods. Our MCR is outlined in Fig. 2.



III. MIN-COST RANKING  A. Notation and Motivation  Given N training data in a D-dimensional space: {x1, ? ? ? , xN } ? RD?1, our goal is to generate binary codes for each data point by learning and selecting the     Fig. 2. The outline of the proposed two-stage MCR binary coding method: discriminative bits learning and salient bit selection.

representative bits according to their discriminative ability.

In practice, data samples are always represented by real- valued high-dimensional vectors. To further obtain binary codes, many hashing techniques learn appropriate hash func- tions directly in the original feature space3 to preserve the similarity between data. However, the noise and the redundant dimensions always exist in the original feature space, which would have a negative effect on measuring the similarity and fail to learn the discriminative hash codes. Recently, inspired by the success on feature selection methods [27], [47], which motivates us to independently investigate the representative power of each dimension, we intend to select the salient bits generated from the dimensions with the most discriminative ability in our architecture.

A simple way to assess the discriminative power of a dimension is through the separability via a linear classifier (e.g., f (x) = ax + b) on this dimension. Obviously, the separability of a linear classifier on only one-dimensional data will be poor, since it is demonstrated to be linearly inseparable. In our work, to better assess the discrimina- tive power of each dimension, we first find their intrinsic neighboring dimensions and then group each dimension with their neighboring dimensions together into a subspace. After that, a linear classifier as the hashing function is learned for each dimension?s subspace. Since each subspace is currently spanned by multiple dimensions rather than one-dimension, it can be easily separated by a linear classifier.4 In fact, the subspaces spanned by each dimension and their neighborhoods not only well preserve the intrinsic local structure between different dimensions but also take the independence of each dimensional data into account.

B. Formulation of MCR  In the following, we first illustrate how we construct the neighborhood for each dimension?s subspace and then our Min-cost Ranking (MCR) algorithm is described in detail for salient bits learning and selection.

3For example, in ITQ, the hashing function is directly learned from all dimensions of the data ignoring the independence between different dimen- sions.

4In our learning phase, since a pair-wise-label binary classification problem is involved, any subspace with more than two-dimensional data can be linearly separated.

1) Neighborhood Construction for Dimensions: Let us denote x?1, ? ? ? , x?D ? RN?1 as the dimension vectors of x1, ? ? ? , xN , i.e., [x?1, ? ? ? , x?D]T = [x1, ? ? ? , xN ]. A direct construction of the neighborhood for x?i is to use the k-nearest-neighbor algorithm, i.e., to measure the L2-norm between dimensions. However, a neighborhood constructed by k-nearest-neighbor is found using the Euclidean distance, which is very sensitive to data noise. It means that the neighborhood structure is easy to change when unfavorable noises come in. To better explore the neighborhood structure of each dimension and resist the data noise, each dimension x?i is reconstructed as the linear combination of the remaining dimensions by minimizing the "L1-norm" [51]?[53] of both the reconstruction coefficients and data noise, and the non- zero coefficients indicate the L1 neighborhoods. Different from using L2 neighborhood calculated via Euclidean dis- tance which is very sensitive to data noise, L1 neighborhood structure is more robust to data change when unfavorable noises come in and can automatically realize sparsity. Besides, the neighbors selected through the "L1-norm" are also data- adaptive, which can discover the natural locality information of the data manifold and be a nice property for applications with uneven data distributions [51].

In particular, for each x?i , we need to calculate the coeffi- cients ? = [?1, ? ? ? , ?D?1]T ? R(D?1)?1 such that x?i = Xi?, where Xi = [x?1, ? ? ? , x?i?1, x?i+1, ? ? ? , x?D] ? RN?(D?1). Thus, seeking the best reconstruction for x?i leads to the following L1-norm optimization problem:  arg min ?  ?x?i ? Xi??2, s.t. ???1 ? k, (1)  where k is the parameter indicating the maximum number of neighboring dimensions used to reconstruct x?i . This problem can be regarded as a sparse coding problem [54]?[56] and solved by the orthogonal matching pursuit (OMP) [57].

In practice, however, a large number of training data N makes x? be a very long vector (i.e., x? ? RN?1), which would cause heavy computational cost for optimizing Eq. (1).

To further reduce the complexity, we employ K-means clus- tering on original data x1, ? ? ? , xN to generate C data cen- troids denoted as x?1, ? ? ? , x?C . In this way, we can efficiently construct the neighborhood for each dimension by rewriting Eq. (1) as arg min? ??x?i ? ?Xi??2, s.t. ???1 ? k, using ?Xi = [?x?1, ? ? ? , x??i?1, x??i+1, ? ? ? , x??D] ? RC?(D?1) instead of    LIU et al.: LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1293  Xi ? RN?(D?1). In addition, applying K-means to reduce the dimension vectors from RN?1 to RC?1 also effectively benefits the L1 neighborhood construction phase. In particular, when the number of centroids in K-means satisfying C < D ? 1, we can construct the more robust neighborhood for each dimension on these centroid dimension vectors since the current data are overcomplete [51] for the L1-constraint opti- mization. In the later experiments section, we will intuitively discuss this point, as well.

Having obtained the solution ?, we can define that for j < i , x?j is the neighbor of x?i if ? j ?= 0, and for j > i , x?j is the neighbor of x?i if ? j?1 ?= 0. Then for the i - th dimension, corresponding subspace is spanned by the i - th dimension itself and its neighboring dimensions whose corresponding coefficients in ? are nonzero. For instance, if ? = [0.23, 0.46, 0, 0.11, 0 ? ? ? , 0]T for the first dimension, the corresponding subspace is then spanned by the 1-st, 2-nd, 3-rd and 5-th dimensions and the number of dimensions in this subspace is four.5 In this way, the index of neighborhood for the subspace of each dimension will be stored. We suppose the dimension of the i -th subspace is di and denote data in the i -th subspace6 by x1(i), ? ? ? , xN(i) ? Rdi?1.

2) Bit Evaluation: We assign an MCR cost for each dimen- sion (bit) depending on its discriminative power. A criterion to evaluate the discriminative power of a dimension is the margin distance calculated by the optimized linear classifier for the subspace of this dimension. The calculated K-means clustering centroids are not only applied to the above neighborhood construction, but also used to efficiently guide pseudo label assignment of pairwise data. We first assign a pseudo label puv for the data pair (xu, xv ) with K clusters as follows:  puv = {  +1, if xu and xv are in the same cluster ?1, otherwise , (2)  Besides, we also define self-pair label puu = 1 for u = 1, ? ? ? , N . Then our goal is to maximize the distances of negative pairs and minimize the distances of positive pairs.

Considering an example of two negative pairs (x1, x2) and (x1, x3) with pseudo label ?1, both ?x1?x2? and ?x1?x3? are expected to be maximized. However, in the reduced Hamming space, there is an over-fitting situation that pair (x2, x3) will be unnecessarily mapped into a point with the same hash code as shown in Fig. 3. To overcome this problem, we follow [58] to replace the negative pseudo label ?1 with a relaxing parameter ?, where ?1 < ? < 0. The pairwise label is updated as:  puv = {  +1, if xu and xv are in the same cluster ?, otherwise  . (3)  In the i -th subspace, we adopt the linear classifier fi (x) = aTi x+ b for x1(i), ? ? ? , xN(i) , where ai ? Rdi?1, b ? R and  5In this example, ? indicates the coefficients excluded the first dimensions.

Thus the total number in the subspace spanned by 1-st dimension is 1+3 = 4.

In practice, the dimension of each subspace is usually larger than five for acceptable performance.

6It is noteworthy that centroid dimension vectors ? RC?D from K- means are only used for efficiently finding the neighborhood?s index of each dimension. Once the index is obtained, the subspace of each dimension is still formed via the original dimension vectors ? RN?D .

Fig. 3. Illustration of the distances in the real-valued space and the Hamming space for two-dimensional example. To maximize the distance of negative pairs, x2 and x3 will unnecessarily collide at point (1, 1) in the Hamming space, which may cause over-fitting for the hash code learning.

i = 1, ? ? ? , D. In fact, we can denote x? = [xT , 1]T . Then the classifier becomes fi (?x) = [aTi , b]?x, which is equivalent to the linear classifier without the bias b. For convenience, we omit b in the following computation. Therefore, the i -th bit of data x j can be acquired by  sgn( fi (x j (i))), i = 1, ? ? ? , D, j = 1, ? ? ? , N. (4) With the above requirement for the positive and negative  pairs and the maximum margin criterion, we can learn the i -th bit by solving the following optimization problem in the subspace of i -th dimension:  min ai  ?ai?2  , s.t. puvaTi xu(i) ? aTi xv(i) > 1, u, v = 1, ? ? ? , N, (5)  where 12?ai?2 is for the margin regularization similar to the SVM. Note that when u = v, the constraint puvaTi xu(i) ? aTi xv(i) > 1 becomes puu(a  T i xu(i))  2 > 1, which strictly forces every point to be out of the margin. We further define the cost function for the problem in (5) by using the hinge loss penalty as follows:  Fi (ai ) = ?ai?  + ?  ?  u,v  max(0, 1? puvaTi xu(i) ? aTi xv(i)),  (6)  where ? is the balance parameter to control the weight of the two terms. By minimizing the above Fi , we can find the solution a?i . Then for the i -th bit, its MCR cost F?i is defined as:  F?i = Fi (a?i )  ?D k=1 Fk(a?k )  , i = 1, ? ? ? , D. (7)  In our method, if a bit has a small value of the MCR cost, it has relatively strong discriminative power.

Since the optimal solution to the Eq. (6) cannot be directly obtained, the gradient descent method is then applied to solve it. Let us denote  Guv (ai ) = max(0, 1? puvaTi xu(i) ? aTi xv(i)), ?u, v. (8) Then we have the following partial derivative of Guv (ai ):  ?Guv ?ai =  {  0, if 1? puvaTi xu(i) ? aTi xv(i) ? 0 ?puv  (  xu(i)xTv(i) + xv(i)xTu(i) )  ai , else.

(9)     Algorithm 1 Min-Cost Ranking  Note that in our implementation if 1? puvaTi xu(i) ?aTi xv(i) = 0, we can set ai ? ai + ?ai , where ?ai is a small nonzero random vector. The same scheme has also been used in [59] and [60]. Finally, we can write the update rule for ai as follows:  ai ? ai ? ? ? Fi (ai ) ?ai  = ai ? ? (  ai + ? ?  u,v  ?Guv ?ai  )  , (10)  where ? is the step length.

Through repeatedly minimizing the Eq. (6) for subspaces of  all dimensions, we can obtain the bit for each dimension of the original feature as:  [sgn( f1(x j (1))), ? ? ? , sgn( fD(x j (D)))], j = 1, ? ? ? , N.

The main computational complexity of our MCR comes from three aspects: (1) the cost of using K-means for generating pseudo labels in Eq. (2) is O(N K t1); (2) the cost of using K-means to achieve neighborhood construction for dimensions is O(NCt2) and the sparse coding cost is O(C(k(C + (D ? 1)) + k3)); (3) the cost for optimization of each bit is O(t3 Nk3). Thus the total theoretical complexity of our MCR is O(N K t1)+O(NCt2)+O(C(k(C+(D?1))+k3))+O(t3 Nk3) during the training phase, where t1 and t2 indicate the iteration numbers of K-means, and t3 indicates the iteration number in gradient descent of Eq. (10) during each bit?s optimization.

In practice, the parallel computation can make the optimization cost of all bits much less than O(t3 Nk3), since the optimiza- tion of each bit is independent.

We further rearrange bits by MCR cost F?1 , ? ? ? , F?D in the ascending order. The first m bits are then selected and grouped to form the final code, where m is the target code length.

3) Adaptive Gradient Descent (AGD): Moreover, an adap- tive step length is also adopted in the gradient descent pro- cedure to accelerate the convergency. Specifically, with the  Fig. 4. Comparison of MCR cost and empirical error with respect to the number of AGD iterations and bit index on GIST-1M.

initialization ? = 1, for the t-th iteration, we will increase the value of ? by updating ? ? 1.2? in the next iteration if Fi (a  (t) i ) ? Fi (a(t?1)i ). Otherwise, we set ? ? 0.5? .

We summarize the whole MCR algorithm in Algorithm 1.

4) MCR Cost vs. Empirical Error: We first denote Eri =  N2  #{(u, v)|sgn( fi (xu(i)))sgn( fi (xv(i))) ?= puv} as the pair- wise empirical error on the i -th subspace. Fig. 4(upper) compares the average cost function value of Eq. (6), i.e., D  ?D i=1 Fi (ai ), with the average empirical error , i.e.,  D  ?D i=1 Eri , during the AGD iterations. As we can see, both  the MCR cost and the error rate can be well converged within 50 iterations via AGD. In Fig. 4(bottom), we also illustrate the ascending rearrangement of the MCR cost F?1 , ? ? ? , F?D and the empirical error Er1, ? ? ? , ErD after finishing AGD.

It is observed that when the value of MCR cost is small on one bit, the corresponding empirical error will be relatively low, as well. The consistent tendency of the blue and red lines    LIU et al.: LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1295  indicate that the MCR cost can naturally reflect the separability of positive and negative pairs and represent the discriminative power of each bit.



IV. EXPERIMENTS  In this section, we apply the proposed MCR method for large-scale image retrieval tasks. Three realistic datasets are used to evaluate all methods: GIST-1M: it contains one million 960-dim GIST feature vectors. The dataset is publicly available7; SIFT-1M: it contains one million 128-dim SIFT feature vectors. The dataset is publicly available8; Tiny-1M: it contains one million 384-dim GIST feature vectors, which are computed from a subset of 80M Tiny images [61], [62].

For all three datasets, we respectively take 1, 000 images as the queries by random selection and use the remaining to form the gallery database. To construct the training set, 100 000 samples from the gallery database are randomly selected for all of the compared methods. Additionally, we also randomly choose another 50, 000 data samples as a cross-validation set for parameter tuning. In the querying phase, the returned points are regarded as true neighbors if they lie in the top ranked 100, 200 and 1000 points for GIST-1M, SIFT-1M and Tiny- 1M, respectively. We evaluate the retrieval results in terms of the Mean Average Precision9 (MAP) and the precision-recall curves. In addition, we report the parameter sensitive analysis for our MCR method. All the experiments are completed using Matlab 2014a on a workstation with a 12-core 3.2GHz CPU and 120GB of RAM running the Linux OS.

A. Compared Methods and Settings  In the experiments, we denote our method as MCR-L1, since the subspace spanned by the neighborhood of each dimension is based on the L1-norm reconstruction. Beyond that, we have also tested the MCR-L2 method which finds the nearest neighboring dimensions for each subspace using the L2-norm distance (a.k.a the Euclidean distance) instead of L1-norm. As our MCR is the unsupervised hashing method, we compared our methods with other six state-of-the-art unsu- pervised hashing techniques including: LSH [7], PCAH [9], SpH [11], RMMH [15], ITQ [10], DGH [16] and SpherH [14].

In particular, we use RMMH with the triangular L2 ker- nel which can produce the best performance on the nearest neighbor (NN) search in the original paper and the number of samples M for the RMMH hashing function is equal to 32 as recommended by [15]. Besides, to be consistent with the original implementation [10], PCA is applied prior to using ITQ. Furthermore, for SpherH, the spherical Hamming distance is used in our experiments rather than the ordinary hamming distance. We used the publicly available codes of LSH, SpH, ITQ, DGH and SpherH, and implemented PCAH and RMMH ourselves. Targeting on shorter binary codes, we limit the code length of evaluation up to 512 bits on the GIST- 1M dataset, 256 bits on the Tiny-1M dataset and 128 bits on  7http://corpus-texmex.irisa.fr 8http://corpus-texmex.irisa.fr 9The ground-truth is defined by the 100, 200 and 1000 nearest neighbors  via linear scan based on the Euclidean distance.

TABLE I  RETRIEVAL RESULTS COMPARISON OF MAP AT TOP 1000 SAMPLES ON SIFT 1M DATASET  SIFT-1M dataset, respectively. Under the same experimental setting, all the parameters used in the compared methods have been strictly chosen according to their original papers.

For our methods, the number of centroids C in MCR- L1 is selected from one of {100, 200, ? ? ? , 1500} with the step of 100 and one of {50, 200, ? ? ? , 500} with the step of 50 on the cross-validation set for GIST-1M and Tiny-1M, respectively. The pairwise label of each data pair is determined by their corresponding clusters and the relaxing parameter for the negative pair is set as ? = ?0.85. The balance parameter ? for each dataset is then selected from one of the values in the range of [10?3, 102], which yields the best performance on the cross-validation set. We fix the maximum number of the iteration of AGD at 50, which has been proved to converge as shown in Fig. 4. Since bits learning on each subspace of the proposed MCR-L1/MCR-L2 is independent, in our experiments, the parallel computation scheme is adopted via our multi-core processor, which has effectively reduced the training time. Considering the uncertainty caused by K-means, all the reported results by our methods are the mean accuracies from 50 runs.

B. Evaluation Over Hashing Methods  1) Results Comparison: Fig. 5 compares the MAP of 100-nearest neighbor, 200-nearest neighbor and 1000-nearest neighbor search of all methods on GIST-1M dataset and Tiny-1M datasets. Additionally, we illustrate the MAP of top 1000-nearest neighbors on a relatively low-dimensional dataset SIFT-1M in Table I. Generally, the accuracies on the Tiny-1M dataset are lower than those on the GIST-1M and SIFT-1M datasets, since features in the Tiny-1M have larger variations than those on GIST-1M and SIFT-1M. Top-1000 nearest neighbor search produce better results than those on top-100 and top-200 search. Compared with all other methods, our MCR-L1 and MCR-L2 can achieve better results when the code length is shorter than 128, 64 bits on GIST-1M and Tiny-1M, respectively. For low-dimensional dataset SIFT-1M, the best performance of our methods are always achieved at 64 bits as well. Among the compared methods, SpherH consistently gives the best performance, since the spherical distance function is used in SpherH, which is proved to lead a improvement of results in [14]. ITQ, DGH and RMMH share     Fig. 5. Performance comparison of MCR and other state-of-the-art methods on the GIST-1M and Tiny-1M datasets.

Fig. 6. Comparison of precision recall curves with different bits on GIST-1M and Tiny-1M datasets. Ground truth is defined by Euclidean neighbors.

the similar performance on all three datasets at different bits.

Conversely, LSH, PCAH and SpH achieve lower accuracies than other methods. With the increase of the code length, the accuracies obtained by SpherH, ITQ, RMMH, SpH and LSH significantly rise up. However, the opposite tendency occurs with PCAH. The similar circumstance can also be found in [10], [63], and [64].

Different from these compared methods, the performance of our method climbs up sharply at the short code lengths, then goes down when the length of code further increases.

The reason is that there always exists a trade-off between dis- criminative power and redundancy for ranking based selection methods such as [27]. Taking an example of the top-1000 nearest neighbor search on the GIST-1M dataset, with the increase of the code length (from the 1-st to 128-th bit), more informative bits will help the hash codes to achieve higher performance. The best performance can be acquired when the code length reaches an optimal number (128 bits). After that (from 129-th to 512-th bit), the bits with low separability will be added to the code, which brings redundancy and negative effect to the performance. The exactly same tendency can also be seen in [27], due to the similar ranking scheme.

As we can observe from Fig. 5, our MCR-L1 and MCR- L2 can achieve comparative retrieval accuracies with the best-performed method SpherH, but with significantly shorter codes. Besides, our method with L1 constructed subspaces  (i.e., MCR-L1) shows better results than MCR-L2, since L1-constructed subspaces are proved to be more robust to noise, adaptive to the neighborhood and keep the intrinsic structures among different dimensions [51]?[53]. Moreover, from Table I, we can discover our methods have less superi- ority on SIFT-1M dataset compared with other two datasets.

The reason is that long features (> 500 dimensions) may include more redundant dimensions while short features have fewer redundant ones, since we assume all features may have the same percentage of redundant dimensions. Thus, for 128-d SIFT features, fewer redundant dimensions will make our MCR not significantly better than ITQ and SpherH which are regarded as the best performed unsupervised hashing methods in this paper.

Fig. 6 also shows a series of precision-recall curves with different code lengths on both GIST-1M and Tiny-1M datasets with the 1000-nearest neighbor as the ground truth. To avoid confusion, we omit LSH and PCAH in the comparison, which have been demonstrated with low performance in Fig. 5.

By comparing the Area Under the Curve (AUC), our MCR-L1 achieves apparently better performance than other methods on short bits and the performance slightly goes down when long hash codes are adopted.

2) Parameter Sensitivity Analysis: In this section, we illus- trate the effect of some essential parameter settings on retrieval accuracies. Fig. 7 reports the performance by varying    LIU et al.: LEARNING SHORT BINARY CODES FOR LARGE-SCALE IMAGE RETRIEVAL 1297  Fig. 7. Parameter sensitivity analysis of the sparse parameter k and the balanced parameter ?.

TABLE II  PARAMETER SENSITIVITY ANALYSIS OF K (I.E., USING IN PSEUDO LABEL CONSTRUCTION) VS. MAP@64 bits ON ALL THREE DATASETS  parameters k and ? with 128-bit codes on GIST-1M and 64-bit codes on Tiny-1M. When tuning the parameter k from 5 to 30, the retrieval accuracy curves of MCR-L1 appear to be more stable and insensitive to k compared with the curves of MCR- L2. The best values of k on GIST-1M for both MCR-L1 and MCR-L2 are larger than those on Tiny-1M, since the dimension of the original feature vector in GIST-1M is higher.

Furthermore, the balance parameter ? selected from (10?1, 1) can achieve the good performance for both MCR-L1 and MCR-L2 according to Fig. 7. In addition, we also show the effect of performance by varying the number of the clusters (C) in K-means. From Fig. 8, we can observe that with C approaching the dimensionality (D) of the original features, the performance of MCR-L1 will dramatically drop, since when C ? D ? 1, the coding procedure cannot find a group of overcomplete basis for better reconstruction under L1- constraint as we mentioned in Section III-B. While for MCR- L2, the accuracy curves rise up until n ? 300 and n ? 150 for GIST-1M and Tiny-1M, respectively and after that the curves of MCR-L2 become stable. At last, we illustrate the effectiveness of K-means used in pseudo label construction (i.e., Eq. (2) and Eq. (3)) in Table II by varying the cluster number K . From the results, we can discover that for 100,000 training samples, the best K of K-means clustering should be 800 ? 1600 for GIST-1M, SIFT-1M and Tiny-1M datasets.

C. Evaluation Over Bit Selection Schemes  Beyond the comparison with different hashing methods, we also evaluate MCR-L1 with some existing binary bit selection schemes and report the relevant retrieval MAP on the GIST-1M, Tiny-1M and SIFT-1M datasets using different code lengths. One straightforward scheme is to apply random selection (Random) without considering any properties of the bits. Furthermore, we also compare with a (Greedy)  Fig. 8. Mean performance of 50 runs vs. parameter C .

selection [48] scheme which considers the ability of similar- ity preserving on each bit. Besides, another state-of-the-art scheme, called Normalized Dominant Set (NDomSet) [50], is used in our evaluation as well. Since all of the above methods are used for directly selecting dominant bits from binary codes rather than learning binary codes from original feature vectors, we combine these selection schemes with two top-performed hashing methods (i,e, SpherH and RMMH) and the proposed MCR-L1 algorithm in this experiment.

It is noteworthy that for MCR-L1, we only complete the binary code learning without the ranking procedure. From Table III, we can conclude that NDomSet can achieve bet- ter performance together with SpherH and RMMH on all three datasets compared with the other two schemes, since NDomSet considers both similarity preserving and indepen- dence between bits. Besides, the Greedy scheme consistently leads to worse performance than the random selection. Since the MCR cost is calculated for each bit in their neighborhood of dimensions, the relationship between different dimensions has been considered. Meanwhile, for each bit?s subspace, their neighborhood of dimensions could be overlapped via L1- norm and the information can be compensated each other.

Thus the mutual complement of different bits is considered as     TABLE III  PRECISION COMPARISON OF MCR AND OTHER bit SELECTION METHODS  well. In conclusion, our method takes the independence and correlation between different bits into account. Therefore the proposed selection scheme by ranking the MCR cost value (i.e., MCR-L1) can achieve comparative retrieval accuracies with MCR?+NDomSet, but leads to significantly better per- formance than MCR?+ Greedy and MCR?+Random.



V. CONCLUSION  In this paper, a novel unsupervised hashing approach called Min-cost Ranking (MCR) has been proposed for large-scale data retrieval. A two-stage procedure is involved for learning and selecting salient bits in MCR. The experimental results have demonstrated that the proposed method can achieve com- petitive performance compared with state-of-the-art hashing methods but with significantly shorter binary codes, which is more feasible for realistic applications with limited com- putational resources. With the recent progress on very large- scale action dataset [65], which can contain over one million complex action videos collected from websites, in future work, it would be interesting to utilize our method on such dataset for action retrieval tasks.

