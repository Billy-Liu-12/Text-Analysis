Fast Algorithms for Frequent Itemset Mining from Uncertain Data

Abstract?The majority of existing data mining algorithms mine frequent itemsets from precise data. A well-known algorithm is FP-growth, which builds a compact FP-tree structure to capture important contents of precise data and mines frequent itemsets from the FP-tree. However, there are situations in which data are uncertain. To capture important contents (e.g., existential probabilities) of uncertain data for mining frequent itemsets, the UF-growth algorithm uses a UF-tree structure. However, the UF-tree can be large. Other tree structures for handling uncertain data may achieve com- pactness at the expense of looser upper bounds on expected supports. To solve this problem, we propose fast algorithms that use compact tree structures for capturing uncertain data with tightened upper bounds to expected support (tube) for frequent itemset mining from uncertain data. Experimental results show the tightness of tube provided by our algorithms and the compactness of our tree structures.

Keywords-Association analysis, data mining algorithms, ex- pected support, frequent patterns, tree structures, uncertain data

I. INTRODUCTION AND RELATED WORKS  Since the advent of frequent itemset mining [2], numerous studies [6], [14], [15], [16] have been conducted which explore mining frequent itemsets (i.e., frequent patterns) from precise data. Users definitely know whether an item is present in, or is absent from, a transaction in databases of precise data. However, there are situations in which users are uncertain about the presence or absence of items [3], [9], [13]. For example, a physician may suspect (but cannot guarantee) that a fevered patient got a flu or Ebola. The uncertainty of such suspicions can be expressed in terms of existential probability (e.g., such a patient has a 70% likelihood of catching the flu, and a 10% likelihood of being inflected with the Ebola virus, regardless of catching the flu or not). With this notion, each item in a transaction tj in databases containing precise data can be viewed as an item with a 100% likelihood of being present in tj . To handle uncertain data, the U-Apriori algorithm [5] was proposed.

However, it requires multiple scans of uncertain data. To reduce the number of scans, the tree-based UF-growth algorithm [10] was proposed. In order to compute the exact expected support of each itemset, paths in the corresponding UF-tree are shared only if tree nodes on the paths have the same item and same existential probability. Consequently,  the UF-tree may be quite large when compared to the FP-tree [7]. See Figures 1(a) and 1(b).

In an attempt to make the tree compact, the UFP-growth algorithm [1] groups similar nodes (with the same item x and similar existential probability values) into a cluster.

However, depending on the clustering parameter, the cor- responding UFP-tree may be as large as the UF-tree (i.e., no reduction in tree size). As alternatives to trees, hyper- structures were used by the UH-Mine algorithm [1], which was reported [17] to outperform UFP-growth. Another al- ternative is a sampling-based mining approach [4] that instantiates multiple ?possible worlds? of the uncertain data.

At a price of some loss in accuracy, this approach gains efficiency (e.g., it outperforms UH-Mine). As for tree-based approaches, to address some deficiencies of the UF-tree and UFP-tree, both CUF-growth algorithm [11] and PUF-growth algorithm [12] were proposed. For instance, PUF-growth utilizes a concept of item caps together with much more aggressive path sharing to yield a smaller tree structure than the UF-tree and UFP-tree. PUF-growth was reported to be faster than CUF-growth, which in turn was reported to be faster than UH-Mine.

In this paper, we study the following questions: Can we further reduce the upper bound on expected support (e.g., lower than the PUF-tree)? Can the resulting tree still be as compact as the FP-tree? How would frequent itemsets be mined from such a tree? Would such a mining algorithm be faster than PUF-growth? Our key contributions of this paper are as follows:  1) an extension of the concept of item caps for obtaining tightened upper bounds to expected support (tube);  2) tree structures, called tube-trees, for capturing im- portant contents of uncertain data (e.g., existential probabilities, item caps, tightened upper bounds) while maintaining their sizes to be as compact as the original FP-tree and PUF-tree; and  3) frequent itemset mining algorithms, called tube-growth to find all and only those frequent itemsets (i.e., no false negatives and no false positives) from tube-trees capturing uncertain data.

The remainder of this paper is organized as follows. The next section provides background. We then propose our   DOI 10.1109/ICDM.2014.146     Figure 1. An FP-tree, a UF-tree, a tubeS-tree, and a tubeP-tree  concepts of tube and our tube-growth algorithms (which use our proposed tube-tree structure to mine frequent itemsets) in Sections III and IV, respectively. Evaluation results are discussed in Section V, and conclusions are given in Sec- tion VI.



II. BACKGROUND  Let (i) Item be a set of m domain items and (ii) X = {x1, x2, . . . , xk} be a k-itemset, where X ? Item and 1 ? k ? m. Then, for a database of n transactions, each transaction tj ? Item. The projected database of X is the set of all transactions containing X . Each item yi in tj = {y1, y2, . . . , yh} in an uncertain database is associated with an existential probability P (yi, tj) with value 0 < P (yi, tj) ? 1, where P (yi, tj) represents the likelihood of the presence of yi in tj [8]. The expected support expSup(X, tj) of an itemset X in tj is then the product of the corresponding existential probabilities of items within X when these items are independent [8]: expSup(X, tj) =  ? x?X P (x, tj); the expected support  expSup(X) of X in the entire database is the sum of expSup(X, tj) over all n transactions in the uncertain database: expSup(X) =  ?n j=1 expSup(X, tj). An item-  set X is frequent in an uncertain database if expSup(X) ? a user-specified minimum support threshold minsup. Given an uncertain database and minsup, the research problem of frequent itemset mining from uncertain data is to discover from the database a complete set of frequent itemsets having expected support ? minsup. To mine frequent itemsets, the PUF-growth algorithm [12] uses the concept of item cap IC(X, tj) to estimate expSup(X, tj) in an ?ordered? transaction tj = {y1, . . . , yr?1, yr, . . . , yh}, where X = {x1, . . . , xk} ? tj and xk = yr:  IC (X, tj) = {  P (y1, tj) if h = 1 P (yr, tj) ? M1(yr, tj) if h ? 2 (1)  where M1(yr, tj) = max1?q<r P (yq, tj) is the highest existential probability value among all r ? 1 items in the proper ?prefix? {y1, . . . , yr?1} ? tj .

Example 1: Consider an uncertain database with n=5 transactions and m=6 domain items as shown in Table I. For a 4-itemset X={a, b, c, d} and t1={a:0.5, b:0.2, c:0.1, d:0.9, e:0.6}, expSup(X, t1) = 0.5?0.2?0.1?0.9 =  Table I A TRANSACTIONAL DATABASE  TID Sorted transactions t1 {a:0.5, b:0.2, c:0.1, d:0.9, e:0.6} t2 {a:0.6, b:0.1, c:0.2, e:0.8, f :0.7} t3 {a:0.7, b:0.2, c:0.2, e:0.9, f :0.7} t4 {a:0.9, b:0.1, c:0.1, d:0.8, e:0.6} t5 {b:0.9, c:0.9}  0.009, which is bounded above by its item cap IC(X, t1) = 0.9?0.5 = 0.45.



III. TUBE: TIGHTENED UPPER BOUNDS TO EXPECTED SUPPORT  Observe from Example 1 that the item cap IC(X, tj) serves as an upper bound to expected support expSup(X, tj) of k-itemset X in tj , but the bound may not be too tight when k > 2. This problem is worsened when X is a long itemset (of higher cardinality k) because IC(X, tj) ignores the cardinality of X .

A. Tube Based on a Second Highest Existential Probability  To solve the aforementioned problem, we extend the concept of item cap to higher cardinality and aim to obtain tighter upper bounds to expected support. Our first idea is to keep track of the second highest existential probability in the proper ?prefix? of an ?ordered? transaction tj and use it every time a frequent extension (k > 2) is added to the suffix item (i.e., when a frequent item is added to the currently frequent k-itemset to form a (k + 1)-itemset).

For tree-based mining, we build a tree structure such that each tree path represents some transactions in uncertain data.

In a tree path representing a single transaction, each tree node keeps (i) an item yi in tj={y1, . . . , yi, . . . , yh}, (ii) its item cap IC(yi, tj), and (iii) the second highest probability M2(yi, tj) in the proper ?prefix? {y1, . . . , yi?1} ? tj . With this information, we get a tightened upper bound to expected support based on a second highest existential probability (tubeS) for X={x1, . . . , xk} ? tj where xk=yr:  tubeS(X, tj)= {  IC(X, tj) if k?2 IC(X, tj)?  ?k?2 i=1 M2(yr, tj) if k?3  (2)  where M2(yr, tj) is the second highest existential proba- bility value among all r ? 1 items in the proper ?prefix? {y1, . . . , yr?1} ? tj .

Example 2: Revisit Example 1. For the 4-itemset X={a, b, c, d} with expSup(X, t1)=0.009 in t1={a:0.5, b:0.2, c:0.1, d:0.9, e:0.6}, its tubeS(X, t1) = (0.9?0.5)?0.22 = 0.018 is tighter than that provided by IC(X, t1)=0.45.

For a 3-itemset Y ={b, c, e}, tubeS(Y, t1) = (0.6?0.9)?0.5 = 0.27, which is also the tube value for 3-itemsets {a, b, e}, {a, c, e}, {a, d, e}, {b, d, e} and {c, d, e}.

Lemma 1: As expSup(X, tj) ? tubeS(X, tj) ? IC(X, tj), the tubeS serves as a tighter upper bound to expected support of X in tj than the item cap.

Proof: For any X={x1, . . . , xk} ? {y1, . . . , yr}=tj where xk=yr, (i) expSup(X, tj) = P (xk, tj)?  ?k?1 i=1 P (xi, tj) ? (ii) tubeS(X, tj) =  P (xk, tj)?M1(xk, tj)?[M2(xk, tj)]k?2 ? (iii) IC(X, tj) = P (xk, tj)?M1(xk, tj) because: (a) M1(xk, tj) & M2(xk, tj) are the two highest probabilities among all P (yq, tj) for 1 ? q ? r ? 1, including all P (xi, tj) for 1 ? i ? k ? 1; (b) 0 ? M2(xk, tj) ? 1.

Recall that the UF-tree shares a tree path only if tree nodes on the path have the same item and the same existen- tial probability. Here, to make our proposed tree structure compact and practical for mining, we merge tree paths when items are the same, regardless of their existential probabil- ity values. So, given two tree nodes x:IC(x, t1):M2(x, t1) and x:IC(x, t2):M2(x, t2) that share the same path p, we (i) sum their IC values and (ii) take the maximum be- tween the two M2 values. This results in a tree node x:IC(x, t1)+IC(x, t2):max{M2(x, t1), M2(x, t2)}. Then, a tubeS for X={x1, . . . , xk} in p can be computed as:  tubeS(X, p)= {  IC(X, p) if k?2 IC(X, p) ? [M2(xk, p)]k?2 if k?3 (3)  where (i) IC(X, p) = ?  tj?p IC (X, tj) and (ii) M2(xk, p) = maxtj?p {M2(xk, tj)}. Consequently, the tubeS of X in the entire database is the sum of tubeS in every path p in the tree capturing the uncertain data:  tubeS(X) = ?  p  tubeS(X, p). (4)  Lemma 2: As expSup(X) ? tubeS(X) ? IC(X), the tubeS serves as a tighter upper bound to expected support of X (in the database) than the item cap.

Example 3: Continue with Example 2. For the 4-itemset X={a, b, c, d} appearing only in t1={a:0.5, b:0.2, c:0.1, d:0.9, e:0.6} and t4={a:0.9, b:0.1, c:0.1, d:0.8, e:0.6} in Table I, its expSup(X) is 0.009+0.0072 = 0.0162. As t1 and t4 share the same path p, the node for d captures (i) an IC value of (0.9?0.5)+(0.8?0.9) = 1.17 and (ii) a M2 value of max{0.2, 0.1} = 0.2 so that tubeS(X, p) = 1.17?0.22 = 0.0468, which is much tighter than the bound provided by IC(X, p)=1.17. See Figure 1(c).

B. Tubes Based on Existential Probabilities of ?Prefix? Items  Our second idea is to replace the M2 value in each tree node with a new value calculated solely from the existential probability P (x, tj) for the item x represented in that node.

So, for tree-based mining, we build a tree structure such that each tree path represents some transactions in uncertain data. In a tree branch representing a single transaction, each tree node keeps (i) an item yi in tj={y1, . . . , yi, . . . , yh}, (ii) its item cap IC(yi, tj), and (iii) its existential probability P (yi, tj). With this information, we get another tightened upper bound to expected support based on existential prob- abilities of ?prefix? items (tubeP) for X={x1, . . . , xk} ? tj where xk=yr:  tubeP(X, tj)= {  IC(X, tj) if k?2 IC(X, tj)?  ?k?2 i=1 P (xi, tj) if k?3  (5)  where P (xi, tj) is the existential probability of xi ? tj .

Example 4: Revisit Example 1. For the 4-itemset  X={a, b, c, d} with expSup(X, t1)=0.009 in t1={a:0.5, b:0.2, c:0.1, d:0.9, e:0.6}, its tubeP(X, t1) = (0.9?0.5)?0.5?0.2 = 0.045 is tighter than that provided by IC(X, t1)=0.45, but not as tight as tubeS(X, t1)=0.018. For the 3-itemset Y ={b, c, e}, tubeP(Y, t1) = (0.6?0.9)?0.2 = 0.108, which is tighter then tubeS(Y, t1)=0.27.

Lemma 3: As expSup(X, tj) ? tubeP(X, tj) ? IC(X, tj), the tubeP serves as a tighter upper bound to expected support of X in tj than the item cap.

Proof: For any X={x1, . . . , xk} ? {y1, . . . , yr}=tj where xk=yr, (i) expSup(X, tj) = P (xk, tj)?P (xk?1, tj)?  ?k?2 i=1 P (xi, tj) ?  (ii) tubeP(X, tj) = P (xk, tj)?M1(xk, tj)? ?k?2  i=1 P (xi, tj) ? (iii) IC(X, tj) = P (xk, tj)?M1(xk, tj) because: (a) M1(xk, tj) is the highest probability among all P (yq, tj) for 1 ? q ? r ? 1, including P (xk?1, tj); (b) 0 ? P (xi, tj) ? 1 for 1 ? i ? k ? 2.

Again, to make our proposed tree structure compact and practical for mining, we merge tree paths when items are the same, regardless of their existential probability values. So, given two tree nodes x:IC(x, t1):P (x, t1) and x:IC(x, t2):P (x, t2) that share the same path p, we (i) sum their IC values and (ii) take the maximum among the two existential probabilities of x. This results in a tree node x:IC(x, t1)+IC(x, t2):max{P (x, t1), P (x, t2)}. Then, a tubeP for X={x1, . . . , xk} in p can be computed as:  tubeP(X, p)= {  IC(X, p) if k?2 IC(X, p) ? ?k?2i=1 P (xi, p) if k?3 (6)  where (i) IC(X, p) = ?  tj?p IC (X, tj) and (ii) P (xk, p) = maxtj?p {P (xk, tj)}. Consequently, the tubeP of X in the entire database is the sum of tubeP in every path p in the tree capturing the uncertain data:  tubeP(X) = ?  p  tubeP(X, p). (7)     Lemma 4: As expSup(X) ? tubeP(X) ? IC(X), the tubeP serves as a tighter upper bound to expected support of X (in the database) than the item cap.

Example 5: Continue with Example 4. For the 4-itemset X={a, b, c, d} appearing only in t1={a:0.5, b:0.2, c:0.1, d:0.9, e:0.6} and t4={a:0.9, b:0.1, c:0.1, d:0.8, e:0.6} in Table I, its expSup(X) is 0.009+0.0072 = 0.0162. As t1 and t4 share the same path p, (i) the node for d contains an IC value of (0.9?0.5)+(0.8?0.9) = 1.17. Since the prefix {a, b} of X also appears in t2 (with a:0.6 & b:0.1) as well as t3 (with a:0.7 & b:0.2), the prefix of p is also shared with t2 and t3. Hence, (ii) the node for a contains a probability value of max{0.5, 0.6, 0.7, 0.9} = 0.9 and (iii) the node for b contains a probability value of max{0.2, 0.1, 0.2, 0.1} = 0.2 so that tubeP(X, p) = 1.17?0.9?0.2 = 0.2106, which is much tighter than IC(X, p)=1.17. See Figure 1(d).



IV. FREQUENT ITEMSET MINING USING TUBES  Here, we propose two fast algorithms, called tubeS-growth and tubeP-growth, for frequent itemset mining from un- certain data. As implied by their names, both algorithms mine frequent itemsets in a tree-based frequent pattern- growth (i.e., divide-and-conquer) manner. One algorithm mines frequent itemsets by using the tubeS, while another one does so by using the tubeP.

A. Our Proposed tubeS-growth Algorithm  Our tubeS-growth algorithm first constructs a tree struc- ture, called tubeS-tree, to capture important information of uncertain data. Specifically, the algorithm scans an uncertain database to find all distinct frequent items (i.e., every domain item xi with expSup(xi) ? minsup). As expSup satisfies the downward closure property (i.e., if expSup(X) < minsup, then expSup(X ?) < minsup for all X ? ? X), infrequent items can be safely removed. Then, the algorithm scans the uncertain database a second time to insert each database transaction into the tubeS-tree in a fashion similar to that of the FP-tree. For example, ?prefixes? of two tree paths are merged if they share the same items. A key difference is that, when inserting a frequent item xi ? tj , we compute and capture both its IC value and M2 value in the node for xi.

Infrequent items in any transaction are omitted and not inserted into the tree. If a node containing xi already exists in a tree path p, we update the IC and M2 values by (i) adding its IC(xi, tj) into the existing IC(xi, p) and (ii) taking the maximum between its M2(xi, tj) and the existing M2(xi, p), as described in Section III-A. Otherwise, we create a new node for xi with its IC(xi, tj) and M2(xi, tj).

Once the tubeS-tree structure is constructed, our tubeS-growth algorithm then recursively mines frequent itemsets from the tubeS-tree. Specifically, for each frequent domain item xi (e.g., item d in Example 3), the algorithm constructs an {xi}-conditional tree by extracting all relevant tree paths (from xi to the root) and passing both IC(xi, p)  and M2(xi, p). By doing so, the algorithm computes tubeS for every 2-itemset {y, xi} (e.g., {c, d}) where y is a frequent item located above xi (i.e., closer to the root) on a tree path. If tubeS({y, xi}) ? minsup (i.e., {y, xi} is potentially frequent), then the algorithm performs a similar mining step by constructing a {y, xi}-conditional tree to mine potentially frequent 3-itemsets, and so on. Otherwise (i.e., tubeS({y, xi}) < minsup), (i) {y, xi} is guaranteed to be infrequent because expSup({y, xi}) ? tubeS({y, xi}).

Note that, although tubeS(Y ) may be higher or lower than tubeS(Y ?) in general for any itemset Y ? Y ?, tubeS(X) ? tubeS(X ?) for any ?ordered? X ? X ? sharing a common suffix item xi. Hence, if tubeS({y, xi}) < minsup, then (ii) we can safely prune {y, xi} and do not need to construct any {y, xi}-conditional tree.

After finding all potentially frequent itemsets (including true positives and false positives), our tubeS-growth algo- rithm scans the uncertain database a third time to verify whether or not if a potentially frequent itemset is truly frequent (i.e., true positives).

B. Our Proposed tubeP-growth Algorithm  In contrast, our tubeP-growth algorithm constructs its corresponding tree structure, called tubeP-tree, in a similar fashion to that for the tubeS-growth algorithm, except that P (xi, tj) is captured instead of the M2(xi, tj) value (see Section III-B).

Once the tubeP-tree structure is constructed, our tubeP-growth algorithm then recursively mines frequent itemsets from the tubeP-tree in a similar fashion as our tubeS-growth algorithm. For each frequent domain item xi (e.g., item d in Example 5), the tubeP-growth constructs an {xi}-conditional tree by extracting all relevant tree paths and passing only IC(xi, p) but not P (xi, p) because tubeP for any 2-itemset {y, xi} (e.g., {c, d}) is computed using P (y, p) instead of P (xi, p) where y is a frequent item located above xi on a tree path. Similar to tubeS-growth, our tubeP-growth algorithm also scans the uncertain data a third time to remove all false positives.



V. EVALUATION RESULTS AND DISCUSSION  We evaluated the performances of our proposed tubeS-growth and tubeP-growth algorithms by comparing them with the existing PUF-growth [12] algorithm. Recall from Section I that PUF-growth [12] was reported to be faster than CUF-growth [11], UH-Mine [1], UFP-growth [1], and UF-growth [10]. We used (i) synthetic datasets, which are generally sparse and are generated by the IBM Quest synthetic data generator [2], and (ii) several real life datasets (e.g., mushroom, retail and kosarack) from the Frequent Itemset Mining Dataset Repository (http://fimi.ua.

ac.be/data/). We assigned a (randomly generated) existential probability value from the range (0,1] to each item in every transaction in the dataset. As we obtained consistent results     Figure 2. Evaluation results  for all of these datasets, we report the evaluation results on only a subset of these datasets in the remainder of this section: (i) an IBM synthetic dataset containing 100K transactions with an average transaction length of 5, and each of the 1000 domain items in a transaction is associated with an existential probability value within a range of [0.1, 1.0]; (ii) a real-life mushroom dataset with existential probability values within a range of [0.5, 0.6]. All programs were written in C++ and run in a Linux environment on an Intel Core i5-661 CPU with 3.33GHz and 7.5GB RAM.

A. Comparisons on Our Proposed Algorithms with Existing Algorithms  In terms of the memory consumption, both tubeS-tree and tubeP-trees are compact in practice: Their numbers of nodes in the global tree can be equal to that for the FP-tree.

Regarding the tighteness of bounds, when |X | ? 2, both tubeS and tubeP values become the IC value so that the bounds of any frequent itemsset X provided by the tubeS- growth and tubeP-growth algorithms are as good as that provided by the PUF-growth algorithm. When |X ?| ? 3, both tubeS and tubeP values are smaller than or equal to the IC value so that the bounds of any frequent itemsset X ?  provided by the tubeS-growth and tubeP-growth algorithms are tighter or as good as that provided by the PUF-growth algorithm.

However, their overall performances depend on the num- ber of false positives generated. In this experiment, we measured the number of false positives generated by our two algorithms when compared with PUF-growth, for fixed values of minsup with different datasets. From Figure 2(a), our tubeS-growth and tubeP-growth were observed to re-  markably reduce the number of false positives when com- pared with PUF-growth. The primary reason of this im- provement is that the upper bounds for both algorithms are much tighter than PUF-growth for higher cardinality itemsets (k > 2). Hence, fewer candidates are generated in total, and subsequently fewer false positives. In fact, when existential probability values were distributed over a narrow range with a higher minsup, both tubeS-growth and tubeP-growth generated fewer than 1% of the total false positives of PUF-growth. When existential probability values were distributed over wide range with a low minsup, tubeS-growth generated fewer than 55% of the total false positives of PUF-growth.

In terms of runtime, we compared our tubeS-growth and tubeP-growth algorithms with PUF-growth. Recall that PUF-growth was shown to outperform UH-Mine [12] and subsequently UFP-growth [1], [17]. Hence, Figure 2(b) shows that tubeS-growth and tubeP-growth both had shorter runtimes than PUF-growth for the IBM synthetic and real- life mushroom datasets. The primary reason is that, even though PUF-growth finds the exact set of frequent itemsets when mining an extension of X , it may suffer from the high computation cost of generating unnecessarily large numbers of candidates due to only using two values in its item cap calculation: the existential probability of the suffix item and the single highest existential probability value in the prefix of xr in tj . This allows large amounts of high cardinality candidates to be generated with similar expected support cap values as low cardinality candidates with the same suffix item. The use of the M2(xi, p) and P (xi, p) values in tubeS-growth and tubeP-growth, respectively, ensure that those high cardinality candidates are never generated due to     their expected support caps being much closer to the actual expected support.

To evaluate the scalability of tubeS-growth and tubeP-growth, we applied both algorithms to mine frequent itemsets from datasets with increasing size. The evaluation results presented in Figure 2(c) indicate that our algorithms (i) are scalable with respect to the number of transactions and (ii) can mine large volumes of uncertain data within a reasonable amount of time.

The evaluation results show that our algorithms effectively mine frequent itemsets from uncertain data irrespective of distribution of existential probability values (whether most of them have low or high values and whether they are distributed into a narrow or wide range of values).

B. Comparisons between Our Two Proposed Algorithms  In the previous section, we showed that our proposed both tubeS-growth and tubeP-growth algorithms produced fewer false positives and ran faster than the existing PUF-growth algorithm. We also showed that our proposed both tubeS-growth and tubeP-growth algorithms are scal- able with respect to the number of transactions. Here, let us compare between the tubeS-growth and tubeP-growth algorithms.

In terms of the number of false positives, we note in Figure 2(a) that, for k = 3-itemsets, tubeS-growth actually had fewer false positives than tubeP-growth. However, the total number of false positives in tubeP-growth was much lower (e.g., fewer than 20% in total, with fewer than 60% for k = 4-itemsets and fewer than 20% for higher cardinality levels) than that of tubeS-growth. This happens because for lower cardinality (typically around k = 3) itemsets, the P (xi, p) value may actually be higher than the M2(xi, p) value. When different items from different transactions end up contributing to the M2(xi, p) value, the maximum value of a single item over all the transactions containing X is more likely to be higher than the maximum of all the second highest values (regardless of item) in those transactions. The chance of this happening decreases significantly with higher cardinality itemsets however, and as a result tubeP-growth had a runtime less than or equal to that of tubeS-growth in every single experiment we ran.

Regarding the runtime, Figure 2(c) shows that for low values of minsup, tubeP-growth had shorter runtimes than tubeS-growth. The primary reason is that for lower values of minsup, the number of high cardinality candidates being generated increases. In this situation, the probability is higher than the P (xi, p) values in each node will actually be lower than the M2(xi, tj) value of tubeS-growth, tightening the upper bound even further.



VI. CONCLUSIONS  In this paper, we proposed fast algorithms for frequent itemset mining from uncertain data. The algorithms capture  important information from uncertain data (e.g., items and their existential probabilities) in tree structures, from which potential frequent itemsets can be mined using tightened upper bounds to expected support (tube) based on either (i) a second highest existential probability (for the tubeS-growth algorithm) or (ii) existential probabilities of ?prefix? items (for the tubeP-growth algorithm). The bounds provided by the tube guarantee not to generate any false negatives.

Moreover, experimental results show that the bounds pro- vided by tube are much tighter, significantly fewer false positives (e.g., tubeS-growth has 1% to 55% of the false positives generated by the existing PUF-growth algorithm, tubeP-growth gives an additional 20% reduction) are gener- ated in immediate mining steps, and thus mining speeds are faster. All false positives are removed in the final mining steps. Hence, all frequent itemsets returned to the user are guaranteed not to contain any false positives.

