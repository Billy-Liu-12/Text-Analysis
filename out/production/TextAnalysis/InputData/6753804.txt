Cluster-size Scaling and MapReduce Execution Times    Fan Zhang

ABSTRACT Understanding performance scalability in MapReduce  applications presents a challenging problem.  The difficulty lies in the distributed locations of input data and the distributed compute resources that utilize varied network substrates.  User-defined Map and Reduce stages, with numerous application parameters, further complicate the problem. Using small datasets and limited test runs to understand how MapReduce applications will behave with "big data" can have a significant payoff.

In this paper, we evaluate the impact of cluster-size scaling on execution time for a set of Map- and Reduce- intensive applications.  We model the MapReduce framework, specify conditions and implications of power- law conformity, and verify our model with data from benchmark MapReduce applications.

Empirical results indicate that:  (1) within a range of scaling parameters, MapReduce execution times follow a power-law distribution.  (2) Power-law scalability for Map- intensive applications starts from a small cluster size.  (3) Shuffle-intensive applications exhibit power-law behavior starting from larger clusters.  (4) Cluster-scaling performance gains fail to show power-law behavior when computing resources far exceed those needed.

Our findings will facilitate using small-scale test runs to allocate and configure virtual and physical computing resources in large scale clouds.

Keywords:  MapReduce applications; Cluster scaling; Power-law distribution; Small-scale limitation; Large- scale over-provisioning  1. INTRODUCTION The MapReduce programming model [1] is widely  used to address the rapid growth in "big-data" processing demands. Within this framework, applications that process large-volume input data can be deployed on a compute cloud comprising numerous, virtualized resources [2] [3] [4], using services such as Amazon EC2 [5].

Users can often gain insight into the behavior of MapReduce applications by understanding how performance scales with computing resources.  Although MapReduce is known for processing "embarrassingly parallel" workloads, characterizing its performance scalability is not straightforward [6]. The difficulties lie in the user-defined Map and Reduce processes, which include reading, computing, shuffling, and writing data over distributed file systems.  In a large datacenter such file systems typically span multiple network substrates.

The problem becomes even more complicated when employing public clouds, such as Amazon EC2, and Virtual Machines (VM) instances [7].  The VM instances might reside inside one physical host, span several hosts in a single rack, or cross numerous racks and datacenters.

Unpredictable VM locations add further variability in application performance [8].

In this paper, we examine the performance-scaling patterns of MapReduce applications on both a local cluster and Amazon EC2 and identify how the VM count ? that is, cluster size ? affects execution time.  Initial observations, as in Figure 1, suggest a power-law-like relation to cluster size.

Consistent with this conjecture, we observe what we term a small-scale limitation:  For certain Map-intensive applications, such as WordCount, we find power-law behavior even with a small cluster size (a single VM).

For other applications, such as Sort, whose Shuffle phase is more intensive, power-law conformance begins only with larger clusters (two or more VMs).

At the other cluster-size extreme (16 and 32 VMs), we also observe a large-scale over-provisioning, suggesting that power-law behavior breaks down when allocated compute resources significantly exceed optimal requirements, as shown by the rectangular box in Figure 1.

More resources simply degrade, rather than improve performance.

Figure 1.  Across different cluster sizes, WordCount and Sort  performance suggest power-law behavior  Our research estimates appropriate parameters for studying the performance of MapReduce applications as a special case of the power-law phenomena with two bounds, small-scale limitations and large-scale over- provisioning.  We also present here a mathematical model that captures performance changes as the cluster size   DOI 10.1109/CloudCom.2013.39       increases.  Further analysis of the model confirms the bounded power-law for benchmark applications on a local cluster and Amazon EC2 cloud.

Our approach yields insights that MapReduce users can employ when scaling cluster size.  This capability will aid in using test-run results to select and configure computational resources for cloud applications.

The rest of the paper is organized as follows:  After describing related work in Section 2, we begin modeling MapReduce execution time in Section 3.  Section 4 constrains appropriate power-law scaling parameters and investigates the small- and large-scale bounds of power- law behavior.  Empirical studies of Section 5 verify our model with benchmark MapReduce applications on both local cluster and Amazon EC2 virtual machines.  Section 6 summarizes our contributions and suggests directions for future work.

2. RELATED WORK Numerous researchers have developed mathematical  models to study MapReduce scalability issues.  We summarize relevant contributions briefly and introduce the useful power-law distribution, frequently observed across a wide range of natural and artificial phenomena.

A.  Modeling the MapReduce framework  Herodotou [9] subdivided the MapReduce framework into a sequence of read, Map, collect, spill, merge, Shuffle, Reduce, merge, and write stages and developed a Hadoop Performance model that analyzes each MapReduce phase at a fine granularity.  While their model estimates dataflow and computation costs, they reported no empirical tests.

In previous work [10], we built an analytic model of the MapReduce framework and used it to describe how scaling input datasets affects execution time in the Map, Shuffle, and Reduce stages.  The model identifies four types of MapReduce applications and shows their scaling properties and impacts on the execution time.

Yang and Sun [11] proposed an analytical performance model that considers scheduling from a job level and confirmed its predictive accuracy under dataset scaling.  Tian and Chen [12] went a step further, incorporating a cost function to measure the relationship between input dataset size and available resources.  They successfully predicted scaled performance and, in addition, showed how to allocate resources (e.g.  number of virtual-machine instances) optimally.

This work, by contrast, focuses on scaling the cluster size and verifies model predictions in empirical tests.

B.  Computer systems and the power-law distribution  Many interesting empirical quantities cannot be sufficiently represented by just their mean and standard deviation.  For more complex phenomena, the power-law  distribution frequently provides both a good fit and useful mathematical properties.  Such situations appear in numerous, diverse domains, including city population sizes, earthquake intensities, and power-outage impacts.

In computer architecture, cache miss rate follows a power law function of cache capacity [13] [14], and later research revealed that, from both empirical [15] and theoretical [16] perspectives, the miss rate decreases as a power law function of cache size for large workloads.

Internet topology, characterized at an autonomous system level, also follows a power-law distribution [17] [18].

Our investigation, unlike previous studies, observed power-law behavior only within a range of cluster sizes, which constraint is consistent with our analysis.

3. MODELING MAPREDUCE EXECUTION TIME Building on simple assumptions, we derive  expressions for expected execution times of the two critical MapReduce processes, Map and Shuffle.

A.  Notations, definitions, and assumptions  TABLE 1.  NOTATIONS AND BASIC DEFINITIONS  Term Basic Definition I Input dataset size S Shuffled dataset size m Number of Map tasks r Number of Reduce tasks c Cluster size (number of VMs employed) p Average data processing rate of each Map task  bL Bandwidth within one VM (memory data copy) bR Bandwidth between VMs within a local rack bD Bandwidth between VMs outside the local rack ?c   Speedup using c VMs compared with one  Ecm   Execution time of Map stage using c VMs Ecs   Execution time of Shuffle phase using c VMs Pi   Probability that one rack has i VMs, and other  racks have either more than i VMs or none.

Assumption 1:  Within the MapReduce framework, denote the total input data size by I and the total number of Map tasks by m.  Then ? excluding special tasks, such as startup and cleanup ? each Map task will process, on average, approximately I/m data.

Assumption 2:  Let S denote the total Map output to be Shuffled and r be the total number of Reduce tasks.  Each Map task then sends approximately S/(r?m) data to each Reduce task, and the expected amount of data each Reduce task receives is S/r.

The shuffle phase normally belongs to the generalized reduce phase in a typical two-phase MapReduce application. Without causing further confusion, the term Reduce-intensive applications refer to the shuffle-intensive applications in this paper.

??? ? ??????????  ???????	?   ? ? ???  ???   ?? ???  ???    ???????	?   ? ?   ?? ?  ???   ? ?  ?   ? ?  ?   ?? ???  ???    ?????? ?????? ????????? ?????? ?????? ??????  ?????? ?????? ?????? ?????? ?????? ??????  ???????	?   ? ? ???  ???   ? ?  ?    ?????? ??????  ??????? ????? ???   ? ? ???  ?   ? ?  ?    ?????? ??????  ???????? ???????????   ? ? ???  ?   ? ?  ?    ?????? ??????  ???????? ???????????   ? ? ???  ?   ?? ???  ???    ?????? ??????  ???????? ??????????????? ???????????????????????? ?? ??  ?  ???????	?   ? ? ???  ???   ? ?  ?    ?????? ??????  ??????? ??? ???   ? ? ???  ?   ? ?  ???    ??????  ??????????????    ????????  ? ? ? ???? ??????? ?? ? ??????????? ??? ? ????????????  ??????   Figure 2.  MapReduce frameworks of varied cluster sizes  MapReduce employs a hash function to determine the destination Reduce task for each data partition.  This random shuffling tends to deploy data evenly among Reduce tasks.  While some studies [19][20] have observed unbalanced Shuffled data across different Reduce tasks, we claim that, on average, a general uniform distribution is a reasonable assumption.

In addition, as Map-task data volume grows, intermediate data will tend to spread more evenly among Reduce tasks.  Consider, for example, an application with ten Reduce tasks.  In a scenario with 20 data partitions, each Reduce task will receive approximately two.  If the data scale to 200 partitions, 20 will go to each Reduce task.  So, as the number of Shuffled partitions goes up, the likelihood of uniform distribution also increases.

B.  Cluster size and MapReduce execution  Execution of a typical MapReduce application comprises Map and Reduce stages.  In Hadoop [21], the Reduce stage includes Shuffle, Sort, and Reduce phases.

The Shuffle process takes intermediate data produced by each Map task and distributes data partitions over the network to one or more Reducers.  So Shuffle is normally data-intensive.

Sort and Reduce, by contrast, proceed locally and generally faster than Map and Shuffle.  To simplify our mathematical model, we consider the Map stage and Shuffle phase as the dominant modeling targets.

Case a:  One-VM cluster  Let ?c denote the performance speedup when using c VMs compared to one.  For the single-VM case, this factor is unity by definition, and, where p denotes the average processing capacity of each Map task, expected Map-stage execution time is:  E1m = I / (?1?p?m) (1.1) Due to the resource contention in the case of c VMs,  ?c will normally be smaller than c.  So Equation 1.1 generalizes to:  Ecm = I / (?c?p?m) For a single-VM cluster, all data are Shuffled within  the local machine, a process requiring memory-copy only and no network traffic as shown in Fig. 2(a).  So where bL denotes the local bandwidth within one VM, Shuffle- phase execution is:  E1s = S / (m?bL) (1.2) Case b:  Two-VM Cluster  In a cluster of two VMs, each Map task Shuffles half its output data, S/(2?m), to the other VM as shown in Fig.

2(b). The data is being shuffled in parallel to the two VMs.

Therefore; the Shuffle time depends on the slower part.

The two VMs might be co-located in one rack or reside in different racks.  Three expressions:  S/(2?m?bL), S/(2?m?bR), and S/(2?m?bD) represent Shuffle times within one VM, within a single rack, and off-rack, elsewhere in the datacenter, respectively.  For simplicity, we will abbreviate with S(2,bL), S(2,bR), and S(2,bD).

If we indicate by Pa and Pb the probabilities that the two VMs lie in one or two racks, respectively, then:  E2s = max{S(2,bL), S(2,bR)}?Pa+ max{S(2,bL),S(2,bD)}?Pb (2.2) We will return to estimating these probabilities in  Section 3.C.

Case c:  Four-VM Cluster  In the four-VM case, each Map task sends one quarter of its total output data to Reduce tasks in the local VM and the remainder to the other VMs, which might be in the same rack or different racks as shown in Fig. 2(c).

Figure 3 shows a total of five possible different scenarios:  ????????  ?????  ?????  ?????  ?????????  ????? ???? ????  ????  ?????? ?????  ?		?  ??????? ???????? ????????  ?			?  ????????????????  ?????  ????  ???? ????  ????? ???????????  ???? ????  ????????  ??? ???? ??????????? ??? ???? 	?????????	 ? ? ???? ?? ????????  ??? ???? ??????????? ??? ???? ???????????  Figure 3.  Five possible different scenarios of four VMs being in one to four racks, with Map tasks as colored circles.

? In Figure 3(a), all four VMs reside in the same rack.

Each Map task ships one quarter of its output data to     local Reduce tasks (line 11) and three quarters to the other three VMs (lines 12-14).  Denoting by Pa the probability that four VMs live in one rack, and using the Case b Shuffle-time notation, we get the expected Shuffle time as:  E4s(a) = max{S(4,bL), 3*S(4,bR)}?Pa (3) ? In Figure 3(b), three VMs are in one rack and the  fourth is in a different one.  Here we find two kinds of Map task, distinguished by Shuffle time and colored red or gray.  Gray tasks Shuffle a quarter of their output to Reduce tasks in the local VM (line 21), half to Reduce tasks within the same rack but in different VMs (lines 22-23), and one quarter to off- rack Reduce tasks (line 24).

If the probability of such a VM allocation is Pb, Shuffle time is:  E4s(b)? = max { S(4,bL), 2 S(4,bR), S(4,bD)}?Pb  (4.1) and a similar analysis for red tasks, which Shuffle three quarters of their output off-rack, leads to: E4s(b)?= max { S(4,bL), 3*S(4,bD)}?Pb  (4.2) Notice that red Map tasks take longer, due to more off-rack communication, thus they dominate Shuffle time for this case, so we have, finally:  E4s(b) = max { S(4,bL), 3*S(4,bD)}?Pb   (4.3) ? In Figure 3(c), two VMs are in one rack, and the third  and the fourth are in other, separate racks.  Again we find two flavors of Map task, shown in red and gray, and, using the same analysis, we have Shuffle time as:  E4s(c) = max { S(4,bL), 3*S(4,bD)}?Pc  (5) ? For the remaining two cases, Figure 3(d) and (e), we  similarly represent Shuffle times as: E4s(d)= max { S(4,bL), S(4,bR), 2*S(4,bD)}?Pd (6) E4s(e) = max { S(4,bL), 3* S(4,bD)}?Pe (7)  Now, summing all five cases gives the expected Shuffle time with four VMs:  E4s = E4s(a) + E4s(b) + ? + E4s(e) (8) Notice, however, that the expressions for (b), (c), and (e) are the same except for appended possibilities; thus we have:  E4s= max { S(4,bL), 3 S(4,bD)}?(Pb + Pc +Pe) + max{ S(4,bL), S(4,bR), 2 S(4,bD)}?Pd + max { S(4,bL), 3 S(4,bR)}?Pa (9) The similarity of cases (b), (c), and (e) derives from  the fact that at least one rack has exactly one VM, and a four-VM cluster has one, two, and four such racks.

C.  Classification on Generalized cluster size  Based on the preceding analysis, we consider the general case of c VMs. If we follow the enumeration approach in Section B, the case count would become very large and intractable when c becomes very large. As an  alternative, we employ a new classification approach to generalize the expected Shuffle time as below.

Suppose each rack either has at least j VMs or none VM at all, where (1 ? j? c). Consider the following four conditions, which we will then elaborate: ? j = 1.  Here one rack has a single VM, and the rest  have one or more.  Denote the probability of this scenario as P1, a situation shown in Figure 4(a).

Expected Shuffle time is:  Ecs(1) = max{S(c,bL), (c-1) S(c,bD)}?P1   (10)  ? j = 2, where we have: Ecs(2) = max{ S(c,bL), S(c,bR), (c-2) S(c,bD)}?P2 (11)  ? j = c, where Shuffle time is: Ecs(c) = max{ S(c,bL), (c-1) S(c,bR)}?Pc   (12)  ? The general case as shown in Figure 4(b), where j = i, and Shuffle time is:  Ecs(i)=max{S(c,bL),(i-1)S(c,bR), (m-i)S(c,bD)}?Pi  (13)              ?????  ????  ?????????  ???????????? ????????????  ?????  ???? ??????? ???? ???????   (a) Rack 1 has one VM. The rest non-empty racks have at least one VM  ????????????  ???????????? ????????????  ???????????? ????????????  ????????????    (b) Rack 1 has i VMs. The rest non-empty racks have at least i VMs Figure 4.  Two general cases of j (j = 1, i) VMs in at least c racks.

Thus, the composite expected Shuffle time is: Ecs = Ecs (1) + Ecs (2) + ? + Ecs (c)  (14)  Enumerating scenarios above has created a general probability model, upon which we utilize to achieve the segmented power-law execution time conclusion.

D.  Estimating configuration probabilities  We now return to the likelihoods, previously denoted Pa, Pb, ? and P1, P2, ?, of encountering the various VM configurations.  Our analysis rest on two assumptions: ? The VMs are homogeneous ? Available racks are sufficient (number > c) to hold all  VMs.

Now consider the number of distinct configurations possible for a given cluster size.  This situation presents a classical unordered partition problem [22], namely enumerating the ways of putting c identical balls into c identical boxes.

For c VMs, the number of different configurations is n(c), the integer partition number, and from previous analyses, we know n(1) = 1, n(2) = 2 and n(4) = 5.  Since detailed mathematics of this relationship are beyond the scope of this paper, we summarize in Table 2 partition numbers for one- to 32-VM clusters.

TABLE 2.  CLUSTER SIZE AND UNORDERED PARTITION NUMBER  c 1 2 4 8 16 32 n(c) 1 2 5 22 231 8349  In cases a, b, and c of section 3.B, for example, Pa, Pb, Pc, Pd, and Pe, are 1/n(4) = 0.2.  For large cluster sizes, this enumeration method becomes awkward, so we used another representation to generalize in case d.

In the generalized case, we indicated with P1 the likelihood of the j = 1 condition, only one rack having a single VM.  Now let us examine that probability more closely.  Table 3 shows typical P1 values and corresponding scaled cluster sizes.

TABLE 3.  CLUSTER SIZE AND P1  c 1 2 3 4 5 6 7 8 P1 1 0.5 0.667 0.6 0.714 0.636 0.8 0.68  Notice that P1 is larger than 0.6 most of the time.

Besides P1, we must also consider P2, P3, ..., Pc.

Intuitively, P1 > P2 > P3 >..., and those probabilities must all sum to unity.  We can thus conclude that P1 dominates the other conditions and Shuffle time associated with P1 is larger than those associated with P2, P3, ...  .

Furthermore, the rack with exactly one VM will Shuffle off-rack data of size c-1/c, which transfer takes longer than other cases.  Case 1 (j = 1) thus dominates, and we use Ecs(1) to estimate the generalized expected Shuffle time.

Based on these refinements, we can now describe the expected execution time of MapReduce computations:  Ecm =  I / (?c?p?m) (15.1) Ecs ? max {S(c,bL), (c-1) S(c,bD)}?P1 ? (c-1) S(c,bD)?P1 (15.2)  4. SEGMENTED POWER-LAW DISTRIBUTION In this section, we investigate the conditions under  which power-law behavior can be induced based on Equation 15 above in MapReduce applications when scaling cluster size.

A power-law distribution has the functional form f (x) ? a xb, where a and b are constants, and two useful properties: ? Statistical utility ? There exists a constant C that can  normalize the integral, so that ? ? ? ? ??? ?  ? ?.

? Scaling invariance ? The exponent b is known as the  scaling parameter, and scaling the argument x by a constant k simply multiplies the original function by kb:  f (kx) ? a (kx)b = kb f (x)  (16) While power-law distributions successfully describe  many real-world phenomena, few adhere precisely for all values of the function's single argument.  Usually, the power law applies only for values within some range.

A.  Power-law domain bounds  For this study, we interpret the exponent b as performance speedup.  Since additional resources should reduce execution time, b must be negative and, in general, unless (b < -1), no value of C can normalize the function [23]. To qualify our formulation as a conventional distribution function, then, we should also require b < -1.

Our cluster-scaling scenario, however, seems to defy convention, since the reality of resource-contention overhead precludes b < -1.  Where b = -2, for example, a doubled cluster size should reduce execution time by a factor of four.  In practice, however, that situation is unrealistic, and the most optimistic expectation would be doubled performance.  Thus, a reasonable candidate for b must fall between -1 and 0.

Now, for (b < 0), we still cannot normalize f(x), since ?????????? ? ?, and the usual solution is to select a value xmin, above which we can expect to observe a power-law distribution [23].  Similarly, with an infinite upper bound, no constant value C will satisfy ? ??  ????  ? ? ?? ? ?? ? ? ? ? ??, and we choose an upper bound xmax. The resulting integral, ? ? ? ? ??  ????  ???? ? ?? ? ?  ? ? ?? describes a power-law distribution over a closed region [xmin, xmax], and we call the constrained f(x) a "segmented power-law distribution." B.  Cluster-size scaling limits  In seeking power-law performance under cluster scaling, our approach thus prunes empirical MapReduce data by discarding extreme values that clearly deviate from power-law behavior.  We now offer justification for the small- and large-scale limits implied by the finite range [xmin, xmax] and describe a procedure for selecting appropriate power-law domain bounds.

Small-scale limitation  We understand the small-scale limitation intuitively as follows:  For Shuffle-intensive applications, whose execution time is dominated by moving intermediate data,     the relative locations of VMs impact execution time more than in Map-intensive jobs.  Meanwhile, in large clusters, the more numerous VMs tend to distribute more evenly among racks than in small clusters.  More uniformly distributed VMs lead to greater visibility of a power-law distribution in Shuffle execution time.

Consider an example of two possible cluster organizations, where (x, y) indicates two racks with x and y VMs, respectively. The likelihood that a four-VM cluster is organized as (3, 1) significantly exceeds that of finding a 64-VM cluster configured as (63, 1).

In addition, we should exclude the one-VM case in Shuffle-intensive applications because Shuffle-phase data transfer there involves only memory-copy and no network traffic. The Sort benchmark in Figure 1 illustrates this point.

Large-scale over-provisioning  At the other end of the power-law range, we find a large-scale over-provisioning:  Running a not-so-big-data MapReduce application in an excessively large cluster, we observe that performance declines even when we further increase cluster size.

The problem stems from under-utilized computing resources that still entail a processing overhead to maintain task-trackers over all VMs.  Our solution is to select an appropriate xmax as the upper scaling bound and so minimize any performance degradation.

Determining domain-bound values  To verify power-law conformation, a typical strategy [25], which is being supported by a log fit function of the Matlab toolkit, would follow three steps: 1. For each MapReduce application with a given input-  data size, generate two-dimensional data points (cluster size, performance).  Plot cluster size on the abscissa and the corresponding execution time on the ordinate.

2. Use a log-log scale to linearize the data.  Since the power law is ExeTime{x=c} ? c?b, then log (ExeTime {x=c}) = ?b?log(c) + a, and the resulting plot reveals a straight line with a negative slope where the power law applies.

3. Apply least-squares regression to estimate slope and intersection of the best fitting line within the range [xmin, xmax].  If the coefficient of determination (R2) is high, or if the line appears a good fit, then execution times approximate a power-law distribution, and the regression equation gives its parameters.

B.  Scaling invariance We now show that, under certain conditions and like  a power-law function, the expected execution times we derived in section 3 also scale invariantly with cluster size.

Based on the relative execution times in the Map stage and Shuffle phase, we categorize MapReduce applications  into two types, as in Figure 5.  If Map takes more time than Shuffle, the application is "Map-intensive." Otherwise we call it "Shuffle-intensive."    Figure 5.  Two types of MapReduce application  Execution times of Map-intensive applications are dominated by the Map stage and are represented by:  Ec ? Ecmap = I / (?c?p?m)   (16) When the cluster size is increased from c to kc, the  ratio of the execution times is denoted by: Ekc/ Ec = ?kc / ?c ? k   (17)  This ratio is a near-constant value close to k, and the Map stage is easily parallelizable, given the "embarrassingly parallel" nature of dividable input data.

Thus, in Map-intensive MapReduce applications, a cluster with kc VMs can easily achieve a speedup close to k over the c VM baseline.  So these applications display scaling invariance.

When the Shuffle phase dominates execution time: Ec ? EcShuffle ? (c-1) S(c,bD)?P1  (18) If we scale cluster size from c to kc, the ratio of  execution times is: Ekc/ Ec = [1 + 1/k(c-1)]?P?1 /P1 ? 1 + 1/k(c-1) (19) As c gets sufficiently large, this ratio approaches unity  for any k. Thus scaling gains become increasingly consistent with power-law behavior as the cluster size increases.

For example, if we double a small (c = 2) cluster, the execution-time ratio is E4/ E2 ? 1 + 1/2(2-1) = 1.5.  A large cluster (c = 16) under the same scaling, however, yields a gain of E32/ E16 = 1.016.  So, also for Shuffle- intensive applications on a large cluster, our expected execution times display scaling invariance.

For small values of c, on the other hand, Equation (19) illustrates the small-scale limitation and associated divergence from power-law behavior at the lower bound xmin.

Thus, in summary, we can expect to observe behavior weakly consistent with a power-law distribution when:  ? Shuffle-intensive applications run on a small cluster  Map-intensive  Ex ec  ut io  n Ti  m e  Map Stage Shuffled Stage  Shuffle-intensive       ? Map- or Shuffle-intensive applications run on a large cluster.

5. EXPERIMENTAL STUDIES To verify our analyses empirically, we measured  seven benchmark MapReduce applications on two cloud platforms ? a local cluster and an Amazon EC2 cloud ? and evaluated the fit of their execution times to a power- law regression function of cluster size.

A.  Experimental setting Compute resources  Our local cluster comprised a dedicated, 14-host, IBM BladeCenter H with identical hardware and software configurations on each physical machine.  We configured the BladeCenter with VMware vSphere 4.1 and a VMware ESXi 4.1 hypervisor.  Each VM employed two v-CPUs and 2 GBs RAM and had two locally-connected, 300GB, SAS disks.  Each VM ran 64-bit Fedora 13, Apache Hadoop 0.20.2, and Sun/Oracle?s JDK 1.6, Update 20.  To test cluster-size scaling efficiency, we deployed up to 32 VMs on the blades.

The public cloud employed varied numbers of standard and homogeneous instances on Amazon's EC2 cloud.  Each VM was an m1.small instance with 1.7 GB of memory, 1 EC2 Compute Unit (one virtual core and one EC2 Compute Unit), and 160 GB local storage.

These 32-bit platforms deployed on Ubuntu AMI, with EBS boot, and Amazon EC2 AMI Tools.  As in the local cluster, each VM ran Apache Hadoop and Sun/Oracle?s SDK.  We used Apache Whirr [24] to launch and manage up to 64 VM instances on demand.  All the VM instances were run within a single EC2 zone.

Benchmarks  We used the following applications, spanning Map- and Shuffle-intensive processing, as benchmarks: 1. WordCount is a typical Map-intensive application  with combiner enabled by default.  Shuffled data size is less than 0.1% of the input dataset, which is normally generated from a Random Text Writer function using a fixed-size database.

2. The Sort application is more Shuffle- than Map- intensive.  Map simply passes input data and Shuffle moves it all to the Reduce process.  The input dataset is the same size as for WordCount.

3. Random Text Writer (RTW) is normally considered a Reduce-less application that generates input data set for WordCount.

4. Inverted Index(II) takes a list of documents as input and generates word-to-document mapping.

5. Term-Vector (TV), typically used to evaluate a host?s relevance to a particular search, determines the most frequent words in a set of documents.

6. Grep, a generic search tool used in many data analyses, searches for a target pattern within a file.

7. Pi Estimation (PE) is another Map-intensive application but involves no HDFS I/O in Map.  The Map function itself generates all the data it uses and, in separate runs to estimate the value of Pi, samples 106, 107, 108, and 109 points.

Sort is a Shuffle-intensive application while the rest  are all Map-intensive.  Input datasets are all real Wikipedia webpages.  For the first six benchmarks, the input data sizes are 0.5G, 1G, 2G, 4G, and 8G on the local cluster, and 4G, 8G, 16G, and 32G on Amazon EC2.  An average of four experimental runs of each application is given as performance data.

Due to space limitations, we here show four benchmark experiments in local cluster runs and five in EC2 runs.  As the only shuffle intensive application, Sort appears in both contexts.

B.  Data Analysis Local cluster studies  Figure 6 plots the coefficient of determination (R2) for the MapReduce applications tested on our local cluster, and the abscissa enumerates VM counts (c values) corresponding to each (xmin, xmax) combination.  We discuss here how each of these lower- and upper-bound pairs affects goodness-of-fit to power-law execution time under cluster-size scaling: ? In Figure 6(a), most of the R2 values exceed 0.9,  suggesting a power-law distribution of execution time in those cases.  The exception is the 0.5GB line, whose cluster size bounds are (8, 32), (4, 32), and (2, 32).  The outlying datapoints represent the minimum input dataset (0.5GB) to the maximum cluster size (32 VMs).  This typical display of the large-scale over-provisioning implies the 32-VM cluster size is more than enough for the input data size.

? Figure 6(b) similarly illustrates a large-scale limitation.  The difference here is that even the smallest input dataset (106 points) demonstrates a high R2.  The minimum R2 value is above 0.93, which again occurs with the (8, 32) cluster-size range but indicates that the large-scale over-provisioning arises later than in Figure 6(a).  In this case we can extend xmax beyond the value corresponding to c = 32.

? In Figure 6(c), the 0.5GB and 1GB lines exhibit a larger variance in R2 values than other lines, implying that those input datasets are too small to yield power- law scaling gains.  R2 values generally improve, typically exceeding 0.8, with larger cluster-sizes and larger input datasets.  For example, the 8GB line's R2 values are all above 0.9.

The first three applications are Map-intensive and so do not encounter the small-scale limitation, although     each shows a large-scale over-provisioning at some level.

? Figure 6(d) reports data on Sort, a Reduce-intensive application, and shows much larger R2 variance than the Map-intensive benchmarks.  Again, this situation is more likely when processing small datasets (0.5GB  and 1GB) and confirms a large-scale over- provisioning.  In cases where xmin = 1, however, even a large input dataset fails to yield high R2.  For example, in the cluster-size range (1,4) with 8GB input, R2 falls almost to zero.

(a) WordCount on LC       (b) PE on LC   (c)  RTW  Estimation  on  LC       (d) Sort on LC  Figure 6.  Coefficient of determination (R2) of applications on local cluster between xmin and xmax (xmin, xmax)  (a) WordCount on EC2, xmin=1      (b) TV on EC2, xmin=1     (c) II on EC2, xmin=1      (d) Grep on EC2, xmin=1   (e) Sort on EC2, xmin=1     (f) Sort on EC2, xmin=2  (g) Sort on EC2, xmin=4  Figure 7.  Coefficient of determination (R2) of applications on Amazon EC2 between xmin and xmax (xmin, xmax)  Public cloud studies Starting with a small cluster size (xmin = 1), the Map-  intensive benchmarks of Figures 7(a) to 7(d) all show high R2 values (> 0.9) and indicate good fit with a power-law distribution.  This observation confirms that Map-intensive applications, as predicted in section 4.B, do not encounter a small-scale limitation.

At the other cluster-size extreme, all these applications, show degraded R2 values as xmax gets large.  Again, such behaviors illustrate a large-scale over-provisioning.

Figures 7(e-g) describe the Reduce-intensive application Sort.  In Figure 7(e), where we select xmin=1, R2 values are consistently lower than in Figure 7(f).  In Figure 7(g) where xmin begins at 4, R2 values are all above 0.9.

Again, this observation confirms our section 4.B analysis of Reduce-intensive cases.

6. CONCLUSIONS AND FUTURE WORK MapReduce is an easily parallelizable framework,  within which, for certain applications and conditions, simply increasing compute resources yields scalable performance.  This paper describes an initial effort to characterize those applications and investigate how their execution times scale with cluster size.  A major conclusion is the observation of the segmented power-law distribution of the execution time in MapReduce applications.

A.  Choosing appropriate scaling bounds The benchmark MapReduce applications discussed  here all demonstrate power-law performance within the scaling bounds xmin and xmax.  Our results can help users identify the scaling range of power-law behavior.

As our Map-intensive data suggest, those applications do not suffer from small-scale limitations, and lower bound values for them start from a cluster of one VM.  For Shuffle-intensive applications, however, appropriate lower bounds depend on the application:  The more Shuffle- intensive, the larger the lower-bound value required.

Therefore, bound candidates should be profiled using a varied small cluster size, e.g. one, two, four VMs, etc. to determine the scaling start point where a large R2 can be observed.

For a certain volume of input data size, we can always allocate a large pool of compute resources, sufficient to far exceed actual processing needs.  The large-scale over- provisioning, therefore, potentially constrains all MapReduce applications. The xmax value depends on factors such as tolerance to degradation of R2, relative computing intensity between Map and Reduce phases, input dataset size, the number of Map and Reduce tasks and hardware configuration of each compute VM.  This complexity points to the need for a more general model that can address this many-to-one mapping problem.

B.Future work Future work includes studying more MapReduce  applications, with larger cluster sizes, on both public and private clouds and investigating how to extend our analytic framework to a wider range of real-world applications.

Meanwhile, other validation methods of the segmented power-law distribution other than evaluating the regression coefficient, e. g. likelihood ratio test, should also be further investigated.

Furthermore, we intend to delve more deeply into determining the lower-bound cluster size where power-law conformity begins in typical Shuffle-intensive applications.

Finally, we are developing a prediction method, based on power-law behavior that will show its effectiveness in very large clusters and with more complex MapReduce applications.  We will follow this work by proposing a series of data-cleaning and noise-filtering methods to minimize prediction errors.

ACKNOWLEDGEMENTS This publication was made possible by NPRP grant 09-  1116-1-172 from the Qatar National Research Fund (a member of Qatar Foundation).  Statements herein are solely the responsibility of the authors.

