Adaptive and Resource-Aware Mining of Frequent Sets

Abstract  The performance of an algorithm that mines frequent sets from transactional databases may severely depend on the specijic features of the data being analyzed. More- ow) ;  some architectural characteristics of the computa- tional platform used - e.g. the available main memory - can dramatically change its runtime behavior: In this paper we present DCI (Direct Count & Intersect), an efjicient al- gorithm for  discovering frequent sets from large databases.

Due to the multiple heuristics strategies adopted, DCI can adapt its behavior not only to the features of the specijic computing platform, but also to the features of the dataset being mined, so that it  results very effective in mining both short and long patternsfrom sparse and dense datasets. Fi- nally we also discuss the parallelization strategies adopted in the design of ParDCI, a distributed and multi-threaded implementation of DCI.

1 Introduction  Association Rule Mining (ARM), one of the most popu- lar topic in the KDD field, regards the extractions of asso- ciation rules from a database of transactions 'D. Each rule has the form X =) Y, where X and Y are sets of items (itemsets), such that ( X  f' Y) = 0. A rule X + Y holds in 2) with a minimum confidence c and a minimum support s, if at least the c% of all the transactions containing X also contains Y ,  and XU Y is present in at least the s% of all the transactions of the database. In this paper we are interested in the most computationally expensive phase of ARM, i.e the Frequent Set Counting (FSC) one. During this phase, the set of all thefiefrequent itemsets is built. An itemset of k items (k-itemset) is frequent if its support is greater than the fixed threshold s, i.e. the itemset occurs in at least minsup transactions (minsup = s / l O O .  n, where n is the number of transactions in 'D).

The computational complexity of the FSC problem de-  rives from the exponential size of its search space P ( M ) , i.e. the power set of M, where M is the set of items con- tained in the various transactions of D. A way to prune P ( M )  is to restrict the search to itemsets whose subsets are all frequent. The Apriori algorithm [51 exactly exploits this pruning technique, and visits breadth-first P ( M )  for counting itemset supports. At each iteration k, Apriori gen- erates ck, a set of candidate k-itemsets, and counts the occurrences of these candidates in the transactions. The candidates in ck for which the the minimum support con- straint holds are then inserted into Fkr i.e. the set of fre- quent k-itemsets, and the next iteration is started. Other algorithms adopt instead a depth-first visit of P ( M )  [8, 21.

In this case the goal is to discover long frequent itemsets first, thus saving the work needed for discovering frequent itemsets included in long ones. Unfortunately, while it is simple to derive all the frequent itemsets from the maximal ones, the same does not hold for their supports. which re- quire a further counting step. In the last years several vari- ations to the original Apriori algorithm, as well as many parallel implementations, have been proposed. We can rec- ognize two main methods for determining the supports of the various itemsets present in P ( M ) :  a counting-based approach [3, 5 ,  10, 13, 8, 1, 71, and an intersection-based one [15, 9, 161. The former one, also adopted in Apriori, exploits a horizontal dataset and counts how many times each candidate k-itemset occurs in every transaction. The tatter method, on the other hand, exploits a vertical dataset, where a tidlist, i.e. a list of transaction identifiers (lids), is associated with items (or itemsets), and itemset supports are determined through tidlist intersections. The support of a k-itemset c can thus he computed either by a k-way in- tersection, i.e. by intersecting the k tidlists associated with the k items included in c. or by a 2-way intersection. i.e.

by intersecting the tidlists associated with a pair of frequent (k - I)-itemsets whose union is equal to c. Recently an- other category of methods, i.e. the panern growth ones, have been proposed [ l ,  11, 141. FP-growth [ l l ]  is the hest representant of this kind of algorithms. It is not based on  0-7695-1754-4/02 $17.00 Q 2002 IEEE 338    candidate generation as Apriori, hut builds in memory a compact representation of the dataset, where repeated pat- terns are represented once along with the associated repeti- tion counters. FP-growth does not perform well on sparse datasets t171, so the same authors recently proposed a new pattern-growth algorithm, H-mine [141, based on an inno- vative hypeI-structure that allows the in-core dataset to be recursively projected by selecting those transactions that in- clude a given pattern prefix.

In this paper we discuss DCI (Direct Count &Intersect), a new algorithm to solve the FSC problem. We also in- troduce ParDCI, a parallel version of DCI, which explic- itly targets clusters of SMPs. Several considerations con- cerning the features of real datasets to he mined and the characteristics of modem hwlsw system have motivated the design of DCI. On the one hand, transactional databases may have different peculiarities in terms of the correlations among items, so that they may result either dense or sparse.

Hence, a desirable characteristic of a new algorithm should be the ability to adapt its behavior to these features. DCI, which supports this kind of adaptiveness, thus constitutes an innovation in the arena of previously proposed FSC al- gorithms, which often outperformed others only for spe- cific datasets. On the other hand, modern hwkw systems need high locality for effectively exploiting memory hierar- chies and achieving high performances. Large dynamic data structures with pointers may lack in locality due to unstruc- tured memory references. Other sources of performance limitations may he unpredictable branches. DCI tries to take advantages of modem systems by using simple array data structures, accessed hy tight loops which exhibit high spa- tial and temporal locality. In particular, DCI exploits such techniques for intersecting tidlists, which are actually repre- sented as hit-vectors that can he intersected very efficiently with primitive hitwise and instructions. Another issue re- gards VO operations. which must he carefully optimized in order to allow DM algorithms to efficiently manage large databases. Even if the disk-stored datasets to he mined may he very large, DM algorithms usually access them se- quentially with high spatial locality, so that suitable out-of- core techniques to access them can he adopted, also taking advantage of prefetching and caching features of modern 0% [6].  DCI adopts these out-of-core techniques to access large databases, prunes them as execution progresses, and starts using in-core strategies as soon as possible.

Once motivated the design requirements of DCI, we can now detail how it works. As Apriori, at each iteration DCI generates the set of candidates c k ,  determines their sup- ports. and produces the set F k  of the frequent k-itemsets.

However, DCI adopts a hybrid approach to determine the support of the candidates. During its first iterations, DCI exploits a novel counting-based technique, accompanied by an effective pruning of the dataset, stored to disk in horizon-  tal form. During the following iterations, DCI adopts a very efficient intersection-based technique. DCI stans using this technique as soon as the pruned dataset fits into the main memory.

DCI deals with dataset peculiarities by dynamically choosing between distinct heuristic strategies. For exam- ple, when a dataset is dense, identical sections appearing in several hit-vectors are aggregated and clustered, in order to reduce the number of intersections actually performed.

Conversely, when a dataset is sparse, the runs of zero hits in the hit-vectors to he intersected are promptly identified and skipped. We will show how the sequential implementa- tion of DCI significantly outperforms previously proposed algorithms. In particular, under a number of different tests and independently of the dataset peculiarities, DCI results to be faster than Apriori and FP-growth. By comparing our experimental results with the published ones obtained on the same sparse dataset. we deduced that DCI is also faster than H-mine [14]. DCI performs very well on both synthetic and real-world datasets characterized hy different density features, i.e. datasets from which, due to the differ- ent correlations among items, either short or long frequent patterns can he mined.

The rest of the paper is organized as follows. Section 2 describes the DCI algorithm and discusses the various adaptive heuristics adopted, while Section 3 sketches the solutions adopted to design ParDCI, the parallel version of DCI. In Section 4 we report our experimental results. Fi- nally in Section 5 we present some concluding remarks.

2 The DCI algorithm  During its initial counting-based phase, DCI exploits an out-of-core. horizontal database with variable length records. DCI, by exploiting effective pruning techniques inspired by DHP [13], trims the transaction database as ex- ecution progresses. In particular, a pruned dataset D k + l is written to disk at each iteration k, and employed at the next iteration. Let m k  and n k  be the number of items and transactions that are included in the pruned dataset 9, where m k  ? m k + l  and n k  2 n k + 1 .  Pruning the dataset may entail a reduction in U 0  activity as the algorithm pro- gresses, hut the main benefits come from the reduced com- putation required for subset counting at each iteration k, due to the reduced number and size of transactions. As soon as the pruned dataset becomes small enough to fit into the main memory, DCI adaptively changes its behavior, builds a verrical layout database in-core, and starts adopt- ing an intersection-based approach to determine frequent sets. Note, however, that DCI continues to have a level- wise behavior. At each iteration, DCI generates the can- didate set c k  by finding all the pairs of (k - 1)-itemsets that are included in F k - l  and share a common (k - 2)-     prefix. Since Fk-: is lexicographically ordered. the var- ious pairs occur in close positions, and candidate gener- ation is performed with high spatial and temporal local- ity. Only during the DCI counting-phase, c k  is further pruned by checking whether also all the other subsets of a candidate are included in Fk-:. Conversely, during the intersection-based phase, since our intersection method is able to quickly determine the support of a candidate item- set, we found much more profitable to avoid this further check. While during its counting-based phase DCI has to maintain Ck in main memory to search candidates and in- crement associated counters, this is no longer needed dur- ing the intersection-based phase. As soon as a candidate k- itemset is generated, DCI determines its support on-the-fly by intersecting the corresponding tidlists. This is an impor- tant improvement over other Apriori-like algorithms, which suffer from the possible huge memory requirements due to the explosion of the size of c k  [I]].

DCI makes use of a large body of out-of-core techniques, so that it is able to adapt its behavior also to machines with limited main memory. Datasets are readwritten in blocks, to take advantage of YO prefetching and system pipelin- ing [a]. The outputs of the algorithm, e.g. the various fre- quent sets Fb, are written to files that are map-ped  into memory during the next iteration for candidate generation.

2.1 Counting-based phase  Since the counting-based approach is used only for few iterations, in the following we only sketch its main features.

Further details about DCI counting technique can be found in [12], where we proposed an effective algorithm for min- ing shott patterns. In the first iteration, similarly to all FSC algorithms, DCI exploits a vector of counters, which are di- rectly addressed through item identifiers. Fork 2 2, instead of using complex data structures like hash-trees or prefix- trees, DCI uses a novel Direct Count technique that can be thought as a generalization of the technique used fork = 1.

Thetechniqueusesaprejirtable,PRELlXb[],ofsize (7).

In parlicular, each entry of PREF&[ ] is associated with a distinct orderedprefir of two items. For k = 2, P R E m k [  I can directly contain the counters associated with the var- ious candidate 2-itemsets, while, for k > 2, each entry of PREFIXk[  ] contains the pointer to the contiguous sec- tion of ordered candidates in c k  sharing the same prefix.

To permit the various entries of P R E m k [  ] to be directly accessed, we devised an order preserving, minimal perfect hash function. This prefix table is thus used to count the support of candidates in c k  as follows. For each transac- tion t = I t l , .  . . , t ~ ~ [ } .  we select all the possible 2-prefixes of all k-subsets included in t. We then exploit P w H x k [  I to find the sections of ck which must he visited in order to check set-inclusion of candidates in transaction t.

2.2 Intersection-based phase  Since the counting-based approach becomes less effi- cient as k increases [15], DCI starts its intersection-based phase as soon as possible. Unfortunately, the intersection- based method needs to maintain in memory the vertical representation of the pruned dataset. So, at iteration k, k 2 2, DCI checks whether the pruned dataset Dk may fit into the main memory. When the dataset becomes small enough, its vertical in-core representation is built on the fly.

while the transactions are read and counted against c k .  The intersection-based method thus starts at the next iteration.

The vertical layout of the dataset is based on fixed length records (tidlists). stored as bit-vectors. The whole verti- cal dataset can thus be seen as a bidimensional hit-array V?D[ ] [  1, whose rows correspond to the hit-vectors associ- ated with non pruned items. Therefore, the amount of mem- ory required to store VD[ ] [  ] is mk x nk bits.

At each iteration of its intersection-based phase, DCI computes Fk as follows. For each candidate k-itemset c, we and-intersect the k bit-vectors associated with the items included in c (k-way intersection), and count the 1?s present in the resulting bit-vector. If this number is greater or equal to minsup, we insert c into Fk. Consider that a hit-vector intersection can he carried out efficiently and with high spa- tial locality by using primitive bitwise and instructions with word operands. As previously stated, this method does not require c k  to be kept in memory: we can compute the sup- port of each candidate c on-the-fly, as soon as it is generated.

The strategy above is, in principle, highly inefficient, he- cause it always needs a k-way intersection to determine the support of each candidate c. Nevertheless, a caching pol- icy could be exploited in order to save work and speed up our k-way intersection method. To this end, DCI uses a small ?cache? buffer to store all the k - 2 intermediate in- tersections that have been computed for the last candidate evaluated. Since candidate itemsets are generated in lexico- graphic order, with high probability two consecutive candi- dates, e.g. c and d, share a common prefix. Suppose that c and c? share a prefix of length h 2 2. When we process c?, we can avoid performing the first h - 1 intersections since their result can be found in the cache.

To evaluate the effectiveness of our caching policy, we counted the actual number of intersections carried out by DCI on two different datasets: BMS, a real-world sparse dataset, and connect-4, a dense dataset (the characteristics of these two datasets are reported in Table 1). We com- pared this number with the best and the worst case. The best case corresponds to the adoption of a 2-way intersec- tion approach, which is only possible if we can fully cache the tidlists associated with all the frequent (k - 1)-itemsets in Fk-1. The worst case regards the adoption of a pure k- way intersection method, i.e. a method that does not exploit     caching at all. Figure l.(a) plots the results of this analy- sis on the sparse dataset for support threshold s = 0.06%, while Figure 1 .(h) regards the dense dataset mined with sup- port threshold s = 80%. In both the cases the caching pol- icy of DCI turns out to he very effective, since the actual number of intersections performed results to be very close to the hest case. Moreover, DCI requires orders of magni- tude less memory than a pure 2-way intersection approach, thus better exploiting memory hierarchies.

Caut-BUs. w w . m  (b)  Figure 1. Evaluation of DCI intersection caching policy.

We have to consider that while caching reduces the num- her of tidlist intersections, we also need to reduce intersec- tion cost. To this end, further heuristics. differentiated w.r.1.

sparse or dense datasets, are adopted by DCI. In order to ap- ply the .right optimization, the vertical dataset is tested for checking its density as soon as it is built. In particular, we compare the bit-vectors associated with the most frequent items, i.e., the vectors which likely need to be intersected several times since the associated items occur in many can- didates. If large sections of these hit-vectors turn out to be identical, we deduce that the items are highly correlated and that the dataset is dense. In this case we adopt a specific heuristics which exploits similarities between these vectors.

Otherwise the technique for sparse datasets is adopted. In the following we illustrate the two heuristics in more detail.

Sparse datasets. Sparse or moderately dense datasets originate hit-vectors containing long runs of 0's. To speedup computation, while we compute the intersection of the hit-vectors relative to the first two items c1 and c2 of a generic candidate itemset c = {cl, c 2 , .  . . , c k }  E C,, we also identify and maintain information about the runs of 0's appearing in the resulting hit-vector stored in cache. The  further intersections that are needed to determine the sup- port of c (as well as intersections needed to process other k- itemsets sharing the same 2-item prefix) will skip these runs of O's, so that only vector segments which may contain 1's are actually intersected. Since information about the runs of 0's are computed once, and the same information is reused many times, this optimization results to be very effective.

Moreover, sparse and moderately dense datasets offer the possibility of further pruning vertical datasets as compu- tation progresses. The benefits of pruning regard the re- duction in the length of the hit-vectors and thus in the cost of intersections. Note that a transaction, i.e. a column of VD, can be removed from the vertical dataset when it does not contain any of the itemsets included in 9. This check can simply he done hy or-ing the intersection hit-vectors computed for all the frequent k-itemsets. However, we ob- served that dataset pruning is expensive, since vectors must he compacted at the level of single hits. Hence DCI prunes the dataset only if turns out to he profitable, i.e. if we can obtain a large reduction in the vector length, and the num- her of vectors to he compacted is small with respect to the cardinality of ck.

(b)  Figure 2. Evaluation of DCI optimization heuristics for sparse and dense datasets.

Dense datasets. If the dataset turns out to he dense, we expect to deal with a dataset characterized by strong corre- lations among the most frequent items. This not only means that the hit-vectors associated with the mostfrequent items contain long runs of 1's. hut also that they turn out to he very similar. The heuristic technique adopted by DCI for dense dataset thus works as follows: A) we reorder the columns of the vertical dataset, in order to move identical segments of the bit-vectors associated with the most fre- quent items to the first consecutive positions, B) since each     Dslsyt  T25110010K  TZSIZOolWK  mk110.p8m10t  4Wk,30~p16mlt  120q8mlk  I connect4 I Publicly available dM~e dsmaset wiul 130 items aod about 60K trmsactionr. The maximal wansaction size i s  45. I  DeretipliO?  Syntheticdamserwith IK items and IOKvanractions[ll]. Theave~g~sizeofmnraclionsirZS,andtheaveragesi2eofulemaximal ~ t e n t i r l l y  frequent itemsls i s  IO.

Synthetic dataset with IOK ilem and IWK Uansaclims [Ill. The average size of lmsaclions i s  25. and ule average size of the maximal potentially frequent itemsets i s  20.

IOK iiemr and 4WK manractions. The werage size of iraninctions i s  IO, and ule average size of the maximal p~ienlidly hrqucnr itemsets is 8. Synthetic damel created with Ute IBM dalarer generator [SI.

IK i t e m  and 4WK BansaELions. The average size of transa~tionr i s  30. and ule average s i 2  of Le maximal pxen~ially frequent itemsets is 16. Synthetic dalaret created with the IBM dalasl generator 151.

With ais notation we identify a series of synuleue datael5 chanietenzed by I K  iems. The average msael ion $ice is 20. and he weram size of maximal ~otentially frequent itemreis is 8. The numberof msactioni i s  varied for scaling maruremnis.

hbl ic ly  avnilable sparse dafarei also known as Gmrllr. 497 i ~ m  and 59K ~~an~actioni  conmining click-swam data from an e-commprcewebsire gazeile.com  BMS  candidate is likely to include several of these most frequent items, we avoid repeatedly intersecting the identical seg- ments of the corresponding vectors. This technique may save a lot of work because ( 1 )  the intersection of identical vector segments is done once, (2) the identical segments are usually very large, and (3). long candidate itemsets presum- ably contains several of these most frequent items.

The plots reported in Figure 2 show the effectiveness of the heuristic optimizations discussed above in reducing the average number of hitwise and operations needed to in- tersect a pair of bit-vectors. In particular, Figure 2.(a) re- gards the sparse BMS dataset mined with support threshold s = 0.06%. while Figure 2.(b) regards the dense dataset connect-4 mined with support threshold s = 80%. In both cases, we plotted the per-iteration cost of each bit-vector in- tersection in terms of hitwise and operations when either our heuristic optimizations are adopted or not. The two plots show that our optimizations for both sparse and dense datasets have the effect of reducing the intersection cost up to an order of magnitude. Note that when no optimizations are employed, the curves exactly plot the bit-vector length (in words). Finally, from the plot reported in Figure 2.(a), we can also note the effect of the pruning technique used on sparse datasets. Pruning has the effect of reducing the length of the hit-vectors as execution progresses. On the other hand, when datasets are dense, the vertical dataset is not pruned, so that the length of bit-vectors remains the same for all the DCI iterations.

3 ParDCl  In the following we describe the different paralleliza- tion techniques exploited for the counting- and intersection- based phases of ParDCI, the parallel version of DCI. Since  our target architecture is a cluster of SMP nodes, in both phases we distinguish between inrra-node and inrer-node levels of parallelism. At the inter-node level we used the message-passing paradigm through the MF?I communica- tion library, while at the intra-node level we exploited multi- threading through the Posix Thread library. A Count Disrri- burion approach is adopted to parallelize the counting-based phase, while the intersection-based phase exploits a very ef- fective Condidore Distriburion approach [4].

The counting-based phase. At the inter-node level, the dataset is statically split in a number of partitions equal to the number of S M P  nodes available. The size of partitions depend on the relative powers of nodes. At each iteration k ,  an identical copy of c k  is independently generated by each node. Then each node p reads blocks of transactions from its own dataset partition v p , k .  performs subset count- ing, and writes pruned transactions to v p , k + l .  At the end of the iteration, an all-reduce operation is performed to update the counters associated to all candidates of C k ,  and all the nodes produce an identical set F k .

At the intra-node level each node uses a pool of threads, each holding a private set of counters associated with candi- dates. They have the task of checking in parallel candidate itemsets against chunks of transactions read from D p , k .

At the end of each iteration, a global reduction of coun- ters take place, and a copy of Fk is produced on each node.

The intersection-based phase. During the intersection- based phase, a Candidate Distribution approach is adopted at both the inter- and intra-node levels. This pardlelization schema makes the parallel nodes completely independent: inter-node communications are no longer needed for all the following iterations of ParDCI.

http://gazeile.com   ,- . -1  J 1 \ .. ...I  Figure 3. Total execution times for DCI, Apriori, and FP-growth on various datasets as a function of the support threshold.

Let us first consider the inter-node level, and suppose that the intersection-based phase is started at iteration %+ 1.

Therefore, at iteration the various nodes build on-the-fly the bit-vectors representing their own in-core portions of the vertical dataset. Before starting the intersection-based phase, the partial vertical datasets are broadcast to obtain a complete replication of the whole vertical dataset on each node. The frequent set Fx (i.e., the set computed in the last counting-based iteration) is then partitioned on the ba- sis of itemset prefixes. A disjoint partition Fp,x of Fx is thus assigned to each node p ,  where U, Fp,x = Fp It is worth remarking that this partitioning entails a Candidate Distribution schema for all the following iterations, accord- ing to which each node p will be able to generate a unique CE (k > k) independently of all the other nodes, where C,P "Ckp' = 0 ifp # p', andU,CE = Cb.

At the intra-node level, a similar Candidate Distribution approach is employed, but at a finer granularity by using dynamic scheduling to ensure load balancing.

4 Experimental Results  The DCI algorithm is currently available in two versions, a MS-Windows one, and a Linux one. ParDCI, which ex- ploits the MPICH MPI and thepthread libraries, is currently available only for the Linux platform. We used the MS- Windows version of DCI to compare its performance with other FSC algorithms. For test comparisons we used the  FP-growth algorithm1, and the Christian Borgelt's imple- mentation of Apriori2. For the sequential tests we used a Windows-"I workstation equipped with a Pentium II 350 MHz processor, 256 MB of RAM memory and a SCSI- 2 disk. For testing ParDCl performance. we employed a small cluster of three Pentium II 233MHz 2-way SMPs, for a total of six processors. Each SMP is equipped with 256 MBytes of main memory and a SCSI disk. For the tests, we used both synthetic and real datasets by varying the mini- mum support threshold s. The characteristics of the datasets used are reported in Table 1.

DCI performances and comparisons. Figure 3 reports the total execution times obtained running Apriori, FP- growth, and our sequential DCI algorithm on some datasets described in Table 1 as a function of the support'Ihreshold s. In all the tests conducted, DCI outperforms FP-growth with speedups up to 8. Of course, DCI also remarkably out- performs Apriori, in some cases for more than one order of magnitude. For connect-4, the dense dataset, the curve of Apriori is not shown, due to the relatively too long execu- tion times. Note that, accordingly to [17], on the real-world sparse dataset BMS (also known as Gazelle). Apriori turned out to be faster than FP-growth. To overcome such had per- formance results on sparse datasets, the same authors of FP- growth recently proposed a new pattern-growth algorithm, H-mine [14]. By comparing our experimental results with  'We adrnowledge Pmf. Jiawei Han for kindly providing us the latesf  *http://f"zzy.CS.uni-magdeburg.de/-borgelt fully optimized. binary version of FP-growth.

http://f"zzy.CS.uni-magdeburg.de/-borgelt   the published execution times on the BMS dataset, we de- duced that DCI is also faster than H-mine. For s = 0.06%, we obtained an execution time of about 7 sec., while H- mine completes in about 40 sec. on a faster machine.

The encouraging results obtained with DCI are due lo both the efficiency of the counting method exploited during early iterations, and the effectiveness of the intersection- based approach used when the pruned vertical dataset fits into the main memory. For only a dataset, namely T25IIODlOK, FP-growth turns out to be slightly faster than DCI for s = 0.1%. The cause of this behavior is the size of C,. which in this specific case results much larger than the final size of F3. Hence, DCI has to carry out a lot of useless work to determine the suppon of many candidate itemsets, which will eventually result to be not frequent. In this case FP-growth is faster than DCI since it does not require can-  11102 a. 0 1  ? e  I umbr*il.-dr,"mml  (a) bpd. Mnepl>l. -.an  '-2 ~  i f .! I 8 1  I' L .- .2-~-4 LP~-  1 2 i :  P-~,~-~  )m LL--,/&L' -P-..I_-_- --8-  - Y B - t  ! ,  ~ ~ ~ - !  - 1 3 * e  ~ -72-  am2 m.1 OI 3.8 2 -4T- I-,  0)  Figure 4. Total execution times of (a) DCI, and (b) FP-growth, on datasets in the series t20.p8mlk (s = 0.5%) on a PC equipped with different RAM sizes as a function of the num- ber of transactions (ranging from 1OOK to 2M).

We also tested the scale-up behavior of DCI when both the size of the dataset and the size of RAM installed in the PC vary. The datasets employed for these tests belong to the series t20$3mlk  (see Table I )  mined with support thresh- old s = 0.5%, while the available RAM was changed from 64MB to 512MB by physically plugging additional mem- ory into the PC main hoard. Figure 4.(a) and 4.(h) plot several curves representing the execution times of DCI and FP-growth, respectively, as a function of the number of transactions contained in the dataset processed. Each curve plotted refers to a series of tests conducted with the same  F'C equipped with a different amount of memory. As it can be seen from Figure 4.(a), DCI scales linearly also on ma- chines with a few memory. Due to its adaptiveness and the use of efficient out-of-core techniques, it is able to modify its behavior in function of the features of the dataset mined and the computational resources available. For example, in the tests conducted with the largest dataset containing two millions of transactions, the in-core intersection-based phase was started at the sixth iteration when only 64MB of RAM were available, and at the third iteration when the available memory was 512MB. On'the other hand the re- sults reponed in Figure 4.(h) show that FP-growth requires much more memory than DCI, and is not able to adapt itself to memory availability. For example, in the tests conducted with 64MB of RAM, FP-growth requires less than 30 sec- onds to mine the dataset with 2OOk transactions. but when we double the size of the dataset to 400k transactions, FP- growth execution time becomes 1303 seconds, more than 40 times higher, due to an heavy page swapping activity.

-""e..

-"in1 (a)  -I.%  I lam ++ :  a .  . ..

1.

' .~P- ~~ ~~~ . _ _ _ _ ~ ~ ~ -  * . ...L-- ! , $ 8 :  1 1 1 1 1 1  N. F-...a.

(b) Figure 5. (a): Dense dataset connect-4: completion times of DCI and ParDCl vary- ing the minimum support threshold. (b): Speedup for sparse datasets 1000K, 2000K and 3000K with s = 1.5%  Performance evaluation of ParDCI. We evaluated ParDCl on both dense and sparse datasets. First we com- pared the performance of DCI and ParDCl on the dense dataset connect-4, for which we obtained very good speedups. Figure 5.(a) plots total execution times as func- tions of the support thresholds s. ParDCI-2 corresponds to the pure multithread version running on a single 2-way SMP, while ParDCI-4 and ParDCI-6 also exploit inter- node parallelism, and run, respectively, on two and three 2-way SMPs. For what regard sparse datasets, we used     the synthetic dataset series identified as t50-pp32mlk in Table 1. We varied the total number of transactions from looOk to 3000k. In the following we will identify the vari- ous synthetic datasets on the basis of their number of trans- actions, i.e. IOOOk, 2000k. and 3000k. Figure 54b) plots the speedups obtained on the three synthetic datasets for a given support threshold (s = 1.5%), as a function of the number of processors used. Consider that, since our cluster is com- posed of three 2-way SMPs,  we mapped tasks on processors always using the minimum number of SPMP nodes (e.g., when we used 4 processors, we actually employed 2 S M P nodes). This implies that experiments performed on either 1 or 2 processors actually have identical memory and disk resources available, whereas the execution on 4 processors benefit from a double amount of such resources. Accord- ing to the tests above, ParDCl showed a speedup that, in some cases, is close to the optimal one. Considering the re- sults obtained with one or two processors, one can note that the slope of the speedup curve is relatively worse than its theoretical limit, due to resource sharing and thread imple- mentation overheads at the inter-node level. Nevertheless, when additional SMPs are employed, the slope of the curve improves. The strategies adopted for partitioning dataset and candidates on ow homogeneous cluster of SMPs suf- ficed for balancing the workload. In our tests we observed a very limited imbalance. The differences in the execution times of the first and last node to end execution were always below the 0.5%.

5 Conclusions  DCI uses different approaches for extracting frequent patterns: counting-based during the first iterations and intersection-based for the following ones. Adaptiveness and resource awareness are the main innovative features of the algorithm. On the basis of the characteristics of the dataset mined, DCI chooses at run-time which optimiza- tion to adopt for reducing the cost of mining. Dataset prun- ing and effective out-of-core techniques are exploited dur- ing the counting-based phase, while the intersection-based phase works in core, and is staned only when the pruned dataset can fit into the main memory. As a result, our algo- rithm can manage efficiently, also on machines with limited physical memory, very large datasets from which, due to the different correlations among items, either short or long frequent patterns can be mined.

The experimental evaluations demonstrated that DCI significantly outperforms Apriori and FP-growth on both synthetic and real-world datasets. In many cases the perfor- mance improvements are impressive. Moreover, ParDCI, the parallel version of DCI, exhibits excellent scaleups and speedups on our homogeneous cluster of SMPs. The va- riety of datasets used and the large amount of tests con-  ducted permit us to state that the performances of DCI are not influenced by dataset characteristics, and that our optimizations are very effective and general. To share our efforts with the data mining community, we made the DCI binary code available for research purposes at http:IIwww.miles.cnuce.cnr.it/-.palmerildatam/oCI.

