Kongu Engineering College, Perundurai, Erode, T.N.,India.27 ? 29 December,2010.pp.519-523.

Abstract? Most of the research activities in association rule mining focuses on defining efficient algorithms for item set extraction. To reduce the computational complexity of item set extraction, support constraint is enforced on the extracted item sets. Recent existing work, IMine index (Item set-Mine index), a data structure, provides a compact representation of transactional data supporting efficient   item set extraction from a relational DBMS. However, when the transactional database is updated, IMine index needs to be rematerialized. The proposed work presents an incremental update strategy to work on the dynamic transaction of DMBS for efficient item set extraction. Since no support threshold is enforced during the index creation phase, the incremental update is feasible without accessing the original transactional database. The index performance in terms of incremental updates is experimentally evaluated with data sets characterized by different size and data distribution. The execution time of frequent item set extraction based on incremental update strategy of IMine is better than the state-of- the-art algorithm i.e., existing IMine algorithm without update strategy. The experimental result shows the scalability of incremental update strategy for more frequent database updates characterized by a large number of transactions and with different pattern lengths.

Keywords?Item set Mining, IMine and Incremental Update

I. INTRODUCTION This transactional data set D is represented, in the relational  model, as a relation R. Each tuple in R is a pair (Transaction ID, Item ID). The IMine index provides a compact and complete representation of R. Hence, it allows the efficient extraction of item sets from R, possibly enforcing support or other constraints.  The IMine index structure is independent of the adopted item set extraction algorithm. Hence, different state-of-the art algorithms may be employed, once data has been loaded in memory. The in-memory representation suitable for the selected extraction algorithm is employed (e.g., FP-tree for FP-growth, array-based structure for LCM).

The proposed approach of incremental update strategy for item set extraction in frequently changing data items two  incremental algorithms. The first one exploits the implicit assumption made in almost all previous works in constraint- based mining properties on which users define constraints are functionally dependent on the item to be extracted, i.e., the property is either always true or always false for all occurrences of a certain item in the database called them item dependent (ID) constraints. The exploitation of these constraints proves to be extremely useful for incremental algorithms. ID constraints allow the selection of the valid itemsets in advance, based on their characteristics, hold separately from the transactions in which itemsets are found.

The second class of constraints, context dependent (CD) constraints, occurs in application domains, such as in business (e.g., analysis of stock market data) in science (e.g., meteorological forecast) and in the traditional data mining applications of market basket analysis. In the case of stock price, feature on which constraints are evaluated, does not depend only on the stock, but it also depends on another variable (time). Time and stock together provide the context in which price is determined. Therefore, in contrast to ID constraints, the satisfaction of CD constraints cannot be decided without reading the contextual information present in the database transaction. CD constraints proved to be very difficult to be dealt with. CD constraint is not necessarily satisfied by a certain item set in all its instances in the database.



II. LITERATURE REVIEW Depending on the enforced support and/or item constraints  and on the selected algorithm for item set extraction, a different portion of the IMine index should be accessed. The existing work devised three data access methods to load from the IMine index i.e., frequent-item based projection (FP-growth [2]), support-based projection, (APRIORI [1]), array-based (LCM v.2 [6]) algorithms and item-based projection, to load all transactions where a specific item occurs, enabling constraint enforcement during the extraction process. The IMine index allows selectively loading into memory only the index blocks used for the local search. Hence, it supports a reduction of disk reads. Since only a small fragment of the data is actually loaded    Incremental Update Strategy for Indexed Item Set Mining     in memory, more memory space is available for the extraction task.

Several algorithms have been proposed for item set extraction.([8], [1]) These algorithms are different mainly in the adopted main memory data structures and in the strategy to visit the search space. The IMine index can support all these different extraction strategies. Since the IMine index is a disk resident data structure, the process is structured in two sequential steps i.e., the needed index data is loaded and item set extraction takes ace on loaded data. The data access methods allow effectively loading the data needed for the current extraction phase. Once data are in memory, the appropriate algorithm for item set extraction can be applied.

Frequent item set extraction by means of two representative state-of-the-art approaches, i.e., FP-growth [2] and LCM v.2 [6], is described. IMine index supports constraint enforcement was made.

IMine index is a disk-based data structure which supports the tight integration of item set extraction in a relational DBMS.

The index structure is able to cope with both sparse and dense data distributions. The idea of tightly integrating item set mining in a relational DBMS was first introduced in [9]. Since the creation phase in [9] required to build in main memory the entire index before writing it on disk, its capability of mining and manipulating large data sets was rather limited. A complementary approach is proposed in [10], where frequent closed item sets are stored in a disk-based structure named CFP-tree and algorithms have been proposed to retrieve patterns from there. However, for low supports, the solution set becomes too huge to be represented by means of the CFP-tree.

The IMine index, instead, completely represents the entire data set, thus supporting item constrained extraction even without any support threshold.

Various approaches have been proposed to enforce constraints into the item set extraction process [3], [4], [5].

Constraints have been classified as antimonotonic, monotonic, succinct, convertible, and inconvertible [7]. The first four constraint types (and a combination of them) can be pushed deep into the frequent pattern growth algorithm. The approach proposed in [7] can be straightforwardly supported by the IMine index by means of the available access methods. Since the approach proposed in [7] is based on FP-growth, it can suffer the same shortcomings in case of large databases or low supports (mainly, very high memory consumption). The IMine- based approach, instead, allows selectively loading the data set portion needed by the current extraction process, thus overcoming these limitations.

The proposal in this work presents an improvement to the IMine index with an incremental approach to extract item sets from dynamic frequently updating databases. Here incremental is used to emphasize that the mining engine does not start from scratch. Instead, it exploits the result set of previously executed queries in order to simplify the mining process in the pruning of the item set lattice. They are able to exploit the mining constraints of the current query in order to prune the search space even more.



III. INCREMENTAL UPDATE STRATEGY FOR ITEM PRUNING The proposed work of incremental update strategy for item pruning follows the perspective of item dependent and context dependent constraints.

A. Item Dependent Incremental Algorithm The item dependent constraints are particularly desirable  from the viewpoint of the optimization of languages for data mining. In particular, it is showed to obtain the result of a newly posed query Q by means of set operations (unions and intersections) on the results of previously executed queries.

This approach to item set mining is incremental because instead of computing the item sets from scratch it starts from a set of previous results to the current updated version.

Analyze the situation of query containment, that is, to consider situations in which query Q imposes a more restrictive set of constraints with respect to a previous query, here denoted with Qi. Item dependent constraint is exploited to simplify the problem of incremental mining. In order to retrieve the desired rules, it suffices to identify the rules in the previous results that satisfy the new constraints. As the results imply, this is not generally true in a situation involving context-dependent constraints. In fact, in the latter case, one needs to carefully update the statistical measures of the rules as well.

Under the item dependency assumption, whenever a query Qi is found to contain Q, it is rather easy to extract the new results from past ones. It suffices to search in Ri those rules which satisfy the requirements of Q and to copy them verbatim (along with their support counts) into the new result set. The algorithm proposed for item dependency checks which of the rules in Ri satisfy the constraints in Q and updates R accordingly. It is important to notice that testing TB and TH is a feasible and efficient operation. In fact, since the constraints are item dependent, their evaluation does not require to access the whole (possibly huge) facts table. On the contrary, it merely requires to access the dimension tables and to check the constraints using the information found therein. Since those tables does usually fit into the main memory or in the DBMS buffer memory, this rarely becomes a demanding operation. In addition, the E constraint is also easily checked by using the statistical measures stored together with the rules in the past result.

Fig. 1  Incremental algorithm for item dependency  B. Context  Dependent Incremental Algorithm        In the context dependent incremental algorithm, construct the result of a new mining query Q starting from a previous result Ri even when the mining constraints are not item dependent.

The algorithm is best described by considering two separate steps. In the first one, the algorithm reads rules from Ri and builds a data structure which keeps track of them. This structure is called as BHF (Body-Head Forest). Since the BHF is built starting from a previous result set and represent only rules found therein, this corresponds to a first pruning of the search space. Subsequent operations will simply disregard rules that do not appear in it. In the second step, the algorithm considers two relations  Tb = { < i,g > | i E IB, g E G, TB is true} and Th = { < i,g > | i E IH, g E G, TH is true},  Containing the items and the group identifiers (GIDs) that satisfy the mining constraints in query Q. Tb and Th are obtained by evaluating the constraints on the fact table. Their role is to keep track of the context in which the item sets appear. In fact, the context dependent constraints require that their validity is checked group by group. The two relations fulfill this purpose. The search space is pruned. In fact, the constraints are evaluated on the database and the items which do not satisfy the mining constraints are removed, once and for all, from the input relations. Finally, the algorithm updates the counters in the BHF data structure accordingly to the item sets found in Tb and Th. The counters are then used to evaluate the statistical measures needed to evaluate whether the constraint E is satisfied.

Fig. 2  Incremental algorithm for context dependency

IV. EXPERIMENTAL EVALUATION The experimentation is conducted for the data set of  Customer Car Purchase obtained from UCI Repository using Java SDK libraries to prove the efficiency of the incremental update strategy for item set extraction in the mining context.

On experimenting the Car Purchase data set, the devised java program has been run using parameters with  T = 25, I = 10, N = 13, D = 2600, i.e., the average transaction size is 25, the  average size of potentially large item sets is 10, the number of distinct items taken for consideration is 13 and the total number of transactions is 2600. Then, this initial table is updated by adding some attributes which provide the details (and the contextual information) of each purchase.

We added some item dependent features (such as category of product and price) and some context dependent features (such as discount and quantity). The values of the additional attributes have been generated randomly using uniform distributions on the respective domains. A single fact table suffices for the objectives of our experimentation.

While, in fact, the characteristics of the database instance (e.g., total database volume and data distribution) are determinant in order to study the behavior of mining algorithms, this is not so when we are up to study incremental algorithms. Indeed, as simple complexity considerations point out, the important parameters from the viewpoint of the performance study of incremental algorithms are the selectivity of the mining constraints (which determine the volume of data to be processed from the given database instance) and the size of the previous result set.

Fig. 3  Item dependent incremental algorithm (constraint selectivity Vs  execution)  Fig. 3 reports the performances of the item dependent incremental algorithm (ID) as the selectivity of the mining constraints changes. The experimentation is carried out for different constraints on the item dependent attributes, letting the constraints selectivity vary from 0% to 100% of the total number of items.

The Fig. 4 presented tests the same algorithm, but it lets vary the number of rules in the previous result set. Again we sampled twenty points (in the range 0 ? 220). The two figures report the total amount of time needed by the algorithm to complete.

In particular, the bars, which represent the single experiments, are divided in two components: the preprocessing time (spent in querying the database to retrieve    Incremental Update Strategy for Indexed Item Set Mining     and store in main memory the items that satisfy the constraints), and the core mining time (needed by the algorithm to read the previous result set and to filter out those rules that do not satisfy the constraints any more).

Fig. 5 and 6 report the performances of the context dependent (CD) algorithm. The graphs report again the total execution time, specifying how much time was spent for preprocessing and for the core mining task.

Fig. 4  Volume of previous result Vs execution time for Item dependent  incremental algorithm  It is worth noticing, that the CD incremental algorithm performs a greater amount of work with respect to the ID algorithm because the problem it solves is far more complex.

In fact, in the preprocessing phase the algorithm must retrieve all the group/item pairs satisfying the constraints and access to them in order to build and update the BHF data structure.

Only then, it can retrieve the results from the BHF structure.

Fig. 5  Context dependent constraint selectivity Vs execution  The execution times of both algorithms increase almost linearly with the increase of the two parameters (constraint selectivity and previous results), but, as it was expected, the  item dependent incremental algorithm runs much faster than its counterpart.

Fig. 6  Volume of the previous result Vs execution time  Both the algorithms are faster than the IMine algorithm which, is incapable of solving a class of more difficult problems of mining item set extraction from context- dependent constraints. IMine, as the most of the algorithms, operates starting from scratch. Hence, comparisons of incremental update strategy with other mining algorithms on the field of context dependent constraints show better performance quantifying to 7% improvement.

Must be in two column format with a space of 4.22mm (0.17) between columns.



V. CONCLUSIONS The proposed incremental update approach to constraint-  based mining makes use of the information contained in previous results to answer new queries in frequently updating databases. The beneficial factors of the approach are that it uses both the previous results and the mining constraints, in order to reduce the item sets search space. It comprises of item dependent and context dependent constraints for extracting item sets.

Proposed approach makes a significant usage of available results of previous queries (if the incremental approach results effective with respect to a conventional execution). The incremental option for a data mining algorithm is of course preferable in an inductive database system, since it allows the exploitation of all the available information in the system in order to speed up the response time.

The better performance of incremental algorithm depicted in the result section  worked on problems with item and context dependent constraint present a solution for item extraction from frequently updated database. This is done by running first the         mining algorithm of our choice (on the problem defined by the query but without the context dependent constraints) and then applying the incremental algorithm on top of it (with the addition of context dependent constraints). In particular, whenever the mining constraints select a very small part of the original dataset, proposed incremental update strategy is likely to be very fast and efficient.

