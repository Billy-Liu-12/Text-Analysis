Attribute (Feature) Completion  - The Theory of Attributes from Data?Mining Prospect

Abstract  A ?correct? selection of attributes (features) is vital in data mining. As afirst step, this paper constructs all pos- sible attributes of a given relation. The results are based on the observations that each relation is isomorphic to a unique abstract relation, called canonical model. The com- plete set of attributes of the canonical model is, then. con- structed. Any attribute of a relation can be interpreted (via isomorphism) from such a complete set.

Keywords: attributes, feature, data mining, granular, data model  1. Introduction  Traditional data mining algorithms search for patterns only in the given set of attributes. Unfortunately, in  a typical database environment, the attributes are selected primarily for record keepings, not for the understanding of real world.

Hence, it is highly possible that there are no visible patterns in the given set of attributes; see Section 2. The fundamen- tal question is: Is there a suitable set of attributes so that  The ?invisible? patterns can he mined?

Fortunately, the answer from this paper is yes. For this pur- pose, we examine the fundamental issues, such as what is the raw data, target patterns, and  Build a mathematical model that captures exactly what data says  Based on such a model, we are able to develop a theory of attributes, and  Construct the complete set of all attributes of a given relation  This is the main result of this paper. The paper is roughly organized into 3 parts. First is the motivational example (Section 2). followed by two sections of fundamental for- mulations(Section 3, refgdm), and conclude with the theory of attributes based on some foundational investigation of data mining  2 Motivation - Invisible Patterns Let us consider a 3-column numerical relation, Table 1.

The first column is the ?independent variable:? namely the universe of the entities (directed segments). It has three at- tributes, which consists of the beginning points, and?polar coordinates,? the Length and the Degree. This table has  One association rule of length 2, that is, (Xs, 2.0).

By switching to ?Cartesian coordinate system,? the table is transformed to Table 2; Interestingly,  The only association rule disappears  A moment of reflection, one realize that since the associ- ation rule is a real world phenomenon (a geometric fact), the same information should he still carried in Table 2. The question  How can this ?invisible? association rule he mined?

It is obvious that we need the derived attribute Length, which is a function of Horizontal and Vertieal. This phenomenon prompts us to consider the foundation of data mining, in particular  The foundation of attributes (features) from data mining prospect.

We would like to note that attribute (feature) theory from the prospect of database processing is very different this one.

282 0-7695-1754-4/02 $17.00 Q 2002 IEEE  mailto:tylin@cs.sjsu.edu   Table 1. Ten directed segments in polar coor- dinates  Table 2. Ten directed segments in (X,Y)- coordinates  3 Basic Structures - the Data and Patterns 3.1 Raw Data - the Relations  The central objects of the study should he bag relations (we allow repeated tuples). However, without losing the essential idea, for simplicity, we focus on (set theoretical) relations, or more emphatically the relation instances. We will also assume  All attributes are distinct (non-isomorphic); see Sec- tion 4.

Let V be the universe. Let A = {A', A*, . . . , A"} he a set of attributes, and their attribute domains he C = {Cl, Cz, . . . , C"}. Each Cj,  often denoted by Dmi(Aj) , is a set of elementary concepts (attribute values). Techni- cally, they are the so-called semantics primitives in AI [61 or undefined primitive in mathematics. In other words, the  semantics of these symbols are not part of the formal sys- tem.

The main raw data is a relation, which is a set (not a bag) of tuples. We will view a relation as a knowledge represen- tation V -+ Dum(A), where D m ( A )  is the Cartesian product of the Dm(Aj ) ' s .  It is clear, we can view each Ai' as a map V -+ Dom(Aj) (single column represen- tation). Then, the relation, K, is a join of attribute maps; see [15, 171. If one uses the information table (see below), the join is actual join of the relational algebra.

A map or function naturally induces a partition on its domain (the collection of all inverse images of the map), so each A' induces a partition on V (and hence an eqniva- lence relation); we use Q j  to denote both. We let Q he the collection of Qj's .

In traditional database theory, the image of the map K (knowledge representation) is called the relation. The "in- dependent variable V" plays no explicit role. However, in data mining, it is more convenient to have independent vari- ahles in the formulation. So in this paper, we may also use the graph (U, K ( u ) ) ,  called the information table. Through- out the whole paper K may mean the map, the image, or the graph, by abuse of notation. Since K is determined by A on V, we may use (V, A) for K .

3.2 Target - High Frequency Patterns In association rule mining, two measures, called the sup-  pon and confidence, are imponant. In this paper, we will he concerned the high frequency patterns, not necessarily in the form of rules. So only with the suppon will he con- sidered.

Association rule mining is originated from on the mar- ket basket data [I]. However, in many software systems.

the data mining tools are added to general DBMS. So we will he interested in data mining on general relations. For definitive, we have the following translations: an item is an attribute value, a q-itemset is a suhtuple of length q, a large q-itemset is a high frequency q-pattern. In other words,  A suhtuple of length q is a high frequency q-patterns.

or simply pattern, if its occurrences are greater than or equal to a given threshold.

What are we mining? - Isomorphic class This paper focuses on database mining, more specifi-  cally, extracting high frequency patterns from a given re- lation (freeze at one database), In this section, we offer somewhat a surprised observation that the target patterns, such as  association rules are the common patterns of whole isomorphic class, NOT an individual relation alone.

4.1 Isomorphic Relations and Patterns  Attributes A' and A' are isomorphic iff there is a one- to-one and onto map, s : Dom(A') --+ Dom(AJ) such that Aj(u) = s(A'(u)) V u  E V .  The map s is called an isomorphism. Intuitively, two attributes (columns) are iso- morphic iff one column turns into another one by properly renaming its attribute values.

Let K = (V,A) and H = (V ,B)  be two infor- mation tables, where A = {A ' ,AZ, . .  .A"} and B = { B 1 , B Z , .  . . B'"}. Then, K and H are said to be isomor- phic if every A' is isomorphic to some Bj,  and vice versa.

By our assumption (all attributes are distinct), K and H have the same degree (number of attributes), that is, n = m; See more general version in Section 11. The following the- orem should be obvious.

Theorem 4.1. Isomorphic relations have isomorphic pat- terns.

The impacts of this simple theorem are rather far reaching.

It essentially declares that patterns are syntactic in nature.

They are patterns of the whole isomorphic class, yet many of isomorphic relations may have very different semantics; see Section ??.

The "interesting-ness" (of association rules) may not be captured by the mere counting of the items (and hence the probability theory based on it).

Of course, something like unexpected-ness (which is proba- bilistic in nature) can be captured; the research on this topic will be reponed in future.

5. Modeling what data says-Canonical Models  In classical data model, the (intension) functional depen- dency can never be expressed by the raw data, however, the data does express the extension functional dependency. So it is important to examine very fundamental question,  What is the raw data (a given re1ation)really saying?

In this section, we construct the canonical models for each isomorphic class. In other words, the canonical model express exuctly What raw data says about patterns.

Earlier, we have called them machine oriented mod- els [15, 161, and have shown that it is very fast in computing the high frequency patterns [241.

5.1 Attributes and Equivalence Relations  We have observed that (Section 3.1) each Aj induces an equivalence relation Qj on V .  The set VfQj,  which consists of all granules (equivalence classes), is called the  quotient set. The map Pj : V --f V/Q' : U -+ [U] is the natural projection, where [VI is the granule containing U.

Next, we state an observation ([17]. pp. 25):  Pmposirion 5.1. An attribute A?, as a map, can be factored as A' = PjoNAMEJ,  wherethenamingmap, NAME' : V/Qj -+ C' = Dmn(A') : [ U ]  + NAMEj([u]). is referred to as the interpretation.

1. The interpretation induces an isomorphism from V/Qj to Cj; The interpretation assigns each granule an ele- mentary concept (attribute value); we can regard it as a meaningful name of the granule. A' is a meaningful name of Qj;  2. The natural projection Pj is a map from V to V f Q j .

Formally, it can be regarded as an attribute. It is a sin- gle column representation of V into the quotient set.

3. The natural projection and the induced partition deter- mine each other, we may use Qj to denote the par- tition, the equivalence relation, including the natural projection.

5.2 Canonical Model and Granular Data Model  A relation K, as a map, can be factored through the nat- ural projection CK : V -+ V/Q' x ... x VfQ" and the naming map NAME : V/Q' x . . . x V/Q" -+ C' x . . . x C". Note NMAE is the product of NAMEj and is often referred to as the interpretation. Table 3 illus- trates how K is factored.

The natural projection CK can be regarded as a knowl- edge representation of the universe V into quotient sets. It is called the canonical model of K.

The interpretation induces an isomorphism from Vf Q to C (both are appropriate Cartesian products). The interpretation assigns a tuple of granules to a tuple of elementary concepts (attribute values). Each A' can be regard as a meaningful name of Q j ,  and an attribute value is a meaningful name of a granule(equiva1ence class).

Q j  is an attribute of CK. called a canonical attribute (an uninterpreted attribute). Dmn(Qj) = V/QJ is called a canonical domain; a granule is a canonical at- tribute value [15].

Theorem 5.2.1. Patterns of the canonical model CK is iso- morphic (via interpretation) to the patterns of K.

This is a corollary of Theorem 4.1. To find all patterns of K , we only need to find the patterns on CK (and vice versa).

Canonical Model CK I Relation K  Table 3. The canonical model Cx at left-hand-side is mapped to K at right-hand-side  The canonical model CK is uniquely determined by its uni- verse V ,  and the family Q of equivalence relations. In other words, the pair (V, Q )  determines and is determined by Cx.

From the pospect of first order logic, (V, Q) is a model of some rather simple kind of first order logic, where the only predicates are equivalence predicates (predicates that satisfy the reflexive, symmetric and transitive properlies) [221.

I .  One can regard the canonical model Cx as a table for-  2. We will call a granule of those original Q j  an elemen-  3. A q-tuple of C x  corresponds to an intersection, called  4. High frequencypatterns of (V, Q )  are q-granule whose  matof (V ,Q) .

tary granule.

a q-granule, of q elementary granules in (V, Q).

cardinality is greater than the given threshold.

5. We have assume all attribute are distinct, to see more general version, we refer to [SI.

Definition 5.2.2. The pair (V,Q) is called granular data model; it is a special case of granular structure [IS].

(V, Q )  is considered by both Pawlak and Lee. In his book, Pawlak call it knowledge base; implicitly Pawlak assumed all attributes are non-isomorphic [28], as we have done here.

Since Knowledge base often has different meaning, we will not use it. Tony T. Lee considered the general case see Sec- tion 1 1.

Comllary 5.2.3. The patterns of (V,Q),  CK,  and K are isomorphic.

6 Theory of derived Attributes (Features)  An attribute is also called a feature, especially in AI; they have been used interchangeably. In the classical data model,  an attribute is a representation of property, characteristic, and etc.; see e.g., [26, 271. In other words, it represents a human perception about the data (intensional view [5]) .

However. we should note that in a given relation instance (extensional view [5]) ,  the data itself cannof fully reflect such a human perception. As we have pointed out, the ex- istence of an (extension) function dependency in a given ta- ble cannot imposes an (intension) function dependency on the data model. So in data mining, we should note that at- tributes are defined by the given instance of data (extension view), not what human perceived. Many very distinct at- tributes in intensional view (as human perceives) are actu- ally isomorphic from the extensional view (as data says); see examples in [9,8].

6.1 Attribute Transformations and Function De- pendency  We will examine how a new attribute, that is transformed from the given ones, is related to the given them. Let B be a subset of A and let g he a function defined on D m ( B )  = D m ( B ' )  x . . . x Dm(B')) .  We collect all function values in a set D. Using mapping notation. we have g : D m ( B )  -+ D; it is called an attribute Uansfor- mation. Since attributes can he regarded as maps, we have:  g o  B : V -+ D m ( B )  + D The map g D B : V -+ D is a new attribute. We write Y = goB = g(B',B';. .. ,B') andD = Dmn(Y). Y is called a derived attribute, and g an attribute transformation.

By joining K and Y, we have anew relation K': (joining in the sense of relational algebra on the information tables)  K' = K A Y  = A 1  A . .  . A A" A Y : V + D m ( A ' )  x . . . x D m ( A " )  x Doni(Y)  Next we see how a new derived attribute Y is related the given attributes in the new relation K'.

Proposition 6.1. Y is a derived attribute of B iff Y is exten- sion functionally depended (EFD) on B.

By definition, the occurrence of an (extension) function- ally dependency (EFD) means there is an attribute trans- formation f : Dom(B') x . . .Dom(Bk) -+ Dom(Y) such that f ( B ( v ) )  = Y ( v ) ,  V v E V .  By definition, Y = f ( B ' ,  B', . . . , B k ) ;  this completes our arguments.

Table 4 illustrates the notion of EFD and attribute transfor- mations.

Table 4. An Attribute Transformation in K  6.2 Feature Extractions and Constructions  Feature extractions and constructions in intensional view are much harder to describe formally since features repre- sent human view, and their mathematical relations have to be set up for all possible instances consistently.

We will take extensional view, the view from data's prospect. Let us examine some assertions (in traditional view) from [25]: "All new constructed features are defined in terms of original features, .." and "Feature extraction is a process that extracts a set of new features from the orig- inal features through some functional mapping." By taking the data view, it is easy to see both assertions imply that the new constructed feature is a function (functional mapping) of old features. Note that  Let A = {A', . . .A"} be the attributes before the ex- tractions or constructions, and A"+'. . . A"+"' be the new attributes. From the analysis above, the new attributes (fea- tures) are functions of old ones, we have  f : Dom(A') x . . . x Dmn(A")) -i D o ~ ( A " + ~ ) From the analysis on Section 6.1, .4"+k is a derived at- tribute of A. We summarize the analysis in:  Proposition 6.2. The features constructed from classical feature extractions and constructions are derived attributes in extension view.

6.3 Derived Attributes in the Canonical Model  From Proposition 5.2.1, K is isomorphic to the canoni- cal model CK. So there is a corresponding Table 4 in the  canonical model. In other words, there is a map,  V/Bk  x . . . x V / B k  = V / ( B i  n . . . n Bk) -i V/YE This map between quotient sets implies a refinements in the partitions; thatis,Y~isacoarseningofBE = B'n ... nBk.

So we have the following:  Proposition 6.3. Y is a derived attribute of B, iff YE is a coarsening of BE = B' n . . . B', where Y E A and B C A  7 Granular Data Model of Relation Lattice  In this section, we modify Lee's work At the beginning of Section 3.2, we have recalled the observation of [29, 71 that any subset of A induces a partition on V ;  the pan - tion induced by A' is denoted by Q J .  The power set Z A  is Boolean algebra and hence, a lattice, where meet and join operations are the union and intersection of the A respec- tively. Let A ( V )  be the set of all partitions on V (equiv- alence relations); A ( V )  forms a lattice, where meet is the intersection of equivalence relations and join is the "union," where the "union:' denoted by UjQ', is the smallest coars- ening of all Q j , j  = 1 , 2 , .  . . A ( V )  is called the partition lattice.

Recall the convention, all attributes are non-isomorphic attributes. Hence all equivalence relations are distinct; see Section 3.1. Next proposition is due to Lee:  Proposition 7.1. There is a map  8 : 2A -+ A(V), that respects the meet, but not the join, operations. Lee called the image, Im8, the relation lattice and observe that  1. The join in Imb' is different from that of A ( V ) ,  2. So Imb' is a subset, hut not a snhlattice, of A(V) .

Such an embedding is an unnatural one, but Lee focused his efforts on it; he established many connections between database concepts and lattice theory. However, we will, in- stead, take a natural embedding  Definition 1.2. The smallest lattice generated by Imb', by abuse of language, is called the (Lin's) relation lattice, de- noted by L(Q).

This definition will not cause confusing. since we will not use Lee's notion at all. The difference between L(Q)  and ImO is that former contains all the join of distinct attributes.

The pair (V, L(Q) )  is the granular data model of the (Lin's) relation lattice. It should be clear     Delinition 1.3. The high frequency q-patterns of (V, Q) ,  V q is the high frequency patterns of length one in (V, ImO), and is a subset of the high frequency patterns of length one in (V, L(Q)).

8 Universal Model - Capture the invisibles The smallest lattice, denoted by L'(Q),  that consists of  all coarsening of L(Q) is called the complete relation lat- tice.

MainTheorem8.1. L*(Q)isthesetofallderivedattributes of the canonical model.

Proof: ( 1 )  Let P E L*(Q), that is, P is coarser than some Qj1 n ._. n Qjk, We will show it is a derived attribute. The coarsening implies a map on their respective quotient sets,  y ; V/Qj' x VlQj' ... VlQjk - - V/(Qj' n Qjz . . . Qjh) + V I P  In terms of relational notations, that is  y : D m ( Q j ' )  x . . . x Dom(Qj*) t D m ( P )  Using the notations of functional dependency, we have (equivalence relations are attributes of the canonical model)  P = g ( ~ j l , ~ J z . . . , Q j k )  So g, as a map between attributes, is an attribute transfor- mation. Hence P is a derived attribute.

(2) Let P be a derived attribute of C K .  That is, there is an attribute transformation  Dom(Qj') x _._ x D m ( Q j k ) )  t D ~ L ( P )  As CK is the canonical model, it can be re-expressed in terms of quotient sets,  f : VIQ" x ... x VlQj' -i V I P Observe that VlQjl x .._ x VlQjk = V/(Q" n ... n Qjk), so the existence o f f  implies that P is coarser than Qj' n ... n Q j k .  BydefinitionPisanelementinL'(Q). Q.E.D  Note that L*(Q) is finite, since A ( V )  is finite. The pair (V, L'(Q)) is a granular data model, and its relation format  U K  : v - ~ P ~ L * ( Q ) is a knowledge representation. Its attributes are all the par- titions in L*(Q) , which contains all possible derived at- tributes of K = (V, &). by the theorem. We will not dis- tinguish betweenthe granular data model and its realtiion format:  Definition 8.2. The pair UK = (V,L'(Q)) is the comple- tion of CK = (V, Q)  and is called the universal model of K.

9 Isomorphic Relations  I V I K I (S# I Business I Birth I CITY) 1  TEN TEN  Table 5. An Information Table K  I V I K 1 (S# I Weight I Part I Material I  Table 6. An Information Table K  The two relations, Table 5 ,  6, are isomorphic, but their semantics are completely different, one table is about part, the other is about suppliers. These two relations have Iso- morphic association rules;  1 .  Length one: TEN, TWENTY, March, SJ, LA in Ta- ble 5 and IO, 20, Screw, Brass, Alloy in Table 6  2. Length two: (TWENTY, MAR), (Mar, SJ).

(TWENTY, SJ)in one Table 5 ,  (20, Screw), (screw, Brass),(ZO. Brass), Table 6  However, they have non-isomorphic interesting rules:  1 .  Table 5 :  (TWEBTY, S J ) ,  that is, the business amount at San Jose is likely 20 millions; it is isomorphic to (20, Brass), which is not interesting.

2. Table 6 (SCREW, BRASS), that is, the screw is most likely made from Brass: it is isomorphic to (Mar, S J ) , which is not interesting.

10 Conclusions  In this paper, we successfully enumerate all possible de- rived attributes of a given relation. The results seem strik- ing; however, they are of theoretical nature. Even though L * (&) contains a complete list of all attributes, the number is insurmountably large; it is bounded by the Bell number Bn, where n is the cardinality of the smallest paniton in L?(Q). The exhaustive search of association rules on all those attributes are beyond the current reach. However, by combining the classical techniques of feature selections, we may reach new applications. Classical feature selection has focused on the original set of attributes, now with our new result, it seems suggest that the domain of feature selection should he extended to this complete universal set of derived attributes. We have tentatively called such a selection back- ground knowledge. We will report such research in near future.

Next, we would like to remark that the simple ohser- vation that isomorphic relations have isomorphic patterns has a strong impact on the meaning of high frequency patterns. Isomorphism is a syntactic notion; it is highly probable that two isomorphic relations have totally differ- ent semantics. The patterns mined for one particular a p plication may contain patterns for other applications. So relation with some additional structures need to be ex- plored [23, 14, 15, 17, 20, 21, 111. In particular, it implies that ?interesting-ness? of association tuples may need extra semantics; the mere probability theory based on counting items may not be able to identify them; we only give a sim- ple example (Section 9) more research will be reported in near future.

11 Elementary Operations  In this section, we do not assume the attributes are dis- tinct. The isomorphism of relations is reflexive, symmetric, and transitive, so it classifies all relations into equivalence classes; we call them isomorphic classes.

Definition 11.1. H is a simplified information table of K, if H is isomorphic to K and only has non-isomorphic at- tributes.

Theorem 11.2. Let H be the simplified information table of K. Then the patterns (large itemsets) of K can be obtained from those of H by elementary operations that will be de- fined below.

To prove the Theorem, we will set up a lemma, in which we assume there are two isomorphic attributes B and B? in K ,  that is. degree K - degree H =I. Let s : D m ( B )  t Dom(B?) be the isomorphism and b? = ~ ( Z J ) .  Let H be the new table in which B? has been removed.

Lemma 11.3. The patterns of K can be generated from those of H by elementary operations, namely,  1. If b is a large itemset in H, then b? and (h, b?) are large in K.

2. If (a. _, b, c. . .) is a large itemset in H, then (a. . , b?, c. . .) and (a. . , b, b?, c.. . ,) are large in K.

3. These are the only large itemsets in K.

The validity of this lemma is rather straightforward: and it provides the critical inductive step for Theorem; we ill skip the proof.

