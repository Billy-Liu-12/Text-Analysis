Association Rules Optimization using Improved PSO

Abstract -In this work, association rules are optimized by using  improved particle swarm optimization algorithm (PSO  Algorithm). Here improved PSO algorithm means classical PSO algorithm with additional operator in the forms of mutation of  genetic algorithm. The basic shortcoming of PSO algorithm is to  get trapped into local optima. So for improving this, mutation  operator is used additionally in classical PSO algorithm. This  operator is used after the initialization phase of PSO algorithm.

Firstly, different association rules for generating frequent item  sets are generated by standard Apriori algorithm, then improved  PSO algorithm is applied on these generated association rules for  optimizing them. Experiments are performed on different  datasets taken from UCI machine learning repository and results  are compared with other previously proposed algorithms, called  KNN algorithm and ABC algorithm. These results show that the  proposed algorithms efficiency is better than previously proposed  algorithms.

Keywords: Particle Swarm Optimization, PSO, Mutation,  support, confidence,Association rule, frequent item sets.



I. INTRODUCTION  In today's scenario, due to increasing number of e? commerce companies all over the world, databases are increasing day-by-day. To mine the data from these huge databases in a less time is the difficult task behind all programmers. This can be improved if different improved optimization algorithms are applied on data mining algorithms for retrieving data from the database. In this situation, data mining [1, 2,9] plays an important role for mining data in a less time according to end users specification. There are so many algorithms available in the literature for mining data such as genetic algorithm based mining algorithm [3, 4, 6, 7, 10, 11]. Genetic algorithm is also one of the optimization algorithms which are more discussed in [5].Apriori algorithm is the most popular algorithm for generating frequent item-sets is explained in [8]. Other algorithms like ABC algorithm with mutation algorithm [12], PSO based association rules optimization are also available in literature.

In this work, improved PSO algorithm based association rule optimization technique has proposed and implemented on different datasets for checking the efficiency of proposed algorithm.

DOl 1O.1109/ICCN.2015.76   The overall work is organized as follows. Section 2 describes the Improved PSO algorithm called as PSO with mutation algorithm. Association rule mining is explained in section 3.

Proposed methodologyis discussed in section 4. About experiments and different parameters used in this work for performing experiments are explained in section 5. Finally section 6 summarized the overall work.



II. PARTICLE SWARM OPTIMIZATION ALGORITHM WITH MUTATION  In 1995, a new algorithm, called PSO algorithm, is proposed by Kennedy and Eberhart. This algorithm is nature inspired algorithm based on bird flocking. PSO algorithm works on two different phases. First is called initialization phase and another is called updating phase. This algorithm is very popular due to simple computation and sharing of information.

In this algorithm, the particles, which are also called individuals, are spreadin the multi-dimensional search space.

Here each particle or individuals shows a feasible solution forproblem which is going to be optimized. In this, performance function of the problem represents the fitness function of the PSO algorithm.

Here movements of the particle are influenced by two key parameters using information from particle-to-particle as well as iteration-to-iteration.

As a result of iteration-to-iteration information, the best solution, called pbest, is stored in particles memory and shared information among different particles. The best solution vis? ited by any particle, called gbest, is stored in particles memory as a result of particle-to-particle information. These two parameters are called as social component and cognitive component, respectively. Aftereach iteration of the PSO algorithm, in terms of fitness value if better or more dominating solution found then for each particle best solution is updated. This whole process continues till desired solution is not achieved for the problem.

The i-th particle velocity and position in the multi? dimensional search space are shown by the following vectors, Vi = (viI, vi2, . . .  ,vim)T and Yi = (yil, yi2, . . .  ,yim). The best solution of ith particle, that is visited previously, denoted as Pi = (pil, pi2, . . .  ,pim)T. Here 'g' represents the best particle index. The i-th particle velocity is updated using the following update equation given by    V,d = Vid + c/i (P,d -Xid) + cZrZ (p gd -X,d) , (1) and i-th particle position is updated using the equation given  below  (2)  where constants cl is called cognitive scaling parameter and c2 is called social scaling parameters respectively, rl and r2 represents random numbers, d represents dimension, i represents the particle index and S represents the size of the swarm.

The maximum velocity, represented by V max, is used to manage global exploration ability of particle swarm. Further, for better scheming exploitation and exploration property, in 1998 [2], a new concept in the form of inertia weight introduced. After introducing this concept, the velocity update equation of PSO algorithm is changed and the changed equation is asfollows:  V,d = W*V,d +cl1(P,d - X'd) +cZrZ(pgd -X,d)  The overall procedure is depicted below: 1. Initialization Phase Repeat Repeat  (3)  Initialize particle randomly with sequence of tasks Until (Dimension size) Computer that particle fitness Compute global best (gbest) Until(Swarm Size)  2. Mutation Phase If mutation criteria met then  Random particles are selected from current populationformutation operation  On randomly selected particle, apply mutation operation As a result of mutation operation, update particle position  randomly.

Compute updated particle fitness Update the historical information, if needed, of global best End if REPEAT  3 . Update Phase Repeat  Repeat Using position update equation of PSO, update particle positions Until (Problem Dimension) Compute updated particle fitness  Update historical information, if needed, for global best Until(Swarm Size) Until( not getting stopping criteria)

III. ASSOCIA TION RULE MINING The main aim of association rule mining is to extract  frequent item sets, correlation and association among different set of items in the transactional database, relational databases or other information repository.

These algorithms search association rules in the following form:  IF PQ and RS then HI  IF MN and OP then HELLO Here PQ, RS, MN and OP are different objects. If any  person takes PQ and RS then due to high probability, he will take HI. Similarly if he will choose MN and OP then he will choose HELLO.

In general, expressions which are in the form of P=>Q, called association rules where P represents antecedent and Q represents consequent.

Association rules represent how many times Q has occurred if P has already occurred depending on the chosen support and confidence value. Here support is nothing but the probability of items or item sets in the given database (like transactional or other) and confidence represents conditional probability.

Apriori Algorithm:  In general, Apriori algorithm [8] works on two different phases. In first phase, minimum support value is chosen which is used to find frequent item sets while in second phase, these frequent item sets and the minimum confidence value are used to produceassociation rules.

The Apriori algorithm are described as follows -  Step 1: let Cn be the candidate item set of size n.

Step 2: let Fn be the frequent item set of size n.

Step 3: F! = {Frequent items}  Step 4: REPEAT  Step 5: Cn+! = Candidates generated from Fk;  Step 6: REPEAT for all transactions in the database  Step 7: increment the count of all candidates in Cn+1 that are  contained in t.

Step 8: Fk+! = Candidates in Cn+! with minimum support.

Step 9: UNTIL ( Fn not equal to <j> )  Step lO: return Un Fn

IV. PROPOSED METHODOLOGY  This section shows proposed methodology. In this proposed methodology, Apriori algorithm is used to generate frequent item-sets then applied PSO with mutation algorithm over thesegenerated rules for optimization.

In classical PSO algorithm, two phases that described the overall working of this algorithm, but here one additional phase in the form of Mutation operator is added after the initialization phase. Now modified particle swarm optimizationalgorithm has three phases: initialization phase, mutation phase and particle update phase. With the help of mutation operator,there may be a possibility to change the local best position and the algorithm may not be trapped into local optima. In this work, the mutation phase is implemented on the probabilistic way in each iteration for searching food source during the life process of PSO optimization technique.

Here using randomly generated transactions, Initial population is generated. The following fitness function is used to check the fitness value of an individual-    Fj = 1 / (1 + Fd if Fj>= 0 1 + abs(FD otherwise  The proposed algorithm works as follows: Step 1: Start Step 2: Load dataset Step 3: Using Apriori algorithm, Compute frequent item sets.

Lets F be the set of all items generated by standard Apriori algorithm and X be the output set, consists of all association rules taken from Apriori algorithm, initially set to zero.

Step 4: Set the algorithm termination criteria.

Step 5: Depict each item sets of Z and apply PSO with mutation algorithm on selected members to generate association rules.

Step 6: Compute each generated rule fitness value.

Step 7: If the desired criteria met then insert these generated rules in X.

Step 8: if the termination criteria not met then goto step 3 Step 9: Stop  Block diagram of proposed work:  Apriori algorithm applied on  dataset  Fig I Block diagram of proposed algorithm  Figure 1 shows the operall working of the proposed methodology using block diagram.



V. EXPERIMENTAL RESULTS & PARAMETER SETUP  1. Data Sets : For checking the Efficiency of the proposed methodology,  different datasets from UCI machine learning repository are selected. Currently, there are 187 datasets maintained by this research group. Out of these different datasets, three most popular datasets of Wine, Voting and Iris are usedhere for experiments.

Details description of these different datasets are given below  in table 1 -  TABLE 1. SHOWS THE DETAILED DESCRIPTION OF DA T ASETS  Datasets Features Instances Class  Wine 13 178 03  Voting 16 435 02  Iris 04 150 03  2. Parameter Settings: Few four control parameters are used to test the performance of proposed algorithm, listed in table 2.

TABLE 2 SHOWS THE DETAILED DESCRIPTION OF DATASETS  Control Parameter Value  Max Cycle Number (MCN) 2000  Number of Food Sources 20  Number of Employed bees 20  Number of Onlooker Bees 20  Mutation Probability 0.3  Quality weight (a) 0.5  coverage weight (p) 0.5  Proposed algorithm is compared with standard KNN algorithm and PSO algorithm. Table 3 shows the performance accuracy of proposed algorithm with other algorithm. Figure 2 shows the results in the form of column chart.

TABLE 3 SHOWS PERFORMANCE OF CLASSIFTCA TION  Datasets  Voting  Iris  Wine           KNN(%)  95.10  94.08  96.22  KNN (%) ABC(%)  ABC (%)  97.21  96.44  98.13  Proposed Work (%)  Proposed Work (%)  97.47  97.89  98.75  - Voting  - Iris  - Wine  Figure 2.Performance of proposed work

VI. CONCLUSION  In today's scenario, due to the increasing growth of databases, data mining plays an important to mine specific data from the database. In this work, First of all frequent item sets are generated using association rule miningand then generated association rules(by using Apriori algorithm) are enhanced using improved PSO algorithm. To check the performance of proposed Algorithm, three different kinds of datasets, wine, voting and iris, are used. These datasets are collected from UCI machine learning repository. Experimental results show that the efficiency of the proposed algorithm with previously proposed algorithms. Future work is to use the proposed algorithm with other different databases.

