Detecting Hostile Accesses through Incremental Subspace Clustering

Abstract  In this paper, we propose an incremental subspace clus- tering method for flexibly detecting hostile accesses to a Web site. Typical log data for Web accesses are huge, con- tain irrelevant information, and exhibit dynamic character- istics. We overcome these difficulties through data squash- ing, subspace clustering, and an incremental algorithm. We have improved, by modifying its data squashing functional- ity, our subspace clustering method SUBCCOM so that it can exploit previous results. Experimental evaluation con- firms superiority of our I-SUBCCOM in terms of precision, recall, and computation time.

1 Introduction  Hostile accesses to Web sites have caused serious dam- age to our community. Such examples include the DoS at- tacks to 13 root servers which are spread in the world in October 2002, and the SQL slammers which caused seri- ous damage to Korea and other countries in January 2003.

Recently, the Internet has significantly increased its impor- tance in our daily life, and hostile accesses to Web sites can cause fatal problems.

In order to detect such hostile accesses automatically or semi-automatically, application of fraud detection tech- niques which are based on a machine learning method is ex- pected to be promising. Such methods include association rule [10], classification [7], statistical test [5], meta learn- ing [2], and outlier detection [13]. The last approach might lack in reliability, but can potentially detect novel kinds of attacks. We believe that clustering [9] can substitute for out- lier detection.

Various methods have been proposed for clustering, which partitions a given set of examples to a set of similar groups called clusters [6]. We attribute the reason to various definitions of a cluster and various functionalities in the pro- cedure. For instance, we have defined a cluster in terms of compressibility based on a distance measure, and proposed  a fast clustering method SUBCCOM [11] which can select relevant attributes by squashing data for time efficiency. Al- ternatively, SUBCCOM can cluster a huge amount of data with irrelevant attributes efficiently.

In order to (semi-)automatically detect hostile accesses to a Web site with clustering, the following three condi- tions are considered to be mandatory: efficient handling of huge data (Web access log data are typically huge), detec- tion of relevant attributes (such data typically contain irrel- evant attributes), and an incremental algorithm (such data are incrementally updated). Since SUBCCOM cannot ful- fill the last requirement, we propose its incremental version I-SUBCCOM in this paper.

In the rest of the paper, we define the subspace clustering problem and explain SUBCCOM in section 2. Section 3 in- troduces Web access log clustering for hostile access detec- tion, and I-SUBCCOM is proposed as a time-efficient solu- tion. We experimentally evaluate its performance in section 4, and give conclusions in section 5.

2 SUBCCOM  2.1 Description of the Subspace Clustering Prob- lem  The input of subspace clustering includes m examples x1,x2, ? ? ? ,xm each of which is represented with n numer- ical attributes. We denote the set of these n attributes as V . Note that an example xi represents a point in an n- dimensional space. The output of subspace clustering rep- resents a partition ?1, ?2, ? ? ? , ?c, each of which represents a cluster, of x1,x2, ? ? ? ,xm; and a set v (? V ) of relevant at- tributes. Note that attributes in v span the subspace in which clusters exist.

A pattern extraction procedure in data mining is typically interactive: the user modifies the condition of the procedure in a series of application of methods and investigation of results. In this paper, we assume that the user specifies the number l of attributes in v.

2.2 CF Tree for Data Squashing  The main stream of conventional data mining research has concerned how to scale up a learning/discovery algo- rithm to cope with a huge amount of data. Contrary to this approach, data squashing [4] concerns how to scale down such data so that they can be dealt by a conventional al- gorithm. A CF (clustering feature) tree, which represents a data structure for data squashing, was proposed in a fast clustering algorithm BIRCH [14].

A CF tree represents a height-balanced tree which is sim- ilar to a B+ tree [3]. A node of a CF tree contains, as entries to its child nodes, a set of CF vectors each of which cor- responds to an abstracted expression of a set of examples.

For a set of examples x1,x2, ? ? ? ,xN to be squashed, a CF vector CF consists of the number N of the examples, the add-sum vector  ?N i=1 xi of the examples, and the squared-  sum ?N  i=1 ?xi?2 of the attribute values of the examples.

Since the CF vector satisfies additivity and can be thus up- dated incrementally, BIRCH requires only one scan of the training data set. Moreover, various inter-cluster distance measures can be calculated with the corresponding two CF vectors only. This signifies that the original data set need not be stored, and clustering can be performed with their CF vectors only.

A CF tree is constructed with a similar procedure for a B+ tree. When a new example is read, it follows a path from the root node to a leaf, then nodes along this path are updated. Selection of an appropriate node in this procedure is based on a distance measure which is specified by the user. In a leaf, the example is assigned to its closest entry (i.e. a CF vector) if the distance between the example and the examples of the entry is below a given threshold L. An entry of a leaf represents a set of squashed examples, and is the smallest unit in a CF tree.

2.3 Compressibility as an Evaluation Criterion  It is widely accepted that compressibility is deeply re- lated to inductive learning. For instance, MDL (Minimum Description Length) principle [12], which has been success- fully employed in inductive learning as an evaluation crite- rion, favors a model which effectively compresses the infor- mation content of a given data set and a model.

A CF tree highly depends on the set of attributes which are employed. An entry in a leaf of a CF tree, which we explained in the previous section, contains a set of exam- ples which are close to each other. Therefore, the number of entries in leaves is related to the number of dense regions in the example space, and can be used to define an index of compressibility. In this paper, we propose ??(u) as an evaluation criterion of a subspace spanned by u, where ?(u)  represents the number of entries in leaves of the correspond- ing CF tree.

2.4 Heuristic Search for the Relevant Attributes  Note that subsets of attributes form a complete lattice, of which infimum and supremum are an empty set and V re- spectively. We have realized search for a set v of relevant attributes as a search procedure for an appropriate subset of V in the complete lattice guided by the evaluation crite- rion in the previous section. Forward search and backward search represent brute force search from the infimum and the supremum respectively.

Jumping search, which we proposed in [11], represents a heuristic search method which is expected to be time- efficient without sacrificing accuracy. Our method first builds a CF tree for each attribute ai with L = T1, then ob- tains a set u of attributes each of which evaluation criterion value ??(ai) is no smaller than the average value. In case |u| < l, remaining attributes which have the largest evalu- ation criterion values are added to u so that |u| becomes l.

Finally, our method, from u, performs a backward search with L = T2. Since a single attribute exhibits high com- pressibility, T1 is typically settled to a much smaller value than T2.

Our subspace clustering system, which we call SUBC- COM, first obtains a set u of possibly relevant attributes and the entries y1,y2, ? ? ? ,yp of leaves of the corresponding CF tree with jumping search, then applies the k-means algo- rithm to y1,y2, ? ? ? ,yp. We show the algorithm of SUBC- COM below, where the procedure k-means is modified so that it clusters a set of squashed examples y1,y2, ? ? ? ,yp.

Algorithm: SUBCCOM(x1,x2, ? ? ? ,xm, l, v, ?1, ?2, ? ? ? , ?c) Input: data set x1,x2, ? ? ? ,xm, number of relevant attributes l Output: subspace v, clusters ?1, ?2, ? ? ? , ?c Parameter: entries y1,y2, ? ? ? ,yp in leaves of the final CF tree ? 1 For(i = 1; i ? n; i++) 2 Build a CF tree using values for ai with L = T1 3 u = ? 4 Foreach(ai such that ??(ai) ?  ?n i=1 ??(ai)/n)  5 u = u ? {ai} 6 While(|u| < l) 7 u = u + {a?} where a? = argmaxa??u ??({a}) 8 While(|u| > l) 9 Foreach(ai ? u) 10 Build a CF tree using values for u ? {ai} with  L = T2 11 u = u ? {a?} where a? = argmaxa ??(u ? {a}) 12 Build a CF tree ? using values for u with L = T2 13 v = u      14 (?1, ?2, ? ? ? , ?c) = k-means(y1,y2, ? ? ? ,yp)  3 Proposed Method  3.1 Web Access Log Clustering for Hostile Access Detection  Each time a user accesses to a Web site, an access log is generated. We show below an example of a Web access log.

kali.slab.dnj.ynu.ac.jp - ningen [04/Apr/2001:16:48:57 +0900] ?GET /algodata HTTP/1.0? 401 476  This log, which represents a daily access, consists of seven parts. They are, from left to right, the name of the remote host1, the name of the user in the remote host2, the name of the user in the identification, the day and the time of the accesses, the content of the request, the sta- tus code which was returned to the request, and the bytes which were sent. The content of the request consists of an HTTP command, the name of the requested file, and the version of HTTP. This example shows that a user from ?kali.slab.dnj.ynu.ac.jp? tried to obtain files below ?/algo- data? with an identification name ?ningen?, and obtained a status code ?401? which represents a denial of identifica- tion, probably due to a misuse of a password.

Other than daily accesses, accesses to a Web site also include hostile accesses such as attacks with worms (e.g.

Nimda, Code Red). For instance, an attack with the Code Red has generated the following log3.

202.39.???.??? - - [02/Aug/2001:06:54:59 +0900] ?GET/default.ida?NNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN NNNNNNNNNN%u9090%u6858%ucbd3%u7801%u90 90%u6858%ucbd3%u7801%u9090%u6858%ucbd3%u78 01%u9090%u9090%u8190%u00c3%u0003%u8b00%u53 1b%u53ff%u0078%u0000%u00=a HTTP/1.0? 400 331  An administrator is required to detect hostile accesses from Web access log data and take appropriate measures as  1Here a remote host name can be replaced by its IP address 2A ?-? is shown when the remote host does not employ an identification  service, a daemon which gives information of the user for identification. A typical host does not employ such services  3We modify the IP address to protect privacy of the user  soon as possible. Fast detection is, however, usually dif- ficult since Web access log data consist of a huge number of examples, are updated continuously, and reflect dynamic characteristics of accesses. Suppose we are given 100,000 web access logs of which 1 % are hostile. If an administra- tor inspects 100 randomly-chosen logs of the 100,000 logs, s/he has little chance of detecting hostile accesses.

In the example, however, suppose the 100,000 logs are segmented into five groups, each of which contains 95,000, 4,000, 700, 200, or 100 logs. If each group consists of simi- lar logs, a small cluster would contain peculiar logs each of which is expected to have a better chance of being hostile than a typical log. Therefore the administrator has a much better chance of detecting hostile accesses by inspecting the small groups than random inspection. We thus employ clus- tering as preprocessing in order to facilitate detection of hostile accesses. It should be noted again that Web access log data are huge and dynamic. These characteristics favor an incremental clustering which substitutes the latest data for the oldest data.

Below we define Web access log clustering for hostile access detection. The input represents s sets of examples w1,w2, ? ? ? ,ws in chronological order, and wj is called a processing unit. The processing units which are clustered simultaneously are called a processing object, and we as- sume that a processing object consists of at most h process- ing units. Suppose a processing unit wi+h+1 is added to a processing object wi+1,wi+2, ? ? ? ,wi+h, the oldest pro- cessing unit wi+1 should be deleted beforehand since a pro- cessing object consists of at most h processing units. Alter- natively, this method first grows its sliding window from size 1 to h, then fixes the size to h until the end. In this framework, a method outputs results for each processing object.

The performance of a method is evaluated with precision and recall in terms of hostile access logs. Since it is difficult for a user to inspect all access logs, the user samples a fixed number of logs from each cluster as we have stated. The precision P and the recall R are defined in terms of the sampled examples, where ?#? represents ?the number?.

P = (# of hostile access logs in the sampled examples)  (# of sampled examples)  R = (# of hostile access logs in the sampled examples)  (# of hostile access logs)  3.2 I-SUBCCOM  For the problem in the previous section, SUBCCOM, which we explained in section 2.4, is inefficient since it re- solves the problem from scratch each time the processing object is updated. In order to circumvent this problem, we propose I-SUBCCOM which is an incremental version of SUBCCOM.

I-SUBCCOM exploits previous results by allocating a CF vector for each processing unit in a leaf of a CF tree.

A leaf ?i of a CF tree stores CF vectors yi1,yi2, ? ? ? ,yij (1 ? j ? h) for processing units4, thus examples from a processing unit are squashed to an identical CF vector. A leaf has also a set of entries each yi of which represents an add-sum of yij in terms of j. We employ yi for deciding allocation of an entry of a leaf to an example. In case a processing unit wk is deleted, each yik is first deleted, then each yi is updated.

In case adding a processing unit to the processing object, our algorithm employs entries of leaves of the previous CF tree and the examples of the processing unit. If the process- ing object consists of h processing units, the entries each of which corresponds to the oldest processing unit are deleted in advance. A CF tree is constructed by first using the re- maining entries, then the examples of the latest processing unit.

We show the algorithm of I-SUBCCOM below.

Algorithm: I-SUBCCOM(w1,w2, ? ? ? ,ws , l) Input: processing units w1,w2, ? ? ? ,ws, number of relevant attributes l Output: a set of clusters ?1, ?2, ? ? ? , ?s 1 ? = ? 2 For(i = 1; i ? s; i++) 3 ? = build-itree(?) 4 Let the examples in wi be x1,x2, ? ? ? ,xm 5 SUBCCOM2(? , x1,x2, ? ? ? ,xm, l, u, ?i1, ?i2, ? ? ? ,  ?ic) 6 ?i = {?i1, ?i2, ? ? ? , ?ic} 7 Output ?i 8 If(i > h) 9 Foreach(j) 10 Delete yj i?h+1 from leaves of ? 11 Update yj 12 Foreach(j) 13 ? = ? + {yj}  In the algorithm, the procedure build-itree returns the CF tree ? defined in this section from entries yj in ?, or returns a null tree when ? = ?. The procedure SUBCCOM2 em- ploys ? as its initial CF tree and employs the k-medoids al- gorithm instead of the k-means algorithm in SUBCCOM5.

For instance, suppose we have three processing units w1,w2,w3, and h = 2. Firstly, the processing object con- sists of w1, and SUBCCOM outputs the first results from it.

Secondly, w2 is added to the processing object. The entries  4The number of CF vectors is at most h since a leaf does not necessarily contain examples from all processing units  5While SUBCCOM assumes that an example represents a point in an n-dimensional space, I-SUBCCOM assumes that an example represents a Web access log. In the latter, a mean of a cluster can represent a non- existent Web access log  in leaves of the previous CF tree are employed to construct an initial CF tree, which is then updated with examples in w2, and we obtain the second results. It should be noted that, in a leaf, examples of w2 are stored in a set of CF vectors different from a set of CF vectors for w1. Thirdly, w1 should be deleted before adding w3 to the processing object. The CF vectors of leaves for w1 are deleted and the entries of leaves are updated. I-SUBCCOM obtains the third results by constructing an initial CF tree with the en- tries then updating it with examples in w3.

4 Experimental Evaluation  4.1 Preprocessing of Web Access Logs  We employ Web access log data of our Web site ?www.slab.dnj.ynu.ac.jp? in the experiments. The data con- sist of 205,590 examples measured during 99 weeks. Since I-SUBCCOM assumes numeric attributes for distance cal- culation, we transformed a nominal attribute to a numerical attribute as follows. We attributed an integer value for each category of a part of a Web access log: the remote host name, the user name in identification, the HTTP command, the requested directory, the status code, the bytes which were sent, and the requested file.

? Remote host name: we categorized remote host names into those inside our laboratory, our university, our country6, and the rest; and allocated them 0, 10, 20, 30 respectively. Since most of the accesses were do- mestic, we gave them detailed categories.

? User name: we categorized user names into those with- out names; either of YNU, dnj, member, ningen, stu- dent; and the rest. We allocated them 0, 10, 20 re- spectively. Each of the proper nouns represents a user name provided by our administrators. We believe that most of the rest are due to mistypes, thus considered to be more similar to the proper nouns than those without names.

? HTTP command: we categorized HTTP commands into GET and the rest, and allocated them 0 and 30 respectively. The value 30 is chosen since an HTTP command other than GET is rare, and we considered balance with other attributes.

? Requested directory: we categorized requested direc- tories into those which were accessed at least 1000 times, and the rest; and allocated them 10, and 0 re- spectively. In addition, we allocate 30 as the value if the file name consists of more than 49 ASCII char- acters. This is justified since a hostile access such as  6Until here, the smallest category applies to a remote host name      worms and requests for non-existent files tends to re- quest a file with a long name.

? Status code: we transformed each status code into the first digit multiplied by 10. Status codes are sorted from successes to errors and from user-driven to server-driven7. Since a status code in the data is within 200 and 505, there are only four kinds of values, i.e.

20, 30, 40, and 50.

? Bytes which were sent: we allocated 0 to 0 - 499 bytes, 10 to 500 - 1149 bytes, 20 to 1150 - 4499 bytes, and 30 to the rest. The threshold values were determined so that each bin contains approximately fifty thousands of examples.

? Requested file: the requested files were categorized by their extensions. We allocated 0 to a typical non- executable file (c, csv, dat, doc, gif, hdml, htm, html, ico, jpg, pdf, png, ppt, ps, txt, xbm, xls, xpm, or zip); 10 to a typical executable file (asp, cgi, dll, exe, ida, php, php3, or pl); and 20 to the rest. We believe that a web access log is more likely to be hostile in this order.

4.2 Experiments  The preprocessed data were employed in the experi- ments for evaluating the performance of our I-SUBCCOM and its alternatives. We settled the length of a process- ing unit to one week, h = 4, and the number of clus- ters to 5. The parameters for I-SUBCCOM were set to T1 = 0.01, T2 = 2.0, and those for BIRCH L = 2.0.

We assume that the user is willing to inspect at most 100 examples, thus the user samples at most 20 examples from each of 5 clusters for inspection. If a cluster consists of less than 20 examples, the user investigates all examples in the cluster. It should be noted that no one would easily inspect all access logs nor recognize all hostile accesses. This signi- fies that we need an approximate measure for performance evaluation, thus we employ Nimda and Code Red, both of which have numerous examples and are relatively easy to be recognized8. We measured the precision and the recall described in section 3.1 by assuming that a hostile access corresponds to either Nimda or Code Red. Each precision and recall is an average of 100 trials since the user samples examples to be inspected randomly.

The number l of relevant attributes in a cluster was set- tled to l = 4, 5, 6 for I-SUBCCOM. For comparative pur- pose, we have chosen one of the latest algorithms which select relevant attributes: PROCLUS [1] (l = 4, 5, 6). Two  7Status codes are grouped into 2XX (Success), 3XX (Redirection), 4XX (Client Error), and 5XX (Server Error)[8].

8We recognized Nimda and Code Red if either of the following strings ?root.exe?, ?cmd.exe?, or ?default.ida? is contained in an access log  representatives of ?whole-space? clustering BIRCH and the k-medoids algorithm [9] were also employed . For a fair comparison, BIRCH was modified to an incremental algo- rithm by substituting the representation and the procedure of I-SUBCCOM for its CF tree. In order to evaluate the benefits of machine learning approach, we also employ a ?naive? method which samples 100 examples from the pro- cessing object randomly.

We show the results for precision, recall, and computa- tion time in figures 1, 2, and 3 respectively. The results for precision and recall begin from the 29th week, when Code Red appears in the access logs for the first time9. For vis- ibility, each value represents an average of five successive processing objects (i.e. five successive updates), and the results of l = 5, 6 are omitted due to their inferior perfor- mance.

0.05  0.1  0.15  0.2  0.25  0.3  30 40 50 60 70 80 90  I-SUBCCOM (l=4) PROCLUS (l=4)  BIRCH k-medoids  no clustering  week  pr ec  is io  n  Figure 1. Results for precision   0.1  0.2  0.3  0.4  0.5  0.6  0.7  30 40 50 60 70 80 90  I-SUBCCOM (l=4) PROCLUS (l=4)  BIRCH k-medoids  no clustering  week  re ca  ll  Figure 2. Results for recall  9There are neither Code Red nor Nimda till the 28th week      0.01  0.1       10 20 30 40 50 60 70 80 90  SUBCCOM (l=4) PROCLUS (l=4)  BIRCH k-medoids  week  co m  pu ta  tio n  tim e  [s ]  Figure 3. Results for computation time. Note that the vertical axis is in a log scale  4.3 Analysis of Experimental Results  From figures 1 and 2, some readers might consider that I-SUBCCOM exhibits lower precision and lower recall than expected. The purpose of Web access log clustering, how- ever, is a preprocessing method for detecting hostile ac- cesses, as we stated in section 3.1. We believe that these results signify that our I-SUBCCOM is useful in organizing a huge amount of Web access logs in order to give alerts to administrators.

In figure 1, our I-SUBCCOM (l= 4) almost always out- performs other methods in precision, and we attribute the reason to appropriateness of our approach to this problem.

The k-medoids algorithm is ranked the second probably due to its lack of subspace clustering capability. BIRCH is in- ferior to the k-medoids algorithm probably due to its use of data squashing. PROCLUS (l=4) is often outperformed by the other clustering-based methods since its use of ran- dom sampling in search is considered to be inadequate for this problem. The ?no clustering? approach, which approxi- mately corresponds to random sampling, almost always ex- hibits the worst performance, and this shows effectiveness of the clustering-based approach. An administrator would thus benefit from the use of a machine learning method in this problem domain.

Since recall represents a detection rate of the whole hos- tile accesses by sampling, it measures performance of a more difficult problem than that of precision. Therefore, the differences of the methods in figure 2 are typically much smaller than those in figure 1. However, it can be safely stated that our I-SUBCCOM (l= 4) represents the best method, and we believe that this is due to appropri- ateness of our approach. Analysis of figure 2 is similar to  that of figure 1.

In terms of computational time, BIRCH is the fastest  probably due to its incremental nature and data squashing capability as we see in figure 3. Our I-SUBCCOM (l= 4) is second to BIRCH probably due to its overhead of selecting a subspace for clustering. However, the overall performance in terms of precision, recall, and computational time, shows that our I-SUBCCOM is the best method for detecting hos- tile accesses. The other two non-incremental methods are much slower than BIRCH and I-SUBCCOM, and the differ- ence can be more than 100 times. PROCLUS is faster than the k-medoids algorithm probably due to its use of sampling in search.

It should be also noted that each clustering-based method facilitates detection of rare accesses as a side-effect. For instance, ?GET? represents an outstanding majority in the HTTP commands, and it is difficult to detect other kinds of HTTP commands with the ?no-clustering? approach. On the other hand, the Web access logs with these commands are almost always detected in each clustering-based method since the logs are separated in small clusters. We believe that this fact implicitly validates the use of a clustering- based method for detection of novel hostile accesses which are dissimilar from a typical access.

5 Conclusions  In this paper, we proposed the use of clustering as a promising preprocessing method for detecting hostile ac- cesses to a Web site. Since Web access log data are huge, contain irrelevant attributes, and are updated incrementally, an incremental subspace clustering method seems appro- priate in order to fulfill this objective. We have upgraded SUBCCOM, which we proposed in [11], to an incremental algorithm by modifying its data squashing functionality in order to satisfy all the three requirements.

Experimental evaluation shows that our I-SUBCCOM exhibits promising results in terms of precision, recall, and computation time; and outperforms other alternatives by its overall performance. Our future work includes effec- tive transformation of a nominal attribute to a numerical at- tribute and application of our approach to other kinds of fraud detection problems.

