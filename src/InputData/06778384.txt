Spatial-Temporal context for action recognition  combined with confidence and contribution weight

Abstract?In this paper, we propose a new method for human action analysis in videos. A video sequence of human action in our perspective can be modeled through feature distribution over spatial-temporal domain. Relationships between features and each defined action are also explored to form discriminative feature sets. In our work, we first capture contextual correlations between the local features through multiple windows. We then mine confidences from association rules and learn contributions from trained-SVM based on sample videos. Finally, through the analysis of feature distribution and their interactions over spatial-temporal domain, we combine the contexture correlations and the relationships between words and their related actions to derive weights of bag of feature words for action matching. In most of the case, our experiments have indicated that the new method outperforms other previous published results on the Weizmann and KTH datasets.

Keywords?Human action recognition, spatial-temporal context, local feature, weight

I.      INTRODUCTION Human action recognition is one of the most challenging  problems in computer vision due to large variations in the same class.  It also suffers from camera motion, occlusion, cluttered background and geometric and photometric variances. There are two important issues in action recognition which play a decisive role in the performance of recognition. The first is to extract discriminative and robust features to describe actions.

The second is to choice the best category model that uses such type features for corresponding classification.

For action recognition, many approaches have been proposed over the years including static features such as shapes or contours, dynamic features such as optical flows and local spatial-temporal features. Since the Space-time interest points [1], the local spatial-temporal points have been popularly used as a fundamental low level image features for object, scene and even human action recognition. Reference [3] evaluated and compared previously proposed local spatial-temporal features in a common experimental setup. Juan Carlos Niebles et al. [6] extracted space-time interest points as motion features and performed unsupervised learning of human action categories using pLSA model [4] and LDA model [5] separately.

In recent years, applying spatial-temporal descriptors to ?bag of words? model has been widely used in human action recognition tasks. In [2] work, a histogram which is only consisted of the point types is used as the action descriptor and the distance between histograms are measured to indicate the similarity between the template and the test video. Reference  [2] is the first model embedded the spatial-temporal feature for action recognition. Such approach is useful to represent the local appearance of the 2D+T interest points. Though they are robust to viewpoint, scale change and proved to achieve significant performance, the ?bag of words? methods have their limitations. They model the actions just as bag of unordered and independent words which lost the information about spatial and temporal distributions of interest points and each visual word is considered to have the same contribution to the action in these methods.

To make use of such correlated information, some feature graph models and pairwise models are proposed to describe the actions. U.Gaur et al. [7] proposed a "String of Feature Graphs" based on a string representation of the video which respects the spatial-temporal ordering.  W.Brendel et al. [8] also learned a spatial-temporal graph for human activities. Ryoo etal. [9] presented a histogram called ?featuretype?featuretype? relationship? to capture both appearance and relationship information between pairwise visual words. Although these feature graphs achieved promising classification results, they are not self-adaptive and time-consuming. For the pairwise models, they result in a significantly larger number of possible visual words.

Recent researches have focused on using spatial and temporal context as additional information to describe the local features. Sun etal. [10] proposes a hierarchical  model to structure the spatial-temporal context information of SIFT points. Their model includes three levels: point-level context, intra-trajectory context and inter-trajectory context. Savarese etal. [11] defines a local histogram to capture the number of occurrences of each visual word in a local region, and concatenates all the local histograms as descriptor. In [12], multiple GMM is utilized to describe the context distributions of interest points over multi-scale local regions. Jiang Wang etal.[13] proposes a novel representation to capture contextual interactions between interest points in each multi-scale spatial- temporal contextual domain. All of these contextual features have been proved to be bene?cial to action recognition tasks, but most of the existing contextual features may add some noises to the model.

In action recognition, space-time interest points and some contextual features always contain noises or irrelevant information to certain action and it not only increases computational complexity but also degrades the classification performance. Therefore, in this paper, we propose a novel method for action recognition that takes the feature spatial- temporal context and its contribution to each action into  2013 Second IAPR Asian Conference on Pattern Recognition  DOI 10.1109/ACPR.2013.114   2013 Second IAPR Asian Conference on Pattern Recognition  DOI 10.1109/ACPR.2013.114     consideration. We have made three contributions: (1) we use spatial-temporal context as additional information to represent the relationship among the 2D+T local features and design a simple but discriminative local contextual descriptor. (2) We mine confidences from association rules and learn contributions from trained-SVM. The former mines a statistical relation between each visual word and its related actions through data mining and the latter learns a causal relationship between them from a linear SVM. (3) Create weights of feature words for each action based on the confidences and contributions. The spatial-temporal context fully expresses the character of these local features which are dynamic and dependent in both spatial and temporal domain. The confidences and contributions represent the relationships between features and each defined action. And the weight of feature word makes each visual word more discriminative to action recognition and less sensitive to noise. Because of these improvements, the method generally outperforms the existing methods on Weizmann and KTH datasets.

Fig.1. Use the BOW to describe the action of wave2 and run in Weizmann dataset. The x-axes represent the visual words and the y-axes are their frequency of occurrence.



II.      WEIGHT FOR VISUAL WORD The information about the local space-time interest points or  the spatial-temporal context extracted in a video sequence is very large. It contains both useful component for recognition and also some other interferences. Without any processing, the noises or the irrelevant features may degrade the classification performance. Just as shown in Fig.1, the difference between the action of wave2 and run mostly focus on some bins of the histogram, which have high contributions to the related actions, while other bins have less or even negative influence on classification.

In this paper, we present a new method to mine the confidence from association rules through data mining and learn the contribution from trained-SVM. After finding the latent relation in the internal of features, we formulate the confidence and contribution together into a function to generate the weights of visual words for action matching.

A. Confidence To mine the latent relations in the internal of features, we  use a statistical approach to find the association rules between feature words and their related actions.

Association rules describe relationships between two disjoint item sets X and Y(X ?Y). X is known as antecedent and Y as consequence. The X ?Y represents the pattern when X occurs,  Y also occurs. If Y is an item set of actions and X is an item set of feature words, the association rules mined from training data can express the relationships between features and actions. For an association rule X?Y, we can calculate the support and the confidence.

Fig.2 .The weights can remove some noises and irrelevant local features, the left is image with original interest points in KTH, while the right just shows the points with positive weight.

Confidence (X ?Y) = support (X Y) / support(X)  =P(X ? Y) / P(X) = P (Y | X)                     (1)   Conf (Xi , Xj?Y)=P(Xi?Xj?Y)/P(Xi?Xj) = P (Y | Xi , Xj)   (2)   The support in (1) is for frequent item set which makes sure the feature word is frequently appeared in the action and rule out the possibility that it is a noise point. In fact, the support is a joint probability of sets X and Y. It represents the probability of the condition for the two sets occur together. If {X, Y} is a frequent item set, both subsets X and Y must be frequent item set as well. The frequent item sets are searched for all of the training samples by method of data mining [14].

The Con?dence in (1) is used to evaluate association rule which expresses a positive correlation between the features and actions. It is the support of the set of all items that appear in the rule, divided by the support of the antecedent of the rule.

Interesting association rules are those rules whose support and confidence exceed thresholds known as minSup and minConf.

Hence, the association rule can make the resulting frequent feature set to be distinctive between inter-action classes. If the antecedent set contains more than one feature words, the rule will not only describe the relationship between features and actions, but also reflect some contextual information among the features. Especially if there are two feature words in the antecedent set, we call it pair-confidence.  Like in (2), we use the Conf (Xi , Xj?Y) to represent the pair-confidence. It measures the confidence coefficient when the combined feature words Xi and Xj  occur and the action Y occurs too. We use the pair-confidence to describe the contextual information among features in the next section.

B. Contribution For every visual word, we train its contribution to each  action. We use the method of [15] to get these contributions through a trained-SVM model.

In training sets, by following [2], each video sample is converted to a K-dimensional BOW histogram by mapping each local feature to its respective visual word and tallying the     word counts over all the local features. The interesting action is then selected as positive instances while others are negative instances for learning a linear SVM. It means the resulting scoring function has the form like (3) which is the discrimination function of the linear SVM.

( ) ( ), ( )f x H x y bw Hi ii= < > +?                 (3)   H(x) is the histogram of test sample; H(y) denote a serious of support vectors and w denote their weights; b is bias. The test sample has N local features which can be described by the set of X=(xi, ci, H(xi))i, where the xi refers to the 2D+T-position of the local feature, the ci is the type label of the local feature and the H(xi) denotes its value in the histogram.

From (3), we can find that the SVM classifier response is the sum of product between the test and every support histogram. For the linear SVM, because of its linearity, we can separate every visual word from the histogram and exchange the order of the two summation notation. Thus the response of the classifier for the test sample can be rewritten into (4):    ( ) ( ( )) * ( ) K  f x H byw xHi ii jj j  = +? ?                    (4)  ( ) * ( ) K N  f x H b ba x aj j j j i  = + = +? ?                     (5)    Define aj =?wi*Hi(yj) (j=[1,K]) are the contributions to the action corresponding to the feature words from 1 to K. From (5), we can find that the SVM response of the classifier is the sum of N feature words? contributions. By writing the classifier score of the test video as the sum of its N features? visual word weights, we now have a way to associate each local feature word occurrence with a single weight?its contribution to the total classifier score. At this point, we can associate each feature word with a particular value which can reflect its contribution to a certain action.

C. Weight based on the confidence and contribution After getting the confidence and contribution, we can  conclude that each individual local feature has its different importance to related actions. A function is formulated to combine the confidence and contribution together to compute the weights of visual words for each action.

2 2(1 ) exp( )  ( )  2 2(1 ) exp( ), 2  i  ax a y  cW ax a y  x y c  C  + ?  = + ?  ??  (6)   Fig.3. A cuboid around the interest point (pentagram) and the circles with different colors are other interest points with different type labels of visual word. The center interest point is not only decided by itself, but also affected by its neighborhood.

W(Ci) in (6) denotes the weight of i-th visual word; x and y  refer to the confidence and the contribution of the i-th feature word respectively; a is a influence coefficient to balance the confidence and contribution; And the c is a scale factor which decides how much impact the weights would have on the histogram. There are three reasons we chose the (6) to compute the weight. First the weight must be a value whose range should be in [0,1] and their sum must be 1. So the denominator of the equation plays the role of normalization and meets the first condition. Secondly, the equation of the weight should be in proportion to the confidence and contribution. It should also have the capability to decide how much impact the two would have on the weight respectively. Thirdly, the exponential function is easy for computer to calculation. Through the weights of visual words for each action, local features can be increased to have strong discrimination. It can also achieve certain level of denoising as shown in Fig.2.



III.      MULTI-SCALE SPATIAL-TEMPORAL CONTEXT An interest point is not isolated but surrounded by its spatial-  temporal context. If just collecting these isolated and unordered local features, we would lose amount of information in both spatial and temporal domain. In order to represent the context among 2D+T local features, we propose a simple but discriminative local contextual descriptor using a set of co- occurrences pairs of interest points in a local region.

Consider a video sequence is represented as a collection of spatial-temporal local features.  And each interest point which is located at (x,y,t) has an associated type label. There is a finite set of N discrete label classes {c1,c2?..cN}, each of which is represent by its local feature  word. {H(c1), H(c2)?H(cN)} refers to the updated histogram based on work [2] where it just calculates the occurrence number of each visual word. That is to say, each appeared interest point has the same contribution weight 1 to histogram. In our work, we will assign different weights by taking contextual information and weight of visual word into considering.

Fig.4. Example of a local region(cuboid) and its location in the spatial- temporal pattern of visual words.

Since we are interested in the spatial-temporal configuration of these local features, we need to describe the contexts.

Obviously, it has a strong relationship between a local point and its neighborhood. To represent the interaction around the interest point x, we choose a regular local region called cuboid to encode the contextual information as shown in Fig.3. Within a given cuboid around the point x, distance can partly weigh the relationship between x and other points like (7).

( ), ( , )i j i jCon X X X Xd?                          (7)  22 2( )( ) ( ) exp( ( ))( , ) ii k i k  x y t  k i j  y yx x t t X Xd  ? ? ? ?? ?  ? + += (8)    Where Con(Xi ,Xj) evaluates how much impact from point Xj to Xi would have and x? , y? , t?  denote independent spatial- temporal scale parameters. The distance between two points is defined by (8). In addition, weight of visual word and pair- confidence which we calculated in the previous section are also used to calculate the relationship and then we get the arithmetic expression of Con(Xi ,Xk).

( ) ( ) ( ) ( )   ( ) {   ,  , , i  i k i k i k  W if Con X X  conf d else C i k X X X X  = =  ? (9)    In (9), W(Ci) refers to weight of Ci and conf(Xi ,Xk) denotes the pair-confidence of point Xi and Xk. Two conditions are considered: the first case is to describe the point Xi itself that represents appearance of the 2D+T local interest points. The second one is to describe the relationship between co- occurrences pair points Xi and Xk that expresses the spatial- temporal contextual relationship.

Last we accumulate the Con(Xi ,Xk) of all the interest points in the cuboid around the point Xi and get the contribution  Con(Xi ) to histogram as describe in (10). It is clear that when we calculate the histogram, the contribution of each interest point is taken into account which is not the same value 1.

( ) ( ), k Xi  i i kX Con X Con X X  ?? =?                    (10)    In our method, action can be characterized by appearance of local interest points and their spatial-temporal relationships through our descriptor. It has the advantages that the computational complexity of our method is very low and yet it has capability to guarantee a high level discriminability.

Because different actions have different spatial and temporal directions, using the same size of local region may not be suitable for all kinds of actions. For example, the period of action jump is longer than the action of walk and in other words, for action of jump, we need to structure interest points in a cuboid with a larger temporal scale. To capture the distribution of spatial-temporal relationships between local features at different space-time scales, multi-scale local regions are used to generate multiple sets of local context features.

These contextual domains with different shapes and sizes can capture different types of spatial-temporal contextual information as Fig.4 shown.



IV.      EXPERIENCE Two action recognition datasets Weizmann and KTH are  used to evaluate the capabilities of our recognition method and both have got satisfied results. The Weizmann dataset includes videos of 10 different action categories by 9 actors, that are wave1, wave2, run, bend, walk, jump, side, jack, skip and pjump. The KTH dataset has about 2400 videos consisted of 6 different action categories by 25 actors, that are boxing, handwaving, running handclapping, walking and jogging, under four different contexts.

TABLE I.  A COMPARISON OF ONLY CONFIDENCE, ONLY CONTRIBUTION AND COMBINE THEM TO TEST ON KTH  Only Confidence  Only Contribution Combine confidence and contribution  93.83 91 94.67    TABLE II.  THE NUMBER OF ASSOCIATION RULES (CONFIDENCE) AND POSITIVE WEIGHTS TRAINED BY SVM (CONTRIBUTION) ON KTH  Action Confidence Contribution  boxing 8 73 handclapping 12 57 handwaving 7 78  jogging 17 50 running 20 50 walking 11 41     We start our experiment by mining the association rules and training a linear SVM in training set. After this process, we get the weights of feature words based on the confidence and contribution. We then calculate the spatial-temporal context information for each interest point in the test video sequence.

Finally, we use the KNN to classify the action sample which is described by a weighted histogram which contains information of local interest points for their appearance and relationship between co-occurrences pairs.

We test the contribution and confidence separately in the KTH dataset. The test result is reported in the TABLE I. From the table, it has indicated that the performance of confidence is better than the contribution and there is no doubt that combined both of them can get the best result. For the six actions in the KTH dataset, the number of association rules we mined is smaller than the number of positive weights trained by linear SVM as shown in TABLE II. It shows the association rules refer to a stronger relationship between local features and actions, while the weights trained by SVM are more comprehensive. Therefore, by combining the two and giving a larger coefficient (a) to the former, we can achieve the best performance.

TABLE III.  A COMPARISON OF OUR APPROACH AGAINST OTHER PUBLISHED RESULT ON WEIZMANN AND KTH  TABLE IV.  THE CONFUSION MATRICES OF  KTH DATASET    Our classification results outperform or reach the similar to all previous published results on the Weizmann and KTH action recognition datasets. We use a leave-one-out cross- validation strategy for Weizmann. Similar to other papers? setup, we use 16 actors for training and other 9 actors for testing in KTH datasets. A comparison of our approach against best published classification result for the two datasets is shown in TABLE III. The TABLE IV is the confusion matrices of KTH dataset. As it can be seen from the tables, although their best result is a little better on Weizmann, our approach is more compact and produces an improvement of 1.2% over the state-of-the-art method on KTH. Although we can find that the recognition results for ?running? are the poorest, the two actions running and jogging are very similar with the naked eye. It has significantly inspired that our approach which is based on work [2] creates a nearly 15% improvement on KTH.

That is to say, adding spatial-temporal context and information  of word and action relationships to the BOW model has shown its discriminability and robust.



V.      CONCLUSION This paper proposes a method to take the spatial-temporal  context and information of relationship between visual words and actions into consideration. Action clip is described by a weighted histogram which contains the information of local interest points for their appearance and relationship between co-occurrences pairs. The weights of feature words for each action are computed based on the confidence mined from association rules and the contribution learned from trained- SVM. Our algorithm is evaluated on the Weizmann and KTH dataset with significant results.

