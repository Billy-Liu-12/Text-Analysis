MINING FREQUENT CLOSED PATTERNS WITH ITEM CONSTRAINTS IN

Abstract: In order to efficiently filter the useful association rules  through a large number of mined rules, some item constraints that are boolean expresssions are integrated into the associations discovery algorithm. The set of frequent closed patterns uniquely determines the complete set of all frequent patterns, and it can be orders of magnitude smaller than the latter. According to the features of data streams, a new algorithm, call DSCFCI, is proposed for mining frequent closed patterns with item constraints in data streams. The data stream is divided into a set of segments, and a new data structure called DSCFCI-tree is used to store the potential frequent closed patterns with item constraints dynamically. With the arrival of each batch of data, the algorithm builds a corresponding local DSCFCI-tree firstly, then updates and prunes the global DSCFCI-tree effectively to mine the frequent closed patterns with item constraints in the entire data stream. The experiments and analysis show that the algorithm has good performance.

Key words: Data mining; data streams; association rule; frequent  closed itemsets  1.   Introduction  A data stream is an unbounded sequence of data arriving at high speed and changing unceasingly along with the time. Frequent patterns mining in data streams has been studied extensively in recent years, with many algorithms [1-4] proposed and implemented successfully.

However, frequent patterns mining often generates a huge number of frequent itemsets and their  corresponding association rules, including many redundant, useless ones which are difficult to comprehend and manipulate. There is an interesting alternative proposed by Pasquier et al. [5]: mining only frequent closed itemsets and their corresponding rules, which has the same power as association mining but substantially reduces the number of rules to be presented.

Moreover, in practice, users are often interested only in a subset of association rules that contain some specific items. Such rules may be called association rules with item constraints. While item constraints can be applied as a post-processing step, integrating them into the mining algorithm can dramatically reduce the execution time and space complexity, and increase both efficiency and effectiveness of mining.

In this paper, we propose a new algorithm for mining frequent closed patterns with item constraints in data streams. A DSCFCI-tree structure is used to store the potential frequent closed patterns which satisfy the item constraints. The data stream is partitioned into a set of segments. With the arrival of each batch of data, the algorithm builds a corresponding local DSCFCI-tree firstly, then updates and prunes the global DSCFCI-tree effectively to mine the frequent closed patterns in the entire data streams.

The remaining of the paper is organized as follows.

In section 2, the basic concepts of frequent closed patterns mining are introduced. Section 3 presents the algorithm for mining frequent closed itemsets with item constraints in data streams. A performance study of the algorithm is reported in section 4, and we conclude our study in section 5.

2.   Problem Statement  Definition 1 Let be a set of  items. An itemset  },...,,{ 21 mxxxI = X is a non-empty subset of I . A  transaction is a 2-tuple, where tid is a transaction-id and  ),( XtidT = X  an itemset. A data stream  is a set of transactions . The number  of transactions in containing itemset  DS ,...),...,,( 21 NTTT  DS X is called the support of X , denoted as . sup.X  Definition 2 Given a data stream ?a support threshold  and an error parameter  DS s ? . Let  denote  the current length of the stream, i.e., the number of tuples seen so far. An itemset  N  X  is a frequent itemset, if and only if NsX )(sup. ??? . An itemset X  is a potential frequent itemset, if and only if NX ?>sup. .

An itemset X  is an infrequent itemset, if and only if  NX ??sup. .

Definition 3 An itemset X  is a closed itemset if  there exists no itemset Y  such that (1) Y  is a proper superset of X , and (2) every transaction containing X also contains Y . A closed itemset X  is frequent if  NsX )(sup. ??? . A closed itemset X  is potential frequent  if NX ?>sup. .

Definition 4 An item constraint B  is a boolean expression over I . We assume without loss of generality that B  is in disjunctive normal form (DNF).

That is, B  is of the form , where each disjunct  is of the form ,  and each  is either  or  for some .

Here,  means containing item , while  means  not containing item  and can be omitted. An itemset  nBBB ??? ...21 iB imii aaa ??? ...21  jia ix ix? Ixi ?  ix ix ix?  ix X  satisfies B  if there exists a certain  such that . }),...,2,1{( nkk ? XBk ? Definition 5 A DSCFCI-tree is a tree structure  defined below.

(1) It consists of one root labeled as ?null?, a set of  item-prefix subtrees which contain the complete information of the potential frequent closed itemsets satisfying given item constraints as the children of the root, and a potential-frequent-item-header table.

(2) Each node in the item-prefix subtree implies an  itemset represented by the path from the direct child node of the root to the current node. It consists of 5 fields: , nameitem ? portsup , ,  , , where registers which item this node represents,  isfci update nextnode nameitem ?  portsup  registers the support of the itemset implied by the node,  represents whether the itemset is closed, if the value is 1, the itemset is closed.  represents whether the support of the itemset is updated,  links to the next node in the DSCFCI-tree carrying the same item-name, or null if there is none.

isfci  update nextnode  (3) Each entry in the potential- frequent-item-header table consists of 3 fields: , nameitem ?  portsup , and  (a pointer pointing to the first node in the DSCFCI-tree carrying the item-name ).

nextnode  3.   DSCFCI: Algorithm Design and Implementation  3.1.   General Idea  The data stream is partitioned into batches . The size of  (the number of  transactions in ) =  ,...)2,1( =iNi iN  iN || iN ?? ?  ?? ? ?  or ?? ?  ?? ? ? 1n , where  is a positive integer. Here, for the sake of the  description of DSCFCI algorithm, =  n  || iN ?? ?  ?? ? ?  .

When all the transactions for  have arrived, DSCFCI algorithm calls the existing frequent closed patterns algorithm FPclose [6] to mine all the potential frequent closed itemsets (  1N  ? -frequent closed itemsets) from  firstly, then obtains the itemsets satisfying item constraints  1N B  and uses them to create  corresponding DSCFCI-tree.

All the remaining batches  are  processed as follows.

)2( ?iNi  (1) FPclose algorithm is called to mine all the potential frequent closed itemsets from , and the  itemsets satisfying item constraints iN  B  are obtained from them;      (2) The itemsets obtained from step (1) and their subsets satisfying B  are compressed into the local DSCFCI-tree of ; iN  (3) The local DSCFCI-tree of  and the global  DSCFCI-tree of the first  batches are merged into the global  DSCFCI-tree of the first  batches;  iN 1?i  ),...,,( 121 ?iNNN i  (4) The global DSCFCI-tree of the first  batches is pruned and the error of support is guaranteed not to exceed  i  ? . If needed, DSCFCI algorithm could be asked to produce all the frequent closed itemsets satisfying item constraints B  of the first  batches at this time. i  3.2.   Construction of the Local DSCFCI-tree  For the first batch , the local DSCFCI-tree only stores the potential frequent closed itemsets satisfying item constraints  1N  B . But for the remaining batches , in order to test whether there are new  closed itemsets coming up, not only the potential frequent closed itemsets but also their subsets satisfying  )2( ?iNi  B  are stored in the local DSCFCI-tree.

Procedure Build_iDSCFCItree Input: An incoming batch , item constraints iN B , an  error parameter ? , and ( of the first  batches, if , is empty).

listoldf _ listf _ 1?i 1=i listoldf _  Output: The local DSCFCI-tree of , and  (  of the first batches).

iN  listnewf _ listf _ i Method:  1. Scan  once to find all the local potential frequent items (items whose support is no less than  iN  || iN? ) and sort them in support descending order.

The sorted potential frequent item list forms  . listf _ 2. Sort each transaction t  of , according to  , and then insert it into an FP-tree.

iN  listf _ 3. Call FPclose algorithm to mine all ? -frequent  closed itemsets (potential frequent closed itemsets) from the FP-tree.

4. Create an DSCFCI-tree with only the root node, if  ( ) then  sort each potential frequent closed itemset satisfying  1=i B  according to ,and  insert it into the DSCFCI-tree,  else  sort each potential frequent closed itemset and its subsets satisfying  listf _  B  according to ,and insert them into the DSCFCI-tree, and for the final node of each closed itemset in DSCFCI-tree, make its value of field be 1 and the value of field  listf _  isfci portsup be the itemset?s support in the FP-tree.

5. merge and  into .

listoldf _ listf _ listnewf _  3.3.   Incremental update of the Global DSCFCI-tree  Theorem 1 If an itemset X  satisfies the following conditions simultaneously, (1) It is a closed itemset in a certain batch of data stream, (2) it has not been pruned yet, (3) it is frequent in data stream seen so far, then it is a frequent closed itemset in data stream seen so far.

Proof: Suppose X  is a closed itemset in batch and has support . If any proper superset of iN a X  does not exist in other batches, according to condition (3), X  is a frequent closed itemset in data stream seen so  far evidently; otherwise, if Y  is a proper superset of X  and Y  comes up in other batches of data stream  with support , then the support of b X  is at least and it must be larger than the support of ba + Y ,  X is a frequent closed itemset too. Thus, we have the theorem.

When merging two DSCFCI-trees into a new one, we should consider two conditions for each itemset X in the original DSCFCI-trees. (1) If X  is closed in one of the original DSCFCI-trees, X  is still closed after merging and the support of X  in each DSCFCI-tree should be added together. (2) If X  is not closed in both of the DSCFCI-trees, we should compute the closure of X  in each DSCFCI-tree and label them as 1X , 2X  separately. If ??? 21 XX , then 21 XX ?  is a newly produced closed itemset.

Procedure Update_DSCFCItree Input: (the global DSCFCI-tree of  the first  batches), item constraints treeoldDSCFCI ?  1?i B , (the local DSCFCI-tree of batch treeDSCFCI ?      iN ), and  of the first  batches. listf _ i Output:  (the global  DSCFCI-tree of the first  batches).

treenewDSCFCI ?  i Method:  1. If ( ) then  is ,  exit.

1=i treenewDSCFCI ? treeDSCFCI ?  2. Create  with a root node and a potential-frequent-item-header table constructed according to  of the first batches.

treenewDSCFCI ?  listf _ i  3. Scan  (depth-first search).

For each potential frequent closed itemset  treeoldDSCFCI ? X  encountered { 4.    Call procedure FCI to compute the closure of  X  labeled as  in , and gain the support of  , then for the final node of  )(XFCI treeDSCFCI ?  )(XFCI a X  in , make its value of field  be 1.

treeDSCFCI ?  update 5.    Sort X  according to  and insert it  into , update the support of  listf _ treenewDSCFCI ?  X  as . } aXX += sup.sup.

6. For each node ?  in , if  ( treeDSCFCI ?  0. =update? )  then { 7.    Gain itemset X  represented by the path from  the direct child node of the root to ?  in . treeDSCFCI ?  8.    If  ( 1. =isfci? )  then Call procedure FCI to compute the closure of X  in ( ), and gain the support of  .  If closed itemset  treeoldDSCFCI ? )(XFCI  )(XFCI b X does not exist in , sort it according to  and insert it into , update its corresponding final node  treenewDSCFCI ? listf _  treenewDSCFCI ? ?  as  bXport += sup.sup.?  and 1. =isfci? .

9.    else Call procedure FCI to computer the closure of X  in  ( ) and in  ( ) separately, gain their support  and d , label  as  treeoldDSCFCI ? )(XFCI  treeDSCFCI ? )(' XFCI c  )(')( XFCIXFCI ? 'X . If ??'X  and 'X  does not exist in  and treenewDSCFCI ? 'X  satisfies B , sort it according to  and insert it into , update its corresponding final node  listf _ treenewDSCFCI ? ?  as  dcport +=sup.?  and 1. =isfci? .  }  10. Call procedure Prune_DSCFCItree to prune , delete  and .

treenewDSCFCI ?  treeoldDSCFCI ? treeDSCFCI ? Procedure FCI Input: An itemset X , a global or local DSCFCI-tree.

Output: The closure of X Method:  1. Sort X  according to  of DSCFCI-tree. listf _ 2. Suppose that the last item of sorted X  is x , look  up x  in the potential-frequent-item-header table and follow its  pointer to search branches of DSCFCI-tree for node  nextnode ?  such that (1)  1. =isfci? , and (2) itemset Y  represented by the path from the direct child node of the root to ? subsumes X .

3. Among all the itemsets found in step 2, the one containing the least number of items is the closure of X  in the DSCFCI-tree.

Procedure Prune_DSCFCItree Input: Item constraints B ,  (the  global DSCFCI-tree of the first batches).

treeDSCFCI ?  i Output: Pruned DSCFCI-tree.

Method:  1. For each node ?  in (bottom-up search) {  treeDSCFCI ?  2.    1sup.sup. ?= portport ?? 3.    If  ( 0sup. =port? )  then  Delete  ? ,  if the left itemset in the branch does no satisfy B , then delete the branch.    }  Theorem 2 Through calling procedure Prune_DSCFCItree to prune the global DSCFCI-tree of data stream, the estimated support of a closed itemset is      less than the true support by at most N? , where denotes the current length of the stream, i.e., the number of transactions seen so far.

N  Proof: Suppose that there are  batches of data in stream seen so far. For a closed itemset  i X , its support  is reduced by  altogether. According to procedure Prune_DSCFCItree, it is evident that . Combined with the fact that the current length of stream is and  the length of each batch is  k ik ?  N  ?? ?  ?? ? ?  , we infer that  Nii ? ?  ? =??? ?  ?? ??? 1 . Therefore, Nk ?? .

Procedure Print_FCI Input: (the global DSCFCI-tree of the  first  batches), a support threshold , an error parameter  treeDSCFCI ? i s  ? .

Output: A list of frequent closed itemsets along with their  estimated support.

Method:  1. Scan  (bottom-up search), for each node  treeDSCFCI ? ?  2. If ( 1. =isfci?  and )(sup. ?? ?? sport ?? i  ?? ?  ?? ? ?  )  then output the itemset represented by the  path from the direct child node of the root to ? and its support.

3.4.   The Algorithm  Algorithm DSCFCI Input: Data stream , item constraints DS B , a support  threshold , an error parameter s ? .

Output: A list of frequent closed itemsets satisfying B .

Method:  1. Divide data stream  into batches  with =  DS  ,...)2,1( =iNi || iN ?? ?  ?? ? ?  or ?? ?  ?? ? ? 1n ,  where  is a positive integer.  n   2. For each batch  { iN 3.    Call procedure Build_iDSCFCItree to construct  the local DSCFCI-tree of . iN 4.    Call procedure Update_DSCFCItree to  construct the global DSCFCI-tree of the first batch.

i  5.    Call procedure Prune_DSCFCItree to prune the global DSCFCI-tree.

6.    If needed, call procedure Print_FCI to produce a list of frequent closed itemsets satisfying B . }  Notice: When the length of each batch is ?? ?  ?? ? ? 1n , in  procedure Prune_DSCFCItree, the value of field portsup  of each node is reduced by  instead of 1. n  4.   Experimental Evaluation  4.1.   Test Environment and Datasets  Our algorithm was written in VC++ 6.0. All of our experiments were performed on a PC using a 1.7GHz Pentium IV processor, 384MB of RAM. The operating system in use was Windows XP.

Three separate data sets (T15I10D1000k, T15I7D1000k, T7I4D1000k) were generated by the IBM synthetic data generator. Here, |T| represented the average length of transactions, |I| represented the average length of potential frequent itemsets, and |D| represented the total number of transactions in data sets. There were 1K distinct items in each data sets, and the default value for all other parameters of the synthetic data generator were used.

The streams were broken into batches of size 50,000 transactions. The support threshold  was varied (as to be described below) and the error parameter  s ?  was set  to . Item constraints , 1, 2, 3, 4 represented four items which were selected randomly from the 30 items came up first.

s1.0 )43()21( ???=B  4.2.   Experimental Results      tim e  (s )      1 3 5 7 9 11 13 15 17       S=0.001  25 S=0.0015 S=0.002  number of data stream segments  Figure 1. The execution time of  T15I7D1000k                    In figure 1, the y axis represents the time spent in  reading in a batch of stream, constructing its local DSCFCI-tree, updating the global DSCFCI-tree, and producing frequent closed itemsets satisfying item constraints B . The y axis in figure 2 represents the memory capacity used for storing the global DSCFCI-tree.

As can be seen from the two figures, when the value of support  is fixed, the time and space requirements of the algorithm grow as the stream progresses, but tend to stabilize. For example, the memory required by the global DSCFCI-tree with support 0.0015 increases relatively quickly as the first seven batches of stream arrive, and tends to stabilize at roughly 3.8MB with small bumps. This is because the number of potential frequent closed itemsets satisfying item constraints goes up relatively quickly at first, and becomes relatively steady later through introducing pruning technology. The stability results are quite nice as they provide evidence that the algorithm can handle long data streams. As the value of support  decreases, the execution time in figure 1 and the memory required by DSCFCI-tree in figure 2 increases quickly. This is because there are more (potential) frequent closed itemsets satisfying item constraints with slower support.

s  s  Figure 1 and Figure 2 show the experimental results of data sets T15I7D1000k. The experimental results of others two data sets are similar to these figures.

1 3 15 17          1 3 5 7 9 11 13 15 17  number of data stream segments  s t o r a g e  c a p a c i t y    Figure 3 shows the average time spent in updating  the global DSCFCI-tree once with three different item constraints (constraint 0: no constraints, constraint 1:  , constraint 2: ). We can draw a conclusion that with the reinforcement of the constraints? degree, the number of frequent closed itemsets satisfying corresponding item constraints decreases, and the time spent in updating the global DSCFCI-tree decreases too. Moreover, the space requirement of the global DSCFCI-tree goes down simultaneously. Therefore, integrating item constraints into the mining algorithm can reduce the execution time and space complexity of the algorithm.

)43()21( ??? 4321 ???  5.   Conclusions  In this paper, we propose an efficient algorithm for mining frequent closed itemsets in data streams. The contributions of our study include: (1) proposing a novel data structure, DSCFCI-tree, for storing (potential) frequent closed itemsets, and developing a new method for incremental updating DSCFCI-tree efficiently, (2) applying a pruning technique for efficient pruning global DSCFCI-tree to reduce the space requirement of the DSCFCI-tree and the time spent in traversing the DSCFCI-tree dramatically, (3) integrating item constraints into the mining algorithm and thus reducing further the execution time and space complexity of the        40 constraint 0  S=0.001  Ti m  e( s)  S=0.0015 constraint 1  constraint 2 S=0.002  5 7 9 11 13 Number of data stream segments  Figure 3.  comparison of execution time with different constraints(s=0.0015)  Figure 2.The memory usage of DSCFCI-tree      algorithm.

Our performance study shows that DSCFCI  algorithm is efficient and effective.

