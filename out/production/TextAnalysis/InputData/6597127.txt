Approximate Incremental Big-Data Harmonization

Abstract?The needs of ?big data analytics? increasingly require IT organizations to ingest, process, and extract business insights from ever larger volumes of data that arrive far more rapidly than before, as well as from new sources such as social media, mobile devices, and sensors. However, in order to extract insights from diverse information feeds from multiple, often unrelated sources, these first need to be correlated or harmonized to a common level of granularity. We formally define this commonly arising data harmonization problem. We show how to correlate disparate data sources using map-reduce, but in an approximate and/or incremental manner as often required in practice. We motivate our techniques through a real-life enterprise data-harmonization case study for which we describe our performance results on big-data technologies; namely, Map Reduce, Hadoop and PIG.

Keywords-ETL-MR, Approximate-Join, Harmonization, In- cremental ETL, Map-Reduce, BigData

I. INTRODUCTION  Interest in ?big data analytics? is growing rapidly within many enterprise IT organizations as they are being called upon to ingest, process, and extract business insights from ever larger volumes of data, which is arriving far more rapidly than before, and from many new sources such as social media, mobile devices, and sensors. In recent years a new collection of data management tools have emerged from the web companies; in particular from Google, Yahoo, and Facebook. The Hadoop open-source platform comprising of the distributed file system (HDFS), the HBase ?no-SQL? database, and a distributed programming framework based on Google?s MapReduce paradigm has come to represent the larger set of similar tools, which are increasingly being referred to as ?big-data technologies?.

As has been demonstrated by many of the web companies, the ingestion, storage, and deep analytical processing of large-scale data sets using statistical machine learning and data mining techniques are often more efficiently performed using these new big-data technologies as compared to tra- ditional business intelligence platforms built on relational databases.

However, in order to extract valuable business insights from such diverse information feeds using deep analytics, data from multiple, often unrelated sources, first needs to be correlated and reduced to a common level of granularity.

Further, the many valuable insights result only when data  from ?new?, external ?big data? sources is fused with internal data from within the enterprise [1]. Moreover, the latter currently resides mainly in traditional relational databases.

As a result, business intelligence (BI) platforms based on relational databases are being increasingly challenged by the need to ingest and process vast volumes of data from new ?big data? sources together with more traditional data from inside the enterprise. Consequently, BI is becoming dom- inated by Extraction-Transformation-Load (ETL) processes that consume about 70% of BI resources and effort [2], [3] when engineered using a traditional BI technology stack.

On the other hand, if we accept that deep analytical tasks that follow ETL processing are probably best performed using big-data technology, it makes sense to apply the same big-data technology to the ETL process as well. In this paper we describe our practical experience of applying big-data technology to some real-life enterprise ETL scenarios by exploiting recent research results published in the literature.

We motivate the need for approximate and incremental correlation in many real-life scenarios involving disparate data sources. We formally describe this as the data harmo- nization problem. We describe a technique for approximately correlating keys between different data sources using ma- chine learning, resulting in what we call soft-key correlation.

The MapReduce (MR) paradigm is well-suited for ETL jobs [4]. In this paper we describe how to implement ETL on map-reduce and optimize the application of multiple constituent operations in a sequence. Further, we show how to incrementally incorporate data-sets arriving at disparate times, and so produce more ?real-time? business intelligence.

The paper is organized as follows: We begin with an overview of enterprise BI, and formally describe the data harmonization problem in Section II. In Section III we describe a real-life case study where data harmonization was required. In Sections IV and V we describe our technique to address the data harmonization problem: Using map-reduce for ETL and real-time data harmonization, and machine learning for correlation of incongruous join-keys. Then in Section VI we describe how these techniques are combined to solve the data harmonization problem in the real-life example, for which we also provide implementation details and experimental results of our solution on a Hadoop cluster.

Finally we conclude in Section VIII, after a brief description  2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.24     of related research work in the area of Incremental ETL and Soft-Key-Correlation in Section VII.



II. OVERVIEW A. Data Harmonization (DH)  The typical flow of data in enterprise BI is that of online analytical processing (OLAP), which begins with the extraction of data from source, i.e., transactional systems; into a staging area. Once all relevant transactions for a specified time period have been recorded in their respec- tive source systems and extracted into the staging area, data is transformed into a Star or Snowflake schema and loaded into the enterprise data-warehouse through an ETL ( Extraction-Transformation-Load) process. ETL jobs extract data, cleanse it, transform it to fit in the target schema (de- normalization and aggregation) and then load data into the warehouse. A key assumption in such ETL processes is that dimensional data is congruous and can be correlated with the help of master data, e.g., product-code to product-name mapping, which can be used as a dimension table in a star or snowflakes schema.

Data obtained from disparate systems is often found to have dimensions with different schemes of representation because of which it is difficult to correlate and fuse such data-sources. In some cases the different schemes of repre- sentation may refer to different units of measurement, e.g., sales being reported on daily basis which are to be fused with weekly inventory status or with survey-data reported fortnightly. In another case the schemes of representation may refer to different levels of granularity, e.g., ?product category? vs ?product-name?. Even when data elements from different sources refer to the same dimension, they still often use different encoding schemes. We call such data as ?incongruous?: In the case of internal data sources, such incongruity arises due to the absence of common enter- prise reference data. However, when data sources include external feeds such as social media [5], incongruous data is inevitable. The problem of incompatible data encoding is found to be widespread when trying to fuse survey-data and social media data with data from transactional systems: Both consumers as well as field surveyors tend to refer to product-names or store-names with their popular names instead of their corresponding ?Stock Keeping Unit? (SKU) names or ?Store-Code? as used in the transaction systems.

It is therefore imperative to transform the dimensions and measures to a common scheme of representation before data from disparate sources can be fused, i.e. before a join operation is possible. We define Data Harmonization as the process of joining incongruous data-sets, by means of transformation into a common scheme of representation before the join.

We achieve dimensional transformation of incongruous data, arising due to disparate encoding schemes by first cre- ating a mapping-file that forms the basis for data correlation.

Figure 1. CPG Case Study Schema Details.

As shown later in this paper, such a mapping relation can sometimes be derived from the data itself using supervised machine learning; we call this Soft-Key Correlation. Once such a mapping is available, we transform data to a common scheme of representation via basic transformation constructs, and then join these transformed data-sets to create harmo- nized data through ETL on MapReduce.



III. CPG CASE STUDY  In this section we present a real-life enterprise case study from our experience. A consumer-packaged-goods company receives point-of-sale data from various retailers as well as retail metrics collected through manually conducted sample surveys. Figure 1 shows the schema of the four input sources and of the harmonized data, with dimension columns marked in bold, columns underlined contain both dimension and measure information. All these data-sets are described below.

? Point of Sale Data (POS): This data is collected at the checkout counter of retail stores, consolidated for one week and is collected from all the stores that the CPG company supplies its products to. It contains SKU (Stock Keeping Unit, a code for each unique product for sale in a store), Store#, and #UnitsSold for four weeks of a month.

? Survey Data - Share of Shelf (SOS): This and the next two data sets are obtained from a fleet of surveyors who visit stores to conduct surveys. SOS data indicates what share of the total number of shelves in a store, or of a section of the store, is given to each product or product category.

? Survey Data - Out of Stock (OOS): Out of Stock data indicates whether or not an SKU was available for sale in a store on the date of a surveyor?s store visit.

? Survey Data - Miscellaneous (MISC): This data con- tains a set of survey questions such as, ?is visible from     main entrance?? along with their answers. The question- answer pairs are associated with product-names or product categories as observed in given store on the date of a surveyor?s store visit.

These disparate data sources were to be joined into one harmonized format so that the key performance indicators (such as units sold or total sales) are stored at a weekly level together with possible lead measures that might impact sale, such as SOS or OOS; it is required to harmonize all the above data-sets into one schema. Statistical analysis on such historical data can then possibly discover temporal correlations, such as the probability with which a drop in share-of-shelf (SOS) results in a drop in sales, by what factor, and after how much time lag.

Since survey-data is manually entered and comes from multiple provider agencies, product and category names rarely match between data sources, and can also vary over time even from the same source. In actual practice, the scenario is even more complex when it is required to interpolate between sparsely collected data-points, e.g., if the survey-data is not collected from one of the stores, it can required to extrapolate across ?similar? stores; or, if the social-media data could not be collected from some of the regions, requiring us to extrapolate across similarly behaving regions.



IV. INCREMENTAL DATA HARMONIZATION  We first describe data harmonization in batch-mode, as- suming the data from disparate systems has already arrived; and then motivate need for Incremental Data Harmonization, followed by a detailed description of how the data can be harmonized in incremental mode.

Once data from disparate sources is available, the data can be harmonized following a three phase process. The phases are: Annotation(A), Transformation(T), and Join(J); we also refer to this as the ?ATJ? process. In the annotation phase, we map the columns of the input data with the column speci- fications given in an ?annotation-file? (pre-created). In the Transformation step, multiple transformations are applied on all input data-sets so as to bring them all into a com- mon scheme of representation. The transformations include standard ETL processing such as ?cleansing and validation?, ?schema alteration (pivot, un-pivot)?, and ?dimension? and ?measure? transformations.

?Dimension transformation?, performed through a left outer-join with a mapping file, converts the input dimension values to target scheme of representation: The mapping files are created using soft-key-correlation, which leverages machine-learning to create these mappings, and is described in Section V. Such transformations need not be one-to-one, and are often many-one (e.g. SKU to product-category) or many-one (e.g. approximating selected metrics across mul- tiple ?similar? stores). ?Measure transformation? corresponds to addition of newer derived columns in the data, such as  ?weekly gain/loss in sales?, which are used for predictive modelling. Once all transformations are complete, a full- outer join is performed on the common keys of all the datasets, to obtain the harmonized data.

In most enterprise operations, input data for harmoniza- tion does not arrive at the same time even for the same time-period. Therefore data needs to be stored in a staging area and can be harmonized after all the data for a specified time-period has arrived. This delay could be avoided if harmonization could be performed incrementally, i.e., by harmonizing data as soon as it arrives. However, there are challenges in doing this; we list those challenges below and then describe our approach for incremental data harmoniza- tion.

If data from only one source arrives for a time-period, we could harmonize and store it in the target database. Later, when data from another source arrives, we might delete the incompletely harmonized data and harmonize both the data-sets all over again, join them, and then store in the target-database. This approach poses problems: For a large enterprise typically all the jobs are pre-scheduled and all 24 hours of a day are pre-booked. If all ETL jobs were to run many times for the same time period, it may become difficult to manage the ETL schedules given fixed computing resources. Secondly, data deletion from the warehouse is not usually recommended by administrators, except for archival purposes, as it requires re-indexing of large volumes of data.

We suggest that instead of re-joining the entire data, both new and old, newly arrived data is only transformed (i.e., not joined). For merging the transformed new data with previously harmonized data, a subset of relevant data should be scanned and efficiently retrieved from the previsouly harmonized data. The transformed new data should then be joined with this subset of old harmonized data, and inserted afresh in the target database, with a suitably higher version number: This incremental harmonization is carried out in five phases 1) Annotation, 2) Transformation, 3) Scan, 4) Join, and 5) Index. This scheme is implemented using map-reduce, resulting in what we call the ATSJI procedure.

A high level data flow diagram for incremental data har- monization (ATSJI) is depicted in Figure 2, the numbers alongside the arrows indicate data flow sequence. Only one of the candidate data-sets for harmonization flows through a particular stage at any point of time. However, many incremental new data-sets can be processed in parallel in a pipelined manner. Also, an instance of the framework itself works in a ditributed manner on a cluster of machines.

Annotation: A set of annotation files is created for the  data harmonization task: One file for each of the input data containing its column specifications, and one file containing column specifications of the harmonized data. These files are used by subsequent stages of the procedure.

Transformation: The input data is transformed into the output schema assigning NULL to the measures that are     Figure 2. Overview of ATSJI, the incremental data harmonization procedure.

not present in the current data-set. For example, if there are a total of 10 measures in the output schema and only 2 of the measures are reported in the current data-set, even then all 10 measures are included in the transformed data, marking the remaining measures as NULL.

We perform dimension transformations through a left outer join with the corresponding mapping file(s). These joins convert the input data from one tuple to many tuples, many to one or occasionally a one-one transformation.. In addition, the many-to-one transformation requires aggrega- tion, and one-to-many transformation requires apportioning of corresponding measure columns, for example, daily sales data needs to be aggregated to weekly sales data, and share-of-shelf data needs to be apportioned to individual products. Another type of transformation involves pivot or un-pivot operation, which causes a reduction or increase in the number of tuples respectively. After transformation, the data flows to both scan and join phases.

Scan: We aim to process chunks of data as they arrive,  rather than wait until all the data is available. As a result, some incoming data records may need to be joined with records processed in earlier iterations. In our case-study for example, data from some stores might arrive after the rest; or share-of-shelf data might arrive later than point-of-sale data, etc. Therefore, in ATSJI, once we transform newly incoming data we need to scan previously harmonized data for records that may need to be joined with the freshly transformed data. Such a scan is performed using a ?Group Range Index? table as depicted in Figure 3, which is itself created / updated in previous iterations of ATSJI as we describe later in this section; for the moment assume we  Figure 3. Group Range Index in ATSJI  have such a table. Further, this is a relatively small table and can therefore be easily loaded into each mapper.

For scanning and indexing we require that harmonized data is grouped according to k out of its N dimensions. In our case study for example, the k(= 2) dimensions might be ?week? and ?region?. The assumption is that data will arrive in sets of chunks, each for a given week and set of regions. Note, we do not assume that all data, even of the same type, such as point-of-sale, for a particular week or region arrives together. However, the choice of k dimensions should be such that it is highly likely that each incoming data-set contains relatively few chunks if grouped in this manner. As depicted in ?T2? in Figure 3, data is grouped on first two dimensions ?D1? and ?D2?. The ?Group Range Index?, shown as T3 in the figure, stores the minimum (e.g. w7,xyz,#322,Delhi) and maximum (w7,xyz,#724,York) values of the primary key in the harmonized data, for each combination of the k grouping keys; so that after a lookup of maximum and minimum values of the primary key, it becomes possible to do range scan on the primary key of harmonized data.

In the scan stage of ATSJI, map tasks lookup the Group Range Index in parallel for each group present (e.g.

?w7 xyz?) in the freshly transformed incoming data, to find the range of keys in the previously harmonized data that are guaranteed to contain records that share key-values with the incoming data set. (Note: these ranges may contain more data than is needed; for example the incoming data may be only for a few stores, yet the range scan for a week and region will return data for all stores in the matching region and week.) Multiple map tasks retrieve the required subsets     Figure 4. Keyword Matching Overview  of previously harmonized data; using at worst as many range scans as the number of groups in an incoming data-set, and at best no scans: If no incoming group has a matching record in the Group Range Index, this indicates that the fresh data set is completely ?new?; i.e., no data for any incoming week- region pair has previously been harmonized.

Join: Next both the freshly transformed as well as ?match- ing? subset of previously harmonized data are inputs to the join stage. A full outer join is performed between all the incoming data and the matching subset of harmonized data.

Note that the values of any measure columns already present in the matching subset are replaced with new measures as computed by the join.

Most importantly, the output of the join stage is always inserted into HBase data store. Thus, even if partially har- monized data was previously present, fresh data is inserted against the same key, which is easily possible since HBase merely maintains a new version of such records and no up- dates are performed. It is this property that makes HBase an efficient choice for such incremental harmonization. On the contrary in traditional relational databases updates are costly, and which is why ?one never updates a data warehouse?.

Index: After an incoming dataset is harmonized and stored in the warehouse, the ?Group Range Index? is updated using an additional map-reduce phase. As can be explained using Figure 3, starting with input data in ?T1?, we group by k dimensions as in ?T2? (Here k < total number of dimensions in the data), We then find minimum and maximum values of the primary keys for each such group (which requires map-reduce) to create ?T3?. Data of this table ?T3? is then inserted or updated in the ?Group Range Index? table.



V. SOFT-KEY CORRELATION  Recall the challenge of matching keys when data is repre- sented differently across data sources, such as two different codes ?Lt. Coffee Can 100?, and ?Light Coffee Can 100  gms? both referring to the same SKU (stock-keeping-unit).

We refer to this problem as Soft-Key Correlation, which is similar to the approximate join problem [6]. In contrast to approximate joins, where UDFs (user defined functions) are used to compare entity-names during execution of join, we first generate the mapping files which are then used by the ATSJI framework for data harmonization.

As shown in Figure 4 we start with a set of manually la- beled pairs taken from data sources of a given harmonization problem, to serve as a training set for supervised learning.

Generate Features: Mismatch between entity-names may arise due to typographical errors, abbreviations, different order of words, partial words, omission of unimportant words, and combinations of these possibilities, e.g., {?Lt.

Coffee Can 100?, ?Light Coffee Can 100 gms?}, {?D Cofee Pkt 500?, ?Dark Coffee Packet 500 gms?}. We submit that there is no single similarity metric that applies to all the problems, and neither is it easy for a developer to judge which metrics or thresholds to use as these are data- dependent. For example, a metric like ?Jaccard similarity? is helpful when the position of words in a string is irrelevant for matching, ?JaroWinkler? takes care of typographical errors, and ?TF-IDF? based metric assigns weights to words according to their importance. We used similarity scores of ?overlap coefficient?, ?Jaccard similarity?, ?cosine similarity?, ?soft TFIDF?, ?Monge Elkan?, ?Jaro Winkler?, ?Soundex?, and other metrics mentioned in [7] as features.

Feature Selection: Since some of the features are more im- portant than others in a given problem-set, we drop features that correlate poorly with the labels in training set. For this, we calculate statistical correlation between labels and the features. Subsequently, absolute values of these scores are normalized and are referred as feature weights. We traverse these features in descending order of their weights, and drop all un-traversed features if the difference between weights of two consecutive features is above a prior-threshold(t1), or if the absolute value of a feature-weight is below another prior-threshold(t2).

Classification and Mapping File Generation: We use a Support Vector Machine (SVM) to learn a discriminative statistical model, from the training set against the selected features. This model is then used for classification of all the pairs of strings into ?matching? or ?non-matching? categories.

These mappings are stored in a file, which has been referred to earlier in Section II-A, i.e., the mapping-file.



VI. IMPLEMENTATION AND RESULTS  Data from the CPG case study presented in section III was harmonised via data transformations implemented in map reduce as follows: POS: SKU names present in the POS data need to be cleansed and then #UnitsSold for various weeks of a month needed to be un-pivoted. Transformation sequence is :     ?cleanse? and ?un-pivot?. When executed using PIG, these were merged to one map-reduce job automatically.

SOS: Product names reported in this dataset do not match with SKUs, sometimes have typing mistakes, and some of these correspond to shelf names and not the product names. All the shelf names need to be transformed to SKUs.

We took transformation sequence as : ?Many-to-One? ( for survey date to week number of the year) and ?One-to-Many? (for Shelf name to SKU). Note: If there are n records in raw SOS data, ?One-to-Many? operation explodes the data k times(= n.k) and ?Many-to-One? operation reduces by a factor of m (= n/m). It is therefore preferred to perform the ?Many-to-One? prior to ?One-to-Many?, so that subsequent map-reduce jobs deal with smaller amount of data. We performed two experiments for SOS transformation, one with ?Many-to-One? first and second with ?One-to-Many? first. In both of these cases PIG merged the two operations into one map-reduce job. However the communication cost of these experiments can be calculated as n + (n/m) and n+ (n.k) respectively. This was corroborated by the exper- iments performed. It took 78 seconds for first experiment and 274 second for the second experiment for one month of data where-in m = 1 and k = 12.

OOS: SKU names need to be transformed to the same scheme as used in POS data and survey date needs to be transformed to week number of the year.

MISC: These are miscellaneous inputs as entered by sur- vey staff. (i) Colloquial product-names are used just as in the case of SOS so a similar soft-key-correlation-based transformation to a canonical form is required. (ii) A new pivot column needs to be created for every question in the ?Questions? column. For example, after pivoting an entry such as ?is visible from main entrance?? in the ?Questions? column becomes a new column taking values ?yes? or ?no?.

Both these operations can be implemented in a single map- reduce phase if they are applied in the optimal sequence, i.e., ?pivot?, followed by dimension transformation.

We have used the Hadoop platform and PIG language to implement Incremental ETL-MR. We augmented the native PIG library through user defined functions and wrote PIG script templates for Data Harmonization modeling constructs. The actual PIG scripts that implement our tech- nique are generated from these generic templates and a data harmonization specification particular to a set of data sources, captured as an XML model.

Incoming data is first loaded into HDFS, and PIG scripts for the ATSJI framework is invoked to harmonize the data.

The harmonized data as well as the Group Range Index is maintained in HBase. Output schema details and mapping- relation files are stored in distributed cache of Hadoop1 for local access to various computing nodes in the cluster.

HBase maintains the data in {Key, Value, Version} format 1http://developer.yahoo.com/hadoop/tutorial/module5.html  Figure 5. Batch (ATJ) vs Incremental (ATSJI)  therefore we only need to insert to HBase, even if a key is already present, in which case the inserted record is stored as a new version of the old data. As a result we never need to delete any data from HBase. Finally, since HBase is a distributed database we were able to have many nodes writing / reading in parallel, which is normal in map-reduce, but also came in useful for the ?scan? stage of ATSJI.

A. Performance Results  For the purpose of this paper, we generated suitably con- strained but random data for the CPG case-study described in section III. The sizes of different data sets used are as follows: POS data had 0.42 GB for 4.16 million rows, SOS data had 0.68 GB for 7.5 million rows, OOS data had 0.76 GB for 7.5 million rows, and LIS data had 1.3 GB for 11.2 million rows. We used a Hadoop cluster of five machines with ?Intel Xeon 1.87GHz? CPU(4), 4 Cores and 32GB RAM each. All these nodes were mounted in the same rack and connected through a single switch.

To study the additional cost of repeated harmonizations and range scans when harmonization is done incrementally using ATSJI, harmonization was performed for six months, each time in four increments, assuming that different data sources (e.g. POS, SOS, etc.) arrive at different times in the same month. The k dimensions as required by ATSJI were chosen to be time and store#, the rationale being that data from each source itself often arrives in bits and pieces, for a few regions over a given time period.

Figure 5 demonstrates two scenarios: First when the data is harmonized in batch mode, marked as ATJ. In this case harmonized data can be generated only after all the required data-sets (POS, SOS, OOS and MISC) have arrived. The second scenario is that of Incremental ETL-MR where data is harmonized as soon as it arrives. Thus, if data arrives in four different batches, it is harmonized four times. Even then, as shown in Figure 5, the cumulative cost of four incremental ATSJI harmonizations is only 2.5 times that of the single batch ATJ procedure (i.e., not four times).

Additionally, the time required to harmonize each increment is always less than the time required when using the ATJ batch process, even with the overhead of the scan stage, and can sometimes be as much as 30 - 40 % lower (as in the case of POS, OOS, and SOS data).

B. Soft-Key Correlation Results  To analyze effectiveness of feature selection, and SVM classifier proposed in section V we considered two data sources: Point-of-Sale data (POS - S1) and Share-of-Shelf data (SOS - S2). In S1 product-name dimension is repre- sented in terms of ?Stock Keeping Unit Names? ( SKU - P1), whereas in S2 the same dimension is represented as product category name (P2). (Note: Here Pi denotes the set of unique elements in the product-name dimension for data source Si).

We needed to generate a mapping file between S1.P1 and S2.P2, so as to transform S1 and S2 to a common scheme of representation. There is many-to-one relationship between P1 and P2. The strings used as SKU name can typically be broken into many a sub-strings which allude to ?product category name?, ?size?, ?quantity in the pack?, ?weights?, etc, i.e., the product-category names are usually included in SKU name, but often abbreviated through first few characters of product category name, or using phonetic codes generated by removing vowels, etc. This clearly involved cases where mapping the elements of P1 to that of P2 required multiple string similarity metrics and no single metric could map all the elements with high accuracy ( see Figure 7). A random subset from the cross product of P1 and P2 was used as training-set and the remaining pairs as test-set, to build classification model and to test its accuracy, respectively.

Here, |P1| = 460 and |P2| = 23, where |Pi| is the cardinality of set Pi.

Two of the 13 features generated were found to have weaker correlation with the output-labels (?matching? and ?non-matching?) than all the other features. These were discovered by using t1 = 0.3 (prior-threshold for difference) and t2 = 0.1 (prior-threshold for weight), see Figure 6-b.

Further, this was corroborated by the results shown in Figure 6-a, where it can be observed that these two features could not improve the F1 score.

Figure 7 shows the maximum F1 score taken from precision-recall curve for some of the important metrics and that of combined metric for the 11 chosen features. It can be observed that the combined metric improves the F1 score of the best performing single metric by approximately 35%.

Further, the Gaussian-kernel-based SVM classifier built on all the chosen 11 features, performs much better than SVM without kernel. To summarize, soft key correlation was able to match product-category names with 91% accuracy.



VII. RELATED WORK  Incremental ETL has been attempted, but only for refer- ence data, by keeping a log of all changes performed [8]  Figure 6. a) Comparison of features weights, b) F1 score Vs. number of features used (adding features, one at a time, in decreasing order of their weight)  Figure 7. Maximum F1 scores for different metrics  and then updating those in the warehouse. Unlike ATSJI framework, most of the real-time ETL solutions [9] do not deal with a situation where new data needs to be re- joined with already transformed data, they prescribe only the insertion of new transaction data into the warehouse.

Incremental ETL problem described in this paper is very different from the incremental map-reduce performed on data-set that is pushed again onto a map-reduce based system after some amendments or modifications as described in [10]. Here, by incremental ETL we indicate a process of on- arrival data harmonization, which gets called as soon as data from a source arrives, and performs the transformation of the newly received data only, instead of repeated transformation of previously received data along with the new data.

Approaches similar to our ?scan? stage of ATSJI have been successfully used when extraction of data is performed from a warehouse to load into data-marts [11]. We have implemented ATSJI including the ?scan? stage for a special class of ETL on HBase and have also reported its overhead as compared to a purely batch ETL. However, these do not include support for the full range of ETL transformation     operations as we do.

There are two lines of work similar to our work on  soft-key correlation, one focusing on providing approximate join (theta join) in databases, and the other focusing on learning an adaptive similarity metric suitable for string matching, applicable across domains. Gravano et. al in [6] describe efficient approximate join by augmenting the database with positional q-grams tables and then using simple SQL queries, but the approach does not provide adaptive metric capability.

Adaptively combining distance metrics has been proposed for the name-matching task and duplicate record detection in [12], [13]. [14] proposes to learn rules for similar objects identification, by using similarity scores of attributes as an- tecedents of the rules. In [15], a fuzzy match similarity func- tion has been proposed to find k closest records to a given record combining the ideas of edit distance, cosine similarity and IDF based weights. Approximate key-matching features are also available in data integration packages such as the open-source ?Talend? 2 tool, where it goes by the name ?fuzzy match? and uses single similarity metric with user defined threshold for matching. Our approach, however, combines soft-key correlation with map-reduce-based data harmonization, which is novel.



VIII. CONCLUSIONS  We have formally defined Data Harmonization, a com- monly arising business scenario where data from disparate and incongruous sources need to be correlated. We have then shown how data harmonization can be accomplished as a formally defined ETL process using the map-reduce programming paradigm and approximate ?soft-key? corre- lation. We have further shown how this can be achieved incrementally, minimising staging delays as data arrives in bits and pieces.

We have described performance results of a motivating example and case study. We also analyzed related work in this domain and have submitted that our approach to achieve incremental outputs is novel, especially in the context of enterprise BI and ETL procedures that efficiently exploit map-reduce and related BigData tools such as HDFS and HBase.

