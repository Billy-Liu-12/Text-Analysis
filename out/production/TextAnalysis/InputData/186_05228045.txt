Improved Classification Association Rule Mining

Abstract- Classification aims to define an abstract model of a set of classes, called classifier, which is built from a set of labeled data, the training set. However, in large or correlated data sets, association rule mining may yield huge rule sets. Hence several pruning techniques have been proposed to select a small subset of high-quality rules. Since the availability of a "rich" rule set may improve the accuracy of the classifier, we argue that rule pruning should be reduced to a minimum. A small subset of high-quality rules is first considered. When this set is not able to classify the data, a larger rule set is exploited.

This second set includes rules usually discarded by previous approaches. To cope with the need of mining large rule sets and to efficiently use them for classification, a compact form is proposed to represent a complete rule set in a space-efficient way and without information loss.

An extensive experimental evaluation on real and synthetic data sets shows that improves the classification accuracy with respect to previous approaches.

Keywords- Datamining, associative classification, association rules, condensed representations

I. INTRODUCTION  Classification aims to define an abstract model of a set of limiting rule pruning has already been discussed in classes, called classifier, which is built from a set of labeled data, the training set. The classifier is then used to appropriately classify new data for which the class label is unknown.

Different approaches have been proposed to build accurate classifiers, for example, naive Bayes classification [I] , decision trees [2], and SVMs [3]. Recently, association rules [2] have become a valuable tool for classification purposes.

(for example, CAEP [2], CMAR [I] , CBA [2], and ADT [3]).

B.Eswara Reddy Associate Professor  J.N.T.University Anantapur  Andhra Pradesh, India eswarcsejntu@gmail.com  In associative classification, the rule consequent is a class label, and the classifier is a set of association rules. Since association rules represent the correlation among values of different attributes simultaneously, in general, associative classifiers yield better accuracy than decision trees and rule- based classifiers. The generation of an associative classifier consists of two steps. First, classification rules are extracted from the training data. Then, pruning techniques are applied to select a small subset of high-quality rules and build an accurate model of training data. Usually, a large rule set is mined to allow a wide selection of rules and the generation of accurate classifiers. However, in large or correlated data sets, rule mining may yield a huge number of classification rules.

Rule extraction becomes difficult (or at least time consuming), and it becomes hard to optimally exploit the generated rules.

Hence, pruning techniques, in particular, support-based pruning, are exploited to reduce the complexity of the extraction task. Most pruning techniques may go too far, by discarding also useful knowledge together with low-quality rules. Since the availability of a large rule model may improve the accuracy of the classifier pruning should be limited to a minimum. The opportunity of limiting rule pruning has already been discussed in [3] and [I] . Liu et aI. [1] proposed multiple support thresh-olds to limit the number of extracted rules for frequent classes, without hiding infrequent classes.

CMAR [3] proposed a modified database coverage technique, which is the first step toward the reduction of excessive pruning. To address both an excessive rule set size and over- pruning, we propose a new associative classifier that relies on a lazy pruning approach coupled with a compact representation of the rule set We named our classifier e, which stands for Live and Let Live! (that is, pruning only takes place when strictly necessary). The lazy pruning technique performs a reduced amount of pruning by eliminating only "harmful rules," that is, rules that only misclassify training data.

lAMA 2009    GENERATING CLASSIFICATION ASSOCIATION RULES USINGTFPC  II.

selected correctly, then the existence of a rule 4"~ cl should make it unnecessary to consider any other rules whose antecedent is a superset of X. In practice, however, we may still find a rule r~ c2, say, where Y is a superset of X, which has higher confidence and to which we would wish to give higher precedence. It remains possible, also, that there will be a further rule ,?~ c1, where Z is a superset of Y , with still higher confidence, and so on. This reasoning leads other methods to a process in which all possible rules are first generated and then evaluated.

The problem of small disjuncts was first noted in [3], where it was showed that existing classifiers create models that are good for large disjuncts but are far from ideal for small disjuncts (which correctly classifies only few training instances). To investigate the performance of TFPC, we carried out experiments using a number of data sets taken from the UCI Machine Learning Repository. It is hard to assess the accuracy of small disjuncts because they cover few instances, yet removing all of them is unjustified since many of them may be significant and the overall accuracy would be degrade.

In this paper we adopt an alternative heuristic: If we can identify a rule 4"~ c which meets the required support and confidence thresholds, then it is not necessary to look for other rules whose antecedent is a superset of X and whose consequent is c. It will still of course be necessary to continue to look for rules that select other classes. This heuristic both reduces the number of candidate rules to be considered, and the risk of over fitting. We use a method derived from our TFP (Total From Partial) algorithm to generate a set of CARS. This method, described in [3], first builds a set- enumeration tree structure, the P-tree that contains an incomplete summation of support-counts for relevant sets. Using the P-tree, the algorithm uses an Apriori- like procedure to build a second set enumeration tree, the T-tree, that finally contains all the frequent sets (Le. those that meet the required threshold of support), with their support-counts.

During classification, L3 adopts a two-step approach in which high-quality rules (that is, rules used in the classification of training data) are considered first, and "unchecked" rules, that is, rules unused during the training phase, are used next to classify unlabeled data. Rules of the secondtype are only considered when unlabeled datcannot be classified by means of the first type of classification is a well-studied problem (see [2,3] for excellent overviews) and several models have been proposed over the years, which include neural networks [1], statistical models like linear/quadratic discriminates [1], decision trees [2], and genetic algorithms [1]. Among these models, decision trees are particularly suited for data mining. Decision trees can be constructed relatively fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand [1]. And yields improved accuracy over decision trees [2]. As an alternative to decision trees, associative classifiers have been proposed. These methods first mine association rules from the training data, and then build a classifier using these rules. This classifier produces good results and yields improved accuracy over decision trees. Decision trees perform a greedy search for rules by heuristically selecting the most promising features. They start with an empty concept description, and gradually add restrictions to it until there is not enough evidence to continue, or perfect discrimination is achieved. Such greedy (local) search may prune important rules. Associative classifiers, on the other hand, perform a global search for rules satisfying some quality constraints.

This global search, however, may generate a large number of rules, and many of the generated rules may be useless during classification (Le., they are not used to classify The T-tree is built level by level, the first level any test instance) In this paper we propose a novel lazy comprising all the single items (attribute-values) under associative classifier, in which the computation is consideration. In the first pass, the support of these items is performed on a demand driven basis. We place our counted, and any that fail to meet the required support associative classifier within an information gain framework threshold are removed from the tree. Candidate-pairs are then that allows us to compare it to decision tree classifiers. generated from remaining items, and appended as child nodes.

Our method can overcome the large rule-set problem of Most work on lazy classification [1] was based on nearest traditional (eager) associative classifiers, by focusing on the neighbor algorithms [3].

features that actually occur within the test instance while generating the rules. We show that the proposed lazy classifier outperforms its eager counter- part, since in the lazy approach only the "useful" portion of the training data is mined for generating the rules applicable to the test instance. Due to this local focus, the lazy classifier can better classify a test instance, for which a global, eager rule-set may not work that well. Simple caching mechanisms are used to avoid work replication during lazy associative classification. First we demonstrate that associative classifiers perform no worse than decision tree classifiers.

Then we show that lazy classifiers outperform the corresponding eager classifiers. Our claims are empirically confirmed by an extensive set of experimental results. Timings are also showed in order to evaluate different classifiers with respect to computational complexity of the training data set.

III EXPERIMENTAL RESULTS

V. DECISION TREE AND DECISION RULES  IV EAGER ASSOCIATIVE CLASSIFIER  Figure1.Test instance data set  high I false Icool~ sunny I  ~ Outlook I Temperature I Hmnidity I Windy I yes ratny cool nOffilal false no ratny cool nOffilal true yes overcast hot hi~l false no SlUU1Y lnild high false yes ratny cool nOlmal false yes SlUU1Y cool notmal false yes ratny cool nOffilal false yes SlUllly hot nOffilal false yes overcast mild high tlue no SlUllly mild high true  In this section we describe eager associative classifiers, and demonstrate why they perform better than decision trees. We start by discussing how decision rules may be generated from decision trees. Then we describe associative classifiers that are based on information gain, so that we may compare them regarding the rules that are generated by each approach.

Given any subset of training instances S, let Si denote the number of instances with class ci, and let lSI = Pi si be the total number of training instances. Then pi = si lSI denotes the probability of class ci in S. The entropy of S is then given as E(S) = Pi pi log pi, For any partition of S into m subsets Si, with S = [mi=1Si, the resulting split entropy is given as E({Si}) = Pm i=1 ISillSI E(Si). The information gain for the split is then given as I(S, {Si}) =E(S) - E({Si}).A decision tree is built using a greedy, recursive splitting strategy, where the best split is chosen at each internal node according to the information gain criterion.

The splitting at a node stops when all instances are from a single class or if the size of the node falls below a minimum support threshold, called minsup. Figure 1 shows an example of training data, and Figure 2 shows the corresponding decision tree  CMAR [3] and CPAR [1]. In the first set of experiments, as in [3] and [2], we have assumed a support threshold of 1% and a confidence threshold of 50%.For the implementation of CPAR, we used the same parameters used in [10],Le. minimum gain threshold = 0.7, total weight threshold = 0.05, decay factor = 2/3, and similarity ratio 1: 0.99. We tried to use the same data sets as those used to analyze CMAR and CPAR. However in many cases the data sets that were used in [2] and appear to be no longer available, and in others were found not have identical parameters to those reported. The data sets chosen therefore comprise a subset of those used in [3] and [1], augmented by a further selection from the UCI repository. The choice of addi- tional data sets concentrated on larger/denser data sets (2000+ records) because the majority of the data sets used in the reported analysis of CMAR and CPAR were relatively small (less than 1000 records). Most work on lazy classification [1] was based on nearest neighbor algorithms [3]. The problem of small disjuncts was first noted in [1], where it was showed that existing classifiers create models that are good for large disjuncts butare far from ideal for small disjuncts (which correctly classifies only few training instances). It is hard to assess the accuracy of small disjuncts because they cover few instances, yet removing all of them unjustified since many of them.

A lazy decision tree was proposed in [1] and it was shown that the lazy approach is superior than the corresponding eager one (Le., C4.5). Despite all the improvements obtained by using lazy algorithms, we are not aware of any proposals of lazy associative classification algorithms, as well as an assessment that demonstrates why they perform better than both decision and eager associative classifier. Due to the local focus, the lazy classifier can better classify a test instance, for which a global, eager rule-set may not work that well simple catching mechanism are used to avoid work replication during lazy associative classification. First we demonstrate that associative classifiers to perform to worse than the decision tree classifier. Then we show that lazy classifier outperform the corresponding eager classifier. Our claims are empirically confirmed by an extensive Set of experimental results. Timing are also showed in order to evaluate different classifiers with respect to computational complexity.

To investigate the performance of TFPC, we carried out experiments using a number of data sets taken from the UCI Machine Learning Repository. The implementation of TFPC was as a Java program, and for comparison purposes we have used our own (Java) implementations of the published algorithms for    Figure 2. Decision tree  The last row of the table in Figure 1 shows one test instance which is recognized by the decision tree in Figure 2. The decision tree can be considered as a set of disjoint decision rules, with one rule per leaf. In that way, a decision tree can be simulated by a set of decision rules. In this case, the information gain for each decision rule is calculated in the same way as it is calculated for each path of the decision tree. Thus, a decision rule has the same value of information gain of its corresponding path in the decision tree.



VI. ENTROPY-BASED ASSOCIATIVE CLASSIFIER  We denote as Class association rule (CARs) those association rules of the form X ! c, where the antecedent (X) is composed of feature variables and the consequent (c) is just a class. CARs may be generated by a slightly modified association rule mining algorithm. Each item set must contain a class and the rule generation also follows a template in which the consequent is just a class. CARs are essentially decision rules, and as in the case of decision trees, CARs are ranked in decreasing order of information gain. Finally, during the testing phase, the associative classifier simply checks whether each CAR matches the test instance; the class associated with the first match is chosen.

Note that, seen in the light of CARs, a decision tree is simply a greedy search for CARs, using a level-wise search algorithm that only expands the current best rule with other features. On the other hand, an eager associative classifier mines all possible CARs with a given minsup. It is also interesting to note that sorting the final rule-set on information gain, and using the best CAR for classification, is also a greedy strategy. While the greedy approach has its limitations, eager associative classifiers are not limited by the prefix problem of decision rules, that is, once the best feature is chosen a teach node, all nodes under that sub tree must contain it.

let V be the set of all 11 training instances let T be the set of all 111 test instances  I . let C" be the set of all rules {X -+ G} mined from V  2. sort C" according to information gain  3. for each t., E T do  4. pick the first rule {X -+ G} E C"IX ~ f;  5. predict class G  Figure 3. Eager Associative Classifier  Figure 3 shows the basic steps of the eager associative classifier. In the initial step, the algorithm mines all frequent CARs, and sorts them in descending order of information gain. Then, for each test instance ti, the first CAR matching ti is used to predict the class. Figure 4 shows an associative classifier built from our example set of Training instances in Figure 1, using the algorithm showed in Figure 3. Three CARs match the test instance of our example (last row of Table 1):  1. {windy=false and temperature=cool play=yes}  2. {outlook=sunny and humidity=high play=no}  3. {outlook=sunny and temperature=cool play=yes} Rule {windy=false and temperature=cool play=yes}  Would be selected, since it is the best ranked CAR. By applying this CAR, the test instance will be correctly classified. Intuitively, associative classifiers perform better than decision trees because associative classifiers allow several CARs to cover the same partition of the training data. In our example, the test case is recognized by only one rule in the decision tree, while the same test case is recognized by three CARs in the associative classifier.

Selecting the proper CAR to be applied is an issue in associative classification. Next we present a theoretical discussion about the performance of decision trees and eager associative classifiers.

Theorem 1 The rules derived from a decision tree are a subset of the CARs mined using an eager associative classifier based on information gain.

Proof 1 Let max E be the maximum entropy of all decision tree rules. Select a set Ce from all CARs that their entropy is at most max E. It is clear that the decision tree rules are a subset ofCe    Theorem I states that, for a given minsup, CARs contain (at least) all information of the corresponding decision tree. Since each decision tree rule may be seem as a CAR, and since all possible CARs were enumerated , then the decision tree can be built by choosing the proper CARs  Theorem 2 CARs perform no worse than decision tree rules, according to the information gain principle.

D, only the instances sharing at least one feature with the test instance A are used to form DA. Then, a rule-set ClAis generated from DA. Since DA contains only features inA, all CARs generated from DA must match A. The lazy associative classifier is presented in Figure 5.

let V bethe setofall? traininginstances let T be the setofallm test instances

I. let Ce be thesetofall rules {A' """"' c}minedfromV  2. sort ee according toinformationgain v v  3. for each ti ET do  4. pickthefirst rule {A' """"' c}Eee lA' ~ u 5. predict class c  Proof 2 Given an instance to be classified, and, without loss of generality, a decision tree with just pure leaves, the decision tree predicts class c for that instance. We analyze two scenarios: first , just one CAR matches the instance, and second, more than one CAR matches. When just one CAR matches, it is the same as the decision tree rule, since the set of CARs subsumes the set of decision rules. In this case, the associative classifier and the decision tree make the same prediction. When more than one CAR matches an instance, the prediction may be either the same class (say c) as the matching decision rule or another class. If the associative classifier predicts c then the two approaches are equivalent .In case a class other than c is predicted, by definition, the best matching CAR provides a better information gain than the decision rule, and thus, according to the information gain principle, the CAR will make a better prediction  Figure 5. Lazy Associative Classifier  Now we demonstrate that the lazy associative classifier produces better results than its eager counterpart. Given a test instance A, and a set of CARs C, we denote by CA those CARs {X -.c} in C where X ~ A.

Theorem 3 Let A be the set of features in a given test instance.Let Ce A be the set ofCARs obtainedfrom the eager associative classifier induced by A, and Cl A be the set of CARs obtainedfrom the lazy associative classifier induced by  e I A. For a given rninsup, we have C A ~ C A  Proof 3 By definition, both CeA and CIA are composed of CARs {X -.c} in which X -.A, that is, all CARs contain only features in A. Also the training instances matching A (i.e., the projected training data) are a subset ofthe set of  true Ilowhot  no - - - true yes overcast hot - - yes - hot - - yes overcast - - true no - - - true  I ?(yes) Iovercast I  I Play IOutlook ITemperature IHumidity IWindy I  VII LAZY ASSOCIATIVE CLASSIFIER  Theorem 2 states that the additional CARs of the associative classifier that are not in the decision tree, cannot degrade CAR is only used if it is better than all decision rules (according to the information gain principle).However, eager associative classifiers generate a large number of CARs, most of which are useless during classification. For instance, from the set of 13 CARs showed in Figure 4, only 3 match the test instance (the remaining 10 CARs are useless). Next, we present a lazy classifier and compare it to the eager version described in this section.

Unlike the eager associative classifier that extracts a set of ranked CARs from the training data, the lazy associative classifier induces CARs specific to each test instance. The lazy approach projects the training data, D, only on those features in the test instance, A. From this projected training data, DA, the CARs are induced and ranked, and the best CAR is used. From the set of all training instances,  All training instances (i.e., DA ~D). Thus,for a given minsup, if a rule {X -.c} is frequent in D, and then it must also be  I e frequent in DA. Since C A is generatedfrom DA and C A is  e I generated from D (andDA ~ D), C A ~ C A The next example illustrates Theorem 3. Figure 6 shows the training data given in Figure I (i.e., D), projected by the features in the test instance (i.e., A) showed in the last row in    Figure 6. The projected training data (Le., DA) is composed of the 5 instances showed in the figure. Suppose minsup is set to 40%. In this case, the set of CARs, Ce, found by the eager classifier is composed of the two CARs:  1. {windy=false and humidity=normal!play=yes} 2. {windy=false and temperature=cool!play=yes}  None of the two CARs matches the test instance, and thus,  CeA =0;. On the other hand, the projected training data has  less instances (DA ~ D), and therefore, CARs not frequent in D may be frequent in DA. This is because a frequent CAR must occur at least 4 times in D (sinceIDI=10), but only 2 times in DA (since IDAI=5). The lazy classifier found two CARsinDA: 1. {outlook=overcast! play=yes} 2. {temperature=hot! Play=yes} These lazy CARs not only predict the correct class, but also are simpler than the eager CARs. Next we discuss how the lazy CARs perform when compared to the eager CARs Theorem 4 Lazy CARs perform no worse than eager CARs, according to the information gain principle.

e Proof 4 Theorem 3 showed that, for a given minsup C A ~  CiA. Let Re be the best rule in CeA (according to the  information gain principle), and let RI be the best rule in CIA.

Two scenarios have to be considered when determining a class for the test instance A. In the first scenario, RI is identical to Re ; in this case the same class is predicted by both eager and lazy classifiers. In the second scenario, RI is better than Re, and thus RI must provide a better prediction. _  Theorem 4 states that the CARs added by the lazy classifier do not degrade the classification accuracy. This is because an additional lazy CAR is only used if it is better than all eager CARs (according to the information gain principle).

Intuitively, lazy classifiers perform better than eager classifiers because of two characteristics:  ? Missing CARs: Eager classifiers search for CARs in a large search space, which is induced by all features of the training data. While this strategy generates a large rule-set, CARs that are important to some specific test instances may be missed (this is particularly true for skewed/unbalanced distributions).

Lazy classifiers, on the other hand, are context-sensitive and focus the search for CARs in a much smaller search space, which is induced by the features of the test instance.

? Highly Disjunctive Spaces: Eager classifiers generate CARs before the test instance is even known. In this case, the difficulty for the classifier is in anticipating all the different directions in which it should attempt to generalize its training examples (Le., what CARs must be generated). For this reason, eager classifiers often combine small disjuncts in order to generate more general Predictions (more general CARs should be applicable to more test instances). This can reduce classification performance in  highly disjunctive spaces, where single disjuncts may be important to classify specific instances. Lazy classifiers, on the other hand, generalize their training examples exactly as needed to cover the test instance. Thus, lazy classifiers are often most appropriate when the search space is complex, and there are myriad ways to generalize a case.

VIII EXPERIMENTAL EVALUATION  In this section we show the experimental results for the evaluation of the proposed classifiers in terms of classification effectiveness and computational performance.

Our evaluation is based on a comparison against C4.5 [1] and Lazy DT [1] decision tree classifiers. We also compare our Numbers to some results from other associative classifiers, such as CPAR [2], CMAR [1] and HARMONY [1], and to some results from rule induction classifiers, such as RISE [2], RIPPER [3], and SLIPPER [3]. We used 26 datasets from the VCI Machine Learning Repository to compare  IX EAGERAND LAZYASSOCIATIVE CLASSIFIERS  We continue our analysis by comparing the effectiveness of eager (EAC) and lazy classifiers (LAC). Table 1 shows their corresponding error rates. For very small datasets eager and lazy classifiers perform similarly, since the CARs that were generated by both classifiers were essentially the same for the parameters used. For instance, the result obtained with labor and zoo datasets were exactly the same.

Also, we can observe that lazy classifiers perform better when the dataset is sparse (Le., auto, pima, diabetes, german, and wine datasets). The error reduction in these datasets range from 13.9% to 52.7%. This result is expected, since the small disjuncts problem is more likely to happen in sparse datasets.

Further, we can also notice that the lazy classifiers always outperform the corresponding eager ones, except for the ionosphere dataset. We believe that, for this dataset, the lazy classifiers have over fitted the data.

9.1 Rule Induction and Associative Classifiers We also compared the proposed eager and lazy associative classifiers against RISE, RIPPER and SLIPPER, using results reported in [3,2,1]. Table 2 shows the relative performance for each classifier (Le., the accuracy of one classifier divided by the accuracy of the other classifier), when compared to eager associative classifiers. Each number in this table indicates how many times EAC is superior than the corresponding adversary RISE, RIPPER or SLIPPER (in terms of accuracy). The SLIPPER algorithm won in 6 of the 26 datasets (and lost in 1), showing to be most competitive rule induction classifier. The RIPPER classifier won in 4 datasets and the RISE classifier won in only one dataset.

Table 2 also shows the relative performance of rule induction classifiers when compared to the lazy associative classifiers.

RISE and RIPPER lost in almost all datasets, and SLIPPER was the most competitive one. Compared toLAC (inf. gain), SLIPPER won in one dataset (and matched    ~lnlI 11..ulr ....- ,"",CA.'('~0.- CJoriIiim a..ifIm CIIIIiIm;  0403 I.II:\IIlT lAC CIll alAI. IIADIllRr tAl: I.4C IN: W; lI[.l!'lIl 111"- 111."" '1IIIf CIIIIl .- IqIII. .- lilt,.. lilt .....

mZI1 ~.l 41 U 3.li 2.7 U ~l lli 40.0 46.1.... us 152 U.o 13.4 13.9 - n.? U.4 3.1 6.Z- lU "'./ = 11.] ZLll jJlJl :ru 2l.ll 'ol :Ii,V,- :u ).1 u 4.J :1.6 I.' 1.1 4. 'I, II" 0lo0o 11.1 17.2 llU lli.1 11.1 - lS.5 15.7 IU loU lB .", 1\l.l1 1:1.l1 lU 1.1l - 142 M.l H 10,7  '1IiIbclcI 21.0 :l4.l1 :i4.ll 2'-3 l.U - lJ: 21-' 103 :lll,- 211.' :zti.J 11.2 15' Ul . D.4 UJJ 1,1 211.1 sI- 21.l 25.S 2Jl..lI 21.4 2U 50.2 :ll.D 27A 2.5 5.4..... 11.9 17.7 11.1 Its 11.1 GJ 10 17.2 U IU '''''- Zi,' :lIll IJj 1!.l 1'-' ZiJl 11. 1M :IIi J4J ~ ".l 17.2 1:l.A 111 lU I1j W 17.2 H 11.0 Iijp> 1.2 1.2 1.2 1.1 16 - LD 1.2 0,0 lAl ,~ a.Q IJl 1~ ILl IlJ I.V U J,U z., n. ' .1 3.l U 7.J ~ 6.1 11 4.li 1.3 ]!U..... 2l .V 2Q.4 19.1 I1.V JU - ll'.J 11.lJ 9.V 9,0 loiI1 ~.l 25.S 24..2 27.1 11.5 2SA 22.1 2S.S 1,7 lA, I)oqR n .v &l au lUi ItJI - ll'.l IlL! j,i II,V- 1,1.3 ;;)jI U~ U .6 M" ZIJI ;a.u lU V,V RUlid: 2.1 2.1 2.1 21 2j - 1.0 !.J 0,0 4.B-- n,l :l4.ll 12.lI 21.1 2.0.0 - :IlJ.' IlI.lI I,D JIU~.,.....,.., 0.5 O./i 0.6 WI 1IJ 1.7 0.5 U 0,0 0.0 nIoido ?),6 31.1 )OJ 11.3 11.2 . :IllI 3D.O I.l 14,0- :lA,S 22.7 22.3 2U JU UJ 1'.; IIl9 13A 21.5..- 'U 13 '1:1. 11.4 ).lJ i.l loA 1!J I ,' )'/,V., 1.1 I.J " ' .4 U I.V c.: ).4 1M lA1,-. 11.1 16.2 I", 11'.1 I 14.1 Ill.l' IA.O 14.3 1.1 lI.l  classifier, because it uses many different local models to form its implicit global approximation to the target function. Eager classifiers commit at training time to a single global approximation. An important feature of the proposed lazy classifier is its ability to deal with the small disjuncts problem.

Based on this observation, we present evidence showing that a lazy associative classifier outperforms the corresponding eager one. Our claims were con- finned by empirical comparisons to C4.5 and LazyDT decision tree classifier, using datasets from the DCI data repository. We also compared the proposed Classifiers against other three associative classifiers and three rule induction Classifiers and outperformed them in most of the cases. So far, our classifiers use only the best CAR for sake of classification. In the future we will combine simple and complex CARs in order to enhance classification. Finally, we will explore more realistic application scenarios.

