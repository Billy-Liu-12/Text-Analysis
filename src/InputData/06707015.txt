CRNN: Integrating Classification Rules into Neural Network

Abstract? Association classification has been an important type of the rule-based classification. A variety of approaches have been proposed to build a classifier based on classification rules. In the prediction stage of the extant approaches, most of the existing association classifiers use the ensemble quality measurement of each rule in a subset of rules to predict the class label of the new data. This method still suffers the following two problems. The classification rules are used individually thus the coupling relations between rules [1] are ignored in the prediction. However, in real-world rule set, rules are often inter-related and a new data object may partially satisfy many rules. Furthermore, the classification rule based prediction model lacks a general expression of the decision methodology. This paper proposes a classification method that integrating classification rules into neural network (CRNN, for short), which presents a general form of the rule based decision methodology by rule-based network. In comparison with the extant rule-based classifiers, such as C4.5, CBA, CMAR and CPAR, our approach has two advantages. First, CRNN takes the coupling relations between rules from the training data into account in the prediction step. Second, CRNN automatically obtains higher performance on the structure and parameter learning than traditional neural network. CRNN uses the linear computing algorithm in neural network instead of the costly iterative learning algorithm. Two ways of the classification rule set generation are conducted in this paper for the CRNN evaluation, and CRNN achieves the satisfactory performance.



I. INTRODUCTION  IN recent years, the association rule mining integrated withclassification, which is called association classification, has been widely studied [2][4]. The performance of associa- tion classification, such as CBA [3][5] and CMAR [6], shows that it is even better than traditional rule based classifier such as C4.5 [7]. These methods use association rule mining algorithm, such as Apriori [8], FP-growth [9], to generate a lot of rules and adopt strategies to select useful rules for the classification tasks. The general prediction schema of the rule based classification is shown as in Fig. 1. Three methods are usually used in the prediction task, which are single rule based prediction, top-K rules based prediction, and group rules based prediction.

Wei Li is with the Advanced Analytics Institute at the University of Technology, Sydney, Australia and the Key Laboratory of Medical Image Computing of Ministry of Education, Northeastern University, Shenyang, China (email: lewe01@gmail.com).

Longbing Cao and Xia Cui are with the Advanced Analytics Insti- tute at the University of Technology, Sydney, Australia (email: long- bing.cao@uts.edu.au, xia.cuil@gmail.com).

Dazhe Zhao and Jinzhu Yang are with the Key Laboratory of Medi- cal Image Computing of Ministry of Education, Northeastern University, Shenyang, China (email: zhaodz@neusoft.com, yangjinzhu@neusoft.com).

This work is in part by Australian Research Council Discovery Grants (DP1096218 and DP130102691) and ARC Linkage Grant (LP100200774), and by Chinese NSF grants (61172002 and 61001047) and the Central University project N110618001.

As shown in Fig. 1, the single rule based prediction relies on the sorting of the rules, and the class label of the first satisfied rule is taken as the predictive result. It is more reliable using top-K satisfied rules to make the final decision in the second approach. The measurement (e.g. expected accuracy is used in [15]) of each rule is conducted, and then an average value on the K rules that belong to same class will be generated. Accordingly, the class with the highest value is selected. However, the rules are not always consistent in class tags for a new coming data object. Therefore, the rules are grouped by the class labels, and then the classifier uses the overall effects of the group rules and yields to the group with highest total performance, which is as shown in the third method. E.g., ?2 is used to measure the qualify of rules in the group [6], and then the total sum (or weighted sum) value is obtained. Finally, the label of the group with the highest ?2 value is chosen as the prediction outcome.

Classification  Rules Set  (R ,R , , R  m )  Training  Dataset     R  R  R m...

R R  R 1K...

Decision by one rule in order  R N1 R N2  R NK......

Class 1 Class N  R R  R 1P... R  N1 R N2  R NQ......

Class 1 Class N  Decision by top K rules  Decision by group rules  Class 1 Class N Class 1  Predicted  Class  New Data  Fig. 1. The classification rule based prediction methodology  However, the methods shown in Fig. 1 may suffer some weakness as shown below.

1) The coupling relations between the rules are ignored in prediction stage. The rules are treated individually in determining the new data object. In fact, different rules often contain same items. The new data object may fully or partly satisfy many rules, and these rules have similar or partially similar rule logic that means these rules share inter-relation and as a whole contribute to the class prediction. Moreover, a subset of rules are used in the prediction decision as shown in Fig. 1 in extant approaches.

2) A general expression of the rule based decision methodology is lacked at present. The rule?s support and confidence, which are originally used to measure the rules, are abandoned in these methods while some alternative measurements which have the theoretically similar functions as support and confidence are used for. For example, the Laplace accuracy of a rule r is defined as (nc +1)/(ntot + k), where k is the number    of classes, ntot is the total number of data objects that satisfy the rule body p, and nc data objects belong to class c. However, the value of Laplace accuracy is just the confidence of rule when the data set size N is big enough.

Proof: (nc + 1)/(ntot + k) = (nc/N + 1/N)/(ntot/N + k/N) ? (nc/N)/(ntot/N) = support(r)/support(p) = confidence(r), where 1/N ? 0 and k/N ? 0.

In order to solve the above two problems, rules are used in prediction model as rule network. The support and confidence are also integrated into the classification model.

The idea of this paper is from artificial neural network [11] (ANN for short) which can cover all the rules and rules parameters. As we known, the ANN classifier has been studied in machine learning for many years and it has been used in a variety of applications. ANN has many advantages in practice, but a coin has two sides. The training process is usually longer than other classification approaches; moreover, the structure of ANN needs to be designed manually and the hidden nodes form a black box which is hard for user understanding.

This paper proposes a new neural network structure based on the classification rules (CRNN for short), which integrates the advantages of association classification and neural net- works and is different from [10]. This new classification method makes full use of classification rules and obtains much more efficiency in both the network structure and parameter learning than the traditional neural networks. In our approach, CRNN relies on the rules mined from the training data set, so the quality of rules has still impact on the performance of CRNN. We use two different rule set generation methods, which are the association rule mining with rule selection strategy [6][9] and FOIL (First Order Inductive Learner) based rule searching with the rule support and confidence [12], to conduct the CRNN prediction accu- racy evaluation. CRNN tackles the problem of the general expression of rule based prediction decision and complex computing process of the structure and parameter learning in neural network. The contributions of this study are as follows.

1) We propose a method to integrate all the classification rules into classification model as well as the coupling relations between rules.

2) This paper introduces a new structure of neural net- work. CRNN is created by the rule set and the structure of CRNN is determined automatically.

3) CRNN shows a new approach to obtain the parameters in neural network efficiently. The parameters in CRNN are fast obtained once the classification rules are given.

The subsequent sections are organized as follows. Section II describes the definition of the research problem and an overview of the proposed method. Section III introduces the two methods of classification rule generation. Section IV shows the design and construction of the CRNN model.

CRNN classification is presented in Section V using the  CRNN model. Our approach is evaluated in Section VI and we make the conclusions in Section VII.



II. PROBLEM STATEMENT  Given a set of data items I = I1, I2, ? ? ? , In, where I1, I2, ? ? ? , In are items of attributes values. Let D denote the data set which has N data records. Each record contains a number of attributes and every attribute contains a sub- set items in I . Each continuous attribute should be firstly discretized into categorical attribute items. Each data record corresponds to a class tag of Y , where Y = {Y1, ..., YM}.

Definition 1: (Classification Rule) Let pattern A contain one or more data items combined as I?i ? ? ?? Ij , where A links to a class tag Yk, we call the form, r : I?i ? ? ?? Ij ? Yk, a Classification Rule. The support of the classification rule r is defined as the percentage of the pattern A in the data set, written as sup(r).

sup(r) = sup(A?Yk) = #(A?Yk)  N (1)  where #(A?Yk) is the size of the data records that cover pattern A and belong to class Yk . The confidence of the classification rule, conf(r), is defined below.

conf(r) = sup(A?Yk)  sup(A) =  sup(r)  sup(A) (2)  The problem we want to solve is as follows. Defining a model F based on neural network that incorporates the classification rule set, {r1, r2, ..., rp} , from data set D, then for the new data record x, making prediction using the model F, namely Yk = F (r1, r2, ..., rp)|x.

Training  Dataset Discretization if nesserary  Association rule based  classification rules  generation FOIL based  classification  rules  generationRules  selection  Classification rules set  CRNN construction and parameter learning  & optimizaiton  Classification model of CRNN  CRNN classifier  Testing  Dataset  Fig. 2. CRNN framework  The framework of the CRNN is shown as Fig. 2. The approach is divided into two parts. Firstly, two ways are shown to generate the classification rule sets. The CRNN    model is theoretically designed and then the construction al- gorithm is introduced. Secondly, the classification procedure of the CRNN is described using the above model. 21 data sets are used to evaluate the classifier and the approach achieves satisfactory performance.



III. CLASSIFICATION RULE GENERATION In this paper, two different ways are used to generate the  rule set, that are the association rule mining (ARM for short) based rule generation and the FOIL based rule generation. A small data set is shown in Example 1.

Example 1 (Mining Classification Rules) Let D be the training data set as in Table I (the first 6 columns). There are four attributes in every record and three class labels in total.

TABLE I DATA SET FOR EXAMPLE 1  TID A B C D Class Transaction record 1 a2 b1 c2 d1 Y1 a2, b1, c2, d1, Y1 2 a1 b2 c1 d2 Y2 a1, b2, c1, d2, Y2 3 a2 b3 c2 d3 Y1 a2, b3, c2, d3, Y1 4 a1 b2 c3 d3 Y3 a1, b2, c3, d3, Y3 5 a1 b2 c1 d3 Y3 a1, b2, c1, d3, Y3 6 a1 b3 c1 d1 Y2 a1, b3, c1, d1, Y2 7 a1 b3 c3 d2 Y1 a1, b3, c3, d2, Y1  A. ARM Based Rule Generation A new data set D? is organized based on the original data  set D, in which each record is converted to a transaction data record with the class label Yi. The new data set is shown in Table I (the last column). This new data set is easy to be processed by the traditional association rule mining methods.

The process of ARM based rule generation is different from traditional association rule mining, in which a rule consists of a class tag Yi as the right part of the rule. The ARM based classification rule generation is defined as in Algorithm 1.

Algorithm 1: ARM based rule generation. ARM-rg Data: A transaction database, D; the minimum support  threshold, ?.

Result: A rule set, RuleSet.

1 begin 2 initial RS = ? 3 find all the frequent pattern set pts that meets support ? 4 initial pattern and support map table, patternset = {}  for each p in pts do 5 given pattern p = (A ? Yi, sup) or p = (A, sup) if p  doesnt contain class label Yi then 6 add p in to patternset, patternset[A] = sup  7 for each p in pts do 8 given pattern p = (A ? Yi, sup) or p = (A, sup) if p  contains class label Yi then 9 add tuple (A, Yi, sup, sup/patternset[A] ) into  RuleSet  10 end  The Algorithm 1 has twice scans on the frequent patterns set. The first traversal is used for computing the classification  rule confidence, while the second traversal finds all the classification rules with support and confidence parameters.

14 classification rules in Example 1 are obtained as shown in Table II (sup = 2/7).

TABLE II CLASSIFICATION RULES MINED IN EXAMPLE 1  Rule ID Rules Support Confidence r01 a1 ? Y3 0.29 0.41 r02 a1, d3 ? Y3 0.29 1.00 r03 a1, b2 ? Y3 0.29 0.67 r04 a1, b2, d3 ? Y3 0.29 1.00 r05 d3 ? Y3 0.29 0.67 r06 b2, d3 ? Y3 0.29 1.00 r07 b2 ? Y3 0.29 0.67 r08 a2 ? Y1 0.29 1.00 r09 a1 ? Y2 0.29 0.41 r10 a1, c1 ? Y2 0.29 0.67 r11 c1 ? Y2 0.29 0.67 r12 c2 ? Y1 0.29 1.00 r13 a2, c2 ? Y1 0.29 1.00 r14 b3 ? Y1 0.29 0.67  The classification rules generated by Algorithm 1 can be the raw rule set as the input of the CRNN model construction.

However, the number of the classification rules is usually very large. Therefore, it needs to select the high quality rules for classification. There are many selection strategies in the previous work [6][13][14]. In this paper, the database coverage method which is proposed in [2] is used to select the classification rules.

Given two rules R1 and R2, R1 has higher rank than R2, if and only if (1) conf(R1) > conf(R2); (2) if conf(R1) = conf(R2), but sup(R1) > sup(R2); (3) if conf(R1) = conf(R2) and sup(R1) = sup(R2), but R1 has less items than R2. According to this rule ranking methodology, the classification rules are sorted in the descending order firstly.

Every rule is tested for how many records are covered in the data set. The rules that cover at least one record are selected until all the data records are covered with a user predefined minimum threshold.

The new rule set generated by the rule selection strategy is much smaller than the original one. If the minimum data set coverage is set to 2 in Example 1, only 3 rules in Table II are remained, that are r13 : a2, c2 ? Y1, r04 : a1, b?2, d3 ? Y3 and r09 : a1 ? Y2.

B. FOIL Based Rule Generation  FOIL is used to find the rules that distinguish the positive examples from the negative ones [12]. Usually, FOIL is applied on each class when the data set has multiple class labels. The multiple class problems are transformed into several binary class problems. Rules are obtained for every class and then the rules for each class are merged to form the final rule set of the whole data set. The rules generated by the FOIL do not fit with the CRNN model since the rules lack the support and confidence parameters. The support and confidence need to be appended additionally.

In the FOIL based rule generation procedure, a measure- ment is required to constrain how to select an attribute value to form a rule. The FOIL gain is usually used to get the information gain when an attribute value, Ai, is added to the current rule r. Let |P | be the number of the positive examples and |N | be the number of the negative examples. Once an attribute value, Ai, is added to the r?s body, we get the new numbers of the positive and negative examples, as |P ?| and |N ?| respectively. Thus the FOIL Gain of Ai is computed as follows.

fgain(Ai) = |P ?| ( log  |P ?| |P ?|+ |N ?|  ? log |P | |P |+ |N |  ) (3)  The attribute value with maximum FOIL Gain is selected to append the rule body until all the positive examples are covered. The FOIL based rule generation is presented in Algorithm 2.

Algorithm 2: FOIL based rule generation. Foil-rg Data: A data set, ds; the minimum foil gain threshold, ?.

Result: A rule set, RuleSet.

1 begin 2 initial RuleSet = ?, given class labels set  Y = Y1, ? ? ? , YM for each c in Y do 3 initial rs4c = ? for class c 4 get positive and negative data sets, PD, ND of c 5 while PD ?= ? do 6 rbody = ?, PD? = PD, ND? = ND 7 while |ND?| > 0 and  rbody.length < max? rule? len do 8 searching Ai with maximum fgain 9 if fgain(Ai) < ? then  10 break  11 adding Ai into the rbody 12 delete examples not satisfying rbody in  PD?, ND?  13 adding the rbody into rs4c 14 delete all the examples satisfying rbody in PD  15 for each rb in rs4c do 16 computing the sup(r) and conf(r) of rule  r : rb ? c 17 adding rule tuple (rb, c, sup(r), conf(r)) into  RuleSet  18 end  Four rules are generated from the data set shown in Example 1 where the fgain is set to 0.5. The rule set are {a2 ? Y1, b3, c3 ? Y1, c1 ? Y2, d3, a1 ? Y3}.



IV. CRNN MODELLING  According to the ANN classification, we define the CRNN model structure. The CRNN structure relies on the rule set found in data set.

A. CRNN Model Design  The multilayer ANN structure is commonly seen. One usually needs to determine the input and output nodes as per the underlying problem and the number of nodes in the  hidden layer as well. Thus a parameter learning algorithm is used to learn the weight parameters between the nodes in ANN. Similarly, the CRNN model is designed as a four layer neural network which includes the input layer (IN layer), pattern and class layers (PN and CN layer) and output layer (ON layer). We call the hidden layer as a middle layer since that the nodes in the middle layers of CRNN can be interpreted in actual meaning. The structure of CRNN is shown in Fig. 3.

Input layer Middle layer Output layer  Xq wqi wij YkIN PN CN ONwjk  Fig. 3. Structure of CRNN model  In comparison with traditional ANN, there are some difference in the CRNN model.

? The nodes between the layers are partially connected while the nodes in ANN are not. In particular, PN node has and only has an input link, and CN node has and only has an output link.

? The nodes in the middle layers of CRNN stand for ac- tual meaning. Furthermore, the nodes and links between the middle layers are determined by classification rule set obtained from the training data set. Different datasets fall into different CRNN model structures automatically.

? The parameters, Xq , wqi, wij , wjk and Yk, in CRNN require no complex and time-consuming learning algo- rithms (such as back propagation) to be computed out.

These parameters are all obtained from the classification rule set.

B. CRNN Construction Using Rule Set  Every classification rule actually contains five data el- ements: the rule id, attribute items, class label, sup- port and confidence, which is written as: {RID : (set of items, class, sup, conf)}. The mapping between the elements of a rule and the nodes of the four-layer CRNN structure are shown as Fig. 4.

The detailed mapping steps are described as follows.

? The input nodes in CRNN stand for the attributes of  the data record. That means the dimension of data set is the number of the input nodes. For example, the supermarket retail data can be presented by only one    Classification  rules  Attributes in  dataset  Categorical values  of attributes  Classes in  dataset  RID  Attribute  Item  Class  Support of rule Sup  Confidence of  rule Conf  CN node  IN node  PN Node  ON node  wjk  wij  Second middle  layer  Input layer  First middle  layer  Output layer  Parameters to  be learned  Classification Rules Set CRNN  Fig. 4. Data mapping between rules set and CRNN  node in CRNN. The data set in Example 1 has four attributes, A, B, C, D, thus four input nodes appear in the CRNN.

? The nodes in the first middle layer contain all the items that appear in the rule body. Every input node has links to its own attribute value.

? The nodes in the second middle layer correspond to the classification rules. Every rule is represented as one node in the CN layer. The connections between the PN layer and CN layer depend on whether the items in PN and CN layers are contained in the same rule.

? The output nodes represent the class labels of the data set, and every node in the CN layer only links to its class node in the ON layer.

? wjk and wij are the support and confidence of the rule respectively. wij in CRNN gives the confidence of the rule in CN when the items in the rule body appear in the PN node. wjk represents the linkage of rule CNj and class Yk.

? wqi is always 1.0 which means the attribute Xq has items of attribute value in PNi.

Based on the above steps of CRNN structure building and parameter setting, the CRNN model construction is presented in Algorithm 3.

Given a classification rule set, RuleSet = {r1, r2, ? ? ? , rR}, every rule r has nr items in the rule body, the class label Yr ? {Y1, Y2, ? ? ? , YM}. The rule r in RuleSet is formed as (RuleIDr, Pr, Yr, supr, confr). The CRNN is represented by a graph data structure G = (V,E).

Example 2 (CRNN Construction) 14 rules in Example 1 are used to construct a CRNN model by Algorithm 3.

The first rule is r01 : a1 ? Y3. The nodes, XA, PNa1 , CNr01 and ONY3 , the linkages (XA, PNa1 , 1.0), (PNa1 , CNr01 , 0.41) and (CNr01 , ONY3 , 0.29) are created. For the rule r02 : a1, d3 ? Y3, the nodes XA and ONY3 exist, thus only the nodes, XD, PNd3 and CNr02 need to be appended into the network. The edge between XA and PNa1 has been already in the CRNN. The links (XD, PNd3 , 1.0), (PNa1 , CNr02 , 1.00), (PNd3 , CNr02 , 1.0) and (CNr02 , ONY3 , 0.29) are added. All the nodes and edges eventually are obtained  Algorithm 3: CRNN model construction. CRNNCON Data: A rule set generated from data set, RuleSet.

Result: The CRNN network, G.

1 begin 2 initial G =< V,E >, V = ?, E = ?, where V is the  nodes set and E is the links set with weights.

3 for each r in RuleSet do 4 r = (RuleIDr, Pr, Yr, supr, confr) and  pr = i r 1 ? ? ? ? ? irmr if node of Yr not in V then  5 create node ONr , and add it into V  6 if node of RuleID not in V then 7 create node CNr , and add it into V  8 create link(CNr , ONr , supr) and add it into E 9 for each e in Pr do  10 if node of attribute that contains e not in V then  11 create node Xe, and add it into V  12 if node e not in V then 13 create node PNe, and add it into V  14 if link (Xe, PNe, 1) not in E then 15 create link (Xe, PNe, 1.0) and add it into E  16 create link (PNe, CNr , confr) and add it into E  17 normalize all the weights in every output node 18 end  in CRNN until all the rules have been processed. We will get to the final neural network structure as shown in Fig. 5.

Input layer Middle layer Output layer  Xq wqi wij YkIN PN CN ONwjk  r01  r03  r05  r06  r07  r02  r04  r08  r10  r12  r13  r14  r09  r11  a1  d3  b2  a2  c1  c2  b3  A  B  C  D  Y3  Y2  Y1  Fig. 5. The CRNN instance in Example 2  The construction of the CRNN takes |RQ| time in CRN- NCON algorithm relying on the rule set size and rule body length. R is the number of rule set and Q is the length of items in rule set, Q = n1+n2+ ? ? ?+nR. Through the above description, we get the following characteristics of the CRNN classification model.

? Structural autonomy: CRNN connections between the nodes are determined by association rules, and IN nodes and PN nodes are fixed by the data itself naturally. The input layer of CRNN has the same number of the nodes    as the dimension of the data set attributes. The PN nodes in the first middle layer are related to the items in the association rules. There are identical CN nodes to the rules. CRNN network can be fixed without user manual design by contrast to the traditional multi-layer neural network.

? Parameter setting: The parameters in the CRNN net- work are assigned directly. The weights between the IN nodes and PN nodes are of Boolean type. If the item of the PN node, which belongs to a attribute, appears in an association rule, thus the weight between the input node and the PN node is set to 1, otherwise it is set to 0 (the links in current building process and examples are not considered). The weights, wij and wjk, are also from the association rules. CRNN parameters are not obtained by learning algorithms as the traditional neural network, which makes the CRNN construction quick.

? Interpretable relationship: The weights wij describe the relations between patterns and classes. This is identical to the meaning of the classification rule, when the pattern appears, the class label will be predicted with confidence. The relations between the CN and ON nodes are obvious that the class labels are predicted correctly in terms of probability. The supports of the rules just represent the frequency in the data set (approximate with probability).

? Transparent layers: As we know, the hidden layer in the traditional neural network is designed by users and the meaning of the hidden node is usually hard to be interpreted for users. But this situation is changed in the CRNN. The hidden layer which is called middle layer in the CRNN has intuitive explanation and is transparent to the user.



V. PREDICTION USING CRNN  This section introduces a CRNN classifier for the new data object prediction.

A. Building a Classifier  There are many activation functions used in the traditional ANN, such as threshold function, piecewise linear function, sigmoid function, Gaussian function and so on. A piecewise linear function is used in this paper, which is defined as F : y = x, x ? (0, 1], as shown in Fig. 6(a). The parameters of CRNN cannot be negative, and the maximum value of x is 1.0.

(a)  x  y  0 1   ? F  x1  x2  xn  ...

w1  w2  wn  Y  (b)  Fig. 6. CRNN activation function and node computing  Generally speaking, the node computing model in the neu- ral network is shown as in Fig. 6(b). Suppose a neural node has n input links, X = (x1, x2, ? ? ? , xn)T , and the weights for every link are represented as W = (w1, w2, ? ? ? , wn)T .

The output Y is computed as follows.

Y = F (WTX + b) = F  ( n?  i=1  wixi + b  ) =  n? i=1  wixi + b  (4) where the b is the bias and F is the activation function.

Let the rule set be RuleSet = {r1, r2, ? ? ? , rR}, the class labels set {Y1, Y2, ? ? ? , YM}, and each class Yk has nk rules, that are rYk1 , r  Yk 2 , ? ? ? , rYknk (1 ? k ? M ), and  R = M? k=1  nk. Every rule rYkp has the support as s Yk p , the  confidence as sYkp . The items in the rule body are assumed as {Ip1 (Yk), I  p 2 (Yk), ? ? ? , Ipm(Yk)} . Then the output node Yk  can be computed according to the above node computing model.

Yk =  nk? t=1  F (rt)wt + b c k  = sYk1 F (r1) + s Yk 2 F (r2) + ? ? ?+ sYknkF (rnk) + b  c k  (5)  F (rt) is the output of the node in the CN layer, and F (rt) is obtained below.

F (rt) = conf(I t 1(Yk))F (I  t 1(Yk)) + ? ? ?  +conf(Itmt(Yk))F (I t mt(Yk))  = cYk1 (rt)S(I t 1) + c  Yk 2 (rt)?(I  t 2) + ? ? ?  +cYkmt(rt)?(I t mt) + b  c k  (6)  Since the weights between the IN layer and PN layer are Boolean values, ? is an indicator function that ? is 1 when a PN node has output, otherwise ? is 0. Assuming a Boolean vector, Vt = (1, 1, ? ? ? , 1, 0 ? ? ? , 0), is the input vector of the CN layer where the element with zero value indicates that there is no link to CN node. Thus the value of rt is obtained as:  F (rt) = (c Yk 1 , c  Yk 2 , ? ? ? , cYkmt , 0, ? ? ? , 0)V  T t + b  r t (7)  We take this value into the Yk, then we will get the Yk.

Yk = s Yk 1 {c  Yk 1 (r1)?(I  1 ) + ? ? ?+ c  Yk 1 (r1)?(I  m1) + b  r 1}  +sYk2 {c Yk 2 (r2)?(I  1 ) + ? ? ?+ c  Yk 2 (r2)?(I  m2) + b  r 2}  + ? ? ?+ sYknk{c  Yk nk (rnk)?(I  nk 1 ) + ? ? ?+ cYknk(rnk)?(I  nk mnk  )  +brnk}+ b c k  = (sYk1 c YK 1 (r1), ? ? ? , s  Yk 1 c  YK 1 (r1), 0, ? ? ? , 0)V T1 + s  Yk 1 b  r  +(sYk2 c YK 2 (r2), ? ? ? , s  Yk 2 c  YK 2 (r2), 0, ? ? ? , 0)V T2 + s  Yk 1 b  r  + ? ? ?+ (sYknkc  YK nk  (rnk), ? ? ? , sYknkc YK nk  (rnk), 0, ? ? ? , 0)V Tnk +sYknkb  r nk  + bck  (8)    V1, V2, ? ? ? , Vnk is normalized by a unified order. E.g. one sorts the rules by the rule number. A new Boolean vector U from V is got according to the rule order. The final output value of class, Yk, is represented as follows.

Yk = ( sYk1 c  Yk 1 , s  Yk 2 c  Yk 2 , ? ? ? , sYknkc  Yk nk  )???? U1I U2I ? ? ? UnkI  ????  + ( sYk1 , s  Yk 2 , ? ? ? , sYknk  )???? br1 br2 ? ? ? brnk  ???? +bck  (9)  where I is a unit column vector, namely IT = (1, 1, ? ? ? , 1).

According to the structure of the CRNN, UtI is just the number of the items in the rule rt. When the new data comes, the above method is used to compute Y value for every class.

The class label with the maximum value of Y is selected as the class label of the new data. However, one problem is still in Equation (9) because the parameters of b are unknown. In this paper, we set all the parameter of b to zero. The final computing model of Yk is shown as following formula.

Yk = ( sYk1 c  Yk 1 , s  Yk 2 c  Yk 2 , ? ? ? , sYknkc  Yk nk  )???? U1I U2I ? ? ? UnkI  ???? (10) All the s and c are known for us and the U can be known  if a user gives the new data. The class k with maximum of Yk is regarded as the class label of the new data, which is shown as Equation 11.

kpredict = argmax k  {Yk}, k ? {1, 2, ? ? ? ,M} (11)  Example 3 (CRNN Classification) Let the new data be x = (A = a1, B = b2, C = c2, D = d1), predicting the class label of x using the CRNN model as shown in Fig. 5. All the values of Y are calculated as follows.

Y1 = ( sY18 c  Y1 8 , s  Y1 12c  Y1 12 , s  Y1 13c  Y1 13 , s  Y1 14c  Y1  )???? U8I U12I U13I U14I  ???? = 0.435  (12)  The computing processes of Y2 and Y3 are the same as Y1.

Y2 and Y3 are 0.264 and 1.085 respectively. The maximum value is Y3, so the class of the new data x is Y3.

B. General Form of Rule-based Decision  It is known that wij is conf(r) and wjk is sup(r) in Algorithm 3. The value of Yk is transformed as follows when wij is set to be conf(r)/L, where L is the length of the rule body.

Yk =  ( sYk1  cYk1 L , sYk2  cYk2 L , ? ? ? , sYknk  cYknk L  )???? U1I U2I ? ? ? UnkI  ???? = ( sYk1 c  Yk 1 , s  Yk 2 c  Yk 2 , ? ? ? , sYknkc  Yk nk  ) (13)  where UtI is just the number of the items in the rule rt. This is a kind of general form for the rule based decision process.

Let us exam the three methods in Fig. 1, and we can get that they are the special forms of the above decision expression where the node activation function F is set as threshold type.

We assume the CN node is in activation when all the PN nodes (items of rule body) have output, which means the rule is fully satisfied as in [5][6][15].

? We treat Yk (sum of sYki c Yk i ) in another way rather than  in sum expression. All sYki c Yk i within Yk are sorted in  the descending order. The class label with first highest value sYki c  Yk i is selected which is the first method  described in [5] ? For the second method in Fig. 1, the satisfied top-K rules  are selected for each class Yk, and the class label with highest Yk value of the K rules (i.e. K = 5) is treated as predicting class. For example, the Laplace accuracy serves the similar purpose as the confidence when the data set size is big enough. The class label with the highest arithmetic average of Laplace accuracy of the top-K rules is regarded as the predicting tag in [15]. Yk here is the weighted average of confidence (the support value in Yk is the weight sum of confidences, because the sum of the supports for Yk is 1.0 in Algorithm 3 [line 15]).

? Let the rule be r : p ? c, the sum of sYki c Yk i  (sup(r)conf(r) for short) has the following equal rela- tion: Yk =  ? sup(r)conf(r) =  ? sup(p, c) sup(p,c)sup(p) =? sup2(p,c)  sup(p) . The above Yk has similar function with the sum of ?2 for each rule in the group [6].



VI. EXPERIMENTAL RESULTS  In this paper, 21 data sets from UCI ML Repository are used to evaluate our approach. Discretization of continuous attributes is done using the same method in [16]. We have conducted the accuracy study on these data and compared CRNN with C4.5 [7] and CPAR [15]. The CRNN algorithm is implemented in Python v2.7, and all the other approaches are tested by their authors. All experiments are conducted on a desktop computer with Intel Core 2 CPU of 2.80GHz, 4GB memory and Windows 7. The rule sets are generated by two different ways, which are ARM based rule generation (CRNN-a for short) and FOIL based rule generation (CRNN- f for short). The performance of CRNN on the two rule sets is shown as in Table III.

The parameters of the CRNN model are set as following.

The support and confidence of association rule mining are set to 0.05 and 0.9. The database coverage threshold is set to    TABLE III ACCURACY: C4.5, CPAR, CRNN-A AND CRNN-F  ID Dataset C4.5 CPAR CRNN-c CRNN-f 01 austra 84.7 86.2 86.5 85.9 02 breast 95.0 96.0 97.3 95.4 03 cleve 78.2 81.5 84.2 84.5 04 crx 84.9 85.7 86.5 85.9 05 diabetes 74.2 75.1 75.4 73.7 06 german 72.3 73.4 71.0 72.0 07 glass 68.7 74.4 65.5 66.9 08 heart 80.8 82.6 84.3 84.8 09 hepatic 80.6 79.4 84.4 81.3 10 horse 82.6 84.2 79.6 84.8 11 hypo 99.2 98.1 95.2 99.1 12 iris 95.3 94.7 94.7 96.0 13 labor 79.3 84.7 90.0 77.7 14 led7 73.5 73.6 73.8 62.0 15 lymph 73.5 82.3 82.4 83.1 16 pima 75.5 73.8 73.2 72.5 17 sick 98.5 96.8 93.9 96.3 18 sonar 70.2 79.3 80.3 81.2 19 wave 78.1 80.9 78.9 83.0 20 wine 92.7 95.5 87.6 92.2 21 zoo 92.2 95.1 90.1 90.1  Avg. 82.38 84.44 83.56 83.26 #top-1 3 4 7 7  10. With regard to the FOIL based rule generation, the fgain is set to 0.1. As the result shown in Table 4, the average accuracy of CRNN is better than C4.5. CRNN-a and CRNN- f have higher performance in some data set which are shown as bold font in Table III.

TABLE IV EFFICIENCY OF CPAR, CRNN-A AND CRNN-F  Average CPAR CRNN-c CRNN-f Time Rules Time Rules Time Rules  Arithmetic 0.2 261.4 85.0 55.1 14.4 26.3 Geometric 0.04 105.6 19.4 39.8 1.7 19.7  Table IV shows the algorithm efficiency (in second) and average number of rules used in CRNN. We compute the arithmetic and geometric average of the running time and rules for the CPAR, CRNN-a and CRNN-f on the 21 datasets.



VII. CONCLUSIONS  The rule base classification and neural network classifica- tion are two popular methods in machine learning and data mining. Association classification invokes a new aspect of classification methodology. This paper combines the classi- fication rule and neural network to form a classifier which takes advantage of rule base classification and neural network approaches. The evaluation result shows that CRNN achieves high accuracy and efficiency, which can be credited to the features of CRNN. Firstly, CRNN uses network structure to make prediction, which takes all the rules rather than a part of rules in the rule set and coupling relations between the rules into consideration. Secondly, the parameters in CRNN are all from the rules without the complex learning procedure. Therefore, such a neural network is efficient.

Moreover, CRNN has two good characteristics. CRNN holds a general expression of the rule based decision methodology.

The nodes in CRNN are all transparent for users meaning that a clear interpretation of hidden nodes in CRNN can be given while it is hard for traditional neural network.

CRNN introduces a new approach towards efficient and high quality classification model integrating the classification rules into neural network. Our further work will focus on the following two tasks. The bias of node computing model is set to zero in this paper, and we will do more study on how to learn the bias parameters by a linear computing method.

From the evaluation results, we know that the rule set is critical for CRNN. Better rule set makes higher accuracy of CRNN. We will do more research work on mining effective rule set.

