GraphRank: Statistical Modeling and Mining of Significant Subgraphs in the Feature Space

Abstract  We propose a technique for evaluating the statistical sig- nificance of frequent subgraphs in a database. A graph is represented by a feature vector that is a histogram over a set of basis elements. The set of basis elements is chosen based on domain knowledge and consists generally of vertices, edges, or small graphs. A given subgraph is transformed to a feature vector and the significance of the subgraph is com- puted by considering the significance of occurrence of the corresponding vector. The probability of occurrence of the vector in a random vector is computed based on the prior probability of the basis elements. This is then used to ob- tain a probability distribution on the support of the vector in a database of random vectors. The statistical significance of the vector/subgraph is then defined as the p-value of its observed support. We develop efficient methods for comput- ing p-values and lower bounds. A simplified model is fur- ther proposed to improve the efficiency. We also address the problem of feature vector mining, a generalization of item- set mining where counts are associated with items and the goal is to find significant sub-vectors. We present an algo- rithm that explores closed frequent sub-vectors to find sig- nificant ones. Experimental results show that the proposed techniques are effective, efficient, and useful for ranking fre- quent subgraphs by their statistical significance.

1 Introduction  Recent advances in science and technology have generated a large amount of complex data. As a powerful abstract data type, graphs are often used to represent these complex data.

In the database community, graph models have been used for schema matching [1], web documents, multimedia [2], and social networks [3]. In biology, graphs have been used to represent molecular structures, protein 3D structures [4], and protein interaction networks [5].

Mining structured patterns in a collection of graphs is useful for understanding the intrinsic characteristics of sci- entific data. In drug development, frequent pattern mining can reveal conserved substructures in a category of medi- cally effective chemical compounds [6]. In studies of pro- tein interaction networks, conserved patterns in multiple species reveal cellular machinery [5]. In the analysis of protein structures, the presence of conserved subgraphs in protein contact maps can reveal evolutionarily significant patterns of chemical bonds and interactions [4].

A number of techniques have been developed to find fre- quent subgraphs [7, 8, 9, 10, 11, 12, 13, 14] in a transac- tional database, i.e., a large collection of graphs. However, the usefulness of frequent subgraph mining is limited by two factors:  1. Not all frequent subgraphs are statistically significant.

2. There is no way to rank the frequent subgraphs. This hinders the identification of subgraphs of real inter- est, especially when the number of discovered frequent subgraphs is large.

For illustrative purposes, consider a sample graph database shown in Fig. 1 and some frequent subgraphs shown in Fig. 2. The support of a subgraph is the number of graphs that contain the subgraph. A subgraph is frequent if its support is above a given threshold. Neither the sup- port nor the size of a subgraph is sufficient to measure the statistical significance of a subgraph, and to rank the listed subgraphs.

A  B B  C  A  B C  B  A  B B B C  C  G1 G2 G4 G5  A  B C  G3  Fig. 1. A sample graph database  0-7695-2701-9/06 $20.00  ? 2006     Which subgraph is the most statistically significant?

A  B  A  B B  A  B B  A  B B  C  A  B B  C  Subgraph Structure Support  g1  g5  g4  g3  g2  4 {G1, G2, G3, G4}  1 {G2}  2 {G2, G4}  2 {G1, G2}  3 {G1, G2, G4}  Fig. 2. Frequent subgraphs and their supports  1.1 Our Approach In this paper, we propose a technique for computing the sta- tistical significance of frequent subgraphs. The statistical significance of a subgraph g with observed support ?0 is defined as the probability that g occurs in a database of ran- dom graphs with support ? ? ?0, namely the p-value of g. Using this measure, we can rank the frequent subgraphs, and/or remove insignificant ones.

The main challenge of the above procedure is how to es- timate the probability that a subgraph occurs in a random graph. As graphs have flexible structures, it is difficult to estimate such probability directly in the graph space (Note that the problem of determining whether a graph is a sub- graph of another is NP-complete). Milo et al [15] adopted a simulation approach: generate many random graphs while maintaining some empirical measures such as degree of ver- tices, number of edges, and then count the ones that contain the subgraph. However, this approach is neither scalable to a large collection of graphs nor precise for computing and comparing small p-values.

We address the above challenge by transforming graphs into a feature space (Fig. 3). First, we use domain knowl- edge to define a set of basis elements such as vertices, edges, or small subgraphs. A graph is simply regarded as a collec- tion or a histogram of basis elements; this defines its feature vector. Then, we approximate the question of significance of a subgraph by considering the significance of its feature vector in the feature space (Fig. 4). This is a simpler prob- lem that admits closed-form solutions. Although structural information of a graph is lost in the feature space, statistics on the basis elements are still captured. As shown by the experimental results, this approximation is suitable for the discovery of significant subgraphs.

In the second half of the paper, we address the problem of feature vector mining, a simplified version of graph min- ing. Vector (aka histogram and multiset) mining is an im-  Frequent subgraphs  Graph mining  Feature vectors  Target Graph DB ??????  ??????	?  Transform  Fig. 3. Represent graphs as feature vectors  Probability distribution of x?s support in a database  of random vectors  Support of g in target DB  P-value of g  A subgraph g How  significant is g?

Feature vector x  Probability that x occurs in a  random vector  Fig. 4. Compute p-value of a subgraph  portant generalization of frequent itemset mining. We de- velop ClosedVect, an algorithm that explores closed sub- vectors to find significant ones. We prove that ClosedVect is optimal in terms of the number of search states.

We validate the quality of our technique through experi- ments on chemical compounds and synthetic graphs. In par- ticular, we find that a specific subgraph, neither largest nor most frequent, turns out to be the largest common subgraph in a specific class of medically effective compounds. This finding validates the practical usefulness of our approach.

We also demonstrate the efficiency of the computational methods and the feature vector mining algorithm.

The main contributions of our work are as follows:  1. We propose a technique for computing the p-values of frequent subgraphs, and show that frequent subgraph can be ranked by this measure.

2. We address the problem of feature vector mining, and present an algorithm for mining significant closed sub- vectors. This is an important problem in its own right.

The remainder of the paper is organized as follows. Sec- tion 2 discusses how to represent graphs as feature vectors.

Sections 3 and 4 present a probabilistic model and a simpli- fied model. Section 5 describes the feature vector mining.

Experimental results are reported in Section 6. We conclude with a brief discussion in Section 7.

2 Representing Graphs as Feature Vectors  We view a graph as a collection of basis elements B = {b?1, ..., b?m}. These basis elements can be vertices, edges, or small graphs. Each basis element b?i is associated with a prior probability ?i. We first discuss how to select basis elements and transform graphs into feature vectors.

0-7695-2701-9/06 $20.00  ? 2006    2.1 Feature Selection The selection of basis elements is application-dependent and may require domain knowledge. A basic approach is to select all types of vertices or edges as features. The draw- back of this approach is that it does not capture any struc- tural information of graphs.

For other graphs such as chemical compounds, one may choose small graphs such as Benzene rings. In this case, the number of small graphs could be large and they may overlap structurally. Thus, selecting a representative subset would be more appropriate. The following criteria for se- lection can be used: 1) frequency: frequent basis elements are more representative of graphs; 2) size: large basis ele- ments carry more structural information (but would be less frequent); 3) structural overlap: overlapping basis elements are relatively not independent; 4) Co-occurrence: basis el- ements that frequently occur together are not independent.

Based on these criteria, one may select basis elements by a greedy approach [16]: choose the kth best element accord- ing to its benefit gained (e.g., frequency) and its relevance (e.g., overlap, covariance) to the previously selected k ? 1 basis elements.

For the sample database shown in Fig. 1, we use all kinds of edges as the basis, i.e., B={A-B, A-C, B-B, B-C, C-C}.

The prior probabilities are empirically computed using their frequency in the database, i.e., ? = ( 617 ,  17 ,  17 ,  17 ,  17 ).

2.2 Transforming Graphs into Feature Vectors After a basis is selected, we transform (sub)graphs into feature vectors. We denote a feature vector by x = (x1, ..., xm), where xi counts the frequency of feature b?i in the graph. The size of x is defined as |x| = ?xi. Vector x is a sub-vector of y (and y a super-vector of x) if xi ? yi for i = 1, ...,m, and is denoted by x ? y. The floor of two vectors x and y is a vector v where vi = min(xi, yi) for i = 1, ...,m. The definition extends to a group of vectors.

The ceiling of a group of vectors is defined analogously.

For example, the feature vector of subgraph g3 under the basis B is (2, 0, 1, 0, 0).

3 Probabilistic Model  In this section, we model the probability with which a fea- ture vector x (corresponding to a subgraph) occurs in a random vector (corresponding to a random graph), and the probability distribution of x?s support in a database of ran- dom vectors. Statistical significance is obtained by compar- ison to its observed support.

3.1 Probability of occurrence of a feature vector in a random vector  We regard the basis B as a set of m distinct events, one for every basis element, where basis element b?i is associated with its prior probability ?i. A feature vector of a certain  size ? is thus regarded as an outcome of ? independent trials.

Given a feature vector y = (y1, ..., ym), |y| = ?, the  probability that y is observed in ? trials can be modeled by a multinomial distribution:  Q(y) = ?!? yi!

m? i=1  ?yii , (1)  In other words, Eqn. (1) gives the probability of observing y in a random vector of size ?.

Let x be the feature vector of a subgraph g. Then, the probability that x occurs in a random vector of size ? is a cumulative mass function (c.m.f.) of Eqn. (1):  P (x; ?) = ?  ys.t.yi?xi,|y|=? Q(y) (2)  In other words, this is the probability that x occurs in a ran- dom vector of size ?. The size constraint ? is reasonable: the larger a random vector, the more likely that x will occur in the vector.

For example, the feature vector of subgraph g3 is x = (2, 0, 1, 0, 0). The probability that x occurs in a random vector of size 3 is P (x; 3) = 0.066.

Eqn. (2) can be efficiently computed using a divide-and- conquer approach (see [17] for details).

3.2 Probability distribution of a feature vector?s support in a database of random vectors  Now we consider the support of x in the context of a database of random vectors. This support is a random vari- able that follows a probability distribution. Let n be the number of vectors in the target database, we summarize the sizes of the vectors by ? = (?1, ..., ?d) and n = (n1, ..., nd), where ni is the number of vectors of size ?i, and  ? ni = n.

If we regard a random vector as a trial, and the occur- rence of x in the vector as a ?success?. Then, the database of random vectors corresponds to n trials, and the support of x corresponds to the number of successes in n trials. If the sizes of the vectors were identical, say ?, then the sup- port can be modeled as a binomial random variable, with parameters n and P (x; ?). When the sizes are distinct, each size will correspond to one binomial random variable with parameters ni and P (x; ?i). Then, the support of x is the sum of the binomial random variables: the probability of x?s support being equal to ? is given by  R(?; x, ?, n) = d?  ? tj=?  bino(tj ; ni, P (x; ?i)) (3)  where bino(t; n, p) = (nt )p t(1?p)n?t is the binomial prob-  ability distribution. In other words, the jth binomial con- tributes tj successes, with the sum of them equal to ?. All  0-7695-2701-9/06 $20.00  ? 2006    0 1 2 3 4 5  0.1  0.2  0.3  0.4  0.5  0.6  Support ?  P ro  ba bi  lit y  p-value = 0.09  20 =?  Fig. 5. Probability distribution of g3?s support and its p-value  possible combinations of tj give the total probability of ob- serving ?.

For the sample database of Fig. 1, a random database would have ? = (3, 4) and n = (3, 2). Fig. 5 plots the prob- ability distribution of g3?s support in the random database.

Eqn. (3) can be efficiently computed using a divide-and- conquer approach (see [17] for details).

3.3 Statistical Significance of a Feature Vector Let ?0 be the observed support in the target database. Then, the p-value, i.e., the probability of observing a support of at least ?0 in the random database, is given by  R(? ? ?0;x, ?, n) = n?  ?=?0  R(?;x, ?, n). (4)  The smaller the p-value, the more statistically significant is the feature vector.

?  0?  p-value  g1 3.84 4 0.67 g2 1.65 3 0.20 g3 0.55 2 0.09 g4 0.85 2 0.20 g5 0.16 1 0.15    Table 1. P-values of the subgraphs in Fig. 2; subgraph g3 has the smallest p-value.

Now, we are ready to answer the question regarding sig- nificance raised in Fig. 2. Table 1 shows the p-values of the subgraphs as well as their expected supports. Among the subgraphs, g3 has the smallest p-value. Thus, we can claim that g3 is the most statistically significant (though it is neither the largest nor the most frequent).

4 A Simplified Model  In this section, we present a simplified model in which the computation of p-values is much more efficient. First, we  relax the constraint on the size of random vectors, and con- sider the probability that a sub-vector occurs in a random vector of arbitrary size. The probability can be written as  P (x) = P (Y1 ? x1, ..., Ym ? xm) (5) Further, if we assume that different types of basis elements are orthogonal, then the above joint probability can be de- coupled into a product of probabilities:  P? (x) = m?  i=1  P (Yi ? xi) (6)  where P (Yi ? xi) is the probability that element b?i occurs at least xi times in a random vector. Since P? (x) is fixed, we then model the support of x by a single binomial distri- bution, with parameters n and P? (x).

Under this model, we compute the p-value as follows.

1. Empirically obtain the prior probabilities P (Yi ? j) for every basis element b?i and every j (up to the maximum possible value). For example, element b?1 =?A-B? occurs twice (G1 and G2) in the sample database, thus P (Y1 ? 2) = 25 .

2. Compute P? (x) using Eqn. (6). For subgraph g3, x = (2, 0, 1, 0, 0). Thus P? (x) = P (Y1 ? 2) ? P (Y3 ? 1) = 25 ? 35 = 625 .

3. Compute the p-value of x by ?n  ?0 bino(?; n, P? (x)),  or equivalently by the regularized Beta function I(P? (x);?0, n). When both nP? (x) and n(1 ? P? (x)) are large, the binomial distribution can be approxi- mated by a normal distribution.

5 Feature Vector Mining  As frequent subgraphs are represented as feature vectors and evaluated for statistical significance, an interesting question arises: can we directly search top-K significant sub-vectors, or sub-vectors above a significance threshold?

To our best knowledge, the problem of feature vector min- ing has not been addressed before. Feature vector mining is important in two aspects. First, feature vectors, also known as histograms and multisets, are common ways to summa- rize complex data. As a result, feature vector patterns are profiles of structured patterns, and feature vector mining can work as a foundation of structured pattern mining. Second, feature vector mining is an important generalization of the well studied frequent itemset mining: each item is now as- sociated with a count instead of a boolean value.

We develop ClosedVect, an algorithm that explores fre- quent closed sub-vectors to find significant ones. The algo- rithm consists of two phases: exploring closed sub-vectors and evaluating the significance of a closed sub-vector.

Alg. 1 outlines the phase of exploring closed sub-vectors.

The algorithm explores closed sub-vectors in a bottom-up,  0-7695-2701-9/06 $20.00  ? 2006    depth-first manner. At each search state, the algorithm ?jumps? to a future state that has an immediately smaller supporting set along a branch (line 3). The corresponding sub-vector is then promoted as the floor of the supporting set (line 6). To prevent duplicates, each search state is as- sociated with a beginning position b. Any future state must extend at a position greater than or equal to b. If an exten- sion designated at position i results in a starting position of less than i, then it must be a duplicate extension (lines 7-8).

The evaluation phase (line 1) computes the p-value of a sub-vector and reports top-K significant ones (see [17] for details). Lines 9-10 estimate a lower bound on the p-value of the super-vectors of x? and prune it if this bound is too high. This pruning is discussed further in [17].

Alg. 1 ClosedVect(x, S, b) x: current sub-vector; S: supporting set of x, i.e., vectors that contain x; b: beginning position at which bins can be extended;  1: Eval(x, |S|); 2: for i := b to m do 3: S? ? {Y | Y ? S, Yi > xi}; 4: if |S?| <minSupport then 5: continue; 6: x? := floor(S?); 7: if ?j < i such that x?j > xj then 8: continue; 9: if p-value(ceiling(S?), |S?|) ? maxPvalue then  10: continue; 11: ClosedVect(x?, S?, i);  Fig. 6 shows a running example of Alg 1. The underlined number denotes the beginning position b. Duplicate search states are pruned. For instance, the extension to ?2 3 2? at position ?3? leads to a supporting set ?{h1, h3}?, of which the floor is ?3 4 2?. This extension violates the search order and is pruned (lines 7-8).

h1:   4 5 6 h2:   3 2 4 h3:   3 4 2 h4:   2 3 3  2 2 2 {h1, h2, h3, h4}  2 3 3 {h1, h4}  3 2 4 {h1, h2}  3 4 2 {h1, h3}  4 5 6 {h1}  2 2 3 {h1, h2, h4}  2 3 2 {h1, h3, h4}  3 2 2 {h1, h2, h3}  3 4 2 {h1, h3}  4 5 6 {h1}  X X  4 5 6 {h1}  3 2 4 {h1, h2}  XX  Fig. 6. A running example of ClosedVect  An algorithm is complete if it finds all answers. It is com- pact if every search state finds at least one distinct answer. It is duplicate-free if it has no duplicate search states or dupli- cate answers. The following theorem shows the correctness and efficiency of ClosedVect (see [17] for proof).

Theorem 1. (Correctness and Efficiency of ClosedVect) Algorithm ClosedVect explores closed and only closed sub-vectors. It is complete, compact, and duplicate-free.

In other words, ClosedVect is optimal in terms of the number of search states because every search state corre- sponds to a distinct closed sub-vector.

6 Experimental Results  In this section, we report experimental results that validate the quality and efficiency of the proposed techniques. We use the NCI AIDS Antiviral Screen chemical dataset and focus on the category of confirmed active (CA) which con- tains 422 compounds. The p-value computation and the fea- ture vector mining algorithm were implemented in Java us- ing Sun JDK 1.5.0. All experiments were performed on an Intel 2.8GHz, 1G memory running MS Windows XP Pro.

We also experimented with a synthetic dataset and a web page visits dataset. The complete results appear in the full paper [17].

6.1 Evaluating the Quality of the Results We generate two sets of basis elements to transform sub- graphs into feature vectors. The first set consists of all dif- ferent edges (39 in total), namely 1-edge basis. The second set is constructed by selecting 30 out of all possible sub- graphs of 3 edges (322 in total) using the greedy approach in Section 2. We call this the 3-edge basis. For each case, we compute the prior probabilities using their frequency in the background dataset (42,000 compounds in total). Then, we use CloseGraph [10] to find frequent closed subgraphs from the CA category. With minSup = 5%, 7879 closed sub- graphs are discovered. For each subgraph, we compute its p-value using the two bases and the two models (exact and simplified) respectively.

Fig. 7 shows the p-values of the subgraphs vs. their ranks using 3-edge basis and the exact model. We also compute their p-values in the category of confirmed moderately ac- tive (CM) for cross-validation. As shown in the figure, the p-values of the subgraphs are much smaller than they would be in the context of the CM category. Further, a large num- ber of the subgraphs are statistically insignificant. Using a       ?80  ?70  ?60  ?50  ?40  ?30  ?20  ?10   Rank  P ?  va lu  e  CA CM  Fig. 7. P-value vs. rank with cross-validation  0-7695-2701-9/06 $20.00  ? 2006    p-value cutoff, say 0.01, we are able to reduce the number of discovered subgraphs by one order of magnitude.

O N  O  N  O  O  N N+  N  Fig. 8. The most significant subgraph in CA  Fig. 8 shows the most significant subgraph found in our results. We found that this subgraph is the largest common subgraph in the chemical class of Azido Pyrimidines1. The AZT compound, a super graph of this subgraph, is ranked 3rd in the exact model and 2nd in the simplified model. The compound has been widely used for HIV inhibition. The findings validate the practical usefulness of our approach.

We compare our ranking with a naive ranking approach: rank by size (in case of tie, rank by support). Table 2 shows the rankings of three subgraphs: the most significant sub- graph (AZT*), the largest subgraph, and Benzene. There is no current scientific evidence regarding the importance of the largest subgraph. As shown in the table, ranking by p-value is much more appropriate than the ranking by size.

Rank by p-value  3-edge basis 1-edge basis Subgraph Support Size strict simpl. strict simpl.

Rank by size  AZT* 15% 19 1st 1st 40th 69th 428th Largest 5% 34 914th 142nd 752nd 751st 1st Benzene 70% 6 886th 1424th 6820th 1875th 6969th     Table 2. Ranking by different approaches  6.2 Feature Vector Mining We evaluate the performance of algorithm ClosedVect us- ing the feature vectors of the chemical compounds. The experimental settings are: minSupport = 5?25%; K = +?; maxPvalue = 1, 0.01.

Fig. 9(a) shows the running time of ClosedVect w.r.t.

minSupport. The running time without p-value evaluation is only in seconds. This demonstrates the high efficiency of ClosedVect in exploring closed sub-vectors. With p-value computation, the simplified model adds a little amount of overhead, which is much less than that of the exact model.

Fig. 9(b) shows the number of closed sub-vectors w.r.t.

minSupport under the exact model. With the maximum p- value threshold set at 0.01, the number of closed sub-vectors is reduced by one order of magnitude.

7 Conclusions  Statistical significance and ranking are useful in the post- processing of data mining results. In this paper, we pro- posed a technique for evaluating the significance of frequent  1http://dtp.nci.nih.gov/docs/aids/searches/list.html  5 10 15 20 25      minSupport (%)  R un  ni ng  ti m  e (s  ec )  exact model simpl. model w/o Eval()  (a) Running time  5 10 15 20 25      minSupport (%)  # of  c lo  se d  su b?  ve ct  or s  p?value <= 1 p?value <= 0.01  (b) # of closed sub-vectors  Fig. 9. ClosedVect on chemical compounds  subgraphs in the feature space. We also addressed the prob- lem of feature vector mining, and developed an algorithm that efficiently searches significant closed sub-vectors. Ex- perimental results validated the quality, performance, and practical usefulness of the presented techniques.

Acknowledgements: We would like to thank Xifeng Yan and Jiawei Han for providing the code of CloseGraph. The work was supported in part by NSF grants IIS-0612327 and DBI-0213903.

