ARTS: Adaptive Rule Triggers on Sensors for Energy Conservation in Applications using Coarse-Granularity Data

Abstract  Communicating extensive in-network data generated by resource-constrained wireless sensor nodes is an energy consuming process. To minimise the amount of data ex- changed in sensor networks, several researchers have pro- posed novel and efficient protocols to perform data aggre- gations, clustering or regression on sensor nodes. Most of these approaches focus on optimising conventional mining techniques to work on resource-constrained sensor nodes.

However, the application of association rules for sensor net- works is an area of study that has not been investigated.

This is due to the high computational cost of obtaining meaningful rules. Thus, in this paper, we propose Adaptive Rule Triggers on Sensors ARTS, to extract highly correlated rules from sensor data and apply them. The learnt rules are used to extend sensor lifetime by controlling sensor op- erations using triggers. Our approach is optimised to run on non-critical sensing applications/data-aggregation ap- plications that can tolerate a coarse-granularity for sensed data. For this category of applications, our approach can derive meaningful rules efficiently to further conserve en- ergy of wireless sensors. In this paper, these energy sav- ings are evidenced in our experiments that adapt ARTS to a state-of-the-art clustering protocol.

1 Introduction  A wireless sensor network (WSN) is characterised by a collection of nodes deployed in a physical environment to detect some phenomena through sensing, data processing and wirelessly communicating the information to an exter-  nal observer [1]. Present-day applications of sensor net- works have included: (i) Monitoring applications that serve to collect intricate information about the sensed environ- ment such as in environmental monitoring [3] and health monitoring [14] (ii) Applications that focus on detecting abrupt sensor fronts such as the surveillance system de- scribed in [8], volcanic activity detection [2] and undersea events detection [15]. For the latter category of sensing ap- plications, a compromise on the granularity of sensor data is sometimes necessary to detect events quickly and more efficiently. By taking advantage of this sensing requirement in these applications, we believe that useful rules can be extracted from the sensed data efficiently to extend sensor lifetime. Hence, in this paper, we propose ARTS: Adap- tive Rule Triggers on Sensors. The essence of our approach is to generate expressive rules from highly correlated sen- sor attributes and execute rule triggers that can power-save sensors in real-time. We derived ARTS from our higher- level concept in [4], in which sensor data patterns, termed more generally as contextual information in the sensor net- work has been shown to achieve considerable energy sav- ings for sensor networks offline. Motivated by our results, we build on our framework and extend the work to perform rule learning on sensor streams to mine for such patterns and from the potential rules discovered, trigger sensor op- erations that conserve their energy. Use of ARTS is ideal in these cases: (i) When coarse granularity sensor data is adequate, as in the large class of applications mentioned above (ii) Where qualitative differences in sensing are suf- ficient, and (iii) Where the exact sensor value need not be predicted. While the above cases may seem to be limita- tions of ARTS, our approach is essentially taking advan- tage of this fact in many applications in order to provide   DOI 10.1109/ICESS.2008.57     an extremely low overhead technique for significant energy conservation. The low overhead of using ARTS enables it to conserve sensor energy in various other approaches that would involve data aggregation, clustering or as a way to supplement the answering of queries.

To demonstrate our approach, we implemented ARTS in a state-of-the-art clustering technique. This technique, HEED: Hybrid, Energy-Efficient, Distributed Clustering [16] enables efficient aggregation of sensed information through informed selection of relay nodes. The goal of HEED is to identify a set of cluster heads, which can cover the areas that the sensor nodes monitor, on the basis of the residual energy of each node. This is achieved using a prob- ability function to determine the likelihood a node will be- come a cluster head in order to select the node that attracts more nodes in terms of proximity and which has most resid- ual energy left. HEED, however, has the drawback that ad- ditional sensor energy would be depleted in the changing of the cluster heads at each reclustering cycle. To address this limitation, it is essential to prolong the time between chang- ing the cluster heads and running the HEED protocol over longer time intervals. We observe from our experiments with HEED that the rules discovered by ARTS contribute to improving HEED?s performance while the confidence of the rules indicates the accuracy of predictions made by ARTS.

The experiments show energy savings of up to 15% with an overhead of 0.06%.

The rest of this paper is organised in the following way.

In section 2, we present an overview of existing research in relation to our work. Section 3 details the ARTS al- gorithm, broken down into the data model arriving sensor streams, followed by the algorithm for our approach. The modus operandi for ARTS working in sensor network or- ganisations is given in section 4. To validate our methods, we run TOSSIM, a tinyOS simulator [10], for using HEED vs. HEED with ARTS algorithm on sensors and record our observations in section 5. We conclude in section 6.

2 Related Work  In this section, we discuss existing work pertaining to in-network discovery of spatial and temporal relationships between sensor data elements such as data regression to pre- dict exact sensor values. Interestingly, to the best of our knowledge, current efforts in in-network processing tech- niques for sensors have excluded rule learning from sensor data due to the high computational cost to discover rules.

However, we acknowledge that some aspects of rule learn- ing have been addressed in data stream association rule min- ing techniques for non-WSN type applications, and in some manner, existing inference-based approaches. Nevertheless, application of current association rule mining techniques has not been demonstrated to work in WSNs. Unique chal-  lenges facing WSNs that have not been addressed are the handling of multidimensional data from sensors efficiently to generate the rules, and the usefulness of the generated rules in sensor networks. We propose ARTS to solve these two problems.

2.1 Probabilistic Models in Sensor Net- works  As studied in [6], probabilistic models have commonly being used to predict missing values and identify outliers in traditional database system. More recently, the notion of us- ing correlations in data has been applied to sensor networks, network monitoring and data stream management.

Correlations in sensor data have been manipulated to perform distributed regression [7] and facilitate distributed inference [13] in sensor networks. In [7], the authors pro- posed a distributed message-passing algorithm for perform- ing regression within a sensor network. Their approach models local correlations in sensor data using kernel lin- ear regression such that sensor nodes would communicate constraints on the model parameters to estimate coefficients of the kernel basis functions. The resulting coefficients es- timated and locations of the kernel would then assist nodes in answering queries for its local region. In another study, [13] proposes an efficient three-layer architecture compris- ing spanning tree formation, junction tree formation and message passing to solve inference problems in sensor net- works. In this architecture, nodes would first organise them- selves into a spanning tree to obtain a stable wireless con- nection neighbours. Subsequently, nodes would compute information necessary to convert the spanning tree into a junction tree that enables efficient and robust message pass- ing for inference algorithms. More recently, in [12], the authors investigated correlations that can be formed when sensors in loading truck experience similar vibrations when the trucks send out the same load. The correlation informa- tion of the sensor nodes then allowed them to group trucks carrying out the same load. The unique contribution in their work lies in the incremental calculation of the correlation matrix.

The studies above describe significant approaches that have exploited correlations to enhance current sensor net- work operations. However, in contrast to our work, their approaches are optimised for sensor network applications in which the accuracy of sensed data collected is important, whereas in our work, coarse-grained data is sufficient for data processing. ARTS is built with the goal to efficiently extract rules that can be used to power-save sensors in a timely manner. The target application areas of ARTS are in sensing environments where data aggregation is possible and where coarse-granularity of data is acceptable.

3 ARTS Algorithm  3.1 ARTS: Data Representation  Our initial aim is to mine for correlations in sensor data.

In our definition, a data correlation in sensor networks is a statistical interdependence between any variable sensor val- ues in {a1, a2, . . . , an} where ai, (i = 1. . . n), can be any possible sensed data sample for sensors S1, S2, . . . , Sn. Let us assume we have sensors S1, S2, S3 collecting sound s, temperature t and light l samples. The light reading l1 of S1 might be similar to l2 of S2 because they are detecting readings in the same area. Thus, they would be strongly positively correlated.

Typically, data sampled such as t1 or l1 is continuous data and would arrive in a random manner. As sensors are resource-constrained with limited processing capabili- ties, it is infeasible for sensors to perform intensive com- putations on raw sensor data. To generate rules relevant for our purpose, we choose to consider only discrete sen- sor values, where the range is pre-defined. For example, for a light reading in [0, 1000], we assign states ?L? for read- ings in range [0, 299], ?M? for readings in range [300, 699] and ?H? for readings in range [700, 1000]. The arrival time of sensor data value sets is normally random (Refer Fig- ure 1). Thus, we model data transactions as data value sets {Sn; tn, sn, ln, time} in packet format, where time can be the number of minutes from a landmark time or the actual time that it arrives from a sensor (Refer Figure 1) 1.

Figure 1. Sensor Transactions  A direct application of APRIORI on sensors would re- quire the generation of k itemsets from the k ? 1 item-  1Data arrival rate figure is derived from [11]  sets that are frequent, creating too many rules, which is infeasible for sensor nodes. Instead, in our work, we pro- pose to count the number of transactions that are frequent, omitting the generation of any k itemsets and also concen- trate on only highly correlated rules. This is contrary to the weighted transformation method as used in [11] where itemsets are generated. We assume that transactions are pro- cessed in batches b1, b2, . . . , bx where 1 < x < k and k < number of transactions in bx, and that x must also be suffi- ciently large as transactions cannot occur with equal proba- bilities (i.e. each transaction of support 1).

With reference to the table in Figure 1, for a batch bx, the support of any transaction n, with all elements considered, is simply:  support(n) := total number of transactions with n in bx /total number of transactions in bx.

Subsequently, the confidence of a rule, for instance, (S1temperature ? S1light) i.e., an ? an?1, generated from a transaction over a user-defined support is given by  confidence(an, an?1) := support(an, an?1)/support(an)  The confidence measure allows us to trigger rules as long as the premises hold. For instance, if we have a high confidence for a rule stating that an implies an?1, the rule is extracted and we send only reading an to the base- station/central node. Upon receiving the reading an and utilising knowledge of the rule, the reading of an?1 can be inferred.

3.2 ARTS: Algorithm  In this section, we describe our algorithm, ARTS for mining rules from sensor data packets arriving at node M or at cluster heads. In our later experiments, the rules dis- covered are then used for cluster heads to infer readings of their neighbours and control cluster members? operations.

3.2.1 Rule Mining  ARTS is shown in Algorithm 1 and 2. We first explain each step of the rule mining process in detail:  Step 1 S is made up of heterogeneous/homogeneous sensors as in ?4 that form a group. Transaction batches are collected at a sensor node. Each sensor in S has one or more sensor attributes in A.

Step 2 Each sensor in S has a finite amount of energy that can be derived from packets the sensors send. Thus, e     Input: Transaction batch bn Output: Rule r begin  1. Let S = s1, s2, ..., sn be the set of sensors operating in a group, and A = a1, a2, ..., ak be the set of sensor attributes, and thus, sensor attribute pairs SA = s1a1, s1a2, ..., snak.

2. Obtain energy levels of sensors in S and sort them in ascending order of energy levels, e ? A : EnergyS = s1e, s2e, ..., sne.

Sorted Energy list, EnergySSort = ssort1 , ssort2 , ..., ssortn .

3. Generate Covariance matrix for batch bn.

4. Pick two sensors initially based on the probability measure.

5. Transpose continuous transaction values to discrete forms.

6. Record transaction and count into frequentItems, frequetItemsCount lists.

7. for i = 1 to th do currentSupport = transCounti/h if currentSupport > maxSupport then maxSupport = currentSupport  end 8. if highestSupport >= thresholdSupport then Get most frequent transaction in list GenerateRules(frequentT ransaction) else if Number of bits set > 2 then if all bits set then Reset all bits to 0 else Remove one bit reflecting current highest correlation in matrix  9. Add generated rule with unique rule id to ruleQueue.

end  Algorithm 1: ARTS Algorithm: Miner  is in the attribute set A. We assume that we dynamically update an energy map with new sensor readings.

Step 3 For batch bn, generate the coefficient matrix M for a maximum of n ? k rows by n ? k columns for sensors and their attributes. The correlation is calculated based on the current numerical values of the sensor attribute values.

The equation used is:  Correlation Coefficient(x,y) = (h*SUM(x*y) - SUM(x)*SUM(y))/ (SQRT(h*SUM(x?2)-SUM(X)?2) * SQRT(h*SUM(y?2)-SUM(y?2))  where SUM(x) refers to summation of all transaction values in bn for attribute x, and x in SA.

Step 4 When counting transactions, we only like to con- sider attribute combinations that contain sensors with high correlation values in M and equal combination of sensors with low/high energy in EnergySsort, the sorted energy list for the sensors.

Probability of two selected sensor = Abs(MAXENERGY-energyValue(sensorA*2) /MAXENERGY)  * highest correlationCoeff between two sensors  The equation enables us to choose sensors with the biggest difference in their energy levels and highest corre- lation coefficient.

The logic in choosing a transaction combination with the biggest contrast in energy levels is so that rules that are de- rived from the algorithm would form a trigger that could maximise the lifespan of our sensor network. In general, the rule with the lowest number of high-energy sensors and highest number of low energy sensors has the best combi- nation. For example, in a rule that has S1&S2 ? S3, S3?s reading is being implied by both S1 and S2?s readings. S1 and S2 are controlling sensors and thus will consume en- ergy whereas S3?s energy is conserved. Thus, if S3 is a dying sensor/low resources, we minimise its energy use and extend the life of the network. Preference is hence given to such combinations.

On the other hand, we are also interested in not just any rule derived from the algorithm. We generally want rules with a high support and confidence, i.e. given A&B ? C, a high likelihood of C, given A and B. In the same way as the logic above, this is so that the trigger to be generated can rely on S1 and S2?s reading to predict S3. Pearson?s product moment correlation coefficient gives us the measure of tendency of the variables to increase or decrease together.

The pre-processing of transactions in this manner increases the quality of the rules obtained.

Given k is the maximum number of attributes that any sensor in S would have, we initialise k number of bits to zero, and transaction attributes with bits set to zero won?t be regarding in the counting and vice versa.

At initialisation, the algorithm would need to have at least two highly correlated attributes and sensors that come from both end of the energy spectrum.

Subsequently, extra bits that would be added would need to have a high correlation coefficient with bits existing in the bits array. The bits array is reset to two again when all bits have been set.

Step 5 Numerical values of the sensor attributes are re- quired at the pre-processing step of the algorithm. Follow- ing the pre-processing, we convert all numerical values for individual sensors to discrete values so as to generate rules in value ranges only and to reduce complexity of the algo- rithm.

Step 6 We maintain a list of frequentItems and frequentItemsCount. This list is updated for every transaction in the batch. The frequentItems list stores the most frequent transactions in order and their corre- sponding counts in frequentItemsCount. The sizes of frequentItems list and frequentItemsCounts list are user-defined. frequentItems can contain a user-defined number of items but with transaction size >= 2 and <= k.

Step 7 We get the highest support from transactions al- ready in list.

Step 8 We check if the current transaction in the batch has a support greater than the threshold. Note that sup- port is calculated from using the frequentItems list and frequentItemsCount list. We maintain a record of all transactions with support greater than threshold. If the sup- port within the batch is greater than the threshold, generate the rules from this transaction. Set one more bit in the bits array in accordance with the probability measure. The ra- tionale for doing this is so that rules that will be generated next will involve more sensors in the same grouping that has met the threshold to conserve more energy.

Step 9 After the rule is generated, it is added to a rule- Queue if it is above the threshold confidence. The rule- Queue is served periodically per user-defined intervals. Pre- liminary, we can rank the rules using the confidence of the rule. Out of our rules, we then create a hash table for the list of sensors to monitor with their expected values.

3.2.2 Rule Triggering  Rules in the ruleQueue are processed in the following way: Step 1 The rule that is obtained from the mining algo-  rithm is made up of two integral parts to rule triggering, i.e.

Rule antecedents and rule consequents. Rule antecedents are the sensors and their values, which we store to infer consequent sensors? values when the monitoring values are present. The same rule id is used for both the monitorList and triggerList to reference the rule being used at any time.

A simple conflict resolution strategy that we used in the ARTS algorithm in triggering is that, for a new rule, if the antecedent of the new rule is the consequent of earlier active rules before it, it won?t be made active.

Step 2 At runtime, we periodically call a sensor routine  Input: Rule Output: Rule Trigger begin  1. Divide rule into two parts with same rule id, if rule is valid.

2. Update monitorList with antecedent sensors.

3. Update triggerList with consequent sensors.

4. Set rule to ACTIVE state for trigger on next timer fire.

end  Algorithm 2: ARTS Algorithm: Trigger  to pick k random readings from the current batch to com- pare sensors? readings with their expected values according to the rule list. If the values of any of the antecedent sensors change, we refer back to the rule, invalidate the rule active status and deactivate it to return the consequent sensor(s) to regular operation. The number of error predictions we can tolerate is batchSize x (1 ? thresholdConfidence).

Step 3 The triggerList is a list that stores the current rule id, the status of the rule trigger(ACTIVE/INACTIVE) and the consequent sensors to trigger. At each periodic timer fire, an ACTIVE rule will be triggered by a sendMsg mes- sage from the cluster head to the consequent sensors.

Step 4 Currently, in our implementation, we utilise the trigger message to command sensors to choose the data type of future messages. This is formed by different permuta- tions of the default message type. For instance, if a sensor would send only light and temperature readings, the varia- tions of this are: (i) Send only light reading (ii) Send only temperature reading (iii) Send none (iv) Send all. In the future, we hope to experiment how triggers can be used to reduce data transmissions when answering sensor queries.

4 Sensor Network Organisation  In this section, we discuss the two types of sensor net- work organisations and the network requirements in which ARTS would work.

4.1 A Heterogeneous Sensor Network  A combination of heterogeneous sensors can operate in groups. With reference to Figure 2, other than performing the learning algorithm on sensors individually, we can del- egate control sensors with more energy/processing power to mine for inter-streams association among sensors. This hierarchy of roles in a sensor network would enable us to power-save nodes while the basic sensing operations are still preserved in different sensor patches or arranged sensor groups. In Figure 2, ? refers to sensors being controlled and ? refers to sensors used to determine control information.

R oles ch  an ge  M or  e e  ne rg  y sa  ve d  a s  th e  n o  . o f s  e n  so rs  w ith  su ch  r ol  es in  cr e  a se  s  Less Resourceful (e.g. mica2dot)  More Resourceful (e.g. mica2)  G1 LYD--P 1 G2 LYB?P 2  .

.

-----------  [G1P1G2P1,3]  G1  G2  G3  M  Serial  Sleep  Figure 2. Heterogeneous Sensor Network  Illustrated in Figure 2, node M would collect readings from sensor groups G1, G2 and G3. M , in this instance, can be connection to a PDA with more computing resources than a sensor. In M ?s memory, a hashtable is maintained where every unique transaction for example, LLL is de- noted by, P1, and a separate transaction table which is up- dated per x units of time, where x is the highest periodicity of the data stream from all currently recorded sensors. To determine the periodicity x, a naive approach we can apply is to let the program initialise itself in the first n seconds runtime and record the shortest time of arrivals of transac- tions.

4.2 A Homogeneous Sensor Network  Alternatively, ARTS can operate on a homogeneous sen- sor network with the aid of clustering protocols such as HEED. One characteristic of HEED that makes it useful for ARTS is that it allows load balancing among sensor nodes, given that all nodes carry equal processing capabilities and energy levels. This is done by HEED?s periodic reclustering step that switches cluster heads when the residual energy is low.

The switching of nodes when they have low energy is an important feature. If we assume that the same cluster head is responsible for aggregating and routing data over a long period of time, the sensor network lifetime will be shortened. On another note, the running of ARTS on cluster heads managing smaller groups of sensors implies that the amount of stream data that arrives at cluster heads will not grow exponentially, i.e. the running of 1000 nodes would mean running ARTS on possibly 15-20 cluster heads.

5 ARTS Implementation and Results  ARTS has been implemented in TinyOS code, where iHEED [17] cluster nodes are modified to direct messages to  ARTS when they are elected as cluster heads. ARTS is pre- programmed into all nodes that run iHEED. By default, the sensing information that iHEED nodes send is light read- ing. Consequently, the trigger message that is used in the following TOSSIM run commands a node not to send the their light information when the related rule is true. By us- ing the rules discovered, the aggregate information for the consequent sensors are replaced by an approximate value for the discrete state. In this instance, the light value states low, medium, high translates to approximate values of 250, 500, 750 respectively.

5.1 Performance  In this experiment and the following experiments, we use the Credit-Point System (CREP) as discussed in [17] as the energy metric to determine the total network energy residue of sensor nodes when they are running iHEED with/without ARTS. All experiments were done in TOSSIM, and the times recorded reflect the intervals in which the nodes have been programmed to fire their timers.

12000 14000 16000 18000 20000 22000 24000  S im  ul at  io n  tim e  x1  m  s  Residual energy in x10^3 credit points  Residual energy plot for running iHEED with/without ARTS  "heed_50" "heedwARTS_50Conf0.5" "heedwARTS_50Conf0.7" "heedwARTS_50Conf0.9"  Figure 3. TOSSIM run with different Confi- dence levels  Figure 3 shows a run of ARTS using different confi- dence thresholds ranging from 0.5 to 0.9, with 50 nodes.

At time=140, we note that an energy savings of around 15% is achieved through using ARTS. However, given the trend observed in Figure 3 i.e. the graph showing the use of HEED and the graph using HEED with ARTS diverges as time passes, we expect that for a long enough time, fur- ther significant savings can be achieved. From observation, the results show that on any of the confidence thresholds, HEED cluster heads running ARTS outperforms the nodes that run with just using HEED. We also note from the re- sults, that running ARTS with lower confidence does not necessarily imply that more energy can be conserved (for instance, compare plot using 0.5 threshold vs. 0.7 thresh-     old). The reason is because if a lower confidence is used, there is a higher error rate in the packets and the node will have to communicate more trigger messages to controlled nodes to rapidly switch them on/off (Refer algorithm).

5.2 Overhead  To measure the amount of overhead in using ARTS, we ran a similar experiment with 50 nodes, using a 0.75 con- fidence threshold for ARTS. Figure 4 shows that the over- head while using ARTS increases only marginally over the default iHEED (at time=140, the overhead is 0.06%), con- sidering the amount of energy saved in Figure 3 when we used a 0.7 confidence threshold.

0  20  40  60  80  100  120  140  160  R es  id ua  l e ne  rg y  in x  ^3  c re  di t p  oi nt  s  Simulation time x10000ms  Overhead Energy Consumed for running iHEED with/without ARTS  "heed_50" "heedwARTS_50Conf0.75"  Figure 4. TOSSIM run with 50 nodes  5.3 Scalability  Figure 5 serve to test the scalability of the algorithm when it is run on a higher number of nodes, with both ex- perimental runs using a confidence threshold of 0.75. In this figure, we also observe that as we increase the number of nodes that we use, we see a linear increase in the amount of energy conserved.

5.4 Accuracy  We refer to our previous study for the accuracy rates of the rules generated. In [5], we performed a PC simulation run of the ARTS algorithm, assuming that we have a node M that collects light, temperature and microphone readings from three other sensor streams coming from sensors S0, S1 and S2. The synthetic data that we have generated has the attribute that: (1) S0 light readings and S1 light readings have a positive correlation of 0.8 +/- 0.04 (2) S1 light read- ings and S1 temperature readings have a positive correla- tion of 0.8 +/- 0.04 (3) S2 light readings and S2 temperature           25000 30000 35000 40000 45000 50000  S im  ul at  io n  tim e  x1  m  s  Residual energy in x10^3 credit points  Residual energy plot for running iHEED with/without ARTS  "heed_100" "heedwARTS_100"  Figure 5. TOSSIM run with 100 nodes  readings have a negative correlation of -0.8 +/- 0.04. Table 1 shows the rules obtained when ARTS run on this dataset for 8 minutes. The column on success rate shows the percent- age of packets that have been correctly predicted, knowing the rule value of the rule antecedent.

Discovered Rules  Time ID Rules Conf. SuccessRate% 1 R1 S1L[H ] ? S0L[H ] 0.83 82.9% (34/41) 1 R2 S0L[H ] ? S1L[H ] 1.0 100% (34/34) 3 R3 S1L[H ] ? S0L[H ] 0.83 82.9% 3 R4 S0L[H ] ? S1L[H ] 1.0 100% 3 R5 S0M [Y ] ? S0L[L] 0.5 45.3% (24/53) 3 R6 S0L[L] ? S0M [Y ] 1.0 85.7% (24/28) 5 R7 S1T [L] ? S0L[H ] 1.0 100% (20/20) 5 R8 S0L[H ] ? S1T [L] 0.73 58.9% (20/34) 7 R9 S1T [L] ? S0L[H ] 1.0 100% 7 R10 S0L[H ] ? S1T [L] 0.64 58.9%  Table 1. Table of Rules Discovered in PC Sim- ulation Run  6 Conclusion  We have presented in this paper a rule learning al- gorithm, ARTS, to mine for highly correlated rules in resource-constrained sensor networks. We demonstrated, through our experiments on the Berkeley mote platform, that HEED with ARTS perform favourably over HEED alone in any of the TOSSIM runs. Our results above show that the total lifetime of a sensor network using HEED is improved by using ARTS on the created cluster heads. The contributing factors to the improvement in sensor lifetime by using ARTS are:  1. Reduced reclustering frequency of nodes due to the over-     all extended sensor network lifetime.

2. Reduced number of transmissions from cluster members to their respective cluster heads, by allowing the use of approximated aggregate data values.

ARTS exploits the fact that coarse-granularity data is sufficient for many non-critical sensing applications, in or- der to provide an extremely low overhead technique for significant energy conservation. ARTS can also be used in other clustering algorithms such as LEACH (Low En- ergy Adaptive Clustering Hierarchy) [9] because ARTS is designed to complement such existing approaches by the application of rules, rather than altering their operations to conserve energy. ARTS looks at a selected subset of the sensor transactions, so that the high computational cost of rule learning can be significantly minimised and yet, obtain rules useful for triggering to conserve energy for a wireless sensor network. The overhead of our rule learning process is minimal(0.06%) compared to the reclustering overhead, while energy is shown to be conserved for a 100 nodes sen- sor network.

