Mining Association Rules with Linguistic Terms ?

Abstract  Some problems of mining association rules with  linguistic terms are discussed. First, an incremental  updating algorithm of association rules with linguistic  terms is presented. The collection of frequent  linguistic attribute sets and its negative border along  with their support count are maintained, which makes  scan the entire database once at most in the process of  updating association rules. The experiment shows that  the updating algorithm can not only update  association rules effectively but also avoid the  repeated cost. Secondly, the parallel algorithm for  mining association rules with linguistic terms is  presented. The Boolean parallel mining algorithm is  improved to discover frequent linguistic attribute sets,  and the association rules with at least confidence are  generated on all processors. This parallel mining  algorithm has fine scaleup, sizeup and speedup.

Keywords: data mining; association rules; linguistic  terms; parallel; incremental updating algorithm  1. Introduction  The mining of association rules is one of the most  important issues in the field of data mining. The problem  of mining Boolean association rules is introduced in [1].

Many known algorithms, such as Apriori[2] and DHP[3],  have been presented. The problem of mining quantitative  association rules is introduced in [4]. The algorithm finds  association rules by partitioning the attribute domain, and  then transforming the problem into binary one. Although  this algorithm can solve problems introduced by  quantitative attributes, it introduces some other problems.

First, the equi-depth partition cannot embody the actual  distribution of the data. On the one hand, it may not work  very well on highly skewed data and tends to split  adjacent values with high support into separate intervals  though their behavior would typically be similar. On the  other hand, it is not easy to distinguish the degree of  membership. Secondly, it causes the sharp partition  boundary problem. Ref. [5] uses fuzzy set to soften  partition boundary, and presents the concept of fuzzy  association rules. Ref. [6] uses fuzzy cluster algorithm to  partition the domain of quantitative attribute into several  linguistic terms, then the problem of mining association  rules with linguistic terms is introduced by combining  linguistic terms. Ref. [7] improves the work in [6] further,  and presents the algorithm that can fit for the large  database. Ref. [8] integrates fuzzy set concepts and the  priori mining algorithm to find fuzzy similar association  rules in given transaction data sets. Ref. [9] proposes a  mining approach for extracting linguistic weighted  association rules from quantitative transactions. Ref. [10]  discovers fuzzy association rules over the extended  database with filled predictive values.

In this paper, some problems of mining association rules  with linguistic terms are discussed. First, an incremental  updating algorithm of association rules with linguistic  terms is presented, this updating algorithm can not only  update association rules effectively but also avoid the  repeated cost. Secondly, the parallel algorithm for mining  association rules with linguistic terms is presented, this  parallel mining algorithm has fine scaleup, sizeup and  speedup.

2. Mining association rules with linguistic terms  Let },,,{ 21 ntttT =  be a relational database, jt  represents the jth record. Let },,,{ 21 miiiI =  be the attribute set, ][ kj it  represents the value of the j  th  record in attribute ki . For mining association rules  with linguistic terms, values of the record in attribute      are first partitioned into several linguistic terms by  fuzzy c-means (FCM) algorithm[11]. Then, a new  database is constructed by the original database T. In  the new database, attributes are linguistic terms, so  attributes are called as linguistic attributes. Values of  the record in attributes are obtained as follows. For  example, let 1ki  be a linguistic term of attribute ki ,  ki  is an attribute in the new database. Value of the jth  record in attribute 1ki  is ])[(  kjk iti , ])[(  kjk iti  is a  membership degree.

Let I  still be the linguistic attribute set, let )( kj yt  still  represent the value of the jth record in linguistic attribute  ky . Let IyyyX p ?= },,,{ 21 , IyyyY qppp ?= +++ },,,{ 21 ,  YX ? =?. An association rules with linguistic terms is an implication of the form YX . The support of X is  defined as n  yt  XSup  n  j  p  m mj?  = == 1 1 )(  )( , where ? = =  n  j  p  m  mj yt 1 1  )(  is  called as the support count of X. Linguistic attribute sets  with at least a minimum support are called frequent  linguistic attribute sets. The support of YX  is defined  as n  yt  Sup  n  j  qp  m mj?  =  +  == 1 1 )(  , the confidence of YX  is defined  as )(XSup  Sup Conf = . Because )( kj yt  falls in [0,1], we can  know that all subsets of a frequent linguistic attribute set  must also be frequent, so we can easily modify Apriori  algorithm to mine association rules with linguistic terms.

Further details can be obtained in [7].

3. An incremental update algorithm for  association rules with linguistic terms  In practice, the database is updated frequently rather  than static. After being inserted, deleted and updated, new  association rules will present and some others will  become inapplicable. Ref. [12] introduces the problem of  maintaining Boolean association rules and gives the FUP  algorithm that can maintain association rules when new  transactions add to a database. Ref. [13] proposes a more  general algorithm FUP2, which can maintain Boolean  association rules when some records are inserted, deleted  or updated. Ref. [14] uses negative borders to maintain  rules. In this section, an incremental update algorithm for  association rules with linguistic terms is presented.

3.1 An incremental update algorithm  In order to find association rules with linguistic terms in the  updated database, a naive way is directly to run mining  algorithm again. Since there exist much unnecessary repeated  computing cost, this method is inefficient. Because association  rules with linguistic terms can be directly generated from  frequent linguistic attribute sets, the rule maintenance problem  becomes the problem of maintaining frequent linguistic  attribute sets. Next, we only consider the insert condition; the  deletion condition is similar to the case.

When new records add to the database, some frequent  linguistic attribute sets can become infrequent. Similarly,  some infrequent linguistic attribute sets can become  frequent. In order to solve the update problem efficiently,  the membership degree of new records in linguistic terms  should be known first. One approach is using FCM  algorithm to cluster again. But when the volume of data is  very huge, this approach will cost much time. Because the  update problem usually assumes that only few new  records add to the database, we can consider that the  cluster centers iv  will not change approximately. So we  can compute the membership value directly by the  equation ciddu c  k  kjijij ,,2,1,)/(/1  2 == =  [11].

Definition 1. The negative border )(FLNBd  of a  collection of linguistic attribute sets FL is defined as  follows. Given a collection )(FPFL ?  of linguistic attribute sets, closed with respect to the set inclusion  relation, in which F is the collection of all linguistic  attributes and )(FP denotes the subset space of F. The  negative border )(FLNBd  of FL consists of the minimal  linguistic attribute sets FX ?  not in FL.

In the rest of this section, DB denotes the original  database, db denotes the records that newly add and DB+  denotes the updated database. Also DBFL , dbFL  and +DBFL  denotes the collection of frequent linguistic  attribute sets and )( DBFLNBd , )( dbFLNBd  and  )( +DBFLNBd  denotes the negative border of the original  database, incremental database and the updated database  respectively. Next, we maintain the collection of frequent  linguistic attribute sets and its negative border along with  their support count in the database.

On one hand we check if there exist some old frequent  linguistic attribute sets to become infrequent. Let DBFLf ? , since we know the support count for f in DB  and db, the support count for f in dbDB ?  can be easily obtained. If f does not have minimum support in  dbDB ? , then it is removed from DBFL .

On the other hand some new linguistic attribute sets  may become frequent in the updated database. There exist  two conditions.

(1) If none of the linguistic attribute set in )( DBFLNBd  gets the minimum support, no new linguistic attribute set  will add to +DBFL .

(2) If attribute set in )( DBFLNBd  gets the minimum  support, move it to +DBFL  and recompute the negative  border. If )()( DBDBDBDB FLNBdFLFLNBdFL ??? ++ , we  have to find the negative border closure of +DBFL  and  scan the whole database (DB) once to find the updated  frequent linguistic attribute set and its negative border.

Algorithm 1. An incremental update algorithm for  frequent linguistic attribute sets  Input: the original collection of frequent linguistic attribute sets and its negative border along with their support count; cluster centers of each quantitative attribute; db; minimum support  Output: updated frequent linguistic attribute sets and its negative border  Methods: (1) Compute membership values of each record in db  according to the original fuzzy cluster centers, and then compute the frequent linguistic attribute sets in db.

(2) Compute support count of the original frequent linguistic attribute sets and its negative border in db.

(3) Compute the total support count of the original frequent linguistic attribute sets and its negative border in DB?db. Check if they have minimum support, add them to updated frequent linguistic attribute sets.

(4) For each frequent linguistic attribute set in db, if it is presented in original negative border, check its total support. If it has minimum support, add it to updated frequent linguistic attribute sets.

(5) Compare +DBFL  with DBFL , if not equal, recompute the  updated negative border, else terminates the algorithm.

(6) If )()( DBDBDBDB FLNBdFLFLNBdFL ??? ++ , re- compute the closure of updated negative border, and scan the  whole database once to obtain the final updated frequent  linguistic attribute set and its negative border.

3.2. Experimental analysis  Experimental database Abalone is taken from UCI  Machine Learning Repository, which has 4177 instances.

The quantitative attributes including length, diameter,  height, whole weight, shucked weight, viscera weight, shell  weight and rings are respectively denoted as I1?I8. To  simplify, we mine the association rules with consequent I8.

The condition of other consequents can be discussed in the  same way. Quantitative attributes are partitioned into three  linguistic terms: large, middle and small, which are denoted  as integer 1, 2 and 3. Each element in rules is represented by  <attribute: linguistic term> such as <I1: 1>. We select 4000  records to construct the original database, 177 records  construct the incremental database db. We do the experiment  at PII 233MHZ with 32M RAM. Set minimum support to be  0.22 and minimum confidence to be 0.5, total 11 association  rules listed in table 1 are obtained using incremental update  method and mining the whole database respectively. These  results are equal. The former 10 rules are presented in the  original database and the last rule is an added rule. These  results show that incremental update method can discovery  the updated association rules. The time for mining in the  whole database is 389 seconds (including the cost of the  FCM algorithm), while the incremental update method only  use 2 seconds. Let the minimum confidence be 0.5, table 2  shows the times of scanning the whole database at the  different minimum support. These results show that  incremental update method can mine the updated association  rules with linguistic terms. In addition, it can avoid the  repeated cost introduced by doing mining task directly in the  whole database.

Table 1.  Association rules with linguistic terms  Incremental update mining Mining in the whole database  <I1:1> => <I8:2> <I1:1> => <I8:2>  <I2:1> => <I8:2> <I2:1> => <I8:2>  <I3:2> => <I8:2> <I3:2> => <I8:2>  <I4:2 >=> <I8:2> <I4:2 >=> <I8:2>  <I4:3> => <I8:3> <I4:3> => <I8:3>  <I5:3> => <I8:3> <I5:3> => <I8:3>  <I6:2> => <I8:2> <I6:2> => <I8:2>  <I6:3> => <I8:3> <I6:3> => <I8:3>  <I7:2> => <I8:2> <I7:2> => <I8:2>  <I7:3> => <I8:3> <I7:3> => <I8:3>  <I1:1> and <I2:1> => <I8:2> <I1:1> and <I2:1> => <I8:2>  Table 2.  The times of scanning the whole database  MinSup Incremental update  mining  Mining in the whole  database  0.20 1 4  0.10 1 6  0.07 0 8  0.05 0 8  0.03 1 8  0.01 0 8  4. Parallel mining association rules with  linguistic terms  In order to mine association rules with linguistic terms  on the distributed linked PC/workstation, we first use  parallel fuzzy c-means (PFCM) algorithm[16] to partition  the quantitative attributes. Then, we adopt the similar idea  of the Count Distribution algorithm[15] to design the  parallel algorithm.

4.1. Parallel mining algorithm  The key step for generating frequent linguistic attribute  sets is obtaining the whole support count by exchanging  the local support count of the candidate linguistic  attribute sets. First, each process obtains local support  count of the candidate linguistic attribute sets from hash  tree asynchronously and put into the array LcntArr.

Secondly, each process aggregates to compute  components of the array by function  MPI_Reduce_Scatter () and broadcasts them to each  process, which only receives the whole support count      with respect to its partition. Third, each process gathers  the whole support count of the candidate linguistic  attribute sets to the array GcntArr by function  MPI_AllGatherv (). The parallel algorithm based on the  message passing interface (MPI) is shown in Algorithm 2.

Algorithm 2. The parallel mining algorithm  Input: Subset of data iD ),,2,1( si = ; minsup; minconf.

Output: Association rules with linguistic terms (ARLTs).

/* Parameter: nproc (the number of process), myid (process  label), mysize (data size of the subset), pPMat (partial  partition matrix), pARLTs (partial association rules with  linguistic terms) */  Methods:  {  MPI_Init (&argc,&argv);  MPI_Comm_size (MPI_COMM_WORLD, &nproc);  MPI_Comm_rank (MPI_COMM_WORLD, &myid);  Data_input (Di);  MPI_Barrier (MPI_COMM_WORLD);  If (myid==0)  startwtime=MPI_Wtime ();  for (cl=0; cl<COLUMN; cl++)  /* Clustering by PFCM */  {  PFCM (Di, mysize, cl, pPMat[cl]);  }  /* Construct a new database */  Transform (pPMat);  istree=ist_create (m, minsup, minconf);  /* Discover frequent linguistic attribute sets */  do {  for (i=0;i<mysize;i++)  ist_count (istree, pPMat, mysize, i);  extract_count (istree, LCntArr);  MPI_Reduce_scatter (LCntArr, PartGCntArr, PartSize,  MPI_SUM, MPI_COMM_WORLD);  MPI_Allgatherv (PartGCntArr, PartSize[myid], GCntArr,  PartSize, disp, MPI_COMM_WORLD);  Writeback_count (GCntArr, istree);  l=ist_addlvl (istree);  } while (l==0);  /* Generating association rules with linguistic terms */  Gen_rule (istree, pARLTs);  If (myid==0) {  MPI_Gather (pARLTs, ARLTs, MPI_COMM_WORLD);  endwtime = MPI_Wtime ();  printf ("wall clock time = %f\n",endwtime-startwtime);  }  MPI_Finalize ();  If (myid==0){  Return ARLTs;  }  }  4.2. Experimental analysis  We implemented our parallel algorithm for mining  association rules with linguistic terms on the distributed  linked PC/workstation. This workstation consists of six  computers with 128M RAM, which are interconnected  via a 10M/100M hub. We use the parallel message  passing software MPICH1.2.4.

The experiment is implemented on an abalone dataset  from UCI machine learning. We first copy abalone  dataset 32 times, and obtain a 5.72MB dataset with  133,664 records, which is regarded as the initial dataset.

To see how well our parallel algorithm handles large  dataset when more computers are available, we perform  scaleup experiments where the dataset is copied from the  initial dataset in direct proportion to the number of the  computers in the workstation. The number of maximal  computers is set to 6.  In the experiment, attributes are  partitioned into three fuzzy sets by PFCM algorithm. Let  minimum fuzzy support be 0.01, let minimum fuzzy  confidence be 0.1. The performance results of scaleup are  shown in figure 1. In addition to the absolute response times  as the number of computers is increased, figure 2 also plots  scaleup, which is the response time normalized with respect  to the response time for a single computer.   pfarm curve is  our parallel algorithm, another curve is  ideal condition. In  our parallel algorithm, we can also obtain fine performance  of sizeup and speedup. Because this paper is limited to 5  pages, we donot show more results.

5. Conclusions  Some problems of mining association rules with  linguistic terms are discussed. First, an incremental  updating algorithm of association rules with linguistic  terms is presented. The collection of frequent linguistic  attribute sets and its negative border along with their  support count are maintained, which makes scan the  entire database once at most in the process of updating  association rules. Secondly, the parallel algorithm for  mining association rules with linguistic terms is  presented. The Boolean parallel mining algorithm is  improved to discover frequent linguistic attribute sets,  and the association rules with at least confidence are  generated on all processors. In our parallel algorithm, the  real memory of every computer must save all candidates  fuzzy attribute sets. When the number of quantitative  attributes is increasing, our parallel algorithm may be  ineffective. How to mine the fuzzy association rules in the  database with lots of quantitative attributes is a further  work.

