Evaluation of Filtration and Pruning Approach for Apriori Algorithm

Abstract ? One of the Association rule mining (ARM) algorithm,  Apriori, is most popular algorithm. Pruning approach used in this  algorithm differentiates between potential frequent and infrequent  itemset well before verifying them in the given database. An  alternate approach known as filtration does the same. In this paper,  five experiments are carried out to prove that filtration approach  works as efficient as Apriori?s pruning approach but it requires an  extra data structure.

Keywords ? Data mining; Association Rule Mining; Frequent  itemset; Apriori algorithm.



I.  INTRODUCTION  Every organization deals with the saleable items in their  databases. Association Rule Mining (ARM), one of the data  mining [3] technique, identifies unusual and interesting  associations among two or more than two items. Association  rules helps in marketing decisions to make business more  profitable.

Many association rule generating algorithms are available in  literature, namely, Apriori, FP-Growth and Elcat etc. but  Apriori algorithm is voted highest in use by research  community due to its simplicity [9]. Apriori algorithm uses  three steps iteratively. In the first step, candidate itemsets are  generated; in the second step, candidate itemsets are pruned to  generate potential itemsets and in the third step, database is  scanned to check frequentness of the potential itemsets.

A filtration approach [2] exists in the literature as an alternate  to the pruning approach of Apriori algorithm. Both approaches  have the same objective, i.e., they are used to remove candidate  itemsets whose candidature is certain to be infrequent without  scanning the database.

In this work, we carried out five experiments and observed that  filtration approach is as efficient as apriori?s pruning approach  but it requires an extra data structure.

This paper is organized as follows. Section I.A describes the  problem statement. Section II nd  summarizes the related work in  this field. In section III rd  , Apriori?s pruning approach and  filtration approach is compared with an example and both  approaches are evaluated on standard data sets.  In the last  section, conclusion and future directions are stated.

A. Problem Statement  The problem of mining association rules comprises two phases.

In the first phase, itemsets which satisfy the given minimum  support are generated and in the second phase, association rules  which satisfy the given minimum confidence are identified.

The following is a formal statement of the association rule  mining problem to be solved. Let, a database, ? ?  ??1, ?2, ? , ? ?, be a set containing n transactions where each transaction have items from a set of items. A subset of items, ?, containing k numbers of items is known as k-itemset. Itemset ? is said to be frequent when all its k items exist in the minimum  number of transactions called minimum support (minsup). The  scope of this paper is limited to list all the frequent itemsets. In  other words, it is limited to first phase only.



II. RELATED WORK  In the field of ARM, Agarwal and Srikant [1] proposed most  popular algorithm, called Apriori. It is a three steps process:  frequent itemsets joining, candidate itemsets pruning and  potential itemsets scanning and counting. In the first step,  particular pairs of k-frequent itemsets are joined to generate  k+1-candidate itemsets; in the second step, k+1-candidate  itemsets are pruned to generate k+1-potential itemsets and in  the last step, potential k+1-frequent itemsets are scanned and  their support is counted in the database against the given  minimum support value. These steps are iterated until large k-  frequent itemsets are generated. The same pruning approach is  used by Mannila et al [4].

An algorithm, namely DHP, used hashing technique to improve  the Apriori algorithm [6]. Alternately, Mueller [5] introduced  prefix tree to improve the Apriori algortihm.

A probabilistic approach to discover frequent itemsets is given  by [7, 8] uses the concept of recursive median. It partitions the  frequent itemsets into two categories before the start of first  step of Apriori algorithm.

Modified Apriori algorithm exists in the literature [2]. This  algorithm replaced the pruning approach used by Apriori  algorithm with a new pruning approach known as ?filtration?.

In other words, an alternate approach to second step of Apriori  algorithm is presented. Following lemmas differentiate between  pruning step of [1] and filtration step of [2]. Lemma 1 explains  the pruning approach and Lemma 2 explains the filtration  approach.

Lemma 1: if any itemset Z is a candidate k-itemset and at least  one of its k-1-subsets are found infrequent then Z will be  infrequent.

Proof:  Let ?, ?, ?, . . . , ? be all k-1-subsets for candidate k- itemset Z.  Moreover, let, ? be an infrequent itemset, i.e., ?????????? ?  ?? ??? ,  ?? 1 ! ? ! ". (1) It is obvious that support of candidate itemset Z can?t be greater  than itemset ?, i.e.,      ????????#?  !  ?    (2) Using associative property from equation ?(1)? and ?(2)? it can  be stated that:- ????????#? ?  ?? ???          (3) In other words, itemset Z will be infrequent if at least one of its  k-1-subsets is found infrequent.?  Lemma 2: if any itemset Z is infrequent then none of it superset  can be frequent.

Proof:  Let Y be an itemset containing all the items of itemset Z  and number of items in set Y is more than number of items in  the itemset Z, i.e., Z ? Y. As if Z is infrequent, it can be stated that:-  ????????#?  ?  ?? ???   (4) It is obvious that support of itemset Y can?t be greater than  support of any of its subset in a given database because  cardinality of itemset Y is more than cardinality of itemset Z,  i.e., |#|  ?  ||. It can be said that:- ?????????  !  ????????#?   (5) Using associative property from equation ?(4)? and ?(5)? it can  be stated that:- ?????????  ?  ?? ???   (6) In other words, itemset Y can?t be frequent if it is superset of  any itemset Z which is infrequent.?  Next section III.A and III.B demonstrates an example to  explain the work carried out in this paper and section III.C  shows the experimental results.



III. DIFFERENCE BETWEEN PRUNING AND  FILTRATION APPROACH  Difference between pruning and filtration approach is revealed  in part A and part B of this section.

A. Pruning approach: Let & ? ?1,2,3, ? , 10? be a set of ten items. A database, ? ???1, ?2, ?3, . . , ?20?, be a set of twenty transactions. Each transaction, ?) , have a list of items shown in Table I. Value of minimum support, minsup, is taken five. Frequency of each  individual item present in database ? is shown in Table II.

Following 1-frequent itemsets *? ? +?1?, ?3?, ?4?, ?5?, ?6?,?7?, ?8?, ?9?, ?10? 2 are found while comparing the frequency of each item with the  given minimum support.

Thereafter, Apriori algorithm?s first step is executed and  following 2-frequent candidate itemsets are generated. 3? ? 4?1,3?, ?1,4?, ?1,5?, ?1,6?, ?1,7?, ?1,8?, ?1,9?, ?1,10?, ?3,4?, ?3,5?,?3,6?, ?3,7?, ?3,8?, ?3,9?, ?3,10?, ?4,5?, ?4,6?, ?4,7?, ?4,8?, ?4,9?,?4,10?, ?5,6?, ?5,7?, ?5,8?, ?5,9?, ?5,10?, ?6,7?, ?6,8?, ?6,9?,?6,10?, ?7,8?, ?7,9?, ?7,10?, ?8,9?, ?8,10?, ?9,10? 5    In the second step, pruning approach is applied according  to Lemma 1 and following potential 2-frequent itemsets are  generated.

6? ? 4?1,3?, ?1,4?, ?1,5?, ?1,6?, ?1,7?, ?1,8?, ?1,9?, ?1,10?, ?3,4?, ?3,5?,?3,6?, ?3,7?, ?3,8?, ?3,9?, ?3,10?, ?4,5?, ?4,6?, ?4,7?, ?4,8?, ?4,9?,?4,10?, ?5,6?, ?5,7?, ?5,8?, ?5,9?, ?5,10?, ?6,7?, ?6,8?, ?6,9?,?6,10?, ?7,8?, ?7,9?, ?7,10?, ?8,9?, ?8,10?, ?9,10? 5.

Table I. Database D  Transaction Items  Purchased Transaction  Items  Purchased  ?? {1,4,6,7,8,9} ??? {3,7} ?? {1,3,4,5,6,7,8,9} ??? {1,4,7,8,9} ?? {3,4,6,8,9} ??? {1,2,3,4,5,9,10} ?7 {1,4,6} ??7 {1,4,6,7} ?8 {1,5,7,8,9,10} ??8 {1,3,4,6,8,9,10} ?9 {3,4,6,8,9} ??9 {4,8,10} ?: {2,4,6,9} ??: {1,5,8,9} ?; {2,3,4,8} ??; {1,4,6,9,10} ?< {3,7,8} ??< {1,4,5,6,9} ??= {1,2,3,4,6,7,9} ??= {1,4,6,9}   Table II.  Individual Items Frequencies  Item  number Frequency  Item  number Frequency  ?1? 13 ?6? 12 ?2? 4 ?7? 8 ?3? 9 ?8? 11 ?4? 16 ?9? 14 ?5? 5 ?10? 5   In the third step, database D is scanned and frequency of  itemsets of set 6? is counted which is shown in Table III. This frequency is compared with given minimum support and  following 2-frequent itemsets are generated.

*? ? > ?1,4?, ?1,5?, ?1,6?, ?1,7?, ?1,8?, ?1,9?,?3,4?, ?3,6?, ?3,8?, ?3,9?, ?4,6?, ?4,7?, ?4,8??4,9?, ?5,9?, ?6,8?, ?6,9?, ?7,8?, ?7,9?, ?8,9??. These itemsets are used to generate 3-frequent candidate itemsets in the next  iteration. All the above three steps are executed iteratively  In the second iteration, following 3-frequent candidate itemsets  and following 3-frequent potential itemsets are generated by  first and second step respectively.

3? ? @A B ?C, D, E?, ?1,4,6?, ?1,4,7?, ?1,4,8?, ?1,4,9?, ?C, E, F?,?C, E, G?, ?C, E, H?, ?1,5,9?, ?C, F, G?, ?1,6,8?, ?1,6,9?,?1,7,8?, ?1,7,9?, ?1,8,9?, ?3,4,6?, ?3,4,8??3,4,9?, ?3,6,8?, ?3,6,9?, ?3,8,9?, ?D, F, G?, ?4,6,8?,?4,6,9?, ?4,7,8?, ?4,7,9?, ?4,8,9?, ? 6,8,9?, ?7,8,9? I  JK          6? ? 4?1,4,6?, ?1,4,7?, ?1,4,8?, ?1,4,9?, ?1,5,9?, ?1,6,8?,?1,6,9?, ?1,7,8?, ?1,7,9?, ?1,8,9?, ?3,4,6?, ?3,4,8?,?3,4,9?, ?3,6,8?, ?3,6,9?, ?3,8,9?, ?4,6,8?, ?4,6,9?,?4,7,8?, ?4,7,9?, ?4,8,9?, ? 6,8,9?, ?7,8,9? 5   Table III 2-Frequent Itemsets Frequency  Potential 2-  Frequent  Itemsets  Frequency  Potential 2-  Frequent  Itemsets  Frequency  ?1,3? 4 ?4,8? 8 ?1,4? 11 ?4,9? 12 ?1,5? 5 ?4,10? 4 ?1,6? 9 ?5,6? 2 ?1,7? 6 ?5,7? 2 ?1,8? 6 ?5,8? 3 ?1,9? 11 ?5,9? 5  {1,10} 4 ?5,10? 2 ?3,4? 7 ?6,7? 4 ?3,5? 2 ?6,8? 5 ?3,6? 5 ?6,9? 9 ?3,7? 4 ?6,10? 2 ?3,8? 6 ?7,8? 5 ?3,9? 6 ?7,9? 5  ?3,10? 2 ?7,10? 1 ?4,5? 2 ?8,9? 8 ?4,6? 12 ?8,10? 3 ?4,7? 5 ?9,10? 4  At this point, it is to be noted here that 3-frequent candidate  itemsets ?1,4,5?, ?1,5,6?, ?1,5,7?, ?1,5,8?, ?1,6,7? and ?4,6,7? are bold faced. The reason for making itemsets bold faced will be  discussed in part B of this section.

In Table IV, frequency of the 3-frequent potential itemsets is  shown. From this table, following 3-frequent itemsets are  generated while compared with given minimum support  *? ?  >?1,4,6?, ?1,4,7?, ?1,4,9?, ?1,6,9?, ?1,7,9?,?1,8,9?, ?3,4,6?, ?3,4,8?, ?3,4,9?, ?3,6,9?,?4,6,8?, ?4,6,9?, ?4,8,9?, ?6,8,9? ?. Similarly, in third iteration, following 4-frequent candidate itemsets 34 ? > ?C, D, F, G?, ?1,4,6,9?,?C, D, G, L?, ?M, D, F, H?,?3,4,6,9?, ?M, D, H, L?, ?4,6,8,9??, 4-frequent potential itemsets  67 ? + ?1,4,6,9?,?3,4,6,9?, ?4,6,8,9?2, and 4-frequent itemsets *7 ?+ ?1,4,6,9?,?3,4,6,9?, ?4,6,8,9?2 is generated by first, second and third step, respectively. At this point again, the significance of making the  following 4-frequent candidate itemsets +?C, D, F, G?, ?C, D, G, L?,?M, D, F, H?, ?M, D, H, L?2 bold faced will be discussed in part B of this section.

Frequency of 4-frequent potential itemsets is shown in Table V.

There will be no more 5-frequent candidate itemsets generated  by Apriori algorithm for this example because none of the pairs  from following 4-frequent itemsets +?1,4,6,9?, ?3,4,6,9?,?4,6,8,9? 2 meets the joining property.

B. Filtration Approach  In section III.A, it is observed that potential k-frequent itemsets  may be generated by having an alternate approach known as  ?filtration?. In other words, it replaces the apriori?s pruning  approach. In the filtration approach, not only k-frequent but k-  infrequent itemsets are also generated.

Continuing the discussion from part A of this section, in the  first step of second iteration bold faced 3-frequent candidate  itemsets ?1,6,7? and ?4,6,7? are useless to scan and count because itemset ?6,7? is not 2-frequent. Similarly, ?1,4,5?, ?1,5,6?, ?1,5,7?, ?1,5,8? are useless to scan and count because ?4,5?, ?5,6?, ?5,7?, ?5,8? are not 2-frequent, respectively.

But this approach requires an extra data structure for storing  infrequent itemsets. Infrequent itemsets are used to eliminate  candidate itemsets according to Lemma 2. Because of this, in  the third step of first iteration 2-infrequent itemset is stored in  the set  N*? > ?1,3?, ?1,10?, ?3,5?, ?3,7?, ?3,10?, ?4,5?, ?4,10?, ?5,6?, ?5,7?, ?5,8?,?5,10?, ?6,7?, ?6,10?, ?7,10?, ?8,10?, ?9,10??.Eliminating these  ??1,4,5?, ?1,5,6?, ?1,5,7?, ?1,5,8?, ?1,6,7?, ?4,6,7?? itemsets from 3- frequent candidate itemsets following 3-frequent potential  itemsets are generated.

6? ? 4?1,4,6?, ?1,4,7?, ?1,4,8?, ?1,4,9?, ?1,5,9?, ?1,6,8?,?1,6,9?, ?1,7,8?, ?1,7,9?, ?1,8,9?, ?3,4,6?, ?3,4,8?,?3,4,9?, ?3,6,8?, ?3,6,9?, ?3,8,9?, ?4,6,8?, ?4,6,9?,?4,7,8?, ?4,7,9?, ?4,8,9?, ? 6,8,9?, ?7,8,9? 5. Eliminated 3-infrequent itemsets are stored in the set N*? ? ??1,4,5?, ?1,5,6?, ?1,5,7?, ?1,5,8?, ?1,6,7?, ?4,6,7??. Moreover, in the third step of second iteration following itemsets +?1,4,8?, ?1,5,9?, ?1,6,8?, ?1,7,8?, ?3,6,8?, ?3,8,9?, ?4,7,8?, ?4,7,9, ?7,8,9? 2 are found infrequent.

These infrequent itemsets are added in the set N*? resulting into following updated set  N*? ? >?1,4,5?, ?1,4,8?, ?1,5,6?, ?1,5,7?, ?1,5,8?,?1,5,9?, ?1,6,7?, ?1,6,8?, ?1,7,8?, ?3,6,8?,?3,8,9?, ?4,6,7?, ?4,7,8?, ?4,7,9?, ?7,8,9??. Similarly, bold faced itemsets +?1,4,6,7?, ?1,4,7,9?,?3,4,6,8?, ?3,4,8,9?2 from the 4-frequent candidate itemsets are useless to scan and count because 3-  frequent itemsets ??1,6,7?, ?3,6,8?, ?3,8,9?, ?4,7,9?? are not 3- frequent, respectively. This will result the following set     67 ? ??1,4,6,9?, ?3,4,6,9?, ?4,6,8,9?? as a 4-frequent potential itemset and following set N*7 ? +?1,4,7,9?, ?3,4,6,8?, ?3,4,8,9? 2 as a 4- infrquent itemset.

In the above part A and part B of this section, it is shown that  filtration approach requires an extra data structure to store  infrequent itemsets which is not required in Apriori?s pruning  approach.

Table IV 3-Frequent Itemsets Frequencies  Potential 3-  Frequent  Itemsets  Frequency  Potential 3-  Frequent  Itemsets  Frequency  ?1,4,6? 9 ?3,4,9? 6 ?1,4,7? 5 ?3,6,8? 4 ?1,4,8? 4 ?3,6,9? 5 ?1,4,9? 8 ?3,8,9? 4 ?1,5,9? 4 ?4,6,8? 5 ?1,6,8? 3 ?4,6,9? 10 ?1,6,9? 7 ?4,7,8? 3 ?1,7,8? 4 ?4,7,9? 4 ?1,7,9? 5 ?4,8,9? 6 ?1,8,9? 6 ?6,8,9? 5 ?3,4,6? 5 ?7,8,9? 4 ?3,4,8? 5   C. Experimental Results  Five experiments are performed on five different synthetic  datasets generated as described by [1].  Values of the various  parameters set for generating standard datasets are shown in  Table VI. Each experiment is executed three times for both  Apriori and proposed algorithm. Average time taken (in  seconds) by both algorithm is shown below with respect to  minimum support value. Evaluated results of five experiments  for the algorithms using pruning and filtration approach are  shown in Fig 1.1 to Fig 1.5.

All experimental results show that filtration approach is an  improvement over Apriori?s pruning approach.

Table V 4-Frequent Itemsets Frequencies  Potential 4-Frequent Itemsets Frequency ?1,4,6,9? 5 ?3,4,6,9? 5 ?4,6,8,9? 5

IV. CONCLUSION & FUTURE WORK In Apriori algorithm, objective of pruning approach is to remove  infrequent itemsets from the pool of candidate itemsets that are  generated during joining process. Filtration approach, an alternative to  Apriori?s pruning process, does the same job. It generates the same  number of candidate itemsets as generated by the Apriori?s pruning  approach but it is fast in its response. It is observed from the  experiment that filtration approach works more efficiently than the  Apriori?s pruning approach but requires extra memory space. In  future, filtration approach may be helpful in improving FP-Growth  and Eclat algorithm.

