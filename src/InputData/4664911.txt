Dependency-Based Mapping between Symbolic  Language and Extended Conceptual Graph

Abstract?The goal of our research is to develop a grammar induction system that can assign descriptive sentences to ontology models represented by an Extended Conceptual Graph (ECG) which is a conceptual modeling language for describing the semantics of an agent?s internal knowledge model. In the proposed system, the ECG model of the agent is converted into a symbolic language sentence. For this, the system should learn first the rules of association between sentence elements and ECG model elements. In order to learn the grammar rules, the system should be presented with training sentences carrying information on the ordering and inflection of words. This paper presents the training algorithm of the grammar induction system in details.



I. INTRODUCTION The goal of our research is to develop a grammar  induction system that can assign descriptive sentences to ontology models working on the basis of statistical learning methods [1], [2]. Within the project, the initial task is to learn the rules of association between sentences and ontology representations. This problem is strongly related to natural language processing (NLP), where most of the grammar learning algorithms use either free text without annotation, or text with syntactic annotation.

Experiences show, that syntactic annotation yields better results over blind-learning, but at the same time it requires large amount of preprocessing and restricts the training pool. In the field of ontology-based semantic annotation, the interest of researchers arises only in recent years and the attempts are focusing on word-level annotation [3].

The novelty of our project lies in the sentence-based approach. The application of an ontology for supporting grammar induction is a new area to be explored with only one antecedent [4]. See the schematic illustration of the investigated grammar induction system in Fig. 1.

For the representation of ontology models we have developed the Extended Conceptual Graph (ECG) model.

The proposed system works on the basis of two methods: 1) a training method, and 2) an operational method. The aim of this paper is to present the training algorithm of the grammar induction system.

Figure 1.  Schema of the ontology-based grammar induction system  The paper is organized as follows. Section II gives the required background information to our project describing the Extended Conceptual Graph semantic model, its symbolic language representation, and the Dependency Grammar theory. In Section III, we introduce the proposed structure of the grammar induction system, which covers the dependency-based mapping of symbolic language elements into ECG concepts and the algorithm for learning the ordering and transformation of words. In Section IV we conclude the experiences and the upcoming problems of our investigations.



II. BACKGROUND  A. Extended Conceptual Graph Extended Conceptual Graph (ECG) [5], [6] is a  conceptual modeling language that can be used to describe the semantics of an agent?s internal knowledge model. The model is characterized by  ? a predicate-centered schema language, ? the fine distinction between different categories  of concept and relation types, and ? generality and reusability of model fragments.

The elements of the model are grouped under three main primitive types: concepts, relationships and containers; where containers are structural modules, while concepts and relationships are further subdivided into several categories according to various aspects. Fig.

2 lists the allowed concept and relation types together with their graphical representations. Based on these notations, Fig. 3 shows an ECG model which is an example for a model with multiple predicates, describing the observation ?The black circle is in the white triangle?.

The highlighted fragment is an example for a single- predicate model representing the observation ?The circle   Figure 2.  Graphical representation of ECG model elements      Figure 3.  ECG representation of an agent?s internal knowledge  is in the triangle?. The model is mapped from the agent?s environment, and the first-order predicate logic (FOPL) representation of the selected fragment can be seen in (1).

( ) ( ) ( )  (_1), (_ 2) _1, _ 2,  isIn Subject Object IsA Circle IsA Triangle  ? ?  (1)  The ECG model was constructed deliberately so that it provides support for a formal specification that enables the mapping of semantics into a symbolic representation.

B. Symbolic Language The symbolic representation of the ECG model is  defined by a symbolic language. It consists of symbolic expressions of two kinds:  ? symbolic terms, referring to ECG concepts, and ? true symbolic assertions on terms, i.e. sentences.

In Fig. 3 the highlighted fragment can be given by  several true symbolic assertions, for example: 1. The circle is in the triangle.

2. The circle is located in the triangle.

3. The circle can be found in the triangle.

4. The circle is in the triangle to be found.

5. In a triangle stands the circle.

6. There is a circle in the triangle.

The symbolic terms in these examples and the referred  concept representants are listed in Table I. From this we can infer the following characteristics of symbolic terms:  ? they may be multi-word representations, and ? their components may not appear continuously.

Therefore we can state, that in symbolic expressions,  both the symbols and the arrangement of symbols communicate meaning. Accordingly, in this paper we address the following problems:  TABLE I.

CORRESPONDENCE OF CONCEPT REPRESENTANTS  AND SYMBOLIC TERMS  Concept representant Symbolic terms  Circle the circle a circle  Triangle the triangle a triangle  isIn  is in is located in can be found in is in ? to be found in ? stands there is ? in  ? the mapping of concepts into symbolic terms, and ? the arrangement (order) of symbolic terms.

Our aim is to learn the rules governing these  transformations. For this, an appropriate grammar formalism is required. Since the predicate stands in the focus of the ECG model, the corresponding formal grammar is dependency-based which considers sentence components as complements of the predicate.

C. Dependency Grammar Grammars, and theories of grammar, can be classified  according to whether the basic unit of sentence structure is the phrase, or the dependency between two words. A dependency-based linguistic approach to the description and analysis of natural language syntax is constituted by distinguishing a head-dependent asymmetry, and describing the relations between a head and its dependents in terms of semantically motivated dependency relations. Dependency grammar (DG) is a class of syntactic theories developed by the French linguist Lucien Tesni?re [7]. His model is based on the stemma, a graphical representation of the grammatical dependencies between the words in a syntactic construction. In the sentence, the verb is seen as the highest level word, governing a set of complements, which govern their own complements themselves.

Tesni?re has had a major influence on linguistic theories that place more emphasis on semantics than on syntax (see [8] for a review). Klein and Simmons [9] adopted dependency grammar for a machine translation system. Valency theory [10] and Meaning-Text Theory [11], [12] are two ongoing developments of the dependency approach. Schank adopted the dependency approach in his Conceptual Dependency Graph, but he shifted the emphasis to concepts rather than words [13].

For a recent survey on dependency models see [14].

Dependency theories have also been strongly influenced by Case Grammar [15] which provides a convenient set of labels for the arcs of the graphs. Extensible Dependency Grammar [16] and Word Grammar [17] are two general frameworks for dependency grammar which aim at modeling not only the syntactic but also the semantic and phonological levels of linguistic representations.

The main reason why we turn our attention towards dependency structures is that the availability of syntactically annotated text for training is strongly limited and the preparation of annotated training data requires deep linguistic knowledge. On the other hand, word-word dependency structures are far easier to understand and their detection can be automated, and the simpler structure guaranties that the training data are noise-free.

Dependency structures are strongly preferred for our purposes also because in a phrase structure tree discontinuous constructions can not be represented. This restriction poses problems for the analysis of word order variation, even for rigid word order languages. On the other hand, dependency grammars are not defined by a specific word order, and are thus well suited to languages with freer word order, such as Hungarian, Scandinavian and Slavic languages. The modeling of other European languages where discontinuous constructions are frequent, such as German, French and Dutch can also benefit from a dependency-based approach.

In spite of its benefits little attention has been paid on statistical dependency analysis of English sentences. Link Grammar [18] analyzes both word-word dependencies (called links) and their relation (called connector) represented as the subject of the sentence or the object of the verb. In [19] and [20] Eisner proposes probabilistic methods for dependency parsing while [21] reports on a method for deterministic dependency analysis using Support Vector Machines (SVMs).

Dependencies are widely accepted in theories of semantic structure. In fact, one of the main attractions of traditional DG is its close correspondence to meaning:  ? syntactic dependencies are meaningful, i.e. they usually carry semantic relations; and  ? syntactic dependencies are more abstract than surface order.

Nevertheless syntactic dependencies are distinct from semantic dependencies. The usual problem is how to map syntactic dependency structure to a semantic one. A distinguishing feature of our project is the aim of finding a mapping from semantic (conceptual) dependency structure to a syntactic one, where consequently the elements among which dependencies are examined are not words but concepts and syntactic units, respectively.



III. DEPENDENCY-BASED GRAMMAR INDUCTION  A. The Structure of the Grammar Inference Module In the proposed system, the ECG model of the agent is  converted into a sentence of the symbolic language. The base model of the conversion is illustrated in Fig. 4 and includes the following steps:  1. The ECG graph is converted into a rooted tree where the root node is the kernel predicate in focus.

2. The word symbols of the nodes are transformed into inflected forms that reflect the syntactic and semantic roles of the nodes.

3. For the pool of generated words the ordering is determined which results in a sentence.

It can be seen, that the key components of the grammar model are the module that determines the ordering of the words and the module that stores the transformation rules.

It is assumed that during the transformation, the inflected form depends on the whole context. In order to learn the grammar rules, the system should be presented with training sentences carrying information on the ordering and inflection of words. As the problems of ordering and transformation can not be separated from each other, the learning algorithm should perform these activities in parallel. In the next sections, the training algorithm of the grammar system is presented in details.

Figure 4.  Working methods of the grammar induction system  Let denote T{N,?} the ECG tree generated for a given snapshot, where N is the set of nodes, and ? is a symbol for the set of edges in the tree. The root or kernel predicate is denoted by root(T), and path(n) is the path to a given node n, where n ? N. It is assumed that  ? a word symbol is assigned to every node: w(n), n ? N,  ? a role is assigned to every edge in the tree: r(e), e ? ? .

Thus every path has a corresponding compound role which is the sequence of roles for the path members.

The training sentences are given with S = {s}, where s is a sequence of words, si = wi1?wim. The set of words in s is denoted by w(s). The first goal of the learning process is to determine a mapping of the elements in s to the nodes of the T conceptual graph:  : ( ) .m w s N?    (2)  The second task is to learn the valid ordering patterns.

The ordering of the words is considered here as a state transition process. The different state transitions have different probabilities and the information on word ordering is given with a state transition matrix. The generation of the most possible sequence is based on the Markov model.

B. Mapping Symbolic Language Units to Concepts Considering first the mapping function, the search for  the assignment can be considered as an optimization task as there are no strict rules for word matching. For a given pair of sentences there may exist several possible matching functions. The goal of the algorithm is to find the most possible matching. As a first try, the algorithm searches for similar patterns in previous examples. The assignment continues only if pattern matching fails. For the evaluation of word matching, the following measures are introduced:  ? edit distance between the words, and ? strength of dependency between the words.

The first measure, the edit distance of strings is a  known metric between strings [22]. The distance is measured between a word in the sentence and the known word representations of the concept. The d(w1,w2) distance value is defined as the shortest editing path that transforms w1 into w2. The editing path contains usually three distinct steps:  ? deletion of a character, ? insertion of a new character, ? substitution of a character with another one.

In the uniform model, every step has a cost value 1; and if a character remains unchanged in a word, then the elementary cost is equal to 0. The set of possible transformation paths can be given with a transformation matrix. The shortest path from one edge of the matrix into the opposite corner can be determined by Bellman?s dynamic programming method [23]. It has a relatively large cost value O(n?m), where n and m denote the number of characters in the two words.

Based on the edit distance, the formal similarity between the words in the training sentence and the nodes    of the ECG tree can be calculated. Thus, a ranking of word-pairs is defined at the end of this matching phase.

The resulted matching coefficients are only approximation as formal similarity does not necessarily mean semantic similarity. Examples include cases when  ? inflection results in a completely different format (teach ? taught),  ? formally similar words have different meaning (bank: 1 ? river bank, 2 ? financial institute).

Moreover, the identification of predicate concepts on a formal basis is not always trivial, as in the syntactic level predicates may consist of multiple words not necessarily forming continuous constructions. Also, an edit distance based analysis alone will not give the dependencies of determiners.

The next method to measure similarity is related to the dependency relationship between the words. We propose dependency analysis with the leave-one-out (LOO) method [24], [25] which is an important statistical tool for assessing generalization performance. The method was originally developed for cross validation which attempts to prevent over-fitting ? a problem of machine learning when the learning algorithm tunes itself too finely to the training data and will be unable to generalize to new, unseen data points ? by estimating how well a learning method will be able to predict future data, assuming that the new data is sampled from the same distribution as the training data. It can also be used to choose between several learning algorithms, by finding the lowest cross validation error for the given data distribution. When using the leave-one-out method, the learning algorithm is trained as many times as the number of training data (N), using all but one of the training set data points (N-1).

Therefore the resulting regression or classification rules are essentially the same as if they had been trained on all the data points. The advantage is that all the data can be used for training. The disadvantage is that the method is expensive. The LOO algorithm is executed on the pool of semantic ECG models. The main intuition behind the method is that the new words in the sentence will correspond to the new element of the ECG graph.

In a sentence, a dependency between the words is defined as follows: w1 depends on w2, if w1 can occur in the sentence only if w2 also occurs. The dependency is denoted by  2 1.w w  (3)  The dependency graph of the sentence ?The circle is in the triangle? looks as follows:  , , , ,  .

is in the circle the triangle circle the triangle the  (4)  Taking another sample sentence: ?In a triangle stands the circle?, the dependency graph has the following elements:  , , , ,  , .

stands in a triangle the circle circle the triangle in a  (5)  For the detection of dependency relationships, the sentence reduction method is applied. This means, that some word or words are omitted from the sentence. The meaning of the altered sentence must be evaluated by a teacher. The new sentence may be judged as  ? C: correct, but having a reduced meaning compared to the original sentence, or  ? U: grammatically not correct, but understandable with a modified, reduced meaning, or  ? N: the sentence is not correct and not understandable.

To demonstrate the process of evaluation, consider the sentence ?In a triangle stands the circle?:  1. A triangle stands the circle : N 2. In triangle stands the circle : C 3. In a stands the circle : N 4. Stands the circle :  U 5. In a triangle the circle : N 6. In a triangle stands circle : C 7. In a triangle stands the : N.

The algorithm proposes a dependency relation between  those words by the elimination of which the reduced sentences have different evaluation levels, denoted by li.

Thus, the w2  w1 dependency holds if  2 1 1  1 2 2  , , ( ) , , ( )  w s w s l s l w s w s l s l  ? ? = ? ? = 1 2 ,l l>  (6)  where the ordering of the evaluation levels is given as  .C U N> >  (7)  C. Ordering Relation of Words The next module of the learning phase is the  determination of the ordering relation. As valid orderings can be learnt only from test cases, the system should test which orderings are valid and which are false. For this step, the system generates some permutations of the original sentence and the altered sentence must be evaluated by a teacher. The altered sentence can be  ? S: correct, with the same meaning, or ? C: correct, with an altered meaning, or ? U: grammatically not correct, but understandable  with a modified, reduced meaning, or ? N: the sentence is not correct and not  understandable.

The precedence relations among the evaluation levels are  .S C U N> > >  (8)  After evaluating the generated test sentences, a list of valid orderings are present. Taking a valid ordering, the sentence determines a valid sequence of state transitions where a state corresponds to a word in w(s). It is assumed that every state may occur only once, there is no repetition. If wi denotes a state, the sequence of N events is represented as follows [26]:    1,..., nw w  or 1 .

nw    (9)  The probabilities of sequences can be computed by using the chain rule of probability for decomposition [2]:  ( ) ( ) ( ) ( ) ( ) ( )  2 1 1 1 2 1 3 1 1    | | ... |  | .

n n n  n k  k k=  P w = P w P w w P w w P w w =  P w w  ?  ??  (10)  The bigram model is an N-gram model where the probability of a state is approximated by the preceding state [26]:  ( ) ( )11 1| | .nn n nP w w P w w? ??  (11)  The property that the probability of a state depends only on the previous state is called Markov property and has the following formulation [27]:  ( ) ( )  1 11 1  1 1  | ...

| , t+ i i t it+ t  t+ i t it+ t  P = s = s , , = s =  P = s = s (12)  where n,,...1 are random variables. If the probability of a state depends only on the previous state, the model is called first-order Markov model. The Markov model method [28] is essential in speech recognition, handwriting recognition, machine translation, spelling correction, part-of-speech tagging, natural language generation and in any tasks where words have to be identified from noisy, ambiguous input.

In a Markov chain each state is associated with a finite set of signals. After each transition, one of the signals is emitted. In our model, the signal-set of a state contains only one signal that corresponds to the state itself.

As an example, let us take the following two example sentences evaluated as valid sentences:  1. The circle stands in a triangle.

2. In a triangle stands the circle.

The state transition matrix related to these sentences can be found in Table II. The S symbol denotes the start state, the beginning of the sentence. The rows of the matrix correspond to the initial states and the columns denote the end states. The values in the matrix cells are equal to the frequency of the corresponding state transition. Taking the restriction that a state can not be repeated, this matrix will generate exactly the two training sentences.

TABLE II.

STATE TRANSITION MATRIX FOR THE TWO GIVEN SENTENCES  S the circle stands in a triangle S  1  1 the   2 circle    1 stands  1  1 in     2 a      2 triangle    1  D. The Algorithm for Generating the Mapping Function For the testing of the proposed grammar induction  system, a prototype grammar module was implemented in Java language. The core element of the module is the processing of a new training sentence. The proposed algorithm of the learning process can be summarized in the following steps.

Input: s: sentence T: ECG graph  Algorithm: 1. Searching for previous patterns.

2. Determining the concept-set of T: W?(w?).

3. Determining the word set of s: W(w).

4. Performing similarity analysis:  4.1. Calculating the edit-distance matrix for W ? W?.

4.2. Generating a list of candidate mappings based on the edit-distance matrix (M: W ? W?).

4.3. Evaluating and ranking candidate mappings.

5. Performing dependency analysis of W:  5.1. Generating reduced sentences.

5.2. Evaluating the generated sentences.

5.3. Building the dependency graph.

6. Updating candidate mappings based on the generated dependency graph.

7. Selection of accepted mappings and their incorporation into the knowledge base of the module.

8. Performing analysis on word ordering: 8.1. Generating permutations of s.

8.2. Evaluating the generated sentences.

8.3. Building the state transition matrix.

9. Extension of the selected mappings with the generated state transition matrix.

The workflow diagram of the training algorithm is displayed in Fig. 5.



IV. SUMMARY In this paper we proposed a training algorithm for the  investigated grammar induction system. The algorithm executes two tasks: 1) assignment of sentence elements to ECG concepts, and 2) learning of word order variations.

The mapping module includes three methods based on 1) pattern matching, 2) formal similarity, and 3) word dependency. The ordering module involves two methods based on 1) pattern matching, and 2) generation of all possible word order variations.

The algorithm works for single-predicate cases. The mapping of multiple semantic predicates into a single sentence needs special considerations and requires the introduction of multiple levels of analysis.

ECG graphs are stored in XML format. The search for previous samples in the mapping module is precise for the predicate concept and its role relationships. Category concepts, however,  can be replaced  and their  inflections     Figure 5.  Workflow diagram of the training method  are inherited by the new sample concept representants.

After processing some well-selected samples, this step essentially improves the training process.

The experiences gained so far give rise to some problems concerning the Hungarian language which will be addressed in our future investigations. These are as follows:  ? In Hungarian, the subject of a predicate may be implicit at the syntactic level, i.e. suffixed to the predicate.

? In Hungarian, the semantic predicate is not always represented at the syntactic level.

