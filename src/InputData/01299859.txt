Dominique Francisci  13s Laboratory,

Abstract- This.paper addresses the problem of the good- ness of dependency rules extracted by mining data. Our approach is experimental and based on the idea that model quality has to  be .measured according to several criteria of quality such as accuracy, interestingness or domain-dependent criteria. Most works on model qual- ity are focusing on one criterion at a time only and do not take into account multiple factors simultaneously. A few works combine different measures? in weighted ex- pressions. In order to combine multiple measures, we have first realizeda comparative study which highlights the relative contribution of different factors and reveals trade-offs among some of them. This situation suggests looking in the rule  search,space for compromises rather than for, hest rules which may not exist. Thus we show that a multi-objective evolutionary approach is able to reveal interesting rules which are ignored by standard  . solutions.

1 Introduction  Data Mining may be defined as the discovery of unexpected relationships by analyzing such large volumes of data that automated processes are necessary. The extracted knowledge is expressed as a model or a pattern like sets of rules, neural networks or clusters for instance. As mentioned in [4] a model refers to a global structure that covers all the data while a pattern, makes a summary or description of a locally restricted region of the data space.

Numerous data mining applications are more concerned with the search of local patterns rather than global models.

In this paper, we focus on rule-like patterns A i B where A and B are conjunctions of attribute-value terms A n  op a, Att is an attribute, a is a value and op E {=, <, >, 2 ,  5) .

In a data mining process, it is quite frequent to search for rule based models since they are easily understandable by end users and have been found to?be useful concepts for learning interpretable knowledge from data. One essential issue is to measure the interestingness of the dependency link between the premises A and the consequent B.

Standard algorithms currently use basic measures for rule selection. But the specific goal of data mining has led to the definition of more specific indices which address different facets of rule goodness. In the following, we give an overview of standard algorithms and propose measures of interestingness.

Martine Collard 11s Laboratory,  University of Nice-Sophia Antipolis, 06903 Sophia Antipolis - Cedex France  mcollard@unice.fr  1.1 Standard methods  Numerous algorithms have been proposed for rule in- duction from data in the machine learning literature.

Tree induction or separate-and-conquer rule learning for instance are used for prediction in data mining.too. F e y provide classification rules which right-hand side attributes are predefined and represen; the class. These methods generally search for general prediction rules covering many data rather than specific rules. They aim to discover a set of rules that form an accurate classifier and this model is intended to apply to all the data. Thus the quality?of an extracted model is  evaluated according to its accuracy that is the ratio of correct classifications the entire set of rules makes on previously unseen examples.

As mentioned above, in frequent practical data mining ap- plications, rules are rather considered individually outside the model they would form together. The goal is often to discover interesting patterns which are not only valid but novel, unobvious or surprising. These ?good patterns may not cover a large range of data. Criteria for goodness evaluation are measured on a rule individually rather than collectively.

One technique for generating a set of individually inter- esting and useful rules is to build a classification tree and then to evaluate each of the branches as individual rules according to specific targeted quality criteria. But some rules produced by standird classification make no sense to the user since they use biases and specific heuristics to generate the classifier. Thus this kind of technique may loose interesting rules as shown in 1191.

On the other hand, so called association rules are among the most popular representation for local patterns in data mining. They originated in applications involving market- basket analysis where frequent relationships between products are expected to be discovered. In these rules the target is not predefined and the right-hand side of such a rule may be a conjunction of attribute-value terms. An association rule may he seen as a probabilistic statement about the co-occurrence of events which satisfy statistical constraints on the database like minimum Support and minimum Confidence. The Support is defined as p(A n B )  and Confidence represents the conditional probability p ( B / A ) .

Multi-Criteria Evaluation of Interesting Dependencies according to a Data Mining Approach  0-7803-78044 /03/$17 00 D 2003 IEEE I568  mailto:mcollard@unice.fr   1.2 Dependency rules  Classification and association rules are two important techniques in data mining. Both of them are essential in practical applications. Besides this syntactical similarity, A. Freitas [8] highlights essential trade-offs between the two patterns. However while they refer to different concepts, one can observe they are merged for data mining applications. For instance, [2] proposes to integrate both kinds of rules in building classifiers which are more fitted to data mining objectives. This approach uses association rule mining techniques for classification tasks. The most popular association algorithm APriori [ I ]  is adapted for this task. According to this technique, rules are selected if they satisfy minimum Sumort and Confidence which are not standard criteria for classification rules. Thus these rules predict a class-attribute value and they are considered individually. In [I91 the system incorporates ideas from tree induction and rule learnink. It is designed in order to search for classification rules for financial applications. The evaluation of rules comes in two flavors: number of errors on unseen data for the entire model, and Confidence and Support for individual rules. It is argued that in financial problems, it is unlikely that a model could be built that classifies all cases accurately. The system is designed to find models, even with few useful rules and with high accuracy. . Rule goodness.is determined on the.basis of Support, Accuracy, Entropy or some combination of such metrics on individual rules.

In this paper, we follow a similar approach: we study the interestingness and usefulness of rules which are considered individually. Furthermore, we look at rules as dependency links and we consider either a rule which right-hand side is a class-attrihute or not. The framework is restricted to rules whose target is one attribute-value term only for convenience and simplicity. Our objective is to extend this study to more general rules. We do not make a distinction between classification rules or association ones since we are only interested in measuring the quality of the dependency link between premises and consequent of a rule.

1.3 Interestingness of dependencies  Simple criteria for rule selection like Accuracy for entire model or Support and Confidence for individual tules are known to he insufficient for extracting useful and interesting information. For global classifier quality, and particularly for medical applications or information retrieval, Sensitivity and Specificity give more precision on the-model quality. While usefulness and interestingness may have different meanings. one can distinguish objective and subjective approaches. Subjective criteria generally are based on a comparison of learned rules against an a priori knowledge on data.

Ohjective measures take their origin from the proposition of G .  Piatetsky-Shapiro [I41 who observed the weakness of the Confidence factor and defined the rule interest.

Numerous other measures have been proposed for eval- uating the quality of the extracted information: the Li f t factor [ 121, the JMeasvre of Goodman and Smyth [IO], the measure defined by Sebag and Schoenauer [16], the Conviction [15]. It has been observed [4] that some of them are quite identical since they rank rules in the same manner. But they may provide a complementary approach since their definition was motivated in order to fill the gap.

1.4 A multi-criteria issue  Note that if standard algorithms are involved, the rule selection according to multiple criteria is made as post- processing the set of rules extracted first by the algorithm.

This approach may result in undiscovered interesting rules.

Few non-standard approaches have been proposed in order to apply a multiple criteria selection. For instance [ l l ]  and [5]  use a genetic algorithm (CA) which fitness combines multiple factors in a weighted sum or  a product. Note that these propositions may ignore rules which are good compromises according to different criteria.

In the context which we consider, i.e. the data mining and particularly the extraction of rules, few papers related to this metaheuristic in multi-objective optimization exist. For instance in [13], the authors suggest multi-criteria based metrics that can he used as comparators for an objective evaluation of data mining algorithms. They take into account all the available positive and negative features of these algorithms to construct a unique evaluation metric.

Indeed, most existing studies only use the Accuracy rate to compare algorithm performances. In [61, the author highlights the problem of feature selection which can be considered as a multi-objective problem since it generally evolves feature subset cardinality minimization and performance maximization. He uses the typical example of ROC analysis, where performance is assessed in terms of classifier Specificity and Sensitauity.

The author proposes an extension of the multi-objective evolutionary algorithm NPGA to handle Specificity and Sensitivity of classifiers. In the 'same way in [91, the authors use a CA to find the best subset of features that minimize both the error rate and the size of the tree discovered by a tree induction algorithm. Finally, we can quote (31 which use a GA in the marketing context, to select the customers likely to answer to a mailing request.

1.5 Our approach  In this paper, we argue that the opportunity of varying and combining different quality criteria is an essential advantage in a data mining process and we propose a evo- lutionary multi-objective method to address this problem.

Our approach is experimental and based on the idea that rule quality is a multi-criteria issue as detailed above. We have chosen different interestingness measures and we experimentally have shown that they may be not correlated and even present trade-offs.

The paper is organized as follows.. . Section 2 presents a review of existing measures of interestingness. In Section 3, measures are compared by pair. Then Section 4 is devoted to the discovery of best compromises via a genetic algorithm which implements a multi-objective optimiza- tion. Section 5 summarizes and concludes the paper.

2 Interestingness Measures : positive correla- tions and trade-offs  In this section, we detail measures of quality proposed in the data mining literature. These measures may refer both to global model quality and individual rule quality. Even if they were defined for specific shapes like classifiers or associations, they take all sense to evaluate dependencies A + B as shown in [ I  I ]  and [7].

2.1 Measures of interestingness  Classifier models expressed from tree induced from a training set are evaluated according to their success rate on a test set i.e. the rate of truly classified examples in the test set. The accuracy of a rule A i B is defined in terms of probability by p(A n B )  + p(-A n +). The sensitivity and specificity criteria were defined for a classifier in order to give a more precise idea of its perfor- mance. They specially take sense when class distribution is unbalanced. For a rule A + B ,  sensitivity becomes p(A f lB) /p (B)  orp(A/B) and evaluates the coverage of B by A while the specificity becomesp( -An/TB) /p (d? ) or ~ ( T A / T B )  and evaluates the coverage of -B by TA, The choice of a quality measure depends on the specific facet of interestingness we want to favor. For instance, in a classification task for a medical diagnostic test, the main objective is to reduce the error which consists in predicting a patient in class ?healthy?. In this case, experts from the domain may consider it is more essential to have best results in the classification of examples of class ?patient? even if examples from class ?healthy? are classified as ?patient?. This means to optimize seiisitivity rather than specificity. In other domains, one may want to optimize both sensitivity and specificity simultaneously.

For association rules, maximizing support allows to select frequent patterns, but con: fidence has proved its limits in filtering them since it  favors num&ous rules which are most often irrelevant. Indeed the weakness of the confidence factor p(B /A)  is to ignore p(B) .  Most interestingness measures generally compare the a priori probability p ( B )  and the a posteriori probability p(B/A) .  For instance, the l i f t  measure [I21 defined as p(B/A) /p(B) and the Rule Interest measure from G .  Piatetsky-Shapiro defined as IAl (p(B/A) .  - p ( B ) )  measures the departure from independence between A and B. As remarked by [151, they measure the cq-occurrence of A and B since they are symmetric. The convidzon 1151 defined by p ( A ) p ( - B ) / p ( A  n -B) is not symmetric .and is related to the logical implication A + B which means -A V B.

M. Sebag and M. Schoenauer [161 defined the measure  p ( A  n B ) / p ( A  n ?B)  which measures the ratio between positive and negative examples. The JAdeasure [IO] defined as p ( A  n B)loy(p(A f l  B)/p(A)p(B))  + p(.4 n -B)log(p(A n TB)/p(A)p( -B))  by combining a factor of generality and a factor of goodness-of$t quantifies the information content of the rule. Tlie rule quality is not reduced to one criterion only. Furthermore, the goodness cannot be specified in an absolute way, it depends on specific goals of the search process. For instance one can search for large rules which cover a whole class. On the contrary it may be useful to find small rules which quality involves rarity and precision. Thus the questions we address here are : how to combine multiple criteria simultaneously ? Is there equivalence among measures ?

What is the best solution for finding the best rules according to multi-criteria ?

2.2 Correlations and Trade-offs  Some measures are known to be strongly correlated partic- ularly if the search space is reduced by fixing p(B). This is the case when we search for classification rules on one given class. For instance rule Interest and support rank rules in the same order if p(B) is constant. The three measures l i f t ,  conviction and sebag have the same property. For a given class, sensitivity and specificity represent comple- mentary indices of coverage. One can recall that these two factors are plotted in ROC curves in order to evaluate the performances of a classifier against positive and false nega- tive examples.

Since trade-offs may exist, it is quite obvious that stan- dard techniques which are not designed for multi-objective optimization may ignore good compromises.. In order to make an experimental study, we have identified several cases where trade-offs are apparent. To highlight correla- tions among couples of measures, we have plotted sets of points where each point maps a rule measured according to two measures. We have studied randomly generated rules which are expected to represent the search space. Result- ing graphs below show three situations of trade-off. Thus a multi-objective optimization method has been implemented on these three cases to elicit hest compromises. Thereafter the presentation focuses on these cases. Case 1 is related to criteria Sensibility (Se )  and Specificity ( S p )  which characterize the precision and the non-coverage of negative examples and are good indicators of rule quality in diagnos- tic test and document retrieval. In case 2, we study two com- mon factors which are Support (Supp) and Conviction (Conv). Finally case 3 shows the relative behavior of the Rule Interest (RI) and the Sebag coefficient.

Experiences have been built on data files from the UCI repository [181. Randomly generated rules are useful for estimating the density of the rule search space according to values of criteria but they obviously may not give a com- plete vision of it. For each data file up to lo5 rules were generated.

In this paper, for simplicity reasons, we consider rules A B where B is one attribute-value term only.

I570    On Figure I ,  we can observe results for the couple (Sensi t ivi tyl  Specificity) on the Vote data set. This data set contains 435 descriptions of politician candidates ac- cording to 17 categorical attributes. These data were used for classification. The class attribute politic class takes value "democrat" or ."republican". By observing Figure I it  seems that there is not any optimal rule : not any rule is plotted in the upper right corner. This graph suggests an a p parent trade-off between the two measures on this dataset.

A solution to obtain best compromises may be to optimize the product on the rule set discovered by a standard algo- rithm.

0 0.1 0.: 0.3 0.. ' 0.5 0.6 0.7 a 0.9 L r / r ,  Figure I :  Random rules evaluated according to Sensi t ivi ty and Specificity  R u l e  Interest  as a symmetric criterion measuring the departure from independence between A and B and Sebag [ lh]  which is sensitive to the negative examples, present an apparent tradeoff too like we can see in  Figure ??. The graphical representation highlights the correlation between the two measures when R I  is negative. We can observe that Sehng is closed to zero in that case ; indeed, in this situation , (A n E )  tends to be small and thus it  tends to be less than p ( A  n 4).

The Coiiuiction index introduced in [ 151 to replace the confidence is used in an extension of the two-steps A P r i o r i  algorithm as follows : first the frequent itemsets of minimum support value are selected and then itemsets with high ccrnuiction, value are kept. However, we can see on Figure 2, from a particular value of support. for instance 0.3. some solution would be discovered because of their relative high support value, but some others solu- tions with high conviction value would be ignored by an APriori - like algorithm. For instance, so called sma l l disjuncts which are rules covering a small number of ex- amples, but may be good according to their conviction value would not be discovered. This is a typical issue to address with a multi-criteria optimization method.

The Ru le  In t e re s t  [ 141 as a symmetric criterion mea- suring the departure from independence between A and B and the Sebag coefficient [ 161 which is sensitive to the neg- ative examples, presents the same aspect like we can see in Figure 3. Indeed, for a particular value of Sebag, there  0 a.* 0.1 0.15 I? 0.z 0.3 0 . 5  0..

Mi.1  Figure 2: Random rules evaluated according to Suppor t and Conviction  exist several good solutions according to R u l e  I n t e r e s t .

3 Multi-Criteria Selection Rule  This section presents the results we have obtained with an evolutionary optimization method. We have applied the NSGA (Non dominated Sorting Genetic Algorithm)[ 171 which allows to get a good diversity into solutions.

sib,  -ir,  Figure.3: Random rules evaluated according to Sebag fac- tor and Rule In t e re s t  3.1 Multiobjective optimization  The multiobjective optimization or Pareto optimization can be defined mathematically as follows : each solution X i  is associated with an evaluation vector F = ( F l ( X i ) ,  ..., F,v(X;)) where N is the number of ob- jectives. One solution XI is said to dominate  another so- lution X Z  if V j  : F,(X1) 2 F j ( X 2 )  and 3k : Fk(X1) > Fk(X2), where F, and Fk are respectively the j - th  and k- th-objectives, i , j  E 11, ..., N } .  Neither solution dominates theotherif 3m1,mz :&,(XI)  > Fml(Xz) ,Fm2(Xz)  > Fm2 ( X l ) .  The Pareto front is defined as the set of non dom- inated solutions. First, the goal is to approximate as  best as possible the Pareto front.

The objective functions represent in our case the measures     of extracted rules and the variables X, represent the rules.

Genetic algorithms iG.4) are related to the mechanisms of natural evolution. that mainly is to say selection. reproduc- tion and mutation. They describe the way individuals in a population evolve in response to their en\ 'ironment over suc- cessive generations and are selected according to the princi- ple of the survival of the most fitted.

GAS belong to the metaheuristic family built to solve dif- ficult optimization problems. From the data mining point of view, they have mainly the advantage to allow evaluating rules globally while other methods like decision tree induc- tion are evaluating one term at a time. Funhemore, one of the main difference of the GA compared to standard opti- mization algorithms. is that these algorithms use a popula- tion of solutions instead of a single solution.' If a problem has several optima. a GA is able to capture these different local optima and this is done during a single run.

In the implementation we present in this paper. each indi- vidual represents either the antecedent of a rule or the entire rule. In the first.case the dependency rule is simple : its roriclusiori part is fixed at the beginning of the execution and represents a goal attribute value. Thus. in that case to discover several rules predicting different goal attribute val- ues, i t  is necessary to run the GA several times, one for each value. In the second case. the GA'is searching for solutions in the whole search space of dependency rules.

The algorithm we present here is based on the NSGA (Non dominated Sorting Genetic Algorithm) algorithm pre- viously described in  [I71 . It uses the Pareto dominance to determine the fitness of an individual as follows : each indi- vidual has a rank which represents the number of individu- als in the current population which dominate it. This rank is defined as follows for an individual i : r a n k ( i )  = 1 + p ( i ) where p ( i )  is the number of individuals which dominate the i individual. Non dominated individuals are assigned the rank 1 and dominated individuals have a greater rank. The individuals of same rank belong to a same category which is assigned to a virtual fitness value equals to 1/P with P the Pareto rank of this category. .To obtain a diversity of indi- viduals in this category (or ecologic niche). the fitness value of individuals is modified in f i t rms/ in ,  where the bias m, is equal to E,"=, P ( d ( i : j )  and P ( d ( i ,  j)) = 1 - - if d ( i : j )  < d i s l n f  and 0 otherwise. K is the number of individuals in the considered category and d ( i ,  j )  is the Euclidian distance (based on the two measures considered) between individuals z and j .  The distance d i s J n f  is the distance of influence and defines the radius of the ecologic niche.

The GA aims at finding the best approximation of the Pareto set of solutions. Ideally, this set must be contained in the final population. Nevertheless, this result is not guaranteed. Indeed, the stochastic nature of the GA via mutation operator may lead non dominated individuals to disappear. On the other hand, the extent of the Pareto front is not necessary known a priori and, therefore, the population dimension in consequence cannot be determined precisely. These two phenomena suggest archiving non dominated solutions found during the GA execution. When  a non dominated solution is met by the GA due to. a crosso\'er or a mutation. it is stored into the archive. Finally solutions presented to the user are those which are present in the archive. they give an approximation of the Pareto front but they may not be solutions contained in the final population as usually.

3.2 Rule Extraction with the multi-objective GA  This section presents our approach towards the individuals encoding. the genetic operators and the fitness function. It shows the results we have obtained by the impkmentation of this method and i!s application on several databases.

3.2.1 Individuals representation  For rule extraction we have chosen the Michigan approach of CA : an individual codes a single rule. An individual genome represents the conjunction of attribute-value terms corresponding to the rule antecedent. The coding is a po- sition coding consisting in a sequence of genes ranked in the same order than corresponding attributes in the data set. Each condition is coded by a gene and consists in an attribute-value Aft op a where Att is an attribute. op is one of the operators =, <. >. 5 or 2 and U is a value of Att.

Each gene is composed with a Boolean field which indicates if the gene is actiYe or not. that is to say if the condition is present in the rule. So. while individuals have the same genotype length. associated rules are length-variable. The algorithm can handle integer, real or'categorical variables.

For a categorical attribute. the only operator is = and for integers and reals available operators are =, <, >, 5 and 2.

3.2.2 Genetic operators  The crossover we used consists first in randomly choosing a site. Then it  exchanges the genetic material located after this site in both individuals. The site is chosen to avoid the generation of empty individuals. The mutation operator modifies the rule length. Either. i t  specializes a rule by the adjunction of a condition if at least one gene is not active.

or it generalizes it  by deleting a condition. The'adjunction of a condition is made by randomly generating an operator and a value belonging for the mutated gene. The selection we used is the tournament selection with two individuals.

3.3 Experimental results . ,  We have chosen to apply the multi-objective GA on (Sensibility. Specificity), (Sebag.Rule Interest)  and (Suppoit, Conviction) couples. The GA has been tested on the b-otr dataset presented in section 2.2. The GA parameters have been widely tested according to data sets characteristics. For instance, on the' Vote dataset, we have retained : 200 individuals in the population, lo0 generations. and a distance of influence equal to 0.05. The crossover probability is 0.8 and the mutation probability is 0.05. . .

In the following graphs are representing a rule space.

Each graph contains three types of information: we have generated lo5 random rules for a representation of the search space and we have plotted rules discovered by the multi-objective CA and rules discovered by a tree induction algorithm. The following parameters have been used in the CA for each graph : population size : 200 individuals, number of generations : 50, crossover rate : 0.8, mutation rate : 0.0625 and distance of influence : 0.05.

The figure 4 shows resulting rules obtained with the multi-objective CA. In this first example, we can observe discovered rules on the Pareto frontier for the Sensitivity and Speci ficity measures. For instance, the following rule with (Se  = 0.87: S p  = 0.96) is on this frontier:  IF (physician-fee-freeze=y) AND (el-Salvador-aid=y) AND (duty-free-exports=n) THEN Class=republican  We can observe on this graph too that a tree induction algo- rithm is not able to find the same compromises since it only optimize one criterion.

Figures 5 and 6 present respectively the results obtained with the multi-objective GA for the (Sebagfactor, Rule Interest) and (Support, Conviction).

. . . .  . . ~ . ' . ~  : .  . ~ .  * : . ;  ..,....,... . ,. .. . . i  .

0 a., 0.2 0.3 0.4 0.5 *.s 0.7 0.8 0.9 1 *Irl  Figure 4: Sensitivity against Speci ficity. Cross : random method - Triangles : CA - Rhombus.: Tree induction  Most rules obtained by the multi-objective CA are not discovered by the tree induction algorithm. Here are specific rules-among these ones. The first one with (Se = 0.89286: Sp = 0.96255) generalizes several rules resulting from tree induction. The second one whose coordinates are (Se = 0.95833, Sp  = 0.95131) does not share any common term with tree induction resulting rules.

IF (physician-fee-freeze=y) AND (duty-free-exports=n) THEN class=republican  IF (physician-fee-freeze=y) AND (crime=y) THEN class=republican  The rules found by tree induction have a Specificity index equal to 1. They have also a very low Sensitivity while rules found with the CA are compromises. For in- stance, if we consider the context of the medical domain and particularly the diagnostic of diseases, the rules found by tree induction are not the best..

R D so I* U0 le %Id  Figure 5 :  Sebagfador against RULE Interest. Cross : random method - Triangles : CA - Rhombus : Tree induc- tion  r-,.

Lwiri  Figure 6 Support against Conuiction. Cross : random method - Triangles : CA - Rhombus : Tree induction  4 Conclusion  In this paper, we have proposed a multi-criteria approach for extracting rules according to different criteria of goodness. Indeed, in order to answer to the main objective of data mining, i t  becomes essential to be able to select useful information. Usefulness means not only interesting- ness, surprisingness hut precision, comprehensibility and domain-dependent qualities too. Even only one of these qualities, interestingness; may be appreciated differently according final objectives. Numerous criteria and measures have heen defined and implicitly considered as sufficient.

Some of them are known to he equivalent and others     are complementary. The comparative study we have led on different measures has shown several phenomena of tradeoffs between them. According to these observations we have addressed the problem of selecting interesting rules as a multi-criteria optimization problem. A multi-objective evolutionary algorithm has been chosen since this kind of stochastic methods is able to explore large search spaces and allows defining easily different evaluation functions.

Thus this approach emphasizes the idea that interestingness cannot be defined in an absolute way. It allows to.easily parameterize the rule search process and to make varying the parameter values according to different and multiple goals. Next steps on this work will focus on combining this preliminary multi-objective approach which provides a selection of objectively interesting patterns with subjective interestingness filters.

Bibliography  [ I ]  R. Agrawal and A. Srikant. Fast algorithms for mining association rules. In Proc. VLDB 1994, pages 487- 499,1994.

[2] W. Hsu B. Liu and Y. Ma. Integrating classification and association rule mining. In Proceedings of the 4th lnternatiorial Corlference on Knowledge ~Discov- er?. arid Data Mining (KDD-1998), Plenav Presenta- tion), New,York, 1998.

[3] S. Bhattacharyya. Evolutionary algorithms in data mining : Multi-objective performance modeling for direct marketing. In Proc. of the 6th International Conference 011 Knowledge Discovery and Data Miri- irig (KDD-2000), Boston, August 2000. AAAI Press.

[4] H. Mannila D. J. Hand and P. Smyth. Principles of data mining. Adaptive Computation and Machine Learning. J. Wolfskill, 5 Cambridge Center, 4th floor, Cambridge, MA 02142,2001.

151 H.S. Lopes D.L.A. Araujo and A.A. Freitas. A parallel genetic algorithm for rule discovery in large databases.

In Proc. 1999 IEEE Sysrenis, Man arid Cybernetics ConJ, v. Ill, pages 940-945, Tokyo, October 1999.

[6] C. Emmanouilidis. Evolutionary multi-objective fea- ture selection and r w  analysis with application to in- dustrial machinery fault diagnosis. In Eurogen 2001, 2001.

[7] I. W. Flockan and N. J. Radcliffe. Ga-miner : Paral- lel data mining with hierarchical genetic algorithms, final report. In EPCCAlKMS GA-Miner - Report 1.0, University of Edimburgh, November 1995.

[8] A.A. Freitas. A multi-criteria approach for the eval- uation of rule interestingness. data mining. In Proc.

brternaiional Conference, pages 7-20, Rio de Janeiro, Brazil, September 1998. WIT Press.

[Y] A. A. Freitas G. L. Pappa and C.  A. A. Kaestner.

A multiobjective genetic algorithm for attribute se- lection. In A. Lofti J. Garibaldi and R. John, ed- itors. Proc. of the 4th l~iter~~ational Corlfererice 011 Recent Advarrces in Soft Computirig (RASC-2002J, pages 116-121. Nottingham Trent University, Decem- ber 2002.

[ I O ]  R. M. Goodman and P. Smyth. Rule induction using information theory. In G. Piatetsky-Shapiro and W. J.

Frawley, editors, Knowledge Discovery in Databases (KDD-1991). MITPress, 199 I.

[I I ]  N. J.Radcliffe and P. D. Surry. Co-operation through hierarchical competition in genetic data mining. In EPCC-TR94-09, 1994.

[ 121 International Business Machines. IBM intelligent Mirier: User's guide. IBM, 1996.

[ 131 G. Nakhaeizadeh. Development ofinulti-criteria met- rics for evaluation of data mining algorithms. In D. Pregibon D. Heckerman, H. Mannila and R. Uthu- rusamy, editors, Proc.' of the 3rd I~zterr~ationhl Con- ference 011 Knowledge Discoven arid Data Mining (KDD-1997), pages 3 7 4 2 ,  Newport Beach, Califor- nia, August 1997. AAAI Press.

[ 141 G. Piatetsky-Shapiro. Discovery, analysis and presen- tation of strong rules. In G. Piatetsky-Shapiro and ed- itors W. J. Frawley, editors, Knowledge Discovey in Databases. MIT Press, I99 1.

[I51 J.D. Ullman S. Brin, R. Motwani and S. Tsur. Dy- namic itemset counting and implication rules for mar- ket basket data. In Proc. ACM SIGMOD Biternational Conference on Management of Data (SlGMOD-1997), Tucson, Arizona, USA, May 1997.

.

[I61 M. Sebag and M. Schoenauer. Generation of rules with certainty and confidence factors from incomplete and incoherent learning hases. In Proc. of the European Knowledge Acquisition Workshop EKAW-1988). 1988.

[17] N. Srinivas and K. Deb. Multiobjective optimization using non-dominated sorting in genetic algorithms.

Technical report, Department of Technical Engineer- ing, Indian Institute of Technology, Kanput, Indian, 1993.

[ 181 Irvine University. Uci machine learning ftp.ics.uci.edu/pub/machine-learning-databases.

[I91 D. Chou V. Dhar and F. Provost. Discovering inter- esting patterns for investment decision making with glower - a genetic learner overlaid with entropy reduc- tion. Data Mining and Knowledge Discoven, Journal, 4(4):25 1-280,2000.

