Estimating Itemsets of Interest by Sampling

Abstract begin with describing preliminaries in next section.

In Section 3, we estimate the size of the sample of a  Many applications such as marketing and stock in- given very large database. In Section 4, we present vestment may be time limited and few considera- a method to identify the approximating support of tions to accurate itemsets. For these applications, itemsets of interest. In Section 5, we evaluate the ef- we present a mining model to quickly estimate the fectiveness of the proposed approach experimentally.

approximate support of frequent itemsets of interest Finally, we summarize our contributions in section 6.

in large scale databases in this paper. An efficient al- gorithm is thus designed to  reduce the searched space by pruning uninterested frequent itemsets. 2 Preliminaries  Let I = {il, iz,. . . , i ~ }  be a set of N distinct literal 1 Introduction called items. D is a set of variable length transac-  tions over I. Each transaction contains a set of items One of the main challenges in data mining is to  iden- i , , i z , .  . . ,it E I. A transaction has an associated tify frequent itemsets for very large databases that unique identifier called T I D .  An association rule is comprise millions of transactions and items. Conse- an implication of the form A + B (or written as quently, some recent efforts have focused on designing A --t B) ,  where A,  B c I ,  and A n B = 0. A is efficient algorithms [4, 51. The main ?limitation? of called the antecedent of the rule, and B is called the these approaches, however, is that they require mul- consequent of the rule.

tiple passes over a database. For very large databases In general, a set of items is called an itemset. The that are typically disk resident, this requires reading number of items in an itemset A is the length (or the database completely for each pass resulting in a the size) of A ,  sometimes denoted as [Al.  Itemsets of large number of disk I/Os. Accordingly, many vari- some length k are referred to as a k-itemsets. For an ants of Apriori algorithm such as sampling [4] and itemset A .  B, if B is an m-itemset then B is called OPUS bascd algorithm 151 have been reported. an m-extension of A.

However, many applications such as marketing and Each itemset has an associated measure of statisti- stock investment may be time limited and little re- cal significance called support, denoted as supp (or p ) .

quirement on the accurate of the supports of item- For an itemset A c I ,  supp(A) = s (or p(A) = s), if sets. Indeed, for a short-time stock investor, ?time the fraction of transactions in D containing A equals is money? to him. So, heishe needs approximating s. A rule A + B has a measure of its strength frequent itemsets from stock databases quickly. The called confidence (denoted as conf) defined as the approximating frequent itemsets are enough to  enable ratio p ( A  U B)/p(A).

himlher to make a optimal decision on his/her invest- The problem of mining association rules is to gen- ments so as to obtain a good reward. Reversedly, if a erate all d e s  A + B that have both support and stock investor pursues accurate supports of frequent confidence greater than or equal to some user speci- itemsets in databases, heishe may lose the best in- fied minimum support (minsupp) and minimum con- vesting time. Hence, quickly approximating mining fidence (minconf) thresholds respectively.

models have being expected to  be developed. For ap- There are two main reasons obliging us to ex- plications, we construct an algorithm to discover ap- plore high-speed models for identifying approximat- proximating frequent itemsets of interest from very ing frequent itemsets in very large databases. One large databases in this paper. is that the databases in many applications often in-  The rest of this paper is organized as follows. We volve a million or even over several millions of trans-  0-7803-7293-X/01/%17M) 0 2001 IEEE 131 2001 IEEE International Fuzzy Systems Conference    actions and often millions of items. Mining such large obtained. This seems in some sense to  be a 'good' databases is a time consuming procedure. For exam- estimate of 0.

ple, the applications such as marketing and stock in- In this way, a database D can be taken as a trial.

vestment may be time limited and few requirement For any itemset A,  it is 1 if the itemset A occurs in a on accurate supports of itemsets mentioned above. In transaction T (written as T ( A ) ) ,  else it is 0 (written other wards, faster algorithms of digging up approx. as 7T(A) ) .  Suppose the probability of A occurring imating frequent itemsets in large scale databases is in the database is p and the probability of A not eager to  be explored. Fortunately, some techniques occurring is y = 1 - p .  Hence, this given database in Statistics are helpful to  this problem. can be taken as a Bernoulli trial according to the  Another one is that, some applications which in- definition in [l]. In particular, we can approximate volve large quantities of data in databases, solving the probability p of A by central limit theorem. We exact frequent itemsets may often be computation- estimate the sample size by a theorem as follows.

ally expensive and in many cases not even physically tractable, they have to  be relied on approximate re- Theorem 1 Let D be a large database, TI, Tz, .. ., sults when sources are bounded. To mine frequent Tm be the tmnsactions in D ,  A be an itemset in D , itemsets for these applications, approximation is tac- q > 0 be the degree of asymptotic to frequent itemsets, itly an efficient and viable method. E 2 0 be the upperprobability of P[IAve(X,)-pl 5 q] ,  Recently;'%ome fast model of mining approximate where Ave(Xn) is the auemge Of A occurring in n frequent itemsets by Chernoff bounds have been pro- transactions in D .  suppose records in D am matched posed in [3,  41. In this paper, we adopt a slightly Bert ld lI  k i d S .  If n random records of D is enough different approach to  this problem, which can mine for determining the approximate frequent itemsets in approximate frequent itemsets quickly. D according to central limct theorem, n must be as  Our model is required to discover frequent itemsets foiiowS: of interest using a 2-step procedure: (1) Generate a random subset of a given large database; and (2) Generate all frequent itemsets in the random subset  minimum support.

(1) Z?1+<1/2 n >  ~  by pruning, having support greater than or equal to 4v2  where zz is a standard normal distribution function.

n 3 Generating Instance Set We now illustrate the use of this theorem by an  example as follows.

In probability theory, if a situation is such that only two outcomes, often called success and failure, are ~~~~l~ 1 suppose a new process is possible, it is usually cailed a trial. The variable ele- doping silicon ment in a trial is described by a probability distribu- (unknown) is the tion on a sample space of two elements, 0 representing in this way is defective. We assume that the defec- failure and 1 success; this distribution assigning the tive =hips are independent of each many Probability - to 0 and 0 to  1, where 5 2 chips, n,  mast we pmduce and test so that the pmpor- Suppose we consider n independent repetitions of a tion of defective chips found (Aue(X,)) does not dif- given trial. The variable element in these is described fer  fmm at least by a probability distribution on a sample space of 2" 0.99? That is, we want n such that P(IAve(X,)-pl < points, the typical point being z = (zirzzl ...,zn), 0.01) > 0.99, q = 0.01,E = 0.995,z0.~~ = 2.57, we where each z, is 0 or 1, and zi represents the result hawe of the ith trial. The appropriate probability distri- bution is defined by  _.

for used in  electron;c devices,  that each  by  more than 0.01, with  n =  ~ 2'572 = 16513, 4 * 0.012  p e ( z )  = om(zl(1 - Q)n-m(=),  where m(z) = E:=, z, is the number of Is in the results of the n trials, this being so since the trials are independent.

Given an z in this situation it seems reasonable to estimate 0 by m ( z ) / n ,  the proportion of successes  considerably smaller than the value n = 27000 that is needed b y  using the approzimating model in Chernof bounds p, 41.

Based on Theorem 1, the method of generating random database from real database is as: (1) gen- erating a set X of pseudo-random numbers, where     1x1 = n ;  and (2) generating the random database RD from D using pseudo-random number set X .  That is, for any zi E X ,  get (zi + 1)th record of D and append it into RD.

Note that generating random database RD of the given database D doesn't mean to establish a new database RD. It only needs to build a view RD over D.

4 Identifying Approximate Fre- quent Itemsets of Interest  In this section, we present an efficient algorithm by pruning all uninterested frequent itemsets.

Piatetsky-Shapiro [2] argued that a rule X --t Y is not interesting if support(X U Y )  e suppurt(X) x support(Y). Using this argument, we establish an al- gorithm below for searching itemsets of interest in a database quickly.

Algorithm 1 FrequentIternsetsbyPmning  mininterest: minimum interest; Input: D: data set; minsupp: minimum support;  Output: Frequentset: frequent itemsets;  (1) let frequent itemset set Frequentset t 0; generate the sample RD;  (2) let L1 t {frequent 1-itemsets}; let Frequentset t Frequentset U L I ;  (3) for ( k  = 2; Lk-1 # 0; k + +) do begin //Generate k-itemsets of interest in RD.

let ck t the k-itemsets in E D  by Lk-1; for any transaction t in RD do begin  //Check which k-itemsets are included in t .

let Ct t k-itemsets in t contained by C,; for any itemset A in Ct do let A.cmnt t A.cuunt + 1;  end  let LI; t {.IC E CK A (p(c) = (c.count//RDI) >= minsupp)};  let Frequentset t Frequentset U Lk; //Prune all uninterested k-itemsets in ck  for any itemset i in ck do if an itemset i is uninterested then  kt c k  c k  - { i } ; end  (4) output frequent itemsets in Frequentset;  ( 5 )  endall.

The algorithm FrequentItemsetshyPruning gener- ates all frequent itemsets of interest in sample RD.

It is similar to the former algorithm FrequentItem- sets. So we only elucidate the differences in Step (3). Step (3) generates all sets Lk for k 2 2 by a loop, where Lk is the set of all frequent k-itemsets in RD generated in the kth pass of the algorithm, and the end-condition of the loop is Lk-1 = 0. For k >= 2, we need to prune all uninterested k-itemsets from the set C,. That is, for any itemset i in ck, if ip(X U Y )  - p ( X ) p ( Y ) /  < mininterest for any ex- pressions i = X U Y of i, then i is an uninterested frequent itemset, and it must be pruned from ck.

To demonstrate the use of the above algorithm, we present the following example.

Example 2 Let RD a subset of a given transaction database D with 10 transactions in Table 1 is obtained from a grocery store. Let A = bread, B = coffee, C = tea, D = sugar, E = beer, F = butter. As- sume minsupp = 0.3 and mininterest = 0.07. The supports of frequent itemsets in Frequentset b y  Fk- quentItemsetsbyPruning are shown below.

Table 1 Transaction databases in RD I "hnsaction ID 1 Items  In Table 1, there are six 1-itemsets: A,  B ,  C , D ,  E ,  and F in RD. For minsupp = 0.3, they are all frequent 1-itemsets in Frequentset. And L1 = { A , B , C , D , E , F } .

In Table 1, the set CZ of the 2-itemsets is: AB,  AC, AD, AE ,  AF ,  BC,  BD,  B E ,  B F ,  CD,  C E ,  C F , DE, D F ,  and EF.  Each one of the above 2-itemsets contains a t  least a subset of L I .  For minsupp = 0.3, LZ = { A B ,  AC, AD, BC, BD,  BF,CD, C F }  are all frequent 2-itemsets.

However, for minsupp = 0.3 and mininterest = 0.07, Lz = { B C , B D }  are all frequent 2-itemsets in Frequentset listed in Table 2 below, for the al- gorithm FrequentItemsetshyPruning. Certainly, b e cause Ip(A U B )  - p(A)p(B)I = 0.05 < mininterest, Ip(A U C )  - p(A)p(C)l = 0 < mininterest, Ip(A U     D )  - p(A)p(D)I = 0 < mininterest, Ip(B U F )  - p ( B ) p ( F ) /  = 0.05 < mininterest, Ip(C U D )  - p(C)p(D)l = 0.06 < mininterest, and Ip(C U F )  - p(C)p(F)I = 0 < mininterest, so AB, AC,  AD,  B F , CD, and C F  are not of interest. Hence, AB, AC, AD,  B F ,  C D ,  and C F  are pruned from Lz before it is appended into Frequentset.

Table 2 2-ltemsets in Frequentset I Item 1 Number of I Support I  In Table 1, because all 3-itemsets are pruned are pruned from C, because all of them are not of in- terest. And Step 2 ends. The frequent itemsets in Frequentset are output.

As we have seen, there are only two frequent 2- itemsets in Frequentset by the algorithm Frequen- tItemsetshyPruning after the uninterested frequent itemsets are pruned. Totally, there are eight frequent itemsets of interest in Frequentset.

5 Experiments To study the effectiveness of our model, we have performed several experiments. Our server is Ora- cle 8.0.3, and the algorithm is implemented on Sun SparcServer using Java, and JDBC API is used as the interface between the program and Oracle. To evaluate our model, we have used market transaction databases from the Synthetic Classification Data Sets in Internet (http://www.kdnuggets.com/). We eval- uated two methods: the sampling approach based on Chernoff bounds [3,4] (denoted LRD) and our model (denoted C R D ) .  Below Table 3 summarizes the pa- rameters for the data sets.

Table 3 Synthetic data set characteristics (T = row size on average, I = size of mazimal frequent sets on auerage).

1 Data set name 1 lRI I T I I I lrl I  The comparison of our model on C R D  with the model on LRD in running time is illustrated in Fig- ures 1 and 2. From the figures, our model CRD needs lesser time than the model LRD hecause the latter needs larger sample than the former and the former applies more heuristic information to reduce the searched space.

Figure 1: Running time for minsupp = 0.01  7 !e- Figure 2: Running t ime for rninsupp = 0.015  6 Conclusions Mining frequent itemsets is an expensive process.

Mining approximate frequent itemsets on a sample of a large database can reduce the computation cost significantly. [3] and [4] applied the Chernoff hounds to discover frequent itemsets in large databases. For many applications such as marketing and stock in- vestment, they may be time limited and little re- quirement on the accurate of the supports of itemsets.

For example, a short-time stock investor take time as money. To satisfy these applications, we have pre- sented an efficient model for identifying approximat- ing frequent itemsets of interest in this paper. Our experiments have shown that the proposed approach is effectiveness and efficiency.

