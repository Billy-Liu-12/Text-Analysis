Aggrandizing Hadoop in terms of Node  Heterogeneity & Data Locality

Abstract- The growth of data has increased exponentially in recent years. In this context, data-center scale computer systems are built to meet the high storage and processing demands of these applications. Such systems are composed of  hundreds, thousands, or even millions of commodity computers connected through a LAN housed in a data center. It has a much larger scale than a traditional computer cluster. Hadoop enables the distributed processing of large data  sets across clusters of commodity servers. It is designed to scale up from a single server to thousands of machines, with a very high degree of  fault tolerance. Strength of Hadoop is in its  ability to detect and handle failures. The original Hadoop native task scheduler implicitly assumes that cluster nodes are homogeneous. This assumption is used to identify a slow task and re?  execute it. However, this assumption does not hold where the cluster nodes are heterogeneous, since speculatively identifying a slow task will give rise to erroneous conclusions. In  MapReduce, the sub-task is transferred to a node for execution. The input to the subtask, if not present in the node, must be transferred from another node in the network. Transferring data  takes time and delays execution. In this paper, we have proposed a methodology for improving Hadoop in-terms of Heterogeneity and Data Locality. The performance of improved version  can be measured using these metrics, i)execution time, ii)response time, iii)tasks submitted, iv)time  related to jobs i.e. arrival, start and completion,  v)completed task, vi)fairness, vii)locality and  viii)mean completion time.

Keywords- Big Data; Cloud Computing; Schedulers; Data Computing; Heterogeneity

I. INTRODUCTION  In recent years the growth of data has increased exponentially. In today's world, data is growing exponentially, doubling its size every three years. Huge amounts of data are being generated from digital media, web authoring, scientific instruments, physical simulations, and so on. Electively storing,   Suresh Jaganathan Assistant Professor  Department of Computer Science and Engineering  Sri Sivasubramania Nadar College of Engineering,  Chennai, Tamilnadu, India Email: whosuresh@gmai1.com  querying, analyzing, understanding, and utilizing these huge data sets, presents one of the grand challenges to the computing industry and research community. The popular solutions are to build data-center scale computer systems to meet the high storage and processing demands of these applications [1]. Such a system is composed of hundreds, thousands, or even millions of commodity computers connected through a local area network housed in a data center. It has a much larger scale than a traditional computer cluster, while enjoying better and more predictable network connectivity than wide area distributed computing. Large scale data computing is a main technology driver for the many Internet services we see today. Data computing manifests as two classes of workloads. The first are usually called customer-facing applications that process many interactive requests from hundreds of millions of users within a short response time. The second are called backend applications that conduct data analysis jobs in batch mode, where each job may involve petabytes of data. Processing this data for the job at hand to achieve a competitive advantage should be the driving goal.

Data is termed "big data" when it outgrows the current ability to process, store, and manage it efficiently. Storage of data has become very cheap in recent years, and it has become easy to collect enormous data. However, our ability to actually process big data has not scaled fast.

Traditional tools designed for the analysis and storage of data, were not designed to deal with big data problems. Well known tool for processing and storing large amounts of big data is Hadoop [2]. Hadoop uses hundreds or thousands of computers to solve the big data problem, rather than using a single machine.

With Hadoop data mining, analylics, and processing of big data becomes efficient.

Google's research inspired the implementation of an open-source project Apache Hadoop. In      Hadoop, cluster refers to a group of coordinated computers and nodes refer to the individual computers in the cluster. Hadoop's core is the Hadoop File System (HDFS) [3] and Map Reduce. They are designed in a manner to handle large volumes of data across a number of nodes. In order to respond to client application's request, Hadoop performs parallel processing across commodity servers. The key idea is in parallelizing the data access across the commodity servers.

IT. SCHEDULERS  A. The LATE Scheduler  The performance of Hadoop Map Reduce framework depends entirely on task schedulers.

The task submitted to a failed node is re-run on.

If a node is available but its performance is poor, then a backup copy of the task is executed in another node. The LATE (Longest Approximate Time to End) scheduler [4] is based on an intuition: "tasks with the largest estimated finish time should be backed up on fast nodes". Main innovative techniques in LA TE include techniques to estimate a tasks time to complete and to estimate which nodes are fast nodes. A scheduling algorithm ranks running tasks based on their estimated time to completion, allow fast nodes to request and get top ranked tasks to run. The LATE mechanism deals with both heterogeneity and node failures.

The LATE scheduler uses three configurable parameters, called SlowNodeThreshold, SlowTaskThreshold and Speculative Cap to help improve scheduling results. i) SlowNodeThreshold is the percentile of speed below which a node is deemed too slow to receive speculative tasks. ii) SlowTaskThreshold determines whether a task is slow enough to be speculated on. Speculative Cap marks the number of speculative tasks that can be running at once, to prevent overloading.

The LATE scheduling algorithm runs on the scheduler node and works as follows: i) when a worker node asks for a new task and the total number of running speculative tasks is less than the Speculative Cap. ii) Ignore the request if the total progress of the node is less than SlowNodeThreshold. iii) Rank every running task that is not speculated by estimated time to completion. Find out the highest ranked task with progress rate under SlowTaskThreshold and launch a copy of this task.

Merits of LATE are: i) LATE focus on estimating the remaining execution time. ii) LATE speculatively executes only tasks that will improve job's response time.

Demerits of LATE are: i) LATE use an improved strategy to launch backup tasks, but frequently choose wrong tasks to re-execute.

This is because LATE does not approximate TimeToEnd (TTE) of running tasks correctly.

B. Self-adaptive MapReduce Scheduling Algorithm (SAMR)  SAMR [5] shares a similar idea with LATE scheduling algorithm, of using static method to compute the progress of tasks. However, SAMR also uses historical information to tune weights of map and reduce stages and to get more accurate progress scores than LATE.

SAMR also estimates the remaining execution time to fmd slow tasks. However, SAMR does not use the fixed stage weights for map and reduce tasks. Unlike Hadoop default and LATE schedulers, which assume MI, M2, RI, R2, and R3 are 1, 0, 1/3, 113, and 113. SAMR records M I, M2, R I, R2, and R3 values on each Task Tracker node and uses these historical information to facilitate more accurate estimation of task's TTE (Figure. 1). SAMR falls short of solving one crucial problem. It fails to  lformation 2. Set up stage weights and ,d R3) ::::?t:  n  t??:mainingr-__ -----'  :al information 3. Calculate stages weights after  tasks have been finished  Figure 1. Historical Information  consider other factors such as different job types and different job sizes that can also affect stage weights.

Merits of SAMR are: i) SAMR use historical information recorded on each node to tune the weight of each stage dynamically. Instead of setting M2=0, SAMR takes the two stages of a map task into consideration for the first time.

Demerits of SAMR are: i) Although SAMR uses historical information stored on each node to set a more accurate estimate of PS than LA TE, it does not consider that different job types  can have different weights for map and reduce stages. In addition, the same type of jobs can even have different weights for map and reduce stages when handling datasets with different sizes.

C. Enhanced Self-adaptive MapReduce (ESAMR)  Like SAMR, ESAMR [6] is inspired by the fact that slow tasks prolong the execution time of the whole job and different amounts of time are needed to complete the same task on      different nodes due to their differences, such as computation and communication capacities and architectures. [n addition, to consider other factors that affect a task's progress ESAMR incorporates historical information recorded on each node and uses K-means cluster identification algorithm to tune parameters dynamically and find slow tasks accurately. As a result, ESAMR significantly improves the performance of Map Reduce scheduling in terms of launching the backup tasks.

The performance of Hadoop Map Reduce [2] frame-work depends on its task scheduler.

The task scheduler handles node crashes by re? running a failure task on another machine. The scheduler speculatively executes a backup copy of its tasks in order to minimize jobs response time. The original Hadoop native task scheduler monitors task progress. [t makes implicit assumptions that cluster nodes are homogeneous and tasks progress linearly. These assumptions do not always hold. Heterogeneity seriously impacts the scheduler using a fixed threshold for selecting speculative tasks. Too many speculative tasks maybe launched. The wrongly speculated tasks may be chosen first, resulting in contention and wastage of resources. Heterogeneity will be a problem as multiple generations of hardware are being used. These factors indicate that the identification of stragglers (poorly performing node) in Map Reduce will be a very important problem.



III. PROPOSED METHODOLOGY  The architecture of the proposed methodology consists of the following five distinct levels as shown in Figure.2.

The first level of the proposed architecture is the operating system. The operating system acts as an intermediary between programs and the computer hardware. It manages hardware resources and provides common services for system software and application software. The second level is the Java Virtual Machine (lVM).

???????? ??????????????u  Figure 2. Proposed Architecture  The third level components are classified into three broad categories. i) Data Storage, ii) Data Processing, and iii) Coordination. Data Storage: HDFS [3] is a data storage component of Hadoop. The Hadoop Distributed File System (HDFS) is designed to run on commodity hardware. [t is a distributed file system. HDFS is a highly fault-tolerant system and is designed in a manner to be deployed on hardware. HDFS provides high throughput for access to application data. [t is also suitable for applications that have very large sets of data.

Data Processing: Map Reduce is the data processing component of Hadoop. Map Reduce is the key algorithm used by Hadoop for distribution of jobs in a cluster. The operation of a Map Reduce job runs in parallel if every Map and Reduce is independent of all the other Map and Reduce executions. Coordination: Coordination among the components used in Hadoop environment is achieved through Zookeeper component [1]. Maintenance of configuration information, naming, providing distributed synchronization, and group services is done by Zookeeper. Distributed applications use all the mentioned services in one form or another.

The fourth level is the network layer.

Computers and other hardware components are interconnected by communication channels that allow sharing of resources and information. The fifth level is the user application layer.

[V. EXPERIMENTAL RESULTS  [n our proposed work we created a test bed consisting of a master node and 6 slave nodes.

The configuration details of the participating systems are given in Table I and Figure.3.

Hadoop is configured on the systems and the results are calculated.

[n a Map Reduce task, the input data-set is split into independent chunks which are processed in parallel. The outputs of the map tasks are sorted, which are then passed as input to the reduce tasks. The input and the output of the executed jobs are stored in a file-system.

TABLE I. NODES CONFIGURATION  Nodes Con figu ration Master 8GB RAM, i5 Processor Node Client-l  8GB RAM, i5 Processor Client-2 Client-3 2GB RAM, Pentium-4 Client-4 Processor Client-5 1 GB RAM, AMD Athlon Client-6 Processor      Test Bed  Master Nooe  Slave-I Slave-3 Slave-6  The tasks are scheduled and monitored, failed tasks are re-executed. In Hadoop both the Map Reduce framework and the Hadoop Distributed File System run on the same set of nodes. By this framework, the tasks are scheduled to execute on the nodes where data is already present.

In each node of the cluster in the Map Reduce framework a master Job Tracker and one slave Task Tracker is present. The master performs scheduling, monitoring and re? execution of the failed tasks in the component's slave nodes. The tasks are executed by the slave nodes as directed by the master. The input/output locations are specified by the applications. The map and reduce functions are specified through the implementation of appropriate interfaces or abstract classes. The job in jar or executable format is submitted to the Job Tracker. The Job Tracker distributes the job to the slaves, schedules the tasks, monitors them and provides status infonnation to the job? client.

The Map Reduce framework performs operations on the <key, value> pairs. The framework considers the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job.

The framework serializes the key and value classes.

(input) <kl, vI> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> ( output)  Word Count example scans the text files and counts the occurrence of each word. The input to the program is a set of text files and the output is text file, where each line of which of the output file contains the word and the occurrence of the word, separated by a tab.

A line is taken as input and the mapper breaks it down into words. A key/value pair of the word and 1 is produced as output. In each reducer the counts of each word is summed up and a single key/value with the word and total occurrence is given as output.

In an effort to optimize the performance, the reducer is also used as a combiner on the map outputs. Each word is combined into a record  this reduces the amount of data transferred across the network.

There are a large number of simulating tools available to simulate distributed environments.

However, there are only two or three simulators available which specifically targets the Hadoop environment. One such simulating tool is called MRPrf. Based on testing that was performed on the MRPrf, accurate results for jobs of different type of algorithms or different cluster configurations could not be generated. Cardona et al [7] implemented a simulator to simulate Map Reduce jobs. The simulator was not able to accurately simulate wide range of Map Reduce based applications due to lack of consideration of a number important parameters. Mumak [8] is another implementation of Hadoop environment simulator. Mumak uses data from real experiment to estimate completion time for Map and Reduce tasks with different scheduling algorithms.

These earlier works have produced a design and an implementation for Map Reduce simulator (MRSim)[9] to simulate the behavior of newly developed data mining Map Reduce based algorithms in a Hadoop environment.

MRSim can be used to evaluate the behaviors and the scalability of newly developed algorithms for Map Reduce environments.

MRSim extends SimJava to accurately simulate the Hadoop environment. Using SimJava [10], MRSim simulates interactions between different entities within clusters. The Evaluation of MRSim is performed using a number of Map Reduce based applications which have been implemented recently. Evaluation results show a high level of accuracy from different aspects.

A. Test Results using MRSim  The following results were obtained by simulation of word count application in MRSim Test Bed environment set:  Homogeneous Nodes:  Participating Nodes: Master Node, Slave-1 and Slave-2.

Map Start Compl Time Size etion s Time Time Taken  64M 27 10:00: 10:03: 00:03:03 B 55 58  14 ?2 10:08: 00:02:59 MB 21  10: 10: 10: 13:  00:03:48 MB II 59  10: 15: 10: 19:  00:04: 14 MB 06 20   10:21: 10:25: 00:04:18 MB 32 50      Heterogeneous Nodes:  Participating Nodes: Master Node, Slave-3 and Slave-4.

Size Ma Start Completi Time ps Time on Time Taken  64MB 27 10:30:  10:33:52 00:03:  03 49 128M  14 10:35: 10:38:58 00:03: B 22 36 256M  10:40:  10:44:49 00:04:  B 11 38 512M  10:45:  10:50:19 00:05:  B 06 13 1024M  10:51 :  10:56:23 00:04: B 32 51 . .

PartIcIpatzng Nodes: Master Node, Slave-5 and Slave-6.

Size Ma Start Completi Time ps Time on Time Taken 64MB 27  11: 10: 11:14:51 00:04: 05 46  128M 14 11: 16: 11 :20:53  00:04: B 12 41  256M  11:22: 11:28:24 00:05:  B 55 29 512M  11 :30:  11 :36:39 00:06: B 32 07 1024M  II :38:  II :44:17 00:05:  B 22 55 . .

PartIcIpatzng Nodes: Master Node, Slave-I, Slave-2, Slave-3 and Slave-4.

Size Ma ps 64MB 27  128M  B 256M  8 B 512M  7 B 1024M  7 B  Start Completi Time on Time  10:44: 10:47:36  10:52:  10:55:21  11:18: 11 :22:30  11 :08: 11:12:20  11: 13:  11: 17:23  Timetaken [using M R-SI M] for various c lus1er c onfigu ra;;ions  [Word-cwnt Application)  Time Taken 00:02:  00:02:  00:04:  00:03:  00:04:   7r------?---r---6.5 Cluster I Ylle 1 ........... .

i 6 Clu steo:'-:-'YjJe 2"' ? -""- " ;::; 5.5 ?.Gkrsler "'Ylle 3 .? ? 5 _ .. . --.- C ??I!..;?'pe''l-?-......??"' . 6  ? :?i .:===::?=:: :::;.;???:!:::::::>'-;;::;:::::::::.::=:=:::: ? 2.5 i= 2 1.5 l L------?---?--  64 120 256 512 1024 ElI?uck Size[f'IIB]  Figure 3. Result-Using MRSim  The result obtained for the varying block sizes for all the test bed sets is illustrated in Figure 4.

B. Test Results using Hadoop The following results were obtained by  executing word count application in each of the Hadoop's Test Bed environment set:  Homogeneous Nodes:  Participating Nodes: Master Node, Siave-l and Slave-2.

Size Ma Start Completi Time ps Time on Time Taken  64MB 27 14:16:  14:18:46 00:02:  02 44 128M  14 14:20: 14:23:04 00:02:  B 09 55 256M  14:24:  14:28:39 00:04: B 25 14 512M  14:30:  14:34:15 00:03:  B 18 57 1024M  14:35:  14:39:10 00:04:  B 08 02  Participating Nodes: Master Node, Slave-3 and Slave-4.

Star Com Tim Siz Ma t pleti e  Tim on Tak e ps Tim e en e 64 27 19:3 19:4 00:0  MB 7:33 1:24 3:51  14 19:4 19:4 00:0 MB 3:59 7:36 3:37  8 09:0 09:0 00:0 MB 5:15 9:51 4:36  09:2 09:2 00:0  MB 3:12 8:23 5: 11  09:3 09:3 00:0 4M 7 0:17 5:06 4:49 B  Participating Nodes: Master Node, Slave-5 and Slave-6.

Com Time Size Ma Start pleti Take ps Time on  Time n  64M 27 09:23 09:28 00:04 B :14 :01 :47  14 09:30 09:35 00:04  MB :23 :02 :39 256 8 09:41 09:46 00:05 MB :04 :34 :30  09:50 09:56 00:06  MB :12 :21 :09      10:03 10:09 00:05  4M 7 :46 :39 :53 B  Participating Nodes: Master Node, Slave-I, Slave-2, Slave-3 and Slave-4.

Start Compl Time Size Maps etion Time Time Taken  64MB 27 10:44:5 10:47:3 00:02:4  4 6 2 128M  10:52:2 10:55:2 00:02:5  B 4 1 7 256M  II: 18: I 11 :22:3 00:04:1  B 9 0 I 512M  11:08:2 11: 12:2 00:03:5  B 2 0 8  11:13:1 11: 17:2 00:04:0  MB 9 3  Timetaken [using Hadoopj for various cluster configurations  [Wordcount Application] 7.-----,-----.-----.-----?  'OJ' 6.5 Cluster Type' 1 ......... ..

CD 6 Cluste? Jype'2-"''''''':: I "5 5.5 .D lu?s?ler Type 3 ? 5 Clus\.el.:typtf"4' .. " ...... .,,_  ! :1 =;::;::;;:?'4=:== CD E i=  2.5  1.5 1 L-----'---------'---------'--------'   64 128 256 Block Size[MB]  512 1024  Figure 4. Result-Using Hadoop  The test bed has been configured into four sets. The application has been executed in each set with varying input block size of 64MB, 128MB, 256MB, 512MB and 1024MB in MRSim and Hadoop. The outcome of the execution of all the input block sizes in the various test beds is illustrated in Figure 5. The Homogeneous (8GB RAM: Cluster Type 1) set consists of a master and two slave nodes all running on 8GB RAM. The Heterogeneous (2GB RAM: Cluster Type 2) set consists of a master (8GB RAM) and two slave nodes running on 2GB RAM. The execution time of the application for all the input block sizes is delayed by about \ minute when compared with Homogeneous (8GB RAM) set. The Heterogeneous (\ GB RAM: Cluster Type 3) set consists of a master (8GB RAM) and two slave nodes running on 1 GB RAM. The execution time of the application for all the input block sizes is delayed by about 1 minute when compared with Heterogeneous(2GB RAM) set.

The Heterogeneous (8GB+2GB RAM : Cluster Type 4) set consists of a master(8GB RAM), two slave nodes running on 8GB RAM and two other slave nodes running on 2GB RAM. The execution time of the application for all the input block sizes is decreases by about 0.5 minute when compared with Homogeneous (8GB RAM) set.



V. CONCLUSION  Hadoop has been successfully implemented in various industries to process the huge data load. However, the current hadoop scheduler has two major shortcomings. The Hadoop scheduler's assumption that all participating nodes are homogeneous forces it to speculatively execute erroneous tasks in a heterogeneous environment. The other drawback arises in having to transfer a data from elsewhere in the network to a node containing the task. Currently we developed a test bed containing 7 systems with different configurations and checked Hadoop's performance with an application. Four different sets of test are conducted using Test bed. The input data is a 1.6GB file which has been split into varying block sizes of 64MB, 128MB, 256MB, 512MB and 1024MB. The standard word count example program has been executed in the four sets, for five different input block sizes and a graph has been plotted for the time taken to execute. The time taken to execute the program will be lesser when the stated shortcomings are rectified, which is in progress.

