A Support-Based Vertical Partitioning Method for Database Design

Abstract?In association rule mining, support is a measure of association between two sets of items, which indicates the relative occurrence of both sets within the overall set of transactions.

In this paper, we propose a support-based vertical partitioning method that is easy to implement and can find an optimal vertical partitioning scheme. We present several experimental results to clarify the validness of the proposed method.

Index Terms?Association rules, databases, data mining, sup- port measure, vertical partitioning,

I. INTRODUCTION  Vertical partitioning is a design technique used for query optimization in databases, this is achieved by grouping the at- tributes accessed together by the queries as vertical fragments.

Vertical partitioning is an NP-hard problem [1]. Therefore, the use of heuristics is required to find a solution. Affinity is a widely employed measure in heuristic vertical partitioning techniques [2], [3], it measures how two attributes are accessed together by the queries ("togetherness"). The main disadvan- tage of this measure is that it only involves two attributes, and hence it does not show the "togetherness" of more than two attributes [4].

Data mining is a process for extracting non-trivial, implicit, previously unknown and potentially useful information from data in databases [5]. One of the most important problems in data mining is the discovery of association rules for large databases, this is called association rule mining. Association rules are a simple and natural class of database regularities.

The purpose of association rule mining is to discover the co-ocurrence associations among data in large databases, i.e., to find items that imply the presence of other items in the same transaction [6]. Its formal definition is "given a finite multiset of transactions D, the problem of mining association rules is to generate all association rules that have support and confidence at least equal to the user-specified minimum support threshold (min-sup) and minimum confidence threshold (min- conf ) respectively [7].

Gorla and Betty [8] developed an association rule-based vertical partitioning algorithm. One disadvantage of this algo- rithm is that it is necessary to specify minimum support and confidence threshold in order to find the fragments, and it is difficult to set the suite value of such thresholds to obtain the  optimal solution. In this paper, we use support as a global affinity measure between attributes, and we present a vertical partitioning algorithm, called SVP, that takes an Attribute Usage Matrix (AUM) as input and also needs a minimum support threshold (min-sup) but it is automatically determined by the algorithm. SVP consists of three steps: in the first step, an Attribute Support Matrix (ASM) is obtained from the AUM; in the second step, the min-sup value is determined, and finally the optimal fragments are found using the Connection Based Partitioning Algorithm (CBPA) [9].

The rest of the paper is organized as follows: in Section II we give an introduction of association rule and its measures. In Section III we propose the support-based vertical partitioning method. Section IV presents several experiments to show the efficiency of the proposed method. Finally, Section V is our conclusion.



II. ASSOCIATION RULES  A. Introduction  Given a set of transactions D, where each transaction contains a set of items, an association rule is defined as an expression X?Y, where X and Y are sets of items and X?Y=?.

The rule implies that the transactions of the database which contain X tend to contain Y [6].

Support and confidence are two measures of association.

The support factor indicates the relative occurrence of both X and Y within the overall set of transactions and is defined as the ratio of the number of tuples satisfying both X and Y over the total number of tuples. The confidence factor is the probability of Y given X and is defined as the ratio of the number of tuples satisfying both X and Y over the number of tuples satisfying X. The support factor indicates the frequencies of the occurring patterns in the rule and the confidence factor denotes the strength of implication of the rule [5].

Let N to be the total number of tuples and |A| to be the number of tuples containing all items in the set A. Define  support(X) = p(X) = |X|  N (1)  support(X ? Y ) = p(X ? Y ) = |X ? Y |  N (2)    confidence(X ? Y ) = P (X ? Y )  P (X) = |X ? Y |  |X| (3)  The problem is to find all the association rules satisfying user-specified minimum support and minimum confidence con- straints that hold in a given database [6].

B. Association Rule-Based Vertical Partitioning  Gorla and Betty [8] proposed an association rule-based ver- tical partitioning method. This consists of four steps: 1) First, it has to discover large itemsets which are the combinations of attributes that have support above the predefined minimum support (min-sup). The Apriori algorithm [10] is adapted for this step, 2) the algorithm filters the large itemsets, i.e., large itemsets with confidence value smaller than the predetermined minimum confidence (min-conf ) are discarded, 3) it derives the partitions selecting the disjoint large itemset. In this step the algorithm generates several vertical fragmentation schemes, and 4) the vertical partitioning schemes are evaluated to find the optimal partitioning scheme.

The main disadvantage of this algorithm is that it can find different vertical partitioning schemes using different min- sup and min-conf values. Finding the optimal scheme implies setting the suite value of such thresholds, but any guideline is given to find the correct values. Even with the suite min- sup and min-conf thresholds, the algorithm will find several partitioning schemes and it is necessary to evaluate them using a cost function in order to get the optimal scheme. It would be more efficient that the algorithm automatically could set the values of the thresholds, this would delete the complexity of determining both the adequate value of the thresholds and the optimal vertical partitioning scheme.

C. Affinity-Based Support Measure  Vertical partitioning algorithms require an Attribute Usage Matrix (AUM) as input. Table I shows an AUM of a relation with six attributes and four queries [11]. The 0/1 entries in the AUM show whether or not a given attribute is used by a given query, and the access column shows for each query the frequency of access to attributes per unit time period (e.g. a day).

Since vertical partitioning places in one fragment those attributes usually accessed together there is a need for some measure that would define more precisely the notion of "to- getherness". This measure is the affinity of attributes, which indicates how closely related the attributes are [12].

For a given pair of attributes ai and aj , of a relation schema R={a1, ...,an} that is accessed by a set of queries Q={q1,...,qq}, let acck to be the total access of a query qk and use(qk,ai)=1 when query qk accesses attribute ai, affinity is defined as  aff(ai, aj) =  q?  k=1|use(qk,ai)=1?use(qk,aj)=1  acck (4)  TABLE I ATTRIBUTE USAGE MATRIX 1 (AUM-1)  Queries/Attributes a1 a2 a3 a4 a5 a6 Access q1 0 1 1 1 0 0 acc1 = 15 q2 1 1 0 0 1 1 acc2 = 10 q3 0 0 0 1 1 1 acc3 = 25 q4 0 1 1 0 0 0 acc4 = 20  The main disadvantage of affinity is that it does not show the "togetherness" when more than two attributes are involved.

Hence this measure has no bearing on the affinity measured with respect to the entire cluster [4]. To solve this problem, we define an affinity-based support factor for vertical partitioning as follows:  support(ai, aj) = aff(ai, aj)?q  k=1 acck (5)  Here, support(ai,aj) shows the probability of accessing both attributes i and j with respect to all the accesses of the queries.

These values are used in a support-based vertical partitioning algorithm to find an optimal vertical partitioning scheme.



III. THE SUPPORT-BASED VERTICAL PARTITIONING ALGORITHM (SVP)  Algorithm 1 shows the procedure of SVP, it has three steps: generating the Attribute Support Matrix (ASM), setting the value of the threshold and getting the optimal vertical partitioning scheme (VPS).

Algorithm 1 SVP  input:  Total access frequency of queries (total_acc) AUM: Attribute Usage Matrix output:  Optimal vertical partitioning scheme (VPS) begin  {Step 1: Generating the ASM}  getASM(total_acc, AUM, ASM) {generate the ASM from AUM using equation 5}  {Step 2: Setting the threshold}  getSVM(ASM,SVM) {get the SVM from ASM} quick_sort(SVM) {sort the SVM} setThreshold(SVM, min-sup} {find the suite value of min-sup}  {Step 3: Getting the optimal VPS}  CBPA(ASM, min-sup, VPS) {get the optimal VPS using the found value of min-sup}  end. {SVP}  A. Generating the ASM  In the first step, ASM is obtained using the AUM of the relation to be fragmented and the variable total_acc, which is the summation of the access frequency of all queries, this is    presented in Algorithm 2. ASM is a symmetric n ? n matrix (n is the number of attributes of the relation). Each element of ASM is one of the support measures defined in equation 5.

Algorithm 2 getASM  input: AUM: attribute usage matrix, total_acc.

output: ASM: Attribute Support Matrix begin  support ? 0 for i from 1 to n by 1 do  for k from 1 to q by 1 do if AUMk,i= 1 then  support ? acck/total_acc ASMi,j ? ASMi,j+support for j from i+1 to n by 1 do  if AUMk,i= 1 & AUMk,j = 1 then support ? acck/total_acc ASMi,j ? ASMi,j+support ASMj,i ? ASMi,j  end_if  end_for  end_if  end_for  end_for  end. {getASM}  Table II shows the ASM from AUM of Table I. Each value gives us the percentage of the queries which contains two attributes together and the diagonal gives us the percentage of the queries which contains an attribute. For example, the 57% of the queries uses the attribute a4,(ASM4,4), in the 36% of the queries the attribute a4 is used with the attributes a5 and a6 (ASM4,5, ASM4,6) and in the 21% with the attributes a2 and a3 (ASM4,2, ASM4,3).

B. Setting the Value of the Threshold  We use the algorithm Connection-Based Partitioning Algo- rithm (CBPA) [9] to get the fragments. This algorithm finds fragments by trying to connect attributes. Two attributes belong to the same fragment only if they are "connected", i.e. if the value of support of such attributes is greater than or equal to min-sup. So it is necessary to find the suite values of min-sup to get the optimal vertical partitioning scheme.

CBPA does not find the optimal solution if most (at least half) of the attributes can be classified as connected attributes, which means that the min-sup value must be carefully choosen.

Therefore, we determined that the suite value of min-sup must be the support value for the 71% to the 75% of data. To set the value of the threshold, we first use the Algorithm 3, which gives us a Support Value Matrix (SVM) from the ASM.

Using this algorithm, we get a 2 ? number matrix (number is the number of different support values of a ASM), which stores both the different support values in the ASM and their occurrences.

In the example, getSVM (Algorithm 3) takes as input the ASM of Table II and generates the SVM presented in Table  TABLE II ATTRIBUTE SUPPORT MATRIX 1 (ASM-1) FROM AUM-1  a1 a2 a3 a4 a5 a6  a1 0.14 0.14 0 0 0.14 0.14 a2 0.14 0.64 0.5 0.21 0.14 0.14 a3 0 0.5 0.5 0.21 0 0 a4 0 0.21 0.21 0.57 0.36 0.36 a5 0.14 0.14 0 0.36 0.5 0.5 a6 0.14 0.14 0 0.36 0.5 0.5  Algorithm 3 getSVM  input: ASM: Attribute Support Matrix output: SVM: Support Value Matrix begin  number ? 0 ban ? 0 for i from 1 to n by 1 do  for j from 1 to n by 1 do for s from 1 to number by 1 do  if ASMi,j = SVMSV ,s then SVMOC ,s ? SVMOC ,s+1 ban ? 1 s ? number  end-if  end_for  if ban=0 then number ? number+1 SVMSV ,number ? ASMi,j SVMOC ,number ? 1  end_if  ban ? 0; end_for  end_for  end. {getSVM}

III. Here number=7 because we have seven different support values in ASM, so getSVM give us a 2?7 matrix, the first row of the matrix (Support Value) presents the different support values an the second row (Ocurrence) stores the number of times that each value appears in ASM.

TABLE III SUPPORT VALUE MATRIX 1 (SVM-1) FROM ASM-1  1 2 3 4 5 6 7 Support Value, SV 0.14 0 0.64 0.5 0.21 0.57 0.36  Occurrence, OC 11 8 1 7 4 1 4  The next step of the proposed method is sorting the SVM in increasing order using the algorithm quick-sort, we use this algorithm because is one of the fastest and simplest sorting algorithms, it has an average running time of ?(n log n) [13].

Table IV shows the SVM after applying quick sort.

To find the suite value of min-sup we use the Algorithm 4.

First, the occurrences of each value (second row of SVM) are accumulated and are stored in the third row of SVM (Accumulation). This is shown in Table V.

Next, the total number of data (n2 , where n is the number    TABLE IV SORTED SVM-1  1 2 3 4 5 6 7 Support Value, SV 0 0.14 0.21 0.36 0.5 0.57 0.64  Occurrence, OC 8 11 4 4 7 1 1  Algorithm 4 setThreshold input: SVM: Support Value Matrix output: min-sup begin  accum ? 0 for i from 1 to number by 1 do  accum ? accum + SVM2,i SVM3,i ? accum  end_for  total_number ? SVM3,number percentage ? total_number * 0.75 percentage2 ? total_number * 0.71 for i from 1 to number by 1 do  if SVM3,i ? percentage then if SVM3,i?1 ? percentage2 then  min-sup ? SVM1,i?1 i ? number  else  min-sup ? SVM1,i i ? number  end_if  end_if  end_for  end. {setThreshold}  TABLE V SVM-1 COMPLETE  1 2 3 4 5 6 7 Support Value, SV 0 0.14 0.21 0.36 0.5 0.57 0.64  Ocurrence, OC 8 11 4 4 7 1 1 Accumulation, AC 8 19 23 27 34 35 36  of attributes of the relation, in the example n=6, so the total number of data is 36) is multiplied by 0.75 and 0.71 respectively. In the example, this gives us 27 and 25.56. Then, the third row of SVM (Accumulation) is scanned to find a value greater than or equal to the first value (27 in the example), when such value is found (SVMAC,4), the previous value is evaluated, if it is greater or equal to the second value (25.56 in the example) then min-sup is equal to the SV of that column, otherwise min-sup is equal to the SV of the other column.

In Table V we can see that SVMAC,4 = 27, but SVMAC,3 ? 25.56, so min-sup=0.36.

C. Getting the optimal VPS  Finally to get the optimal VPS we use the algorithm CBPA, the input to this algorithm is the ASM and the threshold min- sup determined in the previous step. In the example the VPS obtained with min-sup=0.36 is {a1}, {a2,a3}, {a4,a5,a6} and this is the optimal VPS generated by Son and Kim [11] using  their Adaptable Vertical Partitioning (AVP) algorithm.

Algorithm 5 CBPA  input: ASM: Attribute Support Matrix, min-sup output: VPS begin  {initialize an array VPS of n elements with 0} f ? 1 for i from 1 to n by 1 do  if VPSi=0 then VPSi ? f for j from i to n by 1 do  if ASMi,j ? min-sup and VPSj=0 then VPSj ? f end_for  f ? f + 1 end-if  end-for  end.{CBPA}

IV. EXPERIMENTS  In this section, we show several experimental results to evaluate the efficiency of the SVP algorithm.

For the first experiment, consider the AUM of Table VI, which was used as an example by Gorla and Betty [8].

Table VII shows the SVM. In this case the values of the threshold found using the algorithm setThreshold was min- sup=0.6 because SVMAC,3>36*0.75=27 and SVMAC,2 ? 36*0.71=25.56, the optimal VPS generated was {a1,a4,a6}, {a2,a5}, {a3} and this is the optimal solution found by Gorla and Betty.

TABLE VI ATTRIBUTE USAGE MATRIX 2 (AUM-2)  queries/attributes a1 a2 a3 a4 a5 a6 Access q1 1 1 0 0 1 0 acc1 = 1 q2 0 1 0 0 1 0 acc2 = 3 q3 1 0 0 1 0 1 acc3 = 3 q4 1 1 1 1 1 1 acc4 = 2 q5 1 1 1 1 1 1 acc5 = 1  TABLE VII SVM-2 COMPLETE  1 2 3 4 Support Value, SV 0.3 0.4 0.6 0.7  Occurence, OC 19 4 8 5 Accumulation, AC 19 23 31 36  We used the example of Table VIII as second experiment which was used in [1]. Table IX shows the SVM of this example. The value determined for the threshold by the algo- rithm setThreshold in this experiment was min-sup=0.18 be- cause SVMAC,5=100*0.75=75 and SVMAC,4=100*0.71=71, we found the same optimal VPS than [1] {a1,a5,a7}, {a2,a3,a8,a9}, {a4,a6,a10}.

TABLE VIII AUM-3  Q/A a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 Acc q1 1 0 0 0 1 0 1 0 0 0 25 q2 0 1 1 0 0 0 0 1 1 0 20 q3 0 0 0 1 0 1 0 0 0 1 25 q4 0 1 0 0 0 0 1 1 0 0 35 q5 1 1 1 0 1 0 1 1 1 0 25 q6 1 0 0 0 1 0 0 0 0 0 25 q7 0 0 1 0 0 0 0 0 1 0 25 q8 0 0 1 1 0 1 0 0 1 1 15  TABLE IX SVM-2 COMPLETE  1 2 3 4 5 6 7 8 9 10 Support Value, SV 0 0.07 0.11 0.18 0.22 0.27 0.33 0.38 0.49 0.51  Occurrence, OC 30 12 20 9 4 4 12 1 4 4 Accumulation, AC 30 42 62 71 75 79 91 92 96 100  TABLE X AUM-4  Q/A a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 a14 a15 a16 a17 a18 a19 a20 Acc q1 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 50 q2 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 50 q3 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 50 q4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 50 q5 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 15 q6 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 15 q7 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 15 q8 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 0 1 15 q9 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 10 q10 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 10 q11 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 10 q12 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 1 10 q13 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 10 q14 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 5 q15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 5  TABLE XI SVM-2 COMPLETE (ROW 1=SV, ROW 2=OC, ROW 3=AC)  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 0.02 0.03 0.05 0.06 0.08 0.09 0.12 0.16 0.17 0.19 0.20 0.22 0.23 0.25 0.27 0.29 0.30 0.31 0.33  122 30 70 60 4 8 2 2 8 14 6 28 8 16 11 3 5 1 1 1 122 152 222 282 286 294 296 298 306 320 326 354 362 378 389 392 397 398 399 400  In the third experiment, we used the AUM also used in [1] presented in Table X. Table XI shows the SVM.

In this example, the value found by the algorithm setThresh- old was min-sup=0.12 because SVMAC,9>400*0.75=300 and SVMAC,8 ? 400*0.71=284, the optimal VPS generated is {a1, a4, a5, a6, a8}, {a2, a9, a12, a13, a14}, {a3, a7, a10, a11, a17, a18}, {a15, a16, a19, a20} and this is the optimal solution found by [1].

As we can demonstrate with the experiments, SVP always found the optimal VPS. It overcomes the problems of Betty and Gorla?s algorithm because it only generates one solution, which is the optimal. Betty and Gorla algorithm gets different solutions and it is necessary to use a cost function to evaluate them and find the optimal. Also SVP solves the problem of setting the value of the thresholds for Betty and Gorla and CBPA algorithms.



V. CONCLUSION  A support-based vertical partitioning method was presented, the main advantages of this method are that it uses the probability of accessing a pair of attributes respect to all the access of the queries as a global measure and it can auto- set a minimum support threshold to find the optimal vertical partitioning scheme.

