A Quality Enhancement of Crowdsourcing based on  Quality Evaluation and User-Level Task Assignment

Abstract: Crowdsourcing has recently been used in various applications, and the possibility of its utilization and importance is expected to increase continuously in the future. However, crowdsourcing cannot always ensure the precision of the results, which are generated by unspecified individuals. In particular, a more sophisticated task has more complex problems that are related to the accuracy of the result. In this paper, we propose a novel framework to improve the quality of work in a crowdsourcing environment. In this framework, we analyzes the characteristics of workers and allocates the appropriate task to individuals to improve the quality of work. It also provides cumulative voting for correct assessment instead of the majority representation method, which is more commonly used. Our experiments show that this framework facilitates effective work allocation.

Keywords: Crowdsourcing, Task Distribution, Quality Analysis

I.  INTRODUCTION Crowdsourcing refers to the practice of obtaining needed  services, ideas, or content by soliciting contributions from a large group of people, particularly from an online community, rather than from traditional employees or suppliers. This practice has attracted significant interest because of the proliferation of smart devices and the development of Web technology, and it is expected to solve various real-world problems that cannot be handled properly by traditional computing methods. The concept of crowdsourcing was first introduced by Howe[1]. Brabham defined crowdsourcing as a online, distributed problem-solving and production model[2].

Crowdsourcing has been applied to handle real-world problems, such as reCAPTCHA[3], Duolingo[4], and Amazon Mechanical Turk[5]. These frameworks provide platforms to trade crowdsourcing task via web.

However, a difficult problem that is yet to be addressed is the maximization of the capability of the masses as the crowdsourcing diffuse into the real world more and more. The current focus on the crowdsourcing task is too simplistic and does not consider the capability of the public. Existing crowdsourcing tasks assume that a set of simple jobs will be  handled, even though the public can resolve complex and difficult work, such as article working[7], translation[8,9], and planning[10]. Wikipedia[11]is a great example that shows public capability. Therefore, an important consideration in the use of crowdsourcing is to assign appropriate tasks to each individuals who can resolve the problem properly according to the characteristics of the task. Moreover, to increase the quality of the results obtained through crowdsourcing, an accurate evaluation of the results of each task is important.

We suggest a framework that can improve the quality of results in an environment to solve problems by means of crowdsourcing. This framework consists of task management, worker management, task distribution, and quality analysis.



II. RELATED WORK Most existing works on crowdsourcing typically focus on the  extension of traditional techniques into crowdsourcing and its optimization. CrowdDB[12], Qurk[13] and Deco[14] are techniques that use crowdsourcing to extend existing database systems and can use SQL-like query in a crowdsourcing platform. Studies on query utilization for crowdsourcing have been conducted, including the optimizations of approaches such as SELECT, JOIN, SORT, MAX, GROUP BY, and Entity Resolution. Others works focused on query result size prediction under an open world assumption[15], selecting the best workers from the set of specified workers and budget[16], and adopting workers in advance and grasping the work quickly to reduce searching space[17,18].

Reliability management is important for crowdsourcing. [19] suggests a method for managing the stepwise reputation of workers. Moreover, studies on the control of spammers, who solve a task randomly, and streakers, who resolve most of the work alone, were introduced. [20] suggests a task-worker matrix that recommends work based on the history of past tasks for efficient allocation of work.

Numerous works address query optimization for cost reduction. However, these works rely on simple worker selection, which does not consider the characteristics of each worker. And by using a majority representation system as a     quality evaluation, there are some waste of quality and low accuracy of evaluation. Thus, we introduce a novel framework for high quality crowdsourcing that using efficient work distribution and lower cost and higher accuracy evaluation voting system.



III. SYSTEM ARCHITECTURE We provide a novel framework that consists of task and  worker management, task distribution, and quality analysis, as illustrated in Figure 1. The task and worker management component analyzes and manages requested tasks? characteristics and registered workers. Then the task distribution component utilizes this information to assign the appropriate tasks to workers. Finally, the quality evaluation component evaluates the results of crowdsourcing and elects the best qualified result to be returned to the service requester.

A. Task management A total task set T={t1,?,tm} should be considered, and the  size of task is |T|=m. Task Level ???  refers to task difficulty, which is determined by analyzing the crowdsourcing task characteristics. For example, crowdsourcing translation can use Flesch?Kincaid Grade Level (FKGL)[21] to determine the task difficulty.

Each task information can be collected to calculate Task Level of similar future work through predetermined difficulty, actual evaluation of worker, duration of labor, and analysis of the result. The framework supports the service requester to configure the working set to have a difficulty distribution similar to the skill distribution of the existing workers.

B. Worker management Let worker set ? = {??, ? , ??} and the total number of the  workers be |?| = ?. Further, expected arriving time of each workers is ??? ? ? = {??? ? ??, ? , ??? ? ?? }, and the time duration is ??  to resolve the task of each worker ?? . The worker management component also deals with this information and  analyzes the capability of workers and evaluates each workers Skill Level ?? ?.

For the current time ??? ? ??? , ????  denotes the set of workers that are currently connected to the system. Let system variable ?? be the waiting time of tasks that the system can wait for additional workers. Additional workers who are expected to arrive sooner or later are denoted by ???  as presented below.

??? = {??|???  ???  ? ???  ?, ??? ? ?? +  ??  ?  ??? ? ??? + ??} As a result, the total set of available workers is ?????? =  ????  ?  ???  at ??? ? ??? . Additionally, we assume that the number of tasks is greater than the number of workers. In other words, if |?????? | = ??, then ? ? ??.

C. Task Distribution  Assigning the appropriate tasks to workers significantly affects the quality of the task in a crowdsourcing environment to solve complex problems. For example, we assume that we have a task ?? with ??? = 10 and workers ?? , ??  with ?? ? = 10 and ?? ? = 5. The task should be assigned to ?? than to ?? .

We assume that the arrangement that minimizes the difference between the level of skill of workers and the difficulty of the tasks is the most efficient. Specifically, the problem of finding the best work placement is as follows:  Problem 1. The Best Task Distribution  Given two sets ? and ?????? , find the function ? that has the minimum ? |??? ? ???(?)|?????????  in all possible one- to-one functions ? ? ??????  ? ?.

Brute-force search is the simplest method to solve problem 1, but we have ? ???  ways, and the time complexity is ?(?! (? ? ??)!? )  in the worst case. To reduce time complexity, we can use dynamic programming and greedy algorithm.

Figure 1. A framework for high quality of crowdsourcing     The greedy method matches each element of ??????  to each element of ? based on the minimum difference between skill level and task difficulty. Although this method does not guarantee the optimal solution, it has only O(???)  time complexity.

Meanwhile, by using dynamic programming, we can find optimal solution. In Figure 2, the dynamic programming algorithm for solving Problem 1 is described. First, the algorithm sorts the elements of ? and ??????  based on ??  and ?? . Then the minimum summation of difference between task and skill level ?(?, ?) is as follows: ?(?, ?) = ? ?? {? ?? ?????????(? ? 1, ?) + ??? ? ? ?????,     (2) ?(?, ? ? 1)},  ? ?  ?  This algorithm has O(????)time complexity, such that we can solve the problem 1 in polynomial time.

D. Quality analysis When the tasks are completed, the result is transferred to the  framework. In the case of a simple task, a proper work assignment guarantees sufficient quality. But for the difficult and complex tasks, the best performance result must be selected among several receiving candidates.

Assume that we have two candidates, and we should select the better one. For simple task example, ?What is the biggest number between 2 and 3?? This case is trivial because 3 is easily determined to be bigger than 2. Quality analysis can be quickly and easily implemented through a plurality voting system, which is every workers has one ballot.

However, in some cases, selecting the better option is difficult despite high worker accuracy because the task results have some pros and cons. For example, ?Which translation is  better than the other? or ?Which place should we go to between Sungnyemun and the Namsan Tower if someone visit Seoul for the first time?? Answers to these questions are subjective, which makes it difficult to determine a correct answer. In this case, unanimity is not guaranteed despite 100% worker accuracy because workers vote for their own answer.

As a result, query result evaluation in crowdsourcing is a difficult problem. We define this difficulty in quality evaluation in crowdsourcing as follows:   Definition 1. Hardness of Quality Evaluation  When we evaluate the result of crowdsourcing, we cannot guarantee unanimity every time. Even if the accuracy of all workers is 100%, the share of one candidate cannot be 100% because of their pros and cons.

We define this issue as hard evaluation.

Definition 2. Relative Quality  When all of the workers' accuracy is 100% for the hard evaluation, we define the share of each candidate as its relative quality.

Definition 3. Correct answer in hard evaluation  Correct answer in hard evaluation refers to the answer that has highest relative quality.

In hard quality evaluation, the plurality voting systems  have increased error rate because each worker casts one vote.

For example, we assume that two candidates, A and B, which have a relative quality ?? = 0.7  and ?? = 0.3 , respectively.

Also we assume the average accuracy of each worker is 90%.

When five workers are evaluate the two candidates based on the plurality voting system, the probability of selecting A is 0.7 ? 0.9 + 0.3 ? 0.1 = 0.66 , whereas that for B is 0.34 .

Then the probability of selecting A as the higher quality, i.e.

the correct answer for this evaluation, (0.66)? + (0.66)?(0.34)???? + (0.66)?(0.34)????? = 0.78 . Also the required average ballots for this evaluation, i.e. the cost, is 3(0.66)? + 4(0.66)?(0.34)???? + 5(0.66)?(0.34)????? + 3(0.34)? + 4(0.34)?(0.66)???? + 5(0.34)?(0.66)????? = 3.98 .

If the relative qualities of two candidates are 1.0, 0.0, the average correct answer rate is 0.99, and the average cost is 3.31. The plurality voting system has decreased answer rate and increased average cost when it apply to hard evaluation.

In our proposed framework, we apply a cumulative voting system for the hard evaluation. In hard evaluation, there is no candidate which has absolutely outstanding quality. The cumulative voting system, that each worker has more than one ballots, can close to the relative quality more quickly and accurately than plurality voting system. In the framework, we apply the cumulative voting systems as follows: Each worker grades each candidate from 0 to 100 points in increments of 10 points and the sum of all points is 100. Then the candidate answer with the highest point is to be the result answer.

We use Zipf-like distribution [22] and gravity model for estimation modeling to predict the evaluation under the cumulative voting system. For example, as the previous  Figure 2. An algorithm for finding the best task distribution by dynamic programming     example, we assume the worker who prefer A gives 70 point to A and 30 points to B by following the relative qualities. We define ??(?) as the probability that a worker who prefer A gives ?  points to A. And ??(?)  is also a probability that a worker who prefer B gives ? points to A; not to B. We set up two Zipf-like probability density functions as shown in Figure 3 and Table 1.

The skewness of each p.d.f., namely, ?? , ?? , can be calculated using the gravity model. We assume that, in the previous example, ?? = ?? ? 100 = 70 , ?? = ?? ? 100 = 30 , and ? = |?? ? ??| . To define the skewness, we use Equation (3).

? = ? ?? ? ????           (3) where ?  is a constant to define the skewness. Then we calculate ?? and ?? using ? as below:  ? = ?? ? ?? = ?? ?  ??         (4)  That is, ?? = ?? (?? ? ?)? , ?? = ?? (?? ? ?)? . The proposed model defines the skewness of two distributions using ?. So, if we estimate worker?s average accuracy, we can determine ? . Let the worker?s average accuracy is ? as the probability that every worker casts over 50 points to their preferred candidate based on relative quality. ? is described by Equation (5).

? = ? (??(100 ? ?)?? + ??(?)??)???,??,?,??  = ? ? (1/???? ?(100 ? ?)) ??  ? (1/???? ?(100 ? ?))?????  ???  ? ?????,??,?,?? + (1/???? ?(?))  ?? ? (1/???? ?(?))?????  ???  ? ???        (5)   As a result, we can determine ? by finding the solution of  the equation ?(?) = 0.

?(?) = ? ? (1/???? ?(100 ? ?)) ??  ? (1/???? ?(100 ? ?))?????  ???  ? ?????,??,?,?? + (1/???? ?(?))  ?? ? (1/??? ??(?))?????  ???  ? ??? ? ?   ?(?)  is a monotone decreasing function where ?  is a  positive real number and it has only one solution for ?(?) = 0.

Therefore we can use the Newton?Rapshon method to solve the equation. In the previous example, we apply ?? = 0.7 , ?? = 0.3, and ? = 0.9 to this equation, then we obtain ? ? 21.566777 . Figure 3 and Table 1 presents the whole probability distribution of this model as ?(?) = ??(?) ? ?? + ??(?) ? ??.



IV. EXPERIMENTAL RESULTS  A. Best distribution We begin with experiments on normal distribution data sets  with values from 20 to 80 and deviation of 10. The results of best distribution experiments are shown in Figure 4.

These result shows that using dynamic programming results in a significant reduction in the difference in accumulation of difference between Skill Level and Task Level. When the number of workers(??) is fixed and the number of tasks(?) various, the greedy and dynamic method can achieve best distribution(or almost that). But using the random method cannot get the proper distribution at any ?? and ?.

Figure 3. An example of cumulative voting modeling  Table 1. The probabilities of cumulative voting modeling example  Figure 4. The best distribution experiments result.

Furthermore, the result of the greedy method approaches the optimal solution. In most cases, we can use the dynamic method for find best distribution. But when the size of problems is too large to using dynamic method, we can use the greedy method for the close approximation of the best distribution.

B. Plurality voting (PV) vs. Cumulative voting (CV) The result of the comparison between PV and CV is  depicted in Figure 5, 6, and 7. In the Figure 5 and 6, PV and CV are compared by five evaluators with precision ? on the hard evaluation problem, where ?? = 0.7, 0.8. In addition, for the needed shares to conclude the evaluation, experiments are conducted by setting 40% (200 points) and 50% (three votes, 250 points).

Figure 5 shows that the precision of CV with 50% needed share(CV w/50%) is more accurate than that of PV with 50% share(PV w/50%). It shows that CV is more suitable for the hard evaluation problems than PV. However, in Figure 6, the number of average necessary ballots of CV w/50% is greater than that of PV w/50%. In other words, CV w/50% need more cost for more accurate decision.

However, in Figure 5, although we reduce the needed share for the CV from 50% to 40%, the rate of correct answer of CV is not decrease as so much, and it is higher than PV as ever.

Also, in Figure 6, the number of average necessary ballots of CV w/40% is lower than that of the CV w/50% and even taht of the PV. So, we can reduce the cost by reduce the needed share when we use the CV.

The proposed framework can determines the CV model according to the average accuracy of evaluators and the relative qualities of candidates. Then the framework can predict the expected rate of correct answers and the necessary ballots(costs). Adopting this prediction can control the rate of correct answers and the average number of necessary ballots through the management of needed share for determination the evaluation result.

Figure 7 shows the result of a comparison between the average necessary number of ballots (dotted line) and the ratio of correct answers(solid line) of CV and PV in accordance with the needed share for determination of evaluation (? = 0.9, ?? = 0.8). CV has a higher ratio of correctness than PV at any needed share, but the necessary cost of CV is also higher than that of PV as we seen before. Nevertheless, the decrement of correctness ratio of CV is tolerable, we can reduce the needed share rate and cost to focus on the target correctness ratio. For example, where the needed share is 50%, the correctness ratio of PV is 88.6%, and the average number of necessary ballots is 3.8. Meanwhile, in CV, where the needed share is 20%, the correctness ratio is 87.2%, and the average number of necessary ballots is 1.9. In conclusion, cumulative voting system maintains the correctness ratio by modeling the votes and can obtains a higher correctness ratio with relatively less or small additional cost.



V. CONCLUSION In this paper, we presented a novel framework to improve  the quality of result in a crowdsourcing environment. This framework using the best task distribution and cumulative voting system for the hard evaluation. As presented in the experimental results, we show that we can get the best task distribution by dynamic programming and apply cumulative voting system for higher answer rate and lower cost quality evaluation by proposed modeling method.

Figure 5. The accuracy of jury vs. % of correct answers.

Figure 6. The accuracy of jury vs. # of average necessary ballots.

Figure 7. The  % of shares to win vs. % of correct answers(solid line) and # of average necessary  ballots(dotted line)     For the future work, we plan to compare various criteria to apply to real-world task distribution. Also, more accurately estimating the relative quality and average correctness of evaluators, which are parameters of the model, is needed.

