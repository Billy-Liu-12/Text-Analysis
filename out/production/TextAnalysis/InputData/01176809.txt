AN EFFECTIVE PROJECTION-REDUCTION ALGORITHM  FOR MINING LONG FREQUENTS

Abstract: An effective Projection-reduction Algorithm for mining  long patterns frequent is pmented. A new ideal of top-down mining frequent items is adopted, and some of new conceptions such as transaction and items assofiation information tables, key-items and reduction item, projection DB. etc. are proposed, The algorithm presented is very effective for mining long frequents, the validity of proposed algorithms is proved througb analysis computing complexity, Some examples of computation are given also. The computing complexity of the algorithm has relation to the average lengtb of items reduction, the complexity approximate8 to 0.5 X M% X 0(Zs X N?*) in worst of ease. here, S is the average lengtb of items reduction under min-support given by user, N? is the number of tuples in the database, N is numbers of the transaction in databases, M is the average length of item sets in databases. On the side, using heuristic information for pruning useless candidate frequent itemsets, the efficiency of algorithm is improved notably. It is very effective for mining long frequent, since S is very short for long pattern frequent, the computing complexity approach to polynomial time.

Keywords: Top-Down, Data mining, Frequent item, Associate rules, Reduction, Projection.

1 Introduction  Instead of beginning with I-itemsets as Apriori algorithm to generate the lager candidate itemsets, we take the itemsets of transactions as candidates directly and find frequent. If this itemset ?s support is not lager than min-sup, we will generate the new candidates by with of erasing the key-item which works on the support from the itemset in order to increase their support. Then the algorithm determines which of the new candidates is frequent itemset, if not, repeat the above procedure until finding frequent itemset. From the above description, we can say this algorithm mine from top to down.

We uses the fact that if a set of items S satisfies min-sup, then any subset of S satisfies min-sup too. That means we needn?t go on finding the shorter frequent itemsets if we have got the frequent itemset. Hence we can speed up the efficiency of determining the frequent itemsets without the combination.

51 5  The thought of generating key-item from itemset Cr is to find the subset of the itemset which impacts on the support most.

The thought of generating candidate itemset can be described a recycle as follow: if the support of itemset doesn?t pass the min-sup threshold, then the algorithm will cut off a key-item to increment the support as much as possible; once the support satisfy the min-sup, and now the itemset is the frequent itemset so that all subsets of a frequent itemset are also frequent.

Pay attention to that the key-item is maybe not sole; and every itemset containing the key-item will possibly be frequent.

We will give some related concepts and definitions first, and then introduce the key-item and candidate generation algorithm.

2 Related concepts and definitions  .Definition 1. Information table of transaction-item mutuality: Let I=[il, i?,..., im) be a itemset, D=(U, A) he a transaction database, where U={X,,XZ. ..., xn), A=T U C, and C=[CI,C2 ,..., Cn), C, L I is the itemset. T=(tl.t2 ,.., t,,] is the set of transaction identifiersvm). So for every object X,EU there is a unique TID and a itemset. In order to describe this relationship between transaction and itemset exactly we define S = ( U, A, V, f ) as information table of transaction-item mutuality (information table for short) and V={O,l),f: TXC-V.

Example l., Suppose we have a transaction database as depicted in Tablel, then the above information table of transaction-item mutuality is shown as Table 2.

Table 1. Transaction database  0-7803-7508-4/02/$17.00 02002 IEEE  http://lf53.com    Table 2 Information table of transaction-item mutuality  TID I A  I B  I C  ID IE TI  T4 1 0  I I  1 0  1 0  I I  Comparing infomation table with transaction database , It is clearly that the former is the extension of the latter, and the latter is the subset of the former. So all of the information on the transaction and itemset can been showed in the table. For example, the database has the same TID with table; Every record of the database form a row in the table; The itemsets of the database are consistent with the attributes of the table; and the itemsets of the database denoted by T, accords with the attributes set of T, row in the table. But we should notice that there are two values for every attribute in the table: I means there exists the item in the corresponding row of database , while 0 means not.

In additional to the same TID, the total of 1 in every row of the table is equal to the length of the itemset in the record of database; and the total of 1 in each column is the support of the corresponding item in database. In table it is significant whether or not the value of the attribute is 1, but as for counting support , only those items that have 1 are used.

Now if we have a transaction database, then there must be a corresponding information table, the reverse is true. So we look the transaction database same as its information table except for statement especially.

On second thoughts we can regard the discemihility matrix of the information table as the discemihility of the database, reduct of the table as database ?s reduct, and table?s core as database ?s core.

Definition 2. The projection of the database: Given a itemset in the database, its items and the corresponding transactions compose a subset of the origin database. We regard this procedure as projecting of the database, and the subset of the database is called the projection database on the itemset.

Defiition 3. Core and reduct of the itemset: There.

exists a projection database if given a itemset of origin database, so the core and reduct of the project is that of the itemset. The length of the itemset decides the number of information table?s attributes.

Definition 4. Key-item sets: If the itemset of the database is candidate frequent itemset, so the core and reduct of this itemset i s  the key-item set of this candidate frequent itemset.

Detinition 5. Reduct database : Let X is the given itemset, and Y is the reduct of X. then the transaction set T of containing items of X and the elements of X compose the projection A of the origin database, and the elements of reduct Y and corresponding T compose the projection of A, called reduct database of the origin database. There are the  same transactions both in the reduct database and projection database A. Every itemset of the reduct database is the subset of that of the projection database, while every itemset of the projection database is superset of that of the reduct database.

The property nf the key-item set: Assume that X is a itemset in the projection database of transaction database projected and its support of X is support(X)=m, Y is reduct of X, then the support of Y is support(Y)=m.

Proof:  There are two case as follow, after a transaction database is projected:  ( 1 )  Any itemset in projection database is not same each other. X correspond to unique a transaction, Let Ti be its identifier, then the support of X is support ( X )  = 1.

Due to Y be a subset of X, so that support CY) >support  (X)  = 1. Assume that the support of Y is support ( Y ) >support (X)  , then there exists itemsets Xj correspond to Ti, here Xi contain Y but does not contain X, this means that Y can not discern Ti and Tj. so that Y is not reduct, this is contrary to that Y is reduct known, hence support (X) =support ( Y ) ( 2 )  Itemset X repeat K times in the projection  datahase. removing K-1 repeat from the database, Reduct of itemset is computed as above, support ( X ) =support  CY) . Then the K-l repeats is considered, Because Y is subset of X, Every transaction of containing X has Y, support CY) = support ( X )  =K, hence, support CY) = support (XI  =m.

Deduction 1: If the support of given itemset X is m, denoted as support(X)=m, and Y is the reduct of X, EX-Y, then suppolt(X) < support(Z), specially, support(X)=support(Z) is hue when Z is also the reduct of X.

Deduction 2: If the support of given itemset X is m, denoted as support(X)=m, and Y is the reduct of X, Y d[lCX, X Z G ,  but Y is not the subset of X2, then ,support(X) <support(Z).

3 The structure of candidate itemset and reducts of itemset  3.1 The structure and generating algorithm for candidate itemset  According to the property of itemset reduct and deduction as above, we have generating algorithm for candidate itemset as follow:  Suppose a itemset in database is X=(xl, -., xm] I maximum frequent itemset x?(initiating set is empty).

Input: The itemset given X=(xl, -, xm) ,  maximum frequent itemset x?.

Output: The C set of containing s candida? itemset with length m-l and maximum frequent itemset X .

51 6    Proceedings of the First lnternational Conference on Machine Lrarning and Cybernetics, Beijing, 4 5  November 2002  A I g n ri th m Gen-Candidate(X, X',C )  Q= 4 Z=X-Q  Ql=gen-keyitems(Z) //generating /key- itemset QI from itemset X, here, Q1=(ql, -.,  while (support (Z )  < minsupport and (Z)# 4 )  e l ; Q= Q U Q1 /key-itemset length is s,  E X - Q //l<s<mo  endwhile C= 4 S=IQI If Zisfrequent then  x'=x'uz endif for i=l to s  Yi=X-[qi) //generating the combination //of items with length being m-1;  If yi is not in X' then //generating //candidate frequent items:  C=CU( Yi 1 else  Yi= 4 //avoiding repeat //calculating :  endif endfor  EndGen-Canditae  3.2 Reducts of the infnnnation table  As discussed above, the key-item itemsets are the core and reducts of the itemsets. It is a NP problem to find all of the reducts of the given information system, and it is insignificant for computing the frequent itemsets. We only need to find one set of the reducts in order to determine the frequent itemset, and it is feasible in fact.

There are two methods to find reducts and core of a given set of transactions: one is Discemibility Mahix, by which we can get the core of itemset easily and then compute all of the reducts by logic operating ; the other method is to find a specifically reduct by with of heuristic information, which uses the definition of the reduct. The latter is efficient as well as simple and convenient. For example, Xiaohua Hu's algorithm for finding a specifically reduct[8], Its complexiry about is M X O(N' X N ) ,  where M is the number of item, N is the number of tuples in the relation generalized by the database, the maximum of N is N X M, N is the number of records in database.

It is more simple as for the information table system. We can get a more efficient algorithm if we improve the reduct algorithm by using the definition of reduct and the property of information table.

A reduct is the essential part of a information system, which can discern all objects discernible by the original  information system. A core is the common parts of all the reducts. The exact definition of reduct is as follows.

Let T=(U,C,D) he information system, and U=(xI.x 2 . . . ~ m ]  is a non-empty finite set of objects. D is set of decision attributes, while the elements of C are called the condition attributes. then the C-positive region of D is defined as  POSc(D)= U CX , where CX = IND(c)X, and XE U/D.

That is to say, POSc(D)= U X , where XE U/lND(C) and X CY , E U/D.

Let CEC, amihute c be dispensable in T if POS (c.(~,)  (D)= POSc(D), otherwise attribute c is indispensable in T.

T= (U, C, D) is independent, if all CEC are indispensable in T.

If the set of attributes R E C, T '  = (U, R, D) is independent, and POSR(D) = POSc(D), then the set of attributes R is called a reduct of C; The intersection of all reducts of C is called core of C.

In information system, due to U/D=([t,),(tz) ,..., ( ~ ) ] , POSc(D) is the union of the X, where XE U/TND(C) and X=(ti), here U/IND(C) is the quotient set of U dived by IND(C).

So set POSc(D) is the union of set contained only an element in the indiscriminate class of condition attribute C.

In other words, Set POSc(D) constitute of transaction identifier according to items which is not repeat in transaction database, In a other words, Set POSc(D) is made up of the transaction identifier according to items with support 1 ~ So POSc(D) is calculated by computing the non-repeat items in the database.

Hence, we present discrimination vector to compute the number of non-repeating itemsets in database, in order to save memory.

Definition 6. Discrimination vector:  Given a transaction database D = (U, A), and U = (XI, x2 ,__., xm), A = T/C , C=[CI ,..., CJ, Ci is a set of transaction. T= Itl, tz. .:.. tm) is a set of transaction identifiers. Suppose the transaction itemset Cl= iliz ... i,, Yiconsisting of all itemsets in Ci that are different with the itemsets in C1, then Y=CIY *... Y. is the discrimination vector (DV for short) of the database. If we limit Yi G CI, then the vector Y is the discrimination vector of C1.

Property of DV ( Discrimination Vector ):  1. We can compute Ci by using Cl,Yi, and then get the discrimination itemsets between any two itemsets.

2. If Yi=@ or Y,=Yj, then Ci=C1 or Ci=Ck Basing on the property of discrimination vector, we can compute POSc(D), K(C,D), SGF(a,R,D) easily.

K(C,D) = I POSc(D)I/IUI represents the dependency between C and D.

SGF(a,R,D) = K(RU (a), D) - K(R,D) is the significant value of the itemset.

I POSc(D)I is the number of those nonempty and indifferent items in the discrimination vector Y comparing with C.

51 7     I  51 8  .~  Suppose Yic Y is the itemset in discrimination vector, and Si=YinC, where Si#@ and SifSj, then Y=IPOSc(D)I is the number of Si in the vector.

K(C,D)=I POSc(D)I for convenience.

give more details.

Because IUI is a const in this procedure,, we can set  The method for reduct of itemset see [81, here do not  4 Miningalgorithm  4.1 Primary algorithm  Assume that transaction database has N records, each record has two.parts: the transaction identifiers and the itemset, every the transaction identifiers is unique.Tk denote the k-th transaction, L denote frequent itemset.

The top-down method for mining frequent from database: op-Down-Miner L=$;  For k=l to N X=Tk.C; // X=the candidate itemset in  //transaction identifiers Tr; Del-minItems(X); //remove the items with  //support less than min-support from X: If L=Q then  Gen-Candidatel(X, L, C): //Generating //candidate frequent itemset C and frequent itemset L //from itemset,  Else  endif  Endfor out L;  The algorithm for generating frequent itemset:  Gen-Candidate(X, L, C);  Gen-frequent(C, L):  EndTop-Down-Miner  Gen-frequent(C, L) Nl=ICI For i=l to NI If support(Ci)unin-support then Gen-Candidate(Ci,L,Y);//Generating candidate //frequent itemset from the key-itemset.

Gen-frequent(Y,L);Ncursion call //Gen-frequent(Y, L).

Else L = L u c i ;  endif Endfor EndGen-frequent  This is a recursion algorithm for describing principle , when all the candidate frequent generated are frequent or without any candidate frequent generated, the algorithm finish.

5 Dseussion of the complexity and efficiency  Top-down-Minfrequent algorithm provides an efficient way to generate maximum frequent itemsets which efficiency can be demonstrated by analyzing its worst case time complexity. Suppose there are N records in the database, the time complexity in the worst case is analyzed as follows. For each itemset of record , it generates frequent itemsets once. so the total times is N. suppose the average length of itemset is M, the average computing length of reduct is S, and S candidate itemsets whose length is M-l generated each time, it needs 1/2MZ for computing candidate itemsets whose length are M, M-I, ..., 2, 1 . _ _  .

and the time spend on each is at most O(2?). So the sum is U2M2 + O(2?). Since it takes M X O(N X N?) to generate reducts each time(seen [121) , the total computing time is about M X O ( N  XN?)X(1/2)M2X0(2S)XN=0.5XM3NX O(2? X N?*), here, N is the number of tuples in the relation generalized by the database.

In fact, M is dynamic and degressive, and the average computing length of reduct(S) is also dynamic and degressive because it changes according to M. in general, 1 < S < M, S=M could be Vue only when all generated candidates are reducts no matter how long the length of candidate itemsets is. And now the complexity is about 0.5 X M3NX O(2?X N Z  ). The frequent itemsets must be very short because M keeps down to 1 or 2. but it is impossible for mining long frequent itemsets since long frequent itemsets will pass the min-supp threshold when M is large.

That is to say, it will end far away O(2?). Additionally, the computing complexity can be depressed because we can prune the unnecessary branches during the mining procedure. Hence the computing complexity for mining long frequent itemsets should be 0.5 X M3N X O( 9 (S,M) X N 2  ) where ?1 (S, M) is a variable accounting to S and M.

The computing complexity approach to that of polynomial times. In most cases, the average computing length of reduct (S) will shorter after erasing the items whose support is smaller than the given min-supp. The more the frequent itemsets are determined, the faster the speed will he since the algorithm can prune in time by comparing the known frequent itemsets with candidate itemsets when candidate itemsets already include the frequents.

Especially, it will be a good idea to adopt Apriori approach if S is near to M and M is very large, and there are scarcely long frequent itemsets.

At last, the mined maximum frequent itemset is complete, since we get frequent itemsets by determining each element of the candidate itemsets ,and each candidate itemsets scan the database using the discrimination vector.

6 Conclusions  As shown above, in this paper, we proposed a top-down mining method for mining long frequent itemsets. Our algorithm combined two different mining methods, one for mining association rules, the other for mining classification rules , we think it will be a good idea to combine two or more kinds of methods. What?s more, it is still a primary     algorithm, so there are many issues that should be studied in future to improve it. The following are some topic for future research as examples: using Apriori algorithm in proper time, self-adjusting mining strategy accounting to the length of reduct, how to make the best use of the former result, how to get the least number of reducts, developing incremental mining algorithm based on top-town algorithm and so on.

