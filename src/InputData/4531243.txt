Identifying Temporal Trajectories of Association Rules

Abstract?We propose a novel postprocessing technique for identifying sets of association rules that expose a user-specified temporal development. We explicitly do not use a learning approach that requires the database to be subdivided into time frames. Instead, a global probabilistic learning method is used for induction. The resulting association rules are then matched against a set of fuzzy concepts. These concepts comprise user- built linguistic propositions that describe the evolution of rules that might be considered interesting. The proposed technique is evaluated on a real-world data set. To present the results, we introduce a modified rule visualization along the way that is an extension of our previous work [1].



I. INTRODUCTION  Since the advent of association rules [2], the basic and  universal concept of identifying frequent patterns inside data  has proven to be a fruitful and indispensable technique for  modern data analysis. However, the more data there is under  analysis, the more patterns and thus, rules, can be found.

Furthermore it is often desired that the rules to be found  refer to only a tiny fraction of the total data set. A loaning  company for example may want to assess its contracts  regarding patterns that may characterize nonpayments. Since  this refers to only a small portion of their contracts, the  notion ?frequent? has to be relaxed considerably to enable  these subsets to be found. This in turn will also return an  excessive amount of frequent subsets and rules that relate to  correctly repaid loans which overgrow the really interesting  rules.

This insight has led to numerous approaches that can  be summarized under the notion ?rule mining? or rule  postprocessing as we will refer to it in this paper. The  main issue is to select from a set of rules that obey some  basic criteria (such as minimum support and confidence, see  section II for details) those rules that qualify for interesting  candidates to be shown to the user. For example, the user  is not interested in too general rules that may be already  known or rules that are structurally different but describe  to a large extent the same items of the database. More  generally speaking, the user is allowed to choose from several  heuristics and evaluation measures that specify what is meant  by ?interesting? or ?relevant?.

Another quality of rule induction and postprocessing was  introduced by considering temporal aspects. That is, patterns  normally do not pop up all of a sudden but evolve over time.

For example, a misconfigured vehicle mark may operate well  in summer but develop more and more problems during  the transition to the winter period. Thus, the number of  failed cars (and thus possibly interesting failure patterns)  grow larger slowly. In contrast to this, some countermeasures  undertaken will take some time to have an apparent effect,  thus rendering the decrease of the failure pattern slowly as  well. Most approaches require a partition of the data under  analysis into time windows or frames (references are given  in section III).

In addition to these considerations, the introduction of  fuzzy concepts has enriched the framework of pattern and  rule mining. One has to distinguish the stage in which  fuzzy theory enters the process. Some works fuzzify the  concept of association rules [3], others allow for fuzzy time  windows [4].

In this paper we propose a method of postprocessing rules  in terms of a fuzzy description of their temporal behavior  or evolution. By evolution in time we mean the change  of support, confidence or whatever evaluation measure. We  leave the notion of association rules untouched, i. e., we use  the original non-fuzzy definition since our experience shows  that decision makers prefer definite rules to immediately  determine which database entries are covered and which are  not. However, the definition of what renders the temporal  development interesting is highly subjective and thus we  allow the user to specify her intentions about that in a fuzzy  way.

The remainder of this paper is organized as follows:  Section II briefly sketches the concepts of association rule  induction and introduces the notation used in this work.

Section III discusses previous approaches to postprocess sets  of association rules. Section IV presents our approach to  the learning task of association rules that spares us from  predefining time windows. Section V explains the proposed  technique in detail before section VI gives empirical evidence  of the benefit. Finally, section VII summarizes and proposes  further paths of investigation.



II. BACKGROUND ON ASSOCIATION RULES  The classic approach [2] of association rule inference  consists of first finding subsets of items (so-called itemsets)  that occur together in more than a predefined fraction (the  minimum support) of transactions and then trying to identify  a single item within each itemset such that the probability of  observing this item given the remaining items of the item-  set exceeds some other predefined threshold (the minimum  confidence). By transaction, we mean a tuple (like a row of  a database table) with exclusively nominal attributes. A rule  then has the following form:  A1 =a1 ? ? ? ? ?An =an ?? C =c abbr = ~a? c  To quantify a rule, a multitude of rule evaluation measures  has been devised (see, e. g. [5]). The most well-known ones  that are used here are:  ? confidence: conf(~a ? c) = P (c | ~a) ? recall: recall(~a ? c) = P (~a | c)  ? lift: lift(~a ? c) = P (c|~a) P (c)

III. RELATED WORK  As stated in the introduction, there is a lot of research  going on in the area of rule postprocessing. Therefore, we  briefly skim over some works to help the reader classify our  approach. Since we do not fuzzify the concept of association  rules, we will not go into this topic here but just refer to  works of Dubois et al. [3] or De Cock et al. [6].

Association rules w. r. t. temporal changes of patterns are  discussed by Koundourakis and Theodoulidis [7] where  the time-relevant attributes are transformed into intervals to  operate on them with known algorithms. Li et al. [8] followed  an alike approach where a temporal pattern defines points in  time where discovered items are expected to be frequent.

Zaki [9] discusses sequential itemset mining, i. e., he ad-  dresses the task of finding commonly occurring subsequences  of itemsets.

Many works on this topic require some predefined partition  of the data, namely the introduction of intervals through  discretization of the time variable. We will use such a data-  driven approach as well but only for the task of postpro-  cessing. If the time variable is continuous, it is also possible  to induce the widths of the time windows from the data as  proposed e. g. by Chakrabarti et al. in [10]. Also, the borders  of these windows not necessarily have to be crisp. Au and  Chan [4] propose a method that uses fuzzy time window  memberships to assign rules to respective windows.

Besides works on objective rule evaluation measures ([11],  [5]), there are also efforts undertaken to involve the user  into the process of postprocessing rules. Several subjective  approaches have been published to aid the description of  interestingness. Russ et al. [12] use relevance feedback from  the user to model user-centric importance.

AkAl  Ai Aj Am  C  Fig. 2. A Bayesian network structure. C?s direct parents {Ai, Aj , Am} encode a strong stochastic dependence on C.



IV. GLOBAL LEARNING APPROACH  To identify rules that exhibit an interesting temporal evo-  lution, one usually has to find appropriate temporal partitions  of the data. Then, a rule induction is conducted within each  of these fragments after which the resulting rule sets are  investigated for interesting temporal trajectories. A problem  that arises when applying an algorithm of this class is the  freedom of choice how to choose the time window sizes. This  answer may be inherently given, if the attribute carrying the  temporal information is discrete and has a small domain.

In all other cases one has to decide beforehand what way  to follow: let the user choose the desired partition (e. g.

subdivision of the data according production weeks, months,  quaters, etc.) or apply some technique to automatically find  a good partition [10]. Both approaches may be considered  unsatisfactory by users since the manually chosen partition  requires a priori knowledge about the underlying time frames  whereas the automatical induction of the frame sizes might  be considered somewhat patronizing.

Another fact that can make the temporal evaluation of rules  difficult is the possibility of not finding a rule in every time  slot. How to tell about the behaviour of a rule (i. e., the values  of rule evaluation measures) if it does not occur in all the  time slots?

To cope with these aspects, we devise the following rule  induction method.

Instead of locally searching for itemsets that meet pre-  defined requirements we use a global evaluation method to  find a model that tries to explain the entire data set. More  specific, we were inspired by Bayesian network learning  techniques. A Bayesian network uses an directed, acyclic  graph G = (V,E) to encode the decomposition of a mul- tidimensional probability distribution which resembles the  given database D. The decomposition induced by the graph  consists of a set of conditional distributions assigned to each  node given its direct predecessors (parents). For each value  of the attribute domains, the original distribution can be  reconstructed as follows:  ?a1 ? dom(A1) : ? ? ? ?an ? dom(An) :  P (A1 =a1, . . . , An =an) = ?  Ai?V  P ( Ai =ai |  ?  (Aj ,Ai)?E  Aj =aj  )    Fig. 1. The temporal evolution of the full set of association rules as they are generated by the global evaluation method described in section IV. The three charts depict the rule set at the beginning (left), the middle and the end (right) of the time span of the data set. The location of every rule is determined by its recall and lift value. The horizontal line at y = 1 indicates a lift value of 1 (lift-1 line). A rule located on this line has the property that its antecedence does not have an effect on the consequence probability. It is obvious that a manual assessment of every single rule will become tedious. Therefore, the approach presented in section V is used to thin out the rule set. Two results can be seen in figure 4 and figure 6.

One principal method of inducing this decomposition (and  thus the graph G) is to efficiently browse through the set of  all graphs and select the most probable candidate G?:  G? = arg max G  P (G | D)  The K2 algorithm [13] applied in our scenario derives from  the above equation the so-called K2 metric which is a  model averaging estimate for the goodness of fit of a graph  candidate G given a database D. Assume that the set of  attributes, V , contains a special attribute C which carries the  class information (and therefore is desired to occur in all  rule consequences later). If we coerce the algorithm never  to consider C a parent attribute but always consider every  other attribute a parent of C (which can be easily done  when using the K2 algorithm), we can assume that the direct  parents of C in the final graph G? to have the strongest  stochastic influence on C. With this argumentation, we can  use these direct parents of C as selected features out of  the set {A1, . . . , An}. Figure 2 shows an example where G = ({Ai, Aj , Ak, Al, Am, C}, E) is an induced Bayesian network structure. The subset {Ai, Aj , Am} of attributes can ? with the above interpretation ? be considered the result  of a feature selection.

Every factor  P ( Ai =ai |  ?  (Aj ,Ai)?E  Aj =aj  )  in the decomposition equation computes the confidence of a  rule of the following form: ?  (Aj ,Ai)?E  Aj =aj ?? Ai =ai  Since the attributes Aj have been selected by a global  evaluation measure, we argue that the stochastical influences  that lead to the selection are either rather omnipresent, i. e.,  reasonably strong in all thinkable time frames or have only an  influence on the class variable in a small temporal fraction of  the data set which is, however, strong enough to considerably  contribute to the global measure. With the rule set induced by  this method, we are able to evaluate every single rule (i. e.,  calculating rule evaluation measures) for every user-specified  time frame.



V. FUZZY DESCRIPTION OF RULE TRAJECTORIES  The last section presented a global rule induction method  that does not rely on predefined minimum support and min-  imum confidence thresholds but creates rules with common  antecedence attributes. Thus, if M = {A1, . . . , Ak} is the set of attributes in the antecedence and C is the class attribute,  then the number of induced rules is bounded by  |dom(C)| ? ?  A?M  |dom(A)| .

The actual number may be smaller since not every rule  necessarily has to cover database entries. However, this  number may still be too large to assess each rule manually.

By assessing we refer to the classification of the temporal  behavior of every single rule. To motivate this, we first briefly  sketch the rule visualization used throughout the remainder  of the paper.

A. Rule History Visualization  Initially, the visualization of a single rule is depicted in  figure 3: Every rule is represented as a circle the size of  which represents the absolute number of database entries that  are covered by the rule. Being covered refers to fact whether  the rule can be applied to every entry, i. e., the antecedence  and consequence match the respective values of the database  entry.

Fig. 3. The visualization of a single association rule as it is used in this paper. The outer ring encodes the values of the antecendece attributes whereas the interior represents the class value and the confidence of the rule.

The interior consists of  a pie chart representing  the distribution of the val-  ues of the consequence at-  tribute, i. e., the distribu-  tion of the class attribute  given the antecedence. For  the sake of clarity we  just depict the percentage  of one designated class  value (e. g. the value rep-  resenting failure). The an-  tecedence of the rule is  shown in the border of the  circle. For every possible  attribute (i. e., for every at-  tribute except the class attribute) a unique fragment is  reserved. If the respective attribute is referenced in the  antecedence, the corresponding fragment is filled with a color  that uniquely represents the value of the attribute?s domain.

This way of representation, of course, is only feasible if  the number of attributes and the size of their domains is  small. Otherwise, a rule can simply be represented by a circle  showing the confidence. However, if the real-world data used  to evaluate the method to be proposed in this section, the  underlying domain allowed for a representation as described  above.

When given a set of rules, we have to locate each visual  representation in a chart. We proposed [1] to assign as  coordinates the value of association rule evaluation measures.

More specific, we use the lift value of a rule as its y-  coordinate and the recall as its x-coordinate (the confidence  as a third interesting measure is represented as the pie chart  and the support is represented by the circle area). Of course,  every other selection of four measures is possible, however,  we found the named measures intuitive and will use them in  every figure of this work. To present the temporal evolution  of a rule set (w. r. t. the evaluation of selected measures), an  animation is generated that displays the current state of the  rule set at any given time (frame). To present the results in  this printed medium we will use three pictures: one of the  beginning of the timespan, one intermediary state and one  chart of the last time frame. The three rightmost charts of  figure 1 depict the temporal evolution of a rule set with 760  rules. As easily can be seen, there is a demand of thinning  out the rule set because it is not practical to assess the full set  manually since it produces an overwhelmingly large number  of moving objects even when dealing with relatively small  rule sets.

Filtering the rule set for predefined temporal patterns first  and foremost calls for a language or framework in which to  define the desired behaviour. We decided to use an approach  based on fuzzy rules to describe the temporal properties of  the evolution of rule measures. The goal is to enable the  user to specify linguistically what kind of change of the rule  evaluation measures he is interested in. For example, the user  may be interested in rules that exibit a fast increase of the lift  as well as a moderate increase of confidence. When using  the fuzzy approach described below, the user can specify  individually what ?moderate? or ?fast? means by defining  a fuzzy partition over the domains of the rule evaluation  measures of interest (this domain will very often be [0, 1]).

Since we will be able to compute for every rule of the rule  set a membership degree to which extent the respective rule  evolution belongs to the user-specified concept, we can use a  threshold to limit the set of resulting rules that are shown to  the user or order all rules by descending membership degree.

The basic idea is to allow the user to define a fuzzy  rule antecedence that contains linguistic variable assignments  over the domains of rule evaluation measures. Multiple such  assignments are combined with well-known fuzzy connec-  tives as t-norms or t-conorms. That is, the membership degree  of any rule ~a ? c to the example fuzzy description from above  ??liftis fast ? ?confis moderate?  will be evaluated as  ? ( ?  (fast) ?lift  (~a ? c), ? (moderate) ?conf  (~a ? c) )  where ? is a t-norm that represents a fuzzy conjunction.

Since we intend to assign a membership degree of the  change of any rule evaluation measure (represented by the  ? in the linguistic variable name), we need to quantify this change rate from the data set. In this work, we used  a straightforward approach of using the slope of a regression  line: for any rule, the values of the desired rule evaluation  measures are calculated for every time frame that the data set  contains. This yields a series of points that are used to induce  a regression line the slope of which is a simple (but quite  powerful as can be seen in the evaluation chapter) indicator  of the overall trend of the rule measure.

The last prerequisite that is needed is the fuzzy partition  of the domain of ?eval where eval represents any rule evaluation measure. In this paper it is up to the user to specify  how to partition this domain.

With these ingredients at hand, the rule analysis can be  summarized as follows:  1) Create a set of linguistic descriptions (fuzzy rule an-  tecedences) that refer to the temporal changes of rule  evaluation measures.

2) Provide fuzzy partitions of the domains of the change  rate of the measures from step 1.

3) Evaluate for every rule the membership degrees for the  linguistic concepts from step 1.

4) For every linguistic concept, order the rules according  to their membership degrees such that for a given  threshold the set of rules can easily be determined  whose membership degrees exceed this threshold.



VI. EVALUATION  In this section we will present empirical results that  underpin the fruitfulness of the suggested approach. We will  also point out some critical issues that are addressed in  the discussion (section VII). The data set under analysis    Fig. 4. Result of matching the complete rule set against the linguistic concept ??liftis unchanged ? ?confis incr?. No threshold was used, i. e., every rule with a membership degree greater than zero to the concept is depicted. This can lead to some odd results as e. g. rule (b). It obviously has a reasonable lift loss and its confidence decreases towards the end. However, this rule quickly vanishes from the result set if the minimum membership degree is set to 10%. Then, the result set will contain rules that exhibit a behaviour as rule (a): increasing confidence and approximately no lift change.

is a real-world set of approximately 300000 tuples that  exhibit 180 attributes. Since this data set was issued by a  industrial partner, we are not allowed to provide confidential  information such as the meaning of the attribute values or  the specific interpretation of the class variable. All we can  tell is, that every tuple in the data set represents a unique  car that left the production plant of a vehicle manufacturer.

Since for every car the time of a failure was logged as well,  we were able to partition the full set of tuples into six data  sets of (in this case) equal width.

The global evaluation method found the attributes Road-  Type and Temperature (of the area where the car was last  used) to have major impact on the class variable. Therefore,  every depicted rule has two attributes in its antecedence and  thus every rule visualization only has two colored ?teeth?  on its outer ring. Figure 1 did already present the entire  rule set that was found intractable to assess manually. In  the following we will assess this rule set with the help of  two linguistic concepts and present for both the resulting  rule trajectories.

1) Example 1: For the first analysis, we are interested  in rules that exhibit a rather constant lift but become more  probable over time (i. e., their confidence is increasing):  ??liftis unchanged ? ?confis incr?  The fuzzy partitions of the evaluation measure domains used  in these examples are depicted in figure 5. Figure 4 shows the  resulting rule subset. We depict all rules that have a positive  membership degree w. r. t. the linguistic concept, i. e., we do  not use a threshold. It is easily recognizable when compared  to figure 1 that the remaining rule set is considerably smaller,  even though we did not further restrict the set by setting a  minimum membership degree. Further can be stated, that  most of the rules exhibit the properties described by the  linguistic concept, i. e., the confidence (the pic chart of  every rule visualization) is increasing whereas the lift (the  y-coordinate) does not increase or decrease considerably as  Fig. 5. The fuzzy partitions of the domains that describe the lift change and the confidence change rates that are referred to in the linguistic concepts of section VI.

can be seen from the fact that most of the rules remain  around the lift-1 line. Of course, a lift increase from 1 to 1.5 can be considered enormous. If so, than this fact has to be reflected in the fuzzy partition of the respective domain.

The evolutions of two rules are explained in the caption of  figure 4.

2) Example 2: The second example uses a slightly mod-  ified linguistic concept. Now we are interested in rules that  exhibit a lift and confidence increase:  ??liftis incr ? ?confis incr?  Additionally, we specify a minimum membership degree  to this concept of 20%. The result rule set is depicted in figure 6. This example shows that in linguistic concepts that  use more than one criterion one has to select the fuzzy  partitions of the respective evaluations measures carefully.

As stated in the caption of figure 6, the finding of rule (c) can  mainly explained by its rather strong increase in confidence.

It outweighs the temporary lift decrease such that it still  matches the linguistic concept. However, this property can  also interpreted as robustness: a global increase of some  measure not necessarily has to consist of small increases  exclusively. It may also contain light short-term decreases  that do not effect the global trend (a stock price may be a  good analogon here).

Fig. 6. Result of example 2 with linguistic concept ??liftis incr ? ?confis incr?. The small number of rules result from the fact that the minimum membership degree was set to 20%. Obviously the rules show a raise in confidence. Also the lift increases slightly (remember, we described an increase (incr) and not a fast increase (fast incr)), however, most of the contribution for the membership degree can be amounted to the pronounced climb in confidence.



VII. CONCLUSION, DISCUSSION AND FUTURE WORK  In this paper we proposed an user-centric approach to  postprocess a set of association rules with the help of one or  more linguistic concepts that describe what kind of temporal  behavior the user is interested in. We avoided the problem  of identifying appropriate time frames of the data set before-  hand by suggesting an alternative approach of inducing a set  of rules, namely with the help of a global evaluation method.

However, every other technique may be used as the actual  contribution of this work relies on sets of rules no matter  where they come from. To aid the user in specifying his  intentions about the desired rule qualities, we proposed to use  linguistic concepts, i. e., fuzzy descriptions that are defined  over the domains of interest, here: the domain of change  rates of selected rule evaluation measures. By changing the  fuzzy partitions over these domains the user is able to relax  or tighten the strictness of his concepts. We do not restrict the  user in the design of these concepts, i. e., he may use any  fuzzy connective to combine the linguistic variables. Even  though the application of the proposed approach showed  comprehensible results, it should be considered a first attempt  to develop the field of linguistic postprocessing of rules.

The good results ? even with some severe simplification  such as the calculation of a linear regression to estimate the  growth of a rule measure ? encourage a further investigation  and strengthen the expectation of fruitful results. We are  using this work as a starting point to address the mentioned  problems. Most important, the user should be allowed to  customize the way of qualifying and quantifying the trend  or temporal behavior that he likes to be described by a  linguistic variable. For example, the current linear regression  estimation could be augmented by a regression of higher  grade such that other shapes of long-term behavior can be  identified. The membership to such a polynom would then  be based on the coefficients and call for multidimensional  membership functions. Another ad-hoc extension could be  the introduction of rule measure derivation of higher order.

The current concept of asking for changes in the values  of rule measures can be identified with considering the  first derivative in time. Using the second derivative, trend  reversals could be described and detected.

