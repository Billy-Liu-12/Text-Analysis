

Incremental Mining of Association Patterns on Compressed Data  Ng, Vincent To-Yee Wong, Jacky Man-Lee Paul Bao  Department of Computing The Hong Kong Polytechnics University  Hung Hom, Kowloon, Hong Kong  ABSTRAm  Introducing data compression concept to large databases has been proposed for many years. In this project, we propose a new algorithm for the compression of large databases. Our goal is to optimize the YO effort for finding association rules.

The algorithm partitions the databases into two parts and all transactions will be compressed with the help of a reference transaction found in the small partition.

We also compared the proposed compression algorithms with a normal compression algorithm - the binary compression. Empirical evaluation shows that the proposed algorithm performs well both in reducing the storage space and the YO process required to find the large itemsets for association NleS.

1. INTRODUCTION  In the operations of a large database, it is impossible to upload the whole database into the main memory.

This leads to the frequently read/write operations to the disk or tape. As data volume increases, the YO readwrite frequency pays an important role for the performance of database mining. By reducing the YO operations during the process, it is possible to improve the overall efficiency.

A possible method is to use compression to improve the efficiency of database mining. In data compression, most of the algorithms are based on the sequential data compressors of Lempel and Ziv [1,2] and adopted a model of compression by Shannon?s model of communication [3]. Some methods such as the statistical modeling technique [4] and the move- to-front coding [5] are also popular with high compression rate.

However there are many differences between the data compression and database compression. Many algorithms used in data compression are not suitable for database compression. The difference between them is discussed in section 3.

In section 2, we have the problem definition first.

Section 4 presents the idea of Tuple Differential Coding [6] .  In section 5 ,  we will introduce the Apriori-algorithm used to find the large itemsets for association rules [IZ]. It is followed by the description of binary compression in section 6 and the proposed compression algorithm, Referenced-Binary compression in section 7. Section 8 discusses the preliminary experimental results and we concludes in section 9.

2. PROBLEM DEFINITION There are different kinds of databases. Statistical database is one kind of databases that is commonly seen in daily life. Statistical databases refer to the database holding information for statistical analysis.

They generally have the following characteristics:  9 - Usually large in size and the retention is indeJnite. The data is normally in raw form.

More stable than ordinary databases.

Similar to relational databases that each data sei is made up by tuples and have a fued  number of attributes.

The records are usually clustered Because of their nature, the statistical database requires a lot of U0 operations  The different characteristics of a database may affect the performance of a compression algorithm. In this project, we focus on the statistical databases that have the following characteristics:  Large item range - Items per transaction is small Largedatavolume  Here, we focus on the optimization of YO operations in finding large itemsets for the association rules. The algorithm we adopted to find the association rules is the Apriori Algorithm [12]. Our idea of compression is based on the characteristics of the statistical  The work of the authors are supported in part by the Central Grant of The Hong Kong Polytechnics University, research project code GS979.

0-7803-7@78-3/0~10~~ (C)zoOl IEEE. Page: 441    databases that many tuples in these kinds of databases have similar pattems. In binary compression, many zeros are introduced after the compression. However, we observe that tuples in the statistical database are having similar pattems. If we find a reference tuple, we can represent the whole database with less space by storing only the difference between the tuples and the reference. In our work, we also consider we consider how to discover association rules incrementally when given 2 or more partitions of a database. We also apply the compression algorithm on different partition ratios to review the final compressed file size.

The algorithm we proposed is divided into two parts.

The first part is pre-processing, and the second part was the incremental mining. The details of these two parts will be shown in section 7.

3. DIFFERENCE BETWEEN DATA COMPRESSION AND DATABASE COMPRESSION  The requirements of database compression are very different from data compression. First of all, in database compression data are inherently lossless.

We can only apply the lossless techniques so as to fully recover the data from the compressed format.

Existing lossless data compression techniques can be classified into two classes, statistical techniques and textual substitution techniques. The first one focuses on how to capture the frequency of occurrence of a source word in the data stream. The second one belongs to the class of Lempel-Ziv techniques [1,2] that compress data by replacing a string with pointers to previous occurrences of the same strings.

The requirements for database compression conflict with the ideas of data compression in many ways. For example, database compression techniques should not affect standard database operations such as tuple access, insertioddeletion and any kinds of modification in the database. We can say that database compression algorithms should support random and non-FIFO access to the compressed database. This may lead to the data-model consistency problem of the data model proposed in [31.

There are lot of database compression techniques, such as the bit compression, adaptive text substitution [SI and the tuple differential coding [6,7l. In this paper, we focus on how to reduce the redundant information in the transactions of a statistical database  with the ideas similar to the Tuple Differential Coding.

4. THE TUPLE DIFFERENIAL CODING  Wee Keong Ng and Chinya V. Ravishankar introduced the TDC in [6, 9, 10, 111. The idea of TDC is to capture and store the differences among the tuples. If the differences require smaller space to store than the original format, compression is achieved.

The detail description of the algorithm is in [7]. The algorithm can be simplified as the following steps:  1. Define the whole database as a R-space 2. Use a p - function to convert each tuple into a  unique integer.

3. Partition the whole database into several blocks 4. For each blocks use thefirst tuple as the reference 5. Record and store the diflence between each tuple  and the reference tuple in each block.

In this project, we are interested on how to find a good reference transaction and how to store the differences. The details will be given in section 7.

5. THE APRIORI ALGORITHM  Rakesh Agrawal and Ramakrishnan Srikant proposed the Apriori algorithm 1121. This algorithm is used for finding the large itemsets in the process of finding association rules.

The algorithm and the notations are shown in Fig. 1 and Fig. 2:  I ,  ={Large I-itemset); For (m = 2;1k-, f 0;mtt) begh;  CDk = apriori-gen(Ik.1); For all transactions T E D begin;  C&= subset of (CD,,T); For all candidates CD E CD, begin  CD.counter+t; end;  Ik =(CDE CD, I CD.counterbin-support); end; Final answer = q It;  Fig. 1 The Apriori Algorithm.

Page: 442    CDK  lk  k - itemset  The apriori-gen function in Fig. 1 uses the set of all large (k-I) itemsets (i.e. I k )  as the input and returns a superset of the Ik The followings show how this function works:  The set of candidates of K-itemset. These candidates itemset were potentially the large itemset and each element in this set contains two fields: 1) ltemset itself 2) support counter The set of large k-itemset which already satisfj the minimum support, each of the element in this set contains two fields: 1) Itemset itself 2) support counter The itemset having k items  The insertion to CO,: Select the a.iteml,a.item2,.. .,a.itemk.l,b.itemk.I from Ik- I,a, Ik.,,b; where the a.iteml=b.iteml ,..., a.itemk.* = b.itemk.z, a.itemk.1 <b.itemk.l;  The Prune Step: Delete the itemset CD E CQ, such that the subset of k-1 itemset was not in it,  6. THE BINARY COMPRESSION In database compression we need to consider the integrity of the database. The data cannot be lost after the compression. The binary compression algorithm is a lossless compression algorithm that transforms a transaction with ASCII numbers to a binary representation.

For each item in the transaction, we use one bit to represent it, ?1? for exist and ?0? for not exist. For example, if the item range is only 0 to 10, each transaction needs 11 bits to store, i.e.

ASCII representation Binary representation  <Trans. I >  4589 0001100110 <Trans.> 1 2 3  0 1 1 1 0 0 0 0 0 0  . .  . .

When we use the ASCII format to store the transactions, say transaction 1, we need 8 X 4 bits = 32 bits ( 1  byte for one character) to store the transaction, but for binary representation we only need 1 1 bits to store the transaction.

Apparently, the binary representation (compression) performs quite well in reducing the data volume.

Actually, the performance of the binary compression greatly depends on the data distribution or the item range. In the previous example, the item range is only 0 to IO, so each transaction only needs 11 bits to store. If the item range changes to 0 - 500, we will then have:  ASCII representation Binary representation  Grans.  1> 1 455 499 0 1 0 0 ... ... 0 0 I 0 <Trans. 2> 2 500 0 0  1 o... ... 0 0 0  1  Now for transaction 1 in the previous example, the ASCII format only needs 13 X 8 = I04 bits (including the space), but after the binary compression, it requires 501 bits. The main problem of binary compression is when the item range gets bigger while the itemdtransaction is small, the bit-vector contains a lot of zeros. In the example, over 99% of the bits are zero.

7. REFERENCED-BINARY COMPRESSION In performing association mining, when we use the binary compression as a pre-processing step, there many unused zeros in the bit-vector and it will slow down the mining performance. Here, we like to propose another compression algorithms that maintains the data integrity at the same time. As before, we divide the mining process into two parts, the pre-processing and the incremental mining. Here is the outline of the Referenced-Binary Compression:  1. Partition the data file according to a threshold ratio say I:n, processes the small partition and find the large item-set in this partition. During this process we? also record which transaction contains the largest number of I-itemsets. This transaction will be used as the reference transaction in step 3.

2. Output the large item-set found in step 1 to a file.

3. Compress the whole data file according to the reference transaction in the following way:  U) Convert the ASCII transaction format into the binary one.

0-7803-7@78-WOl/$10.00 (C)uIol IEEE Page: 443    Check byte-by-byte the difference between the reference transaction and the transaction to be compressed.

If the byte pattem is different, record the position of this byte.

Repeat c) until the whole transaction has been scan once.

For bytes that are different from the reference transaction, build up a new bit- stream, like the following:  Byte- Byte- mit ion content  ... Byte- Byte- wsition content  Check if the size of the bit-stream formed in step e) is greater than the originals bit- stream. If so, write out the original bit stream; otherwise write out the bit stream formed in e) Repeat a) to f) until the whole data file was processed  Fig. 3 shows the block diagram of the algorithm.

After the pre-processing, two files are generated. The first one contains the large itemsets of the small partition of the data file. The second file is the compressed data file.

The files generated by referenced-binary compression can be further processed. After we have the two large item-set files (the small partition and the large  whole database according to the rules below.

Let S be the large itemsets for the small partition, and L be the large itemsets for the large partition.

partition), we can find the overall large itemsets of the  %E S n x e  L ,  scan for x in L to check if the occurrence of x > support percentage. If so, then x is a large item-set for the whole data file.

3 y ~  L n y s S ,  scan for y in S to check if the occurrence of x > support percentage. If so, then y is a large item-set for the whole data file.

3x E s n x  E L , x must be the large itemset for the whole data file.

The threshold ratio is chosen so that the workload of the pre-processing will not be so heavy. If the ratio is too big, the volume of  the data processed in the pre- processing will be large and it is not guarantee that we can find a better reference. The main target of the pre-processing is to find out a reference transaction.

This transaction is used as the reference for the compression of the whole database. When the differences between transactions are small, we will only need to pre-process a small partition of the database file to get a representative transaction.

Database w Partitioning  A A  Large Partition  Apriori Algorithm  l - l  Large iteinsets  Fig. 3 Block diagram of the Reference-binary algorithm.

0-7803-7078-3/OU$lO.lM (C)U)ol IEEE. Page: 444    8. EXPERIME~TAL RESULTS Initially, we like to determine what is the appropriate partition-ratios to find the reference transaction in a database. We create a data file with 10,000 transactions randomly and each transaction has an average of 30 items. In Fig. 5 ,  the results show the compressed sizes are optimal near the 25% partition ratio.

We carried out two other experiments. One is to test for the improvement of the storage space and the other is to test for the improvement of the process time. The parameters used in these experiments are:  Experiment 1 : 1. Total 10000 transactions in the data tile.

2. 10 data files with different item ranges were tested.

3. Average number of items per transaction is 30.

4. The support percentage is 5% 5. Partition ratio is 1:4  1. 10 data files with different number of transactions were tested.

2. Average number of items per transaction is 30.

3. The support percentage is 5% 4. Partition ratio is 1 :4  Experiment 2:  In experiment 1, we try to compress different data files with different item ranges and recorded their file sizes, shown in Fig 4.

Oab Ske V S L a  Ram0 lor Dlnaal  Data Fomu( 12"Q  I a l  . . . . .  I  Fig. 4 Data Size V.S. Item Range for Different Data Formats.

In the experiment 2, we process the compressed files to find the overall large itemsets and recorded the CPU time. We repeat the same experiment with the original binary file (without compression). The result in Fig 6. shows that our algorithm out-performs the original one.

Fig.5 Different partition ratios V.S Compressed file sizes.

In Fig. 6, we can see that the referenced-binary compression decreased the time for finding large itemsets. Also, as the item range increases only a small increase in the compressed file size is resulted.

If the data volume is large this difference in file size (original and compressed) can be significant.

M i n i i  l ime V.S MRmnd Fils Sue  J  Fig. 6 Process time V.S. Different file sizes with different data formats.

On the other hand, the referenced-binary compression algorithm has another advantage. When new data was appended, we can consider the new data as the large partition of the database describe in section 7. We can used the large itemset file of the original database as the reference for the new data and perform this algorithm again.

9. CONCLUSION Many databases having the characteristics described previously. One example is the selling records for a shop. For each transaction, the number of itemdtransactions was small and new data is added to the database everyday (i.e. fiequently update). The  0-7803-7U78-3/0U$lO.W (02001 IEEE. Page: 445    algorithm we proposed is suitable for this kind of databases. For databases that need to be updated frequently and satisfy the characteristics listed in section 2, the referenced-binary compression gives an efficiency way to compress the databases. When a database is updated, we do not need to scan the old database every time. With the help of the itemsets file that stored the large itemsets of an old database, we can decrease the workload of finding large itemsets for association rules.

With the referenced-binary compression, the storage space, the efficiency of analyzing or finding the association rules from the statistical database will be improved.

REFERENCE  J. Ziv and A.Lempe1. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory. Vol IT- 23, No. 3, May 1977, pp.337-343.

J.Ziv and A. Lempel. Compression of individual sequences via variable rate coding.

Vol. IT-24, No. 5, September, 1978, pp. 530- 535.

C.E Shannon, ?A Mathematical Theory of Communication,? Bell System Technical J.

Vol. 27, No. 3, pp379-423, 1948.

T.Bell, I.H. Witten, and J.G Cleary. Modeling for test compression. ACM Computing Surveys, Vo1.32 , No.4, December 1989, pp.

557-589.

J.L. Bentley, D.D. Sleator, R.E. Tarjan, and

V.K. Wei, A locally adaptive data compression algorithm. Communications of the ACM, Vol. 29, No. 4, April 1986, pp. 320- 330.

W.K. Ng and C.V Ravishankar, ?Data Compression System and Methods Representing Records as Differences between Sorted Domain Ordinals Representing Fields Values,? U.S. Patent No. 5,603,022, Feb. 1997.

Wee Keong Ng, Chinya V. Ravishankar, ?Block-Oriented Compression Techniques for Large Statistical Databases?, IEEE Trransactions on Knowledge and Data  Engineering, Vol. 9, No. 2, pp314-328 March - April 1997.

T.A. Welch, ?A Technique for High Performance Data Compression?, Computer, Vol. 17, no. 6, pp 8-19, June 1984.

W.K. Ng and C.V Ravishankar, ?Attribute Enumerative Coding: A Compression Technique for Tuple Data Structures,? Proc.

Fourth Data Compression Conf., p.461 , Snowbird, Utah, Mar. 29-3 I ,  1994.

W.K. Ng and C.V Ravishankar, ?A Physical Storage Model for Efficiency Statistical Query Proceedings,? Proc. Seventh IEEE Int?l Working Conf. Statistical and Scientific databases, pp 97-106, Charlottesville, Va., Sep.

28-80, 1994.

W.K. Ng and C.V Ravishankar, ?Relational Database Compression Using Augmented Vector Quantization,? Proc. 1 Ith IEEE Int?l conf. Data Eng., pp. 540-549, Taipei, Taiwan, March 6-10, 1995.

Rakesh Agrawal and Ramakrishnan Srikant, ?Fast Algorithms for Mining Association Rules?, Proceedings of the 20* VLDB Conference Santiago, pp. 487-499, Chile, 1994.

