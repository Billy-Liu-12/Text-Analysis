<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Procee% of  the Third Internstional Conference on Machine Learning and Cybernetics, Shanghai, 26-29 August

Abstract: The existing association-based classification algorithms suffer f" two major shortcomings: (1) they generate classifiers containing a lot of rules; (2) they consume a large amount of system resources. To remedy these problems, this paper pments a novel algorithm, namely classification based on atomic associalion NI=. Atomic rule mining generates the smallest and simplest rule set for classification. The strong atomic rules with the highest and near-highest confidences can realize partial classification accurately. Multiple passes of partial clandflcations generate the concise and accurate classifier. The experiments BIP performed on 26 standard datasets The new approach is compared with d-on tree induction and the existing associative classitifation. The results show that the propnsed algorithm not only achieves the highest elassitifation accuracy but also generates the smallest classleation rule set; furthermore, it runs far faster than the existing associalive cldleation algorithm.

,Keywords: claesification 1. Introduction Data mining; association rules; atomic rules; Classification is one of the most important tasks in data mining. It aims at extracting a model which can reveal the relationship between the attribute set and the class label.

So far, there have been many classification approaches such as decision tree induction, neural network, genetic algorithm and so on.

In recent years, a promising classification technique, namely classification using association rules has been presented ["-L41. Liu and et al proposed the first associative classification algorithm CBA (i.e., Classification Based on Association rule)"]. CBA uses the Apriori algorithm to generate the class association rules with a support threshold 1% and a confidence threshold 50%. The classifier is constructed by pruning the redundant rules in the extracted class association rules. To avoid running out of memory, CBA sets a hard limit of 8oooO on the total number of mined rules. Although CBA achieves higher classification 0-7803-8403-2/04/520.00 82004 IEEE accuracy than decision tree algorithm C4.5 ['I, it consumes a lot of system resources and generates many classification rules.

To prevent over-fitting of the classification model to the trainin data, Li and et al put forward an algorithm CMAR Id (i.e., Classification based on Multiple Association Rules). This algorithm provides higher accuracy,than CBA. But it generates too many rules and takes up much execution time to build the classification model though it uses the relatively fast frequent-pattem growth algorithm to generate the class association rules.

This paper presents a novel algorithm CAAR, namely Classification based on Atomic Association Rules. CAAR imitates human adopting h p l e  and practical method to distinguish things. Just as human beings classifies things by using the close association between the outstanding feature  (such as color, shape, odor and etc) and the class label, CAAR uses such strong association for partial classification. After multiple passes of partial classifications, the data can be completely classified.

The CAAR algorithm is tested on 26 standard datasets.

The results show that CAAR is very successful for the 20    The results show that CAAR is very successful for the 20 typeP datasets on which classification can be done by using the close association between the instance's outstanding features and its class labels. The comparison experiments are performed on the typeP datasets with three classification algorithms including decision tree induction, CBA and CAAR. The results indicate that the proposed algorithm not only achieves the highest classification accuracy but also generates the smallest classification rule set; furthermore, it runs forty times as fast as CBA. CAAR is comparable with C4.5; both run very fast.

For the other six type-N datasets on which classification can be made only by simultaneously using all attributes, at least multiple attributes, CAAR generates the additional attributes related to the regional and global information of the instances and inserts them into the dataset, which can significantly improve the classification accuracy.

Pmceedmgs of the Tbird IntemationaJ Conference on Machine Leaming and Cybernetics, Shanghai, 26- 29 August 2004 The rest of this paper is organized as follows. Section I datasets requiring the global at least regional information for classification, CAAR doesn't perform so well. To 2 describes research problem and gives the related definitions. Section 3 presents the classification based on atomic association rules. Section 4 provides the experiment results. Finally, section 5 draws the conclusions.

2. Problem definition Let the problem discussed be U=(A, C, DJ where A=(&amp;, AI, ..., A,.I] is a set of nominal attribute variables such that A, n A,= 0 and A,. A,E A, i # j, C is a categorical variable, and D is a dataset consisting of a set of instances.

ID1 stands for the number of valid instances in D. V[X] denotes the finite domain of variable X's values.

A common rule is defined as "antecedent consequent". For a given dataset D and rule r, the support of r is defined as r.sup= p(r.antecedentg A tconsequent) and the confidence of r is defined as r.conf= p(r.antecedent A r.consequent)/p(tant~edent) where p(x) represents the probability that an instance contains the item or itemset x in the dataset D. An item represents a variable taking a value like Az=3. An itemset refers to multiple items occurring simultaneously in an instance, e.g. Az=3 A C=l.

CAAR imitates human adopting a practical method to identify and classify things through the close association between an instance's single feature and its class label. In CAAR algorithm, there are two important strategies. One is only mining the atomic rules to generate the smallest classification rule set. The other is using strong atomic rules with the highest and near highest confidences for accurate prediction. The related definitions are given as follows.

Definition 2.1 A class association rule is the rule that the categorical variable is included in the rule's consequent.

Definition 2.2 An atomic class association rule is the class association rule that both antecedent and consequent have only one item.

For short, the atomic class association rules are called atomic rules. An atomic rule can be represented in the form of A,=a C=c, given as AR(A,,a,c) where A, E A, a? V[.4,], CE V[C!.

Definition 2.3 A stmng atomic rule is the atomic rule satisfying the minimum support (called minsup) and the minimum confidence (called minconf) thresbolds.

The typical algorithms of associative classification like CBA extract strong class association rules at minconf 50% and minsup 1%. But in CAAR, minconf is set to 0 . 9 8 ~  maxconf where maxconf is the maximum confidence of all atomic rules. CAAR aims to extract the strong atomic rules with the highest and near-highest confidences to    rules with the highest and near-highest confidences to constlllct a concise and accurate classification model.

The shortcoming of CAAR is the selectivity. For those evaluate whether a dataset is suitable for CAAR. an effective technique is proposed. The mean confidence and support of top-10 strong atomic rules are used to measure the selectivity of CAAR. When there are less than 10 strong atomic rules, all of them are used for measurement.

Definition 2.4 For the atomic rules rl and 12, the strength order in classifying capacity is determined as follows. The rule rl is stronger than r2 when rl.conf&gt;rz.conf, or rl.conf=r2.conf and rl.supxz.sup.

Definition 2.5 For a given dataset, in the first pass of partial classification, Sort the strong atomic rules according to strength order. If the mean confidence of top-IO strong atomic rules (MC10) is less than 80% or the mean support (MSlO) is less than 35, this dataset belongs to fype-N which means this dataset is not suitable for CAAR; otherwise, it belongs to type-P, namely, the dataset is suitable for CAAR.

The classification performance of CAAR is directly related to MClO and MS10. If MClO is relatively low, the classifier will consist of 'the unconfident rules which result in lower classification accuracy. If MSlO is too low, the generated classifier will contain too many rules, and the classification will consume a lot of system resources.

3. Classification based on atomic association rules CAAR imitates the behavior of human beings using the simple approach to distinguish things. CAAR uses the close association between an instance's individual attribute and its class label for partial classification. The remainder of instances is used for the next partial classification.

Through multiple passes of partial classifications, the dataset can be classified completely. A paradigm of CAAR is used to illustrate the principle.

Table 1. An artificial dataset Table 1 Lists an artificial dataset with ten instances.

This dataset contains three attribute variables and one categorical variable.

29 August 2004 Classifying this artificial dataset requires two passes of partial classifications. The classifying process is given as follows Pass one: COlor="Red"~ Class="C" (No: 1,3,8) Size="Tiny"* C1ass"A (No: 2.6) Shape="Square"* Class="A (No: 4.10) The rest 3 Class="B" (No: 5.7.9) The numbers in the parentheses are the identity numbem of the instances differentiated by the relevant rule.

3.1. Thedatastructure Pass two: The first important step of CAAR a lgo r im is to  generate the strong atomic rules. The data structure used for atomic rule generation is a class Counter which uses a 3-dimension array to store the counts of 2-itemsets relevant to the atomic rules. This class is defined in Java language as follows.

public class Counter { public int [][I[] count = new int [ml[nVl[nC]; public void counting (DataSet d)[ ... } public void counting (Instance inst)[ . . I  ) public int getcount(int X, int Vx, int c)( ... ] public int getCount(int X, int Vx)( ... } ...

...

Where m=IAl, nV=max(lV[AJI). Ais A, nC=IV[C] I and max is the maximum function. The getcount method of Counter can readily get the counts of both an item (X=Vx) and an atomic rule (X=Vx * C=c) where the parameter X is the subscript of an attribute in the set A, Vx is the subscript. of the attribute value, and c is the subscript of class label in V[C]. The counts of atomic rules can he accessed by using three subscripts.

For example, given an atomic rule, say As="blue" C="B", if V[As]=("red", "green", "blue"] and V[C]=["A, "B", "C", " D ) ,  the relevant subscripts of this rule is 5 for Ar. 2 for "blue", and 1 for "B". Calling the function getCount(5.2.1) returns the value of count[5][2][1], the count of 2:itemset for this rule. To obtain the count of the antecedent As="blue", just compute count[51[21[01+ count[51[21[1]+ count[5][2][2]+ count[51[21[3].

3.2. Generation of strong atomic rules Let counter be an object of Counter. If an atomic rule AR(A.,a,c) is represented in the subscript form ARCAX, ia, ic), the count of 2-itemset, support (sup) and confidence ( c 0 4  of this rule are computed as follows.

AR(A.,a,c).count=counter. getcount (iAx, ia, ic) AR(A.,a,c).sup= AR(A.,a,c).count ADol (2) AR(A,,4c).conf=AR(A,,a,c).countlcounter.getcount (iAx, ia) (3) (1) Here, Do is the initial dataset. In CAAR, the number of valid (non-deleted) instances in D is varying. When classification is complete, IDI=O. The steps to generate strong atomic rules are given as follows.

1) Scan the dataset and count the atomic rule-related 2-itemsets occurring in the instances.

2) Extract candidate atomic-rules from the counter.

3) Compute the support and confidence of the candidate atomic-rules and find the maximum confidence maxconf of all  candidate atomic-rules.

4) Extract all strong atomic rules according to definition 2.3. The threshold minsup is set to 1% ana minconf is set to confCoefx maxconf. The default value of confcoef is 0.98 which was determined by experiments.

This measure assures that only the rules with the highest and near-highest confidences can enter the classifier.

33. Building a CAAR classifier Figure 1 shows the main steps to build a CAAR classifier. The counting method of Counter (line 3) scans the dataset D and counts all the atomic rule-related 2-itemsets. NumOfClass function (line 4) gets the number of classes included in the valid instances in the dataset.

AtomicRule-Gen function (line 5) generates the strong 1. RuleSet classifier c 6 2. Counter counter t new Counter () 3. counter.counting (D) 4. WHILE IDIS AND NwnOfclass(D) &gt; I  DO 5.  RuleSet rules c AtomicRule-Gen (counter) 6. counter.clear() 11 for the next counting 7. RuleSet c-rules c PnmeRules(rules,D.counter) 8. classifier t classifier U c rules - 9. ENDDO 10. IF IDIS THEN I/ generate a special rule 11. classifier t classifier U (ea special rule&gt; I 12. END IF 13. retum classifier Figure 1. Algorithm to generate a CAAR classifier atomic rules using the approach mentioned previously. The 26-29 August 2004 PruneRules function prunes the redundant rules and counts the 2-itemset?s occurrence in the valid (non-deleted)    the 2-itemset?s occurrence in the valid (non-deleted) instances for the next partial classifcation.

when the class labels of remainder instances in the dataset are the same, the algorithm generates a special rule which indicates the default class.

Finally, combine all obtained classification rules in sequence to form a classifier.

The PruneRules function (showed in Figure 2) has tbree steps: 1) Sort the strong atomic rules (line 1) according to the strength order of classifying capacity defined in 2.4.

2) Test the strong atomic rules on the dataset (line 2-14). If an instance contains the rule?s antecedent item, the rule?s test-count increases by 1, and this instance is logically deleted from the dataset D otherwise, count the 2-itemsets in this instance for the next partial classification.

3) Prune the redundant rules (line 15-17).

1: rules.sortO 2: 3: 4: delete-flag c false 5: 6: 7: 8: delete-flag c true 9: r.test-count t r.test-count cl I O  goto 13 11: END IF 12: ENDDO 13: IF NOT delete-flag THEN counter.counting (inst) 1 4  ENDDO 15: FOR each Rule r in RuleSet rules DO 1 6 1 7  ENDDO 1 E: retum rules Set test-count to 0 for all strong atomic rules FOR each Instance inst in dataset D DO FOR each Rule r in RuleSet rules DO IF inst contains rmtecedent THEN delete inst from D logically IF r. test-count =O THEN delete r from rules Figure 2. The PruneRules function 3.4. Dyeasing the selectivity of CAAR To reduce the selectivity of CAAR, the type-N dataset requires special preprocessing. Based on the existing  attributes, the vector and weight (called VW) features are generated and inserted into the datasets as additional attributes for classifier construction.

The regional vectors and weights are constructed for every five attributes. A vector string is constructed based on the subscript of corresponding attribute value. The weight is the sum of the subscript values. The global vector and weight are constructed on all attributes.

To save storage space, all vectors can be represented in binary data. For an additional attribute, if it has more than 150 different values, this attribute will be removed.

To illustrate how to construct the VW features, an example is given. Let the sizes of attribute value lists of A,, A2 and AJ be 10, 5 and 12, respectively. Given that an instance is represented in the form of attribute value subscripts, say, (3, 2, 11 J,  the additional attributes of these three attributes are given as follows. The vector string is ?03211?. and the weight is 3+2+11=16.

3.5. The algorithm complexity Let n stand for the total number of instances in the dataset D (that is, n=IDol), N. for the mean size of attribute value lists, and N, for the number of class labels.

In CAAR, let p be the number of passes of partial classifications. In each pass of CAAR, counting the atomic rule related 2-itemsets lequires O(nx m) time where m=IAl, and selecting the classification rules from the strong atomic    and selecting the classification rules from the strong atomic rules requires O(nx IRJ) time where RI is the set of strong atomic rules. The total time complexity is O(px  nX m+ p x n x  IRfI). In our study, p varies f? 3 to 14 and its mean value is 7.1 on 20 typeP datasets. It?s very important that p is determined by the inherent model in the dataset and has no direct relationship with n.

In CBA, let 1 be the maximum number of the rule antecedent items. The cost of CBA is the time for mining the strong class rules plus the time for testing the strong rules to build a classifier. The total time complexity is I I O ( n x ~ l ~ b _ , , + n x  k=l XI Rk IJwhere Rk represents the set of strong class rules containing k items in the antecedent. RO is the candidate atomic-rule set whose size is mx  N.x N,. The IR,I of CBA is far greater than that of CAAR because the CAAR?s minconf (0.98 x maxconf) is much higher than the CBA?s minconf (50%). In addition, in CBA, the total number of mined rules increases exponentially as k becomes large.

In C4.5, the cost is the time for building a tree plus that for pruning with the suhtree raising. The total cost is O(n x mx log(n) + n x (rog(n))?).

It can be proven that the time complexity of CBA is much greater than those of CAAR and C4.5. CAAR deals with the large datasets better than decision tree induction because the time complexity of CAAR is linear with n while the latter is not; furthermore, CAAR can also execute on the disk-resident datasets while the traditional decision-tree algorithms can only execute on the memory-resident datasets.

26-29 August 2004 4. Experimental results The experiments were performed on a 1.5 GHz Pentium-4 PC with 256MB main memory. CAAR was implemented in Java. CAAR was compared with decision tree algorithm 54.8 (i.e., the Java implementation of C4.5 release 8) and CBA implemented in C++ by its authors.

The same 26 UCI datasets were used for experiments.

Before experiments, each dataset was shuffled and all continuous attributes were discretized by using the same method as mentioned in [l]. The missing values in an instance were regarded as not satisfying the rule?s antecedent. The 10-fold cross validation was used for every dataset. The minconf of CBA was set to 50% as proposed in [I]. Other parameters took the default values for both J4.8 and CBA. For the selectivity of CAAR, the 26 datasets were tested and divided into two groups according to their MClOs and MSlOs described in definition 2.5. The results indicate that 20 datasets belong to type-P and the other six belong to type-N.

4.1. Classification performance on typeP datasets Table 2 shows the classification results on the 20 type-P datasets. It?s clear that among these three classification algorithms, the proposed CAAR algorithm generates the most concise and accurate classifier.

CAAR achieves the highest mean accuracy of 87.13%.

Moreover, on the 20 typeP datasets, CAAR obtains 10 highest accuracies wbicb are marked with an asterisk.

Classifier generated by CAAR has the smallest rule set. For CAAR, J4.8 and CBA, the mean classifier sizes on 20 datasets are 13.4, 20.7 and 64.1, respectively. Furthermore, CAAR runs forty times faster than CBA. Though the mean execution time of CAAR is almost the same as that of J4.8, CAAR can execute on the disk-resident datasets and it?s more suitable for classifying the large dataset than 54.8 because the time complexity of CAAR is linear with IDOL The column #Pass in table 2,represents the number of passes of partial classifications on each dataset. It varies from 3 to 14 and the mean value is 7.1.

4.2. Experimental results on 6 type-lri datasets . For the type-N datasets (listed in table 3). the relative support of an atomic rule is proposed and computed as formula (4).

AR(A,,a,c).sup= AR(A.,a,c).count /ID1 (4) Starting from the second partial-classification, the 26-29 August 2004 relative support is greater than the absolute support defined in formula (2). So in the late passes of partial classifications, as ID1 becomes small, the relative support of the rules becomes large. The fine classification can be made by using the rules with high contidence and relatively low (absolute) support, which improves the accuracy as mentioned in [4].

85.51 Table 3 shows the classification results on the type-N datasets. For the limitation of space, only accuracies are given. The accuracies of CAAR with VW (vector and weight) are obviously higher than those without VW (called WO). For example, for the dataset Led7 (No. 23).

the accuracy is increased from 49.3% to 72.3% by adding VW features. The mean accuracy on the type-N datasets rises from 67.47% to 80.13% by using the regional and global features as additional attributes. Obviously, this method of reducing the selectivity of CAAR is very effective.

1M) @sa ss0 4 so  WO V W vw Four cases Both vector and weight are important to VW-CAAR.

Figure 3 shows the results in four cases including WO, V, W and VW where WO, V, W and VW represent CAARs using no additional attributes, only vector, only weight, and both vector and weight, kspectively. For the dataset 23, the vector feature plays a more important role than the weight.

On the contrary, for the dataset 24, the classification using the weight achieves higher accuracy than using the vector.

Compared with J4.8 and CBA, on the six type-N datasets, CAAR with VW features also performs best. On all 26 datasets, CAAR reaches the highest mean Figure 3. CAAR test in four cases classification accuracy of 85.51%.

5. Conclusions This paper presents a novel classification algorithm CAAR, namely classification based on atomic association rules. CAAR uses the strong atomic rules with the highest and near highest confidences to construct the concise and accurate classifier. CAAR is compared with decision free algorithm J4.8 and CBA on 20 type-P datasets. The results show that CAAR achieves the highest average accuracy and generates the smallest classification rule set; furthermore, it is far faster than CBA. While the type-N datasets require global at least regional information for classification, CAAR successfully reduce the selectivity by constructing additional attributes.

Acknowledgements The paper was supported by the Natural Science Foundation of Guangdong Province of China (grant No.

31340). J4.8 was provided by University of Waikato in New Zealand Olttp://www.cs.w~to.ac.n~mUweka/). The 26 standard datasets were selected from the UCI Machine Learning repository at the University of California, h i n e (http://www.ics.uci.edu/-dean@.

(http://www.ics.uci.edu/-dean@.

Referencea [I] B. Liu, W. Hsu, and Y. Ma, ?Integrating classification and association rule mining?, Proceedings of 4th ACM Int. Conf. on KDD, New York, pp. 80-86, August, 1998.

[2] Wenmin Li, Jiawei Han, and Jim Pei, ?%MAR: Accurate and efficient classification based on multiple class-association rules?, Proceedings of the IEEE Int.

Conf. on Data Mining, California, pp. 369-376, November, 2001.

Maria-Luiza Antonie and etc. ?Associative Classifiers for Medical Images?, Lecture Notes in Artificial Intelligence Vo1.2797, Mining Multimedia and Complex Data, pp 68-83, Springer-Verlag, 2003.

[4] Elena Baralis, Silvia Chiusano and Paolo Garza, ?On support thresholds in associative cl&amp;sification?, Proceedings of the 19th ACM symposium on Applied computing, Nicosia, pp. 553-558.2004.

[5] R. Agrawal, and R. Srikant, ?Fast algorithms for mining association rules?, Proceedings of the 20th Int.

Conf. on VLDB, Santiago, pp. 487-499. September, 1994.

[6] J. R. Quinlan, C4.5: Programs for machine leaming.

Morgan Kaufmann, San Francisco, 1993.

