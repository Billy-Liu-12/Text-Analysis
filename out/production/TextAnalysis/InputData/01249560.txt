IEEE lntemational workshop on ImlIigent O m  Acquisition and Advanced Computing System: Te~hn010g~ and Apptications  8-10 Septembe 2W3, Lviv, Ukraine

Abstract: Partition method of interval is adopted in current classification based on associations (CBA), but this method cannot reflect the actual distribution of data and exists the problem of sharp boundary. Quantitative attributes are partitioned into several fuzzy sets by fuzzy c-means algorithm, and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules, which are used to build classifcation system. Becausefizzy c-means algorithm can embody the actual distribution of the data and fuzzy sets can soften partition boundary, the classifcation system of the fuzzy association rules can obtain better classifcatiori accuracy than two popular classification methods: C4.5 and CBA.

Keywords: - classification; quantitative attributes; fuzzy c- means algorithm, fuzzy association rules  1. INTRODUCTION Classification is an important topic in data mining  technology. The problem of classification is concemed with the mining of a set of production rules that can allow the values of an attribure in a database to be accurately predicted based on those of other attributes"~']. The decision tree are the most popular approaches to solve the classification Association rules can also be used for classification (CBA)"].

The categorical and quantitative attrihutes are mainly dealt with in [1-3]. In particular, when dealing with quantitative attributes, their domains are usually divided into equal-width or equal-frequency intervals'". But it introduces some problems. The first problem is that equi- width partitioning cannot embody the actual distribution of the data. The second problem is caused by the sharp boundary. In most cases, the resulting intervals are not too meaningful and are hard to understand.

Ref. 181 uses fuzzy set to soften partition boundary of the domains, and presents the concept of fuzzy association rules, but it does not present partition algorithm that can embody the actual distribution of the data and does not present the mining algorithm for fuzzy association rules which fits for large databases. Ref. [9] uses the relational fuzzy c-means algorithm to partition the quantitative  attributes into several linguistic terms, then the problem of mining association rules with linguistic terms is introduced by combining linguistic terms. The relational fuzzy c-means algorithm can emhody the actual distribution of the data. Furthermore, linguistic terms can soften partition boundary. But combining linguistic terms can obtain excessive association rules with linguistic terms, so the mining algorithm cannot fit for large database. In this paper, quantitative attributes are first partitioned into several fuzzy sets by fuzzy c-means algorithm, and search technology of Apriori algorithm is improved to discover interesting fuzzy association rules, which are used to build classification system.

The rest of this paper is organized as follows. In section 2, Apriori algorithm is improved for mining fuzzy association rules. In section 3, the classification system of fuzzy association rules is built. In section 4, the classification system of fuzzy association rules is simplified. The conclusions are briefed in section 5 .

2. AN ALGORITHM FOR MINING FUZZY ASSOCIATION RULES  Let T ={r,,r,,..-,t~) be a relational database, I , represents the j-th record in T, let I ={ i , , i2;: . , i , , , ]  be the attribute set where ij denotes a Boolean, categorical or quantitative attribute, tj[i,] represents value of the j-th record in attribute ' i t .  Values of the record in attribute need to be partitioned into several fuzzy sets for mining fuzzy association rules.

Let A, and A, he two values of the record in Boolean attribute, then two values can be partitioned into two fuzzy sets A, and A, :  Categorical attribute with fewer values can be partitioned with the same method. Each. quantitative atmbute is partitioned into several fuzzy sets by FCM . . .  . ~ . . .

This work was supported in part by the National Natural Science Foundation of China (NSFC) (60073012), National Grand Fundamental Research 973 Program of China (2M)2CB312000), National Research Foundation for the Doctoral Program of Higher Education of China, Natural Science Foundation of Jiangsu Province, China (BK2001 W), 0pening.Foundation of State Key Laboratoly of Soitware Engineering in Wuhan University, and Opening Foundation of Jiangsu Key Laboratory of Computer Information Processing Technology in Soochow University.

c Coirupondence to: Baowen Xu, Department of Computer Science and Enginening, Southeast University, Nanjing 210096, China  Email: bwxu@seu.edn.cn  0-7803-8138-6/03/$17.00 82003 IEEE 248  mailto:bwxu@seu.edn.cn   'algorithm"01.

After clustering with the FCM algorithm, partition  matrix U and c centers v, are obtained. Fuzzy sets are usually represented with triangular fuzzy numbers for classification. The method is as follows.

Let pi(x.) be the grade of membership of xj  in the fuzzy set with center v k  , let  x ' = { x ,  : pt (I,) 2 p, (xj) ,Vj  E {1,2; ... c ) We first find the samples with the minimum grade of  membership at both sides of the center v k  i n  X k  u [ v , ] , let the left sample with the minimum grade of membership he ,xi , its grade of membership be & ( x ' )  , and let the right sample with the minimum grade of membership be xi , its grade of membership be pt (x ' )  , then the expression of triangular fuzzy numbers f'(.r) or (a ,v , ,b) with center v1 is  In order to mine fuzzy association rules, we first build a new database through original database T. In this new database, attributes are fuzzy sets, so attributes are called fuzzy attributes. Values of the record in fuzzy attributes are obtained as follows: Let i: he a fuzzy set of attribute  i, , then ii is a fuzzy attribute. Value of the j-th record in fuzzy attribute it is i i ( f j [ i k J ) ,  iL( f , [ i ,J )  is the grade of membership of t j [ i k ]  with respecl to fuzzy set i i ,  In this new database, the set of all fuzzy attributes are still denoted I ,  the value of the j-th record in fuzzy attribute y ,  is still denoted l j ( y k )  . It is obvious that t j ( y k )  falls in [O,I]. Let  An association rule is an implication of the form X + Y .

Because attributes in X and. Y are fuzzy attributes, X * Y is called fuzzy association rule. The support and confidence of fuzzy association rule are defined as follows.

Definition 1. The support of X is defined as follows.

~   Fuzzy attribute sets with at least a minimum support are called frequent fuzzy attribute sets.

Definition 2. The support of X follows.

Y is defined as  Definition 3. The confidence of X * Y is defined as follows:  Because f j ( y r )  falls in [O,11, we can know that all subsets of a kequent fuzzy attribute set must also be frequent according to definition 1. With the above finding, we can directly modify Apriori algorithm'"] to mine fuzzy association rules.

3. CLASSIFICATION SYSTEM OF FUZZY ASSOCIATION RULES  Let 1 = [ i , , i 2 , , . . , i m , i }  he the attribute set of classification databases. Attribute i is a categorical attribute with values C,,C,;..,C, , which are all class labels. Let y = ( y , . y 2 , . . . , y m )  be a sample, where y,,y2,'.',ym are the values taken by attributes i l , i 2 ; . . , i m .

In this section, we will discuss how to use fuzzy association rules to classify the sample y . W e  use interesting fuzzy association rules to build classification system. Interesting fuzzy association rules are the rules with at least a minimum support and a minimum confidence respectively. Suppose we use the algorithm in section 2 to discover M interesting fuzzy association rules as following. For !GI, 2, . . ., M  R , :  If X ( l , k )  is B(l,k) and ... and XUk&) isB(l,,k) then i is C,  where X (1, k ) ,  X ( 2 , k ) , .  -, X ( 1 ,  , k )  E [il , i ,  ,.-,i, I ,  B(I, , k )  are fuzzy sets of attribute X ( l , , k ) ,  C, E {C,,C,,-.,C,l .

We use these association rules to build the rule base of classification system. When a sample y is to he classified, compute the discriminant function values of each class gh(y) .h=1,2, . . . ,q ,  according to the following formula  where X ( j , k ) ( y )  is the value taken by attribute X ( j , k )  in the sample Y , B ( j , k ) t X ( j , k ) ( y ) l  is the grade of membership of X ( j , k ) ( y )  .,in fuzzy set B ( j , k )  .

n ; = , B ( j , k ) [ X ( j , k ) ( y ) ]  is the activated degree of sample    y to the fuzzy association rule R, .. We compare these discriminant function values, and .take the class label corresponding to the maximum value as the classification result of the sample y . This inference method considers the information provided by each rule for sample classification. At the same time, because fuzzy association rules are easy to be understood, the classification system built has a better interpretability.

In order to check the accuracy of our classification system, this paper selects Wine dataset from UCI Machine Learning Repository, which has 13 quantitative attributes, 1 categorical attribute and 178 records. Each quantitative attribute is partitioned into three fuzzy sets represented with triangular fuzzy numbers by FCM algorithm.

In the experiment, ten-fold cross-validation method is applied to estimate the classification accuracy. The dataset is randomly divided into ten disjoint subsets, with each containing approximately the same number of records.

Sampling is stratified by the class labels to ensure that the subset class proportions are roughly.the same as those in the whole dataset. For each subset, a classifier is built using the records not in it. The classifier is then tested on the withheld subset to obtain a cross-validation estimate of its accuracy. Then ten cross-validation estimates are averaged to provide an estimate for the classifier built from all the data. The cross-validation estimate in each subset is obtained as following.

?A perfect classification system has a few rules and a good accuracy. In order to control the complexity of the classification system, we first mine 1000 interesting fuzzy association rules on the withheld subset and rank these rules by the their support. Some rules that have high support are selected to evaluate the accuracy. In order to save the computing time, we select the number of  rules at a multiple of 50, such as 50, 100,150 et al. We select 20 times and select the number of rules with the best accuracy. Table 1 shows the experimental results with the classification system of fuzzy association rules (CFAR).

Table 1. Experimental results  We compare CFAR with classification methods: C4Sr6? and CBA. The algorithm of C4.5 is downloaded from http://www.cse.unsw.edu.au/-quanlian, CBA is downloaded from http://www.comp.nus.edu.sg/-dm2.

Where the minimum support is set to I%, the minimum  ~   confidence is set to 50%, and other parameters aTe unchanged. The accuracy of C4.5 is 92.7%, CBA is 91.6%, CFAR is 97.15686%. It is obvious that the accuracy of CFAR is better than CBA and C4.5 in the Wine dataset.

4. SIMPLIFYING CLASSIFICATION SYSTEM  There are two issues that must be addressed in the classification system of @zzy association d e s .  The first is that a huge number of rules could contain noisy information. The second is that a huge set of rules would extend the classification time. This could be an important problem in applications where fast responses are required.

So fuzzy association rules should be pruned. The pruning techniques that we employ are the following.

Definition 4. Let r :  X + C he a fuzzy association rule, the lift of a rule r is defined as follows  conf(C) is the expectation confidence of consequent C without arbitrary condition.

If I W )  is greater than 1, then rule r is positively correlated, meaning X encourages C . If /ifr(r) is  less than 1,  then rule r is negatively correlated, meaning X discourages C . If lift@) is equal to 1, then X and C are independent. Fuzzy association rules that their lifts are less than 1 or equal to 1 should be pruned.

It is insufficient to prune the fuzzy association rules with their lifts. The difference of minimum confidence (minconf-dif) is introduced next to prune the fuzzy association rules farther.

Definition 5. Given two fuzzy association rules r, : X a C and r2 : X ? x  C . We say that the rule r2 is sub-rule of the rule r1 if X?c X .

The improvement of a fuzzy association rule can be defined as the minimum difference between its confidence and the confidence of any proper suh-mle with the same consequent.

Definition 6. Given a fuzzy association rule X * C , the improvement of X * C (imp(X C ) )  is defined as follows.

, .

Given a minconf-dif, if im&X * C )  is greater than  minconf-dif, then the rule X * C contains new information. Otherwise, we consider that the sub-rule of fuzzy association rule X S C  have contained the information provided by X * C . So fuzzy association rule X * C should he pruned.

For example, given two fuzzy association rules: Rule I :  if blood pressure is high and dextrose is normal, then  he has an illness (sup=lO%, conJc41%), Rule 2: if hlood pressure is high, then he has an illness  (sup=12%, conf=40%).

Suppose minconf-dij is set to 5%. It is obvious that  Rule 2 is a sub-rule of Rule 1. The difference between the confidence of Rule I and the confidence of Rule 2 is I%,  http://www.cse.unsw.edu.au/-quanlian http://www.comp.nus.edu.sg/-dm2   which is less than 5%. So rule 2 cannot provide new information and will be pruned.

.To Wine dataset, figure 1 shows the average number o f .  the fuzzy association rule pruned with different minconf-dif in the ten-fold cross-validation method. In addition, we can notice from figure 1 that fuzzy association rules can be pruned effectively with the minconf-dif increasing. Figure 2 shows the average test accuracy with different minconf-dij In addition, we can notice from figure 2 that some useful fuzzy association rules may be,pnhed when the minconf-dif increases, this will make the test accuracy descend. A perfect classification system has a few rules and a good accuracy.

Let minconf-difbe, 0.10, the average number of the fuzzy association rule is 52, the average test accuracy is 96.60%.

Comparing with the classification system in section 3, the average number of the fuzzy association rule descends 68.

The average test accuracy only descends 0.55686%. So we can claim that the simplified classification system is a perfect classification system.

0 '  I 0 005 0 1  015 0 2  025  mmconf-dd  Fig. 1 -Average rule numben.

'9 0.7 $ 0.6 }  0.5 ' I 0 0.05 0.1 0.15 0.2 0.25  minconf-dif  Fig. 2 - Average accuracy.

5. CONCLUSIONS A classification system of the fuzzy association rules is  presented. In this classification system, quantitative atrributes are partitioned into several fuzzy sets by fuzzy c-means algorithm, and search technology of Apriori  algorithm is improved to discover interesting fuzzy association rules, which are used to build classification system. Because fuzzy c-means,algonthm can embody the actual distribution of the data 'and fuzzy sets can soften partition boundary, the classification system of the fuzzy association rules can obtain better classification accuracy than two popular classification methods: C4.5 and CBA.

6. REFERENCES [I]  B. Liu, W. Hsu, Y. Ma. Integrating classification and  association d e  mining. Proceedings of the Intemarional Conference on Knowledge Discovery and Data Mining, USA, New York 1998, pp. 80-86.

[2] M. Mehta, J. Rissanen, R. Agrawal. SLIQ: A fast scalable classifier for data mining. Proc. of the 9' b t ' l  Con$ on Extending Database Technology, India, Mumbai 1996, pp. 544555.

[3] P. Smyth, R.M. Goodman. An information theoretic approach to rule induction from databases, IEEE Transactions on Knowledge and Data Engineering 4 (4) (1992). pp. 301-316.

[4] M.S. Chen, I. Han, P.S. Yu. Data mining: An overview Knowledge and Data Engineering 8 (6) (1996). pp.

866-883.

[5] U.M. Fayyad. Mining databases: Towards algorithms for knowledge discovery, Bulletin of the Technical Commitree on Data Mining 21 ( I )  (1998). pp. 335- 341.

[6] J.R. Quinlan. C4.5: Program for Machine earning.

Morgan Kaufmann. San Mateo, CA, 1993. pp. 20-30.

[7] R. Srikant, R. Agrawal. Mining quantitative association rules in large relational tables. Proceedings of the ACM -SICMOD Conference on Management of Data, Montreal, Canada 1996, pp. 1-12.

[8] M.K. Chan, F. Ada, H.W. Man. Mining fuzzy association rules in database. Proceedings of rhe ACM Sixth Intemational Conference on Information and Knowledge Management, Las Vegas, Neveda 1997, pp. 10-14.

[9] J. J. Lu, Z. L. Song, 2. P. Qian. Mining linguistic valued association rules, Journal of Sofhvare 12 (4) (2001). pp. 607-611. (in Chinese).

[10]R.J. Hathaway, J.W. Davenport, J.C. Bezdek.

Relational dual of the c-means algorithms, Parrem Recognition 22 (2) (1989). pp. 205-212.

[I I ]  R. Agrawal, R. Srikant. Fast' algorithms for mining association rules. Proceedings of the 1994 Santiago, Chile 1994, pp. 487-499.

