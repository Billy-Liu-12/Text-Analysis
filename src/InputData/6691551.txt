P-DOT: A Model of Computation for Big Data

Abstract?In response to the high demand of big data analytics, several programming models on large and distributed cluster systems have been proposed and implemented, such as MapRe- duce, Dryad and Pregel. However, compared with high perfor- mance computing areas, the basis and principles of computation and communication behavior of big data analytics is not well studied. In this paper, we review the current big data computa- tional model DOT and DOTA, and propose a more general and practical model p-DOT (p-phases DOT). p-DOT is not a simple extension, but with profound significance: for general aspects, any big data analytics job execution expressed in DOT model or BSP model can be represented by it; for practical aspects, it con- siders I/O behavior to evaluate performance overhead. Moreover, we provide a cost function implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, and demonstrate the effectiveness of the function through several experiments.

Keywords?big data; computational model; distributed system

I.  INTRODUCTION In many areas such as science, internet and e-commerce etc,  the volume of data to be analyzed grows rapidly [1-4]. Tera- bytes and petabytes datasets are increasingly the norm today, so do requirements for efficiently extracting the value. Given the tendency for big data analytics, performance and scalability problems become increasingly severe.

In response to the high demand of big data analytics, sever- al programming models on large and distributed cluster sys- tems have been proposed and implemented. MapReduce [5, 6] exploits a map function and reduce function to parallelize the user program automatically and to provide transparent fault- tolerance. Dryad [7] specifies an arbitrary DAG to form a da- taflow graph for coarse-grain data-parallel applications. Pregel [8], inspired by Bulk Synchronous Parallel (BSP) model [9], consists a sequence of iterations to process large-scale sparse graph computing. Although these models simplify the compli- cated work of algorithm designers and program developers while mapping their work onto real cluster systems, the basis and principles of computation and communication behavior of big data analytics is not well studied.

To address above issue, DOT model [10] abstracts basic operations and communication patterns with three entities: dis- tributed data sets (D), concurrent data processing operations (O), and data transformations (T), respectively. It also provides principles for designing scalable and fault-tolerant software  frameworks for big data analytics. DOTA model (DOT Ad- vanced) [11], inspired by BSP model, redefines and extends the DOT. It divides data processing into three steps: computation, communication, and aggregation. And essentially, it is a bipar- tite communication model.

However, the aggregation step in DOTA model seems so special for MapReduce that other programming models like Dryad or Pregel do not actually need it. Besides, both DOT and DOTA model assume that the memory capacity of a single machine is sub-linear to the size of data. They ignore the non- linearity of disk I/O performance under contention, which is a critical aspect of estimating the performance overhead of big data analytics applications [12, 13].

In this paper, we propose a candidate for the general and practical model, called p-DOT. It limits the memory capacity per machine which is different from above models, and only allows each machine to perform sequential computations in polynomial time without too high a degree to the size of data.

For general aspects, it is more compatible with the BSP model of computation and communication in high performance com- puting (HPC) areas; and for practical aspects, understanding the effects of I/O behavior can make a better decision about how many machines to schedule on a cluster, which leads to better resource utilization.

We highlight our contributions as follows.

? We develop a computational model p-DOT for big data analytics. Any big data analytics job execution represented by DOT model or BSP model can be rede- fined by p-DOT. We also certify that the processing pa- radigm of p-DOT model is scalable and fault-tolerant.

? Under the p-DOT model, we provide a cost function where I/O behavior is included to evaluate performance overhead. The function implies that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, which can then improve the execution time and perform a qualitative prediction on the number of resources necessary for a particular cluster.

The rest of this paper is organized as follows. Section II discusses the related computational models. Section III de- scribes our p-DOT model and its properties. Section IV gives the cost function of p-DOT model by rigorous analysis. Section V demonstrates the effectiveness of the p-DOT model by sev- eral experiments. And section VI concludes the paper.



II. RELATED WORK BSP: BSP [9] is famous for its explicit requirement on syn-  chronization in each superstep. The program executed on it consists of a sequence of supersteps with periodicity L. In each superstep, each processor/memory component can perform the combination of local computation on local available data and message passing. After each period of L unit times, a global check is performed to ensure all components have finished one superstep. Thus, a superstep includes three phases: computa- tion, communication, and synchronization.

DOT: DOT [10] is a matrix model which describes the da- taflow of a big data analytics job with elementary/composite DOT blocks. An elementary DOT block can be represented in three layers:  ? D-layer (data layer): a big data set is divided into n parts (from D1 to Dn) in a distributed system, where each part is a sub-dataset.

? O-layer (operation layer): n workers (from o1 to on) per- form concurrent operations, while each worker oi only processes Di and stores intermediate results.

? T-layer (transformation layer): a single worker collects all intermediate results, performs the transformation, and finally outputs the ending result.

A composite DOT block is organized by a group of m inde- pendent elementary DOT blocks, which have the identical worker set of the O-layer and share the same big data set as input divided in an identical way. Their formulas are as follows:  [ ] [ ] ( ) [ ] ( )[ ])(,),()( 11   1 nn  n  i ii  n  n DoDottDot o  o DDOTD ===  =  (1)  [ ]  ( ) ( )  ( ) ( )[ ])(,),()(,),(   )()(    ,1,11,11,11    ,  1,    ,2,1,  ,22,21,2  ,12,11,1   nmnmmnn  m  n  i imi  n  i ii  mmnnn  m  m  ncompositecomposite  DoDotDoDot  t  t t  DoDo  t  t t  ooo  ooo ooo  DDTOD  =  =  =  ==  (2)  DOTA: Inspired by BSP model, DOTA (DOT Advance) [11] is a matrix model which explicitly divides data processing into computation and communication. It redefines and extends the DOT model by four layers:  ? D-layer (data layer): a big data set is divided into n parts (from D1 to Dn) in a distributed system, where each part is a sub-dataset.

? O-layer (operation layer): n workers (from o1 to on) per- form concurrent operations, while each worker oi only processes Di and stores intermediate results.

? T-layer (transfer layer): each operator ti,j do transfer on intermediate results from worker oi in O-layer to worker aj in A-layer.

? A-layer (aggregation layer): m workers (from a1 to am) perform concurrent aggregations, while each worker ai only aggregates corresponding intermediate results and outputs the ending result.

The formula is as follows:  [ ]  ( )  ( ) ( )  ( ) ( )( ) ( ) ( )( )[ ])(,,)()(,,)(   )()(    )(      ,11,11,111,11    ,  111,    ,2,1,  ,22,21,2  ,12,11,1     ,2,1,  ,22,21,2  ,12,11,1     nnmnmmnnn  m  n  i nnmi  n  i i  mmnnn  m  m  n  i ii  mmnnn  m  m  n  n  DotDotaDotDota  a  a a  DotDot  a  a a  ttt  ttt ttt  Do  a  a a  ttt  ttt ttt  o  o o  DDOTAD  =  =  =  =  ==  =  (3)  From the definition and formula, it is clear that both O- layer and A-layer contain only computation jobs, and their ma- trix are diagonal since independent computation can be ex- ecuted concurrently; while T-layer merely do the communica- tion jobs, and its matrix is ordinary since communication is interactive. Furthermore, it is obvious that computation jobs can be divided into two disjoint sets O and A by matrix T, which implies that DOTA is indeed a bipartite communication model.



III. THE P-DOT MODEL  A. Model Definition Inspired by BSP model, the computation model p-DOT  consists of a sequence of iterations, called phases. During a phase q, a vertex represents the input/output or intermediate data, a user defined computation operation O is invoked in pa- rallel for each vertex first, and a user defined point-to-point communication operation T is performed between pairs of ver- texes next. If the phase is the last stage p, the output will be stored as a final result; otherwise, it will be served as the input for the next phase q+1. The detail is as follows:  ? D-layer (data layer): a big data set is divided into n parts (from D1 to Dn) in a distributed system, where each part is a sub-dataset.

? O-layer (computation layer): during a phase q, nq work- ers (from o1 to onq) perform concurrent computations, while each worker oi only processes a sub (intermediate) dataset and stores intermediate results.

? T-layer (communication layer): during a phase q (q p), each operator ti,j do point-to-point communication on in- termediate results from worker oi (i [1, nq]) of phase q to worker oj (j [1, nq+1]) of phase q+1.

Fig. 1. Dataflow of p-DOT model  Fig. 1 shows the dataflow of p-DOT model, and the formu- la is as follows:  ( ) ( ) ( )  [ ]  [ ] ( ) ( ) ( ) ( ) ( )],2[,       ,  11,  ,  11,1  ,2,1,  ,22,21,2  ,12,11,1    ,2,1,  ,22,21,2  ,12,11,1             pqmnwhere  ototototDD  ttt  ttt ttt  o  o o  ttt  ttt ttt  o  o o  DD  TOTODOTDOTDp  qq  n  i nmi  n  i i  n  i nmi  n  i in  mnnn  m  m  n  mnnn  m  m  n  n  pp p  p  pp  p  pppp  p  p  p  ?=  =  =  ==?  ?  ====  (4)  From the definition and formula (4), p-DOT model has the following advantages:  ? Compatibility: since p-DOT model consists of a se- quence of phases, and each of which is explicitly di- vided into computation and communication, it is com- patible with BSP model in HPC areas.

? Accuracy: p-DOT model accurately represents the basis and principle of computation and communication beha- vior of big data analytics. The O-layer contains only computation jobs, and its matrix is diagonal since inde- pendent computation can be executed concurrently; while T-layer merely does the communication jobs, and its matrix is ordinary since communication is interactive.

? Understandability: it is easy to understand the definition, and following property, cost function and related proof of p-DOT model just with a common matrix theory.

B. Model Properties Based on the definition of p-DOT model, in this section,  there are two useful theorems to be introduced.

Lemma 1.1: Any processing diagram expressed in DOT model can be represented by p-DOT model.

Proof: Since multiplying by an identity matrix I do not change anything for a matrix A, i.e. AAIIA == , we can con- clude that  [ ]  [ ]  ( )( ) ( )( ) ( ) )2(          **  2***  *  *  *    ,2,1,  ,22,21,2  ,12,11,1     ,2,1,  ,22,21,2  ,12,11,1   =?=  ===  =  =  pTODp  TODTOTODTIOID  t  t t  ooo  ooo ooo  DD  t  t t  ooo  ooo ooo  DDTOD  mn  m  mnnn  m  m  n  mmnnn  m  m  noriginaloriginal  (5)  Thus, any processing diagram expressed in DOT model can be represented by p-DOT model. And the real meaning of ma- trix In and Im is that the former only performs the loading oper- ation on data before communication, while the latter directly outputs the intermediate data as the final results after commu- nication.

Lemma 1.2: Any processing diagram expressed in BSP model can be represented by p-DOT model.

Proof: The program executed on BSP model consists of a sequence of supersteps, in each superstep, it can be described by 1-DOT model as follows:  ( ) [ ]==?  mnnn  m  m  n  n  ttt  ttt ttt  o  o o  DDOTDOTD  ,2,1,  ,22,21,2  ,12,11,1       (6)  where matrix O represents the concurrent computation, matrix T represents the message transmissions, and the multiplication between OD and T represents the implicitly message arriving and synchronizing. In more details, the data vector  [ ]nDDD 1= represents the local available data for each proces- sor, worker oi applys the local computation on each (interme- diate) data Di, and operator ti,j do message passing on interme- diate results from worker oi to worker oj.

Thus, each superstep of BSP model can be described by 1- DOT model, and consequently, the dataflow of BSP model can be described by a combination of multiple 1-DOT models, where ith outputs are (i+1)th inputs. Clearly, this combination is p-DOT model, and any processing diagram expressed in BSP model can be represented by p-DOT model.

Theorem 1 (Pervasive Theory): Any processing diagram expressed in DOT model or BSP model can be represented by p-DOT model.

Proof: It is the corollary of Lemma 1.1 and Lemma 1.2.

Theorem 2 (Scalable and Fault-tolerance Theory): The processing paradigm of p-DOT model is scalable and fault- tolerance.

Proof: Since matrix multiplication satisfies the associative law and multiplying by an identity matrix I do not change any- thing for a matrix, we can conclude that  ( ) ( ) ( ) ( )( ) ( )( ) expression*   originaloriginal  ppppp p  TOD  ITOTOTODTOTODOTDOTDp  =  ===? ? (7)  And since Lemma 3.1 and 3.2 of DOT model claims that the processing paradigm of the DOT model is scalable and fault-tolerant, it is obvious that the processing paradigm of p- DOT model is scalable and fault-tolerance combined with Lemma 1.1.



IV. THE P-DOT COST FUNCTION A major purpose of computational model is to give a sim-  ple standard for quantifying its efficiency on which people can agree. This section introduces a cost function of p-DOT model as a candidate for this role. To make the function effective in practice, we add several restrictions first. We present the cost function by rigorous analyses of the issues involved next, and compare it to the existing model finally.

A. Restrictions Memory: In big data analytics, it is reasonable to assume  there is no single machine that can store the entire dataset and the memory per machine is limited. Thus, we require that the input to any machine be substantially sub-linear to the size of data, so do the intermediate input to a single machine in any phase q to the size of output in phase q-1. However, if per (in- termediate) input is still too big to fit into memory on a single machine, we have to use several times to move it from disk to memory. Since a disk cannot transfer data to memory at more than a hundred million bytes per second, no matter how that data is organized [18], this I/O access cost must be considered for a dataset of a hundred gigabytes or a terabyte.

Machines: In big data analytics, it is reasonable to assume the total number of machines is much smaller than the size of data. Thus we limit the total number of machines available to be substantially sub-linear to the size of data.

Time: Similar to paper [14], we do not restrict the compu- tation power of a single machine, except that we require the total running time be in polynomial time without too high a degree to the size of data. Furthermore, it is important to limit the number of phases, because communication is a time con- suming operation in large-scale distributed systems and large number of phases seems unnatural for a general algorithm.

Thus, we only consider programs that require a finite number of phases, i.e. parameter p is constant.

B. Cost Funciton Definition 3.1: For a big data analytics job which can be  represented by p-DOT model, let n be the total number of ma- chines used in whole execution, and nq be the number of work- ers in O-Layer of phase q, i.e. [ ]{ }pqnn q ,1|max ?= .

Definition 3.2: Assume that each machine has the same memory size m in order to simple understand the cost function of a big data analytics job represented by p-DOT model. Let w (i.e. w1) be the initial input to the whole job execution, wq be the intermediate input of phase q (q [2,p]). If wq/nq>m, i.e.

per (intermediate) input is too big to fit into memory on a sin- gle machine, we have to use ( ) [ ]( )pqmnwk qqq ,11 ?+?=  times to move it from disk to memory.

Lemma 3.1: For a fixed big data analytics job represented by p-DOT model, the computation cost of phase q is O(kq).

Proof: Consider the computation of phase q, which is  [ ]  ( ) [ ]( )qTikiiiqqqq nnkkk  n  n  n  nqq  nidddDandTODDwhere  o  o o  ddd  ddd ddd  o  o o  DDOD  q  qqqqq  q  q  q  q  ,1,      ,,2,1111    ,2,1,  ,22,21,2  ,12,11,1     ?==  =  =  ???  (8)  Formula (8) is concluded by formula (4), definition 3.1 and 3.2, where nq is the number of workers in O-layer, and qD  with size wq is not only the input of phase q, but also the output of phase q-1. If wq/nq>m, i.e. per (intermediate) input Di is too big to fit into memory on a single machine, Di will be changed into a kq- dimensional vector, meaning we have to use kq times to load the input Di, and the nq-dimensional vector qD  will be changed into a  kq nq matrix.

Thus, for theoretical aspects, the computation of phase q is multiplication between a kq nq ordinary matrix D and a nq nq diagonal matrix O. Based on matrix theory, it is easy to find that this computation contains kq nq multiplications and kq (nq 1) additions. As both multiplications and additions are executed on nq workers in parallel, the computation cost is  ( ) ( )( )( ) ( )qqqqqq kOnnkOnkO =??+? 1 . For practical aspects, the (intermediate) input is processed by nq workers concurrently.

The time taken for each worker is Tcomp= kq (Tper_load +Tper_comp), where Tcomp is the total execution time,  Tper_load is per loading time, and Tper_comp is per computation time on per loaded data. Since per loaded data has the same amount, and none of them exceeds the memory storage, it seems natural to limit both Tper_load and Tper_comp to a constant bound for a fixed algorithm, i.e. O(1), and Tcomp is ( ) ( )( ) ( )qq kOOOk =+? 11 .

Therefore, for a fixed big data analytics job represented by p-DOT model, the computation cost of phase q is O(kq) clearly, as desired.

Lemma 3.2: For a fixed big data analytics job represented by p-DOT model, the communication cost of phase q (q p) is O(max(nq, nq+1)).

Proof: Consider the communication of phase q (q p), which is     ( ) [ ]  ( ) ( )[ ]=  =  +  +  +  +  +  +        ,2,1,  ,22,21,2  ,12,11,1   ,2,1,  ,22,21,2  ,12,11,1       pppp  p  p  qq  pppp  p  p  q  q  nnnn  n  n  nn  nnnn  n  n  n  nqqq  ttt  ttt ttt  DoDo  ttt  ttt ttt  o  o o  DDTOD    (9)  Formula (9) is concluded by formula (4) and definition 3.1, where  qqOD is intermediate computation results, and operator ti,j do message passing on result ( )ii Do  from worker oi to worker oj.

Thus, for theoretical aspects, the communication of phase q is multiplication between a nq-dimensional vector DO and a nq  nq+1 ordinary matrix T. Based on matrix theory, it is easy to find that the communication contains nq nq+1 multiplications and nq+1 (nq 1) additions. As multiplications and additions are respectively executed on nq and nq+1 workers in parallel almost simultaneously, the communication cost is  ( ) ( )( ) ( ) ( ) ( )( )11111 ,max1 +++++ =+=??+? qqqqqqqqqq nnOnOnOnnnOnnnO .

For practical aspects, since nq workers send message in parallel, nq+1 workers receive message in parallel, and each worker can both send and receive message at the same time, the time taken for communication is Tcomm =O(max(nq, nq+1)).

Therefore, for a fixed big data analytics job represented by p-DOT model, the communication cost of phase q (q p) is O(max(nq, nq+1)) clearly, as desired.

Theorem 3 (Performance Evaluation Theory): For a fixed big data analytics job represented by p-DOT model on a fixed workload, the cost function is T=O(w/n + n) p, where w is the size of input, n is the number of machines, and p is the phases.

Proof: Based on Lemma 3.1 and 3.2, the cost function of all phases q (q [1,p]) can be expressed as follows:  ( ) ( ) ( )( )( ) ( )  ( ) ( ) ( ) ( )( ) ( )  ( ) =  +  = +  ==  +=?  ==?  =  +?=  +=+==  p  q qqt  qqqqq  q  qqq  p  q qqq  p  q commcomp  p  q tpert  nnwOT  nOnnOandnwOkO  definitiontoduennand  workloadfixedainismmnwk  nnOkOTTTT  cos    cos_cos  ,max  1.4max ,constant,1  ,max  (10)  And since per input to any machine be substantially sub- linear to the size of input in any phase q, we can conclude that  ( ) ( )11],,2[ ??=?? qqqq nwOnwOqq . Otherwise, if not desired, we can adjust parameter nq (i.e. increase or decrease the number of workers in phase q) to make the equation true. Besides, it is reasonable that O(n1)= O(n), and w1= w due to definition 3.2, thus,  ( ) ( ) ( ) pnnwOpnnwOnnwOT p  q qqt ?+=?+=+=  =  cos  (11)  Therefore, for a fixed big data analytics job represented by p-DOT model on a fixed workload, the cost function is T=O(w/n + n) p, as desired.

Corollary 3.1: For a fixed big data analytics job represented by p-DOT model on a fixed workload, the optimal number of machines is near-linear to the square root of input size.

Proof: Consider the cost function certified in Theorem 3, which is T=O(w/n + n) p. For a fixed input size, function T will only relate to parameter n since parameter p is constant in a fixed big data analytics job, i.e. T(n)=O(w/n + n). And it is easy to find the minimum, which is  ( ) ( ) ( )  ( ) min)(, min)(,/  2,0,0  isnTwOnwhen  isnTnOnwOwhen baifonlyandifholdsequalityabbaba  =?  =? =?+??? (12)  Therefore, the optimal number of machines is near-linear to the square root of input size, as desired.

Corollary 3.2: For a fixed big data analytics job represented by p-DOT model on a fixed workload, if the input size is enlarged k2 times, the corresponding optimal number of machines should be expanded O(k) times.

Proof: Consider the optimal number certified in Corollary 3.2, which is ( )wOn = . When w is enlarged k2 times, the new optimal number is ( ) ( ) ( ) ( ) ( ) nkOwOkOwkOwkOn ?=?===? 2 .

Therefore, the optimal number of machines would be ex- panded O(k) times when the input size is enlarged k2 times, as desired.

C. Discussion Big data analytics, is aiming for high throughput which  demands that large distributed systems ?scale out? by conti- nuously adding computing and storage resources through net- works. To describe this key feature, the cost function of p-DOT model extracts only two parameters for a fixed algorithm and workload, i.e. w (the size of input) and n (the number of ma- chines). The graph of function is an open-up curve with a ver- tex ( )wOn =  at the lowest point and increases in either direc- tion. Meaning that for a fixed input size (w is constant), the optimal number of machines is near-linear to the square root of input size, if there are not enough resources ( ( )wOn < ), we can add machines to reduce the execution time; but we cannot add too much machines since excessive machines ( ( )wOn >  ) will bring large amount of communication overheads. Moreo- ver, as the optimal number of machines would be expanded O(k) times when the input size is enlarged k2 times, we can perform a qualitative prediction on the number of resources necessary according to the existing results.

0 50 100 150 200 250 300        16 0.25GB dataset R  un tim  e (s  )  Number of workers 0 200 400 600 800 1000 1200 1400          260 25GB dataset  R un  tim e  (s )  Number of workers 1000 2000 3000 4000 5000 6000 7000         3400 2.5TB dataset  R un  tim e  (s )  Number of workers  Fig. 2. The elasped time of program wordcount.

0 50 100 150 200 250 300     0.25GB dataset  R un  tim e  (s )  Number of workers 500 1000 1500 2000 2500        320 25GB dataset  R un  tim e  (s )  Number of workers 3000 4000 5000 6000 7000 8000        4000 2.5TB dataset  R un  tim e  (s )  Number of workers  Fig. 3. The elasped time of program terasor

V. EXPERIMENTAL EVALUATION In this section, we examine the effectiveness of the cost  function of p-DOT model in detail by running two experiments: wordcount and terasort. These two programs are representative of a large subset of the real programs written by users of Ma- pReduce; while the strategies we adopt to implement their da- taflow are using MPI due to following reasons: a) The validity of computational model do not depend on the underlying pro- gramming language, and the right choice relies on the envi- ronment. b) Our environment, Sunway BlueLight MPP [19] in National Super Computing Center in Jinan [20] is a traditional HPC platform which does not support Hadoop.

A. Configuration All of the programs are executed on a queue of Sunway  BlueLight MPP that consists of approximately 4000 machines.

Each machine has four 975MHz SW?1600 processors, 4GB of memory, and an Infiniband QDR link. The operating system is Linux 2.6.15, and the programs were executed on a weekend, when the CPUs, disks, and network were relatively idle.

The input for wordcount and terasort are generated by GridMix [21] and TeraSort [22] benchmark respectively. And five different amounts of dataset are chosen in test, they are 0.25GB, 6.25GB, 25GB, 625GB and 2500GB (nearly 2.5TB).

B. Evaluation We conduct various experiments to find the optimal num-  ber of workers to each input size of two programs. For the da-  taset varying from 0.25GB to 625GB, the programs use only one processor per machine to avoid I/O interference within a node; while for the 2.5TB dataset, the programs have to use two processors per machine since there are not enough physical machines available in our queue. Thus, the number of workers is actually the number of work processes.

Fig. 2 and Fig.3 presents how elapsed time changed with the number of workers on three different input sizes. It is ob- vious that all the curves are open-up with a vertex at the lowest point and increases in either direction. Meaning that there in- deed exist the optimal number of workers for a fixed input size, and either too much or too little workers are not the better re- source utilization.

Table I shows the optimal number of workers to different input size of two programs, and calculates their expansion ratio using value in same row second column as a standard in Table

II. For example, value ??25? in the second row third column presents that 6.25GB dataset is 25 times larger than 0.25GB dataset, and value ??2.5? in the fourth row third column presents that the optimal number of 6.25GB dataset on program wordcount is 2.5 times bigger than that of 0.25GB dataset.

Fig.4 graphs the expansion ratio of optimal number of workers to different input size according to Table 3. It shows that although there are certain deviations, the curves of both programs are with close to linear, which not only quite matches our expectations (Corollary 3.1), but also can further perform a qualitative prediction on the number of workers needed for a larger input size.

TABLE I.  OPTIMAL NUMBER OF WORKERS TO DIFFERENT INPUT SIZE  Input Size  Optimal Number of Workers 0.25GB 6.25GB 25GB 625GB 2.5TB  Worcount 64 160 640 2000 4000  TeraSort 64 320 800 2500 5000  TABLE II.  EXPANSION RATIO OF OPTIMAL NUMBER TO INPUT SIZE  Expansion Ratio of Input Size  Expansion Ratio of the Optimal Number 1 25 100 2500 10000  Worcount 1 2.5 10 31.2 62.5  TeraSort 1 5 12.5 39.1 78.2  -10          80  Wordcount Value Terasort Value  Ex pa  ns io  n R  at io  o f t  he O  pt im  al n  um be  r  Expansion Ratio of Input Size    Fig. 4. Expansion ratio of optimal number to input size  We analyze the deviations at the end, there are two reasons: a) neither computation nor communication cost of program is completely linear in practical; b) there exists inevitable interfe- rence from other jobs on the experiment platform.



VI. CONCLUSIONS We have presented the computational model p-DOT for big  data analytics in large-scale distributed systems. It reveals the basis and principles of computation and communication beha- vior for big data analytics jobs, and can be used as a bridging model between various big data analytics applications and dif- ferent underlying software frameworks. Besides, we have certi- fied that any processing diagram expressed in DOT model or BSP model can be represented by p-DOT model (Theorem 1), and the processing paradigm of p-DOT model is scalable and fault-tolerance (Theorem 2). Moreover, considering the I/O behavior, we have provided the cost function of p-DOT model (Theorem 3) implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload (Corollary 3.1) and would be expanded O(k) times when the input size is enlarged k2 times (Corollary 3.2).

Our future work has two directions: a) we will study the ef- ficiency model for big data analytics by exploiting the benefits from the p-DOT model; b) under the guidance of the p-DOT model, we will design and implement some more efficient al- gorithms in the area of data mining.

