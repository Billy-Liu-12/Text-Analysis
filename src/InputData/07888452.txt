2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?In this paper, we propose a general model to address the overfitting problem in online similarity learning for big data, which is generally generated by two kinds of redundancies: 1) feature redundancy, that is there exists redundant (irrelevant) features in the training data; 2) rank redundancy, that is non-redundant (or relevant) features lie in a low rank space. To overcome these, our model is designed to obtain a simple and robust metric matrix through detecting the redundant rows and columns in the metric matrix and constraining the remaining matrix to a low rank space. To reduce feature redundancy, we employ the group sparsity regularization, i.e., the `2,1 norm, to encourage a sparse feature set. To address rank redundancy, we adopt the low rank regularization, the max norm, instead of calculating the SVD as in traditional models using the nuclear norm. Therefore, our model can not only generate a low rank metric matrix to avoid overfitting, but also achieves feature selection simultaneously. For model optimization, an online algorithm based on the stochastic proximal method is derived to solve this problem efficiently with the complexity of O(d2). To validate the effectiveness and efficiency of our algorithms, we apply our model to online scene categorization and synthesized data and conduct experiments on various benchmark datasets with comparisons to several state-of-the-art methods. Our model is as efficient as the fastest online similarity learning model OASIS, while performing generally as well as the accurate model OMLLR. Moreover, our model can exclude irrelevant / redundant feature dimension simultaneously.

Index Terms?online learning, similarity learning, low rank, sparse representation, feature selection, overfitting, redundancy  F  1 INTRODUCTION  AN appropriate similarity measure [1], [2], [3], [4], [5],[6], [7], [8] [9] [10] is one of the key issues in many computer vision problems. Compared with using traditional fixed metrics, e.g., Euclidean and Mahalanobis distance.

Similarity learning models including both online and offline learning [11], [12], [13], [14], [15] can lead to more mean- ingful distance metrics automatically, and usually learn a function SW (p1, p2) = pT1Wp2 [16], [17] with a bilinear form parameterized by a metric matrix W ? Rd?d to measure the similarity between any two features p1, p2 ? Rd. Intuitively, SW (p1, p2) assigns high scores if p1 and p2 are similar or from the same class, and vice versa. Specifically, similarity learning with the matrix W as a positive semi-definite matrix is also named as metric learning.

Y. Cong is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China and also the Department of Computer Science, University of Rochester, Rochester, NY 14611 USA e-mail: congyang81@gmail.com J. Liu is with the Department of Computer Science, University of Rochester, Rochester, NY 14611 USA e-mail: jliu@cs.rochester.edu B. Fan is with College of Automation, Nanjing University of Posts and Telecommunications, Nanjing, 210042 China e-mail: jobfbj@gmail.com P. Zeng is with Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China e-mail: zp@sia.cn H. Yu is with the State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110016, China e-mail: yhb@sia.cn J. Luo is with the Department of Computer Science, University of Rochester, Rochester, NY 14611 USA e-mail: jiebo.luo@cs.rochester.edu Y. Cong and J. Liu are co-first authors and contribute the same to this paper.

This work is supported by NSFC (61375014, U1613214, 61533015), CAS- Youth Innovation Promotion Association Scholarship (2012163) and also the foundation of Chinese Scholarship Council.

In this paper, we focus on online similarity learning [16], [18], [19], [20], [21], which learns from one or a small number of instances per iteration and provides an efficient way to incorporate new incoming data. Because the testing data always comes sequentially in practice, the performance of offline similarity learning models with batch training may deteriorate over time as the new incoming data may deviate from the initial training data. In contrast, online similarity learning has more advantages. However, the ?Overfitting? phenomenon always exists: the training data that can be fitted by a simple model but instead is fitted by an unnec- essarily complicate model. Although the complicate model can explain the training data as well as the simple model, its performance on the testing data is usually much worse due to poor generalizability. Therefore, the simplest model is always preferred in practice. In this work, we wish to learn a more dense submatrix W ? by removing the redundant features. There are two main types of overfitting issues in online similarity learning:  a Feature redundancy: Because we do not have enough prior knowledge to design the most effective fea- tures, there often exist redundant or even irrelevant features, which not only increases the computation cost, but also degrades the classification performance especially when the noisy level is high.

b Rank redundancy: If the relevant features with di- mension d reside in a low dimensional subspace r (r < d), a metric matrix with the rank lower than r can distinguish any two samples [21, Theorem 1].

Therefore, a simple low rank model can be less sensi- tive to noise data and more robust against overfitting.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   Redundant Features  (a)  (b)  Fig. 1. An illustrative example: (a) The symmetric matrix W with light blue entries corresponding to small but nonzero values, has various redundancy. This would cause overfitting issue because these irrelevant features are not removed. (b) The proposed method allows to remove irrelevant features and compress redundant features into a smaller, simpler matrix W ?, which reduces overfitting and improves computational efficiency.

To address these two issues, we formulate online simi- larity learning as an optimization problem by considering two regularization terms: a) the `2,1 norm to restrict the feature sparsity and address the feature redundancy, b) the regularization ?max norm? [22], rather than the well known nuclear norm, as the low rank penalty to address the rank redundancy (the nuclear norm requires the SVD calculation for optimization and thus is not suitable for large scale data).

In contrast, the max norm behaves the same as the nuclear norm to emphasize the low rank property on matrices, yet has a lower computational complexity (O(d2)vs.O(d3)).

Fig. 1 is the demonstration of our method, where the metric matrix W learned by our model contains zero columns and rows. We can remove the redundant features (i.e., zero columns and rows from W ) and reserve the condense ones. Generally, the main contributions of this paper are as follows:  i With the help of both the `2,1 norm and the max norm, we design a general framework, referred to as Online Similarity Learning with Low Rank and Group Sparsity (OSLLR-GS), to address the overfit- ting problem caused by various redundancies.

ii For model optimization, we derive an online algo- rithm based on a stochastic optimization technique, which not only leads to a closed form solution for each online iteration, but also reduces the compu- tation burden from O(d3) to O(d2) by avoiding the SVD calculation used by the traditional methods.

iii We design extensive simulated and real experiments to validate the effectiveness of our model. Its per- formance is comparable to the accurate OMLLR [21] model and is as fast as the OASIS model [16], [23].

The rest of the paper is organized as follows. In Sec.2, we review the related works. Sec. 3 and Sec. 4 introduce our model and the online model optimization, respectively. In Sec. 5, we compare our model with the state-of-the-arts. We conclude the paper in Sec. 6.

2 RELATED WORKS  Similarity measurement is a fundamental problem in ma- chine learning and computer vision domain [6], [24], [25], [26], [27] [28], [29], [30]. In comparison with traditional fixed metrics, e.g., Euclidean and Mahalanobis distance, metric learning [1], [2], [3], [4] is the task of learning a distance function over objects. The metric or distance function has to obey four axioms: non-negativity, identity of indiscernible, symmetry and triangle inequality constraints. There are also some Matlab toolboxes for distance metric learning 1 [1], [2]. Generally, depending on the learning paradigm, the metric learning models can be classified as fully supervised metric learning, weakly supervised metric learning, semi- supervised metric learning and unsupervised distance metric learning (e.g., Principal Component Analysis (PCA) and Locally Linear Embedding (LLE) [31]); based on the form of metric, the metric learning models have linear, nonlinear and local variations; depending on the optimality of the solution, they have local and global optimal solu- tions; and there are also online and batch metric learning models. In this paper, we mainly focus on supervised online metric learning models. For example, the large margin nearest neighbor method (LMNN) [18] is proposed to learn a Mahalanobis distance such that the k-nearest neighbors of a given sample belong to the same class and different- class samples are separated by a large margin. LEGO [19], online learning of a Mahalanobis distance using a Log- Det regularization per instance loss, is guaranteed to yield a positive semidefinite matrix. In [20], a metric learning algorithm by collapsing classes (MCML) is designed to learn a Mahalanobis distance such that same-class samples are mapped to the same point; however, the MCML model is too time consuming. Chechik et al. [16], [17], [23] design an Online Algorithm for Scalable Image Similarity learning (OASIS) for learning pairwise similarity. OASIS is fast and scales linearly with the number of objects and the number  1. http://www.cs.cmu.edu/?liuy/distlearn.htm    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   of non-zero features, but it may suffer from overfitting. Ying et al. [32] intend to pursue the metric learning model based on sparse representation. Jin et al. [33] present an efficient online learning algorithm for regularized distance metric learning (RDML). Bian et al. [34] propose a constrained empirical risk minimization framework for distance metric learning (CDML). Kunapuli et al. [35] propose an online regularized metric learning algorithm based on composite objective mirror descent (COMID). Huang et al. [36] intend to propose a unified framework for sparse metric learning.

Most of the above metric learning models have not consid- ered the low rank property of the real data.

To avoid overfitting problem, the low rank constraint has been considered into online metric learning [21], [37], [38], [39], [40]. Liu et al. [40] propose an interesting work to han- dle the similarity metric learning with low-rank constraint for high dimensional data, where the number of training samples n is much smaller than the feature dimension d, i.e., n << d. Mei et al. [27] propose a logdet divergence- based metric learning model. In [38], [39], Shalit et al.

design an embedded manifold of low rank matrix for online metric learning model, and in [37], a Riemannian method is proposed to pursue a low rank positive semidefinite matrix.

[5] theoretically analyses the Learning Vector Quantization (LVQ) using a quadratic matrix of adaptive relevance pa- rameters to pursue the low rank property. In our previous work [21], we propose an online similarity learning model via low rank constraint, which adopts the nuclear norm regularization to pursue the low rank property of the model matrix and can partially overcome overfitting caused by rank redundancy. In comparison with [37], [38], [39], we pursue the minimum expectation as the loss function, which is more robust. However, our previous OMLLR needs to cal- culate the time consuming SVD in each iteration. Therefore, our new model avoids the SVD calculation, which is more efficient than OMLLR (i.e., the computational complexity decreasing from O(d3) to O(d2); moreover, ours can pursue both adaptive distance metric learning and feature selection concurrently. The most similar work is [41], which proposes a novel algorithm to pursue the low rank and group sparsity structures by adopting the nuclear norm and `2,1 norm on the metric matrix W . However, [41] requires extensive computation burden (eigenvalue decomposition for PSD constraint and ADMM framework to separate `2,1 norm and nuclear norm) per iteration to solve the formulation; in comparison, we enforce low rank structure using max norm and group sparsity structure using `2,1 norm on X (W = XXT ) instead of W , which leads to a much simpler and more efficient optimization algorithm. Therefore, our proposed algorithm is more suitable for the online and large case learning scenarios.

3 THE PROPOSED ONLINE SIMILARITY LEARNING MODEL This section introduces our similarity learning model to handle overfitting mainly caused by two types of redun- dancy, i.e., feature redundancy and rank redundancy. Our online similarity learning model contains two parts, the loss function and regularization terms. For the loss function, we calculate the expectation of the model for all the data,  which can generate a robust result. For the regularization terms, in comparison with most previous works pursuing the low rank constraint [21], [37], [38], [39], we consider an additional group sparse constraint to achieve feature selection. Therefore, our model can not only calculate the metric matrix in a low rank space, but also detects the re- dundant rows and columns simultaneously, i.e., our model can pursue a simple and robust metric matrix with low rank and feature sparsity.

3.1 Loss Function  We define the similarity metric function as  SW (p1, p2) = p T 1Wp2 (1)  Intuitively, SW (p1, p2) assigns high scores if p1 and p2 are similar or from the same class, and vice versa. For robust- ness, a soft margin is often used:  SW (p1, p2) ? SW (p2, p3) + 1, (2)  where p1 is from the same class as p2 and p3 is from a different one. One can use various loss functions to penalize a violation, such as hinge loss and logistic loss. Throughout this paper, we use the hinge loss function:  l(W ; p1, p2, p3) = max{0, 1? pT1Wp2 + pT2Wp3}. (3)  Now we are ready to introduce the loss function. T is a set of tuples and each tuple contains three indexes {t1, t2, t3} (similar technology was also used in WSABIE [42]) where pt1 and pt2 are from the same class, which is different from the class of pt3 . We define the average loss as the loss function   |T | ?  {t1,t2,t3}?T  l(W ; pt1 , pt2 , pt3), (4)  where |T | denotes the size of T . If the training data contains multiple classes, T would be all possible tuples satisfying the condition above. Note that the size of T is on the order of O(n3) (n is the size of training data). Let t = {t1, t2, t3} and lt(W ) := l(W ; pt1 , pt2 , pt3). Actually, the loss function Eq. (4) can be considered as the expectation of the loss, which is more robust than the loss function in [16], [23]; and we can simply rewrite it as:  E[lt(W )] =  |T | ?  {t1,t2,t3}?T  l(W ; pt1 , pt2 , pt3). (5)  3.2 Low Rank Regularization and Feature Sparsity Reg- ularization  The nuclear norm of matrices has been widely used to enforce the rank sparsity on matrices. Its main disadvantage is that it requires the SVD calculation, which is quite expen- sive in computation and cannot handle large scale matrices.

To avoid computing SVD, we apply the low rank regular- ization, max norm. The max norm has been introduced in    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   machine learning community for matrix reconstruction [43] and clustering [44]. The max norm of matrices is defined as  ?W?max := inf W=LRT  {?L?2,??R?2,?}  = inf W=LRT ,?L?2,?=?R?2,?  {?L?2,??R?2,?}  = inf W=LRT ,?L?2,?=?R?2,?  max{?L?22,?, ?R?22,?}  = inf W=LRT  max{?L?22,?, ?R?22,?}, (6)  where the `2,? norm is defined by ?X?2,? := maxdk=1 ?Xk.? (Xk. is the kth row of X), L ? Rd?m and R ? Rd?m. To show why the max norm enforces the low rank property like the nuclear norm, we have the following equations:  ?W?? = inf ?u?=1,?v?=1  { ? i  |?i| |W = ? i=1  ?iuiv T i }, (7)  ?W?max =K inf ?u??=1,?v??=1  { ? i  |?i| |W = ? i=1  ?iuiv T i },  (8) where ?.?? denotes the nuclear norm. The range of the equivalence factor K is [1.676, 1.738] [22], [45]. The two equivalent forms above imply that the max norm enforces the rank sparsity with factors in the `? space while the nuclear norm does so in the `2 space. The main advantage of max norm over nuclear norm is lower computation cost, which will become clear in Section 4.

In our problem, to enforce the rank sparsity property, it is natural to model our problem as  min X  : E[lt(W )],  s.t. : ?W?max ? ?2, W ? 0.

(9)  Since W is constrained in the positive semi-definite cone, its max norm can also be expressed as ?W?max = {?X?22,? | W = XXT }. ? is the tuning parameter (? ? 0).

Using the decomposition of W , we can reformulate Eq. (9) as  min X?Rd?m  : E[lt(W )],  s.t. : ?X?22,? ? ?2, W = XXT .

(10)  Note that the positive semi-definite constraint is automat- ically satisfied. This is a noncovnex problem in general because of the decomposition constraint W = XXT , which can easily lead to a local optimum. Fortunately, Burer and Monteiro [46] proved that when X contains sufficien- t columns, this decomposition can still lead to a global solution for the symmetric, positive semidefinite variable W = XXT . In our experiment, we always choose a large number of columns for X . Note that choosing a small number of columns for X can enforce a rigid low rank constraint as well, which has been reported extensively in early literature but tends to fall into a local optimum. In comparison, we choose a large value for m, which can lead to a global optimum, and use the max norm to enforce  the low rank property. In other words, we can simplify the problem of Eq. (10) by removing W :  min X  : E[lt(XXT )],  s.t. : ?X?2,? ? ?.

(11)  In addition to the low rank property for the metric matrix, we also like to select relevant features or detect irrelevant features. Each row of X corresponds to a feature.

To pursue sparse features, a natural idea is to use the `2,1 norm on X to enforce group sparsity, that is, we expect many rows of X to be zeros. If a whole row of X is zero, then the corresponding features is detected as irrelevant.

The final objective of our online similarity learning model can be formulated as  min X  : F (X) : = E[lt(XXT )] + ??X?2,1,  s.t. : ?X?2,? ? ?.

(12)  where the `2,1 norm is defined by ?X?2,1 := ?d k=1 ?Xk.?2,  and ? is the tuning parameter (? ? 0). We will pursue a fast model optimization of Eq. (12) in next section.

4 MODEL OPTIMIZATION This section describes how to solve the optimization prob- lem in Eq. (12). The traditional offline (or batch) methods require the evaluation of the full gradient of the loss function E[lt(XXT )] iteratively, thus leading to a heavy computation load when the size of T is too large. In order to solve the optimization problem in Eq. (12) efficiently, we use the stochastic proximal gradient descent method [47], which only uses a very small number of training samples for model updating in each iteration. The stochastic proximal gradient descent method updates the (i+ 1)th iteration by  Xi+1 = argmin X  :  ?X ? ?iGt(Xi)?2 + ?i??X?2,1,  s.t. : ?X?2,? ? ?, (13)  where Gt(Xi) is a randomly generated subgradient satis- fying E(Gt(Xi)) ? ?E[lt(Xi(Xi)T )] and ?f(.) denotes the subdifferential of f(.). There are many choices of Gt(Xi) to satisfy this condition. As we have  ?E[lt(Xi(Xi)T )] = E[?lt(Xi(Xi)T )] (14)  in our case, we simply choose Gt(Xi) ? ?lt(Xi(Xi)T ) by uniformly sampling t from T , which is a typical way to sample the stochastic subgradient. If lt(.) is the hinge loss, Gt(.) can be  Gt(X i) =  { (pt3 ? pt1)pTt2X  i, lt(X iXi  T ) ? 0;  0, otherwise.

(15)  The subproblem in Eq. (13) is convex but includes a non- smooth term ?X?2,1 and a constraint ?X?2,?. Typically, one can only obtain an approximate solution of Eq. (13), which can be extremely close to the true optimal solution (depending on how many iterations) for some state-of-the- art methods. Fortunately, since the ?.?2,? and ?.?2,1 are conjugate to each other, the problem of Eq. (13) has a closed form solution shown in Theorem 1.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   Algorithm 1 Online Similarity Learning via Low Rank and Group Sparsity (OSLLR-GS) Input: ?(? 0), ?(? 0), c(> 0), m(> 0), and T .

Output: X  1: Initialize i = 0 and X0 = 0 ? Rd?m 2: while True do 3: Let ?i = c?  i 4: Uniformly sample t from T 5: X  i+ 12 k. = X  i k. ? ?i?lt(XiXiT )  6: L = min  ( ?  ?X i+1  k. ?  ,max  ( 1? ?  i?  ?X i+1  k. ?  , 0  )) 7: Xi+1k. = LX  i+ 12 k.

8: Update X?i by Eq. (18) 9: i = i+ 1  10: end while  Theorem 1: The optimal solution to (16)  min X  :  ?X ? C?2 + ??X?2,1,  s.t. : ?X?2,? ? ?, (16)  is for all k = 1, 2, ? ? ? , d,  X?k. = min  ( ?  ?Ck.? ,max  ( 1? ??  i  ?Ck.? , 0  )) Ck. (17)  where we set ?i = c? i  (c = 1 in our case). The proof to The- orem 1 is provided in Appendix. I. Finally, we summarize the stochastic proximal algorithm for Eq. (12) in Alg. 1.

For the general stochastic proximal gradient descent method, there is no guarantee that the sequence {E(F (Xi))} converges. The convergent sequence is {E(F (X?i))} with a convergence rate O(i?1/2) where X?i is the weighted average of all historical iterations defined as  X?i := ( i?  j=1  ?j)?1 i?  j=1  ?jXj , (18)  where we consider X? as our final solution due to it is more stable in practice. Actually, for space limitation we omit some additional conditions which are required to guarantee this convergence and convergence rate. Readers who are interested in it may refer to some recent literatures about stochastic optimization, e.g., [47], [48] for convex optimiza- tion, and [49], [50] for nonconvex cases .

5 EXPERIMENTS In this section, we present several experiments and com- parisons to validate the proposed model. Experiments are conducted on three types of data: a) Synthetic data, which is randomly generated to verify the behavior of the pro- posed model; b) Visual Place Categorization (VPC) 09 video dataset, which was captured in the same fashion as a real online system; c) Caltech 256 image classification dataset; and d) two UCI datasets.

For evaluation, we compare our model OSLLR-GS with the state-of-the-art methods including both online learning methods (OASIS [16], [17], [23], LMNN [18], LEGO [19], OMLLR [21] and MCML [20]) and offline (batch) training  5 10 15 20       Feature Dimension  S in  gl e  V al  ue      OASIS(rank=20) OMLLR(rank=3) OMLLR?GS(rank=3)  Fig. 2. Comparison of the singular values for different models with or without the low rank constraint. The horizontal axis and the vertical axis are the feature dimension and singular value, respectively.

OMLLR-GS  OMLLR  OASIS  Fig. 3. Comparison of the weights for feature selection, where the results of ours, OMLLR and OASIS are shown in the first, second and last row, respectively. Ours with groups sparsity is more efficient for feature selection.

methods (K-Nearest Neighbor and Wu?s method [51]). The accuracy is evaluated by a K-NN classification procedure, i.e., the testing sample is classified by a majority vote of its K neighbors, with the object being assigned to the class most common among its K nearest neighbors (K is a positive integer, e.g., 1), where the similarity is measured by the corresponding similarity learning model. The accuracy is measured by  Acc = ] of samples classified correctly  ] of all samples , (19)  where ] means the number.

For comparison, we choose two criteria as follows:  Wmax = arg max Wi  Acc(i), i ? {1, . . . , N} (20)    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   W = N? i=1  ?iWi/ N? i=1  ?i (21)  where N is the total number of iteration, Wi is the model matrix of i-th step iteration, and acc(i) is the accuracy of the corresponding Wi. The first criterion is the model with the highest accuracy and the second is the average weighted performance of the model. As the accuracy of the online similarity learning model will fluctuate in each updating iteration, the model with greater W in Eq. 21 is more robust in practice and the model in Eq. 20 is only chosen for comparison here.

5.1 Synthetic Dataset To validate the effectiveness of the new constraints in our model in Eq. (12), i.e., low rank and group sparsity con- straints, we generate the synthetic data and compare our model (OSLLR-GS) with the state-of-the-arts. Specifically, our synthetic dataset is a two-classes data with dimension as 20, and we random sample 200 samples from each class, i.e., we totally have 400 samples (30% training, 70% testing).

To achieve this, we first generate two different Gaussian models with the dimension as 5, randomly sample 200 from each model, and project them into 20d feature space by random projection.

i Effectiveness of the low rank constraint: As shown in Fig. 2, we plot the singular values of the model W (W = XXT in our case) using SVD calculation. When the data rank is 5, the rank is 3 for both our OSLLR-GS and OMLLR [21]. In comparison, the rank for OASIS [16], [17], [23] is 20. For the same size of model W , lower rank in an appropriate range means less model complexity and induces lower risk of overfitting. Therefore, Fig. 2 justifies the effectiveness of the low rank constraint.

ii The efficiency of feature selection via group sparsity: The second term of Eq. (12) with the tuning parame- ter ? is used for feature selection with group sparsity.

As shown in Fig. 3, the value of ?Xk.? shows the importance (weight) of the corresponding feature dimension. Our OSLLR-GS successfully selects the 5 most useful features out of the redundant features.

In comparison, OASIS and OMLLR fail to do that.

iii Comparison on the Convergence Rate: We illustrate the convergence curve of our model by adopting the synthetic dataset, where the cost is calculated by Eq. (12). We can see that the cost function of our model converges iteratively as shown in Fig. 5, where we plot the cost every 100 iterations.

In this case, we preset the feature dimension as 20 and the data rank in the embedded subspace as 5.

5.2 Visual Place Categorization (VPC) 09 Dataset The VPC 09 dataset is collected from 6 home environments using a rolling tripod plus a camera to mimic an online robot system, including 12 different scenarios with the resolution 1280 ? 720 for every image. For a fair comparison, we follow the experiments setup in [53] and also adopt 5  0 200 400 600 800 1000 Iterations(100/step)   0.5   1.5   Co st  #107  # 102  Fig. 5. Comparison of the convergence rate of our model.

categories (bedroom, bathroom, kitchen, living-room, and dining-room). The CENTRIST feature [53] is extracted from each image (or frame), resulting in 1302 dimensions with the spatial-pyramid structure. We compare our OSLLR-GS with the state-of-the-art methods, including online methods (OMLLR [21], OASIS [16], [17] and LMNN [52]) and batch training methods (K-Nearest Neighbor, 1-NN and 5-NN, and Wu?s method [53]). The leave-one-out cross validation strategy is used to evaluate all algorithms. All experiments were repeated for 6 times and we report the average perfor- mance. In each run, one home was reserved for testing and all other 5 homes were combined to form a training set.

Fig. 4 compares our OSLLR-GS with OMLLR [21] and OASIS [16], [17], where each subfigure corresponds to Home 1-6 for 3 million iterations with 10k/step. The curves of ?Ours? and OMLLR are generated by weighted expectation W , so convergence is guaranteed; ?Ours(Iter)? and OASIS are the results of individual iterations, therefore they fluctu- ate step-by-step. The statistical results are shown in Tab. 1, our OSLLR-GS generally outperforms other online learning methods, including OMLLR, OASIS [16], [17], LMNN [52] and also K-NN based batch training methods (1-NN and 5-NN). IROS [53] is better than our model because it uses the batch training model. The accuracy of online learning models is expected to be lower than those of the batch train- ing methods, therefore the performance of our OSLLR-GS is acceptable. For video-based frame-level scene classification, there exists a strong Markov Random property between con- secutive frames. In [53], Wu et al. use temporal smoothing to improve the accuracy of the coarse result. In contrast, we only adopt a simple median filter (of width 5) for frame- level temporal smoothing. After temporal smoothing, the accuracies of both online learning and batch training are improved. Our OSLLR-GS is still better than OMLLR as shown in Tab. 1. According to Tab. 1, another interesting point is that all compared methods perform differently for different sub-datasets, i.e., Home1-6. This is because each sub-dataset is somewhat different, e.g., different scenarios, different condition.

5.3 Caltech 256 Dataset  We also test our OSLLR-GS using the Caltech 256 dataset [54]. Following [23], we select 20 and 50 classes from the Caltech 256 dataset (The class label is shown in Appendix. I- I). For each set, images from each class were split into a    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   0 50 100 150 200 250 300 0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  Iterations(10k/step)  A cc  u ra  cy  Home1      OASIS OMLLR Ours Ours(Iter)  (a) Home1  0 50 100 150 200 250 300  0.15  0.2  0.25  0.3  0.35  0.4  Iterations(10k/step)  A cc  u ra  cy  Home2      OASIS OMLLR Ours Ours(Iter)  (b) Home2  0 50 100 150 200 250 300  0.2  0.25  0.3  0.35  0.4  0.45  Iterations(10k/step)  A cc  u ra  cy  Home3      OASIS OMLLR Ours Ours(Iter)  (c) Home3  0 50 100 150 200 250 300 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  Iterations(10k/step)  A cc  u ra  cy  Home4      OASIS OMLLR Ours Ours(Iter)  (d) Home4  0 50 100 150 200 250 300  0.2  0.25  0.3  0.35  0.4  Iterations(10k/step)  A cc  u ra  cy  Home5      OASIS OMLLR Ours Ours(Iter)  (e) Home5  0 50 100 150 200 250 300  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  Iterations(10k/step)  A cc  u ra  cy  Home6      OASIS OMLLR Ours Ours(Iter)  (f) Home6  Fig. 4. The comparison of the accuracy between ours (OSLLR-GS) and other methods, such as OMLLR [21] and OASIS [16], [17] for home1-6.

In each figure, the x-axis corresponds to the iteration steps (10k for each) and the y-axis is the current accuracy, where OASIS and OMLLR are denoted by dash blue and dash black line; ?Ours? (sold red) is the weighted expectation result of our OSLLR-GS, and ?Ours (Iter)?(dash green line) is the result of each iteration, so it fluctuate and cannot guarantee converge.

TABLE 1 The comparison of the average accuracy of our OMLLR and the state-of-the-art methods using VPC 09 dataset.

Filter Methods Home1 Home2 Home3 Home4 Home5 Home6 Avg  No  Ours 42.68 31.54 36.01 35.60 35.12 42.93 37.31  Online OMLLR 42.36 21.53 37.53 40.43 32.22 38.28 35.39  OASIS [16], [17] 25.33 21.32 21.99 20.57 24.84 39.18 25.54 LMNN [52] 39.41 28.75 36.79 39.06 30.74 34.88 34.94  No Batch IROS [53] 44.77 33.33 40.68 43.28 41.10 48.07 41.87  1-NN 41.83 27.48 33.96 38.66 30.85 29.70 33.75 5-NN 41.18 28.23 34.33 39.82 31.62 31.56 34.46  Yes Online Ours 45.65 34.21 38.43 39.89 38.26 44.99 40.23  OMLLR 46.03 21.66 38.59 41.95 33.05 41.29 37.10 Batch IROS [53] 44.58 35.89 40.96 49.93 46.91 55.46 45.62  training set of 40 images and a test set of 25 images. A cross- validation procedure is also adopted to select the values of hyper parameters. For image representation, we adopt the same features used in [23] with the feature dimension as 1000 for a fair comparison. For evaluation, a standard ranking precision measure based on nearest neighbors is also used, i.e., all other training images are ranked according to their similarity to the query image. The number of same- class images among the top k images (the k nearest neigh- bors, e.g., 1, 10, 50) is computed, which yields a measure known as precision-at-top-k and provides a precision curve as a function of the rank k. We also calculate the mean average precision (mAP), a widely used criterion in the information retrieval community.

Our method OSLLR-GS is compared with the state-of- the-art online similarity learning methods, including OM- LLR [21], OASIS [16], [17], [23], LMNN [18], LEGO [19], MCML [20], Loreta [38] [39] and Euclidean (the standard Euclidean distance in feature space). The statistic results are proposed in Tab. 2. In general, our model OSLLR-GS outperforms other methods expect for OMLLR for Top 1 prec., including OASIS and OMLLR with ? = 0. Even the mean average precision of Loreta [38] [39] is better, its performance is not good when K is smaller, e.g., K = 1, which is the most important criterion due to users always adopt K = 1 or K = 5 for K-NN classification in practice.

We compare the effectiveness of tuning parameter, ? and ?, as shown in Tab. 3, where we set ? ? inf and ? = 0    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   0 10 20 30 40 50  0.05  0.1  0.15  0.2  0.25  0.3  0.35  number of neighbors  P re  ci si  on  Var20      Random  Ours OMLLR OASIS MCML LEGO LMNN Euclidean Loreta  (a) 20 Classes  0 10 20 30 40 50  0.05  0.1  0.15  0.2  0.25  number of neighbors  P re  ci si  on  Var50      Random  Ours OMLLR OASIS LEGO LMNN Euclidean Loreta  (b) 50 Classes  Fig. 6. Comparison of the performance of our model, OMLLR, OASIS, LMNN, MCML, LEGO, Loreta and the Euclidean metric in the feature space.

Each curve shows the precision at top k as a function of k neighbors. The results are averaged across 5 train/test partitions (40 training images, 25 test images), the result of random prediction is shown at the bottom. (a) 20 classes (b) 50 classes.

in order to remove the effectiveness of low rank constraint and group sparsity constraint, respectively. When neither constraint is activated, the performance of our model will deteriorate and is similar to OMLLR (? = 0). However, our model still outperforms other state-of-the-art methods. This validates the effectiveness of these two constraints in our model.

Fig. 6 shows the precision curves for retrieval. Interest- ingly, when the class number increases from 20 classes to 50 classes, the performances of all methods are poor. This is because for a fixed number of training steps, e.g., 35K iterations in our case, the higher the number of classes is, the lower the probability of different samples meeting each other is. The performance of our OSLLR-GS gets closer to the best one by OMLLR.

5.4 Comparison on the Model Parameters ?, ? and m In this subsection, we compare the influence of the model parameters ?, ? and m to the model accuracy, where ? in Eq. (12) is used to control the rank redundancy, ? in Eq. (12) is to constraint the group sparsity and m is the number of columns of X ? Rd?m. Following Sec. 5.2, we also adopt the VPC 09 video dataset as well by using Home1 as testing set and Home 2-6 as training set. We first fix all other parameters and change the parameter ? in the range of [0.01, 0.1, 1, 10, 100, 200, 500, 1000], the results are shown in Fig. 7. We can see that the accuracy of our model fluctuates when the value of ? increasing. Intuitively, too big or too small value of ? cannot generate good result.

We change the parameter ? by setting ? from 10?5 to 0.01, and the results are shown in Fig. 8. We can see the accuracy is bad when ? is too small or too large. Especially if ? is greater than 0.1, we cannot get an acceptable result due to there are too many zero rows of the metric matrix.

Actually, both ? and ? are model parameters, we should choose them properly in practice.

We then vary the parameter m by setting m as [3, 25, 50, 75, 126], and the results are demonstrated in Fig. 9.

It is obviously when m is small, the accuracy is bad, and  0.01  0.1    1   10  100  200  500 1000  0.35  0.4  0.45  0.5  0.55  ?  Ac cu  ra cy  Fig. 7. Comparison of the accuracy by varying the value of ?.

1e?005 0.0001  0.001 0.0025  0.005 0.0075   0.01 0.2  0.25  0.3  0.35  0.4  0.45  0.5  ?  Ac cu  ra cy  Fig. 8. Comparison of the accuracy by varying the value of ?.

the performance keep improved during the value of m increasing. This is because the larger value of m may induce a global optimal solution and the matrix X can contain enough information as well. However, in practice, a larger value of m will increase the size of the matrix X and increase both the memory and computation burden as well.

Therefore, we should balance it in practice.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   TABLE 2 Average precision and precision at top 1, 10, and 50 of all compared methods. Values are averages over 5-fold cross-validations; ? values are the  standard deviation across the 5 folds. OMLLR(? = 0) means no consideration of the effectiveness of low rank constraint.

20 Ours OMLLR OMLLR(? = 0) OASIS MCML LEGO LMNN Euclidean Loreta classes Matlab Matlab Matlab Matlab Matlab+C Matlab Matlab+C - Matlab MeanAvg 22? 1.4 23? 1.3 21? 1.3 21? 1.4 17? 1.2 16? 1.2 14? 0.6 14? 0.7 24.3? 0.7 Top 1 32? 1.7 33? 1.7 29? 1.8 29? 2.6 26? 2.3 26? 2.7 26? 3.0 25? 2.6 28.4? 3.2 Top 10 26? 1.6 26? 1.6 23? 1.7 24? 1.9 21? 1.5 20? 1.4 19? 1.0 18? 1.0 26.4? 0.9 Top 50 18? 0.9 20? 1.0 17? 0.6 15? 0.4 14? 0.5 13? 0.6 11? 0.2 12? 0.2 21.4? 0.6 50 Ours OMLLR OMLLR(? = 0) OASIS MCML LEGO LMNN Euclidean Loreta classes Matlab Matlab Matlab Matlab Matlab+C Matlab Matlab+C - Matlab MeanAvg 13? 1.4 14? 0.3 13? 0.4 12? 0.4 N/A 9? 0.4 8? 0.4 9? 0.4 14? 0.6 Top 1 22? 1.5 22? 1.4 18? 1.5 21? 1.6 N/A 18? 0.7 18? 1.3 17? 0.9 17? 0.7 Top 10 15? 0.9 17? 0.3 15? 0.4 16? 0.4 N/A 13? 0.6 12? 0.5 13? 0.4 15? 0.4 Top 50 10? 0.5 12? 0.4 11? 0.3 10? 0.3 N/A 8? 0.3 7? 0.2 8? 0.3 12? 0.5  TABLE 3 Effectiveness of tuning parameters, ? and ?, where ?? inf and ? = 0 means these two parameters have no effect.

20 classes Ours Ours(?? inf) Ours(? = 0) Matlab Matlab Matlab  Mean avg prec 22? 1.4 21? 1.4 21? 1.5 Top 1 prec. 32? 1.7 29? 1.8 30? 1.6 Top 10 prec. 26? 1.6 23? 1.6 23? 1.7 Top 50 prec. 18? 0.9 18? 0.7 17? 0.5  50 classes Ours Ours(?? inf) Ours(? = 0) Matlab Matlab Matlab  Mean avg prec 13? 1.4 13? 1.0 14? 1.3 Top 1 prec. 22? 1.5 18? 1.4 19? 1.4 Top 10 prec. 15? 0.9 15? 0.8 15? 1.0 Top 50 prec. 10? 0.5 10? 0.4 10? 0.4  3 25 50 75 126  0.25  0.3  0.35  0.4  0.45  0.5  m  Ac cu  ra cy  Fig. 9. Comparison of the accuracy by varying the value of m.

5.5 Comparison on the Time Consumption We adopt the Caltech 256 dataset to compare the time consumption of our OSLLR-GS with the state-of-the-art methods as shown in Tab. 4. The efficiency of our OSLLR- GS is competitived with OASIS and much faster than other methods, especially OMLLR. This is because the computa- tional complexity of our OSLLR-GS is in the order of O(d2) with the feature dimension d and taking the columns of X as d?(d ? d? ? 2d); in contrast, OMLLR using SVD calculation is O(d3). All the experiments are performed on the computer with 4G RAM, Pentium IV 2.6GHz CPU.

5.6 Comparison on the Training Data Size In this subsection, we justify the effectiveness of our model by varying the size of training data. As shown in Fig. 10 adopting the Home 1 video data set, the original training  0 0.2 0.4 0.6 0.8 1  0.2  0.25  0.3  0.35  0.4  0.45  0.5  Ratio of original training data size  Ac cu  ra cy      Ours OASIS OMLLR  Fig. 10. Comparison the accuracy of our model with OASIS and OMLLR by varying the size of training data.

data has 23058 data and we decrease the ratio of the original training data size from 1 to 0.1 with each step of 0.1. We com- pare our model with both OASIS and OMLLR, ours nearly outperforms the other two methods for all cases. Moreover, we can see that the accuracy decreases dramatically when the ratio is lower than 0.5; in contrast, the accuracy is similar and competitive when the ratio is between 0.6 and 1, which justifies the effectiveness of our model to overcome overfitting.

5.7 Comparison on the Feature Selection Ability  In this subsection, we compare the feature selection ability of our online similarity learning model with the state-of- the-art feature selection methods, e.g., mRMR [55], CSFC [56]. Two UCI datasets are adopted here, i.e., the UCI-    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2688360, IEEE Transactions on Big Data   TABLE 4 Runtime (in minutes) of all compared methods for around 35K training steps for 20 and 50 classes, respectively.

Ours OMLLR OASIS OASIS MCML LEGO LMNN fastLMNN Matlab Matlab Matlab Matlab+C Matlab+C Matlab Matlab+C Matlab+C  20 classes 41? 3 550? 43 45? 8 0.15? 0.02 7425? 106 533? 49 631? 40 365? 62 50 classes 42? 5 731? 71 25? 2 1.6? 0.04 N/A 711? 28 960? 80 2109? 67  Mfeat dataset 2, which has about 2000 instances distributed in 10 different classes with the feature dimension as 649; the UCI-SPECTF dataset 3, which has about 267 instances distributed in 2 different classes with the feature dimension as 44. In order to evaluate the feature selection ability of the corresponding methods, each method is required to select the same number of feature dimensions from the original feature dimension, and then the accuracy is cal- culated by the traditional 1-Nearest Neighborhood (1-NN) method using the selected feature dimensions. Therefore, the greater the accuracy is, the more efficiency the feature selection ability of the corresponding method will be. For our method, the feature selection is achieved by ranking the feature dimension depending on the value of ?Xk.?2.

The statistic results are show in Tab. 5, where our method outperforms the other two methods in most cases especially for the UCI-Mfeat dataset with more classes and higher feature dimensions.

6 CONCLUSIONS This paper presents a general online similarity learning framework (Online Similarity Learning via Low Rank and Group Sparsity, OMLLR-GS) to address two types of overfit- ting issues, i.e., the feature redundancy and the rank redun- dancy. For modeling, the similarity learning problem is for- mulated as an optimization problem. We use the max norm to restrict the metric matrix in a low rank space and the `2,1 norm to pursue a sparse feature set. For optimization, we apply the stochastic proximal gradient decent method and prove a closed form updating in each iteration with the complexity of O(d2). The experiments on synthetic data have verified the ability of the proposed model to overcome the overfitting problems. Comparisons with the state-of-the- art methods on real world data have also shown that the proposed method is as efficient as the OASIS algorithm and performs as well as the OMLLR algorithm.

APPENDIX I Proof to Theorem 1  The problem is convex and separable in terms of rows.

We can consider each individual problem in the following:  min x  :  ?x? c?2 + ??x?,  s.t. : ?x? ? ? (22)  where x denotes an arbitrary row of X and c is the corre- sponding row of C. ?x? ? ? means the `2 norm of arbitrary row of X is less than ?, which is equal to ?X?2,? ? ? depending on the definition of the `2,? norm of X . Since  2. http://archive.ics.uci.edu/ml/datasets/Multiple+Features 3. http://archive.ics.uci.edu/ml/datasets/SPECTF+Heart  this is a strongly convex problem, the optimal solution is unique and satisfies the optimality condition:  x? ? c+ ???x?? ? N?x???(x?) 6= ?,  where ??x? is the sub-differential set of ?x? and N?x???(x?) is the normal cone on x? of the set ?x? ? ?.

It suffices to verify that x? = min  ( ? ?c? ,max  ( 1? ??c? , 0  )) c  satisfies the optimality condition. First we have  ??x? = { { x?x?} if x 6= 0 {x : ?x? ? 1} if x = 0 (23)  and  N?x???(x) =  ??? {0} if ?x? < ?  {tx : t ? 1} if ?x? = ? ? otherwise.

(24)  One can verify that x? satisfies the optimality condition by enumerating the following three situations  i ?c? ? ?: x = 0; ii ?+ ? ? ?c? > ?: x = (1? ?/?c?)c;  iii ?+ ? < ?c?: x = ?c/?c?.

It completes the proof.

APPENDIX II For the Caltech 256 dataset, we adopt 20 classes and 50 classes for evaluation, which are defined the same as [21], [23]. The labels are defined as below:  i 20 classes: airplanes-101, mars, homer-simpson, hour- glass, waterfall, helicopter-101, mountain-bike starfish- 101, teapot, pyramid, refrigirator, cowboy-hat, giraffe, joy-stick, crab-101, birdbath, fighter-jet, tuning-fork, iguana, dog.

ii 50 classes: car-side-101, tower-pisa, hibiscus, saturn, menorah-101, rainbow, cartman, chandelier-101, back- pack, grapes, laptop-101, telephone-box, binoculars, helicopter-101, paper-shredder, eiffel-tower, top-hat, tomato, star-fish-101, hot-air-balloon, tweezer, pic- nictable, elk, kangaroo-101, mattress, toaster, electric- guitar-101, bathtub, gorilla, jesus-christ, cormoran- t, mandolin, light-house, cake, tricycle, speed-boat, computer-mouse, superman, chimp, pram, friedegg, fighter-jet, unicorn, greyhound, grasshopper, goose, iguana, drinking-straw, snake, hotdog.

