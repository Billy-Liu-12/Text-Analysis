Notice of Violation of IEEE Publication Principles

Abstract    Data with temporal information is constantly generated, sampled gathered and analyzed in different domains such as medicine, finances, engineering, environmental sciences to earth sciences. This paper includes Temporal weighted miner (TWM) algorithm, the importance of each transaction period was first reflected by a proper weighted calculated on the various representations of time series patterns. It partitioned the time-variant database in light of weighted periods of transactions and performs weighed mining. Extensive experimental studies are conducted to evaluate the performance of the TWM.

Explicitly, the execution time of TWM was in orders of magnitude, smaller then those required by other competitive schemes which were directly extended from existing methods such as Apriori.

1. Introduction   Data mining is a set of techniques for generating or testing hypotheses using large data sets. In the clinical data, these techniques are becoming useful.

Large-scale clinical databases provide a detailed perspective on patient phenotype in disease and the characteristics of health care processes. Important information is often contained in the relationships between the values and timestamps of sequences of clinical data. The analysis of clinical time sequence data across entire patient populations may reveal data patterns that enable a more precise understanding of disease presentation, progression, and response to therapy, and thus could be of great value for clinical and transnational research.

The process of data analysis in health care is becoming more and more complicated for a number of reasons:  * new techniques, such as microarrays, have given rise to the generation of data with unusual  characteristics, such regarding few patients described by a large number of variables.

* the integrated analysis of data from different sources concerning the same health-care topic.

* with the wide availability of sophisticated and cheap computing equipment the exploitation of models to support clinical decision making has become a practical option.

As traditional statistical methods have been unable to meet all of these requirements, there has been influx of new methods from a number of fields, in particular from machine learning. Recent work suggests that the combination of temporal data mining methods with techniques from artificial intelligence research on knowledge-based temporal abstraction may enable the mining of clinically relevant temporal features from these previously problematic general clinical data. Data with temporal information is constantly generated, sampled, gathered and analyzed in different domains.

Using temporal data as temporal sequences without any preprocessing fails to extract key features of this data. For this reason, before applying mining techniques, an appropriate representation of temporal sequences is needed.

2. Representation of Time series patterns   In temporal data mining, the representation of data takes place before defining the similarity measures between sequences and applying actual data mining techniques [1]. The data is represented into time series by either keeping it in original form ordered by their instant of occurrence and without any pre-processing [4], or subsequences of a sequence are obtained using windowing and by finding a piecewise linear function able to approximately describe the entire initial sequence [2,3]. Another approach[1,6] to represent data into time series data is segmenting a sequence by iteratively merging two similar segments, that are chosen based on the squared error minimization. An extension to this method is to associate with each segment a weight value, in order to define the  2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.599   2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.599     importance of each segment according to the entire sequence [5].

The two main types of data band ranges use in the representation of time series for TWM. (i) data band range, Dangerous data band (DDB)  consists of the data during the occurrence of a significant event, such as the values of the rise in blood sugar, and (ii) data band type is called the Risky data band (RDB). RDB consists of the data before decease event happens.

RDB can be a clue to forecast a significant event.

According to DDB and RBD, one can estimate the weight of each segment and predict its closeness to DDB and RDB that help us to find any early sign of a crucial event.

2.1. Representation of time series  Specifying the DDB and RDB :  According to DDB and RDB, if a data point (xi,yi) is a dangerous point, a risky point or an out-of-band point. More clearly, If (xi,yi) is a risky point, then ?  ?  yi ? ?, where ? < ? and 0 ? i ? n If (xi,yi) is a dangerous, then ?  ?  yi ? ?, where ? < ? and 0 ? i ? n, If (xi,yi) is an out-of-band point, then (yi < ?,where,0 ? i ? n) or (yi > ?, where 0 ? i ? n) Finding the Function of a Subsequence Pn(x) = a0 + a1(x-x0) + a2(x-x0)(x-x1) + ... + an(x-x0)(x- x1)...(x-xn-1),where ak = f[x0, ..., xk], for k = 0, 1, ..., n.

Segmenting a Subsequence y = fi + 1/h (fi+1 - fi)(x - xi), where f(xi) = fi and f(xi+1) = fi+1 Approximating the Area under a Subsequence To estimate the weight of a subsequence, compute the area that estimates the area under this subsequence f(x). Subdividing the subsequence into n subintervals of length h and then finding the straight-line segment of each subinterval leads to n trapezoids. Hence, to approximate the area under a subsequence, using the trapezoidal rule. The area of the n trapezoids Tfi-1fixixi-1 is calculated.

Estimating the Weight of a Segment To find the weight of a specific segment, one can estimate the closeness of segment to the Dangerous Data Band (DDB) or the Risky Data Band (RDB). The closer the segment of a subsequence is to DDB or to RDB, the greater its weight is according to the weight of the entire sequence [7].

3. Why association is must.

Errors and uncertainty are facts of life in all information systems. It could be worse when underlying dataset is approximate, irregular and obtained from heterogeneous data sources. The problem may present itself as unexpected errors, data  conflicts, and so on. An association rule in a transactional database may not be strong (according to specific support and confidence thresholds) in the whole database, but only when considering the transactions in a specified time interval (e.g., during the winter of 2005). An association rule bound to a time interval, where it is strong, is termed temporal association rule [8]. Starting from short time intervals and progressively extending them to the maximum possible length where the rule remains strong can perform identification of such a rule. Association rules in transactional databases (e.g., people who buy turkey they also buy pumpkins) may hold only in particular temporal intervals (e.g., during the last week of November every year). These association rules can be temporal, Cyclic and cylindrical.

The horizon of frequent pattern mining by introducing a weighted model of transaction-weighted association rules (abbreviatedly as weighted association rules) in a time-variant database. To propose an efficient Time Weighted Miner (abbreviatedly as TWM) algorithm to perform the mining for this problem as well as conduct the corresponding performance studies. In algorithm TWM, the importance of each transaction period is first reflected by a proper estimated weight. Then, TWM partitions the time-variant database in light of weighted periods of transactions and performs weighted mining.

Algorithm TWM explores the mining of weighted association rules, denoted by (X ? Y)W, which is produced by two newly defined concepts of weighted ? support and weighted ? confidence in light of the corresponding weights in individual transactions. An association rule X ? Y is termed to be a frequent weighted association rule       (X ? Y )W if and only if its weighted support is larger than minimum support required, i.e., suppW(XUY ) >min_supp, and the weighted confidence confW(X ? Y) is larger than minimum confidence needed, i.e., confW(X ? Y) >min_conf. Instead of using the traditional support threshold min_ST = ?|D| ? min_supp? as a minimum support threshold for each item, a weighted minimum support, denoted by min_SW = {? |Pi| ?W (Pi)} ?min_supp, is employed for the mining of weighted association rules, where |Pi| and W (Pi) represent the amount of partial transactions and their corresponding weight values by a weighting function W (?) in the weighted period Pi of the database D.

Let NPi(X) be the number of transactions in partition Pi that contain itemset X. The support value of an itemset X can then be formulated as SW(X) = ? NPi(X) W(Pi). As a result, the weighted support ratio of an itemset X is suppW(X) = SW(X) / ? |Pi|?W (Pi).

TWM first partitions the transaction database in light of weighted periods of transactions and then Timely accumulates the occurrence count of each candidate 2-itemset based on the intrinsic partitioning characteristics. With this design, algorithm TWM is able to efficiently produce weighted association rules for applications where different time periods are assigned with different weights. Algorithm TWM is also designed to employ a filtering threshold in each partition to early prune out those cumulatively infrequent 2-itemsets. The feature that the number of candidate 2-itemsets generated by TWM is very close to the actual number of frequent 2-itemsets allows us of employing the scan reduction technique by generating Cks from C2 directly to effectively reduce the number of database scans.

Experimental results show that TWM produces a significantly smaller amount of candidate 2- itemsets than AprioriW, i.e., an extended version of Apriori algorithm. In fact, the number of the candidate itemsets Cks generated by TWM approaches to its theoretical minimum, i.e., the number of actual frequent k- itemsets, as the value of the minimal support increases.

Specifically, the execution time of TWM is, in orders of magnitude, smaller than those required by AprioriW. Sensitivity analysis on various parameters of the database is also conducted to provide many insights into algorithm TWM.

Note that the problem of mining weighted association rules will be degenerated to the traditional one of mining association rules explored in previous works if the weighting function is assigned to be W(?) = 1, meaning that the model considered can be viewed as a general framework of prior studies.

Let n be the number of partitions with a time granularity, in database D. In the model considered, Pi denotes the part of the transaction database where Pi is a subset of D. The mining of transaction - weighted association rules (abbreviatedly as weighted association rules), i.e., (X?Y)W, where X ?Y is produced by the concepts of weighted ? support and weighted - confidence. Further, instead of using the traditiona lsupport threshold min_ST = ?|D| ? min_supp? as a minimum support threshold for each item, a weighted minimum support for mining an association rules is determined by min_SW = {? |Pi| ?W (Pi)} ? min_supp where |Pi| and W (Pi) represent the amount of partial transactions and their corresponding weight values by a weighting function W (?) in the weighted period Pi of the database D.

4. Algorithm of TWM   In general, databases are too large to be held in main memory. Thus, the data mining techniques  applied to very large databases have to be highly scalable for efficient execution. For ease of exposition, the processing of a partition is termed a phase of processing. Time candidate set of itemsets is composed of the following two types of candidate itemsets, i.e.,  (1) The candidate itemsets that were carried over from the previous Progressive candidate set in the previous phase and remain as candidate itemsets after the current partition is included into consideration (Such candidate itemsets are called type ?  candidate itemsets); and  (2) The candidate itemsets that were not in the Progressive candidate set in the previous phase but are newly identified after only taking the current data partition into account (Such candidate itemsets are called type ? candidate itemsets).

Under TWM, the cumulative information in the prior phases is selectively carried over toward the generation of candidate itemsets in the subsequent phases. After the processing of a phase, algorithm TWM outputs a Progressive candidate set of itemsets, their occurrence counts and the corresponding partial supports required. The procedure of algorithm TWM is outlined below, where algorithm TWM is decomposed into four sub-procedures for ease of description. C2 is the set of Time candidate 2-itemsets generated by database D. Recall that NPi(X) is the number of transactions in partition Pi that contain itemset X and W (Pi) is the corresponding weight of partition Pi.

4.1. Algorithm TWM (n, min_supp): Temporal Weighted Miner  Procedure I: Initial Partition based on time i.e. yearly, half yearly, Quarterly etc.

1. |D| = Pi=1, n |Pi|; 2. C2 = ?;  Procedure II: Candidate 2-Itemset Generation 1.  begin for i = 1 to n   // 1st scan of D 2.    begin for each 2-itemset X2 ? Pi 3.     if ( X2 ? C2 ) 4.     X2.count = NPi(X2)?W(Pi); 5.     X2.start = i; 6.      if ( X2.count ? min_supp ? |Pi|?W(Pi) ) 7.      C2 = C2 U X2; 8.       if ( X2 ? C2 ) 9.       X2.count = X2.count + NPi(X2) ?W (Pi); 10.  if(X2.count<min_supp) ? ?m = X2.start, I (|Pm| ? W(Pm))) 11.   C2 = C2 ? X2; 12.   end 13. end  Procedure III: Candidate k-Itemset Generation 1. begin while (Ck ?? & k ? 2)     2.  Ck+1 = Ck*Ck; 3.   k = k +1; 4. end  Procedure IV: Frequent Itemset Generation 1.  begin for i = 1 to n 2.     begin for each itemset Xk ? Ck 3.   Xk.count = Xk.count + NPi (Xk)?W(Pi); 4      end 5.  end 6.  begin for each itemset Xk ? Ck 7.      if (Xk.count?min_supp ? ?m=1,n(|Pm|?W(Pm))) 8.   Lk = Lk U Xk; 9.   end 10. return Lk;   Procedure II (Candidate 2-Itemset Generation) first scans partition Pi, to find the set of all local frequent 2-itemsets in Pi. Algorithm TWM constructs C2 incrementally by adding candidate 2- itemsets to C2 as per the standard norms.

From Step 1 to Step 13 of Procedure II (Candidate 2- Itemset Generation), algorithm TWM processes one partition at a time for all partitions. At the end of processing Pi, an itemset X2 will be kept in C2 only if X2.count>min_supp??m =X2.start, i (|Pm|?W (Pm)).

Procedure III (Candidate k-Itemset Generation), with the scan reduction scheme [9], C2 produced by the first scan of database is employed to generate Cks in main memory.

From Step 1 to Step 9 of Procedure IV, as a result, those itemsets whose Xk.count ? min_supp ? ?m=1, n (|Pm|?W (Pm)) are the frequent itemsets Lks. The output of algorithm TWM consists of frequent itemsets Lks of database D. Finally, according to these output Lks in Step 10, all kinds of weighted association rules implied in database D can be generated in a straightforward manner.

Note that TWM is able to filter out false candidate itemsets in Pi with a partial support threshold to prune candidate 2-itemsets, the CPU and memory overhead of TWM can be further reduced. Owing to the small number of candidate sets generated, the scan reduction technique can be applied efficiently. As a result, only two scans of the database are required. Following problems with early Algorithms are * An early product intrinsically possesses a higher likelihood to be determined as a frequent itemset. * Some discovered rules may be expired. * Different transactions with various weighted addressed by taking ?Time and Weight information? into consideration by TWM.

4.2. Relative performances  A synthetic database of 4 years of transaction for our experimental results.

Figure 1: Relative performance studies between TWM and AprioriW   In all the experiments the transaction length 10, average length of frequent itemset is 5 and 1500 transactions in database D. The notation used is Tx?Iy ?Dm to represent a database in which D = m transaction, |T| = x, and |I| = y ie T10-I5-D1500 for x=10, y=5 and m=1500.

Analysis  Figure 1 shows the relative execution times for both two algorithms as the minimum support threshold decreases from 1% support to 0.1% support. When the support threshold is high, there are only a limited number of frequent itemsets produced. However, as the support threshold decreases, the performance difference becomes prominent in that TWM significantly outperforms AprioriW. Explicitly, TWM is in orders of magnitude faster than AprioriW, and the margin grows as the minimum support threshold decreases. By graphs this can be shows that TWM out performs AprioriW in execution time as minimum support values decreases from 0.8 to 0.1 %   T10-I6-D1500           0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  min_supp (%)  E xe  cu tio  n T  im e  (S ec  )  TWM  Apriori  T10-I4-D1500        0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 min supp (%)  E xe  cu tio  n T  im e  (S ec  )  TWM Apriori     Comparative Study of TWM between Time Granularity Half Yearly and Quarterly for Average estimated weights and with different minimum support values.

Weights Min_ Supp  | C2 | |Ck| | Lk | Length (Ck)  Length (Lk)  Jan-01 1.5 0.4 69 70 32 5 3  Jul-01 1.5 Jan-02 2 Jul-02 2 Jan-03 2.5 0.3 82 95 32 6 4 Jul-03 2.5 Jan-04 3 Jul-04 3 0.2 88 236 32 9 5    Table 1.1 Half Yearly Results of Average Weights with different minimum support Values.

Weights Min_  Supp | C2 | |Ck| | Lk | Length  (Ck) Length (Lk)  Jan-01 1 0.4 55 70 32 5 3 May-01 1 Sep-01 1 Jan-02 1.3 May-02 1.3 Sep-02 1.3 0.3 80 92 33 6 4 Jan-03 1.7 May-03 1.7 Sep-03 1.7 Jan-04 2 May-04 2 Sep-04 2 0.2 85 133 32 9 5   Table 1.2 Comparative Study of TWM with different minimum support values.

It observes by the values of table 1.1 (Half Yearly) and table 1.2 (Quarterly) that when we lower down the values of minimum support from 0.4 to 0.2, number of candidate-k itemsets is reduced from 236 to 133 even though Weights equally scattered in database and count and length of frequent itemsets are same. So mining time will be smaller. This shows that Time factor play a major role if considered appropriately.

