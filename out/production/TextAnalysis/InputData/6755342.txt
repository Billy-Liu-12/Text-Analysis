A Synthetic Information Approach to Urban-scale Disaster Modeling

Abstract?We describe a large-scale simulation of a hypotheti- cal nuclear detonation in an urban region. Simulating such a com- plex scenario requires modeling the population and its interac- tions with interdependent infrastructures such as transportation, communications, and healthcare. Our work represents the first model of a behaving human population, resolved to the individual level, where agents make decisions based on their health state, environmental conditions, and (informational) awareness. This ?big simulation? approach requires a large amount of carefully curated data at the input, which is combined into a ?synthetic information? model. The simulation is database-driven in a novel architecture that enables scaling, and it produces large amounts of data that in turn require advanced analytics in order to extract policy-relevant conclusions. We present results from a spatio- temporal analysis that draw out the connections between spatial variations in population behaviors and health outcomes.



I. INTRODUCTION  Disasters, be they natural or human-initiated, are complex systems, especially in urban settings. They generally affect multiple critical infrastructures in addition to the human pop- ulations, either directly or through cascading and feedback effects.

In this work we present a simulation of the aftermath of a hypothetical nuclear detonation at ground level in Washington DC. Our approach, however, is also applicable to modeling other kinds of disasters, as well as other social phenomena such as epidemics.

It is essential to model human behavior in the aftermath of a disaster, since people?s natural inclinations may be at odds with rescue efforts or with the optimal policies for long- term health outcomes. For example, people tend to look for their family members before evacuating, which might actually expose them to more radiation and lead to worse health outcomes in the aftermath of a nuclear detonation. Population behaviors also impact infrastructures in complex feedback loops. A rush to evacuation can lead to a traffic jam, which can in turn cause overloading of the communication (cellphone) network as too many calls go to the same base station. Being unable to communicate can cause more people to try to move towards family members, exacerbating traffic conditions.

In order to model these complexities, it is necessary to have detailed and faithful representations of the population  as well as the infrastructures. This is a big data problem in itself. Further, a simulation of this scale produces a very large amount of data, as we need to track each individual at a high resolution in time and space. Analyzing the generated data in order to extract useful, policy-relevant insights is also a big data problem.

In this work, we present an approach to these kinds of problems that is centered on the notion of synthetic infor- mation. The data that are needed for the simulation are not directly available anywhere. For example, it is impossible to know, through a survey, where every individual is going to be exactly at the time of the disaster. However, it is possible, through careful modeling, to construct an approximation of the data by combining available data from many sources. The same holds for data about infrastructures, where modeling is also necessary to compensate for the fact that some data are private and inaccessible (e.g., some cellphone network data).

The synthetic information approach involves the generation of a very detailed, large-scale representation of the region of interest, as described in section IV-A.

The simulation runs by transforming the initial synthetic information through successive iterations in order to model population behavior and interactions with infrastructures. In order to do this in a scalable way, we use a novel database- driven simulation architecture. An experiment design generally studies the interaction among multiple variables, thus involv- ing multiple replicates of the simulation, organized into several experiment ?cells?. This results in a very large quantity of data generated, with complex spatial and temporal patterns.

Understanding these patterns is a difficult, and ongoing, ana- lytic problem. We present some results from a spatial quantile regression model that afford some insights into the connections between spatial variations in behavior and health outcomes.

The rest of this paper is organized as follows. We begin by discussing prior work on this problem in order to contextualize our particular approach. Then we describe the scenario, which also defines the region and population that we study through simulation. After that we describe the simulation, going into detail about the construction of the synthetic information using multiple data sources, and the database-driven computational architecture. We then describe a particular experiment and a   DOI 10.1109/CSE.2013.161    DOI 10.1109/CSE.2013.161     novel analytical technique to extract insight into the conse- quences of behavior on medium term health outcomes. We end by discussing the benefits and challenges of large-scale simulations to study social systems.



II. RELATED WORK  The scenario is that a nuclear device is detonated in the middle of Washington DC. More details of the scenario are in section III below. It turns out that very similar scenarios have been studied for sixty years1. The physical effects of the event, such as radiation, blast damage, thermal effects, fallout cloud movement, etc. have been very well studied [1].

More recently, there have also been studies of the human consequences of the event in terms of the effects of different behavioral policies. In particular, a comparison of shelter-in- place and evacuation has been done, and the general recom- mendation is that shelter-in-place is the best policy [2], [3].

However, these studies don?t fully account for human be- haviors. Due to the damage to the communication and power infrastructures, most people close to ground zero will not be aware that the detonation is a nuclear detonation. Even if they know that this is what has happened, and that the best policy is to shelter in place, they will be motivated to get in touch with the family members and to find them. It is important to take such behavioral inclinations into account when modeling the consequences of the event. Our work is the first detailed model of a behaving population for this scenario.

Recently, a number of researchers have proposed high performance simulation platforms for simulating agent-based models; see [4], [5] and the references therein. Our system is a hybrid simulation, which differs from the earlier systems in the following ways: 1) Our system is specifically designed to handle multi- networks; these networks are spatially grounded and share the same basic coordinate system.

2) A key aspect of the work is the use of relational databases for information exchange and coordination between individual modules. Our work is motivated by a similar approach advo- cated by Heber and Gray in the context of finite element mesh simulations [6], [7], though this is the first application of this approach to socio-technical simulations. A similar approach has been used in the context of games by White et al. [8].

3) Extending the work of Heber and Gray, we show how an RDBMS can be used not only to store the input data and the output data produced by the simulations, but also as a part of the simulation engine itself. The RDBMS helps in three ways. First it provides a shared data structure for the individual simulations to exchange information. Situation assessment can be achieved by writing appropriate queries in a functional language such as SQL. Second, it naturally supports an unencapsulated view; individual modules and agents can work with projections of the database. This facilitates par- allelization and scaling. These two aspects of RDBMS are  1http://nuclearsecrecy.com/blog/2011/12/30/friday-image-bullseye-on- washington-1953/  Fig. 1. The Detailed Study Area (DSA) for the simulation.

unique to our system and have not been exploited earlier. The final aspect of RDBMS that is useful is to store all the input and output data. This allows us to support complex visual analytics environments to analyze the outputs and support decision making.

4) Another unique feature of the system is its ability to represent detailed individual behaviors. The individual be- havioral models capture agent behavior as it pertains to its interaction with specific infrastructure networks. Additionally, affects such as panic are also represented and discussed in detail in section IV-A.



III. SCENARIO  In this scenario a 10kT improvised nuclear device (IND) is detonated at ground level on the corner of 16?? and K Streets in Washington DC, which is just behind the White House.

The detonation is assumed to occur around 11 a.m. on a weekday. For our study, the region that we consider is shown in Figure 1. We refer to it as the detailed study area (DSA). The fallout cloud is expected to move largely eastward and east-by- northeastward, as shown by the funnel shape extending to the right in Figure 1. Ground zero is at the center of the circular region from which the fallout cloud emanates.

The boundaries of the DSA are defined by the .01 Gy fallout contour at 60 minutes joined with the thermal radiation contour at 2.1 cal/cm2 bounded by the boundary of the counties neighboring the District of Columbia.

Immediate damage is expected to be catastrophic around ground zero. Within 0.6 miles, no one is expected to survive.

This is borne out by our simulations as well. In the annular region between 0.6 miles and 1 mile from ground zero, there is heavy damage and many casualties. This region is also suggested to be the area where there is the greatest opportunity for saving lives. People further away are largely unhurt, but can be exposed to dangerous levels of radiation directly and via fallout [3].

The various infrastructures are expected to suffer heavy damage as well. Power is assumed to be lost around ground zero, and not restored for the duration of the simulation (48 hours after the detonation). Communication infrastructure is also affected. It is possible, however, to restore cellphone com- munications partially if mobile cell towers (Cells on Wheels, or CoWs) are brought in. The transportation infrastructure is affected because trains will not be running in the area, and     TABLE I DATASETS USED IN THE SIMULATION.

Synth. pop. Data sources Washington DC American Community Survey, base population TIGER/Line shapefiles,  National Center for Education Statistics, National Household Travel Survey, Navteq, Dun & Bradstreet  Transient population Destination DC, Smithsonian visit counts  Dorm students CityTownInfo, DC public access - online Data Catalog  (a) Population data sources.

Infrastructure Data sources Transportation Navteq,  WMATA Communication TowerMaps,  Sprint & AT&T API, CDC TUS  Power Pepco, DC govt, Google Earth, PSSE (Siemens)  Healthcare Dun & Bradstreet, National Registry of Hospitals  (b) Infrastructure data sources  there will be extensive rubble and damage to streets. Hospitals close to ground zero may operate at reduced capacity, though it may be possible to bring in mobile healthcare stations to do triage and administer first aid.



IV. SIMULATION DESIGN  There are two important high-level aspects to the simulation design. First, obtaining and combining all the data necessary to set up initial conditions and conduct the simulations is an important, and difficult, problem. Second, the design of the simulation architecture should be such as to facilitate scaling and analytics. We address these aspects in turn.

A. Synthetic Information  The methodology for generating synthetic information orig- inates with the TRANSIMS project [9] and has been developed and refined in our lab over multiple years and projects.

Synthetic population: We generate a synthetic population for the Washington DC Metro Area by combining information from multiple sources, as listed in Table IIa. The process for generating the population consists of the following major steps: 1. Generating Synthetic Population: This step disaggregates the data from the American Community Survey using iterative proportional fitting to create a set of synthetic individuals grouped into households [10]. The generated synthetic pop- ulation returns the same marginal distributions as given in the ACS data, and also preserves anonymity of individuals because the generated synthetic individuals do not correspond exactly to any real individuals.

2. Locating Households: This step locates each synthetic household into a housing location using data from the ACS and street data from Navteq.

3. Assigning Activities: In this step, each individual is assigned a set of activities to perform during a day. Various activity templates are created by using the National Household and Travel Survey (NHTS) and the National Center for Education Statistics (NCES). Each synthetic household is matched to a survey household based on its demographics and individuals in the synthetic household are assigned the corresponding activities.

4. Locating Activities: An appropriate location (essentially a building) is chosen for each activity for each individual using a gravity model and Dun & Bradstreet location data.

5. Sublocation modeling and constructing social contact net- work: Each location is subdivided into sublocations (similar to rooms within a building). A person is assumed to come in contact with all people present at the same sublocation at the same time, which thus induces a social contact network.

In addition, for this simulation, we generated two more synthetic populations: a population of transients, including tourists and business travelers, and a population of college students who live in dormitories on university campuses in the area. Details of the construction of all three populations can be found in [11].

Synthetic infrastructures: We develop synthetic information models of four major infrastructures: transportation, commu- nication, power, and health. Data sources used in this process are detailed in Table IIb.

The transportation infrastructure model represents automo- bile, metro, bus, and walk networks. Links are weighted based on expected damage and debris. Routing is done using a regular expression-constrained version of Dijstra?s algorithm, which allows taking travel mode into account. The model also accounts for group travel, crowd-following behavior, conges- tion, and ambient traffic (to account for external travelers not represented in the synthetic population). More details can be found in [12].

The communication infrastructure model represents cell tower locations and capacities. Each tower is assigned a coverage area based on its Voronoi cell. As mentioned earlier, cell towers go down close to ground zero if they are in the power outage area. Coverage can be partially restored in the annular region from 0.6 to 1 mile by bringing on Cells on Wheels. We also model higher priority for 911 calls, higher clearing rate for text messages, and the sending of emergency broadcasts advising people to shelter in place. More details can be found in [13].

The power infrastructure model is static in that its status is not updated over the course of the simulation. This is because damage to the power infrastructure is not expected to be repairable in the short term. There is expected to be complete power outage within 0.6 miles of ground zero, and     Fig. 2. The simulation architecture.

partial power outage out to 1 mile.

The healthcare infrastructure model represents hospital ca-  pacities and mobile healthcare units that can provide triage and first aid. Individual health is represented on a scale from 7 (perfect health) to 0 (death). A healthstate of 4 or below represents moderate injuries or worse. More details of this model can be found in [14].

Agent behavior: The behavior model is defined using a decentralized Markov Decision Process, using the framework of options [15]. In this model, individuals behaviors are de- fined as high-level ?options?, which are policies for choosing low-level ?actions? based on the state of the agent and the environment. We restrict actions to be to move (a destination has to be specified) and to call (a callee has to be specified).

The set of high-level behaviors represented are chosen based on evidence from the literature on human behavior in disasters [16], [17], [18].

Six behaviors are represented: household reconstitution, where individuals try to call and move towards their household members, evacuation, where individuals try to make their way out of the DSA, shelter-seeking, where individuals try to take shelter in the closest intact building, healthcare-seeking, where individuals try to move towards hospitals, panic, where individuals run outside, call 911, or try to move towards hospitals even if they are unhurt, and aid & assist, where individuals try to transport people who are hurt to hospitals.

An option specifies a initiation and termination condition for a behavior, as well as the action selection policy. We assume some high-level constraints on option selection, e.g., we assume that panic levels will drop after ?3 hours after the event, and that receiving an emergency broadcast advising people to shelter in place also has the effect of ending panic.

We also assume that small children will only shelter in place unless they are traveling with an adult, and that household members travel as a group once they meet. A more detailed description of the behavior module is given elsewhere [19].

B. Simulation Architecture  Computationally, the architecture of the simulation is di- vided into modules, which interact through the database. This  TABLE II RUNNING TIMES OF INDIVIDUAL MODULES  Module Wall Time Compute Time  Transportation 13.75 hrs 8911 hrs 648 cores  Behavior 3.92 hrs 397 hrs 96 cores  Communication 9.53 hrs 9.53 hrs  Health 4.3 hrs 4.3 hrs  Infrastructure 1.4 hrs 1.4 hrs  is fundamentally different from the standard object-oriented notion of agents that interact through message passing. In our simulation, entities corresponding to agents exist, but are distributed across multiple database tables. We refer to this distributed representation (which could also be done with files, e.g.) as ?unencapsulated agency?. The architecture is shown schematically in Figure 2.

There are multiple benefits to this organization. One of the main benefits is that the modules are independent of each other, given the database schemata. In other words, once the underlying shared representation is fixed, modules can be developed and optimized independently and in parallel at design time. This greatly speeds up the design process.

During execution, modules run in sequence, though some pipelining is possible, and transform the data for the current iteration. This model, where the state of the simulation is maintained in the database and modules act as operators which transform the data, is very scalable because each module can exploit its own inherent parallelism (see Table II). Further, different modules only have to update subsets of the agent information, e.g., the transportation module only needs to update the agent locations. In an object-oriented agent design, this still requires each module to access each agent. This makes parallelization difficult because different agents would be distributed across different compute nodes, resulting in a lot of message passing. In our unencapsulated design, the state of the agents is maintained entirely in the database, and the computation for each module is distributed across compute nodes, which results in more efficient computation.

This design is also different from the standard transactional view of database-supported operations [20], since each module operates upon large fractions of the data and transforms them, rather than small independent transactions. Thus the simulation is really driven by the database, whereas in the transactional view, the database serves to record activities (like sales).

It should be noted however, that our implementation is not as general as standard agent-based simulation platforms. In both approaches, re-design is needed when a new simulation (for a new problem) has to be created, and changes required may arguably be more extensive in our approach than in a general-purpose simulation platform.

However, the synthetic information generated for a particu- lar simulation is re-usable for different classes of simulations.

For example, a synthetic population of a region that is used for simulating the outbreak of an influenza-like illness can be re-used not just for other kinds of epidemics, but also for     (a) Synthetic information setup. (b) Synthetic information dataflow  Data Initial Dynamic (1 run) Complete Design 2M individuals (20 runs, 30 replicates) (2 weeks, full design)  Database 3.55 GB 27 GB 25TB 250TB Disk 1.16 GB 15 GB 20TB 175TB  (c) Storage requirements for the simulation  Fig. 3. The setup of the simulation using synthetic information generated from multiple data sources is shown on the left. The simulation updates and transforms the synthetic information over the course of the simulation. This process is depicted on the right.

disaster simulations, since the synthetic population represents properties which are essential for both kinds of problems domains, such as household structure and activity locations.

Thus the design vs. execution trade-off between our approach and general purpose simulation platforms is not clear-cut.

The simulation consists of iterations that model ten minute intervals during the first hour of the detonation, and thirty minute intervals after that. It is necessary to model shorter intervals early on because conditions such as radiation levels change rapidly after the detonation.

The synthetic information setup, and the order of operation of the modules in each iteration are illustrated in Figure 3.

First the communication module updates base station status, which affects availability of cellphone communication. Then the infrastructure module updates radiation levels and fallout cloud coverage. These two steps are independent of each other and can be run in parallel also.

After that the behavior module runs to update the behavioral state of each synthetic person. This requires looking at many variables including the person?s location, their knowledge of their family members? health status, the availability of cellphone coverage, whether they have received an emergency broadcast, the time since the detonation, their own health state, and so on. Based on these variables, the behavior module sets a high-level behavior and a low-level action (move to a particular location, call family members and/or 911) to be attempted.

Then the transportation module runs to route those people who are attempting to move, and also moves the person along the route. The distance covered during the iteration depends on factors like damage to the roads, the availability of vehicles, and the person?s health state.

After that the communication module runs again to deter- mine which of the attempted calls are able to be completed. It also determines which individuals receive emergency broad- casts during this time period.

Finally the health module runs to update the health state of individuals. It takes into account exposure to radiation, depending on whether the person is indoors or outdoors, and the radiation levels at the locations they have traversed in this iteration. It also maintains queues at hospitals and other healthcare locations, inducts people into the healthcare system, calculates changes in health state due to treatment received, and releases people who are healthy.

At this point the iteration ends, and new tables are created for the next iteration. The space requirements for the simula- tion are summarized in Table 3c.



V. EXPERIMENTS AND ANALYSIS  We have done a number of experiments that are detailed elsewhere [19], [13], [21]. Here we present some new results that focus on one particular aspect: the complex spatio- temporal patterning of radiation exposure of individuals, based on behavioral choices. For example, in Figure 4, we plot the probability of surviving to 48 hours with minor injuries or better, given initial location. We see that the 0.6 mile region has very low values of this probability, as expected, but there are other spatial patterns as well. For instance, to the west and northwest of ground zero, nearby locations have very different probabilities.

Exposure levels depend not just on initial locations, how- ever. They depend on behavioral choices made by individuals in a complex way. Here we focus on analyzing the effect of     Fig. 4. The probability of surviving to 48 hours with healthstate > 4 (minor injuries or better), as a function of initial location.

TABLE III MEAN EXPOSURE BY BEHAVIOR.

HRO Evacuation Shelter Healthcare Panic Aid & Assist 267 211 267 378 369 204  selecting different behavioral options early in the simulation on eventual total radiation exposure. We sub-select all of the individuals that are within 1 mile ground zero, about 260,000 agents for more detailed study. As the response variable, we have selected the radiation of exposure of each agent after 50 iterations of the simulation. All analyses in this section model the dependent variable conditional on the individual?s behavior at iteration 3 (15 minutes after the blast) and other covariates.

We run an ANOVA on the dependent variable, using behavioral option in iteration 3 as the treatment and find a highly significant result (? < 2?16). That is, the mean response varies by behavior. What is perhaps most surprising is which behavioral options result in the best radiation outcomes.

Table III shows the mean radiation outcome by behavior. This contradicts conventional wisdom that sheltering in place is the best strategy. Under this simple analysis, those who did Aid & Assist tended on average to have the best outcomes. This result is completely counter-intuitive, as those aiding and assisting others are typically out in the open, exposing themselves to higher levels of radiation.

We add an additional layer to the analysis and condition on distance from ground zero. This accounts for the fact that individuals who are closer to ground zero are more likely to perform some behaviors more than others. We run an ordinary least squares regression, using both distance from ground zero and indicator variables for behavioral option as covariates and interactions between these variables. As expected, both distance and behavioral option are highly significant. We then use the regression model to predict the expected exposure for each behavioral type at the same distance from ground zero.

We use the mean distance from ground zero as our point of comparison, though any distance would result in the same  TABLE IV EXPECTED EXPOSURE BY BEHAVIOR AT MEAN DISTANCE FROM GROUND  ZERO.

HRO Evacuation Shelter Healthcare Panic Aid & Assist 488 480 356 407 432 504  ordering. The results are shown in Table IV. Having controlled for distance from ground zero, sheltering appears to be the best option, as expected.

Traditional statistical methods, like those we have imple- mented so far, focus on detecting differences in the mean response. While these methods have proven incredibly useful, they do not reveal the whole story if the data (or residuals) are not normally distributed. In our case, eventual radiation exposure is far from Gaussian (see left panel of Figure 5).

Further, even if we disaggregate by early behavioral choice, we still find a long right tail and that the distributions of exposure only vary dramatically at the lower quantiles. This is evident in the right panel of Figure 5, which shows the empirical cumulative distribution function (CDF) of radiation exposure for each early behavioral option separately. An empirical CDF shows the proportion of the data that falls at or below each value on the horizontal axis? around 70% of individuals performing aid and assist had exposures less than 200 mGy, whereas less than 10% of individuals seeking healthcare had such low exposure.

Fig. 5. (left) Histogram of radiation exposures at the 50th iteration. (right) Empirical CDFs of radiation exposure by behavioral option in 3rd iteration.

From a practical point of view, the mean radiation exposure may not be the quantity of interest. Beyond some upper threshold, the exposure no longer matters? the person will die. Below some threshold, it at least does not matter acutely.

For this reason, we repeat the same analysis as above using quantile regression, which fits the conditional ?th quantile, rather than ordinary least squares regression, which fits the conditional mean. Quantile regression[22] estimates the model ??(? ???) = ???, where ??(? ? ??) is the ?th conditional quantile of ? given a vector of covariates, ? , and quantile regression coefficients ??. This can similarly be re-expressed as ? = ???+?? where, ?? is an error term that comes from a distribution that has 0 as the ?th quantile. ?? is then interpreted as the expected increase in the ?th quantile of the conditional distribution of ? given a one unit change in ? .

We fit a quantile regression using the quantreg package [23] in the R computing environment[24]. As explanatory covariates, we again include indicator variables for whether each individual was performing each possible behavioral op- tion in iteration 3. We interact these dummy variables with the distance from ground zero to obtain a separate regression coefficient on distance by behavioral option. We estimate CDFs for each behavioral option again at the mean distance from ground zero using the output of quantile regressions fit at quantiles 0.01? 0.99 in increments of 0.01. This is shown in Figure 6. We see that the ?th quantile of the distribution of radiation for those sheltering is less than that of all other options up until around the 97th percentile. For some unlucky 3% of those taking shelter, they would have been better off evacuating or performing aid & assist even if they had been among the 3% most irradiated under those behavioral options as well. That said, because the vast majority of the distribution is lower for those taking shelter, this analysis still indicates sheltering as the most successful behavioral option.

Fig. 6. Empirical CDFs of radiation exposure (centi-Grays) by behavioral option at the mean distance from ground zero.

The last analysis we undertake is a spatial quantile regres- sion. Although we have controlled for distance, the models we have fit assume a linear trend in distance. Moreover, all points the same distance from ground zero have the same fitted quantile. We instead fit the model ? = ?? + ??(?), where ??(?) is the asymmetric Laplace process as defined in [25].

Essentially, ??(?) is a stochastic process for which, marginally, zero is the ?th quantile, and spatial correlations are maintained.

This model allows us to create a predictive spatial quantile surface across the study space. We compare the 0.5, and 0.95 quantile surfaces for the sheltering and evacuation behaviors.

Figure 7 shows the quantile surface for sheltering minus the  quantile surface for evacuation. In those areas in which this difference is positive, the ?th quantile of sheltering is larger.

Negative areas indicate that the ?th quantile of the distribution of exposure for those who evacuate is higher. Interestingly, it appears that sheltering results in much higher exposures for those individuals very near to the blast across both quantiles tested; this indicates that those who will be receiving very high levels of radiation even when sheltering are better off trying to evacuate. The area for which evacuation is recommended is reduced at the 0.95 quantile; perhaps those that try to evacuate and get delayed or stuck in traffic end up with higher exposures than those that shelter and are at the higher end of the distribution for their location. In practice, those so near to the blast are probably not going to survive anyway, so their behavioral option will not make much difference in terms of the ultimate outcome? survival.

Fig. 7. Difference between the quantile surfaces for sheltering and evacuation at ? = 0.5, and ? = 0.95. Units are in centi-Grays.



VI. CONCLUSION  We have detailed a ?big simulation? of a particular urban disaster scenario using synthetic information methodology, a novel database-driven simulation architecture, and a novel analytic technique.

All three contributions apply more generally to large-scale social simulations. From a big data perspective, there are a couple of important points to be made here.

One, the synthetic information methodology of combining multiple structured and unstructured data sources into a coher- ent model is a fundamental aspect of the ?big data? problem. It is not an easy problem to solve, but the benefits are potentially enormous because it enables deep prescriptive analytics [26], [27] through simulation. A key insight in this methodology is to treat procedures as data too. For example, the way a cell tower works can be specified algorithmically and becomes a part of the synthetic information structure.

Two, large-scale high-resolution data representations em- body a large number of constraints. Running a simulation effectively activates those constraints and provides a view of the interactions between the data in a way that cannot be obtained simply by doing analytics on the static data. However, it also doubles the analytic burden since analytics need to be done both on the input and output of the simulation. This is potentially a small price to pay for the knowledge gained.

Three, cities, and social systems in general, are complex systems that we are just beginning to understand [28]. Done right, the age of big data promises vast improvements in policy and preparedness. However, it is well acknowledged that the crucial issue is not generating or collecting the data, but figuring out what to do with them. This is where big simulations offer a solution. Carefully constructed simulations, combined with high-performance computing infrastructure, deep analytics, and effective interface design can lead to better policy design and resilient infrastructures and populations.

