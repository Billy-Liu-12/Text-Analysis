Proceedings of the 2000 IEEE/RSJ

Abstract In this paper, an absolute localization paradigm based on  the cooperation of an omnidirectional vision system composed of a conical mirror and a CCD camera and a low cost panoramic range finder system is reported. These two sensors, which have been used independently until now, provide some complementary data. This association enables us to build a robust sensorial model which integrates an important number of significant primitives.

We can thus realize an absolute localization of the mobile robot in particular configurations, like symmetric environments, where it is not possible to determine the position with the use of only one of the two sensors. In a first part, we present our global perception system. In a second part, we describe our sensorial model building approach. Finally we present an absolute localization method which uses three matching criteriafused thanks to the combination rules of the Dempster-Shafer theory. The basic probability assignment got for each primitive matching enables to estimate the reliability of the localization. We test our global absolute localization system on several robot's elementary moves in an indoor and symmetric environment.

1 Introduction Autonomous mobile robots cannot rely solely on dead-  reckoning to determine their configuration because dead- reckoning errors are cumulative. That's why they must use exteroceptive sensors that get information from the environment in order to estimate the robot's location more accurately. This leads to a classical localization method based on the fusion of dead reckoning data and exteroceptive data. The fusion method generally used is based on the extended kalman filter (EKF). The perception systems used both with the dead reckoning can be of different natures: a goniometer [3], the SYCLOP system [4], a laser range scanner [2].

Another approach consists in using only exteroceptive data: the robot's configuration is calculated in the environment reference without using previous information.

To answer to this problem, two strategies are generally used. The first consists in marking the robot's evolution world with artificial beacons [5]. The second one consists in using the intrinsic features of the environment (doors, edges, comers.. .)[4] [ 13.

Artificial beacons can be detected fast and reliably and provide accurate positioning information with minimal  processing. This kind of system is generally employed for industrial applications [ 101. Unfortunately, these methods lack flexibility and modularity because it is necessary to fit out the robot's evolution environment.

The other solution consists in referencing on environment characteristic elements and offers a great modularity because the robot can localize itself directly in accordance with the landmarks. This kind of localization is founded on a matching stage between a sensorial model and a theoretical map of the environment. Perez in [6] determines with a panoramic laser range finder the absolute position of its robot. Similarly Yagi uses an omnidirectional vision system to develop navigation and environment map building methods [l]. We can notice that the robustness of these methods is mainly linked to the matching stage. The more precise and rich information the sensorial model will give, the more robust the matching stage will be.

That is why we have worked on a localization approach based on the cooperation of two omnidirectional perception systems: the vision system SYCLOP and a low cost range finder system. The association of these two kinds of complementary information permits to generate a sensorial model with a high descriptive level. The first part of this paper presents the global omnidirectional perception system. The second part deals with the sensorial model building method. Our absolute localization method, based on a Dempster-Shafer multicriteria fusion approach, will be presented in the last part. In the conclusion we will analyze the experimental results reached with our mobile robot SARAH.

2 The global omnidirectional perception system We use an original perception system making cooperate  two omnidirectional sensors: an omnidirectional vision system (SYCLOP) [4] and a low cost and fast panoramic range finder system (Figure 1).

T C  transm lsslon  glass 7w %$era support  Figure I :  The global perception system  These two sensors have been developed and used independently within our laboratory [4] [9]. The rotation  0-7 803-6348-5/00/$10 .OO 02000 IEEE.

- 1499 -  http://iut.u-Dicardie.fr   axis of the laser is in line with the center of the conic reflector. This geometric constraint is taken into account at the time of a previous phase of calibration.

The range finder system is an active vision sensor [9].

This method consists in projecting on the scene a visible light with known pattem geometry (a laser spot in our case). A camera images the illuminated scene with a given parallax. The desired 3D-information can be deduced from the position of the imaged laser point and the lateral distance between the projector and the camera. This perception system allows to obtain an omnidirectional range finding sensorial model. The kind of primitives is the same that a classical range finder laser. The interest of this system is on the one hand its low cost and on the other hand its rapidity. The effective measurable distance region is designated as 0.6m-5m: this distance is thought to be a sufficient distance for a mobile robot to detect obstacles and maneuver around them.

The SYCLOP system, similar to the COPIS one [l], is composed of a conic mirror and a CCD camera. It allows to detect all the vertical landmarks of the environment thanks to a two dimensional  radial straight  /T7gr b ,  / Figure 2: Principle of the omnidirectional sensor SYCLOP  The vertical landmarks are characterized by radial straight lines which are extracted with a treatment based on the Sobel gradient. We can note that we work in fairly constraint environments, which not generate an excessive number of detected landmarks.

This two omnidirectional sensors association permits to manage some complementary and redundant information within the same sensorial model. With the SYCLOP system we exploit the radial straight lines which characterize angles of every vertical object (doors, comers, edges, radiators). With this system, the information of depth cannot be gotten on an unique acquisition: it is not possible to differentiate with this only sensor use the notion of opening (corridor, opening of door ....) and the notion of vertical object (closed door, radiator,. . .) (Figure 31t is therefore interesting to use a sensor providing some complementary information. Then we have associated to SYCLOP an inexpensive range finding sensor capable to  be fast. Following a segmentation stage [9], this sensor permits us to exploit sensorial primitives that are segments (Figure 3). In this case we have the notion of depth, but it is impossible to differentiate two vertical objects placed in the same alignment: for example two closed doors placed on the same wall (Figure 3). It misses the notion of angle that will be provided by the SYCLOP system.

doors I ,yimnment I  I sensorial s~gments dmined wim me cooperation of the I w  ormidrenional Sensors I  Figure 3: Principle of the omnidirectional sensorial cooperation.

Finally this cooperative approach permits to construct a sensorial model whose descriptive level is high. This descriptive level is superior to the one obtained with each sensor individually. Moreover with an appropriated management of the redundant data (separation between two segments for the range finder and radial straight line for the SYCLOP system) we can compensate a sensorial information absence on one of this two sensors (Figure 3).

3 Sensorial Model construction The sensorial model of the evolution world is based on  the taking into account of two types of data (Figure 3): the vertical landmarks angles and the segments characterizing walls. Segments are managed with two points whose coordinates are expressed in the robot?s reference. The managed primitive in the final sensorial model will be segments. These segments will be determined with two types of approaches : > An approach based on the data complementarity: this treatment consists in cutting up segments gotten with the range finder in subsegments (Figure 4). The carving is realized with the radial straight lines of the vision system.

> An approach based on the data redundancy: the redundant aspect is characterized by the detection of a vertical landmark with the two sensors (Figure 4). In certain cases a vertical landmark is detected by the range finder with the end points of segments. We will be able to  - 1500 -    confirm the existence of a segment extremity if a radial straight line corresponds to it. In case of-radial straight line absence we will keep the segmentation obtained with the range finding sensorial model.

final segments end point segment Y.. (mmDlementaw data)  & SENSORIAL MODEL  Figure 5: Principle of the global sensorial model building algorithm  From the SYCLOP image, we treat the radial lines with a  transform. We fixed the threshold detection of a radial line (number of pixels composing a radial line ) to an important  The fusion step, described on Figure 3 and Figure 4, is  segmentation algorithm based on a simplified Hough  value in order to keep the significant radial primitives.

based on the taking into account of three cases :  this case we take as hypothesis to use the radial line 9 The treatment of redundant data (case 1 of Figure 4). In  - .

missing data (case 3)  range finding segment data  (case 1)  /g- 2 Radial line Rangelinding 7 Robot  seg- refersnce case 1 case 2  Radial line  i w w m n  Rango finding segments  R.npe" "  case 3 case 4  reflector reference  Figure 4: The Different cases of the cooperation algorithm.

We have integrated these different cases of cooperation in the sensorial model building algorithm shown on Figure 5 .

The first step consists in extracting line segments from the set of points given by the sensor. We use the recursive Duda-Hart segmentation algorithm [7] [9]. To decrease the noise sensitivity of this algorithm we have added a pre- processing stage on the set of points in order to eliminate the aberrant points.

segment into several final subsegments. This stage is based on the segment intersection determination.

9 The treatment of missing data (case 3 of Figure 4). The notion of missing data is here characterized by a vertical landmark which is not detected with the vision sensor. In this case the range finding breakpoint is considered directly.

During this stage, we classify the segments and subsegments we get according to their detection by one sensor or by the two sensors. This ordering will enable us to ponder all the segments and subsegments according to their probability of existence (for example, if a segment is detected by the two sensors, it has a strong probability of existence) and we will use these computed weights in the matching stage described further.

9 The two extremities of a segment are detected only by the laser range finder (segment S1 in the case 1 of Figure 6). This segment has a weight of 1.

9 One extremity of a segment is detected by the laser range finder and the other extremity is detected only by the conic mirror (segment S1 in the case 2 of Figure 6). This  The different cases are:  - 1501 -    number of range finding points equal to 0 and a length (Cartesian distance) inferior to a predetermined threshold.

This stage permits to decrease the combinatory aspect of the matching stage and to increase the robustness.

This building algorithm enables to get a sensorial model where the number of exploitable primitives is more important than the number of primitives got by each sensor when it works individually. Besides, we can obtain by cooperation a certainty information on the presence of a segment when this segment is detected. by the two sensors.

This information will be used in the matching phase.

4 Absolute localization method The robot configuration is determined by matching the  sensorial model, got by multisensor cooperation, with a theoretical map of the environment. The primitives used for this matching phase are segments. Therefore, all environment's elements like doors, walls, windows,  For each segment, we have considered three correspon- dence tests, which are similar to these used by Crowley [7]: U the angular difference between the two segments, Q the difference in length between the two segments, 0 the distance between the centers of the two segments.

radiators.. . are indexed as segments in the theoretical map.

Figure 7: The three matching crireria.

The fusion of these three treatments is made thanks to the combination rules of the Dempster-Shafer theory [8]. ;iK ...... :;;;;:  I . . . . . . .  & . . . . . . . . . . .

. .  . . , .  . . . .

..a, . . . . . . . . . . . .

r - - - - - + OJ . . . . . . . . . .  : I I . . .  . . . . . . . . .  4  _ . .

0 '  0 s  +&PA d IB B't BPAd W] L+BPA d IB B'tBPAd Nl]  . .  . .

0 W  n 0 ; d m b * O k 0 m d k l D -  I - C B  PA d YES .I- B P.A dbU  Figure 8: basic probabiliry assignments of matching criteria  Our frame of discernment is composed of two elements : YES and NO corresponding to those assertions : "Yes, we can match the two segments" and "No, we can not match  the two segments". For each criterion, we have determined the Basic Probability Assignments (B.P.A.) ml, m2 , m3 shown Figure 8.

We can then perform the combination calculation thanks to the Dempster-Shafer rules [8] which allows to calculate the conflict coefficient k between the three elements of the frame of discernment. If k<l ,  the conflict is not complete and the combination of belief functions for each a element of the frame of discernment is given by:  The segments are matched if B.P.A. for the YES is superior to the B.P.A. for the NO.

The first stage of this localization algorithm consists in determining a list of sensorial segments Ls which have a strong probability of existence. The strong probability notion is managed with the two cases 1 and 2 of the Figure 4: when segment endpoints are detected by the two sensors.

We consider that the length of these segments has been determined with a good accuracy. So, our starting correspondence test is the length of a segment.

In the second stage, we consider a segment Lsk from the list Ls and we search the theoretical map segments which length is similar to the L s k  segment length. Each found theoretical segment is superposed on the sensorial segment Lsk and we apply the third step in order to test the correspondence of the other sensorial segments.

The third step consists in applying the three criteria describe above on all the segments on the list Ls except the segment Lsk. A segment is matched if the B.P.A. for the YES is superior to the B.P.A. for the NO. To choice the optimal matching solution we calculate a V criteria. For each matched segment pair, we increment this V coefficient which characterizes the robustness of the global matching.

Vis managed with the following algorithm: Given : - B the B . P . A .  fo r  the YES of the matched  segment pair - w the weight of the sensed matched segment  computed in the sensorial model building phase.

For each global matching v= 0 for each segment matched  end V = V + (B*W)  end So we can see that V is an interesting indicator of the  global matching relevance since V takes into account the probability of existence of each matched segment (more high the segment's weight is, more high the probability is) and the quality of each matched pair (through the B.P.A.

for the YES).

- 1502 -    These three steps are then repeated for all the Ls list segments. The final solution is the one which permits the maximal V.

-2000  5 Experimental results To test the robustness of our localization algorithm, we  have performed it on several sensorial acquisitions made in an indoor environment (Figure 9). The two omnidirectional acquisitions are made when the robot is stopped. The omnidirectional acquisitions and the localization algorithm are computed in a Pentium PC located on our mobile robot.

A Matrox Meteor I1 video card is used to acquire the omnidirectionnal image and the laser acquisition. Our  \ - cooperative sensorial model  expel '1 mental DerceD ition syst  Figure 9: Our global omnidirectional perception system and the experimental indoor environment.

In order to show the interest of our cooperative approach, we have tested our localization method on symmetric environments (the first picture on Figure 9). The use of one sensor individually instead of the two sensors emphasizes the robustness problem: a strong failure rate has been observed for the matching phase when we use only one sensor [9]. The first environment is a long corridor (length: 50 meters). Figure 10 shows a sensorial model got with our cooperative approach. The robot is located in the middle of the corridor (Figure 9). We can see on Figure 10 the final decomposition on an set of segments which represent doors and parts of wall. We show on this figure the radial straight lines obtained with the omnidirectional conic mirror. We must note that, for this environment, the depth sensor would not have been able to localize the robot: two parallel identical segments would have been detected. The SYCLOP system used alone would have posed the problem of environment symmetry.

The robot final position successfully obtained show the  robustness of our method and its accuracy. We have indeed a position error of 8cm and an orientation error of 3 degree.

. ;. ?..... ......

I  final detected segments in  / I  the robot's  ' \ ' -4000 -3000 -2000 -1000 0 1000 2000 3000 4000  OI"  theontical  robot's + posinon  2000 after the matching stage  0 r r 2  - symmetric environment: a  / laboratory square hall.

55 ,N  position after the matching  2 0  1 1  m,m*  0 la 2a 10 4 0  Figure 1 1 :  cooperative sensorial model and f i n d  position determination in a hull environment.

The same remarks can be done: the use of the two sensors provides enough sensorial information to enable the matching algorithm to converge to a coherent solution.

The third environment is the end of the corridor shown Figure 9. This environment constitutes a favorable experimental configuration: it is not symmetric and it has an important number of exploitable landmarks (figure IO).

-1503-    We can note here on several robot?s configuration determination that our matching selection criteria is highly discriminative: the good configuration has been computed on all the acquisitions.

?)I 2000.

1000.

0 -  -1000.

-2OW -  I \r, ............................ 1 I.... .._.. I-?? ,y-/ -. 4 ?  ............_.....

ctwperative  model 1 , ~, , .)  0 ??- .....___ ,.- --  6ooo -4MK) -xxM 0 2oM) 4wo 6wo (?)  Figure 12: cooperative sensorial model (first figure) and final position determination.

Finally on a complete path makes in the corridor by our robot mobile SARAH, we could note on 40 acquisitions that, on the one hand, all the absolute configurations have been determined correctly, and, on the other hand, the mean error was equal to 11 cm in position and 3 degree in orientation. In spite of an important combinatory aspect, our cooperative localization method proves to be robust and particularly accurate.

6 Conclusion We have presented in this study an absolute localization  approach based on the cooperation between two omnidirectional sensors: an omnidirectionnal vision sensor and a range finding sensor. This association allows to treat two types of complementary data. Then we obtain a highly descriptive sensorial model which integrates an important number of primitives and enables to increase the robustness of the matching stage. The absolute localization paradigm based on this matching stage takes into account several criteria which are merged with the Dempster Shafer rules.

The choice of the optimal matching is based on a highly  discriminative criteria which associates a segment weight and a B.P.A. linked to the matching stage. We have tested our cooperative absolute localization algorithm on several particular environment like for example symmetrical environment. On the one hand, we can note on these experimental results that the robot?s configuration determination is realized in a unique way and on the other hand the absolute robot?s configuration is calculated with a relatively weak systematic error.

