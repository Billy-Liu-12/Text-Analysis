Most Clusters can be Retrieved with Short Disjunctive Queries

Abstract?Simple keyword based searches are ubiquitous in today?s internet age. It is hard to imagine an information system today that does not permit a simple keyword based search. This method of information retrieval has the obvious benefits of being highly interpretable, and having wide usage. However, a general perception is that keyword search may not be as powerful an information retrieval paradigm as those that utilize data mining technologies.

At the same time, the tremendous growth in textual informa- tion in various domains has also given impetus to data mining technologies such as document clustering. Document clustering is a powerful technique, having wide applications in enterprise information management (EIM). However, there is a general perception that the clusters it produces are not always easily interpretable. This hampers its usage in certain settings.

This leads us to the following question: can we retrieve a cluster (from a corpus) using a keyword search with precision and recall that are reasonable from the point of view of a retrieval system? What is the form of such a keyword search? How many keywords do we require? How do we arrive at these keywords?

Not only are these questions natural, they have immediate use in several highly regulated applications in EIM such as eDiscov- ery and compliance, where document sets must be specified using keywords.

In order to answer our question, we construct a framework that uses maximal frequent discriminative itemsets. The novelty of our usage of these itemsets is that although their definition as frequent itemsets is conjunctive, we use them to form a disjunctive query upon the corpus. We then study the results of this query as an information retrieval problem whose target is the cluster.

Our study yields a surprising result: most clusters can be re- trieved, up to reasonable precision and recall, using a disjunctive query of only three terms. Among other ramifications, this gives us a readily interpretable description of a cluster in terms of the disjunctive query that returns it.



I. INTRODUCTION The past decade has witnessed a remarkable growth in the  amount of unstructured textual information from sources such as the world wide web, enterprises, newsfeeds, email, digital li- braries, medical records, and many others. The de-facto means of retrieving information from these unstructured corpora is keyword search. In the internet era, making keyword searches to the web is second nature to most computer users. This carries over to the other sources such as enterprise intranets mentioned above. Keyword search has obvious advantages: interpretability and pervasiveness being perhaps the two key ones.

At the same time, enterprises are looking to data min- ing and machine learning to provide tools to analyze this unstructured information. Document clustering is one of the foremost among such techniques. In EIM, which formed the specific motivation for our research, document clustering can be used at various stages: to understand high-level organization of data [3, 10, 12, 16], to organize search results [7], to extract semantic information such as labels [5], and so on.

The application of text analytics technologies in general, and clustering in particular, to enterprise unstructured information management has significant commercial potential1.

1Clustering technologies are a part of ECM, which was valued at $4.7B in 2012, and is growing at 10% as per Gartner.

However, due to the pervasiveness of the paradigm of keyword search, there are several applications of commer- cial importance in EIM where document collections must be specified by means of keyword search. One example is eDiscovery2, where courts frequently insist that the document collections forwarded for further consideration in response to a litigation must be specified in terms of a keyword search that was made upon the internal documents of the concerned organization. For example, in a patent litigation on printer inks, the court may insist upon all documents that contain any of the keywords ?printer?, ?ink?, ?cartridge?, among many others.

This also means that any documents obtained by clustering the corpus that are to be added to this collection must also be subsumed into a keyword-based description.

The ubiquitousness of keyword searches on the one hand, and the power of data mining on the other, together lead to a natural question: can we describe a cluster in a corpus by means of a keyword search upon the corpus? The interaction between retrieval and clustering has been studied heavily.

However, to our knowledge, all of that study is in one direction only: namely, can retrieval for query sets be improved using clustering. In other words, the target is retrieval, and clustering is a tool to get there. In this work, we reverse the arrow. We ask the question ?can we retrieve a cluster by means of keyword searches?? One way to think about this question is as follows: suppose we do not have access to clustering, can we somehow ?replicate? its functionality through a reasonably constructed keyword search? Or do we miss something irrecoverable if we do not use clustering?

This problem statement is broad, and permits many research questions to be asked. We cast the problem into the precise framework of information retrieval and ask the following research question.

Consider a corpus D that has been clustered into a K-way clustering. Let C be one of the K clusters that is chosen according to some information need.

Can we describe C in terms of a keyword search Q: namely, when the query Q is made on D, the result set has precision p and recall q with respect to the target for our retrieval?the cluster C. Furthermore, p, q must be reasonable from the point of view of a retrieval system.

At first, there seems to be an argument against an affirmative answer. Namely, that whereas clustering is based upon inter- document similarity information, a simple keyword search is oblivious to such information.

However, if the answer to the research question is affirma- tive, then three natural related questions to be asked are the following.

What is the form of Q? How many keywords does Q comprise of? How do we arrive at these keywords?

We conduct a broad exploratory study in order to answer the above questions. We build the following retrieval system.

2$1.4B software market in 2012, expected to double to $2.9B by 2017(Gart- ner).

DOI 10.1109/ICDM.2013.94     For each cluster C, we extract frequently co-occurring sets of terms (that is, frequent itemsets) from the documents in C. Such terms are, by definition, conjunctions. Now, we use these terms in a disjunctive query Q. The corpus D is queried with Q. The result of this query is measured for precision and recall relative to the cluster C. We vary K over a wide range from 5 to 100.

We then study the performance of this retrieval system on two text corpora of varying origins (usenet postings and newsfeeds). Based upon our study, we arrive at a surprising result: a majority of clusters can be retrieved in the manner above using a disjunctive query of only three terms, across the range of K for the usenet dataset, and up to mid-range of K for the newsfeeds, at precision p = 0.1 and recall q = 0.7.

This result is surprising for two reasons: 1) Frequent itemsets by definition are conjunctions. That  they can be used disjunctively in order to retrieve their cluster is remarkable.

2) The number of terms in the query is extremely low considering that clusters are supposed to match documents based on similarity over a considerably high dimension.

We end this section with an outline of our main contribu- tions.

1) Research Question: Our first contribution is to introduce and motivate a basic research question that also arises in real- world applications.

2) Structural Study: We expand the structural study of document clustering by subjecting clusters to retrieval using their frequent discriminative itemsets, but used in the form of disjunctive queries. Specifically, our techniques and study yield a characterization of each cluster based on the size of the disjunctive queries that are required to retrieve the cluster.

Surprisingly, this size is very small for most clusters.



II. PROBLEM SETTING AND MOTIVATION We motivate the problem that we propose to study by means  of a thought experiment that models the workflow in initial segments of some commercially important applications of doc- ument clustering, such as eDiscovery. Though we specifically mention eDiscovery, the initial segments of several knowledge gathering workflows in enterprises are similar. We outline two scenarios to motivate our work.

Let there be two users, U1 and U2. Both are provided with the same corpus D of documents. Both have the same goal: to gather a set of documents that are potentially relevant to a common case that they are handling from among the documents in D. For instance, the case might be related to a certain technology, and the corpus D consists of all files collected3 from hard drives and servers of the organization that created the technology. Every aspect of the creation of the technology must be captured. Both users need to inspect the corpus and make a list of potentially relevant documents for further consideration.

Next, we describe the two motivating scenarios.

A. Retrieval without Assistance by Clustering  Now on to each user. User U1 applies data mining. They first cluster the corpus D, and then identify clusters that are relevant to their case. For ease of exposition since our analysis is done on a cluster-by-cluster basis, let us assume that there is only one such relevant cluster, which we shall call C.

User U2 is aware that certain courts might not agree to gathering documents that have been selected for inclusion by an algorithm whose inclusion criteria are not easily explain- able. Therefore, they decide to identify relevant documents  3Typically, all the files from the concerned organization are collected as part of the initial efforts in eDiscovery.

?the old-fashioned way??using keyword searches, employed as Boolean retrieval (namely, every document in the search results is included), over the corpus. U2 proceeds iteratively through the following three steps.

1) Inspect portions of the corpus, and identify small sets of responsive documents.

2) Identify keywords that occur frequently across these sets of documents.

3) Retrieve all documents in the corpus that contain any of the individual terms taken from the identified frequently occurring sets of terms.

U2 aggregates the results of the searches after each iteration until they feel they have gathered a superset of the relevant documents to the case that they can take to the next stage of the eDiscovery process. Let us call the set of documents U2 has at this stage R, for retrieved.

One of the questions that we wish to study is: Question 1: How do the sets of documents C and R  compare to one another? Has R retrieved C?

B. Retrieval Assisted by Clustering  In this setting, U2 also has access to a clustering algorithm.

However, as earlier, the courts may not agree to document inclusion for further processing based upon their membership into a set of clusters. The courts are likely to insist that the documents produced before them are the result of a keyword search. Therefore, U2 proceeds as follows. They cluster the dataset, and identify clusters that are relevant. However, at this point, they have to translate the criteria for inclusion for a document back to the language of keyword search. Therefore, they ask the following question.

Question 2: ?Can these clusters be expressed, up to accept- able precision and recall as the result of a keyword search for purposes of producing them before a court?? In other words, although U2 employs clustering for their inter- nal considerations, eventually they must express the relevant documents as the result of a keyword search.

In addition to the above scenarios, a concise, interpretable description of a cluster as a simple disjunctive query can be used in almost all settings where clusters must be described to a user.

In this work, we will answer the central questions regarding how often such disjunctive query-based descriptions for clus- ters exist, and how long (in terms of number of terms in the query) are they?



III. RELATED WORK We first survey the literature going ?the other way?: namely,  is clustering useful in information retrieval?

A. Clustering in Retrieval  Earlier systems that used clustering for information retrieval did so to augment (nearest-neighbor) search in response to a query. The evidence is mixed: some studies claim that clustering does not increase the efficacy of retrieval systems (see [9, 14] for cases where clustering produces results inferior to nearest neighbor search), while others maintain that it does [7]. Methodologies also vary between the studies: some perform a static clustering of the entire ambient corpus (most studies reviewed in [15] fall into this category), while others cluster only the results of queries (see [13] and references therein). The use of clustering in information retrieval as a form of browsing, as opposed to search, was investigated by [3]. They called their method of browsing ?scatter-gather.? The efficacy of the system was demonstrated by describing a few sample sessions. [12] discuss scatter-gather on a large text corpus. They conclude that scatter-gather is capable of providing a coherent conceptual image of such large corpora.

The first discussion of scatter-gather on the results of a query is provided by [7], who provide anecdotal evidence of the efficacy of scatter-gather using a few examples.

B. Frequent Itemsets for Clustering  An area that has seen recent research activity is the possibil- ity of clustering document collections using frequent itemsets.

Frequent itemsets arose in work in association mining [8].

Apriori [1] was among the first algorithms proposed for efficient extraction of frequent itemsets. Work on clustering based on frequent itemsets extracted during association rule mining appears as early as [6], but not specifically for text.

More recently, [2, 4, 11] perform document clustering using frequently occurring terms. In order to cluster, [2] measure overlaps between the documents containing frequent itemsets.

They then partition documents by greedily selecting the next frequent itemset in order to minimize overlaps between clus- ters. Their algorithm seems to depend greatly on the order of selection of itemsets. [4] claim the previous algorithm is not scalable on this account, and they propose an approach based on first extracting hidden topics and then clustering. [11] use frequent word sequences to perform clustering, rather than frequently co-occurring itemsets.

All of the works cited above aim to cluster their data. Our goal is different: to compare clusters to document sets obtained by retrieval alone.



IV. THEORETICAL FRAMEWORK In this section, we put into place our theoretical framework.

After preliminaries, we introduce primitives that formalize the tasks that we wish to study.

A. Preliminaries  Our setting will be a document collection (or corpus) D = {D1, . . . , Dn}. A K-clustering C = {C1, . . . , CK} of D is a partition of D into K disjoint non-empty subsets whose union is D. We assume a standard preprocessing of documents for clustering: tokenization of words, stemming, removal of stoplists, and removal of words occurring less than three times in the corpus. The resulting set of tokens will be called terms.

The vocabulary of D is the set of all terms in the documents of D.

An itemset is simply a set of terms. Itemsets are denoted by F , and terms by t, with subscripts when necessary.

B. Maximal Frequent Discriminating Itemsets  We define below a particular type of itemset that we will use in this work.

Definition 1 (Maximal Frequent Discriminating Itemset): Let F = (t1, . . . , t|F |) be an itemset.

1) The support of F , denoted by supp(F ), is simply the set of all documents in which (t1, . . . , t|F |) co-occur.

2) Let ? be a positive integer that is called minimum support.

F is said to be frequent (w.r.t. ?) if supp(F ) > ?.

3) Let C be a cluster. Then F is said to be frequent relative to C if  supp(F ) ? C supp(F )  > ?,  where ? is a constant with 0 < ? < 1.

4) If F is frequent relative to C, it is said to be maximal  if there is no itemset F ? that is also frequent such that F ? F ?.

5) Let F be a maximal frequent itemset relative to C. Then F is said to be discriminating (relative to C) if it is not a frequent itemset for any other cluster besides C.

By frequent, we henceforth mean maximal frequent discrimi- nating relative to a cluster C.

C. Disjunctive Queries As described in ?II, in several commercially important  applications, the first step of document retrieval is initiated by a set of keyword search queries that are gleaned by co- occurrence frequency from sets of relevant documents. Al- though the keywords are gleaned by co-occurrence frequency, they are then used to perform individual searches. In general, several such searches are done; either in succession or in parallel (we will be agnostic to how they are done). Finally, the results of such searches are aggregated by taking the union of the search results obtained from each individual query.

At this stage, we would like to formalize the notion of con- ducting multiple frequently-occurring itemset searches from different locations in the document corpus, and aggregating the results across such searches. This gives us our notion of a disjunctive query.

Definition 2 (Disjunctive Query): Let F1, . . . , Fr be maximal frequent discriminating item-  sets relative to a cluster C. The disjunctive query Q = (F1, . . . , Fr), made to D, is defined as the query whose result set is the set of documents that contain?  {t : t ? F1 ? . . . ? Fr}.

We denote the result of this query by R?(Q,D). The positive integer r is called the depth of the query Q.

To be clear, a document is in R?(Q,D) if it contains at least one term from any of the r itemsets in Q. Therefore, a disjunctive query that uses two itemsets F1, F2 is equivalent to the disjunctive query that uses the single itemset F1 ? F2.

It is important at this point to note the shift from the natural state of frequent itemsets, namely a conjunction of their constituent terms, to their use in disjunctive form in a disjunctive query. Let us take the simplest case of a disjunctive query of depth one: Q = (F ). The result set R?(Q,D) is, in general, considerably larger than the support of F . The former is defined by means of a disjunction, while the latter by a conjunction.

Example 1: It will be useful to provide the reader with a concrete example of the setting of a disjunctive query.

Consider a cluster C in a 10-way clustering of the 20 Newsgroups dataset, that has F = {key encrypt clipper} as a frequent itemset. The support of this itemset, supp(F ) has 182 documents. Namely, all three terms key, encrypt, and clipper, co-occur in 182 documents in the dataset. Of these, 164 documents reside in the cluster C. This makes it, for our choice of (?, ?) = (15, 0.3) not only a frequent itemset in the dataset, but also a frequent itemset relative to C. Furthermore, upon inspection it is also found to be discriminating, and is therefore a frequent discriminating itemset.

Now consider the disjunctive query Q = (F ). When this query is posed to the dataset, we get the result set R?(Q,D).

This consists of precisely those documents in the dataset that contain at least one of the three terms key, encrypt, clipper. The size of R?(Q,D) is 1319. Note that because of the disjunctive nature of the query, this number is much larger than the support of F , which was only 164. ? One of the novel aspects of our work is the investigation of the disjunction of terms that are originally defined in terms of conjunction.

Henceforth, unless otherwise specified, by query, we mean a disjunctive query. Also, in this work, we are not concerned with the ranking of documents within the retrieved sets.

D. Cluster Retrieval Using Disjunctive Queries  Next, we would like to formalize the retrieval problem of gathering clusters by documents obtained via disjunctive searches as described in ?IV-C. To do this, we turn to standard     metrics used in information retrieval. We change the usual direction of clustering-meets-retrieval so that now our ?target? is to retrieve a cluster using only retrieval tools.

Definition 3: Let Q be a disjunctive query, and S be a subset of D. We say that Q retrieves S with precision p and recall q if  |R?(Q,D) ? S| |R?(Q,D)|  = p and |R?(Q,D) ? S|  |S| = q. (1)  Namely, the result of the disjunctive query on D using Q has precision p and recall q w.r.t. S.

The remaining definitions in this section are relative to a precision-recall value set (p, q) and a cluster C in a K-way clustering of D. Also, since our itemsets are assumed maximal frequent discriminating relative to C, we will suppress further specification of that property.

Definition 4: Let C be a cluster in a K-way clustering of D. Let Q = (F1, . . . , Fr) be a disjunctive query made to D that satisfies the conditions of Def. 3 for S = C. Then we say the following.

1) C is retrievable at (p, q).

2) Q is a retrieving disjunctive query, or simply retrieving  query for C at (p, q).

3) C has depth r at (p, q), written d(C) = r at (p, q).

Remark 1:  1) Def. 4, (3) says that a depth of a cluster is simply the depth of its retrieving query.

2) If C has depth r at (p, q), it does not necessarily mean that C also has depth r + 1 at (p, q). This is because although the recall of a candidate retrieving query of depth r + 1 might be higher than q, its precision may fall below p. However, in many cases, there may exist retrieving queries of depth r + 1 that have required precision and recall. This leads to the next definition.

Definition 5: The min-depth of C at (p, q), denoted by dmin(C) at (p, q), is defined as  dmin(C) at (p, q) = min{r : d(C) = r at (p, q)}. (2) Namely, dmin(C) at (p, q) is the depth of the ?shortest? query that retrieves C at (p, q).

Note that a cluster C may have multiple retrieving queries at (p, q).

E. Characterization of Datasets  In this section, we define quantities that capture the aggre- gate behavior of the functions defined in ?IV-D over a K- clustering of a dataset. We wish to define certain functions of K for the corpus. In order to define functions that depend only on K and the dataset, and not on a particular random seeding, we use the repeat-bisect algorithm.

The first set of functions captures the retrievability of clusters.

Definition 6: Let (p, q) be the required precision and recall in the statements below.

1) ?(K) is the proportion of the K clusters that are retriev- able. Namely  ?(K) at (p, q) = |{C : C is retrievable at (p, q)}|  K  2) ?(K, r) is the proportion of the K clusters that (are retrievable and) have depth r. Namely,  ?(K, r) at (p, q) = |{C : d(C) = r at (p, q)}|  K  3) ?(K, r) is the proportion of the K clusters that (are retrievable and) have depth up to and including r.

?(K, r) at (p, q) = |{C : d(C) ? r at (p, q)}|  K  4) ?min(K, r) is the proportion of K clusters that (are retrievable and) have min-depth r.

?min(K, r) at (p, q) = |{C : dmin(C) = r at (p, q)}|  K  In the next section, we will examine the behavior of these functions for standard datasets. We will find that different datasets behave differently with respect to these functions.



V. EXPERIMENTS A. Datasets  We used two standard benchmark datasets for document clustering provided at http://web.ist.utl.pt/acardoso/datasets/.

The first is N20, the 20 Newsgroups dataset, which contains 18821 documents. The second dataset is REUR8, a subset of the Reuters-21758 dataset comprised of 7674 documents.

B. Protocol  The steps of our protocol are described below.

1) We performed a K-way clustering of each dataset, which  we denote by D. For replicability, we used the repeat- bisect algorithm from the CLUTO toolkit4, rather than those which require random seeding.

2) For each cluster C in the K-way clustering, we obtained the maximal frequent discriminating itemsets with re- spect to C at (?, ?) = (15, 0.3). We denote these by {FC1 , FC2 , . . .}.

3) We pruned each frequent itemset, and retained only its three most frequent terms. This was for three reasons.

a) We found that using the additional terms beyond three  in our disjunctive queries did not contribute greatly to the retrieval numbers: they neither reduced the precision, nor increased the recall, by very much.

b) The focus of our work was on ?short? descriptions of clusters in terms of disjunctive queries; therefore pruning each itemset to its three most significant terms was in line with our goals.

c) We wished to ?normalize? the varying lengths of the itemsets. The length three was the least length of the frequent itemsets, and therefore a natural choice, especially in light of the preceding two reasons.

4) From the set {FC1 , FC2 , . . .} of frequent itemsets, we constructed disjunctive queries comprised of single, pairs, and triples. Therefore, our queries Q are of the form (FCi ); (F  C i1 , FCi2 ); and (F  C i1 , FCi2 , F  C i3 ). We stopped at  triples since the focus of our work is on short disjunctive descriptions for clusters, and because our results indicate that going to higher depth is seldom needed.

5) We performed each such disjunctive query Q on D.

For each disjunctive query, we compared the retrieved set R?(Q,D) to the cluster C, as in Def. 3. For our comparison points, we chose (precision, recall) values to reflect workflows from commercially important applica- tions such as eDiscovery5.

Now we will examine the large-scale behaviors defined in ?IV-E.

C. How many Clusters are Retrievable? A Study of ?(K)  We begin with a discussion of the proportion of clusters that were retrievable, as a function of K, at various reasonable values of (p, q). This is captured by the function ?(K) at (p, q).

We also plot the finer-grained information in ?(K, r) for r =  4Available at http://glaros.dtc.umn.edu/gkhome/views/cluto.

5In the early stages of eDiscovery, low precision is expected since all  documents that may even be potentially remotely relevant to a case must be collected.

1, 2, 3. Finally, we plot ?(K, 3) that lower-bounds ?(K). These plots are shown in Fig. 1 for both our datasets.

The number of clusters that have retrieving queries, as a function of the order of these itemsets, shows an interesting variation depending on K.

1) For N20, at (p, q) = (0.1, 0.6), over 80% of the clusters are retrievable until the mid-range for K, and more than 75% are retrievable for higher K.

2) The two datasets show markedly different behavior as K is increased. For N20, the mid-K range maximizes the retrievability, whereas for R8, retrievability is highest at low-K and falls more or less monotonically with K.

3) For middle values of K, the number of clusters of depth two (i.e., having retrieving disjunctive queries of depth two) exceeds those of one or three, across the (p, q) range for N20, and for higher (p, q) for R8.

D. What is the Min-Depth of Clusters? A Study of ?min(K, r)  Next, we plot the proportion of clusters C having min-depth dmin(C) = 1, 2, 3, as a function of K.

As Fig. 2 shows, a majority of clusters have min-depth one.

This is surprising, since it shows that a disjunctive query com- prised of three terms (which, conjunctively, form a frequent itemset) suffices to retrieve most clusters up to precision. Since clustering unearths relationships between documents based upon similarity computed in a very high dimensional space, one would have expected that a majority of retrievable clusters have higher min-depth. However, that is not the case. The results say that if a cluster is retrievable, then it is usually retrievable by a very short disjunctive query.

Therefore, most clusters have very short disjunctive query descriptions upto precision. We may phrase this result as follows: most clusters have very short retrieval ?keys.?

VI. CONCLUSIONS We summarize our findings.

1) At values of (p, q) that reflect real-world workflows, a  significant proportion (majority, for (p, q) = (0.1, 0.7)) of clusters are retrievable by disjunctive queries.

2) A majority of clusters (without qualification for (p, q) = (0.1, 0.7), and among those that are retrievable for higher (p, q)) have min-depth one. In other words, they can be retrieved with high recall using disjunctive queries using just the first three most frequent words in a frequent itemset. This is surprising because one would expect a data mining algorithm such as clustering to be performing a set inclusion that could not be approximated by simple disjunctive queries using so few keywords. It is also surprising that terms whose origin is a conjunction, namely, they co-occur frequently, can be used disjunctively to yield short descriptors for clusters.

3) Datasets differ markedly on their aggregate behavior under the retrievability functions that we have defined.

The frequent itemset?s support lies mostly inside the cluster, but is usually quite small relative to the size of the cluster.

Therefore, the cluster cannot be expressed as the support of a suitable itemset. However, if we investigate instead the disjunctive query formed by the itemset, then it may describe the cluster, up to an acceptable precision and recall.

We now discuss the important issue of interpretability in light of our findings and the demands of enterprise information management. Consider the case of a frequent itemset based retrieval that retrieves a cluster with (p, q) = (0.15, 0.8).

From a cluster-centric viewpoint, it may seem that the retrieval has collected many ?extra documents? and ?missed? 20% of the ?good? documents. However, many applications do not share this view. Take for instance eDiscovery. The ?extra documents? retrieved by frequent-itemset based search are easily explainable: they are in the collected set because they contain important keywords. On the other hand, the documents  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.1, Recall = 0.7  ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.15, Recall = 0.8 ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.2, Recall = 0.9 ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  (a) N20  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.1, Recall = 0.7 ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.15, Recall = 0.8 ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ?( K ,?  )  Precision = 0.2, Recall = 0.9 ?(K,1)  ?(K,2)  ?(K,3)  ?(K,3)  (b) REU R8  Fig. 1: The plots for ?V-C. Y-axes represent proportions of four quantities (described in legend on each figure). Notice that for N20, through most of the range of K, 70-85% of the clusters are retrievable by disjunctive queries of depth at most three. Also notice the difference in characteristics between the two datasets (details in text).

5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0 ? m  in (K ,?  ) Precision = 0.1, Recall = 0.7  ?min(K,1)  ?min(K,2)  ?min(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ? m in (K ,?  )  Precision = 0.15, Recall = 0.8 ?min(K,1)  ?min(K,2)  ?min(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ? m in (K ,?  )  Precision = 0.2, Recall = 0.9 ?min(K,1)  ?min(K,2)  ?min(K,3)  (a) N20  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ? m in (K ,?  )  Precision = 0.1, Recall = 0.7 ?min(K,1)  ?min(K,2)  ?min(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ? m in (K ,?  )  Precision = 0.15, Recall = 0.8 ?min(K,1)  ?min(K,2)  ?min(K,3)  5 20 40 60 80 100 Number of clusters, K  0.0  0.2  0.4  0.6  0.8  1.0  ? m in (K ,?  )  Precision = 0.2, Recall = 0.9 ?min(K,1)  ?min(K,2)  ?min(K,3)  (b) REU R8  Fig. 2: The plots for ?V-D. Y-axes represents proportions of min-depth (described in legend on each figure). Notice, for example, that at (p, q) = (0.1, 0.7) on N20 dataset, more than 60% of the clusters are retrievable with a disjunctive query of depth one, through most of the range of K.

supposedly ?missed? by the retrieval cannot so easily be explained, since their inclusion in the cluster follows no simple rule. In other words, the easy interpretability of retrieval versus the hard-to-interpret nature of cluster inclusion may make retrieval the better option in certain segments of important workflows such as eDiscovery. The natural question then arises: what should we retrieve? Our work studies this question in depth. It says that disjunctive queries formed from certain itemsets are the natural candidates for our retrieval-queries.



VII. FUTURE WORK This work shows a sharp variation of the behavior of  different datasets with respect to certain functions. Could maximizing (by appropriate choice of K) such measures be useful when navigating corpora using clustering?

The functions defined in this work offer a promising line of investigation: the characterization of differing clustering algo- rithms with respect to these functions. Do certain algorithms uncover more non-local associations (namely, those that raise the min-depth of the cluster) than others? Can algorithms be designed to capture more non-local associations? Such algorithms would have applications in EIM.

DEDICATION Dedicated to Yashwant-Rao Pawar, who died on January 14,  1761, at the Third Battle of Panipat, fighting by the Bhau?s side.

