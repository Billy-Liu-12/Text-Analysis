Arabic Text Classification Based on Features Reduction Using Artificial Neural  Networks

Abstract: Text classification is the process of grouping texts into one or more predefined categories based on their content. Due to the increased availability of documents in digital form and the rapid growth of online information, text classification has become one of the key techniques for handling and organizing text data. Despite the huge textual information that is available online and it increases every day, effective retrieval is becoming more difficult. Text categorization is one solution to tackle this problem. In this paper, we present and analyze the results of the application of Artificial Neural Network (ANN) for the classification of Arabic language documents. The work on automatic categorization of Arabic documents using Artificial Neural Network is limited. The system?s primary source of knowledge is an Arabic text categorization (TC) corpus built locally at the University of Jordan and available at http://nlp.ju.edu.jo. This corpus is used to construct and test the ANN model. Methods of assigning weights and features reductions that reflect the importance of each term are discussed. Each Arabic document is represented by the term weighting scheme. Since the number of unique words in the collection set is big, features reduction methods have been used to select the most relevant features for the classification. The experimental results show that ANN model using features reduction methods achieves better result than the performance of basic ANN on classifying Arabic document.

Keywords: text classification, neural network, pca,, vector space model.



I.  INTRODUCTION  Text Categorization (TC) is the task to classify texts to one of a predefined set of categories or classes based on their contents [http://www.thefreelibrary.com., 2013]. It is also referred as document categorization, document classification or topic spotting. Text categorization field is an area where information retrieval and machine learning convene, offering solution for real life problems (C.

Manning, and H. Schultz, 1999). For the sake of improving the precision in information retrieval systems coined with the increasing availability of documents that expand every single day, many researchers on text classification have recently magnetized much attention.

Text categorization overlaps with many other important research problems in information retrieval (IR), data mining, machine learning, and natural language processing. The overlap suggests that automatic text categorization is actually a part of larger picture that  becomes a distinct field of study.  It has many applications (Sebastiani, 2002) such as document indexing, document organization, text filtering, word sense disambiguation and web pages hierarchical categorization. The success of text categorization is based on finding the most relevant decision words that represent the scope or topic of a category. It is well known that the most common representation model of text is vector space model (Salton, 1975). This model represents each term as a vector in space; this raises a major challenge for text classification which is the high dimensionality of feature space that can be tens of thousands of terms. Thus, many different algorithms attempt to tackle this problem by reducing the huge dimensional space without sacrificing the classification accuracy.

The remaining of this paper is organized as follows: section II summaries related works in Arabic text categorization. The detail algorithm of neural networks is described in section III. Section IV explains how to use the features reduction methods to reduce the space of the features. Experiment results are given in section V, and the conclusion is given in section VI.



II.  RELATED WORK IN ARABIC TEXT CATEGORIZATION  As Many researchers have investigated different types of categorization techniques to improve text categorization performance, the under taken work on text categorization is primarily conducted on English and other European languages. However research on text categorization for Arabic language is still limited. Mesleh (2007) applied support vector machines algorithm on classifying Arabic language articles. He also argued why support vector machines (SVM) classifiers perform very well for Arabic text categorization. Sebastiani (2002), conducted a survey that explains the main machine learning approaches to TC, and stated that the machine learning is the dominant approach to TC in the research community. The survey also contains a detailed definition of TC, the history of TC, the constraints that can be enforced on TC, applications of TC, needs, and the motivation. The survey explains the main approaches towards TC that falls under the machine learning umbrella. For each approach it is concerned by three aspects. These three aspects are: document representation   DOI 10.1109/UKSim.2013.135     (pre-processing), classifier construction (categorization), and classifier evaluation (measuring the performance).

S. Al-Harbi et al. (2008) used two popular classification algorithms (SVM and C5.0) to classify Arabic text documents on seven different Arabic corpora by using a recognized statistics technique. El-Kourdi et al (2004) used naive bayes algorithm for automatic Arabic document classification. The average accuracy reported was about 68.78%. Syiam et al (2006) experimental results show that the suggested hybrid method of statistical and light stemmers is the most suitable stemming algorithm for Arabic language and gives generalization accuracy of about 98%. I. Hmeidi et al (2007) reported a comparative study of two machine learning methods on Arabic text categorization. Based on their own corpus, they evaluated K nearest neighbor (KNN) algorithm, and support vector machines (SVM) algorithm. They used the full word features and considered the Term Frequency and Inverse Document Frequency (TF_IDF) as the weighting method for feature selection. Moreover, they used chi-square as a ranking metric. Experiments showed that both methods were of superior performance on the test corpus while SVM showed a better micro average F1 and prediction time. As we mentioned above, since there are more pre-classified digital documents currently available in English, most of the existing document classification tasks in the literature are performed on English language documents.



III.  CATEGORIZATION USING NEURAL NETWORKS  Our system employs a neural network that classifies text into pre-defined output document categories. We applied the supervised classification approach; the problem of text categorization can be solved by using back propagation feed forward neural network, which learns the classification patterns from a set of predefined labeled examples, given an enough number of labeled examples called training set, and the task is to build a TC model called classifier. Then we can use the TC system to predict the category (class) of new (unseen) examples which called testing set (Fouzi et al, 2009).

In our system, the neural network employed is a 3- layer fully connected feed-forward network which consists of an input layer, a hidden layer, and an output layer. All neurons in the neural network are non-linear units with hyperbolic tangent (tanh). Activation function in the hidden layer, followed by sigmoid activation function in the output layer is employed. The neural network is trained with back-propagation algorithm. The input layers are equal to the dimensionality of the reduced feature space which is the decision words weights of the document. Vector and the output layers are equal to the number of pre-defined categories in the particular text  categorization task. For better refined documents classification results, we should go through two phases to achieve good classification accuracy. The first phase is the training phase, which starts from getting the training set. In the second phase, the reduced feature vectors representing the documents are fed to the input layer of the neural network classifier as input signals. These input signals are then propagated forward through the neural network so that the output of the neural network is computed in the output layer. The structure of the three layered back-propagation neural network is shown in Fig.

1 (S. Lawrence, et al, 1997).

Figure 1 Layered Feed-Forward Neural Network Model.

The neural network classifier must be trained before it can be used for text categorization. Training of the neural network classifier is done by the back propagation learning rule based on supervised learning. In order to train the neural network, a set of training documents and a specification of the pre-defined categories -which the documents belong to- are required. More precisely, each training example is an input-output pair:  Ti = (Di, Ci)  where Di is the reduced feature vector of the ith training document, and Ci is the desired classification vector corresponding to Di. The component values of Ci are determined based on the categorization information provided in the training set. During training, the connection weights of the neural network are initialized to some random values. The training examples in the training set are then presented to the neural network classifier in random order, and the connection weights are adjusted according to the back propagation learning rule.

This process is repeated until the learning error falls below a pre-defined tolerance level.



IV.  FEATURES REDUCTION METHODS  We have followed the statistical approach which deals with the documents as a bag of words; regardless of the syntax or the semantic meaning of sentences. After the preprocessing of documents, stemming and stop words     removal processes of the terms in each document, terms weighting and reduction process have applied to reduce the high dimensionality of feature space. Term weighting is one of the important and basic steps in text categorization based on the statistical analysis approach.

This has been widely investigated in information retrieval for the last two decades (Salton and McGill, 1983). Term weighting correlated to a value given to a term in order to show the importance of this term in the specific document or category or the whole collection.

In our experiments, we attempt to combine different weighting schemes to reflect the relative importance of each term in a document and a category and to reduce the dimensionality of the feature space as much as we can without losing the classification accuracy. (Salton,1989) reported that the best weight factor that is capable of measuring the significance of a term in a document set is based on the frequency of a term in a document, and the number of documents that contain that particular term over the entire corpus.  Therefore, in text categorization systems, a good measurement of the importance of a term in a document set is its popular weighting scheme which is the product of the term occurrence frequency (TF) and the inverse document frequency (IDF). The inverse document frequency of the ith term is commonly defined as log (N/df) where N is the number of documents in the document set, and df is the number of documents in which the term appears. Accordingly, the weight function w of term j in document Di can be computed as:  Wij = tfij * idfj    1  Where idfj = log (n/dfj)    2  By this definition, we are attempting to get the terms that are concentrated in few documents which are helpful in distinguishing between documents with different categories. A term that appears in fewer documents will have a higher IDF.  In order to examine the effectiveness of this measure in our categorization system, we propose to combine the TF_IDF value with two other dimensionality reduction methods DF method and CF method. In other word, we used the well known TF _ IDF method to measure the importance of a term for term selection, as well as we have used other measures DF threshold, and CF threshold to diminish the high number of features set. The terms are ranked according to their TF _IDF values, and a parameter d is set such that only the d terms with the highest TF _ IDF values and with DF/CF thresholds are selected to form the reduced feature set. In the document frequency (DF) and the category frequency (CF) features reduction methods, the main idea is to perform ranking of the terms in the collection based on TF_IDF measure and DF or CF threshold values, so that  the most important terms can be selected and the features vectors will be reduced. Now, we are going to look closely into both of these methods, but the key point is to minimize information loss as a result of term selection.

In the DF_CF method, terms are ranked based on the TF_IDF value of each term within a document group based on the DF and CF threshold values. For each document group, the document frequency of a term is defined as the number of documents within that particular group containing the term. And by choosing the inverse document frequency and document frequency TF_IDF as the importance measure, as well as by specifying DF and CF threshold values which aim to reduce the feature vectors size, we are assuming that the important terms are those that appear frequently within a group of documents belonging to the same category. In the DF_CF method, a two phase process is used for term selection. In the first selection phase, we rank the term TF_IDF values of all categories and chose the highest TF_IDF values from each category. In the second selection phase, we define a threshold d and c on the document frequency and category frequency of the terms, so that a term is selected only if its document frequency is higher than the threshold d and lower than the threshold c. We also have used principal component analysis (Jolliffe, 1986 ) to reduce the dimensions into fewer features space.



V.  EXPERIMENTS AND RESULTS  To evaluate any categorization system, text collection which consists of different categories must be available for training and testing purpose.  Bearing in mind that, there is no standard Arabic text collection as a benchmark for researchers who work on classifying Arabic texts. We have picked a sub set of Arabic articles covering different topics from an Arabic corpus built locally at the University of Jordan. The corpus we used is available at http://nlp.ju.edu.jo and free for researchers. We picked seven categories out of this corpus containing 2800 documents which differ in size and content.

TABLE I.  DISTRIBUTION OF TESTING AND TRAINING DOCUMENTS  Category Training documents Testing documents Economy 28 113 History 70 275 Literature 51 205 Politics 43 161 Religion 108 425 Science 168 670 Sports 98 385  The collection we obtained to conduct our experiments is organized in such a way that makes it suitable to experiment with a text categorization system.

Each category in our text collection covers a range of     topics each with a credible amount of data. The categories topics that we used are about economy, history, literature, politics, religion, science, and sports. Table I showed the distribution of the training and testing set which we run our experiments on them.

There are many metrics for evaluating the task of text categorization. These metrics aim to show the classification accuracy and performance, and to offer a possibility for comparing the method with other works.

These evaluation metrics are precision, recall, accuracy, F-measure, and macro averaging (Wikipedia, 2010). We summarized all the evaluation measures in Figure 2 and Table II below.

Figure 2 Text classification evaluation metrics  TABLE II   TEXT CLASSIFICATION EVALUATION METRICS  Evaluation metric Equal Precision TP / (TP + FP)  Recall TP / (TP + FN) Accuracy (TP + TN)/N F Measure 2*Recall*Precision/(Recall + Precision)  A. Characterization of the Feed forward neural network  We used the trial and error approach to find a suitable number of hidden layers. We found the number 10 is good for the number of hidden layers that provide good classification accuracy. The number of input layers is equal to the of extracted feature vectors which is 566 after we applied feature reduction methods. The number of output layers is 7 which is based on the number of categories in the our in-house Arabic corpus. We summarized the characteristics of the error back propagation neural networks parameters that we applied it to classify Arabic documents in Table III.

TABLE III: THE BACK PROPAGATION NEURAL NETWORK PARAMETERS  Parameters Value Learning rate 0.01  Momentum rate 0.9 Number of iteration 1000 Mean Square Error 0.001  During the training phase, we have shown the relationship between the number of epochs and learning rate, in order to see how the accuracy improves with  increasing number of iterations. The Mean Square Error (MSE) is measured for repeated training with increasing number of epochs. The learning error decreases exponentially as the number of iteration increases. We have stopped training after a certain number of epochs.

From our experiments, we find that certain number of epochs can get the tradeoff between categorization accuracy and efficiency.   Figure 3 is showing the relation of the error rate to the number of epochs.

Figure 3  MSE VS epochs for Arabic documents categorization  B. Experiment 1 In the first experiment, we have experimented with the common used feature selection method TF_IDF, in order to reduce the high dimensionality vectors into fewer dimensions.  We ranked the TF_IDF and we have chosen the top highest values from each category. The categorization performance is given in Table IV.

TABLE IV:   CATEGORIZATION RESULTS USING BASIC ANN CLASSIFIER  Category Precision Recall Accuracy Fmeasure Economy 0.92 0.92 0.89 0.92 History 0.88 0.95 0.87 0.91  Literature 0.94 0.97 0.92 0.96 Politics 0.79 0.80 0.79 0.79 Religion 0.96 0.97 0.96 0.96 Science 0.98 1.00 0.99 0.99 Sports 0.99 0.98 0.98 0.98  Also we attempted to put more condition on the TF_IDF values to get the best decision words that represent each category, by applying DF threshold greater than a value and CF threshold less than a value. We also introduced principal component analysis and combined its feature extraction ability to the DF-CF feature reduction techniques, in order to diminish the high dimensionality vectors into fewer dimensions.   The results showed that principal component analysis was effective in reducing     the high dimensionality of the feature space, and at the same time maintaining precision and recall values with an increase in both values.

TABLE V: CATEGORIZATION RESULTS USING FEATURE REDUCTION WITH ANN CLASSIFIER  Category Precision Recall Accuracy F measure Economy 0.95 0.95 0.92 0.95 History 0.97 0.99 0.99 0.98  Literature 0.91 0.94 0.88 0.93 Politics 0.93 0.94 0.91 0.93 Religion 0.96 0.96 0.96 0.96 Science 1.00 1.00 1.00 1.00 Sports 0.96 0.98 0.97 0.99  As we can see the macro averaging F measure results are equal 0.93 and 0.96 from the last two experiments, we can tell that PCA with DF_CF method was effective in increasing precision, recall, and F measures values. We depicted the F measure graph in Figure 4.

Figure 4: Comparison the classification results of ANN with and without features reduction  C. Experiment 2 In this experiment, we varied the size of the training set by incrementally increase the number of training set percentage in each run and measures precision, recall, F measure and macro average in each run. We have experimented with the feature selection method TF_IDF and our proposed threshold method, in order to reduce the high dimensionality vectors into fewer dimensions.  We ranked the TF_IDF and we have chosen the top highest values from each category that satisfy DF and CF threshold condition. The categorization performance is depicted in figure 5.

Figure 5.  The relation between the size ratio of the training set and macro-average results  D. Experiment 3 In this experiment, we compared the performance by varying the number of dimension; the neural network input nodes number is equal to the dimension of the document vectors, so the percentage of the reduction in the dimensionality changed according to the threshold value. We varied the number of feature dimensions by changing the TF_IDF threshold value which led to vary the number of input layers, and we have measured precision, recall, F measure and macro average in each run. We experiment with the well known feature selection method TF_IDF, in order to vary the high dimensionality vectors into fewer dimensions.  We ranked the TF_IDF and we have changed the top highest values chosen from each category. The categorization performance is depicted in figure 6.

Figure 6: relation between  No.  of dimensions & macro-average results

VI.  CONCLUSION  In this paper, we studied and empirically tested the categorization effectiveness and feasibility of Arabic text categorization model based on artificial neural network.

Features dimensionality reduction and selection techniques were applied to reduce the high dimension feature space, which is common for textual data based on vector space model. We combined features reduction methods with PCA to reduce high dimensions features space into a low-dimension input space for the neural network. The introducing of features reduction methods has improved the categorization performance, the reduced size of the vectors also decreased the computational time in the back-propagation neural network.

The experiments on an Arabic corpus have demonstrated that the ANN model is effective in representing and classifying Arabic documents. The results indicated that ANN model using features reduction methods is more able to capture the non-linear relationships between the input document vectors and the document categories than that of basic ANN model. Also the results showed that back propagation learning in neural networks was able to give good categorization performance as measured by precision and recall.

