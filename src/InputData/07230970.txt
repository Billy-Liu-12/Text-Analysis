A Distributed Frequent Itemset Mining Algorithm

Abstract-Frequent itemset mining is an important step of  association rules mining. Traditional frequent itemset mining algorithms have certain limitations. For example Apriori algorithm has to scan the input data repeatedly, which leads  to high 110 load and low performance, and the FP-Growth algorithm is limited by the capacity of computer's inner stores because it needs to build a FP-tree and mine frequent itemset on the basis of the FP-tree in memory. With the  coming of the Big Data era, these limitations are becoming more prominent when confronted with mining large-scale data. In this paper, DPBM, a distributed matrix-based pruning algorithm based on Spark, is proposed to deal with frequent itemset mining. DPBM can greatly reduce the  amount of candidate itemset by introducing a novel pruning technique for matrix-based frequent itemset DllRlng algorithm, an improved Apriori algorithm which only needs to scan the input data once. In addition, each computer node reduces greatly the memory usage by implementing DPBM  under a latest distributed environment-Spark, which is a lightning-fast distributed computing. The experimental results show that DPBM have better performance than  MapReduce-based algorithms for frequent item set mining in terms of speed and scalability.

K?ords-frequent itemset mining, distributed algorithm,  matrix-pruning, Spark, MapReduce

I. INTRODUCTION  In recent decades, with the continuous development of the Internet technology, information communication technology and sensor technology, more and more enterprises and institutions gradually possess huge volumes of data which contains a large number of potentially values. Data mining has become a heat area in the science community because of its ability to mine useful information and knowledge from the massive data of various applications such as business management, production control, market analysis, engineering design and scientific exploration. Of late, association rules mining has become the focus of data mining research, whose aim is to discover certain interesting association or correlation relationship among the itemsets of data. In the procedure of association rules mining, frequent item set mining is the first and most important step. Traditional algorithms, involving Apriori algorithm [1], Fp-growth algorithm [1], and other matrix-based algorithms [2, 3] for frequent itemset mining working on single computer, show good performance in  ? corresponding author: mayunlongJ976@gmail.com    dealing with small amount of data. However, these traditional algorithms are not suitable to mine frequent itemset of massive data. When the amount of data reaches large enough, these algorithms applied to a single machine environment become very inefficient or even almost incalculable because of limited memory and compute capability.

Therefore, researchers have attempted to use the parallel computing technologies to deal with mining frequent itemset from massive data. In researches of M. E. Otey et al. [4] and Md. Golam Kaosar et al. [5], they used Message Passing Interface (MPI), a kind of frameworks for distributed computing, to mine frequent itemset. Although MPI has certain advantages over iterative computations, it has two defects: 1) High communication load. Since the input data are stored centrally in one computer node, the process of mining takes much time to exchange data between different computer nodes. 2) Without fault tolerance. If one problem occurs in the process of calculation, then the progress would restart. In recent, some researchers proposed many kinds of distributed frequent itemset mining approaches base on MapReduce framework, a framework embedded in Apache Hadoop [6] to process large amounts of distributed data in parallel. Apache Hadoop is one kind of open source softwares for reliable, scalable and distributed computation released by Apache Software Foundation, making MapReduce framework freely available for everybody. These MapReduce-based algorithms can be mainly classified as two categories. The first is non-iterative algorithm, Haoyuan Li et al. [7] and Xingshu Chen al. [8] introduced parallel FP-Growth algorithm, derived from traditional FP-growth algorithm, into large-scale data mining. However, the non-iterative algorithm often occur load imbalance, leading to extra execution time cost or even making certain computer nodes out of memory. The second is iterative algorithm, such as Apriori algorithm. Jongwook Woo [9] implemented the traditional Apriori algorithm base on MapReduce and Lin et al. [10] proposed three kinds of Apriori algorithms based on MapReduce. Nevertheless, the focus of these iterative algorithms is on how to transform the traditional Apriori algorithm to distributed algorithm under the MapReduce framework. Moreover, the MapReduce framework is not appropriate for iterative computation because iterative computation need do read-write operation repetitively to Hadoop Distributed File System (HDFS), which leads to high 110 load and time cost. Meanwhile, Apache Spark is a memory-based distributed framework [11].

By introducing innovatively a new abstraction called resilient distributed dataset (RDD) [12], Spark shows great perfonnance in processing iterative computations because it can reuse intennediate results, usually stored in memory unless there is not enough memory in the cluster, across multiple parallel operations. In short, Spark has an excellent perfonnance to process iterative computations.

Accordingly, on the one hand, since traditional algorithms for single machine are not suitable for mining frequent itemset of massive data, thus distributed algorithms are applied to mine frequent itemset. On the other hand, in order to deal with load balance and the poor perfonnance under MapReduce for iterative computations, we implement a distribute algorithm for frequent itemset mining under the Spark framwork. Hence, this paper proposes DPBM for frequent itemset mining among large-scala data by improving Apriori algorithm for single machine to reduce the amount of candidate itemset and implementing it base on Spark to promote the efficiency of iterative computations. The experimental results show that DPBM outperforms the MapReduce-based algorithms in many aspects, such as the speed, scalability perfonnance, etc.

The rest of this paper is organized as follows: related theories of frequent itemset mining and Apache Spark are described in Section II. In Section III, a distributed matrix-pruning algorithm for frequent itemset mining is introduced in detail. And the experimental results are provided in Section IV. At last, section V concludes the future work.



II. BACKGROUND  A. Apache Spark  Apache Spark is an open-source in-memory data analytics cluster computing framework developed in the AMPLab at the University of California, Berkeley in 2009, providing a similar scalability and fault tolerance with Map Reduce. The core of the Spark is RDD. The RDDs in Spark can make algorithm's speed perfonnance to process iterative jobs, such as PageRank algorithm and K-means algorithm, can be as much as ten times faster than the MapReduce's perfonnance to execute the same work. It is the excellent perfonnance to execute iterative jobs that Spark became an Apache Incubator project in June 2013 and became an Apache Top-Level Project in February 2014.

In addition, on the basis of RDD, Spark can realize computation in clusters' memory and fault tolerance. RDD is a read-only data block, which can be created by referencing a dataset in an external storage system, such as HDFS, or any data source offering a Hadoop InputFonnat, or created by others RDDs, and it stores much infonnation, such as its partitions, a set of dependencies on parent RDDs called as lineage etc. By the help of the lineage, Spark recovers data and achieves fault tolerance effectively.

B. Frequent Itemset Mining  Suppose I={I',/2, ... ,IJ is an itemset composed by m types of item. Each transaction is a subset of itemset I   and has a unique label called as TID. The database D consists of a series of transactions. An itemset is a subset of the itemset I , and when the itemset consists of k different items, then we can call it as k - itemset . Let X is an itemset, the support number of itemset X refers to the number of transactions that contain the itemset X and if the support number of itemset X is greater than or equal to the minimum support threshold (hereafter this text will be abbreviated as MinSup) specified by us, then the itemset X is labeled as a frequent itemset. The purpose of frequent itemset mining is to find all frequent itemset in a given database.



III. RESEARCH METHODS  The Apriori algorithm is a traditional iterative algorithm for frequent itemset mining. The main ideas of the Apriori algorithm are described as follows: The first is to scan the database, count the number of each item in the database, i.e.

calculating the support number of each item, and pick out all items which support number are not less than MinSup to fonn frequent 1- itemset. The second is to generate the entire candidate (k+l)-itemset by means of using all the frequent k - itemset and scan the database again to acquire all the frequent (k+ 1) -itemset .

Considering that Apriori algorithm has to scan the database repeatedly and Spark has a remarkable perfonnance to deal with iterative computations, hence this paper proposes DPBM for frequent itemset mining based on Spark. There are two innovative points in this paper. Firstly, on the basis of Apriori algorithm, an improved algorithm for frequent itemset mining is introduced in part A in detail to not only avoid scaning the database repeatedly but also greatly reduce the amount of candidate itemset. The second is to implement the improved algorithm under Spark described in part B to make full use of Spark cluster's memory through storing the intennediate data in memory, avoiding reading data from harddisk again and again.

A. Matrix- Pruning Algorithm for Frequent Itemset Mining  The basic principle of the improved algorithm is to acquire the Boolean vector for each item in a given database and a 2 - itemset matrix to reduce the amount of candidate itemset by using these Boolean vectors and the 2 - itemset matrix. Now assuming that database D has n transactions  T={T"T" ... ,?}, m different items I={I',/2,???,IJ and the MinSup is set to Min_sup . The major steps of the improved algorithm are stated as follows:  Step 1: To obtain the Boolean column vectors for items which are frequent 1- itemset .

To begin with, scan the database D to obtain n dimension  Boolean column vector V;= (b" b2, ? ? ?  ,b. r ' where  b. = '  ) ( j=I,2, . . .  ,n) {1'I.ET.

} 0, Ii eo ? for each item Ii ( i=l, 2, ... , m) .

The support number of Ij is the number of nonzero elements  in ?. Then pick out the items and corresponding Boolean vectors whose support number are not less than Min_sup.

Finally, all the frequent 1- itemset are obtained, and then we sorted them in ascending order according to support number.

Step 2: According to the Boolean column vectors for all frequent 1- itemset , obtain the 2 - item set matrix M.

Use all the frequent 1- itemset in ascending order respectively as the rows and the columns of the 2 - itemset matrix M. And if the number of frequent 1 - itemset is  k , then M is a kx k matrix. The value of M? i"* j) in M is the number of nonzero elements of the vector  generated by logical AND operation between the Boolean  vector ? and the Boolean vector ?, where ? and ? respectively refer to the ith and jth item of the entire sorted  frequent 1 - itemset , and the value of M jj' i.e., the  diagonal elements of M , IS the frequency of  Mu ? Min_sup (i=O,l ,2, ... j-l) on the column j from  M . We only need to calculate the value of ? i ? j) because M is a symmetric matrix. At last, it is easy to acquire the frequent 2 - itemset on the basis of the matrix M.

Step 3: In this step, our main purpose is to obtain frequent (k+l) -itemset by utilizing frequent k - itemset (k? 2).

The following are important property and definition used in this step, which can be easily derived and proved.

Property 1 : If X is a frequent itemset, then any subset of X is a frequent itemset. That is to say, if X is a frequent (k+l) -itemset, the number of frequent k - itemset is more than k+ 1 .

Definition 1: Assuming that ? is the Boolean column  vector of item I r . Then the support number of itemset  Q= { 10, Ip' ... , Iq } is the number of nonzero elements in the vector generated by the logical AND operation between all items' Boolean column vectors from itemset Q.

Suppose that we have obtained Lk ' which is composed by all frequent k - item set . Firstly, if the number of frequent k - itemset is not less than k+ 1 , according to property 1 the frequent (k+ 1) -itemset may exist. Otherwise, there is no frequent (k+l)-itemset . Secondly, as for every frequent k - itemset , just take F which is a frequent k - itemset  as an example, pick out the first item I first and the last item  Ilast in F respectively. Assume that the row in M which   corresponds with I first is u and the row in M which  corresponds with Ilast is v .  Then search each column of  the row V in M in turn. As for the column w (w<v) , if  M? ? Mill_sup, M? ? k ,  and Muw ? Min_sup, then we can  get the corresponding item Inew of the column w and  thus make up a candidate (k+ 1) -itemset by combining the frequent k - itemset F with the item Inew. At last, in the light of definition 1, the support number of the candidate (k+ 1) -itemset can be obtained. And if the support number is not less than Min_sup , then the candidate  (k+ 1) -itemset is a frequent (k+ 1) -itemset .

B. DPBM Implementation Based on Spa rk  In this part, the implementation of the above algorithm for single machine based on Spark is stated as follows. In order to reduce the memory usage, HashMap is respectively used in program to save the Boolean vector for each frequent 1 - itemset and all the frequent 2 - itemset , instead of the 2 - itemset matrix M .

Step 1: To obtain the Boolean vector for all frequent 1 - itemset , then produce all the frequent 2 - itemset on the basis of these Boolean vectors. The Figure 1 describes the specific procedures under Spark.

Fig. 1. The process of step 1 under Spark  As illustrated in Fig. 1, firstly, read the input dataset from HDFS to create the HadoopRDD. Secondly, the HadoopRDD call the flatMap function to produce a FlatMappedRDD which contains all items of the input data. Then the map function is    applied to the FlatMappedRDD to transform each item to a Tuple- (item, 1), hence a MappedRDD is formed on the basis of the FlatMappedRDD. As for the above MappedRDD, use the reduceByKey function to count the support number of each item, pick out the items whose support number are not less than Min_sup . Then according to the support number of  all the frequent 1 - itemset , save all the frequent 1- itemset in the array arr in ascending order. As for every frequent 1- itemset in arr , acquire the Boolean vector and save the <item, vector> key/value pair in a HashMap. At last, the entire frequent 2 - itemset matrix can be acquired by using these Boolean vectors and the array arr , and be saved into a HashMap that the key is a frequent 1- itemset , here take as G an key , and the value also is a HashMap whose key is an another frequent I-itemset that can make up a frequent 2 - itemset with G, whose value is the support number.

Step 2: To get all frequent (k+l)-itemset by using frequent k - itemset. k ? 2) . The Fig. 2 describes the process of executing based on Spark.

This step's ideas are the same as the above stated Step 3 in  part A. Firstly, the RDD that contains frequent k - itemset  call the map function to produce total candidate  (k+l)-itemset . Secondly, utilize the broa dcast va riable which saves the Boolean vectors of all frequent 1 - itemset  to determine every candidate (k+ 1) -itemset whether or not is a frequent (k+l) -itemset by definition I. Finally, all the frequent (k+ 1) -itemset are obtained.

y mapLgetCandidateltemset())  Fig. 2. The process of step 2 under Spark  C. Releva nt Details of Algorithm Design Based on Spark  It is worth noting that there are some RDDs in part A which be used repeatedly throughout the job. In order to promote the performance of entire job, the cache function is applied to   these RDDs to store them in distributed workers of the cluster and achieve in-memory computation. Thus, in this way, the input data only needs to be loaded once and thus the following computations which need to reuse these data will be much faster. For example the HadoopRDD saving the input date which is reused to get the Boolean vectors for all the frequent 1- itemset , and the RDDs saving frequent k - itemset which are reused to obtain the all the frequent (k+l) -itemset .

In addition, considering algorithm's efficiency, some variables, which can be saved in the memory of the Spark cluster, should be shared to use them in multiple parallel operations. Here we apply the broa dcast function, which allows programmers to send shared variables to all workers of the cluster only once rather than delivering a copy of shared variables for each task. For example, the variable which saves the Boolean vectors for each the frequent 1- itemset , the variable saving all the frequent 2 - itemset , etc.



IV. EXPERIMENTAL RESULTS  In order to test the performance of the proposed algorithm, we compare DPBM with the Map Reduce-based Apriori algorithm. The DPBM written by Scala language is implemented in Spark 1.0.2 and the MapReduce-based Apriori algorithm written by Java is run in Hadoop 2.2. Moreover, according the experimental results, we evaluated DPBM in terms of the speed performance and the scalability performance. All the experiments and analysis of results are shown as follows.

A. Experimental Environment a nd Dataset  All the experiments have been done on a cluster which consists of 8 computer nodes installed Centos 7.0. Each computer node is equipped with the same physical environment, Intel Core i5-4440M CPU 3 . 1  GHz, 16GB memory and I TB disk. We chose the classic dataset, Tl0I4D lOOK, which is generated by IBM's data generator and generally used to test for the frequent itemset mining. The detail of the dataset is shown in Table I.

TABLE I. DATASETS DESCRIPTION  Datasets Size Transactions Items  TIOI4D100K 3.84MB 100,000 870  B. Execution Time a nd Scalability Experiment  Firstly, in order to prove the optimization effect that DPBM has the speed advantage over MapReduce-based Apriori algorithm, we compare their separate execution time by changing the support degree while keeping the same Spark cluster that consists of 8 compute nodes.

The speed performance of two different algorithms is shown in Fig. 3. The x-axis denotes the different support degrees and y-axis presents the execution time of algorithms.

As shown in Fig. 3, under all circumstances, the execution time of the MapReduce-based Apriori algorithm is at least 5 times more than the execution time of DPBM. Besides that, the growth of the execution time of the MapReduce-based Apriori algorithm is more rapid than that of DPBM. To sum up, DPBM has better speed performance than the Map Reduce-based Apriori algorithm.

?130 -e ? 110 " ?   = 0 70 '" ? ? 50   0.1 0.2 0.3 0.4 0.5  Fig. 3. Execution time for different support  Secondly, the dataset Tl0I4D100K is used to test the scalability performance by changing the number of computer nodes in Spark cluster, ranging from 2 to 8, while keeping the support degree being 0.5%.

150 - ? Tl0?Dl 00K   10+---------?--------?------? 4 6  The Number or Computer Nodes in Spark Clu51er  Fig. 4. Execution time for various quantities of computer nodes  DPBM's scalability performance is shown in Fig. 4. The x-axis shows the number of computer nodes in Spark cluster and y-axis presents the execution time of DPBM algorithm.

According to Fig. 4, with the raising of the number of computer nodes, the execution time of DPBM base on Spark decreases near-linear. Hence, we can conclude that DPBM can achieve near-linear scalability in performance.

v. CONCLUSION  In order to deal with the poor performance of traditional frequent itemset mining algorithms for single machine and iterative computations under MapReduce environment, this paper proposed DPBM, a distributed matrix-based pruning algorithm for frequent itemset mining based on Spark.

Compared with previous algorithms, DPBM has two improvements. Firstly, improve traditional algorithms for single machine by introducing a novel pruning technique to reduce the amount of candidate itemset. Secondly, implement algorithm's parallelization under Spark framework to promote the efficiency of iterative computations. At last, the experimental results showed that DPBM is more excellent than the Map Reduce-based algorithms in terms of the speed and scalability. In the future work, DPBM will be further optimized by conducting deep analysis of the operational mechanism of Spark.

