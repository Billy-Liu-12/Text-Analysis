A Novel Tool (FP-KC) for Handle the Three Main  Dimensions Reduction and Association Rule Mining

Abstract?: This work attempt to developing the FP-Growth data mining algorithm through use several knowledge constructions to build up a novel tool called Frequency Pattern-Knowledge Constructions (FP-KC) to find the association rules and to satisfy the goal of dimension reduction methods is using the correlation structure among the predicator variables by reduction the main three dimensions (features, samples and value of features). FP-KC attempts to combine between the features of principle component analysis and frequency pattern growth. This done using the three criteria (Eigenvalue, cumulative variability and Scree plot). There are many reasons for developing the FP-Growth data mining algorithm in build up a novel algorithm FP-KC to find the association rules: (a) the size of an FP-tree is typically smaller than the size of the uncompressed data because many records in dataset often share a few items in common.(b) Given the best result, if all the records have the same set of items, and this point always satisfy in the scientific dataset. (c) FP-growth is an efficient algorithm because it illustrates how a compact representation of the transaction data set helps to efficiently generate frequent item sets. (d) The run-time performance of FP-growth depends on the compaction factor of the data set.  The performance of FP-KC test using five huge databases including (Primate splice-junction gene sequences, Diabetes, DNA, GIS and Watermarking). The confidence' degree of the all association rules yield by FP-KC is equal to 95%.

Keywords- PCA; FP-Growth; Knowledge Constructions; Watermarking Database.



I.  INTRODUCTION The choice of data representation, and selection, reduction  or transformation of features is probably the most important issue that determines the quality of an intelligent data analysis solution. Besides influencing the nature of a KDD algorithm, it can be determined whether the problem is solvable at all, or how powerful the resulting model of KDD is. A large number of features can make available samples of data relatively insufficient for mining. In practice, the number of features can be as many as several hundreds. If we have only a few hundred samples for analysis, dimensionality reduction is required in order for any reliable model to be mined or to be of any practical use. On the other hand, data overload, because of high dimensionality, can make some data-mining algorithms non applicable, and the only solution is again a reduction of data  dimensions. The three main dimensions of preprocessed data sets, usually represented in the form of flat files, are columns (features), rows (cases or samples), and values of the features [9]. But how one can combine among these three main dimensions of preprocessed data sets without loss any inform is still open problem as explain in Fig. 1. [1]    Figure 1. Relation among the Three Main Dimensions  Dimension reduction methods have the goal of using the correlation structure among the predictor variables to accomplish the following[ 5]:  To reduce the number of predictor components To help ensure that these components are independent To provide a framework for interpretability of the  results.

.



II. CURRENT STATUS The data mining process requires highly computations  when evolving large data sets. Dimensionality Reducing (the number of attributed or the number of records) reduce this computations. This step is closely related (often dimensional reduction is used as a step in feature extraction), but the goals can differ. Dimensional reduction has a long history as a method for data visualization, and for extracting key low dimensional features. It is including wavelet transforms and principal components analysis, clustering, sampling. The main Taxonomies of dimension reduction problem are decreasing  Technologies of Information and Telecommunications (SETIT)     learning cost, increasing learning performance, reduction of irrelevant dimension, reduction of redundant dimension.

Hussein K.,2002[7]  suggests algorithm to discover terminated item sets(DOTI) and he explains how can maintenance the terminated item sets to extract dynamic rule miner.

Mutthew G. and Larry B., 2003 [9]       describe a method of feature construction and selection using the traditional genetic programming, genetic algorithm and stander c4.5 on number of databases to improve the classification performance of the well-known induction algorithm c4.5.

Nian Yan., 2004[11] introduced the classification of data mining approaches and focused on back propagation neural network and its enhanced applications. The multilayer neural network has been applied in building the classification model.

There are two data sets used for the study. HIV data set from bioinformatics research and credit card data set for the risk management. A pre-training process is designed to construct the neural network classifier fast also proposed a new ensemble method( performance weighted ensemble based on the p value and has proved the strongpoint of it compared to the traditional ensemble method).

Mahdi ,2005[8] suggests a technique that can discover knowledge from any database via soft computing techniques which involve fuzzy set, neural network and genetic algorithm by build DBRule Extractor system.

Samaher, 2008[13] presents a method to design a programming system using hybrid techniques represented by soft computing and data mining to discover knowledge in databases. This work proposes a fuzzy c-mean model for attributes clustering. The genetic algorithm is used to determine which of features are most predictive after that; radial basis functions are embedded in two-layer neural which topology is used in real applications to classify the database records.

Christopher, 2010[2] showed a tutorial overview of several geometric methods of feature extraction and dimensional reduction. He divides the methods into projective methods and methods that model the manifold on which the data lies. For projective methods, he review projection pursuit, principal component analysis (PCA), kernel PCA, probabilistic PCA, and oriented PCA; and for the manifold methods, he review multidimensional scaling (MDS), landmark MDS, Isomap, locally linear embedding, Laplacian eigenmaps and spectral clustering  The Table I presents the comparison among the different works mentioned previously according to (Authors, tools used in these studies, types of datasets that handle by these studies, kind of preprocessing, and natural of results).

TABLE I.   COMPARISONS AMONG THE PREVIOUS DIMENSIONAL REDUCTION STUDIES   .



III. FP-GROWTH ALGORITHM FP-growth is a method of mining frequent item sets with-  out candidate generation. It constructs a highly compact data structure (an FP-tree) to compress the original transaction database. Rather than employing the generate-and-test strategy of Apriori-like methods, it focuses on frequent pattern (fragment) growth, which avoids costly candidate generation, resulting in greater efficiency. In addition, FP-growth Algorithm can be define as powerful computational tools in a generation association rules compare with A priori algorithm.

It is base on FP-tree.

FP-Growth adopts a divide and conquer strategy by (1) compressing the database representing frequent items into a structure called FP-tree (frequent pattern tree) that retains all the essential information and (2) dividing the compressed database into a set of conditional databases, each associated with one frequent item set and mining each one separately. It scans the database only twice. In the first scan, all the frequent items and their support counts (frequencies) are derived and they are sorted in the order of descending support count in each transaction. In the second scan, items in each transaction are merged into an FP-tree and items (nodes) that appear in common in different transactions are counted. Each node is associated with an item and its count.

Nodes with the same label are linked by a pointer called a node-link. Since items are sorted in the descending order of frequency, nodes closer to the root of the FP tree are shared by more transactions, thus resulting in a very compact representation that stores all the necessary information. Pattern growth algorithm works on FP-tree by choosing an item in the order of increasing frequency and extracting frequent item sets that contain the chosen item by recursively calling itself on the conditional FP-tree, that is, FP-tree conditioned to the chosen item.

FP-growth is an order of magnitude faster than the original Apriori algorithm.

FP-growth is a seminal algorithm proposed in this thesis for mining frequent item sets for association rules.

Figure  2. Construction of an FP-tree [12]  Fig.2 shows a data set that contains ten transactions and five items. Each node in the tree contains the label of an item along with a counter that shows the number of transactions mapped onto the given path. Initially, the FP-tree contains only the root node represented by the null symbol. The FP-tree is subsequently extended as in the following [12] :  The data set is scanned once to determine the support count of each item. Infrequent items are discarded, while the frequent items are sorted in decreasing support counts. For the data set shown in Fig. 2,  a is the most frequent item, followed by b, c, d, and e.

The algorithm makes a second pass over the data to construct the FP-tree. After reading the first transaction, {a,b}, the nodes labeled as a and b are created. A path is then formed from null ?> a ?> b to encode the transaction. Every node along the path has a frequency count of 1.

After reading the second transaction, {b,c,d}, a new set of nodes is created for items b, c, and d. A path is then formed to represent the transaction by connecting the nodes null ?> b ?> c ?> d. Every node along this path also has a frequency count equal to one. Although the first two transactions have an item in common, which is b, their paths are disjoint because the transactions do not share a common prefix.

The third transaction, {a,c,d,e}, shares a common prefix item (which is a) with the first transaction. As a result, the path for the third transaction, null ?> a ?> c ?> d ?> e, overlaps with the path for the first transaction, null ?>a ?> b. Because of their overlapping path, the frequency count for node a is incremented to two, while the frequency counts for the newly created nodes, c, d, and e, are equal to one.

This process continues until every transaction has been mapped onto one of the paths given in the FP-tree. The resulting FP-tree after reading all the transactions is shown at the bottom of Fig. 2.

In this work, we are suggest developing of the FP-Growth data mining algorithm to build up a novel tool to find the association rules called Frequency Pattern-Knowledge Constructions (FP-KC). Mainly for many reasons  First: The size of an FP-tree is typically smaller than the size of the uncompressed data because many records in dataset often share a few items in common.

Second: Given the good results, if all the records have the same set of items, and this point always satisfy in the scientific dataset.

Third: FP-growth is an interesting algorithm because it illustrates how a compact representation of the transaction data set helps to efficiently generate frequent item sets.

Forth: The run-time performance of FP-growth depends on the compaction factor of the data set.

As a result, FP-growth finds all the frequent item sets ending with a particular suffix by employing a divide-and- conquer strategy to split the problem into smaller sub problems.



IV. PRINCIPAL COMPONENTS ANALYSIS   It is not easily to satisfy the relation among the main components of Fig.1 "samples reduction, features reduction and values of feature reduction). But, in this work, we try to establish this relation by using the suitable tools and suggest the FP-KC algorithm to verify this goal. At beginning, we can use analyze the records of database and find the interest components by using PCA. After that, passing the result to FP- KC algorithm to verify these relations.  In order to compute the PCA we can follow the following steps [Dian06]:  Input: Process database  Output: PC-database  ? Step1: Compute the standardized data matrix Z =[Z1,  Z2, . . .

, Zm] base on  Zi = (Xi ? ?i ) /?ii from the original dataset.

? Step2: Compute the Eigen values:  o if B be an m ? m matrix, and let I be the m ? m identity matrix (diagonal matrix with 1?s on the diagonal). Then the scalars (numbers of dimension 1 ? 1) ?1, ?1, . . . , ?m are said to be the Eigenvalues of B if they satisfy |B ? ?I| = 0.

? Step3: Compute Eigenvectors:  o Let B be an m ? m matrix, and let ? be an Eigenvalues of B. Then nonzero m ? 1 vector e is said to be an eigenvector of B if     Be = ?e.

? Step4: Compute ith principal components:  o The ith principal component of the standardized data matrix Z = [Z1, Z2, . . . , Zm] is given by Yi = eTiZ,  ? Step5: End PCA procedure     Where ei refers to the ith eigenvector (discussed below) and eTi refers to the transpose of ei . The principal components are linear combinations Y1, Y2, . . . , Yk of the standardized variables in Z such that (1) the variances of the Yi are as large as possible, and (2) the Yi are uncorrelated. The first principal component is the linear combination Y1 = eT1Z = e11Z1 + e12Z2 + ??? + e1m Zm, which has greater variability than any other possible linear combination of the Z variables. Thus:  ? The first principal component is the linear combination Y1 = eT1Z, which maximizes Var(Y1) = eT1? e1.

? The second principal component is the linear combination Y2 = eT2Z, which is independent of Y1 and maximizes Var(Y2) = eT2? e2.

? The ith principal component is the linear combination Yi = e_iX, which is independent of all the other principal components Yj , j < i , and maximizes var(Yi ) = eTi? ei.

There are several methods to specify a certain association rules. One of these methods depend on finding frequency itemset, such as traditional Apriori, FP-growth are described in rules are given which specifies the attributes that determine membership of the target. While the goal of this part of thesis is not only determining the association rules but also finding a solution of the three dimension reduction at the same time. Therefore, this part combines between the result of PCA and develop FP-growth called FP-KC.



V. A NOVEL TOOL  FP-KC The goal of dimension reduction methods is to use the  correlation structure among the predicator variables to reduce the three main dimensions (features, samples and value of features). But how we can combine among these three dimensions without losing any important information it is still as one of the challenge in KDD [Bara10]. The following points show how the novel tool FP-KC deals with this problem and find new solutions of it. These points relate of the Table 1. The following points can be attaching as new row of that table.

? Tools: FP-KC ? Data set: Multivariate ? Preprocessing: Original PCA, Knowledge  Constructions (eigenvalue criterion, proportion of variance explained criterion and scree plot criterion).

? Result: Set of Association Rules.

The main steps of the propose tool can be describe as follow with the block diagram of FP-KC show in Fig.3.

FP-KC Algorithm Input: Principle Component (PC) Database, Set of knowledge construction, min-sup  Output: set of association rules  ? Step1: Construct the FP-tree  o Scan the PCDatabase  o Collect F, the set of frequent items and their support counts  o Sort F in support count descending order as L, the list of frequent items  o Create the root of an FP-tree and label it as "Null"  ? Step2: Test the Knowledge Constrictions Conditions  For each record in PCdatabase test the KC as follow  o  If the record not verification eigenvalue criterion  Then (remove the record from PCdatabase).

o  Else, the record not verify proportion of variance explained criterion Then (remove the record from PCdatabase).

o  Else, the record not verification The scree plot criterion Then (remove the record from PCdatabase). Else, Goto  Step3.

? Step3: Built FP-KC  o For each record verifying Knowledge Constrictions Conditions  o Select and sort the frequent items in records according to the order of L  o Call insert-Tree procedure  ? Step4: Mining the FP-KC using FP-growth procedure  o If Tree contain single path then  For each combination of nodes CN in path  ? Generate pattern CN U ?  ? Support_count=min_support count of nodes  o Else, if tree contain multi paths  For each nodei in the header of tree  ? Generate pattern CN = nodei U ?  ? Support_count= nodei _support count of nodes  ? Construct CN condition pattern then CN conditional FP-TreeCN  ? If  TreeCN < > ? Then : go to step 4.

? Step5: End FP-KC Algorithm                 Procedure of Insert-Tree  ? Step1:  if Tree has child N Then  o  Set i=i+1  o Else, Create New Node(N)  o i=1; N.link=Tree  o  End if  ? Step2: if remind list< > ? Then  o for j=1 to No. of element  o Call insert-tree(P,N)  o Next j  o End If

VI.  EXPERIMENTS ON THE WATERMARKING DATASET  A. Description  1.Title: IVC-DWT vs DTCWT (Watermarking) Database 2. Sources:  Creators: all examples taken from LIRMM (Laboratoire d'information de Robotique et de    Microelectronique de Montpellier)  Donor: Methaq T. Gataa  Date: 21/6/2011  3. Relevant Information Paragraph:  Problem Description:  Twelve Original images and one hundred twenty distorted images were generated by two watermarking algorithms with five different embedding strength.

Discreet Wavelet Transform (DWT)  Dual Tree Complex Wavelet Transform (DTCWT).

4. Number of Instances: 120  5.. Number of Attributes: 7 - class (one of Mos1, Mos2, Mos3, Mos4 and Mos5) - six features(PSNR, WPSNR, WSNR, SSIM, UQI and  C4) 6. Missing Attribute Values: No 7. Attribute information: IVC database provides MOS values by using Double Stimulus Impairment Scale method with 5 categories (Excellent, Good, Fair, Poor and   Bad)   # Attribute         Description: =========     ============  1-6          six fields are main features  PSNR, WPSNR, WSNR, SSIM, UQI and C4 7          one field represent the class that  take one of five types Mos1, Mos2, Mos3, Mos4 and Mos5  8. Class Distribution: Mos1: 27  (23%) Mos2: 24  (20%) Mos3: 22  (18%) Mos4: 34  (29%) Mos5: 13 (10%)                           Figure 3. The Structure of the Novel Tool FP-KC  955    A. B. Result   First: compute the standardized data matrix Base on the mean and stander division of columns of dataset Zi = (Xi ? ?i ) /?ii.

Second: We can compute Eigenvalues as in Table II.

TABLE II. EIGENVALUES OF THE STANDARDIZED DATASET     The value of Eigenvalues greater than one for a given dataset is mentioned in bold font. This means the features of that Eigenvalues is using in the FP-KC algorithm for a given dataset. Where the number of these features is one from the total six Features. Figure 4 shows the relationship of Eigenvalues and cumulative variability.

Figure 4. Relation of Eigenvalue and Cumulative variability  Third:  we compute Eigenvectors as in Table III.

TABLE III. EIGENVECTORS OF EIGENVALUES MORE THAN ONE   Fourth:  Compute principal components as show in Table IV.

TABLE IV. PRINCIPAL COMPONENTS MATRIX     Fifth: Set the Main Parameters of FP-KC  Remove all the factors not verification eigenvalue criterion ( i.e., the eigenvalue less than one).

Remove all the factors not verification proportion of variance explained criterion ( i.e., the value of Cumulative variability large than 99,068).

Remove all the factors not verification The scree plot criterion ( i.e., the maximum number of components that should be extracted is just prior to where the plot begins to straighten out into a horizontal line.

Set min_sup=2  Set min confidence=95%, where   Support _count is frequency of occurrence of an itemset.

Sixth: Generation the Assoaction Rules base on the Main Split Tree  Rule1: If WPSNR <= 34.194 Then class = 1  Rule2: If WPSNR > 34.194 and WPSNR <= 37.645  and WSNR <= 32,17 Then class = 2  Rule3:If WPSNR > 34.194 and WPSNR <= 37.645  and WSNR > 32,17 and WSNR <= 35.464 Then  class = 1  Rule4: If WPSNR > 34.194 and WPSNR <= 37.645  and WSNR > 35.464 Then class = 2  Rule5:If WPSNR > 37.645 and WPSNR <= 41.699  and WSNR <= 38.139 and UQI <= 0,9065 Then class  = 2     Rule6: If WPSNR > 37.645 and WPSNR <= 41.699  and WSNR <= 38.139 and UQI > 0,9065 and C4 <=  0,910167 Then class = 2  Rule7: If WPSNR > 37.645 and WPSNR <= 41.699  and WSNR <= 38.139 and UQI > 0,9065 and C4 >  0,910167 and C4 <= 0,938965 Then class = 3  Rule8: If WPSNR > 37.645 and WPSNR <= 41.699  and WSNR <= 38.139 and UQI > 0,9065 and C4 >  0,938965 Then class = 2  Rule9: If WPSNR > 37.645 and WPSNR <= 41.699  and WSNR > 38.139 Then class = 2  Rule10: If WPSNR > 41.699 and WPSNR <= 45,02  and UQI <= 0,9685 Then class = 3  Rule11: If WPSNR > 41.699 and WPSNR <= 45,02  and UQI > 0,9685 Then class = 4  Rule12: If WPSNR > 45,02 and UQI <= 0,9855 and  WSNR <= 47.558 Then class = 4  Rule13: If WPSNR > 45,02 and UQI <= 0,9855 and  WSNR > 47.558 Then class = 5  Rule14: If UQI > 0,9855 and WPSNR > 45,02 and  WPSNR <= 46.242 Then class = 4  Rule15: If UQI > 0,9855 and WPSNR > 46.242 Then  class = 5  After the previous six steps, the dataset have 120 record and 7 features become knowledge base have 15 rule and 4 features. Fig.5 show the surface of Watermarking database base on the main three features more important in that database .

Figure 5. Surface of IVC-DWT vs DTCWT (Watermarking) Database    In this work, we are using five huge databases to test the performance of the FP-KC, these databases including (Primate splice-junction gene sequences, Diabetes, DNA, GIS and  Watermarking). The description of these databases and the results of FP-KC explained in Table V. All the association rules generated by a novel tool FP-KC of that datasets explained in appendix. Addition, the Relationship among Eigenvalues, Cumulative and Scree Plot of each database explain in Figures 6, 7, 8, and 9.

Figure 6. Relation of Eigenvalues and Cumulative and Scree Plot of Primate splice-junction gene sequences   Figure 7. Relationship among Eigenvalues, Cumulative and Scree Plot of  Diabetes  TABLE V.THE DESCRIPTION THE FEATURES OF ORIGINAL DATABASES      Figure 8. Relationship among Eigenvalues, Cumulative and Scree Plot of  DNA Database   Figure 9. Relationship among Eigenvalues, Cumulative and Scree Plot of  GIS Database

VI. CONCLUSIONS In This work, we attempt to handle the three dimensions  reduction problem, by proposes cooperation tool between PCA and FP-growth, these tool called FP-KC. The generated FP- KC can get accurate association rules and compress the dataset in three dimensions (number of features" by PCA", number of records and features "by FP- Growth" and value of features through standardization matrix). The confidence' degree of the all association rules yield by FP-KC is equal to 95%.

There are many reasons for developing the FP-Growth data mining algorithm in build up a novel tool FP-KC to find the association rules explained in section two from this paper.

As a result, this work success to satisfy the goal of dimension reduction methods is using the correlation structure among the predicator variables to reduction the three main dimensions (features, samples and value of features). FP-KC is combining between the features of principle component analysis and frequency pattern growth through using three criteria including Eigenvalue, cumulative variability and Scree plot related to PCA as knowledge contractions of FP-growth algorithm.

