Reclassification Rules

Abstract  The ultimate goal of knowledge discovery (KD) is to extract sets of patterns leading to useful knowledge for obtaining user desirable outcomes. The key characteristics of knowledge usefulness is that these patterns are actionable.  In the last decade, KD algorithms such as mining for association rules, clustering, and classification rules,  have made a tremendous progress and have been demonstrated to be of significant value in a variety of real-world data mining applications.  However, the results of the existing methods require to be further processed in order to suggest actions that achieve the desired outcome, by giving only previously acquired data.  To address this issue, we present a novel technique, called Reclassification Rules, to gather all facts, to understand their causes and effects, and to list all potential solutions and the responding effects.

Algorithm, StrategyGenerator-II, is proposed to discover a complete set of reclassification rules which meets pre-specified constraints.

1. Introduction   The ultimate goal of knowledge discovery (KD) is to extract a set of patterns that recommends actions leading to desirable outcomes, by giving only previously acquired data [10].  In the last decade, KD algorithms such as mining for association rules, clustering, and classification rules have made a tremendous progress and have been demonstrated to be of significant value in a variety of real-world data mining applications.  Despite such phenomenal success, these types of rules do not capture some situations.  For example, consider the following application, a bank manager who is monitoring a dataset may want to analyze the data thoroughly to improve their understanding of customers and seek specific actions to improve services such as providing  some additional services to retain their royalty.

Segmentation is one of widely used method to analyze the loyalty [28]; it is very useful to identify customers who have a high probability to move to other banks but it may be insufficient to provide recommendations that might help retain customer?s loyalty.

It is very difficult to model the actionability of the patterns, and this is perhaps the greatest barrier to the progression of the field.  Actionability is very difficult to capture, due to its elusive nature in all of its manifestations and across different types of application and settings [13].   Several papers proposed to model the actionability patterns in either a domain-dependent way or post-process analyzing rules manner [15], [14], [16], [17], [18], [19], [20], [22], [23], [8], [27], [26], [9], [25], [7], [21], and [28].  A Domain dependent approach may work well for the specific application, but it can not be generalized to other application domains.  A post-process analyzing approach is widely adopted and can be implemented in one of the two ways.   First of all, an automatic post-processing algorithm requires a little user knowledge to search on the actionable patterns.  Alternatively, a semi- automatic procedure requires the user to interactively analyze the discovered rules to identify interesting ones.  In general, the first approach is preferred, since users may have difficulty to specify precisely and completely exiting domain knowledge.  Both attempts may fail to unearth some useful action strategies.  In this paper, we also study actionability.  However, unlike existing algorithms, we study it in a pure domain-independent way.

We present a novel action-effect model called Reclassification Rule (RR), [(X,???) ? (Y,???)], to discover actionable relationships among attribute- values in a given data set, where (X,???) is a formulated specific recommendation for actions, (Y,???) is an estimated effect obtained, ? means change, and ? means imply.  It summarizes two populations P1 and P2 having properties {(X,?), (Y,?)}   DOI 10.1109/ICDM.Workshops.2008.88    DOI 10.1109/ICDMW.2008.127     and {(X, ?), (Y, ?)}, respectively, and  P1 can be affected by actions (X,???) toward a higher preferable population P2  (see Figure 1).  Its representation is in a symbolic notation, which is very amenable to inspection and interpretation. This characteristic allows business end users and analysts to understand the underlying action-effect in data, and take actions based upon them.

Reclassification rules mining (RRM) and action  rule mining (ARM) [7], [14], [15], [16], [17], [18], [19], [20], [21], [22] both aim to build an actionable model that allows users to perform actions directly.

The structure of reclassification rules is similar to action rules, however it differs in the following ways.

For RRM, the target of discovery is not pre- determined, while for ARM there is one and only one predetermined target.  RRM aims to discover all the patterns W from a given data set, while ARM discovers only one specific subset of W.  In addition, attributes are not required to be classified as stable, flexible, or decision attributes in reclassification rule mining.

RRM is more general than ARM.  It is important that the space of all actionable patterns is known.  If practical implementation of discovered action rules is too expensive to achieve, an alternative way for searching the space of all patterns to obtain the same reclassification effect is needed.

The algorithm StrategyGeneratorII is proposed to discover reclassification rules directly from a given dataset and it is a two-part process: identifying a set of frequent-factor-sets within the dataset and generating reclassification rules from these factors.

2. Motivation   Modeling actionability in a domain-independent manner is a completely new learning approach that traditional learning methods, such as mining for  classification, clustering, and association rules, are not designed to handle.  In general, a learning method is designed to capture the interesting characteristics of similar objects in the data.  The classical methods can learn rules that summarize the data, but not the rules that change the state of the data [24].  One of the main issues is that most existing methods treat populations separately.  However, each individual population is likely to not be interesting, but a group of them together can embody an important piece of knowledge.

Classification rule mining is used to identify the characteristics of each population and used to predict how a new instance will behave (e.g., C4.5 [12], CN2 [5], classification and regression trees [4]).  Most of the time, a classification rule is represented as ? ? ? , where ? is a conjunction of attribute-value pairs and ? is a class attribute-value.  It has been extensively studies in the fields of statistics, pattern recognition, decision theory, machine learning, etc.   [15], [14], [16], [17], [18], [19], [20], [22], [8], [27], [26] argued that mere classification rules would not be sufficient enough to re-classify some objects from a less desired group to the more desired one.  For example, given a set of historical data describing pregnant women over time, classification rule techniques can be utilized to improve our ability to identify which future patients will be at high-risk of requiring an emergency Cesarean-section delivery.  However, it is insufficient to provide recommendations for preventive actions which might help reduce this risk [10].

Association rule mining focuses on correlations between items in a dataset. It has received considerable attention, since the AIS and Apriori algorithms [1], [2].

An association rule is expressed as X ?Y, where X and Y are sets of items, and it implies that transactions in the database that contain the item in X tend to also contain the item in Y.   This research was initially motivated by the analysis of market basket data to help organization to more fully understand customers? purchasing behavior in order to better target market audiences.  Today, applications include cross- marketing, attached mailing, catalog design, store layout, web usage mining, intrusion detection, text mining, etc.  [9], [ 25], and [28] argued that mere association rules are not directly useful to improve business performance and require further processing.

Let us consider a marketing application, association rules can be used to identify related products from existing products, such as predicting potential cross- sell opportunities of home loans to existing credit card customers.   However, it is difficult to estimate the cross-selling effect and to determine the influence of the marketed items on the sale of the other items.

Figure 1.  By taking action (X,???), population (a set of objects) P1 can join another population P2    (Y,?) (Y,?)  (X,?) (X, ?)  P1 P2     Clustering aims at partition of data into groups of similar objects.   The discovered clusters provide useful information in data mining applications such as scientific data exploration, text mining, spatial database applications, CRM, web mining, marketing, etc.  [3], [29], and [6] argue that mere clustering techniques would not be sufficient enough to understand the differences between groups of clusters and to directly suggest actions for each cluster.

Referring back to the preceding bank customer?s royalty example, clustering techniques can be utilized to group customers into different royalty classes but they are not adequate to provide recommendations for retaining customer?s royalty.

Motivated by the above observations, the new modeling technique is needed to present and capture actionable patterns by analyzing the complex inter- relationships between various populations.  In a way, such patterns are equipped with an insight of how relationships should be managed so that the objects of low performance can be improved to a higher one.    .

3. Reclassification Model  3.1 Information Systems   An information system is used for representing knowledge.  Its definition was proposed in [11].  By an information system, we mean a pair S = (U, A), where:   ? U is a nonempty, finite set of objects, ? A is a nonempty, finite set of attributes,  i.e. a: U?Va  is a function for any a ? A, where Va is called the domain of a.

Elements of U are called objects. In this section, for the purpose of clarity, objects are interpreted as customers.  Attributes are interpreted as features such as, offers made by a bank, characteristics, conditions etc.  Each attribute can be nominal, ordinal, or continuous.  Table 1 shows an example of an information system, which consists of 5 objects described by 4 attributes {b,d,h,g}.

3.2 Factor-set Let F = {f1, f2, ?, fn} be a set of distinct literals,  called factors.  In general, any set of factors is called a factor-set.   A Factor-set F is partitioned into constant factors FS and flexible factors FF.   Let us assume that v1, v2 ? Va. Then (a, v1  v2) is an element of F.   If v1? v2, (a, v1  v2) refers to a flexible factor fF. and it denotes the fact that the value of attribute a can be changed from v1 to v2 for a number of objects in S.

Similarly, the term (a, v1 ? v2)(o) means that the attribute value a(o) = v1 can be changed to a(o) = v2 for the object o.  If v1= v2,  (a, v1  v2) refers to constant factor  fS and its value does not change.

Let us consider a customer table in the telecommunication application. Attribute ?Rate? represents the rate charged to a customer and its domain is {standard, special}, where standard is denoting the normal rate and special denoting the promotional rate.  It has two constant 1-factor-set candidates: (Rate, standard  standard), and (Rate, special  special) and two flexible 1-factor-set candidates: (Rate, standard  special), (Rate, special  standard).  (Rate, standard  special) means that Rate = standard can be changed to Rate = special.

(Rate, special  standard) can be interpreted in a similar way.  Terms (Rate, standard  standard) and (Rate, special  special) simply say that the value of attribute Rate should stay unchanged.

3.3 frequent factor-sets   A factor-set which supports (exceeds) user specified minimum left support threshold s1 ? 0 and right support threshold s2 ? 0 is said to be frequent and it is denoted by FF(s1, s2).

By a frequent factor-set ? in S we mean a term   ?= [(X1, ?1? ?1) ? (X2, ?2 ? ?2) ?? ? (Xp, ?p? ?p) ?(Y1, ?1 ? ?1) ? (Y2, ?2 ? ?2) ? ? ?(Yq, ?q ? ?q)], supL, supR  where all its subterms Zi must also be frequent and supL ? s1, and supR ? s2 .

The Left Support defines the domain of a factor- set which identifies objects in U on which the factor- set can be applied.  The larger its value is, the more interesting the factor-set will be for a user. The left hand side of the factor-set ?,  is defined as the set PL  = { ?1, ?2,?, ?q, ?1, ?2,?, ?p}.  The domain DomS(PL) is a set of objects in S that exactly match PL.

Card[DomS(PL)] is the number of objects in S  that exactly match PL  and Card[U] is the total number of  Objects b d h g  o1 0 2 0 0  o2 0 2 1 0  o3 2 1 2 1  o4 2 3 0 0  o5 2 1 0 1  Table 1. Information system     objects in the information system S.  By the left support supL of the factor-set ?, we mean   supL(?) = Card[DomS(PL)] /Card[U].

The RightSupport shows how well the factor-set is supported by objects in S. The higher its value is, the stronger case of the reclassification effect will be.  The right hand side of the factor-set ?  is defined as  PR = {?1, ?2,?, ?q, ?1, ?2,?, ?p}.   The domain DomS(PR) is a set of objects in S that exactly match PR.

Card[DomS(PR)] is the number of objects in S  that exactly match PR.   By the right support supR of the factor-set ?, we mean   supR(?) = Card[DomS(PR)] /Card[U].

3.4 Reclassification rules   The set of all candidate rules is constructed from frequent factor-sets.   The number of factors in a factor-set will be called the length of the factor-set.  A factor-set of the length k is referred to as a k-factor-set.

For instance, {(Rate, special standard), (CallingCharge, $0.02  $0.05) } is an example of a 2-factor-set.  Each rule extracts through k-factor-set where k is at least two.  A candidate rule which confidence is greater than the user specified minimum confidence c is called a strong rule.  RR(s1, s2, c) is a set of all strong rules.

By a reclassification rule in S we mean an expression   r  = [[(X1, ?1? ?1) ? (X2, ?2 ? ?2) ?? ? (Xp, ?p? ?p)] ? [(Y1, ?1 ? ?1) ? (Y2, ?2 ? ?2) ? ? ?(Yq, ?q ? ?q)]  where, for i= 1, 2,..p and j=1,2, ?q, Xi, Yj? A, and (Xi,?i??i) is called the action (antecedent) and (Yj,?j??j) is called the effect (consequent) of the rule.

We say that objects o1, o2 ? U  support the reclassification rule r  in S, if and only if:  ? (?i ? p) [[Xi (o1) = ?i]  ? [Xi (o2) = ?i]], ? (?j ? q) [[Yj (o1) = ?i] ? [Yj(o2) = ?i]], ? (?i ? p) (?j ? q)[  Xi ? Yj = ?]  Statistical significant of a rule r is called left support and right support and denoted as supL(r) and supR(r), respectively.  Left support determines how often a rule is applicable to a given data set, while right support determines how strong a rule is feasible by objects in a given data set.  Because supports indicate the frequency of the factor-set occurring in the rule, supL(r) and supR(r) are the same as the supports for  the factor-set, i.e., supL(r)=supL(?) and supR(r)=supR(?).  The number of objects in S that match {?1, ?2,?, ?q,} in the left-hand-side of the antecedent of a rule r is denoted as Card[DomS(A-PL)].

On the other side, the number of objects in S that match {?1, ?2,?, ?q} in the antecedent of a rule r is denoted as Card[DomS(A-PR)]. The strength of a reclassification rule r is characterized by its confidence, which is denoted as conf(r).  By the confidence of a reclassification rule r in S, we mean   conf(r) = [Card[DomS(PL)]?Card[DomS(PR)] ? [Card[DomS(A-PL)]? Card[DomS(A-PR)]   The length of a reclassification rule r is defined  as the total number of factors in the antecedent and consequent of r.  A reclassification rule of the length k will be referred to as a k-rule.  A reclassification k-rule will be called shorter than a reclassification m-rule if k < m.  If k > m, a reclassification k-rule will be called longer than a reclassification m-rule.  If k = m, both rules have the same length (the same number of factors).

4. Algorithm and Example   The goal of reclassification rule mining is to discover reclassification rules from a given data set that meet the pre-specified thresholds for minimum support, s1 and s2, and confidence c.  StrategyGeneratorII algorithm is proposed to construct a set of reclassification rules by considering changes of values of attributes used in atomic expressions from which more complex expressions are built.  The algorithm consists of two main steps: (1) Generate frequent factor-sets, i.e., the set of factor-sets that have both supports exceeding predetermined minimum thresholds s1 and s2; (2) generate strong reclassification rules, from the frequent factor-sets, that have confidence exceeding a predetermined minimum threshold c.   This algorithm uses a breadth-first search to build candidate factor-sets efficiently.

Generate frequent factor-sets.  This step computes the frequent factor-sets in the data set through several iterations.  In general, iteration k computes all factor-sets of length k from factor-set of length k-1.  It uses breadth-first bottom-up approach where frequent subsets are extended one factor at a time.  In each iteration we generate new candidates from frequent factor-sets found in the previous iteration; the left support and right support for each candidate is then computed and tested against the user- specified thresholds.  The total number of iterations     needed by the algorithm is kmax+1, where kmax is the maximum size of frequent factor-sets.

In the first phase of the first iteration, the generated set of candidate factor-sets contains all 1- factor-sets. In other words, all pairs of values coming from each attribute are candidates.  An attribute having n distinct values in their domains can generate n2 1- factor-set candidates. To effectively prune the exponential search space of candidates, the anti- monotonic property is applied to filter candidates.  Any 1-factor-set with supports below one of required thresholds will be immediately pruned from the candidate list.  Thus, after the first iteration, all frequent 1-factor-sets will be known.

In the next iteration, 2-factor-set candidates are generated by using only the frequent 1-factor-set that passed the thresholds.  Each candidate is formed by merging a pair of 1-factor-sets that have different attributes.  After support-based pruning is applied, all frequent 2-factor-sets, which satisfied the left support and right support thresholds, are extracted.

In the third iteration, the new 3-factor-set candidates are generated by merging a pair of frequent 2-factor-sets that only vary on the last factor. For example, from [(b, 2 0) (d, 2 1)] and [(b, 2 0) (g, 0 1)], a 3-factor-set candidate, [(b, 2 0) (d, 2 1) (g, 0 1)] can be constructed because both of them have the same first factor, (b, 2 0). Then, if its remaining subset is infrequent, the newly-form factor-set is pruned immediately.  In the above example, if factor- set [(d, 2 1) (g, 0 1)] is frequent, then the newly- formed factor-set [(b, 2 0) (d, 2 1) (g, 0 1)] is frequent too.  The procedure repeats in a similar way after the third iteration. In other words, when k?3, a k- factor-set can be generated by merging a pair of frequent (k-1)-factor-sets only if their first (k-2) factors are identical. And, a candidate pruning step is applied to ensure that the remaining k-2 of the candidates are frequent.  If one of them is infrequent, then the newly- form k-factor-set is immediately pruned.  This approach can effectively reduce generating too many unnecessary or duplicate candidates.  The algorithm terminates when no new frequent factor-sets are generated.

Generate strong reclassification rules. The second phase in discovering reclassification rules is based on all frequent k-factor-sets, which have been found in the first phase.  Like rule generation in association rule mining, each rule is a binary partitioning of a frequent factor-set.   Each frequent k- factor-set can generate up to 2k-2 reclassification rules.

A reclassification rule is extracted by partitioning the factor-set Z into two non-empty subsets X and Y, where (X,Y? Z), (X,Y? FS), and X=Z-Y, such that  X?Y.

Those rules with their left support and right support are identical to supports for Z.  Since building reclassification models is the goal, if a frequent factor- set contains less than two flexible factors (fF), no reclassification rule will be generated from it.

StrategyGeneratorII uses a level-wise approach for generating rules, where each level corresponds to the number of factors belonging to the rule premise.

Unlike the Apriori algorithm, it is only interested on the shortest length of premise, not all the lengths.   It adopts the principle of minimum description length to identify the most general rules.  The rule-premise is extended one factor at a time.  It starts with one factor on the rule-premise, such as X?1-factor-set.  If the rule confidence is above the predetermined threshold, it is called a strong rule and marked with a positive mark.

In the next iteration, it generates new candidates from only unmarked rules found in the previous iteration by moving one of the consequence-type factors to the rule premise, now X?2-factor-set.  Repeat the previous step until there is only one flexible factor on the rule consequent, such as Y?1-factor-set. This way, the discovered information can be presented in a short and clear statement which allows actionable knowledge to be delivered more effectively.

Example. Now, referring back to Table 1, we illustrate the StrategyGeneratorII for the construction of reclassification rules step by step.  The minimum support for Left Support is 21%, and Right Support is 40%, and the minimum confidence for rules is 90%.

4.1 Find frequent factor-sets   The objective of this phase is to find all the factor- sets that exceed the minimum left support and right support thresholds.  These factor-sets are granules and called frequent factor-sets.  StrategyGeneratorII algorithm starts with atomic factor-set for S generated in its first iteration.  In order to create 1-factor-set for each attribute we check its domain and pair all available values.  Referring back to example 1, the values of attribute d are ?1?, ?2?, and ?3? in S.  The recommendations for attribute d are with values either staying constant or requiring some changes.  It begins with forming constant 1-factor-set candidates such as (d, 1 1), (d, 2 2), (d, 3 3) and calculating their supports.  If the support is below the minimum of the left and right support thresholds, it is marked with a negative mark such as (d, 3 3).  Flexible 1-factor-sets are efficiently generated by pairing the values of the constant factor-sets.  Two flexible 1-factor set candidates (d, 1 2) and (d, 2 1) are formed.  The supports of all 1-factor-sets are determined during one pass over the information system S.  All infrequent     candidates are discarded and marked with a negative mark.  For attribute d, the candidate (d, 3 3) is marked with a negative mark, so only 4 frequent 1-factor-set (d, 1 1), (d, 1 2), (d, 2 2), and (d, 2 1) are generated from it.  Following the same procedure for the remaining attributes, 1-factor-sets are constructed.  At the first iteration, 16 candidates are formed and 7 of them are frequent constant 1-factor-sets and 6 are frequent flexible 1-factor-sets are generated.   Its result is listed below.

One-element factor-set loop:  // Granules corresponding to values of attributes   Freq. factor-sets left support  right support Mark // constant 1-factor-set (b, 0 0)          2/5 ? 0.21 2/5 ? 0.40 (b, 2 2)          3/5 ? 0.21 3/5 ? 0.40 (d, 1 1)          2/5 ? 0.21  2/5 ? 0.40 (d, 2 2)          2/5 ? 0.21   2/5 ? 0.40 (d, 3 3)          1/5 < 0.21  1/5 < 0.40     ?-? (h, 0 0)         3/5 ? 0.21   3/5 ? 0.40 (h, 2 2)          1/5 < 0.21   1/5 < 0.40     ?-? (h, 1 1)          1/5 < 0.21  1/5 < 0.40     ?-? (g, 0 0)          3/5 ? 0.21   3/5 ? 0.40 (g, 1 1)          2/5 ? 0.21   2/5 ? 0.40 //flexible 1-factor-set (b, 0 2)         2/5 ? 0.21      3/5 ? 0.40 (b, 2 0)         3/5 ? 0.21  2/5 ? 0.40 (d, 1 2)        2/5 ? 0.21  2/5 ? 0.40 (d, 2 1)          2/5 ? 0.21  2/5 ? 0.40 (g, 0 1)         3/5 ? 0.21  2/5 ? 0.40 (g, 1 0)         2/5 ? 0.21  3/5 ? 0.40   Frequent 1-factor-sets are used to generate  candidate 2-factor-sets.  Each 2-factor-set candidate is formed by concatenating any two unmarked 1-factor- sets that belong to different attributes.  At the second iteration, 60 candidates are formed and only fourteen of them are frequent.  Below is the list of two-element terms.

Freq.t itemsets     left support  right support   Mark  (b,0 0)?(d,1 1)    0/5 < 0.21      ?-? (b,0 0)?(d,2 2)    2/5 ? 0.21     2/5?0.40 (b,0 0)?(d,1 2)    0/5 < 0.21       ?-? (b,0 0)?(d,2 1)    2/5 ? 0.21     0/5 < 0.40       ?-? (b,0 0)?(h,0 0)    1/5 < 0.21                             ?-? (b,0 0)?(g,0 0)    2/5 ? 0.21     2/5 ? 0.40 (b,0 0)?(g,1 1)    0/5 < 0.21                             ?-? (b,0 0)?(g,0 1)    2/5 ? 0.21     0/5 < 0.40        ?-? (b,0 0)?(g,1 0)    0/5 < 0.21             ?-? (b,0 2)?(d,1 2)    0/5 < 0.21        ?-?  (b,2 2)?(d,1 1)    2/5 ? 0.21     2/5 ? 0.40 (b,2 2)?(h,0 0)    2/5 ? 0.21     2/5 ? 0.40 (b,2 2)?(g,1 1)    2/5 ? 0.21     2/5 ? 0.40 (b,0 2)?(d,2 1)    2/5 ? 0.21     2/5 ? 0.40 (b,0 2)?(g,0 1)    2/5 ? 0.21     2/5 ? 0.40 (b,2 0)?(d,1 2)    2/5 ? 0.21     2/5 ? 0.40 (b,2 0)?(g,1 0)    2/5 ? 0.21     2/5 ? 0.40 (d,1 1)?(g,1 1)    2/5 ? 0.21     2/5 ? 0.40 (d,2 2)?(g,0 0)    2/5 ? 0.21     2/5 ? 0.40 (d,1 2)?(g,1 0)    2/5 ? 0.21     2/5 ? 0.40 (d,2 1)?(g,0 1)    2/5 ? 0.21     2/5 ? 0.40 (h,0 0)?(g,0 0)    2/5 ? 0.21     2/5 ? 0.40  ...                                                                   ?-?   In order to avoid generating too many unnecessary or duplicate candidates, a k-factor-set will be formed by merging a pair of frequent (k-1)-factor- sets only if their (k-2) factors are identical.  When the subsets of the newly-form factor-set are frequent, its supports will be calculated.  In our example, [(b, 0 2) (d, 2 1)] and [(b, 0 2) (g, 0 1)] are merged to form a candidate [(b, 0 2) (d, 2 1) (g, 0 1)].  Next, [(d, 2 1) (g, 0 1)] is frequent, so [(b, 0 2) (d, 2 1) (g, 0 1)] is qualified for counting its supports.  Its left support and right support exceed the minimum thresholds, so it is a frequent 3-factor-set. On the other hand, [(b,2 2)(d,1 1)(h,0 0)] is formed but its remaining subset [(d,1 1)(h,0 0)] is infrequent, so the factor-set is pruned immediately.  Let us look at another situation: for [(b, 0 2) (d, 2 1)] with [(d, 2 1) (g, 0 1)], the first factor in both factor-sets is different, so no candidate will be generated.   Below is the list of three-element terms.

Freqrent factor-Sets            L.Sup.  R. Sup.   Mark (b,0 0)?(d,2 2)?(g,0 0) 2/5?0.21 2/5? 0.4 (b,2 2)?(d,1 1)?(h,0 0)                                  ?-? (b,2 2)?(d,1 1)?(g,1 1) 2/5?0.21 2/5? 0.4 (b,2 2)?(h,0 0)?(g,1 1) 2/5?0.21 2/5? 0.4 (b,0 2)?(d,2 1)?(g,0 1) 2/5?0.21 2/5? 0.4 (b,2 0)?(d,1 2)?(g,1 0) 2/5?0.21 2/5? 0.4   There are only five frequent 3-factor-sets and  their first two elements are not identical, no 4-factor- set will be generated.  Before the rule extraction, we need to check whether all the frequent factor-sets are qualified to generate rules - at least two elements must be flexible factors (fF) in each factor-set. If a factor-set does not satisfy this requirement, it is not used for rule generation.  In this example, eleven disqualified factor- sets such as [(b,0 0)(d,2 2)], [(b, 0 0)(g,0 0)], [(b,2 2)(d,1 1)], ? etc. are identified.

4.2 Extract Reclassification Rules   The main idea of the reclassification goal is to move objects from an undesirable group into a more desirable one.  Stable atomic factors can not be solely used either in the rule consequent (right-hand-side) or in the rule premise (left-hand-side) for rule generation.

Each rule without at least one flexible atomic factor on both sides is meaningless.  We only generate rules from k-factor-set when k ? 2.  Below is the set of frequent factor-sets that will be used to generate reclassification rules.

Frequent Factor-Sets (b,0 2)?(d,2 1)               2/5, 2/5 (b,0 2)?(g,0 1)                  2/5, 2/5 (b,2 0)?(d,1 2)                  2/5, 2/5 (b,2 0)?(g,1 0)               2/5, 2/5 (d,1 2)?(g,1 0)                  2/5, 2/5 (d,2 1)?(g,0 1)                  2/5, 2/5 (b,0 2)?(d,2 1)?(g,0 1)  2/5, 2/5 (b,2 0)?(d,1 2)?(g,1 0)  2/5, 2/5   For frequent 2-factor-sets, the rule generation is simple and straightforward by partitioning a factor-set into two portions and forming two possible rules.  In our example, factor-set [(b, 0 2) (d, 2 1)] can generate two candidate rules: r1=[(b, 0 2)? (d, 2 1)] and r2=[(d, 2 1)?(b, 0 2)].  Next, we check whether they are strong rules.  Let us consider r1, its confidence c is computed as c = [supL((b, 0 2) ?(d, 2 1))  ? supR((b, 0 2) ?(d, 2 1))] ? [supL(b, 0 2) ? supR(b, 0 2)].  Its value 0.667 is below the threshold, so it is not a strong rule.  For this factor-set, only one strong rule is found, r2=[(d, 2 1)? (b, 0 2)].  Following the same procedure for the other frequent 2-factor sets, their corresponding rules are generated and they are listed below.

(b, 0 2)?(d, 2 1), 2/5, 2/5 Reclassification Rule    Confidence           Mark  (b,0 2)?(d,2 1)    (2?2)/(2?3) < 0.90 (d,2 1)?(b,0 2)    (2?2)/(2?2) ? 0.90    ?+?   (b, 0 2)?(g, 0 1), 2/5, 2/5  (b,0 2)?(g,0 1)    (2?2)/(2?3) < 0.90 (g,0 1)?(b,0 2)    (2?2)/(2?3) < 0.90   (b, 2 0)?(d, 1 2), 2/5 , 2/5  (b,2 0)?(d,1 2)    (2?3)/(2?2) < 0.90 (d,1 2)?(b,2 0)    (2?2)/(2?2) ? 0.90    ?+?   (b, 2 0)?(g, 1 0), 2/5, 2/5  (b,2 0)?(g,1 0)    (2?3)/(2?2) < 0.90  (g,1 0)?(b,2 0)    (2?2)/(2?3) < 0.90   (d, 1 2)?(g, 1 0), 2/5, 2/5 (d,1 2)?(g,1 0)   (2?2)/(2?2) ? 0.90     ?+? (g,1 0)?(d,1 2)   (2?2)/(2?3) <0.90   (d, 2 1) ? (g, 0 1), 2/5, 2/5 (d,2 1)?(g,0 1)    (2/2)?(2/2) ? 0.90      ?+? (g,0 1)?(d,2 1)    (2/3)?(2/2) < 0.90   For 3-factor-sets, initially, all the strong rules  that have only one factor in the rule premise are executed.  These rules are marked with a positive mark and are not used for generating new rule which have one more element in the rule premise.  For factor-set [(b, 0 2) (d, 2 1) (g, 0 1)], one strong rule r4 is generated in the first loop, so it will be not considered to generate rules at the second loop.  r3 and r4 are not strong rules, then candidate rule r6 is generated by merging the premises of both rules.

(b, 0 2) (d, 2 1) (g, 0 1), 2/5,  2/5  Loop 1: // for illustration, each rule is labeled.

r3:(b,0 2)?(d,2 1)?(g,0 1) (2?2)/(2?3) < 0.9 r4:(d,2 1)?(b,0 2)?(g,0 1) (2?2)/(2?2)?0.9 ?+? r5:(g,0 1)?(d,2 1)?(b,0 2) (2?2)/(3?2)?0.9  Loop 2:  r6:(b,0 2)?(g,0 1)?(d,2 1) (2?2)/(2?2)?0.9 ?+?   (b, 2 0) (d, 1 2) (g, 1 0), 2/5, 2/5 (b,2 0)?(d,1 2)?(g,1 0)  (2?3)/(2?2)< 0.9 (d,1 2)?(b,2 0)?(g,1 0)  (2?2)/(2?2)?0.9 ?+? (g,1 0)?(b,2 0)?(d,1 2)  (2?2)/(2?3)<0.9 (b,2 0)?(g,1 0)?(d,1 2)  (2?2)/(2?2)?0.9 ?+?   In our example, nine strong reclassification rules are discovered and listed in Table 2.

Rule Left Sup.

Right Sup. Conf.

(d,2 1)?(b,0 2)                     2/5 2/5 100% (d,1 2)?(b,2 0)                  2/5 2/5 100% (d,1 2)?(g,1 0)                  2/5 2/5 100% (d,2 1)?(g,0 1)                  2/5 2/5 100% (d,2 1)?(b,2 0)?(g,0 1)   2/5 2/5 100% (b,0 2)?(g,0 1)?(d,2 1)   2/5 2/5 100% (d,2 1)?(g,0 1)?(b,2 0)   2/5 2/5 100% (d,1 2)?(b,2 0)?(g,1 0)   2/5 2/5 100% (b,2 0)?(g,1 0)?(d,1 2)   2/5 2/5 100%  Table 2. Set of Reclassification Rules      StrategyGeneratorII algorithm examines the data in an objective way and represents the discovered information in a short and clear statement. Discovered rules are summarizations of opportunities for actions to boost up ones business performance.

5. Conclusion   In this paper, we introduced Reclassification Rules to address the problem of mining a dataset for all actionable patterns, and presented an efficient algorithm, StrategyGeneratorII for this purpose.  The reclassification rule framework has several desirable features: (1) the workable strategy obtained from StrategyGeneratorII is intuitively interpretable in terms of work required to transform one segment of objects to another.  (2) The framework allows comparison of specific parts of two groups.  This makes it possible to focus attention on interesting changes that might not significantly affect the standard model.  In the future work, we intend to optimize the set of actions for a better actionability.

