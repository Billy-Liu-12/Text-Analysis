TSINGHUA SCIENCE AND TECHNOLOGY ISSNll1007-0214ll04/09llpp160?173

Abstract: The Extreme Learning Machine (ELM) and its variants are effective in many machine learning applications  such as Imbalanced Learning (IL) or Big Data (BD) learning. However, they are unable to solve both imbalanced  and large-volume data learning problems. This study addresses the IL problem in BD applications. The Distributed  and Weighted ELM (DW-ELM) algorithm is proposed, which is based on the MapReduce framework. To confirm the  feasibility of parallel computation, first, the fact that matrix multiplication operators are decomposable is illustrated.

Then, to further improve the computational efficiency, an Improved DW-ELM algorithm (IDW-ELM) is developed  using only one MapReduce job. The successful operations of the proposed DW-ELM and IDW-ELM algorithms are  finally validated through experiments.

Key words: weighted Extreme Learning Machine (ELM); imbalanced big data; MapReduce framework; user-defined  counter  1 Introduction  Machine Learning (ML), as a technique for effectively predicting and analyzing data[1], has been widely used for information retrieval, medical diagnosis, and natural language understanding. Many ML algorithms have been investigated, including the  ? Zhiqiong Wang and Shuo Tian are with the Sino- Dutch Biomedical & Information Engineering School, Northeastern University, Shenyang 110169, China. E-mail: wangzq@bmie.neu.edu.cn; dyhswdza@sina.com.

? Junchang Xin, Hongxu Yang, and Ge Yu are with  the School of Computer Science & Engineering, Northeastern University, Shenyang 110169, China. E- mail: xinjunchang@cse.neu.edu.cn; 443577693@qq.com; yuge@cse.neu.edu.cn.

?Chenren Xu is with the School of Electronics Engineering and  Computer Science, Peking University, Beijing 100871, China.

E-mail: chenren@pku.edu.cn.

?Yudong Yao is with the Department of Electrical and  Computer Engineering, Stevens Institute of Technology, Castle Point on Hudson Hoboken, NJ 07030, USA. E-mail: yyao@stevens.edu.

?To whom correspondence should be addressed.

Manuscript received: 2016-08-27; revised: 2017-01-14; accepted: 2017-01-18  Extreme Learning Machine (ELM)[2?12], which has the least human intervention, high learning accuracy, and fast learning speed. However, raw data with imbalanced class distributions, which is known as the Imbalanced Learning (IL) problem, can be found almost everywhere[13], and the existing ML algorithms including ELM cannot resolve this problem effectively. Some methods, such as undersampling[14] and oversampling[15], can rebalance the data distributions, following which standard ML algorithms can be applied. Recently, Weighted ELM (WELM)[16] has been proposed to improve the ELM algorithm for the IL problem by assigning different weights to different samples/classes in the training dataset.

Although ML algorithms, including ELM and WELM, are effective for various applications, their implementation in Big Data (BD) applications still has many challenges[17] because the data volume to be stored and analyzed greatly exceeds the storage and computing capacity of a single machine. One feasible approach is to scale up the corresponding algorithms using parallel or distributed architectures[18]. Several distributed ELMs (e.g., PELM[19], ELM?[20], DK- ELM[21], E2LM[22], and A-ELM?[23]) based on the    Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 161  MapReduce framework[24?28] have been proposed to improve the scalability of the ELM algorithm to resolve the problem of BD learning. However, these algorithms did not consider the IL problem.

A centralized WELM has been proposed to manage the imbalanced class distribution; however, it processes big datasets inefficiently. Some Distributed ELMs (DELMs) have been proposed to resolve the problem of BD learning, but they are powerless to imbalanced data. Therefore, in this study, considering both the large volume of data and imbalanced training samples, we proposed a Distributed and Weighted ELM (DW-ELM).

If the algorithm, which is similar to the DELM, is directly adopted, then two steps in the matrix multiplication need to be calculated when solving the matrices HTWH and HTWT, that is, two MapReduce jobs. The matrix A D HTW would be calculated in the first MapReduce job, and then matrix AH or AT would be calculated in the second MapReduce job. Here A is an L ?N dimensional big matrix, and the amount of data transferred between the two jobs is very large; therefore, the training efficiency is very low. We skillfully apply the parameter W as a diagonal matrix; then, the DW-ELM algorithm is proposed such that matrices HTWH and HTWT can be calculated efficiently in a single MapReduce job, which reduces the amount of data transmission effectively and improves the learning efficiency. Therefore, the DW-ELM algorithm is not a simple combination of DELM and WELM but solves a more complex problem using three matrix multiplications rather than two matrix multiplications of the DELM, which is the core contribution in this study.

This study addresses the issue of IL in BD applications. A DW-ELM based on WELM and the distributed MapReduce framework is proposed. We prove that the most expensive computational part in the WELM algorithm is decomposable and can be computed in parallel. The contributions of this study are as follows. First, we prove that the matrix multiplication operators in the centralized WELM algorithm are decomposable, which indicates that its scalability can be improved by MapReduce. Second, a DW- ELM algorithm based on the MapReduce framework is proposed to learn the imbalanced BD efficiently.

Finally, an Improved DW-ELM (IDW-ELM) algorithm is proposed to further improve the computational performance.

The rest of the study is organized as follows. Section 2 briefly reviews the background for our work. The  theoretical foundation and computational details of the proposed DW-ELM and IDW-ELM algorithms are introduced in Section 3. The experimental results to show the effectiveness of the proposed algorithms are reported in Section 4. Finally, Section 5 concludes this study.

2 Background  In this section, we describe the background for our work, which includes a brief overview of the WELM algorithm[16] and a detailed description of the distributed MapReduce framework[24?26].

2.1 Weighted ELM[16]  For N arbitrary distinct training samples .xi ; ti /, where xi D ?xi1; xi2; ? ? ? ; xin?T 2 Rn and ti D ?ti1; ti2; ? ? ? ; tim?  T 2 Rm, the ELM can resolve the following learning problem:  H??? D T (1)  where ??? D ????1; ? ? ? ; ???L?T is the output weight matrix between the hidden and output nodes, T D ?t1; ? ? ? ; tN ?T  are the target labels, H D ?hT.x1/; ? ? ? ;hT.xN /?T, where h.xi / D ?g1.xi /; ? ? ? ; gL.xi /? are the hidden node outputs, and gj .xi / is the output of the j -th hidden node for input xi .

To maximize the marginal distance and minimize the weighted cumulative error with respect to each sample, an optimization problem is mathematically written as  Minimize W  k???k  C CW    NX iD1  k??? ik  Subject to W h.xi /??? D tTi ? ??? T i  (2)  where C is the regularization parameter to represent the trade-off between the minimized weighted cumulative error and maximized marginal distance. ??? i is the training error of sample xi caused by the difference of the desired output ti and the actual output h.xi /??? . Here W is an N ? N diagonal matrix associated with every training sample xi and  Wi i D 1=#.ti / (3) or  Wi i D  ( 0:618=#.ti /; if #.ti / > AVGI 1=#.ti /; if #.ti / 6 AVG  (4)  where #.ti / is the number of samples belonging to class ti and AVG is the average number of samples per class.

According to the Karush-Kuhn-Tucker theorem[29], the following solutions for the WELM can be obtained:  ??? D  ? I ? CHTWH  ??1 HTWT (5)    162 Tsinghua Science and Technology, April 2017, 22(2): 160?173  when N ? L, or  ??? D HT ?  I ? CWHHT  ??1 WT (6)  when N ? L.

2.2 MapReduce framework[24?26]  MapReduce is a simple and flexible parallel programming model initially proposed by Google for large scale data processing in a distributed computing environment[24?26], with Hadoop (http: //hadoop.apache.org/) being one of its open source implementations. The typical procedure of a MapReduce job is illustrated in Fig. 1. First, the input to a MapReduce job starts as the dataset stored on the underlying distributed file system (e.g., Google File System (GFS)[30] and Hadoop Distributed File System (HDFS)[31]), which is split into several files across machines. Next, the MapReduce job is partitioned into many independent map tasks. Each map task processes a logical split of the input dataset. The map function takes a key-value pair .k1; v1/ as input and generates intermediate key-value pairs ?.k2; v2/?. Next, the intermediate data files from the already completed map tasks are fetched by the corresponding reduce task following the ?pull? model (similarly, there are many independent reduce tasks). The intermediate data files from all the map tasks are sorted accordingly and passed to the corresponding reduce task. The reduce function combines all intermediate key-value pairs .k2; ?v2?/ with the same key into the output key-value pairs ?.k3; v3/?. Finally, the output data from the reduce task is generally written back to the corresponding distributed file system.

3 Distributed and Weighted Extreme Learning Machine  In this section, we present the DW-ELM and IDW-ELM algorithms. To implement the parallel computation of  split 0  split 1  split 2  split 3  map ()  map ()  map ()  map ()  split 4 map ()  reduce ()  reduce ()  reduce ()  part  0  part  1  part  2  Input  Map tasks Reduce tasks OutputShuffle  Fig. 1 Illustration of the MapReduce framework.

the WELM algorithm using the MapReduce framework, we first need to prove the matrix decomposability in the WELM algorithm.

3.1 Decomposability of the WELM algorithm  In imbalanced BD learning applications, the number of training records is much larger than the number of hidden nodes, that is, N ? L. Therefore, the size of HTWH is much smaller than that of WHHT. Equation (5) is used to calculate the output weight vector ??? in WELM. We first analyze the properties of the WELM algorithm and formulate the matrix operations in the form of the MapReduce framework. In this way, we can extend the WELM algorithm for BD applications.

Let U D HTWH and V D HTWT. Thus, we obtain  ??? D  ? I ? C U  ??1 V (7)  Following the matrix operations, we have Eq. (8).

Then, we further obtain  uij D  NX kD1  Wkk ? g.wi ? xk C bi / ? g.wj ? xk C bj /  (9) Similarly, following the matrix operations, we obtain  Eq. (10).

Finally, we obtain Eq. (11).

vij D  NX kD1  Wkk ? g.wi ? xk C bi / ? tkj (11)  According to Eq. (9), we know that the item uij in matrix U can be expressed by the summation of Wkk ? g.wi ? xk C bi / ? g.wj ? xk C bj /. Here Wkk is the weight of the training sample .xk; tk/, and hki D g.wi ? xk C bi / and hkj D g.wj ? xk C bj / are the i -th and j -th elements in the k-th row h.xk/ of the hidden layer output matrix H, respectively.

Similarly, according to Eq. (11), we know that item vij in matrix V can be expressed by the summation of Wkk ? g.wi ? xk C bi / ? tkj . Here tkj is the j -th element in the k-th row tk of matrix T that is related to the training sample .xk; tk/.

The variables involved in the equations for the matrices U and V include Wkk , hki , hkj , and tkj . According to Eqs. (3) and (4), to calculate the corresponding weight Wkk related to the training sample .xk; tk/, we must first obtain the number #.tk/ of training samples that belong to the same class as tk . The number of training samples in all classes can be easily calculated in one MapReduce job. At the same time, the remaining three variables, namely hki , hkj , and tkj , are    Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 163  U D HTWH D266664 h.x1/ h.x2/ :::  h.xN /  T 266664  W11 0 ? ? ? 0  0 W22 ? ? ? 0  0 0 : : : 0  0 0 ? ? ? WNN   h.x1/ h.x2/ :::  h.xN /  377775 D  h h.x1/T h.x2/T ? ? ? h.xN /T  i W11 0 ? ? ? 0  0 W22 ? ? ? 0  0 0 : : : 0  0 0 ? ? ? WNN   h.x1/ h.x2/ :::  h.xN /  377775 D h.x1/TW11h.x1/C h.x2/TW22h.x2/C ? ? ? C h.xN /TWNNh.xN / D  NX kD1  h.xk/TWkkh.xk/ D  NX kD1  g.w1 ? xk C b1/ g.w2 ? xk C b2/  :::  g.wL ? xk C bL/  377775Wkk h g.w1 ? xk C b1/ g.w2 ? xk C b2/ ? ? ? g.wL ? xk C bL/  i D  NX kD1  Wkk  g.w1 ? xk C b1/ g.w2 ? xk C b2/  :::  g.wL ? xk C bL/  h g.w1 ? xk C b1/ g.w2 ? xk C b2/ ? ? ? g.wL ? xk C bL/  i (8)  V D HTWT D266664 h.x1/ h.x2/ :::  h.xN /  T 266664  W11 0 ? ? ? 0  0 W22 ? ? ? 0  0 0 : : : 0  0 0 ? ? ? WNN   t1 t2 :::  tN  377775 D  h h.x1/T h.x2/T ? ? ? h.xN /T  i W11 0 ? ? ? 0  0 W22 ? ? ? 0  0 0 : : : 0  0 0 ? ? ? WNN   t1 t2 :::  tN  377775 D h.x1/TW11t1 C h.x2/TW22t2 C ? ? ? C h.xN /TWNN tN D NX  iD1  h.xk/TWkk tk D  NX kD1  g.w1 ? xk C b1/ g.w2 ? xk C b2/  :::  g.wL ? xk C bL/  377775Wkk h tk1 tk2 ? ? ? tkm  i D  NX kD1  Wkk  g.w1 ? xk C b1/ g.w2 ? xk C b2/  :::  g.wL ? xk C bL/  h tk1 tk2 ? ? ? tkm  i (10)  only related to the training sample .xk; tk/ itself and not the other training samples; therefore, the calculation of the U and V matrices can be accomplished in another MapReduce job.

To summarize, the calculation process of the U and  V matrices is decomposable; therefore, we can realize the parallel computation of these matrices using the MapReduce framework to break through the limitation of a single machine and improve the efficiency in which the WELM algorithm learns the imbalanced big training    164 Tsinghua Science and Technology, April 2017, 22(2): 160?173  data.

3.2 DW-ELM algorithm  The process for the DW-ELM algorithm is shown in Algorithm 1. First, we randomly generate L pairs of hidden node parameters .wi ; bi / (lines 1 and 2). Then, we use a MapReduce job to count the number of training samples contained in each class (line 3). Next, we use another MapReduce job to calculate the U and V matrices according to the input parameters and randomly generate the parameters (line 4). Finally, we solve the output weight vector ??? [according to Eq. (5) (line 5)].

We describe the specific processes of the two MapReduce jobs involved in the DW-ELM algorithm.

The process for the first MapReduce job of the DW- ELM algorithm is shown in Algorithm 2. The algorithm includes two classes, Mapper (lines 1?10) and Reducer (lines 11?16). Class Mapper contains three methods: Initialize (lines 2 and 3), Map (lines 4?7), and Close (lines 8?10), whereas Class Reducer only contains one method, namely Reduce (lines 12?16). In the Initialize method of the Mapper class, we initialize one array c, which is used to store the intermediate summation of the training samples contained in each class (line 3). In the Map method of the Mapper class, first, we analyze the training sample s and resolve the class to which sample s belongs (lines 5 and 6). Then, we adjust the corresponding value in the array c (line 7). In the Close method of the Mapper class, the intermediate summations stored in c are emitted by the mapper (lines 9 and 10). In the Reduce method of the Reducer class, first, we initialize a temporary variable sum (line 13).

Next, we combine the intermediate summations of the different mappers that have the same Key and obtain the final summation of the corresponding element of the Key (lines 14 and 15). Finally, we store the results in a distributed file system (line 16).

The process for the second MapReduce job of the DW-ELM algorithm is shown in Algorithm 3. In the Initialize method of the Mapper class, we initialize two arrays, u and v, which are used to store the  Algorithm 1 DW-ELM 1 for i D 1 to L do 2 Randomly generate hidden node parameters .wi ; bi / 3 Calculate all #.tk/ using Algorithm 2 4 Calculate U D HTWH, V D HTWT using Algorithm 3 5 Calculate the output weight vector ??? D .I=?C U/?1 V  Algorithm 2 First MapReduce Job of the DW-ELM Algorithm 1 class MAPPER 2 method INITIALIZE() 3 c D new ASSOCIATIVEARRAY 4 method MAP(sid id, sample s) 5 t DParseT.s/ 6 num DClass.t/ 7 c?num? D c?num?C 1 8 method CLOSE() 9 for i D 1 to c:Length() do  10 context.write(cid i , count c?i ?) 11 class REDUCER 12 method REDUCE(cid id, counts ?c1; c2; : : : ?) 13 sum D 0 14 for all count c 2 ?c1; c2; : : : ? do 15 sum D sumC c 16 context.write(cid id, count sum)  Algorithm 3 Second MapReduce Job of the DW-ELM Algorithm 1 class MAPPER 2 method INITIALIZE() 3 u D new ASSOCIATIVEARRAY 4 v D new ASSOCIATIVEARRAY 5 method MAP(sid id, sample s) 6 h D new ASSOCIATIVEARRAY 7 .x; t/ D ParseAll.s/ 8 w DWeight(Counts?Class.t/?) 9 for i D 1 to L do  10 h?i ? D g.wi ? xC bi / 11 for i D 1 to L do 12 for j D 1 to L do 13 u?i; j ? D u?i; j ?C w ? h?i ? ? h?j ? 14 for j D 1 to m do 15 v?i; j ? D v?i; j ?C w ? h?i ? ? t?j ? 16 method CLOSE() 17 for i D 1 to L do 18 for j D 1 to L do 19 context.write(triple (0U 0, i , j ), sum u?i; j ?) 20 for j D 1 to m do 21 context.write(triple (0V 0, i , j ), sum v?i; j ?) 22 class REDUCER 23 method REDUCE(triple p, sum ?s1; s2; : : : ?) 24 uv D 0 25 for all sum s 2 ?s1; s2; : : : ? do 26 uv D uv C s 27 context.write(triple p, sum uv)  intermediate summations of the elements in the U and V matrices, respectively. In the Map method of the Mapper class, first, we initialize a local variable h (line 6). Then, we resolve the input training sample s and divide s into the training feature x and its corresponding training result t (line 7). Again, according to the training    Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 165  result t and the result of Algorithm 2, we obtain the corresponding weightw of s (line 8). Next, we calculate the corresponding hidden layer output vector h.x/ (lines 9 and 10). Finally, we separately calculate the local summations of the elements in the U and V matrices and save the result to local variables u and v (lines 11?15). In the Close method of the Mapper class, the intermediate summations stored in u and v are emitted by the mapper (lines 17?21). In the Reduce method of the Reducer class, first, we initialize a temporary variable uv (line 24). Next, we combine the intermediate summations of the different mappers that have the same Key and obtain the final summation of the corresponding element of the Key (lines 25 and 26).

Finally, we store the results in the distributed file system (line 27).

The DW-ELM algorithm requires two MapReduce jobs, where job#1 is used to count the number of samples, # .tk/, of each tk category in m categories; therefore, the total amount of data that job#1 needs to transmit in the Map phase is the total number of training samples multiplied by the number of Mapper.

Assuming the number of Mappers is NMapper. Then, we can obtain  costbasic1 D sizeof .Integer/ ?m ?NMapper (12)  Job#2 is used to calculate the HTWH and HTWT matrices. Therefore, the total amount of data to be transferred to job#2 is the U and V matrix size multiplied by NMapper in the Map phase. Assuming that the number of hidden layer nodes is L, the number of categories ism, and the U and V matrices are L?L and L ?m, respectively, we further obtain  costbasic2 D sizeof .float/ ? .L ? LC L ?m/ ?NMapper (13)  and costbasic D  costbasic1 C costbasic2 D sizeof .Integer/ ? C ?NMapperC sizeof .float/ ? .L ? LC L ?m/ ?NMapper (14)  3.3 IDW-ELM algorithm  The DW-ELM algorithm requires two MapReduce jobs to complete the training process. If we can merge the above two MapReduce jobs into a single MapReduce job, the performance of the DW-ELM algorithm can be significantly improved. In addition to providing several built-in counters, the MapReduce framework allows users to employ their own user-defined counters.

Using a user-defined counter, users can transfer some statistical information between the Mapper and Reducer classes. The task of the first MapReduce job in the DW-ELM algorithm is to count the number of training samples contained in each class; however, we propose an IDW-ELM algorithm based on user-defined counters, using only one MapReduce job to complete the tasks. The framework is shown in Fig. 2.

The process for the MapReduce job of the IDW- ELM algorithm is shown in Algorithm 4. The Initialize method of the Mapper class is similar to Algorithm 3.

In the Map method of the Mapper class, first, we initial a local variable h (line 6). Then, we resolve the training sample s, which means dividing s into the training feature x and its corresponding training result t (line 7). Again, we resolve the class that sample s belongs to according to the training result t, and adjust the value of the corresponding user-defined counter (lines 8 and 9). Next, we calculate the hidden layer output vector h.x/ corresponding to x (lines 10 and 11). Finally, we separately calculate the local summations of the elements in the U and V matrices and save the result to local variables u and v (lines 12?16) (the results of different class need to be stored respectively, so u and v are 3-dimensional arrays.). In the Close method of the Mapper class, the intermediate summations stored in u and v are emitted by the mapper (lines 18? 23). In the Reduce method of the Reducer class, first, we initialize a temporary variable uv (line 25). Next, we calculate the corresponding weight according to the statistical information stored in the user-defined counters to combine the intermediate summations of the different mappers that have the same Key and obtain the final summation of the corresponding element of the Key (lines 27?29). Finally, we store the results in the distributed file system (line 30).

With the IDW-ELM algorithm, only one MapReduce job is required using a user-defined counter to complete the calculation; local accumulation of u and v need to be computed separately in each classifier.

The costimproved1 variable is the amount of counter transmission. Hence, we can obtain  costimproved D costimproved1Ccostimproved2 D sizeof .Integer/ ? C ?NMapperC  sizeof .float/ ?C ?.L ? LC L ?m/ ?NMapper (15)  Assuming that the network bandwidth is B , the    166 Tsinghua Science and Technology, April 2017, 22(2): 160?173  Fig. 2 The framework of the IDW-ELM algorithm.

transmission times of the DW-ELM and IDW-ELM algorithms are, respectively, Tbasic D costbasic=B D  sizeof .Integer/ ? C ?NMapper ? BC  sizeof .float/ ? .L ? LC L ?m/ ?NMapper ? B (16)  and Timproved D costimproved  ? B D  sizeof .Integer/ ? C ?NMapper ? BC  sizeof .float/ ? C ? .L ? LC L ?m/ ?NMapper ? B  (17) If the start-up time of the MapReduce job is Tstart  and if Timproved ? Tbasic < Tstart, then the performance  of the IDW-ELM algorithm is better than that of the DW-ELM algorithm; else Timproved ? Tbasic > Tstart and the performance of the IDW-ELM algorithm is lower than that of the DW-ELM algorithm. Through the analysis of the communication cost of the DW-ELM and IDW-ELM algorithms, we can see that when C is large, the communication cost of the IDW-ELM algorithm will be far greater than that of the DW-ELM algorithm. Under normal circumstances, the number of data classifications is usually small and the network bandwidth is large; therefore, the performance of the IDW-ELM algorithm in the experiment is better than that of the DW-ELM algorithm.

Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 167  Algorithm 4 The IDW-ELM Algorithm 1 class MAPPER 2 method INITIALIZE() 3 u D new ASSOCIATIVEARRAY 4 v D new ASSOCIATIVEARRAY 5 method MAP(sid id, sample s) 6 h D new ASSOCIATIVEARRAY 7 .x; t/ D ParseAll.s/ 8 c D Class.t/ 9 context.getCounter(?Classes?, ?c?+c).increment(1)  10 for i D 1 to L do 11 h?i ? D g.wi ? xC bi / 12 for i D 1 to L do 13 for j D 1 to L do 14 u?c; i; j ? D u?c; i; j ?C h?i ? ? h?j ? 15 for j D 1 to m do 16 v?c; i; j ? D v?c; i; j ?C h?i ? ? t?j ? 17 method CLOSE() 18 for k D 1 to u:Length./ do 19 for i D 1 to L do 20 for j D 1 to L do 21 context.write(triple (0U 0, i , j ), value (k, u?k; i; j ?)) 22 for j D 1 to m do 23 context.write(triple (0V 0, i , j ), value (k, v?k; i; j ?)) 24 class REDUCER 25 method REDUCE(triple p, values ?s1; s2; : : : ?) 26 uv D 0 27 for all sum s 2 ?s1; s2; : : : ? do 28 count = context.getCounter(?Classes?, ?c? + s:get-  Class()).getValue() 29 uv D uvCWeight(count)?s:getValue() 30 context.write(triple p, sum uv)  4 Results and Performance Evaluation  In this section, the performances of our proposed DW- ELM and IDW-ELM algorithms are evaluated in detail by considering various experimental settings. We first describe the platform used in our experiments in Section 4.1. Then, we present and discuss the experimental results.

4.1 Experimental platform  All the experiments are run in a cluster with nine computers connected in a high-speed Gigabit network.

Each computer has an Intel Quad Core 2:66GHz CPU, 4GB memory, and CentOS Linux 5:6 operating system.

One computer is set as the master node and the others are set as the slave nodes. We use Hadoop version 0:20:2 and configure it to run up to 4 map tasks or 4 reduce tasks concurrently per node. Therefore, at any point in time, at most, 32 map tasks or 32 reduce tasks  can run concurrently in our cluster.

In this experiment, the synthetic data that was  randomly generated in the ?0; 1?d data space are used to verify the efficiency and effectiveness of our proposed DW-ELM and IDW-ELM algorithms. The generation procedure of the experimental data is as follows. First, the data center of each class is generated randomly; second, the data of each class are generated according to the multivariate Gaussian distribution, where the former generated center point is used as the mean, whereas the reciprocal of the number of classes is the variance.

Finally, the generated data of all classes are combined into the experimental data. The real-life datasets with yeast, glass, ecoli, pima, and colon are downloaded online. The imbalance ratio of the dataset can be as low as 0.0304, and the highest imbalance ratio occurs at 0.6667. However, both the aduit and banana datasets are from UCI, where the imbalance ratios are 0.3306 and 0.8605, respectively.

Table 1 summarizes the parameters used in our experimental evaluation. In each experiment, we vary a single parameter while setting all others to their default values. The imbalance ratio which quantitatively measures the imbalance degree of a dataset is defined as follows[16]:  Imbalance ratio D Min.#.ti //=Max.#.ti // (18) The DW-ELM algorithm is a MapReduce-based  distributed implementation of the original or centralized WELM algorithm, and it does not change the formulation in the WELM algorithm. Therefore, it does not have any impact on the classification accuracy.

We evaluate the training time and speedup of the DW-ELM and IDW-ELM algorithms. The speedup achieved by an m-computer mega system is defined as  Speedup(m) D Computing time on 1 computer  Computing time on m computers (19)  Table 1 Experimental parameters.

Parameter Range Default Number of nodes 1, 2, 3, 4, 5, 6, 7, 8 8 Dimensionality 10, 20, 30, 40, 50 50  Number of hidden nodes 100, 150,  200, 250, 300  Number of records 9?106, 12?106, 15?106, 18?106,  21?106 21?106  Number of classes 5, 10, 15, 20, 25 15  Imbalance ratio 0.1, 0.2, 0.3,  0.4, 0.5, 0.6, 0.7 0.5    168 Tsinghua Science and Technology, April 2017, 22(2): 160?173  4.2 Experimental results  Table 2 describes the accuracy of the ELM?[20] and DW-ELM algorithms under the real-life datasets. From Table 2, the DW-ELM algorithm outperforms the ELM?[20] algorithm.

The next experiment verifies the DW-ELM algorithm training time and the acceleration ratio under the synthetic datasets. First, the impact of the number of slave nodes in the cluster on the running time of the ELM?[20], DW-ELM, and IDW-ELM algorithms is discussed. As shown in Fig. 3a, the training times for the ELM?[20], DW-ELM, and IDW-ELM algorithms decrease significantly with increasing the number of slave nodes in the cluster, and the training time of the ELM?[20] algorithm is always lower than that of the DW-ELM and IDW-ELM algorithms. Moreover, the IDW-ELM algorithm is always lower than that of the DW-ELM algorithm. The various speedup ratios, through changing the number of slave nodes, are shown in Fig. 3b, and the speedup ratio closely follows a linear growth trend. When the number of slave nodes increases, the number of map and reduce tasks that can be executed simultaneously also increases, which means the amount of work that can be completed simultaneously. Therefore, on the premise of the same amount of work, the training time for the ELM?[20], DW-ELM, and IDW-ELM algorithms decreases with increasing number of slave nodes.

Second, we investigate the impact of the training data dimensionality. As shown in Fig. 4a, with the increase of training data dimensionality, the training times of the ELM?[20], DW-ELM, and IDW-ELM algorithms all increase slightly, while the training time of the ELM?[20] algorithm is significantly lower than that of the DW-ELM and IDW-ELM algorithms.

Also, the training time of the IDW-ELM algorithm is  Table 2 Performance results of the ELM?[20] and DW-ELM algorithms.

Datasets (imbalance ratio) Testing result (%)  ELM* DW-ELM Yeast (0.0304) 80.14 94.22 Glass (0.1554) 93.99 95.47 Ecoli (0.1806) 91.09 93.81 Aduit (0.3306) 72.87 80.76 Pima (0.5350) 70.52 71.81 Colon (0.6667) 84.78 85.16  Banana (0.8605) 87.98 88.04       T im  e (s  )                             S pe  ed up  r at  io  Number of slave nodes  Number of slave nodes  (a)  Running time  (b)  Speedup ratio  *ELM IDW-ELM DW-ELM  *ELM IDW-ELM DW-ELM  Fig. 3 The impact of the number of slave nodes on (a) time and (b) speedup ratio.

always lower than that of the DW-ELM algorithm. The variation trend of speedup ratio of three algorithms is shown in Fig. 4b, where the speedup ratios of three algorithms remain stable with the change of the data dimensionality, and the algorithm performance of the IDW-ELM algorithm is slightly better than that of the DW-ELM algorithm. Increasing the training data dimensionality leads to the running time for calculating the corresponding row hk of the hidden layer output matrix H in Mapper to slightly increase; thus, the training times of the DW-ELM and IDW- ELM algorithms also slightly increase. The DW-ELM algorithm uses two MapReduce jobs to complete the calculation of the U and V matrices, but the IDW-ELM algorithm only uses one MapReduce jobs to complete the same calculation. Although, compared to the second MapReduce job of the DW-ELM algorithm, the amount of middle data transmission slightly increased, but its transmission time is far less than the time required by    Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 169  (a)  Running time  (b)  Speedup ratio  10 20 30 40 50        T im  e (s  )  Dimensionality  DW-ELM IDW-ELM  *ELM  10 20 30 40 50         Dimensionality  S pe  ed up  r at  io  *ELM IDW-ELM DW-ELM  Fig. 4 The impact of dimensionality on (a) time and (b) speedup ratio.

the first MapReduce job of the DW-ELM algorithm.

Therefore, the training time of the IDW-ELM algorithm was significantly lower than that of the DW-ELM algorithm, illustrating the effectiveness of the proposed improved algorithm.

Again, we investigate the impact of the number of hidden nodes. As shown in Fig. 5a, by increasing the number of hidden nodes, the training times of the ELM?[20], DW-ELM, and IDW-ELM algorithms all increase, while the training time of the IDW-ELM algorithm is significantly lower than that of DW-ELM algorithm. In addition, the ELM?[20] algorithm is lower than that of IDW-ELM algorithm. The various of speedup ratios through changing the number of hidden nodes is shown in Fig. 5b, and the speedup ratio remains stable. Increasing the number of hidden nodes leads to the dimensionality of the hidden layer output matrix H to increase, and indirectly leads to the increase of the dimensionality of the intermediate U and V matrices.

This not only makes the computation time of the local  100 150 200 250 300       T im  e (s  )  Number of hidden nodes    DW-ELM IDW-ELM  *ELM  100 150 200 250 300         Number of hidden nodes  S pe  ed up  r at  io  *ELM IDW-ELM DW-ELM  (a)  Running time  (b)  Speedup ratio  Fig. 5 The impact of the number of hidden nodes on (a) time and (b) speedup ratio.

accumulated sum of U and V increase, but also makes the transmission time of the intermediate results in the MapReduce job increase. Therefore, the training times for the DW-ELM and IDW-ELM algorithms increase with the number of hidden nodes. The DW- ELM algorithm uses two MapReduce jobs while the IDW-ELM algorithm uses only one MapReduce jobs to complete the calculation of the U and V matrices.

Overall, the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm, which verifies the validity of the improved algorithm.

Then, we investigate the impact of the number of training records. As shown in Fig. 6a, with the increasing number of records, the training times of the ELM?[20], DW-ELM, and IDW-ELM algorithms all increase, obviously. While the training time of the IDW-ELM algorithm is significantly lower than that of the DW-ELM algorithm and the ELM?[20]  algorithm is significantly lower than that of the IDW- ELM algorithm. The impact of increasing the number    170 Tsinghua Science and Technology, April 2017, 22(2): 160?173                   T im  e (s  )  Number of records (?10 )  Number of records (?10 )  DW-ELM IDW-ELM  *ELM          S pe  ed up  r at  io  *ELM IDW-ELM DW-ELM  (a)  Running time  (b)  Speedup ratio  Fig. 6 The impact of the number of records on (a) time and (b) speedup ratio.

of training records on the speedup ratio is shown in Fig. 6b. Although the speedup ratio does not achieve a linear change, but the speedup ratio of the ELM?[20]  algorithm is slightly higher than that of the DW-ELM and IDW-ELM algorithms. Increasing the number of records means that the numbers of Mapper and Reducer which need to be launched increase. On the other hand, it increases the number of corresponding locally accumulated summation of U and V that need to be transmitted, leading to increased transmission time of the intermediate results. Therefore, the training times of the DW-ELM and IDW-ELM algorithms increase with the increasing number of training records. In addition, the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm, which further verifies the validity of the improved algorithm.

Next, we investigate the impact of the number of classes. As shown in Fig. 7a, along with the increasing number of classes, the training times of the ELM?[20]  and DW-ELM algorithms are basically stable, while  5 10 15 20 25       T im  e (s  )  Number of classes  DW-ELM IDW-ELM  *ELM  5 10 15 20 25         S pe  ed up  r at  io  Number of classes  *ELM IDW-ELM DW-ELM  (a)  Running time  (b)  Speedup ratio  Fig. 7 The impact of the number of classes on (a) time and (b) speedup ratio.

the training time of the IDW-ELM algorithm is slightly increased, and it is always less than the training time of the DW-ELM algorithm. The influence of changing the number of classes on speedup ratio is shown in Fig. 7b, and the speedup ratio remains stable. The number of classes increases, which only increases the number of statistical values in the first MapReduce job and subsequently the number of input values in the second MapReduce job of the DW-ELM algorithm, which has limited impact on the overall training time, so the training time is relatively stable. However, the increase in the number of classes makes the number of output intermediate results of the Mapper class in the IDW- ELM algorithm increase, and to a certain extent, this leads to the transmission time of the intermediate results to increase, so the training time increases slightly. In addition, the training time of the IDW-ELM algorithm is less than that of the DW-ELM algorithm, which means that although the effects of the improved algorithm decrease slightly with the increase in the number of    Zhiqiong Wang et al.: Distributed and Weighted Extreme Learning Machine for Imbalanced Big Data Learning 171  classes, the optimization effect is still good.

Finally, we investigate the impact of imbalance ratio.

As shown in Fig. 8a, with the increasing imbalance ratio, the training times of the ELM?[20], DW-ELM, and IDW-ELM algorithms are basically stable, and the training time of the IDW-ELM algorithm is always lower than that of the DW-ELM algorithm. Also, the training time of the ELM?[20] algorithm is lower than that of the IDW-ELM algorithm. The variation in speedup ratio of changing the imbalance ratio of the training records is shown in Fig. 8b, and the speedup ratio remains stable. Increasing the imbalance ratio did not produce any substantial effects on the calculation process of the MapReduce job, so the training time is relatively stable. Since the training time of the IDW-ELM algorithm is less than that of the DW- ELM algorithm, this further verifies the validity of the improved algorithm.

From the results of the six groups, the training time of the ELM?[20] algorithm is less than that of the DW-  0.1 0.2 0.3 0.4 0.5 0.6 0.7       T im  e (s  )  Imbalance ratio  DW-ELM IDW-ELM  *ELM  0.1 0.2 0.3 0.4 0.5 0.6 0.7         S pe  ed up  r at  io  Imbalance ratio  *ELM IDW-ELM DW-ELM  (a)  Running time  (b)  Speedup ratio  Fig. 8 The impact of the imbalance ratio on (a) time and (b) speedup ratio.

ELM algorithm in each group of experiments. The reason is because in the DW-ELM algorithm training process, the number of samples in each category needed to be first counted, that is, calculate the diagonal elements of the weight matrix W in the Map phase, then the HTH and HTT matrices were calculated in the MapReduce calculation; while only HTH and HTT matrices needed to be counted in the Map phase of the ELM?[20] algorithm. However, the G-mean value of the ELM?[20] algorithm in managing the unbalanced data was significantly lower than that of the DW-ELM algorithm, which apparently was not applicable to the imbalance BD learning.

5 Conclusion  Neither the WELM nor the DELM algorithms manage the imbalanced big training data efficiently since they only consider either the ?big? or the ?imbalanced? aspect of imbalanced big training data, and not both. In this study, we proposed a DW-ELM algorithm based on the MapReduce framework. The DW-ELM algorithm makes full use of the parallel computing ability of the MapReduce framework and realizes efficient learning of imbalanced BD. Specifically, through analyzing the characteristics of the WELM algorithm, we found that the matrix multiplication operators (i.e., HTWH and HTWT) within the WELM algorithm are decomposable. Then, we transformed the corresponding matrix multiplication operators into summation forms, which suited the MapReduce framework well, and proposed a DW- ELM algorithm to calculate the matrix multiplications using two MapReduce jobs. Furthermore, an IDW- ELM algorithm, which only uses one MapReduce job to complete the tasks, was proposed to improve the performance of the DW-ELM algorithm. Finally, in the cluster environment, we performed a detailed validation of the performance of the DW-ELM and IDW-ELM algorithms with various experimental settings. The experimental results show that the DW-ELM and IDW- ELM algorithms can learn imbalanced BD efficiently.

