A MULTI-MECHANISM RULE-EXTRACTION  PIPELINE FOR USE ON UNANNOTATED DATASETS

Abstract: We outline a hybrid methodology- incorporating both supervised and unsupervised-learning components-for rule-based knowledge discovery from unannotated data ie when the classification information is unknown. The motivation for our work stems from the individual effectiveness of various data mining mechanisms ie: (1) class identification via unsupervised datavector cluster formation, (2) datavector simplification and feature selection via attribute discretisation, and (3) symbolic rule extraction via the association of symbolic rules with the structural parameters of a trained neural network (NN).

The basic operational concept involves the pipelined application of various unsupervised and supervised mechanisms ie: (1) k-means, (2) Chi-2, (3) local cluster (LC) network training, and (4) rule extraction from a trained LC network. The featured methodology will be tested and analysed using several well-known datasets.

Keywords: Rule extraction, clustering, discretisation, supervised learning, unsupervised learning  1. INTRODUCTION  Knowledge representation in the form of symbolic rulesets are appealing in a data mining context primarily due to their simplicity and unambiguity, and has long been central to research in artificial intelligence (AI) and knowledge engineering. The formulation of such conceptually clean descriptions from real-life data is, however, complicated by the almost inevitable occurrence of data heterogeneity and incompleteness; respectively resulting from datavectors being composed from combinations of symbolic, discrete and continuous-valued attributes; and datasets including noise, statistically anomalous values and missing attributes.

In addition, classification information pertaining to individual datavectors is often unavailable-or at best available only in some imprecise rule-of-rhumb form-in most data mining applications. The research described in this paper is motivated by the necessity to address such fundamental realities; hence the proposed combinative application of various supervised and unsupervised learning mechanisms so as to enable rule-based knowledge extraction under some fairly generic application scenarios.

The presented rule extraction framework is a hybridised multi-stage application of clustering, feature selectioddiscretisation and NN based rule extraction  mechanisms; as will be outlined individually and in coordination with one another.

11. OVERVIEW AND ANALYSIS OF DATA ANALYTIC MECHANISMS EMPLOYED  2.1  An  K-Means Datavector Clustering  unannotated dataset is simply an undifferentiated collection of multi-component datavectors S = {xi : i E [l, n]} , for which the classification attribute c(xi)=a for a E [ I ,  k] (with k << n presumed) is unknown. Note the intentional suppression of the vector- component index, which will be dealt with in the following section on feature discretisation. Our methodology assumes the possibility of deducing the value of classification attribute from information intrinsic within the datavector itself, which is usually a reasonable assumption for most datasets with ordinal or continuous-valued components. A dataset satisfying this presumption can be represented by k datavector clusters, with cluster membership in our case being determined via the k-means clustering algorithm. This well-known algorithm allows for the division of the dataset into cluster-sets ie S = US,, with each distinct cluster Sa = {xla): c(xi (a )  ) =a, i E [l, n,]} characterised by:-  a  c Xl"' Mean (centre-ofimass): p = -  na  Variance (radius) : = a  Average pair-wise distance (diameter):  with the geometric analogues provided by Zhang- Ramakrishnan-Livny [4].

0-7803-6355-8/00/ $10.00 @ 2000 IEEE 1-382    Dataset clustering via k-means is initiated via the random assignment of all datavectors to the presumed set of constituent clusters, after which the cluster-specific attributes [py 0, 61 are computed. Each subsequent computational round would then reassign the  a  cluster membership for each xi E S such that Ixf?? - pj is minimised for the fresh a value. A set-wide error  E=C/Xra)-pl is also computed after each round, I  which can be regarded as the merit criteria associated with the cluster representation of the dataset. The k-means clustering process is then usually continued until cluster membership stabilises or the global error drops below some specified threshold.

The popularity of k-means in data mining applications is due (in large part) to the basic procedural simplicity, which frequently lends itself to many variations ie tree-like hierarchical clustering as suggested by [4]. The set-cluster representation is, on the other hand, non-deterministic (due to the random initial membership) and dependant on k ie the externally specified cluster multiplicity. There are two diametrically opposed strategies to deal with this issue ie:-  Over-estimation in the initial k value followed by progressive amalgamation of clusters (thereby resulting in a reduced k) with closely-separated centres as  defined by the comparison of lpa-pJ with (a, 6) values for both clusters Under-estimation in the initial k value followed by progressive partitioning of loose clusters (thereby resulting in an increased k) as defined by large (0, 6) values  both of which (in common to basic k-means) would also require multiple iterations to select the best cluster representation ie that resulting in the lowest global error. It should, however, be pointed out that cluster multiplicity can often be deduced for many data mining problems; as would justifiably be the case for both featured datasets ie the widely analysed Breast-Cancer and Irises, which respectively have two (benign and malignant) and three (known Iris sub-species) classification values.

2.2 Chi-2 Attribute Discretisation  Discretisation of ordinal or continuous-valued datavector components is motivated by the desirability of a discrete- valued or binarised input attribute representation for the subsequent rule-extraction phase. Attribute discretisation procedures can be characterised as being supervised or unsupervised-respectively applied on datavectors for which the classification attribute is known and unknown- the former of which y e  considered to be far more compute- efficient. The featured Liu-Setiono [2] Chi-2 algorithm  assumes the successful computation of c(ZIa?) = a via the k-means algorithm, following which supervised attribute discretisation can be executed on each component of multidimensional datavector Zfa? = [ - , xy, a . . ] .

Chi-2 is also considered to be a dynamic refinement of its static predecessor Chi-merge formulated by Kerber, which generates a discretised attribute representation from the progressive pairwise merging of adjacent attribute-value intervals with the lowest significance level as indicated by  (a)  ay? and ej (a) =-nj ni,i+i respectively denoting the actual ni,i+i  (measured) and expected frequency of a-classified datavectors in the j-th attribute interval. The expectation value e?) would in turn be established from:-  J  Total datavector frequency in the j-th interval:  nj = Cay? Paired-interval datavector frequency for a-classified  datavectors: nfJ:l = ay? Total paired-interv?al datavector frequency:  ni,i+i = C nj = C n:;il  a  i+l  J=I  i+l  j=i  Interval pairing via Chi-merge would commence from an initial association of intervals with individual datavector attribute-values, and would continue until all paired-interval x? values exceed an externally specified significance level. Discretisation down to a single interval (encompassing all attribute-values) frequently occurs for at least some of the datavector components, the occurrence of which indicates the insignificance of that particular component for classification purposes. Each classification- significant component would subsequently be representable via discretised vector &Zi) = [ . - a ,  d(Xi), e . . ]  with d(Xi)E {-.-, pj , -..} the set of interval-specific values as represented by the mean of all resultant i-th component values within the j-th interval.

(i)  Chi-2 essentially entails iterated executions of Chi-merge with a dynamic global significance level so as hold set-wide classification inconsistency-arising &om the indistinguishable discretisations d($??) = d($) of datavectors with unequal classification values a # p- below an externally specified limit. Attribute discretisation would also occur in two phases featuring:-  1-383    (1) Multi-component attribute discretisation up to  maximum vector-wide x2 value maximum component-specific' x values (2) Individual component attribute discretisation up to 2  both of which are required to comply with a prespecified inconsistency limit. In [7] and [8], Chi-2 discretisation has been applied in mining rules from NN and rough sets-based methods. Note discretisation results in both dimensional (via identification of classification-insignificant components) and numerical (due to d(z i )  = d(f2.) for i f j )  dataset reduction. This effect would be significant for many data mining applications, as demonstrated for the featured Breast-Cancer and Irises datasets. Chi-2 execution in these cases results in a 15-25 % dimensional and 50-65 % numerical reduction, both of which would simplify the subsequent rule-extraction process.

J  2.3 Local Cluster Training and Rule Extraction  A generalised symbolic description of an annotated dataset can be obtained from a variety of supervised NN training and rule extraction methodologies, as systematically classified by the Andrews-Diederich-Tickle (ADT) taxonomy [9].  The proposed framework features the Rulex procedure formulated by Andrews-Geva (AG) [3] for symbolic rule extraction on LC networks, which feature localised Radial Basis Function (RBF) like activation functions constructed using sigmoidal building blocks. The basic attraction of the featured methodology stems from the relatively fast-compared to the more widely used Multi- Layer Perceptron (MLP) networks with globalised sigmoidal activation-LC parametric convergence during training and also the natural manner in which rules are computed from trained LC network parameters. The latter is in notable contrast to the majority of NN-based rule extraction frameworks with distinct rule extraction processes executed after satisfactory completion of NN training.

The featured NNs are composed of sigmoid pairs  pi (x) = (l+e-ki(xi-ci+bi)r - (l+e-kiGi-ci-bi)r restricted to a single dimension in a multi-dimensional datavector Z = [ .  ., xi, ...I, with i used as the component dimension (as opposed datavector) index. Note the localised functionality ie pi (xi) E 0 far away-as defined by the exponential suppression ki -from the interval [ci - bi, ci + bi] with centre C, and breadth bi . These unidimensional pairs are subsequently  combined by a sigmoidally-activated output node of form  g ( ~ )  =( l+e-K( TPi(Xi)-n]) ; with each individual  pi (retroactively interpretable as a hidden node) localised in the i-th component, but contributing a non-localised ridge-like projection in the other datavectors components [... , xj, - - .] (with j # i). The x parameter is intended to cancel-out all such ridges resulting from c p . ,  thereby  resulting in the sigmoidal superposition being localised near multi-dimensional interval [E - 6, E + 61 . Output node activation would therefore only occur in that interval, with x being dependant on datavector dimensionality and K determining the abruptness of the activation interval.

Restricted LC training would entail the progressive reestimation of free parameters [ E ,  6, E] so as to maximise gradient descent with respect a set-wide error of  form E = Z  q f2(j))--a(J) , with j the hitherto suppressed datavector index and a some extemally- specified (in our case via datavector clustering) classification value.

. I  j I (  *It  The AG-proposed Rulex [3] formalism utilises the localised activation of the q node in terms of constituent pi nodes, such that class membership is established for datavectors in [x(-), f2(+)], with f2(') the lower and upper activation thresholds. This is interpretable in terms of rules of form (if xi E [xi-', x?] then a = l), with threshold values xi - ci k r y  intended as a more precise definition of the neighbourhood around classification centre C, .

Symbolic rule formulation would therefore require the straightforward usage of the post-training [ci , bi , ki] to neighbourhood radius equation (I), with m. = min{pi (xi)} the minimum activation value associated with the i-th hidden node. This value can in turn be straightforwardly established via the relation  (*) -  with  max{pi (xi)} =pi (ci), K a fixed output node parameter and T an externally specified output activation threshold.

1-384    (k-means)  attr  I I In layer Sup NN  (Restricted hidden layer I  . I FiP 1 :  Rule Extraction Architecture  Rule antecedents of conjunctive (AND-connected) form n&, E [x:-?, xr?]) can therefore be straightforwardly I  constructed, with disjunctions (OR-connections) indicated by the presence of multiple rules pertaining to the same classification attribute.

111. DESCRIPTION OF HYBRIDISED FRAMEWORK  3.1 Multi-Mechanism Architecture  The proposed rule-extraction framework would feature sequential application of the following processes: ( I )  Unsupervised cluster formation: k-means (2) Supervised attribute discretisation: Chi-2 (3) Supervised NN training: restricted LC networks (4) Rule extraction from trained NN: Rulex  as illustrated in Fig 1 above, which would (in an actual operational environment) be preceded by a filter stage for the execution of routine preprocessing tasks ie attribute scaling (to mitigate against numerically large datavector components being disproportionately influential), and the elimination of incomplete (from a descriptive viewpoint) or unlikely (via identification of statistical outliers) datavectors. Note the usage of the computed classification attribute from the first processing stage in the next two, eventually resulting in a trained LC network and symbolic ruleset for each distinct classification attribute cx E [ l ,  k].

This enables the usage of the featured framework in support of data mining applications where the classification is a priori unknown.

We are currently investigating the effectiveness of the proposed framework on datasets with a mixture of categorical and ordinal/continuous-valued datavector  attributes, which would constitute an important generalisation over purely ordinalkontinuous-valued datavectors. The categorical datavector components can be handled separately and introduced directly into the NN training stage, however this assumes (without much justification) that computation of classification attribute primarily depends on the ordinakontinuous components.

A more well-founded approach would entail class computation from the categorical components (for instance using methods based on rough set [6] analysis), in parallel with the cluster formation process for the ordinaYcontinuous components. Classification information obtained from independant analysis of categorical and ordinaVcontinuous attributes would subsequently have to be integrated, following which various downstream analytic processes (as previously indicated) can be executed.

Sensitivity analysis [ 1 I]-based on computations of first and second derivatives of the classification error in trained NNs-has been demonstrated as being effective in determining the relative importance (with respect classification) of individual attributes, and would conceivably be useful in such a capacity.

3.2 Experimental Data  The Breast-Cancer and Iris datasets (both obtained from UCI machine learning repository) were chosen due to all their datavector components being ordinal/continuous- valued, with the respective characteristics tabulated below (Table 1).

1-385    Table 1 :  Dataset Characteristics We are currently evaluating the effectiveness of the presented framework on other larger and more complex datasets, and hope to report the results thereof in a subsequent publication.

4. CONCLUDING REMARKS  The provided classification information is required for an evaluation of the k-means process, which is approximately 90-95 % accurate for both datasets. Following this, the Chi-2 procesmxecuted with a 1 % inconsistency threshold-is observed to have the following effects (Table 2) on the respective datasets.

The discretised datavectors and the cluster-assigned classification attributes are then divided into training and test datasets, the former of which is used to generate the trained LC network required for symbolic rule extraction.

Subsequent execution of the Rulex process would result in 2 symbolic rules for Breast-Cancer and 5 for Irises after straightforward elimination of redundant (repeated) or irrelevant (inapplicable with respect test dataset) rules. The classification performance of these rulesets when applied to the test datasets is indicated below.

Note the attainment of 85 YO and 70 % classification accuracy for Breast-Cancer and Irises respectively, the former of which is probably operationally acceptable. It should, however, be pointed out that two of the three Irises classifications-ie the versicolour and virginica sub- species, respectively class values 1 and 2 in Table 3 below-are relatively difficult to distinguish from each other but straightforwardly distinguished from the setosa sub-species (class 0), as reported in an earlier publication [ 101 featuring analysis using Kohonen Self-Organising Maps (SOM). We also noticed a disproportionately significant reduction in setosa and virginica datavectors after attribute discretisation, which is also noteworthy due to the resultant accuracy of setosa classification. If we are allowed to take this dataset-specific oddity into account then the test classification accuracy is an exemplary 100 %.

Tnhle ?4. Test Classification  The proposed sequential application of classification via datavector clustering, feature selection and data simplification via discretisation and lastly knowledge extraction via NN training and symbolic rule generation; appears to be a kndamentally sound methodology for the analysis of unannotated datavectors with ordinal or continuous-valued attributes. Such attribute values would be a natural consequence of instrumentalised data collection, and as such would constitute an important subclass of data mining applications. A truly generic data mining fiamework should, however, be able to handle both categorical and ordinallcontinuous attributes on an equivalent basis; and we anticipate this being an interesting line of research to undertake.

5. REFERENCES  [ 11 Lton Bottou and Yoshua Bengio, ?Convergence Properties of the K-Means Algorithms?, Proc. of 7* Conf. on Neural Information Processing Systems, Denver, USA, 1994.

[2] Huan Liu and Rudy Setiono, ?Chi2: Feature Selection and Discretization of Numeric Attributes?, Proc. 7th Intelligence, Washington D.C., 1995.

[3] Robert Andrews and Shlomo Geva, ?Rule Extraction from Local Cluster Neural Nets?, submitted to Neurocomputing, February, 2000.

[4] Tian Zhang, Raghu Ramakrishnan, and Miron Livny, ?BIRCH: An Efficient Data Clustering Method For Large Databases?, Proc. of 1996 ACM-SIGMOD Int.

Conf. on Management of Data, Montreal, Quebec, 1996.

[5] James Dougherty, Ron Kohavi and Mehran Sahami, ?Supervised and Unsupervised Discretization of Continuous Features?, Proc. Machine Learning the 12* Int. Conf., 1995.

Introduction to Rough Sets?, Proc. of ECAI 98 Workshop on Synthesis of Intelligent Systems from Experimental Data, 1998.

[6] Jan Komorowski and Aleksander 0hm, ?An  1-386    [7] Ismail A. Taha and Joydeep Ghosh, ?Symbolic Interpretation of Artificial Neural Networks?, IEEE Trans. Knowledge and Data Engineering, Vol. 1 1, No.3, pp. 448-463, May/June 1999.

[8] Xiaohua Hu and Nick Cercone, ??Learning Maximal Generalized Decision Rules via Discretization, Generalization and Rough Set Feature Selection?, Proc. 9th Int. Conf. on Tools with Artificial Intelligence (TA1 ?97), 1997.

[9] Alan B. Tickle, Robert Andrews, Mostefa Golea and Joachim Diederich, ?The Truth Will Come To Light: Directions and Challenges in Extracting the Knowledge Embedded Within Trained Artificial Neural Networks?, IEEE Trans Neural Networks Vo1.9, No.6, pp.1057-1068, 1998.

[IO] Alwyn Goh, Hoe Kok Meng and Jagdesh Singh, ?Hybrid Kohonen Multi-Layer Perceptron Neural Network System for Extracting Symbolic Classification Rules from Unannotated Datasets?, Proc. Comp Sc and Info Tech Conf, Confed of Scientific and Technological Associations in Malaysia (COSTAM)., Penang, Malaysia, 1998.

[ 1 11 Jingtao Yao, Nicholas Teng, Hean-Lee Poh, and Chew Lim Tan, ?Forecasting and Analysis of Marketing Data Using Neural Networks?, Journal of Information Science and Engineering, Vol. 14,  [ 121 Dorian Pyle, Data Preparation for Data Mining, San Francisco, CA: Morgan Kaufmann. 1999.

[ 131 John A. Hartigan, Clustering Algorithms, New York: John Wiley & Sons. 1975.

[ 141 Simon Haykin, Neural Networks: A Comprehensive Foundation, 2?d Ed., New Jersey, Prentice Hall Inc., 1999.

pp. 843-862, 1998.

