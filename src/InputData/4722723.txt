Rough Set Page Recommendation Algorithm Based  on Information Entropy*

Abstract?In order to solve the accuracy and speed problems of page recommendation, we used information entropy of data in positive domain to help finding out the rough set rules. A new rough set important attribute mining algorithm based on information entropy is put forward in this paper, according to which a heuristic rules mining algorithm of web page recommendation is designed. At last we used the recommendation rules to help web users to find out pages which are interested to them. The experiment results show that the algorithm can improve speed and accuracy of web page recommendation.

Keywords-rough set; page recommendation;Information Entropy;rule mining

I.  INTRODUCTION With the development of web technology, web pages are on  the rise. How to find useful information from mass of pages is an important challenge. Web log analysis is widely used in mining web usage patterns, including statistical analysis, frequency analysis of page views, classification method based on neural network [1], finding out common access path [2], mining of association rules [3,4], and so on. Most of the data mining algorithms deal with raw web log data directly [5], but log data is often incomplete and there are many uncertainties [6], so the rules found out from these data are not so accurate.

Z. Pawlak put forward rough set theory to address uncertainty of data in 1982, [7].Our rules mining algorithm is based on rough set, so that it could effectively solve the uncertainty  of web page recommendation rules. With the development of rough set theory, more and more people began to use rough set to reduce attributes and discover rules. People want to find the best attribute reduction [8]. However Wong.SKM and Ziarko.W have proved that it is a NP-hard problem, which leading to NP-hard problem is mainly due to an explosion combination of attributes, therefore there is no efficient attribute reduction algorithm. It makes rules mining slowly and can not achieve the speed requirements of real time web page recommendation. Klemttinen introduced the concept of template rules, and pointed out that this method can better identify the requirements of the association rules and the implementation of cooperation recommendation. It is found that this method of mining rules from a large number of data has higher time complexity [9, 10]. To solve real time website  recommendation speed, this paper proposed a rapid important attribute discovery algorithm based on rough set and information entropy. The algorithm reduce attributes of the web log firstly, according to which, a heuristic rules mining algorithm  of web page recommended is put forward then, it can recommend web pages to the current session at last.



II. WEB PAGE RECOMMENDATION MODEL   As Fig. 1 showing, the rough set based web page recommendation system is divided into offline module and online module. The main task of offline processing stage is to decompose the web log click flow, reduce attributes, and find rules from web log. For online processing stage, the module decomposes the web log click flow from the current session, matches the current session with the recommendation rules, and puts forward a personalized recommendation set. At the same time, the results of a recommendation influence the current page and the coming ones.



III. ATTRIBUTE REDUCTION  A. Log Pretreatment All web visiting information is often record in Web logs,  but only the time an user spend on a page can show the user?s interests. We take relevant pages on the website as attributes, and take the time the user spend on the page as the attribute value of the decision table, the  decision attributes are dynamically assigned.

*This work is supported by the Teacher?s Research Foundation of North China Electric Power University (200711031).

web log log pretreatment   attributes reduction  rule mining  matching  recommendationrecommendation set  Figure 1. Recommendation Model Based on Rough Set  log pretreatment current session  Offline Module  Online Module   DOI 10.1109/CSSE.2008.499    DOI 10.1109/CSSE.2008.499     The record time of the visiting in web log is on the point view of server issue, which is not accurate. The less of precision is manifested in the differences between the times of transmission process. An integrity process of an user request A and B is like this: A user request page A (T1), the server receipt of the request (T2), the server response to the request (T3), the client receive the response (T4), client request page B(T5), the server receive the request (T6), then the time the user stay on the page A is appear as T2-T6, but actually it?s T4-T5. The time the users spend on page A is as the following description:  T5))-(T6-T2)-((T4-T2)-(T6   (1)  After further refinement of the time, the time element being measured by the number of minutes, so the decision table can be expressed as follows:  TABLE 1. RAW DECISION TABLE  P1 P2 P3 P4 P5  1 2 0 2 1  0 1 0 1 2  B. Attribute Importance Weight Based on Information Entropy Given the shortcomings of condition entropy when used to  describe attribute importance, we take the information entropy of data in positive domain as a metric when considering on attribute importance. The method is good enough to avoid over characterization of the border elements. The importance of attributes is given as follows: C is the condition set of attributes in decision table T, a subset of attributes A ? C, the attribute importance of the subset is defined as:    H*(A)-H(D|A)SGF*(A,D)=1+ logN  (2)  In formula (2), N is the number of objects in decision table T, and H*(A) is defined as follows:  *( ) ( ) log( ( )), ( )  n  i i i A i  H A p X p X X POS D =  = ? ?? (3)  Set T = <U,R,V,f> is the decision making system, in which  R=C? D, C is the condition attributes set, D is the decision attributes set, a subset of attributes A ? C, a is the arbitrary attribute and a ? C\A. The importance of attribute set A relative to decision making attribute set D is defined as:  ),()},a{(),,( *** DASGFDASGFDAaSGF ??=   (4)  Table T is as follows: TABLE 2. DECISION TABLE T  U p1 p2 p3 p4 p5 p6 p7 p8  a 0 0 0 0 1 1 1 1 b 1 1 1 0 1 0 1 1 d 0 0 0 1 0 1 0 0   According to formula (4), the importance of attribute a and b are calculated as follows: SGF*(a,A,D)=SGF*((a),D)-0=0.40 SGF*(b,A,D)=SGF*((b),D)-0=0.14 We can see from the result above that by improving the method of importance measurement the result is in line with the actual situation. In the above example, almost half of the elements are in the border domain. So when most elements are in the border domain, the approach based on mutual information to measurement will over characterizes the elements in the border domain, The method based on information entropy takes the number of domain elements and their distribution into account, and which can get a reasonable degree of the importance of attributes.

C. Attribute Reduction Before the algorithm is given, let us first think about the  concept of the core attributes set:  C and D, which are equivalence relations of U; C positive domain of D is recorded as posC(D), namely:   }/,|{)(  /  DUYYXXDpos iij CUx  jc j  ??= ? ?  (5)  For U / C classification, the positive domain of U / D is the object collection which can be classified by the knowledge in U / C expression.

C and D are the equivalence relations of U, when posind(C)(ind(D))=posind?C-{r}?(ind(D)), we take r?C of C as the relation which can be omitted in relation to D. Otherwise, r in C can not be omitted in relation to D.

When each r in C can not be omitted in relation to D, we claimed C and D independent. When S ? C, and posS(D)=posC(D), the group S known as the reduction of C in relation to D, and recorded as redD(C).

All of the original groups in C which can not be omitted in relation to D are known as the core of C in relation to D, the formula is as follows:  CORED(C)=? redD(C)  (6)  With the definition of property based on information entropy below, now we can put forward the attribute reduction algorithm based on information entropy, since the definition of attribute core is given.

Because there is a core attribute set C CORED(C), the algorithm can deal with attributes in a gradual increase way, and use  SGF*(a,A,D) as an inspiration , until the important degree of attributes is no longer increase, then we achieve the relative reduction. The algorithm is as follows:     Input: the decision table T, T?<U, C?D,V,f>  Output:Reduction Attribute Set P.

Step 1: For a C? ?  Do If posind(C)(ind(D)) = posind?C-{a}?(ind(D)) Then a?CORED(C) Else  CORED(C)?CORED(C)U{a} P?CORED(C)? Step 2: For \ix C P? ?  Do b=SGF*(xi,P,D)? If max_a=max{SGF*(xi,P,D), \ix C P? } Then a= xi If SGF*(PU{a},D)=SGF*(P,D) Then goto step 3 Else P=PU{a} If C\P<> Null Then goto step 2 Step 3: output P.



IV. RULE MINING  A. Decision Matrix Decision matrix is a promotion of rough set, it comes from  individual matrix, which can be used to calculate the decision rules and attribute reduction of information systems, and it provides a way to keep all information in the premise and calculate the most simple rule set. Decision matrix is defined as follows:  Information systems T = (U, A, V, f), in which U is the domain, attribute set A = C ? D, C is conditions set, D is decision attribute set. The equivalent relations in C are C1, C2,?, Cm, given a concept c ? (C1, C2,?, Cm), the concept belongs to c and the concepts do not belongs to c has different subscript, which are i (i = 1,2,?, ?) and j (j = 1,2,?, ?).The matrix of  concept c in T is defined as Mc(T)=Mij ,which is a ? ? ? matrix. The value of each element in the matrix is as follows:  Mij={(a,a(i)):a(i)?a(j)}?i=1,2,?,?,j=1,2,?,?   (7)  In the above formula, a(i) is the value of an attribute.

For each of the object in concept c, i(i=1,2,?,?), we can calculate the shortest length of rules:   | | ( )ci ijjB M= ? ?  (8)  The sign ???and ???are extended operators, the final decision-making rules of concept c can be set into the following formula:   c iRULEc=|B |,(i=1,2,..., )?  (9)  B. Rule Mining Algorithm There is a lot of noise in the data, so that the confidence of  a rule is not necessarily to be 1, it is necessary to deal decision matrix with confidence. The confidence of concept c in decision matrix is as follows:  Confidence of concept c in decision matrix: All the ? positive domain( ( )CPOS D  ? ) and ? negative  domain ( ( )CPOS D ? )belong to concept c has respectively  subscript i (i = 1,2,?, ?) and j (j = 1,2,?, ?). The decision matrix is defined as:   ?? ,...,2,1,,...,2,1)},()(:)(,{( ==?= jijaiaiaaMcertainij   (10)  Therefore, the ultimate decision making rules of concept c can be expressed as the follows:   ),...,2,1(),(|| ?=??== iMBRULE certainij  certain i  certain i  (11)  With the definition of decision making rules, the following steps are based on the decision matrix.

Step 1: Calculate the division of the condition attributes set C and decision attributes set D in relation to the decision table respectively,  {C1?C2,?,Cn}and{D1,D2,?,Dm}.

Step 2: Calculate ( )CPOS D ? and ( )CNEG D  ? .

Step 3: Calculated decision matrix: certainijM .

Step 4: According to the decision making matrix, mining  decision making rules and find out its reduction.

In order to better explain the operation of algorithm steps,  the illustrate with an example is given below.

With the following information table:  TABLE 3. INFORMATION TABLE T P1 P2 P4  0 0 0  0 0 0  0 2 1  1 2 0  1 0 0  0 2 1  The divisions in table T are as follows: U/INP4({p1,p2})={{1,2},{3,6},{4},{5}} U/INP4({p4})={{1,2,4,5},{3,6}}     Because of the above information form for compatibility information table, ? positive domain has the degree of support of all the rules are for 1.

When p4 = 0, {p1, p2} have the positive domain of {{1,2}, {4}, {5)} and has a negative domain {3,6} relative to {p4}.

Therefore:  M00={(p2,2)} M10={(p1,0)} M20={(p1,0),(p2,2)} Rules are as follows? p2=0? p4=0,( support =1).

p1=1? p4=0,( support =1).

When p4 = 1, {p1, p2} has the negative domain of {{1, 2}, {4}, {5)} and has the positive domain of {3,6} relative to {p4}.

Therefore:  M00={(p2,0)} M01={(p1,1)} M02={(p1,1),(p2,0)} Rules are as follows?(p2=0) ?  (p1=1) ?  ((p1=1) ? (p2=2))? p4=1 After reduction, we can get:  (p2=0) ?  (p1=1)? p4=1,( support =1).

Thus we obtained the greatest general of all the rules. The  difference between objects in positive domain and objects in negative domain is described by decision matrix. We can identify the most general rule with the logical way and we can get the greatest general of all the rules. At the same time, we can do it with the introduction of variable accuracy of rough set, and with confidence to describe the probability of conflict of rules, which can play a good role in the suppression of noise.

On a higher precision require, we need simply modify the value of ? smaller.



V. PAGE RECOMMENDATION EXPERIMENT To apply our method to practical work, we designed  software used for inference rules reasoning and the implementation of real time recommendation. To illustrate the Effectiveness of the algorithm based on information entropy and rough set (IER), we used NASA, C. NET and UOS data to verify the algorithm; we value the effectiveness of the approach with the similarity between the recommendation set and the click flow. In order to show the effectiveness of our algorithm better, we compare it with MFP and TOP-N algorithm, and the results are shown in the table below.

TABLE 4. TESTING RESULTS  Alg.

Data TOP-N MFP IER  NASA 82.25 80.6  82.67  C.NET 78.72 70 77.86  UOS 70.76 72.0  73.14    From above results, we can see that IER method has higher performance on the recommendation of precision in NASA and UOS data set, and in C. NET data set, the performance of the algorithm is no better than  TOP-N. Overall the performance of IER algorithm comes from different data sets is relative stability and the accuracy of recommendation is satisfactory.



VI. CONCLUSION We use visiting time to describe paths, which can more  accurately describe user's interest. In our model, we first pretreated the web log and then do attribute reduction with the support of the information entropy theory. Our approach reduces the amount of computation; thereby it can speed up the rate of discovery rules. Experiment result proved that information entropy based personalized recommendation algorithm can excavate more meaningful rules. In order to guarantee the accuracy of the recommendation, the new path recommended by the system will participate in the next circle.

The system is cold start, which need further study and solution.

