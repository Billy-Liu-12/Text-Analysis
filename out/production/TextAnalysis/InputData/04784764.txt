Toward Improving Scheduling Strategies in Pull-based Live P2P Streaming Systems

Abstract?Several recent P2P streaming systems have adopted mesh overlays to disseminate content to participating peers because this topology appears to be more resilient to churns.

To cope with inferred problems, such as data redundancy, these systems opt for data-driven content retrieval mechanisms (pull mechanisms). Each node has a list of neighbors with whom it periodically exchanges buffer information and requests content fragments. One of the drawbacks of such a mechanism is that it does not offer intelligent selection of sending neighbors based on their characteristics. This is mainly because the most important criteria used to select nodes is the content availability. This can result in some performance degradation, for instance, due to peers that are sending very small or big parts of the needed data. Resiliency may then be weakened and overhead increased.

In this paper we propose to study how the integration of some end nodes characteristics can improve the performance of a typical pull mechanism with random scheduling. We show that the improvement in performance is significant enough despite the fact that the room for improvement is bounded by the limitations of the pull mechanism. Hence we believe that the awareness of end characteristics is an important block upon which we can build more efficient content retrieval mechanisms. The gain in performance can also be amplified by proposing an alternative to the pull mechanism such as a combined pull-push approach.



I. INTRODUCTION  Peer-to-Peer (P2P) architecture has been considered as an attractive and scalable solution for video streaming. It does not require Internet infrastructure changes and it helps eliminating bandwidth bottleneck at the content source server.

Nevertheless, P2P systems, especially for live video stream- ing, face many challenging issues such as efficient and opti- mized overlay construction [1], [2], [3], [4], content retrieval mechanisms [5], [6], [7], etc.

In P2P streaming systems, the content retrieval mechanism allows an end-client to receive data from other nodes using the constructed overlay. This mechanism plays a predomi- nant role in the video streaming process and its efficiency greatly influences the global performance. Existing research works [8], [9], [5], [6] have demonstrated that a multi-sender mechanism achieves better results than a single-sender one.

In such a context, the data-driven approach (mainly pull mechanism) is currently a very simple and suitable approach as it allows the receiver to cope with two main challenges: eliminating/reducing data redundancy and recovering from data loss.

The pull mechanism is heavily based on the content avail- ability at peers: what content is available from which neigh- bor? Thus, a receiving node has to locate the missing content and to request it from the appropriate nodes.

However, data-driven approach adds complexity to the re- ceiver side because it is responsible for scheduling the data sent by its parents, that is to decide which neighbor should send which data. In [5] for example, the authors use buffer maps to advertise available content so that a node is able to request a given content from its neighbors. These maps are periodically sent by each node to its neighbors. This results in overhead and extra load on peers.

In addition, there is more delay because of the three required steps for data retrieval: advertise, request and send. Gridmedia [6], illustrates the benefit of a combined push-pull mechanism on observed playback delay. A client registers itself with some neighbors that will systematically send data to him (push) and the pull is only used initially or when there is missing data.

To be efficient, a content retrieval mechanism has to make the best use of the upload bandwidth of peers and to minimize the overhead. This mechanism also has an important impact on minimizing delay and maximizing throughput or resiliency.

Actually the content retrieval mechanism should be considered as the way to achieve efficient utilization of the overlay network. Indeed the overlay network satisfies some quality requirements but it does not impose how it is used. Different strategies can be designed for that purpose.

For example, when maximizing throughput, a node may have parents with available upload bandwidth but if they don?t have enough content to send, the observed throughput will be lower than expected.

Moreover, when a receiver node assigns the most part of the content to only one of its parents, it will be sensitive to the departure or failure of this parent and then may experience significant quality degradation. Hence the system has a weak resiliency.

In addition when a specific node (for example, one with high upload bandwidth) is experiencing high load for content forwarding, probably there will be higher delays due to more overhead, processing delays and possible packets loss and retransmissions. Hence we can further reduce delay by balancing load on peers as much as possible.

To be compatible with load balancing on peers and criteria optimization, we believe that the content retrieval mechanism must guarantee that the content is distributed almost equally between parents of the same node, i.e., the parents have almost the same content. We will refer to this as Content Balancing.

Indeed, if a peer has significantly more or newer content than the other parents, then any receiving node may try to get most of the content from that node. This results in high load on this parent and then it is impossible to achieve optimization goals.

We claim that the performance of a P2P live streaming system can be enhanced through the introduction of intelligent strategies for neighbor selection where we consider content availability at candidate neighbors along with lower layer characteristics such as upload bandwidth capacity, RTT with the receiving node, etc.

An intelligent selection of neighbors can improve signifi- cantly the performance of a P2P live streaming system without resulting in high overhead. This intelligent selection can take place at two stages: the selection of neighbors (either to construct an initial set of neighbors or to improve it during the session) and the selection of a subset of neighbors to request packets from. In the rest of this paper, we will focus on the latter stage and we will differentiate between neighbors and sending neighbors with the latter set being a subset of the former one.

The work being presented in this paper is part of a larger study on the impact of end-nodes characteristics (such as end- to-end RTTs, upload bandwidth capacity...) and on the impact of P2P live streaming parameters (streaming rate, number of neighbors, etc.) on the performance of the system specially the playback delay and the resiliency. For more details we refer to [10], [11].

Finally, we want to emphasize that this paper makes two contributions. The first one is to show that we can improve the performance of a P2P streaming system by taking into consideration some characteristics of participating nodes (that are easy and not costly to retrieve). We show this through comparisons between some variations of the pull mechanism despite the fact it has many limitations as stated earlier in this section.

The second contribution is to introduce a new method for sending neighbors selection that we call Content Balancing.

This new method performs better than typical pull mecha- nisms, and it is not tied exclusively to a pull mechanism since it can be adapted to other potential mechanisms. We position this work as the first step toward more intelligent overlay construction and content retrieval mechanisms.

This paper is organized as follows. Section II describes some related work. In section III we give an overview of the different pull variations being compared. Section IV de- scribes the simulation environment and assumptions along with comments on the results we obtained. Finally, we draw our conclusions and describe future work in Section V.

Fig. 1. Global Overview of Existing and Proposed Approaches for Requesting

II. RELATED WORK  Scheduling strategies for content retrieval have been studied in previous works [12] such as random scheduling and rarest first scheduling. These techniques focus on the characteristics of data namely its freshness and its frequency among neigh- bors.

Recent works [13], [14], [15] propose more elaborated scheduling techniques that are based on what content a re- ceiving peer is in need for. Indeed, they put more focus on peers than on the content being retrieved. In [15], the authors propose a randomized distributed algorithm which leads to an optimal streaming rate in a fully connected mesh topology.

Each peer proceeds by identifying its neighbors that are the most deprived from data and randomly selects missing packets to forward to them.

In [14], the authors propose a deterministic chunk schedul- ing strategy based on an adaptive queuing mechanism. This strategy achieves near optimal bandwidth utilization. The strat- egy differentiates between forwardable content (F) and non forwardable (NF). Pull signals are used by peers to explicitly request content. Such chunks will be marked F while chunks exchanged among neighbors are NF.

In addition of their complexity, the mentioned scheduling strategies have a weak point in common that is the lack of awareness of peers characteristics such as end-to-end propa- gation delays (rtt), upload bandwidth availability, experienced playback delay, etc.

In most of existing works, the peers characteristics at the network level (network delays, upload bandwidth,...) are typically used to select the set of neighbors only. Application layer characteristics such as the playback delay and the ratio of delivered content are usually not exploited for neighbors selection or scheduling.

Our approach is complementary to the above work. Indeed, we propose to incorporate end node characteristics at both the application and the network level in the scheduling strategy as depicted in Figure 1. Thus we are able to choose the best subset of neighbors to request data from.



III. PULL VARIATIONS  In this section we describe the pull mechanisms we propose to compare. One of the typical pull mechanisms used in P2P     systems is based on random scheduling, where each node maintains a list of neighbors from which it randomly chooses a neighbor to which it sends a request for the needed stream packet. We call it Pure Random mechanism. When a node does not satisfy a packet request, it does not try anymore to resend the packet.

Random scheduling performs quite well under certain cir- cumstances [13], [16]. Its simple design makes it a suitable choice for available commercial P2P streaming systems [13].

In this paper we will show how this strategy, can be im- proved even if we only take in account some small information about end node characteristics such as the maximum upload bandwidth and the content of the buffer maps of neighbors.

Thus we will compare three variations of the random pull mechanism. These variations have the following common process: from the full set or a subset of neighbors, for each needed packet they construct the list of neighbors having this packet in their buffers, i.e., the potential sending nodes. Then they request the needed packet from a random node from that list. These variations differ in how the set of potential transferring nodes is selected.

a) Pure Random (PR): The first variation is purely and simply a random selection where the needed packet is requested from a random neighbor that has it.

b) Pure Random with Sending Node Bandwidth Aware- ness (PRwBA): The second variation takes into account the maximum upload bandwidth of neighbors. Thus a node will not request from its neighbor a volume of data that is higher than its maximum sending capacity. However a node may receive more requests than it is able to satisfy.

c) Content Balancing (CB): The third variation is based on the Content Balancing concept we propose and includes upload bandwidth awareness. We argue that to be able to optimize the selection of sending neighbors and what packets they have to send, the neighbors need to have great similarity in their buffers. Indeed, if a packet is present only at one node or at a small number of nodes, there is no room for optimization of the node that must send it.

To be able to evaluate the content balancing of a neighbors set, we introduce two concepts. The first one is the variance of a set. Recall that each node knows what is the highest sequence number of the packets he has received. Let v be a node in the session with n potential sending neighbors u1..un. Let hi, i ? [1..n] be the highest sequence number of the packets received by ui. When v receives the buffer maps of its neighbors it is able to compute their variance as:  V = max 1?i?n  hi ? min 1?i?n  hi  The second concept to be introduced is a sort of indicator, D, of how recent are the packets to be requested. This can be even seen as a coarse estimation of the playback delay with the argument that requesting recent packets leads to low playback delays.

Let kv be the highest sequence number known to v (obtained  from the received buffer maps).

D = kv ? min 1?i?n  hi  These two quantities are combined linearly with a factor ? to obtain the following function:  F = D + ? ? V To be able to make a fair comparison with the random  mechanism, we compare the variations based on the same number of potential transferring nodes. In the Pure Random variation, each neighbor can be requested to send a packet.

However, in the CB variation, each node maintains a greater number of neighbors but only a dynamic subset of nodes can be requested for packets.

For example, we decide to have 15 potential transferring nodes. For the random variations the number of neighbors is equal to 15. For the content balancing variation, the number of neighbors is 20. However, at each request period, a subset containing only 15 nodes is constructed and the packet can only be requested from nodes in that subset.

The initial subset of 20 nodes is sorted in descending order of hi, i ? [1..20]. Thus, the possible subsets that will serve for requests are u(1)..u(15), u(2)..u(16), u(3)..u(17), u(4)..u(18), u(5)..u(19), u(6)..u(20) where u(i) is the neighbor at the position i in the sorted initial set. The subset to be selected is the one that minimizes the function F .



IV. SIMULATION RESULTS  In this section we first give a brief description of the simulation conditions. Then, we detail and comment the results we obtained. Due to space constraints we do not provide a sensitivity analysis of the CB method. The parameter ? is fixed to 0.5. Increasing or decreasing ? will have an impact on the frequency of the requested packets among neighbors and on how recent they are.

The degree of improvement we would expect is limited by the fact that the scheduling algorithm is the same for all variations, i.e, random scheduling. Indeed, there is no intelligent mapping of a packet to be requested to a specific neighbor.

We recall that the main difference between the compared variations lies in the set of neighbors they operate on.

A. Simulation  We make use of the discrete event based simulator built by Zhang & al. and detailed in [16]. Although, this simulator has many useful functionalities already implemented, it suffers from the lack of documentation. The source code of the simulator is available at [17].

In the simulator the streaming data is divided into packets of size 1250 bytes not including headers. It is assumed that nodes have DSL connections and that bandwidth bottleneck is located on the edge of the network, i.e., end-nodes access networks [16]. Table I shows how bandwidth capacities are distributed (by default).

TABLE I BANDWIDTH CAPACITIES OF END NODES  Download Rate 3 Mbps 1.5 Mbps 768 kbps Upload Rate 1 Mbps 384 kbps 128 kbps  Nodes Fraction 15 % 39 % 49 %  TABLE II SOME DEFAULT PARAMETERS OF THE SIMULATOR  Streaming Rate 300kbps Request Period 500ms  Request Window Size 600 packets = 20 s  We keep the default values used in the simulator for the parameters of the pull mechanism, such as the size of the request window, the request period, the streaming rate, etc (See Table II for some examples). Network end-to-end latencies (rtt) between nodes are real-world values taken by default from a latency matrix computed within the Meridian project [18].

For each pull variation we executed the simulation ten times and then for each comparison criteria we took the average over the ten obtained results. We assume that there is no churn rate because it is not relevant enough for the present study.

We conducted our simulations with 100 nodes. In the random variations the number of neighbors is equal to 15 while in the CB variation the number of neighbors is 20 but only 15 nodes can be requested for packets at each request period (See Section III).

We limit each execution to a duration of 120 seconds. The results shown are the ones computed for the last 10 seconds (this behavior is by default in the simulator).

B. Results  In this section, for each point upon which we base our comparison, we give a description (for more details, see [16]) and we comment the obtained results.

1) Average Playback Delay: The playback delay is an estimation of the video latency a user may experience. It is computed using a sample time interval in the streaming buffer.

This sample interval, of fixed size, must satisfy the condition that within it the ratio of delivery of packets is greater than a threshold (0.99). The playback delay will then be determined by the position of the sample interval in the buffer. The average playback delay is computed over all participating peers.

Table III describes the results we obtained for the average playback delays. We note that the PR and PRwBA lead approximately to the same playback delay. However, both have an increase of around 14% relatively to the playback delay with content balancing. This is due to the fact that with the CB variation, we are making a balance between the  TABLE III AVERAGE PLAYBACK DELAYS WITH 100 NODES AND 15 SENDING  NEIGHBORS  Av. of 10 executions PR PRwBA CB Av. Playback Delay (ms) 16259 16139 14311  Comparison with CB + 14 % + 14 %  TABLE IV AVERAGE PACKET DELAYS WITH 100 NODES AND 15 SENDING  NEIGHBORS  Av. of 10 executions PR PRwBA CB Av. Packet Delay (ms) 8552 7580 7456 Comparison with CB + 15 % + 2 %  TABLE V AVERAGE HOP NUMBER WITH 100 NODES AND 15 SENDING NEIGHBORS  Av. of 10 executions PR PRwBA CB Av. Hop Number 3.57 3.5 3.41  Comparison with CB + 5 % + 2 %  request of recent packets and the number of requests needed to successfully receive a packet.

Thus, the proposed function F (see Section III) manages to choose a set of neighbors that have some of the most recent packets and to increase the probability that these packets are present in more than one node, which increases the probability of request success. Thus, packet arrival delay and playback delay are reduced.

2) Average Packet Delay: The packet delay is the time that a packet takes from its first creation at the source to reach a particular destination. The average packet delay is the average of delays of all packets received by all nodes.

Through the results detailed in Table IV, we note that there is a significant improvement obtained by considering the maximum upload bandwidths of nodes when making requests to them. Indeed, PR leads to a packet delay that is 15% and 13% higher than the CB and the PRwBA. Hence, in CB and PRwBA, we are reducing the number of unsuccessful requests.

Indeed, a packet will need typically less requests and thus less delay to be successfully received.

It is important to mention that a better average packet delay does not lead necessarily to a better playback delay.

We observe this for example between PR and PRwBA where the difference in packet delay is very noticeable while the difference in playback delay is not significant.

One main reason for such result lies in the fact that average packet delay is calculated relatively to individual packets that are successfully received by nodes, while playback delay is calculated relatively to the packets received successfully in a certain time window of the buffer of each node. In other words, the packet delay does not consider the contiguity of received packets while the playback delay does. Other reasons are left for future investigations.

3) Average Hop Number: Similarly to the average packet delay, the average hop number is the average number of hops observed by a packet when successfully received by nodes in the session.

Table V shows that Random variations specially the Pure Random have a higher average hop number than the CB method which can be seen as consistent with the results obtained for the average packet delay.

However, the value of the increase is different. For example the Pure Random variation has 15% higher average packet     TABLE VI AVERAGE DELIVERY RATIO WITH 100 NODES AND 15 SENDING  NEIGHBORS  Av. of 10 executions PR PRwBA CB Av. Delivery Ratio (%) 99.86 99.79 99.92 Comparison with CB + 0 % + 0 %  TABLE VII AVERAGE OVERHEAD WITH 100 NODES AND 15 SENDING NEIGHBORS  Av. of 10 executions PR PRwBA CB Overhead (%) 5.18 5.09 5.78  Comparison with CB - 10 % - 12 %  delay while the average hop number is only 5% higher. This confirms that estimating delays with hop numbers, as it is often done in P2P systems, is not accurate.

4) Average Delivery Ratio: The delivery ratio is the ratio of successfully received streaming data to the streaming rate. By successfully we mean that the streaming packet was received before its playback deadline.

Table VI shows that the average delivery ratio for the three variations remains almost the same. This is due mainly to the fact that they operate on the same number of neighbors. The difference lies mainly in the selection of neighbors.

5) Overhead: Overhead includes all control packets ex- changed by nodes such as buffer map packets and request packets. It is computed in percentage of the used upload bandwidth of nodes in the session.

Table VII shows that the overhead in content balancing variation is higher than in random variations. This is due to the fact that in Content Balancing each node maintains more neighbors than in the random variations. The CB overhead is still considered to be low.

6) Average Upload Bandwidth Utilization: Average upload bandwidth utilization is measured relatively to the aggregate upload bandwidth provided by all peers of the system. It includes both data and control packets forwarding.

The conclusion that can be drawn from Table VIII is that although CB has the lowest bandwidth utilization and the highest overhead, it achieves better performance (as shown in previous results) than the random variations. Thus CB strategy makes a more efficient use of the available upload bandwidth of the system. Actually, it generates a lower number of packets requests.



V. CONCLUSION AND FUTURE WORK  In this paper we have demonstrated that being aware of end node characteristics in the selection of sending neighbors can improve significantly the performance of a pull-based  TABLE VIII AVERAGE UPLOAD BANDWIDTH UTILIZATION WITH 100 NODES AND 15  SENDING NEIGHBORS  Av. of 10 executions PR PRwBA CB Av. Upload BW Utilization (%) 94.42 94.41 92.64  Comparison with CB + 2 % + 2 %  live P2P streaming system. The content balancing strategy we proposed leads to the best results within the pull mechanism.

We believe that this strategy has the potential of amplifying the performance gain within a different mechanism such as a pull-push combined approach. We are currently investigating different alternatives and what is their impact on throughput, resiliency and playback delay in P2P live streaming systems.

