Parallel Apriori Algorithm Based on the Thread Pool

Abstract?Discovery of association rules has a significant meaning in data mining, in which the most influential algorithm is Apriori. Due to the huge size of database and increasing amount of computation, a number of parallel algorithms have already been proposed. In order to achieve high-performance parallel computing, it is necessary to reduce redundant computation and avoid too much communication between parallel tasks, and achieve load balancing. Parallel algorithms, based on multi-processor, are concerned about the distribution of parallel tasks to improve the utilization of computing resources. These methods often require a compromise in computation, communication, load balancing, etc. For the most effective control thread, we first introduce the thread pool into the Apriori parallel algorithms. This method does not require redundant communication or computation, but can achieve load balancing so as to fully utilize the computing resources.

Keywords-parallel; Apriori; thread pool

I.  INTRODUCTION With the development of data storage technology,  amount of data in many areas are increasing rapidly.

Nevertheless, it is not easy to find out the hidden value of these huge amounts of data. Data Mining is committed to data analysis, in order to reveal the information hidden within the data. Association rules is an important task in data mining, in which Apriori algorithm has the greatest impact [1].

It needs to scan all the transactions in the database for the process, which introduces a significant amount of I/Os [2].

Three Parallel algorithms for mining association rules based on multi-processor were presented in [3]: the Count Distribution algorithm, the Data Distribution algorithm, the Candidate Distribution algorithm. Other parallel association rule algorithms are basically based on these ideas by making improvements in terms of communication efficiency or aggregate memory utilization [5-7].

The Count Distribution algorithm is a straight-forward parallelization of Apriori. Each processor generates all candidate itemsets during each pass, makes a pass over its data partition and develops local support counts for the candidates. At the end of each iteration the global candidate, itemset counts are generated by exchanging the partial counts among all the processors. This approach reduces the  communication between nodes through a large number of redundant computations. The Data Distribution algorithm partitions the candidates into disjoint subsets, which are assigned to different processors. Then each processor set generates candidate itemset counts using both local data and data received from other processors. This method can be the most effective way in terms of utilization of system memory, but suffers from huge communication overhead scanning the remote partitions database. The Candidate Distribution algorithm also partitions the candidates. But in order to proceed independently of each processor, it replicates the database selectively.

The above three methods are all based on computing candidate itemset counts in parallel. The only difference is how to allocate computing resources to improve their utilization. However, these methods can`t completely achieve load balancing, and probably will generate idle processors. Algorithm analysis is based on a shared-nothing multi-processor model in [3], while the mainstream multi- processor is SMP structure which belongs to the shared memory model. In this paper, we consider the problem of mining association rules on a shared-memory multi- processor.

Parallel Apriori algorithm based on the thread pool, completely solves the problem of load balancing, which can?t be solved by the three methods proposed above. This method does not need redundant computation, require a lot of communications, increase the complexity of the algorithm, while taking full advantage of computing resources.

The organization of the rest of the paper is as follows.

Section 2 gives a brief review of the Apriori algorithm and thread pool algorithm on which the proposed parallel algorithms are based. Section 3 gives the description of the parallel Apriori algorithms based on thread pool. Section 4 presents the results of the performance measurements.



II. OVERVIEW OF THE SERIAL ALGORITHM  A. Apriori algorithm Let 1 2{ , , , }mI i i i=  be a set of items. Let  1 2{ , , , }mT t t t=  be a set of transactions, where each transaction it  consists of a set of items such that it I? .An association rule can be expressed as X Y? , where   DOI 10.1109/CSSS.2012.555     ,X I Y I? ?  and  X Y = ? . X  (or Y ) is a set of items, which called itemset. X  is called the before itemset, and Y is called after itemset.

The task of mining association rules algorithm is to find all the association rules, whose support is larger than a specified minimum support (minsup) and whose confidence is larger than minimum confidence (minconf) in a given transaction set. The support of rule X Y?  represents the probability of a transaction which contains X Y . The confidence of rule X Y? represents the probability of a transaction which contains X  and also contains Y . If the support is too small, the association rules may not have any value. If the confidence is too small, it's difficult to reliably infer from the X  out of Y . This rule will not be much useful in practical applications.

Steps to generate frequent itemsets:   Figure 1.  The flowchart of generating frequent itemsets.

Apriori algorithm uses the downward closure property of itemset support to prune the itemset, which means that all subsets of a frequent itemset must also be frequent. During the k-th iteration of the algorithm, a set of candidate k- itemset is generated. And then the database is scanned to generate the support for each candidate. Only the frequent k- itemset is retained to generate a set of candidate itemset for the next iteration. This iterative process is repeated until no more frequent k-itemset being generated. The general structure of the algorithm is given in figure 1. In the figure,  kF  denotes the set of frequent k-itemset, kc  denotes the candidate k-itemset, and kf denotes the frequent k-itemset.

There are two main steps: candidate generation and support generation. The set of candidate k-th itemset is generated by merging 1kF ? . A pruning step eliminates any candidate, when a (k-1)-subset is not a frequent itemse. This pruning step can eliminate a lot of unnecessary candidate itemsets.

The set of candidate itemset is stored in a hash tree to facilitate fast support counting. For generating frequent itemset efficiently, all transactions are generated in lexicographical order. Then it needs to scan the transaction in the database to count the support of candidate k-itemset. The last step is toform kF  by selecting the set of itemset meeting the minimum support criterion.

Association rules are generated from frequent item sets.

Figure 2 shows the association rules (confidence is larger than minconf) algorithm flowchart.

Figure 2.  The flowchart of association rules, whose confidence is larger  than minconf).

The confidence formula of association rule ( )f ? ?? ? is . / ( ).f count f count?? . During the confidence generation, the principle of downward closure property is also used to improve efficiency. When changing , the  support of f  is constant, yet the support of ( )subf ??  must no larger than the support of ( )f ?? . So if ( )f ? ?? ?  is an association rule, all ( )sub subf ? ?? ?  must be association rules.

B. Thread pool algorithm The thread pool is a key component in the server, the  database and the operating system. The main purpose of thread pool is managing threads. Several models of the thread pool have been proposed [4]. The earliest model the simplest one creates a thread when a new request, and destroy a thread when the end of processing. Later, in order to reduce the overhead of thread creation and destruction, the worker thread pool is developed, which is based on the producer-consumer model. In this model, the thread pool creates a number of threads at the beginning. When a new request comes in and no idle thread is available, the thread pool will create a new thread, or make the request wait, or deny the request. Otherwise, when idle threads are found to run too long, an idle thread will be destroyed. Recently, there are several reforms in recent algorithms, such as creating several threads in advance according to the request frequency. In this paper the second model is used.

Figure 3.  Producer-consumer model.

There are also several producer-consumer models. The model showed in figure 3 is used in this paper. A concurrent security task queue is used to store requests. The red faces are the busy thread that is processing tasks. The white face people are the idle threads ready to get the task from the task queue. The blue faces are task producers. Under this model, the performance of the task queue is often the bottleneck of the thread pool. As shown in figure 3, the two white faces can?t get the quest at the same time. At the same time, there is only one idle thread to get the task, which means competition happens between the white faces. This competitive relationship brings out the loss of the thread context switching, when multiple white faces take tasks from the queue too quickly. As a result, the overall computing performance will be unsatisfactory. Parallel computing throughput is affected. Therefore, this producer-consumer model should ensure that the processing time of each task is not too short to avoid this bottleneck.

Figure 4.  The flow chart of thread in the thread pool.

The two most important part of the design of the thread pool is thread execution flow and high performance concurrent security data structure. The flow chart of a thread in the thread pool (figure 4) shows how to run a thread.

There are three cycles: a main loop and two inner loops. One of the inner loops is an operated cycle, while the other one is an idle cycle. When a thread is created, the operated cycle gets and executes job from task queue until no task. Then the thread enters the idle loop. After thread enters the idle loop, if the job is null, thread exits the idle loop, else determine the maximum thread idle time is zero. If it is true, remove the task from the queue. If not, determine if the current number of thread pool threads is greater than the lower limit of the thread pool capacity and the last destruction of the thread time is greater than the maximum thread idle time. If it is true, destroy the thread. If not, block the thread in the regular time for the maximum thread idle time.



III. PARALLEL ALGORITHMS Parallel program based on the thread pool, doesn`t need  to consider the problem of task allocation. The thread pool can dynamically assign tasks to the idle threads. Therefore, the decomposition of the algorithm need only consider how to decompose relatively independent tasks, and to take into  account the processing time for each task being not too short.

Otherwise, the task queue will become a performance bottleneck. In the concurrent security task queue, take - take operation is not concurrent. Idle threads failed in the competition switch thread context, therefore the utilization of processor is not good.

Figure 1 shows that the set of candidate k-th itemset is generated by merging 1kF ? . Suppose that A has n itemsets, and arrangs according to the lexicographic order. A candidate k-th itemset is generated by two steps. The first one is merging. The second one is pruning. Then merging- pruning operation can be at most ( 1) / 2n n? ?  times. Every merging-pruning operation does not modify the data in 1kF ? , so all ( 1) / 2n n? ? operations are independent.

A decomposition of ( 1) / 2n n? ? independent tasks is feasible, but the computation time of each individual task may be too short. Because only if 1,k if ?  and 1,k jf ?  of the first k-2 items are the same and only the last item is different,  kc is needed to operate merging-pruning. If the judgment is put into the producer thread, the judgment operations are serial code. Too much serial code can affect the performance of parallel computing. If the judgment is put into a customer thread, the processing time for these tasks is very short, when  1,k if ?  and 1,k jf ?  do not need to merge and prune. Such a short processing time can cause a performance bottleneck problem described earlier in the task queue. Shown in Figure 6, the algorithm is decomposed into n independent tasks.

Each task contains all the merging-pruning of 1,k if ?  with  1, ,k jf j i? > .Although each independent task processing time is uncertain, the general order of processing time is getting shorter. But because of the thread pool management, it is able to ensure that the load balancing, and all processors are busy.

First, the code of task is extracted. Second, configure a synchronous counter. When a task is completed, the counter is decremented by 1. When the counter is equal to 0, notify the main sleeping thread that the current tasks are completed.

Then start the next iteration. It is noteworthy that the main thread would release the processor resources, when waiting for the process.

The modifications of concurrent confidence generation is the same of frequent itemset generation, as shown in figure 7.

Figure 5.  The flow chart of frequent itemsets generated based on the thread pool.

Figure 6.  The flow chart of association rules generated based on the  thread pool.



IV. PERFORMANCE EVALUATION The algorithmic implementation uses java in the eclipse  environment. Operating system:Windows XP. Processor: AMD Phenom (tm) II X4 810 2.59GHz, 3.00 GHz memory.

The experimental platform has four processor, so  the most number of concurrently running threads is four. When there are more than four threads using the processors, at least two threads compete for the same processor. the competing threads shares the time slice in the form of the processor. In this case, although a good response, there is additional thread context switching overhead. The algorithm does not require good response, so the number of threads of the thread pool is fixed at 4. Table 1 shows the time performance of frequent itemsets, which is the comparison of the serial Apriori algorithm and the parallel Apriori algorithm based on the thread pool. The time performance of association rules is showed in table 2.

The distribution of data set has no impact on the parallel algorithm based on the thread pool, because there is not any division of the data set. The data source is simulated, the number of transaction set 944, the minimum support 0.01, and the minimum confidence 0.8.

TABLE I.  THE TIME OF THE SET OF FREQUENT ITEMSET GENERATION  k-th frequent itemset  num serial(ms) Parallel(ms)  1 42 110 32 2 458 375 172 3 2217 1437 656 4 5688 5500 2265 5 8780 12188 4766 6 8881 12375 4766 7 6711 6969 2250 8 4032 2359 828 9 1954 547 187  10 753 94 32 11 211 15 15 12 37 0 0 13 3 0 0  sum 39767 41984 15984  TABLE II.  THE TIME OF THE SET OF ASSOCIATION RULES GENERATION.

Num Serial(ms) Parallel(ms) 1167288 10563 7656     Figure 7.  The processor utilization of the parallel Apriori algorithm based  on the thread pool.

Figure 8 shows the processor utilization of the parallel Apriori algorithm based on the thread pool. It can be seen from the figure that 4 processors are fully utilized. In general, a parallel program modified by serial program is the number times fast speed up of processors, in theory. But the program includes serial codes and lock competitions, so in fact it is less than four times speed up. However, Figure 8 shows that the parallel Apriori algorithm can take full advantage of the characteristics of multi-processor. It has an obvious advantage on time performance. With the increase of the amount of data, its performance value can be reflected much better, because the thread pool has the best load balancing.



V. CONCLUSION The parallel Apriori algorithm based on the thread pool  can effectively avoid the redundant computation, and communication, algorithm complexity. This new algorithm can effectively avoid the inevitable problem, which occurrs by assigning data to the appointed processor. Therefore it has broad application prospects.

