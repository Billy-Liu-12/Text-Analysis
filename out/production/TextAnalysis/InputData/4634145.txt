r-SVMT: Discovering the Knowledge of Association Rule over SVM Classification Trees

Abstract? This paper presents a novel method of rule extrac- tion by encoding the knowledge of the data into an SVM classi- fication tree (SVMT), and decoding the trained SVMT into a set of linguistic association rules. The method of rule extraction over the SVMT (r-SVMT), in the spirit of decision-tree rule extraction, achieves rule extraction not only from SVM, but also over the obtained decision-tree structure. The benefits of r-SVMT are that the decision-tree rule provides better comprehensibility, and the support-vector rule retains the good classification accuracy of SVM. Furthermore, the r-SVMT is capable of performing a very robust classification on such datasets that have seriously, even overwhelmingly, class-imbalanced data distribution, which profits from the super generalization ability of SVMT owing to the aggregation of a group of SVMs. Experiments with a gaussian synthetic data, seven benchmark cancers diagnosis have highlighted the utility of SVMT and r-SVMT on encoding and decoding rule knowledge, as well as the superior properties of r-SVMT as compared to a completely support-vector based rule extraction.



I. INTRODUCTION  S Ince support vector machines (SVM) [5] demonstrate a  good ability in classification and regression, rule extrac-  tion from a trained SVM (SVM-Rule) accordingly performs  well on the decision making for data mining and knowledge  discovery, sometime even better than the SVM. [1], [3], [4].

However, the obtained rules from the SVM-Rule in practice  are less comprehensible than our expectation because there  is a large number of incomprehensible numerical parameters  (i. e. support vectors) that appear in these rules. Compared to  the SVM-Rule, decision-tree is a simple, but very efficient rule  extraction method in terms of comprehensibility. The obtained  rules from decision tree may not be so accurate as SVM rules,  but they are easy to comprehend because every rule represents  one decision path that is traceable in the decision tree.

In the literature, rule extraction based on single SVM  focuses on interpolating the support vector and hyperplane  boundary from a trained SVM to a set of linguistic rule  expression. Nunez et al. [1] extract a rule by first clustering  support vectors by k-Mean, then for each cluster, choosing the  prototype vector (i.e. the center of the cluster), and the support  vector furthest to the prototype vector to build an ellipsoid  using a geometric method. Finally, the ellipsoid is translated to  a linguistic if-then rule by a symbolic interpretation. Nunez?s  method used a reduced number of support vectors, but the  generated ellipsoid rule overlaps with each other. In addition,  this method requires to have parameters such as the number  Shaoning Pang and Nikola Kasabov are with the Knowledge Engineering & Discovery Research Institute, Auckland University of Technology, Private Bag 92006, Auckland 1020, New Zealand (email: {spang, nkasabov}@aut.ac.nz).

of cluster and initial cluster centers as the prior knowledge.

As an improvement over Nunez?s method, Zhang et al [2]  proposed a hyper-rectangle rule extraction (HRE) method.

Instead of clustering support vectors, the HRE clusters the  original data using a support vector clustering (SVC) method  to find the prototypes of each class samples, then constructs  hyper-rectangle by the obtained prototypes and support vectors  from the trained SVM. Because of the merits of the SVC  algorithm, HRE can generate high quality rules even when  training data contains outliers. Later, Fu et al [3] developed  RulExSVM (rue extraction from support vector machine).

RulExSVM generates rules straightforwardly based on each  of the support vectors. The rule condition is structured as a  conjunction of several attribute conditions, each of them is  built upon a hyper-rectangle associated with a certain support  vector. RulExSVM is easy, but needs a tuning and pruning  phase to reduce rules with overlap and outliers.

The above SVM rule extraction methods are regarded as  incomprehensible techniques because they are completely sup-  port vector based rule extraction methods, and the knowledge  of obtained rules is all concealed in a number of numerical  support vectors that are normally not transparent to the user.

On the other hand, support-vector based rule extraction also  has the difficulty of class-imbalance. In other words, when  class-imbalance occurs in a dataset, the number of samples  from one class is much larger than that from the other class  (in the case of binary class dataset, the class with the smaller  number of samples is called ?skewed? class, and the other class  is called ?overweighted? class), SVM often fails to classify  the skewed class correctly, as most classification algorithm in-  cluding SVM assume an balanced distribution of classes [14].

Thus, the extracted rules from SVM-rule favor frequently the  classification of the weighted-class, but lose almost completely  the accuracy of the skewed-class classification.

Towards dealing with both the comprehensibility and the  class-imbalance difficulties for SVM rule extraction, our idea  is to put those numerical support-vector rules into a compre-  hensible decision-tree structure, so that the comprehensibility  of the rule set can be improved because that different sets of  support-vector rules are traceable within the decision tree, and  the number of support vectors can be reduced because part  of support-vector rules have been transformed into a set of  simple and comprehensible decision-tree rules. Additionally,  support-vector rules are encoded over a group of individual  simple SVMs such as linear SVM, instead of SVMs with  a complicated classification hyperplane, so that the class-  imbalance problem can be treated easily by allocating more      computing power (i. e. individual SVMs) for the classification  of the skewed-class. Motivated by this, we propose a new  method for rule extraction from SVM by first encoding the  knowledge of original data into an SVM classification tree,  then decoding the knowledge out of the SVMT in the form  of association rule. The following is a summary of the steps  of the proposed r-SVMT algorithm, which consists of SVM  classification tree construction, and followed by three steps of  rule extraction over the SVMT, including rule extraction over  the tree structure, the SVM node, and the 1-class SVM node,  respectively. Individual steps of the r-SVMT are detailed in  the subsections below.



II. SVM CLASSIFICATION TREE MODELLING  Mathematically, a 2 class SVM tree(SVMT) can be for-  mulated as a composite structural model as follows: Given a  2-class dataset D for classification, and a predefined data par- titioning function P on D, the whole dataset D can be divided through an optimized P? into N partitions {g1, g2, ? ? ? , gN}.

Then, an SVMT can be modelled as,  TSV MT (P, fSvm<2> i  , f<1>Sgnj , f <2> Sgnk  ,x) (1)  where Svm<2>i is a local 2-class SVM classifier. Sgn <1> j  and Sgn<2>k represent a regional ?1-class classifier? of class 1 and class 2, respectively. i = 1, ..., I, j = 1, ..., J, k = 1, ..., L. I,J,L such that I + J + L = N , correspond to the number of three data-partition types: partition with data from  class 1 and class 2, partition with only data from class 1, and  partition with only data from class 2, respectively. I,J,L and  N are determined after the SVM tree model generation.

In the case that gi contains 2 classes data, a typical 2-class SVM fSvm is applied to model a local fi on gi as,  fSvm<2> =  l? i=1  yi((wi) T ??(xi) + b?i )) (2)  where ? is the kernel function, l is the number of training samples, and w, b is optimized through  min  (wi)  T wi + C(  L? t=1  (?i))k, (3)  yi((wi) T ?(xt) + bi) ? 1 ? (?  i),  where C and k are used to weight the penalizing variable ?, ?(x) acts the role of kernel function.

In another case, when gi contains only data from 1-class, gi can be modelled strictly as a 1-class classifier [6], where outlier samples are identified as negative samples among the  positive samples, the origin data of the partition. Following  ([6]), a 1-class SVM function can be modelled as an outlier  classifier by setting positive on the origin data of the partition  S and negative on the complement S?:  fSgn<i> =  { +1 if x ? S ?1 if x ? S?  (4)  where i represents class label ?1? or ?2? in binary classification.

In practice, the above 1-class classifier also can be simplified  as: if x by P belongs to class i, then the output of the 1- classifier is assigned as i.

Thus, we can have the decision function of SVMT f? as,  f?(x) =  ?? ?  fSgn<1> if P(x) ? class 1 fSgn<2> if P(x) ? class 2 fSvm<2> otherwise  (5)  where 1-class classifier Sgn<1> and Sgn<2> are 1-class regional decision maker of SVMT, and typical single SVM  classifier Svm<2> is two-class regional decision maker of SVMT.

Clearly, errors may occur in the classification of constructed  f? , as f? may differ from the true classification function f . Thus, a suitably chosen real-value loss function L = L(f? , f) is used to capture the extent of this error. Loss L is therefore data dependent:  L = |f? ? y| (6)  where y = f(x) represents the true classification value. As f? is applied to datasets drawn from the whole data D under a distribution of g. The expected loss can be quantified as,  E[L] = |f? ? y|g(D)dD. (7)  In experiments, g can be realized by applying a 10-fold cross validation policy on D. Since we have no information of the real classification function f , for simplicity we can fix y as the class label of the training dataset.

Thus, given data drawn under a distribution g, constructing an SVM tree requires choosing a function f ? such that the expected loss is minimized:  f? = arg min f??F  |f? ? y|g(D)dD (8)  where, F is a suitably defined SVM tree functions.

Substituting f? to Eq. (6), the loss function of SVMT becomes:  L =  I? i=1  |fSvm<2> i  ?fi|?2i+  J? j=1  |f<1>Sgnj?fj |q1j+  K? k=1  |f<2>Sgnk?fk|q2k  (9)  where fi,fj and fk are the local true values of f . ?2i represents the distribution probability of the ith partition that contains both class 1 and class 2 data. q represents the distribution probability of a partition that contains the data of one class  (i.e. either class 1 or class 2).

In Eq. (9), L is determined by the performance of every regional classification from a regional SVM classifier or a 1-  class classifier. Given every SVM in SVMT with the same  kernel and penalty parameters, loss function L now depends only on the data distribution of local regions. In other words,  the data partitioning method P eventually determines the loss function of the resulting SVM tree.

In this sense, the construction of SVMT f ? is equivalent to seeking a partition function P? that is able to minimize the  2008 International Joint Conference on Neural Networks (IJCNN 2008) 2487    following loss function,  P? = arg min P?[P]  |f? ? y|g(D)dD (10)  = arg min P?[P]  I? i=1  |fSvm<2> i  ? fi|?2i  +  J? j=1  |f<1>Sgnj ? fj |q1j +  K? k=1  |f<2>Sgnk ? fk|q2k  where, [P] is a set of suitably defined 2-split data partition functions P . The first term of the above function is the risk from SVM classification, and the remaining two terms are the  risks from 1-class classifier fSgn.

The risk from fSgn can be minimized through the training  of a 1-class SVM classifier. In the case that Sgn is simplified by making every decision of classification through P , the classification risk can be simply evaluated by the loss function  of P . Due to the loss function of P has been minimized when the data partitioning is performed. For a binary partitioning  [X1,X2] = P(X), the risk can be the same minimized by an SVM training using X1 and X2 as class ?+1? and ?-1? respectively. So, Eq. (10) can be updated as,  P? = arg min P?[P]  |f? ? y|g(D)dD (11)  = arg min P?[P]  I? i=1  |Svm<2>i ? fi|?2i.

Thus, we can solve the above function by finding out every  individual partition that has the minimized SVM classification  risk,  P? = arg min P?[P]  |Svm<2>i ? fi|?2i. (12)  Eq. (12) is proportional to the size of the region (i.e. number  of samples in the region), thus in practice Eq. (12) can be  implemented by seeking data partitions with different sizes  and a minimized SVM classification risk. To this end, P? is modelled as the following recursive supervised and scalable  data partitioning procedure,  [g1, g2, ? ? ? , gj , ? ? ? gN ] = P n(X, ?0), (13)  Subject to : LSvm(gj) < ? or, gj ? one class  where X is the input dataset. ?0 is the initial partitioning scale, and LSvm = |fSvmi ? fi| is a local 2-class SVM classifier loss function. ? is the SVM class separability thresh- old. Pn represents a recursive partitioning function such that Pn(X, ?, LSvm) = P(P  n?1(X, ?n?1, LSvm), ?n, LSvm). For every iteration, optimal partitions are extracted out, and the  remaining samples go to the next iteration of partitioning.

As a result of Eq. (12) and Eq. (13), partitions with LSvm minimized is produced by P at different scales. Each of them builds a 2-class regional decision maker fSvm<2> . Meanwhile, partitions that contain only one class data are also being  produced. This builds the 1-class regional decision makers  fSgn<1> and fSgn<2> .



III. SVM TREE CONSTRUCTION  To avoid the overfitting of tree learning [15], we construct  an SVM tree including a spanning order preference.

The idea of depth-first spanning is to expand the tree to a  node of the next layer as quickly as possible before fanning  out to other nodes of the current layer. The procedure is as, at a  data partitioning scale ?, once has one data partition judged as classification optimal (either a 1-class partition or a partition  with LSvm < ?), then the tree expansion goes to the next layer, and one terminal node is produced at the current layer  of the tree. Thus Eq. (13) can be realized by repeating the  following binary data partitioning,  [X?,X ?] = P(X, ?, LSvm), (14)  Subjectto : LSvm > ?  where X? is the one optimal partition at scale ? generated by P , and X ? is the remaining dataset such that X = X? ? X  ?.

Breadth-first is another tree spanning order preference. The  idea is to fan out to as many nodes as possible of current layer  before expanding the tree to a node of the next layer. This can  be explained as, at a data partitioning scale ?, if there are maximum ? data partitions judged to be classification optimal (either a 1-class partition or a partition with LSvm < ?), then ? nodes are produced at current layer of the tree. In this case, Eq. (13) uses a multiple data partitioning,  [{X1,X2, ..., X?},X ?] = P(X, ?, LSvm) (15)  Subjectto : LSvm > ?  where {X1,X2, ..., X?} is a set of optimal partitions at scale ?, and X ? is the remaining dataset such that X = X1?X2...? X?} ? X  ?. ? is the number of optimal partition at scale ?, and it is determined by partitioning function ? and the used partitioning scale ?.

A. The SVMT Algorithms  As constructing an SVMT, Eq. (10) is implemented by a  2-step recursive procedures. First, a partition function is taken  to split the data at a certain scale into two or more partitions.

Next, for those partitions that are optimal for the 2-class  pattern classification, a 1-class classifier or an SVM classifier  is built to serve as a node of the SVMT. The following  algorithm presents the detailed steps of constructing an SVMT  (Depth-first spanning (DFS) SVMT or Breadth-first spanning  (BFS) SVMT).

[Algorithm 1. SVMT]  Inputs:Xtrain (Training dataset), ?0 (the initial partition scale), ? (the threshold of SVM loss function), K (the used type of SVM Kernel)  Outputs:T (a trained BFS SVMT) Step 1:Initialize T as the root node, and current partitioning  scale ? as ?0.

Step 2:if Xtrain is empty then iteration stops, return con-  structed SVMT T .

2488 2008 International Joint Conference on Neural Networks (IJCNN 2008)    Step 3:Perform Eq.(15) data partitioning with current  partitioning scale ?, and obtain data partition {X1,X2, ..., X?}, ? ? 1. Note that if ? = 1, then it is a DFS-SVMT; if ? > 1, then it is a BFSSVMT.

Step 4:For each data partition Xi, do the following opera- tions  (a) if Xi belongs to 1-class, then train a 1-class SVM as Eq. (4), and append a 1-class SVM  node to T .

(b) otherwise, Xi belongs to two-class such that the  classification loss function of Xi is greater than ?, train a standard two-class SVM as Eq.(2)and Eq. (4) over this partition, and append a two-  class SVM node to T .

Step 5:if no new node is added to T , then adjust current partitioning scale by ? = ? ???.

Step 6:set Xtrain as Xtrain ? {X1,X2, ..., X?}, and go to Step 2.

To test the above constructed SVMTs, a input sample x is  first judged by the test function T (x) at the root node in the SVM tree. Depending on the decision made by the root node,  x will be branched to one of the children of the root node.

This procedure is repeated until a leaf node or an SVM node  is reached, then the final classification decision is made for the  testing sample T (x) by a node 1-class classifier or a single SVM classification.

In the above SVMT algorithms, K specifies the type of  SVM used in SVMT construction. ?0 determines an initial resolution for SVMT to start zooming in the data and to con-  struct an SVMT over the data. ? is the permitted minimum loss function value, which gives a criterion of partition selection  for optimal SVM classification. The default ? can be set as the biggest the partitioning scale that the used partitioning method  allows. For example, for the K-Mean partitioning method, ?0 can be set as 2; for ECM partitioning method, ?0 can be set as 0.9. Given a serious class-imbalance and class-overlap of  data, a finer scale can be taken to enable SVMT to separate  classes in a more accurate resolution. The default ? is 0. In practice, a small error is given. Certainly, the smaller ?, the more time cost that it takes to get the SVMT trained.

The above SVMTs cope with class-imbalance and class-  overlap in two parallel ways:  1) The recursive data zooming-in schema, where larger  size data partitions(particularly for those partitions of  overweighed class, e. g. class 1 for Fig. 2 (a) dataset)  are separated out in the first few rounds. This reduces  the imbalance interference for learning of the remaining  data. Meanwhile, data with class-overlap/class-mixture  is being left for the next round, thus SVMT is able to  zoom-in the data, and makes decisions only at a scale  and region, where decision can be reached.

2) SVMT model also can be adjusted to favor the smaller  class (the class with less samples than the other) by  defining LSvm merely on the skewed class.



IV. ASSOCIATION RULE EXTRACTION OVER SVMT  A. Logic Association Rules  According to the literature of rule mining, given a n dimensional data space D, a standard logic rule is formed as  if x ? gi then Class(x) = Ck, (16)  where the IF part is the rule antecedent, and the THEN part  is the rule consequence. The rule means that If x belongs to the subspace gi, the it is assigned a class label Ck.

Typically, there is a conjunction of logical predicate function  T (x) = {True, False} to tell if the condition part of the above rule is satisfied or not. In common case, the predicate  function T is a test on a single attribute of D. For example, if x has attribute ai value that belongs to an interval xi ? [ai?, ai+], then x ? gi.

However, when interpolating the knowledge from the de-  cision tree structure of an SVMT, the predicate function T is extended to be on multiple attributes of D because every subspace gi in an SVMT is obtained through a supervised data partitioning Eq. (13), and both the loss function and the par-  titioning function of Eq. (13) are a multivariate computation.

Thus, the predicate function T will be a multivariate function determined by the choice of partitioning function P  T (x) = P(gi,x) (17)  T (x) judges if x is partitioned into gi. It could be a linear or non-linear distance function depending on the choice of  partitioning function.

On the other hand, when interpolating a support vector from  a local SVM into a lignitic rule, the predicate function T is a conjunction of a set of tests on several relaxant attributes,  T (x) = xi ? [ai?, ai+] ? ... ? xj ? [aj?, aj+]. (18)  B. SVM Nodes Interpolation  As discussed above, a trained SVMT has two types of SVM  nodes:  (1) 2-class SVM node V = {g, ?, fsvm}, where g contains 2 classes data with Lsvm < ? satisfied.

(2) 1-class SVM node V <1> = {g, ?, fsvm<1>} or V <2> =  {g, ?, fsvm<2>}, where g contains either class 1 or class 2 data.

For a 2-class SVM node, rule extraction according to [3] is  based on hyper-rectangular boundary built upon support vector  (SV) of a trained SVM. Given support vector sm of class l, the hyper-rectangular boundary for sm is,  {smi + ?2i ? xi ? smi ? ?1i, } (19)  where 1 ? ?pi ? 0, p = {1, 2}, and i = 1, ..., n the sequence number of dimension.

Set Lo and Ho as the lower limit and upper limit of the hyper-rectangular rule along the ith dimension respectively, where,  Lo(i) = smi ? ?1i (20)  2008 International Joint Conference on Neural Networks (IJCNN 2008) 2489    and  Ho(i) = smi + ?2i., (21)  and given a trained SVM, the rules based on support vector  sm can be generated by Fu?s algorithm [3] as:  [Algorithm 2. 2-class SV Rule Extraction]  Input: sm (support vector) Output:Lo (the lower limit of the hyper-rectangular rule),  Ho (the upper limit of the hyper-rectangular rule) Step 1:set l = 1, refers to dimension l Step 2:calculate xl subject to f(x) = 0 and xj = smj  (j = 1, ..., n, and j ?= l) by the Newton?s method Step 3:determine Lo and Ho according to the solutions of  the problem in Step 2. The number of solutions of  xl may be different under different data distribution:  (a) if there is no solution, i.e., there is no cross  point between the line extended from sm along dimension l and the decision boundary, then Lo(l) = 0,Ho = 1  (b) if there is one solution: if sml ? xl, Lo(l) = xl, and Ho(l) = 1, else Lo(l) = 0, and Ho(l) = xl2  (c) if there are two solutions, xl1 and xl2(xl1 ? xl2) : Lo(l) = xl1, and Ho(l) = xl2  (d) if there are more than two solutions the nearest  neighbor xl1 and xl2 of xl2 are chosen from the solutions under the condition xl1 ? smj ,xl2 ? xl2 and the data points {x|xj = smj , j = 1, ..., n, j ?= l, xl1 ? xl ? xl2} are with the same class label with the support vector  sm : Lo(l) = xl1, and Ho(l) = xl2 Step 4:l = l + 1, if l < n, go to Step 2, else end.

Fig. 1 (a) gives an example of hyper-rectangular rule extrac-  tion from a trained 2-class SVM , where the black circles and  points represent the support vectors from class 1 and class 2  respectively. Given a support vector sm = (sm1, sm2) from class 2, there is two cross points (xi1, xi2) and (xj1, xj2) between the extend lines of sm and the decision boundary.

According to Algorithm 4., a hyper-rectangular rule with  Lo(1) = xi1, Ho(2) = xj1, Lo(2) = xi2 and Ho(2) = xj2 is obtained. The coverage area of this rule is identified as the  dashed rectangle in Fig. 1 (a).

For 1-class SVM node, Eq. (4) approximates a 1-class data  through the distinction of the the outliers from the major data  distribution of the class. In this principle, for the knowledge  interpolation of 1-class SVM node, the hyper-rectangle rules  from 1-class SVM node are required to cover as much the  class of data as possible. To expand the coverage of the  hyper-rectangle rule, given two support vectors sm and sn from a trained 1-class SVM, where smi ?= sni along the ith dimension, the above Lo and Ho of Eq. (20) and Eq. (21) are updated as,  Lo(i) = min(smi, sni) (22)  and  Ho(i) = max(smi, sni). (23)  +1  +1  -1  0.5  0.6  0.7  0.8  0.9  1.0  X1  0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 X2  support vector Data sample  0.

0.4 0.

0.6 0.

0.8 0.

1.0  0.

0.

0.

0.

0.

1.

Sm  X1  X2  (xi1,xi2 )  (sm1 ,sm2)  (xj1 ,xj2 )  Class 1 Class 2  Class 1 Support Vector  Class 2 Support Vector  (a)  (b)  Fig. 1. Examples of hyper-rectangular rule extraction from a trained 2-class SVM (a) and 1-class SVM (b) nodes.

It is noticeable that the above method has the problem of  rule disaster. Given m 1-class n dimension support vectors, it generates n.m(m ? 1)/2 rules for Eq. (22) and Ea. (23). In other words, a small number of support vectors gives possible  a huge number of rules for the same partition and the same  class of data. Clearly, the redundancy of rules occurs here.

Alternatively, we use all 1-class support vectors together  for the rule extraction of 1-class SVM, because the data from  1-class SVM, both the outliers and the origin of the class  eventually belong to the same class, and there is no need to  separate one from another from the viewpoint of classification.

Therefore, the above rule extraction of Eq. (22) and Eq. (23)  can be re-written as,  Lo(i) = min(s1i, s2i, ..., smi, ...) (24)  Ho(i) = max(s1i, s2i, ..., smi, ...). (25)  [Algorithm 3. 1-class SV Rule Extraction]  Input: s1, s2, ..., sm (set of support vector from a trained 1-class SVM)  Output:Lo (the lower limit of the hyper-rectangular rule), Ho (the upper limit of the hyper-rectangular rule)  Step 1:set l = 1, refers to dimension l Step 2:calculate Lo(l) by Eq.(24) Step 3:calculate Ho(l) by Eq.(25) Step 4:l = l + 1, if l < n, go to Step 2, else end.

Algorithm 3 describes the algorithm of the simplified 1-  class SV rule extraction. Fig. 1 (b) gives an example of such  rule extraction, where only one hyper-rectangle is built for the  whole 1-class data, and the hyper-rectangle covers most of the  1-class data.

2490 2008 International Joint Conference on Neural Networks (IJCNN 2008)    C. The Proposed r-SVMT Algorithm  Over the encoded SVMT in above Section, the rule knowl-  edge is decoded through decision-tree rule extraction and SVM  node knowledge interpolation. An SVM classification tree is  first modelled on the training data, and followed by three  rule extraction procedures over the obtained SVMT, including  rule extraction over the tree structure, 2-class SVM node,  and 1-class SVM node, respectively. Consequently, the final  rule set is obtained by merging all the obtained rules via  a redundancy pruning procedure. Algorithm 4. presents the  proposed algorithm for rule extraction over SVMT, where  individual steps have been detailed in the above subsections.

[Algorithm 4. Rule Extraction Over SVM Trees(r-SVMT)]  Inputs: Xtrain (Training dataset) Output:R (tuned ruleset), C (Decision tree ruleset), S (Sup-  port vector ruleset)  Step 1:Apply the SVM classification tree algorithm (Algo-  rithm 1) to construct T over Xtrain.

Step 2:For every path in T from the root node to a terminal  node of SVMT, extract rules C by taking all test functions along the path as the conjunctive rule  antecedent, and the class label held by the leaf as  the rule consequence, R = C.

Step 3:For every terminal node (i.e. SVM node or 1-class  SVM node) in T , do the following operations:  (a) if the terminal node that the leaf is attached  is an SVM node, apply Algorithm 2 to the  terminal node for the support vector rule Rsvm2 extraction, S = S?Rsvm2 and R = R?Rsvm2 .

(b) if the terminal node to which the leaf is attached  is a 1-class SVM node, apply Algorithm 3 to  the terminal node for 1-class support vector rule  Rsvm1 extraction, S = S ? Rsvm2 and R = R ? Rsvm1 .

Step 4:labelled samples in Xtrain to fall into a region of each of the above obtained rules in R.

Step 5:if the set of sample in a certain rule region is a  subset of samples covered by another rule, this rule  is removed.

Step 6:repeat Step 3 until no rule is removed from R, output R, C, and S.



V. EXPERIMENTS AND APPLICATIONS  In our experiments, algorithms were implemented in Matlab  Version 6.5, run on Pentium 4 PC, 3.0GHZ 512Mb RAM. For  extracting rules over SVMT, the 1-class SVM is set with a  RBF ? = 3.5 kernel, and the 2-class SVM with a standard linear kernel. All SVMs have the same penalty coefficient  of 0.1. ? is assigned as a K-means clustering partitioning function, and all SVMTs use the same default parameters of  ?0 = 2 and ? = 0.02. Because Polynomial SVMs perform better than linear SVM on most classification tasks, for a  simple validity verification we merely compare the r-SVMT  with the SVM-Rule, and the rule extraction from a trained 2nd  -4 -2 0 2 4 6  -4  -3    -4 -2 0 2 4 6  -4  -3    -2  -1     -2  -1     -4 -2 0 2 4 6  -4  -3  -2  -1      (b)  -4 -2 0 2 4 6  -4  -3  -2  -1       (c)  -4 -2 0 2 4 6  -4  -3  -2  -1       Class 1 Class 2  (a)  x1  x2  x1  x2  x1  x2  Fig. 2. (a) the original data distribution, (b) constructed SVMT hyper-plane, (c) the data partitioning structure of the constructed SVMT hyper-plane.

order polynomial single SVM [3]. Note that all single SVMs  are also set with the same penalty coefficient of 0.1.

To setup the class-imbalance criterion for comparing the  input bias (i.e. bias of class data distribution) with the  output bias (i.e. bias of class classification accuracy) of  the system, we define the class bias ratio (CBR) as, 1 ? min(class 1, class 2)/max(class 1, class 2), where a larger CBR value means a more imbalanced of the class data  distribution, or the class classification accuracy.

A. Synthetic Dataset  We first experimented the r-SVMT with a synthetic dataset  for classification. Fig. 2 (a) shows the distribution of the  dataset, where class 1 and class 2 data are both in a 2D  Gaussian distribution, and the number of class 2 samples is  approximately 1/10 of class 1, which follows that the CBR of  this dataset in terms of class data distribution is 0.9.

In the construction of SVMT, the data is zoomed in recur-  sively by a supervised data partitioning. Fig. 2 (b) shows the  resulting hyper-plane from Algorithm. 1 as the data partition-  ing step goes to the second iteration. The SVMT hyper-plane  2008 International Joint Conference on Neural Networks (IJCNN 2008) 2491    P0  SVM  P2    P3  SVM    SVM   P4   P5  P6  SVM     P7   P8   P9  P10  SVM     P11   P12  P13  SVM    P14  P15 SVM    P16   P17  P18  2P19   P20  P21   P22   P23  SVM    P1    SVM   SVM  SVM  P2             P3    P4      SVM    P5   SVM    SVM        SVM   P6    P7   SVM      Fig. 3. Example of DFS-SVMT (Top) and BFS-SVMT (Bottom) from SVMT rule extraction over Fig. 2 (a) dataset.

in Fig. 2 (b) is a combination of the data partitioning boundary  estimated in Fig. 2 (c) and two pieces of regional SVM hyper-  planes. It turns out that SVMT conducts classification using  less support vectors from SVM, but more decision boundaries  from data partitioning. In other words, SVM is not used unless  it is absolutely necessary.

Fig. 3 gives an example of DFS-SVMT and BFS-SVMT  generated by Algorithm 1. The root node P1 represents the data partitioning function at scale ?0, P2 for the data partitioning at scale ?1, so on so forth. Every 1-class node here is denoted as a circle labelled with class ?1? or ?2?, and  SVM node denoted as a ellipse circled ?SVM? label. Each  SVM node has two sons , which means that each SVM node  provides, class 1 and class 2, two decision choices. They are  symbolized the same as the 1-class node, but they are not  counted here as 1-class node. The DFS-SVMT is seen to have  8 SVM nodes, 14 1-class nodes, and the BFS-SVMT has 8  SVM nodes, 24 1-class nodes. The number of 1-class nodes  is several times of SVM nodes, suggesting that BFS-SVMT  makes decision more dependent on the tree than DFS-SVMT  does. As a result, 4 example association rules extracted over  dataset shown in Fig. 2 (a) by Algorithm 4. are given below,  (1) if 1.89 <= x1 <= 4.95 and ?5.0 <= x2 <= ?2.3 then [x1 x2] in cls.1  (2) if ?5.00 <= x1 <= ?2.00 and 2.42 <= x1 <= 5.00 then [x1 x2] in cls.1  (3) if ?5.00 <= x1 <= ?2.00 and ?2.42 <= x1 <= 2.30 and x1  ? 0.34 + x2 ? 1.87 + 0.987 > 0 then [x1 x2] in cls. 2.

(4) if ?5.00 <= x1 <= ?2.00 and ?2.42 <= x1 <= 2.30 and x1  ? 0.34 + x2 ? 1.87 + 0.987 < 0 then [x1 x2] in cls. 1,  where the first two rules are encoded directly from the obtained  SVM tree structure, and the remaining rules are from a reginal  SVM.

Table I compares the r-SVMT with the SVM-Rule and the  original SVMT on the classification of the above synthetic  dataset, based on the following 6 measurements:  (1) the number obtained rules (N. of Rules).

(2) the number of support vectors (N. of SVs).

(3) the average general classification accuracy (Ave. Acc.).

(4) the classification accuracy of class 1 (Cls1 Acc.)  (5) the classification accuracy of class 2 (Cls2 Acc.)  (6) the class bias ratio in terms of class classification accu-  racy (CBR).

For the CBR 0.9 dataset in Fig. 2 (a), the proposed r-SVMTs  come up with a class classification accuracy with CBR ? 0.3, while the SVM-Rule gives a completely imbalanced output  with CBR = 1.0. It is distinct that the proposed r-SVMTs have profited from the outstanding generalization ability of  SVMTs as both statistics are shown to be quite similar in Table

I. The two BFS type SVMT methods (i. e. BFS r-SVMT and  BFS-SVMT) perform even better than the DFS type SVMT  methods (i. e. DFS r-SVMT and DFS-SVMT), this suggests  that BFS is able to fit the data more appropriated than DFS  for constructing SVMT.

B. Cancer Diagnosis  We have implemented the BFS r-SVMT technique in the  cancer diagnosis of 7 well-known cancer datasets with a two-  class classification problem [16]. Most datasets were biased  in the sense of having an unbalanced number of patients in  the two classes, e.g. normal vs. tumour. Background deducted  and normalized fluorescence values were used, as in the  corresponding publications.

For the 3 datasets that had an independent validation dataset  available, we selected 100 genes from the training dataset by a  typical t-test gene selection, and used them on the validation  dataset. For the remaining 4 datasets, we verify the method  using 10-fold cross-validation, removing randomly one tenth  of the data and then using the remainder of the data as a  training set. For each fold, we similarly selected 100 genes  over the training set, then applied the obtained 100 genes to  the corresponding testing set.

The comparison of r-SVMT to single SVM-Rule and the  original SVMT on 7 cancer diagnosis is shown in Table II,  where rule extraction is measured in terms of diagnosis accu-  racy, the number of rules, and the number of involved support  vectors, respectively. For comparison, Table II also shows the  predictive accuracy of SVMT. Although the r-SVMT does  not perform better than the SVMT, it is still impressive that  Table II indicates the prior generalization ability of r-SVMT  to SVM-Rule is about 10% (((.808 ? .896)/(1 ? .808) + (.941? .735)/(1? .735)+ ...+(.985? .946)/(1? .946))/7 = .1096). More importantly, the comprehensibility of BFS r- SVMT is clearly improved as compared to SVM-Rule because  the average number of support vectors for r-SVMT is only  about 60% that of SVM-Rule. For CNS and Lung Cancer,  the BFS r-SVMT is found with less support vectors, but even  more rules than the SVM-Rule. This inconsistency owes to  the reason that these two cancers are more difficult than the  other type of cancers for a correct knowledge representation,  2492 2008 International Joint Conference on Neural Networks (IJCNN 2008)    TABLE I  THE RESULTS OF COMPARISON BETWEEN SVM-RULE AND r-SVMT ON THE CLASSIFICATION OF FIG. 5 DATASET.

Measurements SVM Rule DFS r-SVMT DFS-SVMT BFS r-SVMT BFS-SVMT N. of Rules 58 23 (8/15) - 33(9/24) - N. of SVs 32 16 16 18 18 Ave. Acc. 91.3% 92.4% 92.7% 90.0% 90.1 Cls1 Acc. 100% 94.9% 94.8% 91.2% 92.0 Cls2 Acc. 0.0% 66.3% 70.5% 76.9% 79.0 CBR 1.0 0.30 0.25 0.16 0.14  TABLE II  THE RESULTS OF COMPARISON BETWEEN SVM-RULE AND BFS r-SVMT ON 7 CANCERS DIAGNOSIS.

Dataset SVM Rule BFS r-SVMT BFS SVMT Acc. No. of Rules No. of SVs Acc. No. of Rules No. of SVs Acc.

Lymphoma(1)[7] 89.6% 233 60 80.8% 221 38 89.6% Leukaemia[8] 73.5% 245 29 94.1% 108 20 94.1% CNS Cancer[9] 61.7% 234 37 62.9% 278 26 65.0% Colon Cancer[10] 71.3% 205 48 75.3% 202 32 77.3% Ovarian Cancer[11] 96.5% 178 34 98.4% 198 21 97.3% Breast Cancer[12] 52.6% 269 62 78.9% 206 33 78.9% Lung Cancer[13] 94.6% 171 23 98.5% 188 17 99.3%  where the BFS SVMT has allocated more efforts on decision  tree spanning.



VI. DISCUSSIONS AND CONCLUSIONS  The SVMT aggregates the decision tree and the support  vector machine into an SVM classification tree, manipulates  the two sides rule extraction by selecting the simple part of  the problem for a comprehensible decision tree representa-  tion, and leaving the remaining difficult part for a support  vector machine approximation. For extracting rules, r-SVMT  is introduced in the form of DFS r-SVMT and BFS r-SVMT,  which encodes the knowledge of rules by an induction over  the decision tree of SVM, and the interpolation of the support  vectors from local SVMs.

The experimental results and applications have shown that  the proposed r-SVMT, compared to single SVM rule extrac-  tion, has the following desirable properties: (1) The r-SVMT is  more accurate, and especially more robust for the classification  of data with a class-imbalanced data distribution. (2) The  r-SVMT has a better comprehensibility for rule extraction  because only about half the number of single SVM support  vectors are kept in SVMT rules. (3) The BFS r-SVMT  commonly outperforms the DFS r-SVMT.

The r-SVMT has improved the comprehensibility of SVM  rule extraction by reducing the number of support vectors  effectively, however there are still a few support vectors left  in the rule of the r-SVMT. To further improve the compre-  hensibility of the r-SVMT, it is suggested that a symbolic rule  interpolation could be developed for the r-SVMT in the future.

