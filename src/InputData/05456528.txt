2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics

Abstract  In the paper, we discussed the characteristics of data mining  on association rules for multi-dimension data. Then through  the multi-dimension data attributes analysis and OLAP  operations, we integrate the OLAP and data mining based  on their advantages to one method which is called On-Line  Analysis Mining (OLAM) [1]. Based on OLAM, an  algorithm for multi-dimension data on association rules has  been reformed. It can improve the efficiency and flexible of  rules searching. Finally, our performance has proved the  algorithm.

Key words: Multi-Dimension Data; OLAP; Data Mining;  Association Rules  1. Introduction  Along with the terrific increasing size of database, there  have been growing interests in mining useful and valuable  information from data warehouse to support  decision-making system and avoiding data warehouse from  being data prison. For now most data exist in the form of  multi-dimension model, for example data cube is an  important shape of data warehouse to be analyzed and  organized. It is defined by dimension and fact.

OLAP (On-Line Analysis Processing) can construct  data cubes of vary types from original data. It also can  examine and analyze data cubes by roll-up, drill-down [1]  which are multi-dimensional, high efficient and multi-view,  then return the result to users.

Data Mining is finding some useful and previewing  knowledge which is underlying in the data warehouse.

Among these knowledge, association rules [2] are the one  of most important.

Traditional data mining algorithms on association rules  are mainly acting on classic relational database which is not  suitable for mining multi-dimension association rules, so  how to find a high efficient and utility DM algorithm for    this problem has become an important task. Compare to  classic single-dimension association rules, multi-dimension  association rules are the relationship of the data of every  dimensions, for example: age (x, 2029) ? business (x,  student) ? buys (x, computer).

Based on mining single-dimension Boolean association  rules, Apriori [1, 5] algorithm is the most classic and  effective algorithm on traditional DM, but they dont suit  for mining multi-dimension association rules. So a large  number of increasingly efficient algorithms have been  proposed [2,3,4,5,6,7].In this paper we use roll-up,  drill-down of OLAP [4] and two prunings skill to improve  the Apriori_Cube algorithm which belongs to OLAM in  order to make it more suitable. At last our performance  study shows that this algorithm is efficient and accurate.

2. Mining Multi-Dimension Association Rules  Based on Data Cube  According to the research of this model in the past, the  process of mining multi-dimension association rules base  on data cube [3]:  (1) Construct data cube from data warehouse based on the  mining task which clients make.

(2) Mine frequent predicate set satisfying the minimum  support from data cube which is built in step 1.

(3) Create association rules in which the users are  interested from frequent predicate set.

Lets discuss every original step.

2.1 The Building Of Data Cube  At the beginning, the server create data cube from data  warehouse according to the task users have make. Present a  task of mining multi-dimension association rules: its  contents have d1  d n dimensions. Then partition the  level of every dimension to meet users mining  requirements and build a data cube by OLAP operations  from data warehouse. Here the level is the different  description of details grade of every dimension. Such as in  time dimension, year is a high level as month, day is the  low ones. Base on this data cube, every dimension contents  | d i | values, | d i | is the amount of different dimension  members from the d i dimension. There exist many  members which are different from each other consisted in  d i dimension. We call the frequency measurement of every    dimension member count. It is recorded in the cell of data  cube. In a word, this mining multi-dimension association  rules problem which involves d1  d n dimensions  correspond an n-dimension data Cube ( d1  d n | count).

d1  d n are the dimensions of the cube as count is its fact  measure [3, 4].

2.2 Ming Frequent Predicate Set From Data Cube  When the Apriori algorithm finds frequent itemset, it uses  the interactive method called search level by level, which  means finding frequent (k+1)-itemset by using frequent  k-itemset. Moreover, finding one Lk (frequent k-itemset)  must scan database once. For the sake of reducing the  amount of itemsets, scanning frequency, and improving the  efficiency of finding frequent predicate set as much as  possible, the algorithm must follow: Every None-Empty  Subset must be frequent [1].

Based on mining multi-dimension association rules, it  is not finding frequent itemset but frequent predicate set.

Such as the example we mentioned before: age (x,  2029) ? business (x, student) ? buys (x,  ???  2010 2nd International Asia Conference on Informatics in Control, Automation and Robotics  978-1-4244-5194-4/10/$26.00 2010 IEEE                                                                  CAR 2010  The Improved On-Line Analysis Mining for Multi-Dimension Data  Based on Association Rules  Yang Bo                   Zheng Yongqing  College of Computer Science and Technology, Shandong University, Shandong, China  digideayb@gmail.com           zyq@dareway.com.cn  Abstract  In the paper, we discussed the characteristics of data mining  on association rules for multi-dimension data. Then through  the multi-dimension data attributes analysis and OLAP  operations, we integrate the OLAP and data mining based  on their advantages to one method which is called On-Line  Analysis Mining (OLAM) [1]. Based on OLAM, an  algorithm for multi-dimension data on association rules has  been reformed. It can improve the efficiency and flexible of  rules searching. Finally, our performance has proved the  algorithm.

Key words: Multi-Dimension Data; OLAP; Data Mining;  Association Rules  1. Introduction    Along with the terrific increasing size of database, there  have been growing interests in mining useful and valuable  information from data warehouse to support  decision-making system and avoiding data warehouse from  being data prison. For now most data exist in the form of  multi-dimension model, for example data cube is an  important shape of data warehouse to be analyzed and  organized. It is defined by dimension and fact.

OLAP (On-Line Analysis Processing) can construct  data cubes of vary types from original data. It also can  examine and analyze data cubes by roll-up, drill-down [1]  which are multi-dimensional, high efficient and multi-view,  then return the result to users.

Data Mining is finding some useful and previewing  knowledge which is underlying in the data warehouse.

Among these knowledge, association rules [2] are the one  of most important.

Traditional data mining algorithms on association rules  are mainly acting on classic relational database which is not  suitable for mining multi-dimension association rules, so  how to find a high efficient and utility DM algorithm for  this problem has become an important task. Compare to  classic single-dimension association rules, multi-dimension  association rules are the relationship of the data of every  dimensions, for example: age (x, 2029) ? business (x,  student) ? buys (x, computer).

Based on mining single-dimension Boolean association  rules, Apriori [1, 5] algorithm is the most classic and  effective algorithm on traditional DM, but they dont suit  for mining multi-dimension association rules. So a large  number of increasingly efficient algorithms have been  proposed [2,3,4,5,6,7].In this paper we use roll-up,  drill-down of OLAP [4] and two prunings skill to improve  the Apriori_Cube algorithm which belongs to OLAM in  order to make it more suitable. At last our performance  study shows that this algorithm is efficient and accurate.

2. Mining Multi-Dimension Association Rules  Based on Data Cube  According to the research of this model in the past, the  process of mining multi-dimension association rules base  on data cube [3]:  (1) Construct data cube from data warehouse based on the    mining task which clients make.

(2) Mine frequent predicate set satisfying the minimum  support from data cube which is built in step 1.

(3) Create association rules in which the users are  interested from frequent predicate set.

Lets discuss every original step.

2.1 The Building Of Data Cube  At the beginning, the server create data cube from data  warehouse according to the task users have make. Present a  task of mining multi-dimension association rules: its  contents have d1  d n dimensions. Then partition the  level of every dimension to meet users mining  requirements and build a data cube by OLAP operations  from data warehouse. Here the level is the different  description of details grade of every dimension. Such as in  time dimension, year is a high level as month, day is the  low ones. Base on this data cube, every dimension contents  | d i | values, | d i | is the amount of different dimension  members from the d i dimension. There exist many  members which are different from each other consisted in  d i dimension. We call the frequency measurement of every  dimension member count. It is recorded in the cell of data  cube. In a word, this mining multi-dimension association  rules problem which involves d1  d n dimensions  correspond an n-dimension data Cube ( d1  d n | count).

d1  d n are the dimensions of the cube as count is its fact  measure [3, 4].

2.2 Ming Frequent Predicate Set From Data Cube  When the Apriori algorithm finds frequent itemset, it uses  the interactive method called search level by level, which  means finding frequent (k+1)-itemset by using frequent  k-itemset. Moreover, finding one Lk (frequent k-itemset)  must scan database once. For the sake of reducing the  amount of itemsets, scanning frequency, and improving the  efficiency of finding frequent predicate set as much as  possible, the algorithm must follow: Every None-Empty  Subset must be frequent [1].

Based on mining multi-dimension association rules, it  is not finding frequent itemset but frequent predicate set.

Such as the example we mentioned before: age (x,  2029) ? business (x, student) ? buys (x,  ???    {a1, b1}, {a1, b2}, {a1, c1}  {a1, c2}, {a1, d1}, {a1, d2}  {a2, b1}, {a2, b2}, {a2, c1}  {a2, c2}, {a2, d1}, {a2, d2}  ......2-itemset  {a1,b1,c1},{a1,b1,b2},  {a1,b1,d1},{a1,b1,c1},  {a1,b1,c2},{a1,b1,d1},  {a1,b1,c1},{a1,b1,c2},.

......3-itemset  computer), predicate set {age, business, buys} is a  3-predicate set. We sign frequent predicate set in the same  way of frequent itemset. Just like using Lk to represent  the set of frequent k-predicate set, C k to represent the set  of Candidate k-predicate set [1].

In the former, there existed many algorithms by using  the idea of Apriori and its several variations [5]. One of  them can mine frequent predicate set from data cube, we  call it Apriori_Cube. The difference between them is  calculating the degree of predicate support instead of  itemset. When mining multi-dimension association rules  from data cube, one predicate set is the very set of  dimension members from different dimensions in data cube  ( d1  d n | count), such as (age-1, business-1, buys-1),  the supporting number of predicate set is the count storage  in the cell of data cube.

2.3 The Formation Of Multi-Dimension Rules  With the frequent predicate set having been found out, it is  easy to pick up association rules [1].

(1)For every frequent predicate set I, seek all its non-empty  subsets;  (2)For every non-empty subset s of the frequent predicate  set, calculate the confident degree of rules s? (I-s). In a  word, confidence = the confident number of I/the  confident number s, if confidence ?? PLQFRQI?? WKHQ? WKH?  rules s? (I-s) is the one we want.

3. The Improve Of Apriori_Cube Algorithm  3.1 The Main Idea Of Improvements  First using OLAP to simplify the data cube; second  improving function gen_candidate ( Lk ) are what we will  do.

1. The dimensions and levels of mining task are determined    by users requiring when data cube is built up. Sometimes,  it cannot certainly find out strong association rules or it may  mine many rules which users arent interested in based on  the levels of dimensions. Such as dimension area, when we  talk about the problems of the world, the country level will  be more useful than the province level. One solution of this  situation is that adjusting the levels based to the amount and  proximity of the rules which have been mined. But it means  mining the data cube once again and while mining  multi-dimension association rules, it also cant determine  the dimensions which need to be adjusted accurately at the  same time. So we think that analyzing the levels of  dimensions at the same time of data mining, then using  operations roll-up and drill-down of OLAP to adjust the  levels, this is called On-Line Analysis Mining, at last  carrying on mining process which will be more propriety  and efficient.

Assume that users make an n-dimensions data cube  and always hope the multi-dimension association rules  mined can contain these n dimensions, on the contrary they  arent interested in the rules containing only (n-1)  dimensions or even less [6]. In a word, it must find out the  frequent n-predicate set. According to Apriori, every 1  subset of frequent n-predicate set must be frequent 1-set.

That means every dimension has a frequent 1-predicate set.

Obviously saying, once if some dimension doesnt contain a  frequent 1-predicate set, the frequent n-predicate set must  not exist.

So we check the frequent 1-predicate set after mining  them from every dimension by algorithm: we can know that  it shows the partition level of dimension we made is too low  if some dimension doesnt have frequent 1-predicate set so  we should raise the level of this dimension by roll-up; in the  opposite, if every 1-predicate set of some dimension is  frequent 1-predicate set, we should drill-down to drop the  level of this dimension [3]. All of these adjustments will be  embedded in mining process in order to strengthen the  flexibility and targeting.

2. In recent years, we also meet many problems like this:  when mining rules, most quantitative dimensions can be  scattered, for example: dimension deposit can be parted in  the form of interval as [0, 20K], [21k, 30k]and so on.

This discretization is commonly completed before mining  [1]. So we can easily know that [0,20k], [21k, 30k] and the  other intervals which belong to one dimension cant exist at  the same time [4]. But there are still some special situations  such as year which is parted by quarter  level(Q1,Q2,Q3,Q4), some users are interested in that  problem what about the sales in both quarter Q1 and Q2, so  when we improve the algorithm we must think about it too.

First of all, it is obviously that when mining  multi-dimension association rules, one predicate set  contains no more than one different level from the same  dimension [5] without special requirements of users.

Combine this point with Apriori_Cube algorithm can help  reduce the amount of useless predicate set, especially with  the great growing of the amount.

Example 1 There are 4 dimensions of the database: A, B, C,  D, when the discretization of them has been complete, we  can built a table of them like Table 1:  A B C D  a1 b1 c1 d1  a2 b2 c2 d2  Table 1:Examp1       Graphic1: Connection of example 1  When the 3-itemset come out, itemset {a1, b1, b2}  must be contained, according we discussed before it is  obviously unsubstantiated. We will use one predicate set  contains no more than one different level from the same  dimension to do the first pruning, so that this condition  will not appear ever.

Second, when users make special requirements, we  also should do some improvements. Such as the example  before, when the algorithm discovered that one predicate set  contains different levels from the same dimension as {a1,  b1, b2} and checked out that this is what the users are  interested in, so we will calculate the sum of the counts of  {a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and  keep it in the frequent 3-itemset. We call this situation SP.

The algorithm improved also has steps connection and  pruning, but the difference is that we use one predicate set  contains no more than one different level from the same  dimension and the SP to do the first pruning in function  gen_candidate ( Lk ); then use original Apriori method do  second pruning by using minsup.

o  ???  {a1, b1}, {a1, b2}, {a1, c1}  {a1, c2}, {a1, d1}, {a1, d2}  {a2, b1}, {a2, b2}, {a2, c1}  {a2, c2}, {a2, d1}, {a2, d2}  ......2-itemset  {a1,b1,c1},{a1,b1,b2},  {a1,b1,d1},{a1,b1,c1},  {a1,b1,c2},{a1,b1,d1},  {a1,b1,c1},{a1,b1,c2},.

......3-itemset  computer), predicate set {age, business, buys} is a  3-predicate set. We sign frequent predicate set in the same  way of frequent itemset. Just like using Lk to represent  the set of frequent k-predicate set, C k to represent the set  of Candidate k-predicate set [1].

In the former, there existed many algorithms by using  the idea of Apriori and its several variations [5]. One of  them can mine frequent predicate set from data cube, we  call it Apriori_Cube. The difference between them is  calculating the degree of predicate support instead of  itemset. When mining multi-dimension association rules  from data cube, one predicate set is the very set of  dimension members from different dimensions in data cube  ( d1  d n | count), such as (age-1, business-1, buys-1),  the supporting number of predicate set is the count storage  in the cell of data cube.

2.3 The Formation Of Multi-Dimension Rules  With the frequent predicate set having been found out, it is  easy to pick up association rules [1].

(1)For every frequent predicate set I, seek all its non-empty  subsets;  (2)For every non-empty subset s of the frequent predicate  set, calculate the confident degree of rules s? (I-s). In a  word, confidence = the confident number of I/the  confident number s, if confidence ?? PLQFRQI?? WKHQ? WKH?  rules s? (I-s) is the one we want.

3. The Improve Of Apriori_Cube Algorithm  3.1 The Main Idea Of Improvements  First using OLAP to simplify the data cube; second  improving function gen_candidate ( Lk ) are what we will    do.

1. The dimensions and levels of mining task are determined  by users requiring when data cube is built up. Sometimes,  it cannot certainly find out strong association rules or it may  mine many rules which users arent interested in based on  the levels of dimensions. Such as dimension area, when we  talk about the problems of the world, the country level will  be more useful than the province level. One solution of this  situation is that adjusting the levels based to the amount and  proximity of the rules which have been mined. But it means  mining the data cube once again and while mining  multi-dimension association rules, it also cant determine  the dimensions which need to be adjusted accurately at the  same time. So we think that analyzing the levels of  dimensions at the same time of data mining, then using  operations roll-up and drill-down of OLAP to adjust the  levels, this is called On-Line Analysis Mining, at last  carrying on mining process which will be more propriety  and efficient.

Assume that users make an n-dimensions data cube  and always hope the multi-dimension association rules  mined can contain these n dimensions, on the contrary they  arent interested in the rules containing only (n-1)  dimensions or even less [6]. In a word, it must find out the  frequent n-predicate set. According to Apriori, every 1  subset of frequent n-predicate set must be frequent 1-set.

That means every dimension has a frequent 1-predicate set.

Obviously saying, once if some dimension doesnt contain a  frequent 1-predicate set, the frequent n-predicate set must  not exist.

So we check the frequent 1-predicate set after mining  them from every dimension by algorithm: we can know that  it shows the partition level of dimension we made is too low  if some dimension doesnt have frequent 1-predicate set so  we should raise the level of this dimension by roll-up; in the  opposite, if every 1-predicate set of some dimension is  frequent 1-predicate set, we should drill-down to drop the  level of this dimension [3]. All of these adjustments will be  embedded in mining process in order to strengthen the  flexibility and targeting.

2. In recent years, we also meet many problems like this:  when mining rules, most quantitative dimensions can be    scattered, for example: dimension deposit can be parted in  the form of interval as [0, 20K], [21k, 30k]and so on.

This discretization is commonly completed before mining  [1]. So we can easily know that [0,20k], [21k, 30k] and the  other intervals which belong to one dimension cant exist at  the same time [4]. But there are still some special situations  such as year which is parted by quarter  level(Q1,Q2,Q3,Q4), some users are interested in that  problem what about the sales in both quarter Q1 and Q2, so  when we improve the algorithm we must think about it too.

First of all, it is obviously that when mining  multi-dimension association rules, one predicate set  contains no more than one different level from the same  dimension [5] without special requirements of users.

Combine this point with Apriori_Cube algorithm can help  reduce the amount of useless predicate set, especially with  the great growing of the amount.

Example 1 There are 4 dimensions of the database: A, B, C,  D, when the discretization of them has been complete, we  can built a table of them like Table 1:  A B C D  a1 b1 c1 d1  a2 b2 c2 d2  Table 1:Examp1       Graphic1: Connection of example 1  When the 3-itemset come out, itemset {a1, b1, b2}  must be contained, according we discussed before it is  obviously unsubstantiated. We will use one predicate set  contains no more than one different level from the same  dimension to do the first pruning, so that this condition  will not appear ever.

Second, when users make special requirements, we  also should do some improvements. Such as the example  before, when the algorithm discovered that one predicate set  contains different levels from the same dimension as {a1,  b1, b2} and checked out that this is what the users are  interested in, so we will calculate the sum of the counts of  {a1, b1} and {a1, b2}, then assign it to {a1, b1, b2} and  keep it in the frequent 3-itemset. We call this situation SP.

The algorithm improved also has steps connection and  pruning, but the difference is that we use one predicate set  contains no more than one different level from the same  dimension and the SP to do the first pruning in function    gen_candidate ( Lk ); then use original Apriori method do  second pruning by using minsup.

o  ???  B4  B3  B2  B1  To sum up the above arguments, we put forward the  improved algorithm.

3.2 Apriori_Cube_Improved Algorithm  Algorithm: Apriori_Cube_Improved  Input: N-Dimension Data Cube ( d1  d n | count), Minimum  support: minsup  Output: The set of frequent predicate set L  Process:  (1) Initialization: L:= ?; L1 =?; k=1; j=0;  (2) Every dimension members of dimension d i (i=1n) make  up the candidate 1-predicate set and we use symbol C1 to  represent the set of them;  (3) L1 =gen_frequent( C1 );  (4) Count the max_support and min_support of frequent  1-predicate set of every one dimension;  (5) For i=1 To n Do  1) If max_support < minsup Then roll-up the i dimension;  2) If min_support > minsup Then drill-down the i dimension;  3) j=j+1;  (6) If j > 0 Then  1) j=0;  2) repeat (2)(3)(4)(5)  // the new improvement of step 1.

(7) L=L? L1 ;  (8) While(k ??Q??DQG?? Lk ????'R  1) C k =gen_candidate_improved( Lk 1? );  //The new function will be expanded below.

2) Lk =gen_frequent( C k );  3) L=L? Lk ;  4) k=k+1;  (9) Output L  New Function gen_candidate_improved ( Lk 1? )  (1) Initialization: C k =?;  (2) For every 2 frequent predicate set I1 and I 2 from    Lk 1? Do  1) If (there exists k-2 items in the same between I1 and  I 2 ) Then C= I1 f I 2 ;  a) If C contains different levels from the same dimension  If there exits requirements of the users Then sup_count of C =  +   , go to b); Else delete C;  b) If all the subsets of c belong to Lk 1? Then  C k = C k ?c; Else delete c  (3) Output Lk ?I  Function gen_frequent ( C k );  (1) Initialization: Lk  ??  (2) For every candidate k predicate set I=( I1  I k ) which  come from the set do  1) Supporting count sup_count=the count storage in the cell of  n-dimension data cube( I1  I k );  Support=sup_count/total_count;  2) If support ??PLQVXS?7KHQ? Lk = Lk ?I;  (3) Output Lk ;  4. Performance Analysis  Example 2 Lets talk about a practical problem just like the  status of sales. Assume that we will mine the association  rules involved 4 dimension attributes of sales, the  minsup=25%. First of all, using OLAP technology to build  a 4-D data cube and the 4 dimension attributes are: time,  location, item, and supplier. For location dimension which  contains area, country and so on, we choose province level.

We use brand level for item dimension, company level for  supplier dimension. Time dimension can be divided as Q1  (1-3), Q2 (4-6), Q3 (7-9), Q4(10-12),and dimension  location(P1,P2,L1,L2) such as china-Shandong, USA-New  York, item(B1,B2,B3,B4),supplier(C1,C2,C3). Then the  sales data cube can be generalized like this:  Graphic 2: The 4-Dimension Data Cube of Sales  The details of this sales data cube are in the table  follow: The amount of cells is 100.

Location Time Item Supplier Count  Cell-1 P1 Q1 B1 C2 5  Cell-2 P1 Q3 B1 C1 2  Cell-99 L1 Q3 B1 C1 3  Cell-100 L2 Q4 B1 C3 11  Table 2: The details data table of sales data cube  We use original Apriori_Cube Algorithm to find    frequent predicate set with minsup_num= 25%*100=25.

According the data table we calculate that sup_count of  every member of dimension L is (P1:8, P2:5, L1:1, L2:24),  and also T, I, S. So there is the process:  Graphic 3: The processes of old algorithm  As we know, through comparing with the minsup_num,  dimension location has no one frequent 1-predicate set, so  that there have no frequent 4-predicate set in the output by  the original algorithm. But users are interested in the  Candidate 1-Predicate set  L T I S  P1  P2  L1  L2  Q1  Q2  Q3  Q4  B1  B2  B3  B4  C1  C2  C3  Frequent  1-Predicate set  T I S  Q1  Q3  B2  B3  C1  C2  I1 I 2  C1              C2                 C3      5 1       8 11  Q 1  Q 2  Q 3  Q 4  P 1  P 2  L 1  L 2  Candidate  2-Predicate set:  {Q1,B2},{Q1,B3}  {Q1,C1},{Q1,C2}  {Q3,B2},{Q3,B3}  {Q3,C1},{Q3,C2}  Candidate  3-Predicate  set:  {Q1,B2,B3}{Q  1,Q3,B2},{Q3,  C1,B2}  Frequent  2-Predicate  set:  {Q1,B2}{Q1,B  3},{Q3,B2},{  Q3,C1}  Output: 1L U 2L U 3L  Frequent 3-Predicate set:  {Q1,B2,C1},{Q1,B3,B2}  {Q1,B2,B3}  ???  B4  B3  B2  B1  To sum up the above arguments, we put forward the  improved algorithm.

3.2 Apriori_Cube_Improved Algorithm  Algorithm: Apriori_Cube_Improved  Input: N-Dimension Data Cube ( d1  d n | count), Minimum  support: minsup    Output: The set of frequent predicate set L  Process:  (1) Initialization: L:= ?; L1 =?; k=1; j=0;  (2) Every dimension members of dimension d i (i=1n) make  up the candidate 1-predicate set and we use symbol C1 to  represent the set of them;  (3) L1 =gen_frequent( C1 );  (4) Count the max_support and min_support of frequent  1-predicate set of every one dimension;  (5) For i=1 To n Do  1) If max_support < minsup Then roll-up the i dimension;  2) If min_support > minsup Then drill-down the i dimension;  3) j=j+1;  (6) If j > 0 Then  1) j=0;  2) repeat (2)(3)(4)(5)  // the new improvement of step 1.

(7) L=L? L1 ;  (8) While(k ??Q??DQG?? Lk ????'R  1) C k =gen_candidate_improved( Lk 1? );  //The new function will be expanded below.

2) Lk =gen_frequent( C k );  3) L=L? Lk ;  4) k=k+1;  (9) Output L  New Function gen_candidate_improved ( Lk 1? )  (1) Initialization: C k =?;  (2) For every 2 frequent predicate set I1 and I 2 from  Lk 1? Do  1) If (there exists k-2 items in the same between I1 and  I 2 ) Then C= I1 f I 2 ;  a) If C contains different levels from the same dimension  If there exits requirements of the users Then sup_count of C =  +   , go to b); Else delete C;  b) If all the subsets of c belong to Lk 1? Then  C k = C k ?c; Else delete c  (3) Output Lk ?I  Function gen_frequent ( C k );  (1) Initialization: Lk  ??  (2) For every candidate k predicate set I=( I1  I k ) which  come from the set do  1) Supporting count sup_count=the count storage in the cell of    n-dimension data cube( I1  I k );  Support=sup_count/total_count;  2) If support ??PLQVXS?7KHQ? Lk = Lk ?I;  (3) Output Lk ;  4. Performance Analysis  Example 2 Lets talk about a practical problem just like the  status of sales. Assume that we will mine the association  rules involved 4 dimension attributes of sales, the  minsup=25%. First of all, using OLAP technology to build  a 4-D data cube and the 4 dimension attributes are: time,  location, item, and supplier. For location dimension which  contains area, country and so on, we choose province level.

We use brand level for item dimension, company level for  supplier dimension. Time dimension can be divided as Q1  (1-3), Q2 (4-6), Q3 (7-9), Q4(10-12),and dimension  location(P1,P2,L1,L2) such as china-Shandong, USA-New  York, item(B1,B2,B3,B4),supplier(C1,C2,C3). Then the  sales data cube can be generalized like this:  Graphic 2: The 4-Dimension Data Cube of Sales  The details of this sales data cube are in the table  follow: The amount of cells is 100.

Location Time Item Supplier Count  Cell-1 P1 Q1 B1 C2 5  Cell-2 P1 Q3 B1 C1 2  Cell-99 L1 Q3 B1 C1 3  Cell-100 L2 Q4 B1 C3 11  Table 2: The details data table of sales data cube  We use original Apriori_Cube Algorithm to find  frequent predicate set with minsup_num= 25%*100=25.

According the data table we calculate that sup_count of  every member of dimension L is (P1:8, P2:5, L1:1, L2:24),  and also T, I, S. So there is the process:  Graphic 3: The processes of old algorithm  As we know, through comparing with the minsup_num,  dimension location has no one frequent 1-predicate set, so  that there have no frequent 4-predicate set in the output by  the original algorithm. But users are interested in the  Candidate 1-Predicate set  L T I S  P1  P2  L1    L2  Q1  Q2  Q3  Q4  B1  B2  B3  B4  C1  C2  C3  Frequent  1-Predicate set  T I S  Q1  Q3  B2  B3  C1  C2  I1 I 2  C1              C2                 C3      5 1     8 11  Q 1  Q 2  Q 3  Q 4  P 1  P 2  L 1  L 2  Candidate  2-Predicate set:  {Q1,B2},{Q1,B3}  {Q1,C1},{Q1,C2}  {Q3,B2},{Q3,B3}    {Q3,C1},{Q3,C2}  Candidate  3-Predicate  set:  {Q1,B2,B3}{Q  1,Q3,B2},{Q3,  C1,B2}  Frequent  2-Predicate  set:  {Q1,B2}{Q1,B  3},{Q3,B2},{  Q3,C1}  Output: 1L U 2L U 3L  Frequent 3-Predicate set:  {Q1,B2,C1},{Q1,B3,B2}  {Q1,B2,B3}  ???  (6) Use one predicate set contains no more than one different  level from the same dimension and SP to pruning Candidate 3  Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special  requirement on location B by users;  (7) Use the SP rule to keep {Q1,B2,B3,L} in this process by users;  (1) Calculate min_support and  max_support of every dimension;  (1)  frequent 4-predicate set which contains location dimension.

And the candidate 3-predicate set {Q1,Q3,B2} is useless,  because when we mine multi-dimension association rules,  we should follow that one predicate set contains no more  than one different level from the same dimension even the  sup_count >minsup_num when there are no requirements  from users. But this time, users require that different  members from dimension item can exist at the same time.

So we redo this mining process by the improved Algorithm:  Graphic 4: The processes of improved algorithm  At last, we got the frequent 4-predicate set which the  users are interested in, and then can format the rules.

5. Conclusion  In this paper, we have studied issues and methods on  efficient mining of multi-dimension association rules based  on data cube. With the development of technology, the size    of database has become much huge than ever before  unimaginable. So the multi-dimension model of database  based on data cube has become useful and popular,  relatively how to mine useful multi-dimension association  rules based on this model has been important too. Among  the algorithms for the problems we choose the most classify  one to improve, to make it more efficient and useful.

An efficient algorithm, Apriori_Cube_Improved, has  been developed which explores the multi-dimension  association rules. First, we use max_support and  min_support of every frequent 1-predicate set to check the  levels of dimensions, and then adjust the data cube by  roll-up and drill-down operation immediately. This step is  embed in the process of mining association rules, so it  makes the rules much more useful and flexible; second,  applying one predicate set contains no more than one  different level from the same dimension and SP pruning  can help reduce the amount of predicate set, especially with  the great growing of the number. So the speed of algorithm  improved is faster than ever.

At last, there are also many other interesting issues and  flaws of the algorithm which call for further study,  including efficient mining multi-dimension association  rules of complex measures and so on. Finally, I should  thank all the people who have give me help for this paper.

Reference  [1] Jiawei Han, Micheline Kamber. Data Mining Concepts and  Techniques. China Machine Press, 2007.

[2] Agrawal R, Imielinski T and Swami A. Mining association rules  between sets of items in large database. Proc. of the ACM  SIGMOD Conf., Washington DC, 1993.

[3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional  Association Rule Mining Method Based on Data Cube.

Computer Engineering, China,2003.

[4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research  Multi-dimensional Association Rule Ming Based on Apriori.

Science Technology and Engineering, China, 2009.

[5] R. Agrawal and R. Srikant. Fast Algorithms for Mining  Association Rules. Proc. of the 20th Int.Conf. on VLDB,  Santiago, Chile, 1994  [6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang.

Mining Multi-Dimension Constrained Gradients in Data    Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy,  2001.

[7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE  Transactions on Knowledge and Data Engineering, 2000.

Candidate  1-Predicate set  L T I S  P1  P2  L1  L2  Q1  Q2  Q3  Q4  B1  B2  B3  B4  C1  C2  C3  Candidate  1-Predicate set  L T I S  P  L  Q1  Q2  Q3  Q4  B1  B2  B3  B4  C1  C2  C3  Frequent 1-Predicate  L T I S  P  L    Q1  Q3  B2  B3  C1  C2  Frequent 4-Predicate set:  {Q3,C1,B2,P},{Q1,B2,B3,  Output:  L1 U L2 U L3 U L4  The Dimension need  to be adjust  Dimension: Location;  Operation: Roll-up to  increase the level of  this dimension;  Result:  P(P1,P2),L(L1,L2)  (2) (3)  (2) Compare With The minsup_num=25;  (3) The Candidate 1-Predicate Set of the data cube adjusted;  The Features Of Every  1-Predicate Set  Location: min_support=13;  max_support=25  Time: min_support=5;  max_support=30;  Item: min_support=10;  max_support=25;  Supplier: min_support=11;  max_support=27;  (4)  (5)  (4) Calculate min_support and  max_support of every dimension;  (5) Compare With The  minsup_num=25, and no one has  to be adjusted;  Frequent 3-Predicate set:  {Q3,C1,B2},{Q1,B2,L},{  Q1,B3,L},{Q3,P,B2}  Candidate 4-Predicate set:    {Q3,C1,B2,P},{Q1,B2,B3,L}(7)  The Features of Every  1-Predicate Set  Location: min_support=1;  max_support=24  Time: min_support=5;  max_support=30;  Item: min_support=10;  max_support=25;  Supplier: min_support=11;  max_support=27;  Candidate 2-Predicate set:  {Q1,B2},{Q1,B3},{Q1,C1}  {Q1,C2},{Q3,B2},{Q3,B3}  {Q3,C1},{Q3,C2},{P,Q1}  {P,Q3},{P,B2},{P,B3},{P,C1}  {P,C2},{L,Q1},{L,Q3}  {L,B2},{L,B3},{L,C1},{L,C2}  Frequent 2-Predicate set:  {Q1,B2}{Q1,B3},{Q3,B2},{  Q3,C1},{P,Q3},{L,Q1}  (6)  Candidate 3 -Predicate set:  {Q3,C1,B2},{Q3,C1,P},{Q1,B  2,L},{Q1,B3,L},{Q3,P,B2},{Q  1,B2,B3}  ???  (6) Use one predicate set contains no more than one different  level from the same dimension and SP to pruning Candidate 3  Set, pruning {Q1,Q3,B2}, keep {Q1,B2,B3} due to the special  requirement on location B by users;  (7) Use the SP rule to keep {Q1,B2,B3,L} in this process by users;  (1) Calculate min_support and  max_support of every dimension;  (1)  frequent 4-predicate set which contains location dimension.

And the candidate 3-predicate set {Q1,Q3,B2} is useless,  because when we mine multi-dimension association rules,  we should follow that one predicate set contains no more  than one different level from the same dimension even the  sup_count >minsup_num when there are no requirements  from users. But this time, users require that different    members from dimension item can exist at the same time.

So we redo this mining process by the improved Algorithm:  Graphic 4: The processes of improved algorithm  At last, we got the frequent 4-predicate set which the  users are interested in, and then can format the rules.

5. Conclusion  In this paper, we have studied issues and methods on  efficient mining of multi-dimension association rules based  on data cube. With the development of technology, the size  of database has become much huge than ever before  unimaginable. So the multi-dimension model of database  based on data cube has become useful and popular,  relatively how to mine useful multi-dimension association  rules based on this model has been important too. Among  the algorithms for the problems we choose the most classify  one to improve, to make it more efficient and useful.

An efficient algorithm, Apriori_Cube_Improved, has  been developed which explores the multi-dimension  association rules. First, we use max_support and  min_support of every frequent 1-predicate set to check the  levels of dimensions, and then adjust the data cube by  roll-up and drill-down operation immediately. This step is  embed in the process of mining association rules, so it  makes the rules much more useful and flexible; second,  applying one predicate set contains no more than one  different level from the same dimension and SP pruning  can help reduce the amount of predicate set, especially with  the great growing of the number. So the speed of algorithm  improved is faster than ever.

At last, there are also many other interesting issues and  flaws of the algorithm which call for further study,  including efficient mining multi-dimension association  rules of complex measures and so on. Finally, I should  thank all the people who have give me help for this paper.

Reference  [1] Jiawei Han, Micheline Kamber. Data Mining Concepts and  Techniques. China Machine Press, 2007.

[2] Agrawal R, Imielinski T and Swami A. Mining association rules  between sets of items in large database. Proc. of the ACM  SIGMOD Conf., Washington DC, 1993.

[3] Gao Xuedong, Wang Wenxian and Wu Sen. Multidimensional  Association Rule Mining Method Based on Data Cube.

Computer Engineering, China,2003.

[4] Sheng Yingying, Yan Ren, Wang Jiamin and Li Jia. Research  Multi-dimensional Association Rule Ming Based on Apriori.

Science Technology and Engineering, China, 2009.

[5] R. Agrawal and R. Srikant. Fast Algorithms for Mining  Association Rules. Proc. of the 20th Int.Conf. on VLDB,  Santiago, Chile, 1994  [6] Guozhu Dong, Jiawei Han, Joyce Lam, Jian Pei and Ke Wang.

Mining Multi-Dimension Constrained Gradients in Data  Cubes. Proc. of the 27th Int.Conf. on VLDB, Roma, Italy,  2001.

[7] M. J. Zaki. Scalable Algorithms for Association Mining. IEEE  Transactions on Knowledge and Data Engineering, 2000.

