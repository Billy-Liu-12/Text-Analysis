Optimized Disjunctive Association Rules via Sampling

Abstract  The problem of finding optimized support association rules for a single numerical attribute, where the optimized region is a union of k disjoint intervals from the range of the attribute, is investigated. The first polynomial time algorithm for the problem of finding such a region maxi- mizing support and meeting a minimum cumulative con- fidence threshold is given. Because the algorithm is not practical, an ostensibly easier, more constrained version of the problem is considered. Experiments demonstrate that the best extant algorithm for the constrained version has significant performance degradation on both a syn- theticmodel of patterned data and on real world data sets.

Running the algorithm on a small random sample is pro- posed as a means of obtaining near optimal results with high probability. Theoretical bounds on sufficient sample size to achieve a given performance level are proved, and rapid convergence on synthetic and real-world data is val- idated experimentally.

1. Optimized Association Rules  The search for meaningful patterns in large datasets is one of the main foci of data mining research. A well- investigated type of pattern is the association rule (in- troduced in [1]) a rule of the form X1X2 . . . Xs ? C, meaning, in essence, that ?when X1, . . . , Xs all hold about a datum, then C tends to hold also?. Often, data is so-called ?market-basket? data, and Xi and C are  ? Some of the work was performed at the National Center for Su- percomputing Applications, supported in part by NSF grant ACI 96-19019.

? Supported in part by NSF grant IIS-9907483.

? Supported in part by NSF grant IIS-9907483.

boolean variables indicating the presence of some item in a customer?s order. Such a rule is useful when it has sufficient support and confidence. The support of a rule is the number or percentage of records for which the antecedent X1 . . . Xs holds. The confidence is a mea- sure of the implication?s validity ? the percentage of time the consequent C holds given that the antecdent holds. The literature is rich with work on how to effec- tively find such rules and variants (survey in [5]).

The idea of optimized association rules was intro- duced by Fukuda et al. [4]. Consider a single large rela- tion. The motivation is that in many settings, a user is interested in a specific attribute c of the data as a con- sequent in an association rule (e.g., c corresponds to ?good credit risk?), as well as a collection of an- tecedent attributes Xi whose values are likely to be predictive of c. The goal is to find one or more in- stantiations of the attributes Xi that will result in rules satisfying constraints on support and confi- dence. For example, in a credit-history database, X1 ? {single, married, divorced} might repre- sent marital status, X2 and X3 might be numeric attributes representing income and age, respec- tively, and c ? {0, 1} might represent creditwor- thiness based on past history or by expert judge- ment. A discovered rule in the existing database such as X1 = married?X2 > $50K ?X3 ? [30, 60] ? c = 1 might be useful in making decisions about new cases.

Let D be a large relation. For any S ? D, define ? The support of S, sup(S), is just |S|.

? The set S+ = {s ? S : s.c = 1}.

? The confidence of S, conf(S), is sup(S+)/sup(S).

? If S1, . . . , Sk are subsets of D, then the cumula-  tive support and cumulative confidence of the sets are, respectively, the support and confidence of the union ?ki=1Si.

Results vary based on the form of the set S. Fukuda et al [4] first defined the problem, and considered the case that S is specified by a single numeric attribute, and later generalized the problem to that of two nu- meric attributes [4, 3]. Efficient algorithms were given for maximizing the cumulative support given a min- imum confidence threshold, for the dual problem of maximizing the cumulative confidence given a mini- mum support threshold, and for maximizing the ?gain? of a rule. Rastogi & Shim generalized the problem to allow unions of categorical attributes [7] and of one or two quantitative attributes [6]. Zelenko gave a poly- nomial time algorithm for unions of categorical at- tributes [12]. Wijsen and Meersman offer an investiga- tion into the complexity of variants of the problem [10].

To better understand our results and their relation- ship to past work, we need just a couple more defini- tions. Let D be a data set of cardinality n, with each da- tum d containing a single real-valued attribute d.r and a boolean ?consequent? attribute d.c. Without loss of generality, assume that each value d.r ? {1, 2, . . . , n}, reflecting the possibility of at most n distinct values.

An interval I is just a pair (a, b) with a ? b, and rep- resents the set D(a, b) = {d ? D : a ? d.r ? b}.

Given a data set D as described above, a value minconf ? [0, 1], and a positive integer k, the max- support-min-cumulative-confidence problem is to find a collection I = I1, I2, . . . , Ik of k intervals with cumula- tive confidence conf(I) is at least minconf, and so that the cumulative support sup(I) is maximized. Alter- natively, the max-support-min-independent-confidence problem is to find a collection I of k intervals so that each interval of I has confidence at least minconf and so that the cumulative support sup(I) is maximized.

Our contributions are as follows: ? We provide the first polynomial time algorithm for  the max-support-min-cumulative-confidence prob- lem for a single numeric attribute.

? For the max-support-min-independent-confidence problem, we analyze the performance of an al- gorithm of Rastogi & Shim which was shown to scale well on random unpatterned data. We test the algorithm on several real-world data sets, and find that the algorithm does not scale as well, but rather behaves as it does on a proposed synthetic model of random patterned data.

? We propose sampling as a preprocessing phase for any algorithm addressing either the independent or the cumulative confidence problem, and derive theoretical bounds on the sample size sufficient to achieve near-optimal performance. Because the bounds are independent of the size of the origi- nal data set, dramatic speedups are possible.

? Experimental results demonstrate the utility of the sampling approach; a sample of size less than 500 was sufficient in all cases to obtain a solution within 5% of optimal on both real world and on synthetic data models.

2. Maximum support meeting mini- mum cumulative confidence  In [7] a reduction from the NP-hard (Weighted) Set Cover problem to the max-support-min-cumulative- confidence problem is given, showing that this latter problem is NP-hard. The reduction translates weights from the set cover problem directly into support and confidence values for the max-support problem, rather than creating a data set D that realizes these values.

Because the NP-hardness relies on very large values of support and confidence that cannot be realized by a polynomially-sized database, it does not neccessar- ily apply to the problem in which a database is given as part of the input. But it is exactly this latter prob- lem that is of interest to us; it is only natural to allow an algorithm for mining a database to at least scan the data, spending time polynomial in n = |D| and k. The NP-hardness result of [7] does not preclude the exis- tence of an algorithm that takes time poly(n, k). We give just such an algorithhm, admittedly impractical due to the degree of the polynomial.

Note that a trivial exhaustive algorithm solves the problem in time O(n2k): The support and confidence for the O(  ( n 2k  ) ) = O(n2k) k-tuples can be computed  and the optimal solution selected. However, we would like an algorithm that runs in time polynomial in k, not exponential. We describe a O(n5k) algorithm, leaving its improvement as an open challenge.

We use dynamic programming in a manner similar to that of [6], taking advantage of a bound on confi- dence c as was done in [12]. For every interval [i, j] on n data points, and every number of disjunctions l, the maximum support l-interval on [i, j] exceeding mini- mum confidence has confidence tally c, where c is in the range [1..n]. For every possible confidence level c and for every interval [i, j], and for every disjunct size l, store the largest support region exceeding minconf (?), together with its support. Call that value S([i, j], l, c).

To solve the problem, we want S([1, b], k, n?).

Consider S([i, j], 1, c). From Fukuda et al. [4], these n3 values can each be computed in O(n) time. For the general case, subsequent values of l can be computed in terms of previous values, for each interval and confi- dence level. Specifically, notice that every solution for l intervals on [i, j] of confidence c can be partitioned into a disjoint union of an optimal solution for 1 inter- val on [i, m] with confidence c?, and an optimal solu- tion for l?1 intervals on [m+1, j] with confidence c?c?, for some choice of m and c?. The task, then is to find the best choice of m, c? ? [1 . . . n]. We know of no other way to do this than searching through all their possi- ble values. Thus S([i,j],l,c) is the set of maximum sup- port among the n2 unions of the form  {S([i, m], 1, c?) ? S([m + 1, j], l ? 1, c ? c?)},  where 1 ? m, c? ? n.

The total running time of the algorithm is O(n5k), which corresponds to O(n2) update time for n3k tab- ulated values.

3. Maximum support meeting mini- mum independent confidence  3.1. The RS Algorithm  Rastogi and Shim [6] attack a more tractable ver- sion of the problem, in which a disjoint union is found consisting entirely of intervals each meeting the mini- mum confidence constraint. Because of the additional constraint, they are able to obtain a relatively fast al- gorithm (denoted ?RS?), whose performance will be of interest in our discussion of the efficacy of sampling.

The RS algorithm, like Fukuda et al?s [4, 3] assumes initially that the data has been pre-bucketed, so that the input consists of n buckets b1, . . . , bn, where each bucket corresponds to a particular discretized value of the quantitative attribute?s range. The problem is to find k intervals each of which contains some contigu- ous collection of buckets. Each bucket may be thought of as an indivisible unit, corresponding to a weighted point with weight equal to the support of the bucket, and with ?confidence? value c a real number in the in- terval [0, 1], as opposed to the set {0, 1}. They assume also that the buckets have been sorted in order of in- creasing value of the attribute. The RS algorithm has three phases  1. Preprocess the data by merging all adjacent buck- ets that have confidence at least minconf. There is never any reason to include one of these with- out its sufficiently high confidence neighbors - they will all be in the same interval.

2. Find in linear time, all ?partition points?. A par- tition point is one that is not contained in any in- terval of confidence at least minconf. Note that no interval in the solution to the problem can con- tain any of these partition points.

3. Solve the k-interval problem separately on the sub- problems between the partition points and merge the solutions to obtain a global choice of the k best intervals.

Suppose that the n original buckets are divided into m subgroups of n1, n2, . . . , nm buckets separated by m ? 1 or more partition points, with ?i ni = b.

Then if bmax = max{ni} is the number of buckets in the largest subproblem, algorithm RS takes time O(b2maxmk + mk  2). The first term is for solving the m subproblems, and the second term is the cost of com- bining the m local solutions into one global solution.

If there are many partition points, the data can splinter into a linear number of constant-sized sets. In this case, the run time will be dominated by the sec- ond term mk2, and will grow only linearly with the size of the data set, as shown in [6]. On the other hand, if  there are few partition points, the data will not lend it- self to the divide-and-conquer approach, since bmax will be large (O(n)). In this case the run time will be domi- nated by the first term, and will increase quadratically with the size of the data set.

The question is: ?Where between these two extremes will the algorithm?s performance fall in practice??  In order to empirically test the utility of the prepro- cessing and divide and conquer approach, in [6] the al- gorithm was run on synthetic data. Binned data was created as follows: For each bi, 1 ? i ? n, a support value si and confidence value ci were chosen to repre- sent si data points, ci ?si of which corresponded to data points x satisfying the consequent requirement x.c = 1.

The values si were chosen uniformly in [0, 2n ], and then normalized so that the sum of all si was 1. The confidence values ci were chosen uniformly in [0, 1]. As a consequence, there is no correlation between the con- fidence values in bucket bi and bi+1. The net result of using random data was that the partitioning algo- rithm performed very well, because, as noted by the au- thors, long stretches of high confidence were unlikely.

The algorithm was able to handle problems with up to 100,000 initial buckets in less than 15 seconds. Even with this much data, bmax was no more than 20. In other words, linear growth was observed.

It can be shown that if minconf > .5, with prob- ability approaching 1 exponentially quickly, the ran- dom data fragments into constant-sized chunks sepa- rated by partition points, explaining the dramatic im- provement offered by the partitioning algorithm. On the other hand, if minconf < .5 ? ?, then with prob- ability approaching 1 exponentially fast, the single in- terval encompassing all of the random data is an op- timal solution, and, a constant time algorithm suffices (choose the entire data set!). It is only when minconf is close to .5 (the mean confidence) that the algorithm?s performance degrades, as observed empirically in [6].

3.2. Our Data Models Described  While the RS algorithm was shown to provide dra- matic speedups, this was only for a data model corre- sponding to uniform random data. It is reasonable to assert that real world data sets of interest would be less random than the artificial data generated in [6]. It is precisely the randomness of their data model that cre- ates the fortuitous partitioning whose result is speedy run times.

The true utility of any method can only be demon- strated empirically by performance analysis on many different instances of real-world data. On the other hand, the availability of parameterized data models may be used to advantage to gain insight into how an algorithm?s performance depends on various data char- acteristics. Toward this end, to better understand how the RS algorithm performs, we develop a model of ran- dom patterned data, and carefully examine the role played by various parameters controlling the data dis-      0.2  0.4  0.6  0.8   1.2  0 100 200 300 400 500 600  domain labels  c o  n f id  e n  c e  Series1  Figure 1. Example of patterned data  tribution and the effect on the run time of the algo- rithm. We also consider the behavior of the algorithm on real world data selected from a census database (prediction of marital status from income), and from forestry data (prediction of ground cover type from el- evation). For the patterned synthetic data and for the real world data sets we find that the dramatic improve- ment observed on unpatterned random data is not typ- ical, and that the behavior of the RS algorithm on the real world data more closely matches our patterned data model, for which quadratic runtime dominates.

We conclude that such adverse data is not rare.1  Random patterned data  Our synthetic data was generated according to the following method. Each bucket received support as in [6]. We assumed, however, that confidence would be a function of the numeric attribute value (i.e., the bucket number), so that low confidence buckets would tend to cluster, as would high confidence buckets.

We generated a simple triangular wave-form with varying numbers of peaks and valleys. A typical peak had confidence values rising from .2 to .8, and then falling back to .2. Call this multipeak piecewise linear function function f . We randomly generated the con- fidence for bucket i to be a binomially distributed ra- tio with mean f(i), and N = 20 trials. Figure 1 shows a typical example data set generated in this manner.

We do not mean to suggest that this synthetic model is ?the right? one - indeed, we do not believe such a thing exists. The point is to consider how the algo- rithm performs when parameters such as the size and frequency of peaks change, empirically validating the  1 No special effort was made to find these data sets - they were selected due to their availability, their size, and the presence of a quantitative attribute with many possible values that was likely to be predictive of an associated categorical attribute.

0.2  0.4  0.6  0.8   1.2  2500 3000 3500 4000 4500 5000  domain labels - income  c o  n fi  d e n  c e  - s in  g le  census data  Figure 2. Census data  tradeoff suggested between the two terms in the com- plexity bounds for the divide and conquer algorithm.

We generated data with n = 2000, 4000, 8000, and 16000 buckets (distinct domain values for the quantita- tive attribute). This was done in two ways. For ?fixed peaks? data, the number of peaks for these datasets were, respectively, n/50, n/100, n/200, and n/400, thus keeping the number of peaks constant at 40, and hence the peak width increasing from 50 to 400. For ?fixed peak width? data, the number of peaks was set at n/200 for each data set, so that the peak width was fixed at 200, and the number of peaks varied from 10 to 80. Because each valley was likely to contain at least one partition point, and the neighborhood around each peak was unlikely to do so, this effectively allowed con- trol of the parameters bmax (here, the peak width) and m (here, the number of peaks) in the RS algorithm.

Real-world data  In addition to the patterned synthetic data, we ex- tracted a real world data set from the 1999 census data for the Los Angeles/Long Beach area. The total-family- income and marital-status attributes were projected from a database of 88443 households with positive in- come levels. The marital-status attribute was summa- rized by a boolean attribute whose values were 0 for a married head of household, and 1 otherwise. Finally, the data was bucketed according to the total-family- income, so that one record existed for every value in the income domain. A record consisted of an income level together with a tally of the number of households with that income, and a tally of the number of unmar- ried heads of household. The largest final data set con- sisted of 11990 records. This same procedure was fol- lowed on smaller census data sets to achieve smaller do- mains for our tests. Figure 2 shows a small portion of the census data set.

As a simple test point for scientific data in this con-      0.2  0.4  0.6  0.8   1.2  0 100000 200000 300000 400000 500000 600000  domain labels (n=1978 elevations)  c o  n fi  d e n  c e  - lo  d g  e p  o le  p in  e  forestry data  Figure 3. Forestry data  text, we tested the RS algorithm on the forestry data available from the UCI KDD archive at http://kdd.ics.uci.edu/. We used a similar procedure to that of the census data to extract the elevation and forest cover type for 581,012 soil samples. The eleva- tion and cover type attributes were projected from the set of observations. The cover type attribute was con- densed into a single boolean attribute whose value was 1 if the forest cover was ?lodgepole pine?, and 0 oth- erwise. Finally, the data was bucketed according to unique elevations. Each record in the final data set con- sisted of an elevation followed by a tally of the num- ber of observations that were taken at that elevation and another tally of the number of samples at that el- evation classified as cover type ?lodgepole pine?.

The final data set had only 1978 elements (eleva- tions). Figure 3 shows the entire forestry data set.

3.3. Results of RS algorithm on patterned and census data  We ran2 the RS algorithm on each patterned data set for k = 5 intervals. We fix the value of k, since we are most interested in characteristics of the data, and how those characteristics affect running time. For each setting of the parameters, each experiment was run 30 times, and the average run times were recorded. As ex- pected, because there were stretches of correlated con- fidence, within each peak there were sequences of buck-  2 The experiments presented in this paper were implemented in Perl and performed on an iBook laptop. The goal of our inves- tigation was not to determine the maximum problem sizes that can be handled by the algorithm based on the limits of current technology, but rather to see howwell the algorithm scaleswith various parameters. Our results are off by a fixed constant fac- tor when compared to implementations with faster hardware and software. (Our run times are consistently 50 times slower than that of the implementation used by Rastogi & Shim.)            0 4000 8000 12000 16000  domain size (n)  r u  n ti  m e  ( s )  fixed peak width (n/200)  fixed peaks (40)  uniform random data  census data  forestry data  Figure 4. Size vs. run time (minconf=0.65, k=5)  ets that neither merged together in the initial prepro- cessing phase, nor resulted in a partition point.

Figure 4 gives the running time for finding 5 inter- vals, as a function of the domain size. We expect the most salient parameter to be the peak width (bmax). As discussed earlier, this value is a small constant for the uniform random data, so run time scales linearly with a small slope as the amount of data increases. For the fixed peak-width data (n/200), we also have a linear in- crease in running time, but with a larger coefficient as bmax is likely to be near 200. However, when the num- ber of peaks is fixed, the number of buckets bmax within each peak grows with the amount of data, and the al- gorithm exhibits its characteristic quadratic increase in running time.

Also shown are the running times for the census datasets. Our admittedly subjective evaluation is that this data behaves more like fixed-peak data, hence exhibits quadratic running time. Also striking is the amount of time required for finding the best 5 inter- vals on the fewer than 2000 records of the forestry data set. Referring back to Figure 3 we see that the data has a single peak, and it appears that not much can be gained by partitioning the peak into five inter- vals so as to exclude some small amount of ?uncon- fident? data. We suspect that the algorithm spends a lot of time fighting the law of diminishing returns. This view is supported by our sampling results in the next section, where most of the gain in support can be ob- tained by looking at relatively few records.

Another view is given in Figure 5, where for a fixed data size, runtime is plotted against the number of peaks in the synthetic data set. (The census and uni- form random lines are plotted for comparison, and do not vary.) As the number of peaks increases for a fixed data size, the number of domain values within a peak decreases (bmax), and the running time decreases quadratically, consistent with the complexity bounds               0 50 100  number of peaks  r u  n ti  m e  ( s )  synthetic peaked data  uniform random data, unknown # of peaks  census data, unknown # of peaks  Figure 5. Shape vs. run time (n=4000, minconf=0.65, k=5)  0.00  100.00  200.00  300.00  400.00  500.00  600.00  700.00  0.5 0.6 0.7 0.8 0.9 1  minconf  r u n ti m e ( s ) census data  (n=3888)  uniform random data  peaked data, 20 peaks  Figure 6. Minconf vs. run time (n=4000, k=3)  of the RS algorithm.

Finally, Figure 6 shows the effects of varying the de- manded minimum confidence, while holding the other parameters fixed. Rastogi & Shim noted that as min- conf approached the mean confidence (.5) for their uni- form random data, the runtime increased dramatically.

This phenomenon holds as well for our synthetic pat- terned data, as well as for the census data (which had mean .63, explaining why the curve is shifted). It seems reasonable that for any data set there would be many possible feasible intervals with confidence close to that of the mean, so setting minconf near the mean is invit- ing an algorithm to consider perhaps far more possibil- ities than is practical, with perhaps only nominal gain in support.

4. Sampling  We consider sampling as an alternative means for efficiently handling data with large domains and for which the divide and conquer algorithm offers no sig- nificant speed up.

While a worst-case quadratic (or any polynomial) time algorithm is theoretically acceptable, in data min- ing applications it is often impractical to implement an algorithm which requires more than a fixed num- ber (one, say) of scans of the data. Absent a linear time exact algorithm, we turn to sampling to reduce the problem size, and, when we run the RS algorithm on a sample-driven coarsening of the data, thereby re- duce the computation time.

Our data summarization technique is similar to that employed by Fukuda, et al. [4]. Let D denote the origi- nal data, |D| = n, and let B denote a coarsening of D.

Create B as follows:  ? Select a random sample of buckets, S, from D ac- cording to support (|S| = s). This can be done in linear time using the reservoir sampling of [9].

? If sample is unsorted, sort according to the numer- ical attribute of interest in time O(s log s).

? Partition the original data into a new set of buck- ets B, whose bucket boundaries are those in S.

Thus, B consists of the buckets in S, together with a new bucket for every interval between sampled buckets, |B| ? 2s+1. This requires time O(n log s) if the data is unsorted, O(n) otherwise.

The burning question is, how many examples do we need to assure that the support of an optimal solution on our sample is likely to deviate only slightly from the support of an optimal solution on the whole data set?

We choose a dense enough sample so that, with high probability, any interval containing more than some small, user specified, amount of data is likely to be sam- pled, and thus, when the actual data is compiled into the new buckets, no interval?s support deviates from its original support by more than this small amount.

Then, this new set of bucketed data is used as input to algorithm RS. The choice of parameters gives the user a tradeoff between the error that she is willing to tol- erate, and the amount of time required by the RS al- gorithm on the sample.

Definition 1 An ?-significant interval, I, is an interval with sup(I) > ?.

We choose a sample of sufficient size from D to as- sure, with high probability, that a bucket is sampled for every ?-significant interval in D. In so doing, we as- sure (whp) that the buckets representing the accumu- lation of data between sampled buckets have weight no more than ?. Call the resulting coarsened data B an ?-coarsening of D.

Lemma 2 Given an independent uniform sample S of size s from a relation R containing n rows, if  s ? max(4 ?  log ? , ?  log ?  )  then Pr(? an ?-significant interval I with I ? S = ?) < ?.

Proof: Follows from the fact that the VC-dimension of the class of intervals is 2, and the sufficient sample size bounds given by Blumer et al. [2].

Consider any interval I on the original data D and notice that the support of I can be estimated on the bucketed data B by selecting the largest interval I ?, I ? ? I, so that the endpoints of interval I ? fall on bucket boundaries. Then, sup(I) ? sup(I ?) < 2?, since the ? error can occur at each of the 2 endpoints. Now, let I =  ? Ik be a k-interval, and let I ? =  ? I ?k, where  I ?k ? Ik, are the intervals of largest support whose end- points fall on sampled bucket boundaries, as in the sin- gle interval case. Then, because there are k intervals, sup(I) ? sup(I ?) < 2k?. Thus we have: Lemma 3 Given a data set D, and B, an ?2k - coarsen- ing of D, for any k-interval I on D, and I ? on B with I ? = arg maxi?I sup(i), sup(I) ? sup(I ?) < ?.

We have assured that, with probability more than (1? ?), the support of any k-interval can be computed on our coarsened data set with error no more than ?.

That is, for association rule F : A ? I ? C, sup(A ? I) can be approximated by some interval I ? on the set of buckets B so that sup(A ? I) ? sup(A ? I ?) < ?. No- tice that the argument holds for the condition A ? I?C as well. That is, there exists an interval I ? ? B such that sup(A ? I ? C) ? sup(A ? I ? ? C) < 2?. This ob- servation aids in analyzing the error in estimating con- fidence.

Finally, we are in a position to bound the error in- curred by running the RS algorithm on our coarsened data set B. The optimal solution to the problem is a measure of support. There are two types of error in support we can incur. First, we may overlook small in- tervals. We have bounded the magnitude of this type of error by ?. Second, algorithm RS searches among intervals exceeding the minimum confidence threshold for the optimal solution. Danger lies in the case we fail to consider some sufficiently confident interval be- cause we underestimate the confidence of that interval on the bucketed data. If an interval is actually confi- dent, and we cannot detect it, we are at risk of toss- ing out large chunks of support. Instead of quantifying this neglected support, we create a new, slightly lower confidence bound that these deficient intervals will al- most certainly meet, and so they will be considered by the algorithm for inclusion in the optimal solution to the problem. In the next lemma we show that any con- fident interval on the data set D can be approximated by an interval of slightly lower confidence on the coars- ened data set B. Hence, the theoretical results show  that for a suitably smaller confidence threshold (which necessarily depends on the optimal support), with high probability only the first kind of error (where small in- tervals are overlooked) can occur.

Lemma 4 For any k-interval I on D, let k-interval I ? ? I be the largest subinterval of I on B. Suppose the minimum confidence threshold is ?, and that sup(I) ? sup(I ?) < ?. Then, with probability greater than 1 ? ?,  conf(I) ? ? ? conf(I ?) ? ? ? ?(1 ? ?) sup(I ?)  .

The proof, which is largely algebraic, is available from the authors. Here?s an intuitive explanation: The fact that the original region meets minimum confidence is an indication that the average confidence over the re- gion is no less than the threshold. Since the sampled portion of the region is deficient, the unsampled region must have excess confidence. How deficient can the con- fidence of the sample be? At most, the unsampled re- gion has (1??)? excess, where the (1??) arises because we are measuring excess above ?. This excess, in terms of average confidence for the sample, must be normal- ized by the support of the sample. In effect, we are re- distributing the excess across a region of size sup(I ?).

The previous lemmas combine to give the following bounds demonstrating that small error (?) in total sup- port can be achieved on a sample if a suitably smaller value of confidence (depending on the support) is cho- sen. Our empirical results demonstrate, however, that this reduction in the minconf ? is not necessary in prac- tice.

Theorem 5 Let RS(D, ?) denote the maximum sup- port k-interval exceeding confidence ? on data set D. Let B = {b1, b2, . . . , bm} be an ?2k -coarsening of D. Then  sup(RS(D, ?)) ? sup(RS(B, ? ? ?(1 ? ?) sup(RS(D, ?))  )) < ?.

These analytic results imply a reasonable practical approach to sampling, with the only complication aris- ing from the fact that the error in confidence depends on the support of the interval of interest. The sampling bounds are independent of the size of the domain, and thus, tradeoffs are simply between sample size and ac- curacy. In the next section we demonstrate that con- vergence to optimal solutions occur surprisingly quickly across different data sets.

The bounds we obtained are significantly different than those given by Toivonen [8], and Zaki et al. [11], for example. In both of these papers, a goal is to es- timate the support of an itemset so as to determine with high probability whether or not it qualifies as ?fre- quent? by meeting a minimum support threshold. In this context, dependence on the error parameter ? is quadratic (that is, there is a factor of 1?2 ), whereas our bound improves this by relying only linearly on 1? . The     key difference is that we are using sampling not to ob- tain uniformly good estimates of the support of inter- vals, but rather to select admissible endpoints of in- tervals. The actual support of these intervals are com- puted exactly by a linear scan of the original data. So, the error induced by sampling is not from inaccurate estimation of support, but rather solely from the ne- cessity of representing an arbitrary interval using only ?admissible? intervals (with endpoints in the sample).

RS Algorithm run on sampled data  Our sampling experiments were conducted as fol- lows: for a data set D, and for sample size s and for 30 repetitions, a sample of s buckets was randomly selected from D and the optimized support set was found on the sample. The maximum support discov- ered among the 30 runs is compared to optimal.

We performed the following experiments using our synthetic and real world data models:  ? fixed peak width (n/200), varied data size n ? {2000, 4000, 8000, 16000};  ? fixed data size (n = 4000), varied number of peaks p ? {n/400, n/200, n/100, n/50};  ? fixed number of peaks (p = 40), varied data size n ? {2000, 4000, 8000, 16000}.

? Census: varied n ? {1836, 3888, 6661, 11990}; ? Forestry: fixed n = 1978.

All of our experiments demonstrate that conver- gence to the optimal support is quite rapid for all data sizes and independent of the variability in our synthetic data. The results are remarkably consis- tent, with more than 95% convergence occurring for all sample sizes larger than 500. The same re- sults are observed for the census data, as well as for the forest cover data. We show here a typ- ical convergence graph from our experiments.

Real Data  0.90  0.92  0.94  0.96  0.98  1.00  1.02  0 500 1000 1500 2000  sample size  fr a c ti  o n  o f  o p  ti m  a l  1836 records, census  3888 records, census  6661 records, census  11990 records, census  forestry data  These experimental results are consistent with the the- oretical results derived in the previous section. Indeed, the theory promises that convergence to optimal for a  particular level of minconf, ?, will occur, given a re- laxation in ?. Our experiments indicate that such a relaxation is unnecessary.

When viewed together, the theoretical and experi- mental results demonstrated here offer sampling as a reasonable approach to data reduction in the case of op- timized support association rule finding. The theoret- ical results suggest the appropriate tradeoffs between accuracy and sample size to be considered by the prac- titioner.

