An Algorithm for Reusable Uninteresting Rules in Association Rule Mining

Abstract  In this paper, we present a new framework for reusable association rule mining based on ?2 and odds ratio. We start at mining the association rules using standard Apri- ori algorithm. The strong rules are defined as association rules, while the weak rules will be evaluated by our pro- posed method. Firstly, the weak rules must be converted to 2 ? 2 contingency table. We then compute the relation- ship between variables using ?2 and odds ratio. If the weak rules are related to each other with positive or negative re- lationship, then the weak rules will also be determined as association rules. Our system is evaluated with experiments on the crime data.

1. Introduction  Association rule mining is a popular technique for dis- covering interesting relations between variables in large databases. Since its introduction [2] the association rule has received much research attention in KDD as a pattern discovery method. The association rule has been applied on a varie-ty of applications, e.g. market-basket analysis and crime pattern discovery. Association rules are commonly generated from frequent occurrences of itemsets by which they must be satisfied a user-defined minimum support and confidence at the same time. Chen et. al. [4] give a compre- hensive overview on data mining with database perspective.

As described in [4], the support supp(X) of an itemsetX is defined as the proportion of transactions in the data set, the frequent occurrence of itemset X . The confidence of a rule is defined as conf(X ? Y ) = supp(X ? Y )/supp(X). In other words, the confidence of an association rule is a per- centage value that shows how frequently the rule occurs in transactions. The confidence value therefore indicates how reliable this rule is. The higher the value, the more often this set of items is associated together. Given a database D of a set of transactions where each transaction T ? D  is a set of items, the association rule can be expressed as X ? Y , where X and Y are set of items, called itemsets.

The relation of two variables X ? Y is implied that when- ever a transaction T contains X then T probably contains Y as well with X ? Y = ?. To achieve this, the interest- ing rules must be selected from all possible rules, which resulted in thousands or millions of itemsets. It is usu- ally used a constraint on various measures of significance and interestingness of rules. The most common constraints are minimum thresholds on support and confidence. Based on these constraints, the rule is considered as an interesting rule if it satisfies both of minimum support and confidence thresholds. The thresholds are then a significant parameter in association rule mining. When mining association rules there are two mainly problems to deal with: If we define too low threshold, redundant rules may be included in the asso- ciation rules, while a too high threshold made significant rules vanishing. More practically, the number of rules gen- erated from all possible rules depends on the user-defined minimum threshold of support and confidence. The support and confidence thresholds control the number of association rules that are created. In general, the rules which are defined as an uninteresting rule will not be used in a system. How- ever, these rules may be useful for some criterion. There- fore, if the abandon rules can be verified that they must be reused again, we may get a relationship between variables more accurately.

In this paper, we propose a new framework for reusable abandon rules using chi-square test and odds ratio. We per- form the first step at finding interesting rules by using Apri- ori algorithm [8]. The uninteresting rules are then trans- formed to a 2 ? 2 contingency table. Next, The chi-square test is used to find the hypothesis. We then use the odds ra- tio to measure a probability that the event has occurred. The rule will be classified as an association rule if it satisfies the chi-square test and odds ratio criteria. We show our exper- iments on crime data that were collected from the southern of Thailand with 243 records.

The organization of this paper is as follows. Section 2     describes a basic principle of association rule mining tech- nique and methods to verify uninteresting rules, the chi- square test and odds ratio measurement. In section 3, we introduce a new framework for data mining that uses chi- square test and odds ratio by which a more accurate associ- ation rules can be achieved. Section 5 presents our experi- mental results. Finally, we conclude in section 6.

2. Basic Principles  In our framework, the database D consists of crime data, where each transaction contains 7 items in average out of a total set of 1,701 items. The association rule problem is to mine all rules existing in a database D with respect to minimal thresholds on certain quality measures, the sup- port and confidence. The relation between items can be discovered which indicates that the itemsets are related to each other. The mathematical model of association rule mining can be described as follows. Let us define by I = {x1, x2, . . . , xn} a set of n binary attributes, called items. Let D be a set of transactions, called the database.

Each transaction T ? D has a unique ID (Transaction ID- TID) and contains a subset of the items in I. A set X ? I with k = |X| is called a k-itemset (or an itemset). The as- sociation rule is expressed as X ? Y where X,Y ? I are itemsets and X ? Y = ? holds [2, 8]. A formal rule for supporting an itemset is that a transaction T ? D supports an itemset X ? I if X ? T also holds. The transaction T supports an itemset X with respect to database D such that supp(X) = |{T ? D|X ? T}|/|D|. Therefore, the rule X ? Y has support s if s% of transactions in D con- tain X ? Y . In addition, the support of a rule X ? Y is defined as supp(X ? Y ) = supp(X ? Y ). Another constraint on association rule is the confidence which is de- fined as conf(X ? Y ) = supp(X ? Y )/supp(X). The rule X ? Y holds with confidence c if c% of transactions in D that contain X also contain Y .

Given a database D, the main problem of mining asso- ciation rules is to generate the association rules in which the support and confidence must be greater than the user- defined minimum support (minsup) and minimum confi- dence (minconf), respectively. In practical application, the number of rules generated from the crime databaseD grows exponentially with |I|. The association rules can be gen- erated from the database D by using the following con- strains. If supp(X) ? minsup, the itemset X is frequent such that F = {X ? I | supp(X) ? minsup} is a set of all frequent itemsets. Having generated the set of fre- quent itemsets F , for every X ? F if the confidence of all rules X\Y ? Y, Y ? X and X,Y 6= ? then drop those rules that do not achieve minconf [9]. Such rules with high confidence and strong support are referred as strong rules.

Most of association rule mining techniques employ only the  strong rules and reject the weak rules (the dropped rules).

Therefore, we may lose some useful rules with weak sup- port and confidence but they may have some relationship with each other. In our proposed framework, the dropped rules will not be rejected immediately. Those dropped rules must be verified by chi-square test and odds ratio such at the rules can also be appended in association rules.

2.1. Contingency Table  In general, the contingency tables are a tool for recording and analyzing the relationship between two or more vari- ables, usually categorical variables [7]. A contingency table represents frequencies for particular combinations of values of two discrete random variables, where each cell in the ta- ble indicates a mutually exclusive combination of them. It can be used to present a joint distribution of two variables like the cross-tabulation in [7]. A contingency table can then be used to express the relationship between two vari- ables as shown in table 1.

Table 1. The contingency table [1] Y Total  X 0 1 0 Oij Oij Ri 1 Oij Oij Ri  Total Cj Cj n  Table 1 shows the simplest kind of contingency table, in which each variable has only two levels; this is called a 2?2 contingency table. The right-hand column and bottom row are called marginal totals and the bottom right-hand corner is the grand total. The value in each cell can be represented as Oij , where i = 1, 2, . . . , r and j = 1, 2, . . . , c with r and c are the number of row and column, respectively. In practical, any number of rows and columns may be defined.

The values in each row and column of the contingency table allow us to see the proportion of the two variables. If the proportions of individuals in the different columns varies between rows, and vice versa, we may assume that the table presents contingency between the two variables. Therefore, if there is no contingency, the two variables are independent.

The statistical significance of the difference between them can be evaluated with Pearson?s chi-square test.

2.2. Chi-Square Test  A chi-square test may be applied on a contingency table for testing a null hypothesis of independence of rows and columns. A chi-square test is a statistical hypothesis test in which the test statistic has a chi-square distribution is true.

In general, the Pearson?s chi-square (?2) test is one of the     most common used for chi-square tests whose results are evaluated by reference to the chi-square distribution. The hypothesis testing is as follows.

H0: This hypothesis indicates that there is no relationship between two variables or they are mutually independence.

H1: There is a statistical significance of relationship between two variables.

Given the observed values Oij , the chi-square value (?2) can be computed as  ?2 = r? i=1  c? j=1  (Oij ? Eij)2  Eij , (1)  where Oij is an observed frequency and Eij is an expected (theoretical) frequency, asserted by the null hypothesis. The ?2 depends on the degree of freedom (df) which is defined by  df = (r ? 1)(c? 1). (2)  An expected frequency (Eij) of Oij is  Eij = RiCj n  , (3)  where n is the grand total of the contingency table.

2.3. Odds Ratio  The odds of an event happening is the probability that the event will happen divided by the probability that event will not happen. It is defined by the ratio of the odds of an event occurring in one group to the odds of it occurring in another group [6]. In case of 2?2 contingency table, we can calculate a value that represents the direction of relationship between two variables. The contingency table composes of two level of variables, success and failure, where 1 is defined as ?success? and 0 as ?failure.? The odds of success event is then  Odds1 = O11 O01  , (4)  and odds of failure event is  Odds0 = O10 O00  . (5)  Hence, the odds ratio (?) can be easily computed as  ? = Odds1 Odds0  = O11O00 O10O01  . (6)  3. The Proposed Algorithm  In this section, we introduce a new framework for as- sociation rule mining technique in which the uninteresting rules can also be reused as association rules where their support and confidence do not satisfy the minsup and minconf . This is the main contribution of this paper.

Rules that satisfy both minsup and minconf are called strong rules, otherwise they are called weak rules. In our framework, the strong rules will be defined as the association rules, while the weak rules must be evaluated more extensively. The extended Apriori algorithm is shown in Algorithm 1.

Algorithm 1: The extended Apriori algorithm for reusable uninteresting rules  1. Computes the support for each itemset such that supp(X) = |{T ? D|X ? T}|/|D|, then perform the following test:  (a) If supp(X) ? minsup, the itemsetX is frequent such that F = {X ? I | supp(X) ? minsup} is a set of all frequent itemsets with which we know the support value of each frequent itemset  2. Perform the confidence for each rule where X ? F :  (a) Calculates the confidence value conf(X ? Y ) = supp(X ? Y )/supp(X)  (b) Verify the confidence value for every rule:  i. If conf(X ? Y ) ? minconf , then define those rules as association rules (strong rules)  ii. Otherwise determine those rules that do not achieve minconf as uninteresting rules (weak rules)  3. Transform the uninteresting rules to a 2 ? 2 contin- gency table  4. Computes the chi-square test using equation 1, then perform the follwing test:  (a) If ?2 > ?, then evaluates the odds ratio  i. Calculates the odds ratio using equation 6, and defines the rules as the association rules if ? 6= 1, otherwise abandon the rules  (b) Otherwise (?2 ? ?), drop the rules  The following subsections are the step to evaluate the unin- teresting rules.

3.1. Converting to contingency table format  In this process, the uninteresting rules must be trans- formed to the 2 ? 2 contingency table. We are particularly interested in the rules that represent the relationship of the two variables X ? Y where, for example, the first one is the period of time (X) and the second one is the severity (Y ). An example of the distribution of X and Y is shown in table 2.

Table 2. The example of distribution of crime in contingency table  Period of Time Total Severity Daytime Nighttime Normal 45 12 57  Hard 15 28 43 Total 60 40 100  The values in the contingency table will be used to calcu- lated the hypothesis testing and the direction of relationship using chi-square and odds ratio.

3.2. Testing for Independence  The independence of two variables can be measured us- ing chi-square test ?2 (equation 1) from which the joint dis- tribution in table 2 is calculated. Having calculated the ex- pected values using equation 3 and the ?2 using equation 1, we evaluate the ?2 value with a certain criterion ?. The ?2  is examined as follows.

1. If ?2 > ?, the null hypothesis is rejected and an al- ternate hypothesis is accepted. Therefore, the relation- ship of two variables is significance.

2. If ?2 ? ?, the null hypothesis is accepted which means that there is no relationship between two vari- ables or they are mutually independence.

If the null hypothesis is rejected, the uninteresting rule will be evaluated with the next step, otherwise abandon it.

3.3. Testing for the direction of relationship  In the final step, we use the odds ratio (?) to measure the direction of relationship between two variables, which in- dicates how much they are related significantly. The odds ratio is computed using equation 6, and it has three proper- ties which can be described as follows [5].

1. The value of odds ratio is non-negative number, that is the result of odds ratio is in the range [0,+?).

2. If X and Y are independent, then Odds1 = Odds0 and ? = 1. The relationship between two variables depends on the values of the odds ratio, possible values of which are ? < 1, ? = 1 and ? > 1.

? If ? > 1, there is a positive association between two variables. The odds of success in row 1 is greater than row 0.

? If 0 < ? < 1 , there is a negative association between two variables. The odds of success in row 0 is greater than row 1.

? If ? = 1, there is no relationship between two variables.

3. The value of odds ratio is not altered even when the contingency table is oriented that is the row and col- umn are inverted.

Therefore, if the odds ratio of an uninteresting rule is not equal to 1, then an uninteresting rule will be defined as as- sociation rules.

4. Experiments  To carry out the algorithm we proposed, we mined the crime data using WEGA software. The experiments were performed on the crime data set which was collected from disorder event in the southern of Thailand, since 2005.

There are 243 records and each record composes of seven attributes namely ?transporttype?, ?incidenttype?, ?place- type?, ?timezone?, ?weekday?, ?month? and ?lose?. We were particularly interested in the rules with a great rela- tionship between two variables.

4.1. The strong and weak rules  a. The experiments were conducted using WEKA software. Their properties were defined as ?Algorithm = Apriori?, ?LowerBoundMinSupport = 0.0?, ?Upper- BoundMinSupport=1.0?, ?MetricType=Confidence?, ?MinMetric=0.0? and ?NumRules=1000?. Some results on relationship between variables were shown in table 3.

The examples shown in table 3 were arranged in order of confidence values, from maximum to minimum. The rules were generated about 1,000 rules in which only 39 rules were related between two variables. Let us described by the examples, the rule number 30 was ?timezone=day 194 ? month=down 192 conf:(0.99)? which means that ?all inci- dents were occurred in the day time with 194 events but only 192 of which were occurred in the months between June and December.? This rule has confidence value = 0.99 and support value = 0.79. The rule number 902 has lowest confi- dence with small support value, ?transporttype=vehicle 107     Table 3. Examples of some relationship between rules Rule No. Relationship  29. incidenttype=general 198? month=down 196 conf:(0.99)  30. timezone=day 194? month=down 192 conf:(0.99)  274. month=down 239? timezone=day 192 conf:(0.8)  278. incidenttype=general weekday=norm month=down 155? timezone=day 124 conf:(0.8)  478. transporttype=nonvehi 136? placetype=nonroad 96 conf:(0.71)  479. transporttype=nonvehi 136? incidenttype=general 96 conf:(0.71)  892. transporttype=nonvehi 136? placetype=nonroad weekday=norm 70 conf:(0.51)  893. transporttype=nonvehi weekday=norm 103? incidenttype=general timezone=day month=down 53 conf:(0.51)  896. transporttype=nonvehi weekday=norm 103? incidenttype=general timezone=day 53 conf:(0.51)  900. transporttype=vehicle 107? incidenttype=general timezone=day lose=nond 55 conf:(0.51)  902. transporttype=vehicle 107? placetype=road 55 conf:(0.51)  907. incidenttype=general month=down lose=nond 111? transporttype=vehicle weekday=norm 57 conf:(0.51)  911. transporttype=nonvehi placetype=nonroad 96? weekday=norm month=down lose=nond 49 conf:(0.51)  912. transporttype=nonvehi placetype=nonroad 96? weekday=norm lose=nond 49 conf:(0.51)  916. incidenttype=general month=down 196? transporttype=vehicle 100 conf:(0.51)  921. incidenttype=general lose=nond 112? placetype=nonroad weekday=norm month=down 57 conf:(0.51)  989. transporttype=nonvehi weekday=norm 103? timezone=day lose=nond 50 conf:(0.49)  ? placetype=road 55 conf:(0.51).? b. There were many redundant rules in table 3 which  can be pruned. The number of useful rules that were gen- erated depends on a user-defined threshold. We then set the parameters for generating some useful rules as ?Lower- BoundMinSupport = 0.7? and ?MinMetric=0.8.? The re- sults were shown in table 4 which remained only 5 rules out of 1,000 rules. These 5 rules were satisfied the support and confidence thresholds and were determined as strong rules. Therefore the weak rules which were not satisfied the thresholds must be evaluated to either include or exclude from association rules.

Table 4. Some strong rules with ?Lower- BoundMinSupport = 0.7? and ?Min- Metric=0.8?  No. Relationship 1. incidenttype=general 198? month=down 196 conf:(0.99) 2. timezone=day 194? month=down 192 conf:(0.99) 3. weekday=norm 191? month=down 189 conf:(0.99) 4. month=down 239? incidenttype=general 196 conf:(0.82) 5. month=down 239? timezone=day 192 conf:(0.8)  4.2. Independence of two variables  In this section, the abandon (weak) rules from subsection 4.1 must be transformed to 2 ? 2 contingency table. Then, the ?2 was computed using equation 1. For example, the 2?  2 contingency table of rule number 902 was shown in table 5. The ?2 of table 5 was 12.16, its value was greater than the cutoff value 10.83, which indicates that the relationship between variables was significance. For cutoff value 10.83, 8 of 37 rules were chosen as candidate rules, which were shown in table 6. The candidate rules must have ?2 > 10.83 and must be evaluated with the next step. The other rules with ?2 ? 10.83 must be rejected.

Table 5. The contingency table for rule num- ber 902  PlaceType Total TransportType nonroad road  nonvehi 96 40 136 vehicle 52 55 107 Total 148 95 243  4.3. Direction of relationship  The 8 candidate rules from subsection 4.2 must be eval- uated by computing the odds ratio and compared with Lift or Interest [3]. From table 6, the rule number 37 has a sig- nificance relationship between variables, with odds ratio = 2.54. Therefore, the rule number 37 presents the positive relationship between ancestor and descendent. The results from table 6 can be described as follows.

a. The odds ratio of rule number 24 and 37 was the same, however, they were quite different in both confidence and     Table 6. The rule examples with some criteria Rule No. Ancestor Descendant Confidence Support ?2 Odds Ratio Lift Cosine  1 placetype=road 95 ? month=down 94 0.99 0.39 0.34  2 lose=die 90 ? incidenttype=general 86 0.96 0.35 18.76 7.87 1.17 0.64  3 placetype=road 95 ? incidenttype=general 88 0.93 0.36 12.85 4.34 1.14 0.64  4 transporttype=vehicle 107 ? timezone=day 92 0.86 0.38 4.49  5 placetype=road 95 ? weekday=norm 81 0.85 0.33 4.12  20 placetype=nonroad 148 ? incidenttype=general 110 0.74 0.45 12.85 0.23 0.91 0.64  21 lose=die 90 ? weekday=norm 66 0.73 0.27 2.36  22 lose=nond 153 ? incidenttype=general 112 0.73 0.46 18.76 0.13 0.9 0.64  23 placetype=nonroad 148 ? timezone=day 107 0.72 0.44 13.36 0.24 0.91 0.63  24 transporttype=nonvehi 136 ? placetype=nonroad 96 0.71 0.4 12.16 2.54 1.16 0.68  25 transporttype=nonvehi 136 ? incidenttype=general 96 0.71 0.4 24.29 0.12 0.87 0.59  35 placetype=road 95 ? lose=nond 50 0.53 0.21 7.14  36 timezone=day 194 ? transporttype=nonvehi 102 0.53 0.42 4.49  37 transporttype=vehicle 107 ? placetype=road 55 0.51 0.23 12.16 2.54 1.31 0.55  support values.

b. The odds ratio of 8 rules can be interpreted as signif-  icance as the Lift value. The rules number 2, 3, 24 and 37 present a positive relationship between ancestor and descen- dant, while the other rules indicate a negative relationship.

Those 8 rules will be appended to association rules.

We can conclude from these test results that the proposed algorithm can help us finding some useful rules from the un- interesting rules by which those rules can be reused again.

5. Conclusion  We have introduced a new framework for reusable asso- ciation rule mining using ?2 and odds ratio. Our primary contribution in this work was a method for evaluating unin- teresting rules in which those rules can be reused again. In our framework, the weak rules will not be rejected immedi- ately. Instead, those rules must be verified by ?2 and odds ratio. Our experimental results demonstrate that we can re- cover 8 useful rules out of 37 uninteresting rules and, there- fore, provide a new way for reusable uninteresting rules.

