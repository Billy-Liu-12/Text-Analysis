A New Associative Classifier for Text Categorization

Abstract  Text categorization has become one of the key techniques for handling and organizing text data. In practical text classification tasks, the ability to interpret the classification result is as important as the ability to classify exactly. Associative classifiers have many favorable characteristics such as rapid training, good classification accuracy, and excellent interpretation. In this paper, Closed-AC, which is a new associative classifier for text categorization, is proposed. Firstly, rough set is used to dimension reduction. Then, only generic rules composed of closed itemsets are used for classification. Experimental results show benefits of the proposed associative classifier.

1. Introduction  Text categorization refers to the task of automatically assigning documents into one or more predefined classes or categories [1]. In recent years, there has been an increasing number of statistical and machine learning techniques that automatically generate text categorization knowledge based on training examples. The goal of text categorization is to classify documents into a certain number of predefined categories.

Recently, a new classification technique, called associative classification, is proposed to combine the advantages of association rule mining and classification [2]. Given a training data set, the task of an associative classification algorithm is to discover the classification rules which satisfy the user specified constraints denoted respectively by minimum support and minimum confidence thresholds. The classifier is built by choosing a subset of the generated classification rules that could be of use to classify new instances. Many studies have shown that associative  classification often achieves better accuracy than do traditional classification techniques [2, 3].

To apply an associative classifier to the text classification problem in the real world, however, we need to remove several obstacles encountered during the training and testing phase. One of those is a high dimensional feature space. Dataset in the area of text classification, in many cases, has a very large number of features that are distinct lexical words. In associative classification, however, we consider all subsets of those words. Therefore, the effective number of features grows exponentially, and we cannot take into account all of them due to computational intractability. To overcome this problem we adopt the attribute reduction based on rough set [4].

Another obstacle in associative text classification is the large number of classification rules that are produced in the training phase. Since using all of them becomes both inefficient computationally and ineffective in classifying, we should select a part of those rules that have high quality. Liu et al. [2] proposed a pruning by database coverage, which is a kind of validation process using the training set for the purpose of choosing the best classification rules among others. Li et al. [5] refined the concept of the database coverage. In addition, they proposed two other pruning methods. One is to prune low-ranked rules in terms of the confidence and support of the rules. The other is to prune the rules in which the correlation between the pattern and the class variables is weak. In this paper, we adopted the problem of numerous redundant rules by using generic rules composed of closed itemsets [6] only, rather than the total set of strong association rules.

In Section 2 we introduce the general aspects of the associative classification. In Section 3 we explain the overall architecture of our text classification system using association rules and address the issues such as dimensionality reduction, and associative classifier based on generic rules composed of closed itemsets.

Experimental results and analyses of text classification ___________________________________      using a large dataset are presented in Section 4, and we conclude our works in Section 5.

2. Associative classification  2.1 Association rule mining  An association rule is a kind of co-occurrence information on items [7]. Let I={i1, i2, ?, iM}  be a finite set of items and D be a dataset containing N transactions, where each transaction D is a list of distinct items. A set X I is called an itemset. An itemset with k items is called a k-itemset. The support of an itemset X, denoted as sup(X), is defined as the number of transactions in which X occurs as a subset.

For a given D, let min_sup be the threshold minimum support value specified by user. If sup(X) min_sup, itemset X is called a frequent itemset. The set of all frequent k-itemsets is denoted by Fk.

An association rule is an implication of the form A B, where A I, B I and A B= . Besides the above-mentioned support, the rule A B has confidence, denoted by conf(A B), is the percentage of transactions in D containing A which also contain B.

Rules that satisfy both a minimum support threshold (min_sup) and a minimum confidence threshold (min_conf) are called strong. The problem of association rule mining is to discover all strong rules.

2.2 Associative classifier  Consider the association rule in the view of a classification rule. Let A={A1, . . .,An} be a set of attribute domains, and a data object obj=(a1, . . . ,an) be a sequence of attribute values, i.e. aj Aj, 1 j n. Given a pattern P=ai1 . . . aik where aij Aij for 1 j k and ij ij? for j j?, a data object obj is said to match pattern P if and only if, for 1 j k, obj has value aij in attribute Aij.

Definition 1 (Associative classifier). Let C={c1, . . . , cm} be a set of class labels. An associative classifier is the mapping R from the set of attribute values to a set of class labels  R: (A1, A2; . . . ; An) C                     (1) According to Eq. (1), given a test datum  obj=(a1, . . . ,an), the associative classifier returns a class label c C.

Let a pattern variable be P and a class variable c. If we rewrite the rule in the form of  R: P c and have a training set T={(Pi, ci)}, then the learning process is to induce the rule set R for which the element has the sup(P c) min_sup and conf(P c) min_conf. The procedure of associative classification rule mining is  not much different from that of general association rule mining. One difference is that, in associative classification rule mining for text categorization, the information of the distribution of word patterns matching each class is additionally maintained.

Now that we have a classification system, it requires a decision on which class to assign a new test document. First, we search for the rules in which the pattern matches the document. Next, from these rules, we perform a prediction based on some predefined decision criterion.

3. Associative classifier for text categorization  3.1 Overall architecture  The overall system architecture for associative classification is shown in Fig. 1. The left-hand side of the figure denotes the training process and the right- hand side the testing process.

Training  document  Preprocessing  Generic  classification  rules exaction  R1  ??  Rk  Decide  the class  Associative classifier  R1: p1 c1 ??  Rn: pn cn  Pattern  matching  New  document  Preprocessing  Fig. 1 Overall architecture of associative classier for text categorization  First, raw data for training is processed to fit to an appropriate form for training. This is called Pre- processing. The approach begins with a standard practice in information retrieval (IR) [8] to encode     documents with vectors, in which each component corresponds to a different word, and the value of the component reflects the frequency of word occurrence in the document.

From the perspective of association rule, a vector of document can be treated as a transaction, and each word in the vector can be viewed as an item. Thus, we can mine classification rules. Because the initial number of rules is very large, we only generic rules composed of closed itemsets. Finally, we construct a classification rule database with these selected rules.

When a new document comes in to be classified, we convert it into a pattern of words and search the database for matching rules. With the rules matched, we decide which class the test document is assigned to.

3.2 Dimensionality reduction by rough set  In practice, the resulting dimensionality of the space is often tremendously huge, since the number of dimensions is determined by the number of distinct indexed terms in the corpus. As a result, techniques for controlling the dimensionality of the vector space are required.

The theory of rough set [4] is a major mathematical tool for managing uncertain that arises from granularity in the domain of discourse, that is from the indiscernibility between objects in a set. The intention is to approximate a rough (imprecise) concept in the domain of discourse by a pair of exact concepts, called lower and upper approximation. These exact concepts are determined by an indiscernibility relation on the domain, which, in turn, may be induced by a given set of attributes ascribed to the objects of the domain. The lower approximation is the set of objects definitely belonging to the vague concept, whereas the upper approximation is the set of objects possibly belonging to the same. These approximations are used to define the notions of discernibility matrices, which play a fundamental role in the reduction of knowledge.

Attribute reduction techniques eliminate superfluous attributes and create a minimal sufficient subset of attributes for a decision table. Such minimal sufficient subset of attributes, called a reduct, is an essential part of the decision table which can discern all examples discernible by the original table and cannot be reduced any more. A subset B of a set of attributes C is a reduct of C with respect to D if and only if,  (1) POSB(D)= POSC(D), (2) POSB {a}(D) POSC(D), for a B.

Where is the positive region of D with  respect to B. A set C of condition attributes may  contain more than one reduct. The set of attributes common to all reducts of C is called the core of C. The core contains all indispensable attributes of a decision table and can be defined as,  ( )BPOS D  COREC(D)={c C|POSC {c}(D) POSC(D)}    (2) Our algorithm for computing a reduct is outlined as  follows.

Step 1. Compute COREC(D). For each condition  attribute in C, remove it from C and check whether it changes the positive region. Let COREC(D) be the set of all condition attributes whose removal does not change the positive region.

Step 2. Check whether COREC(D) is a reduct of the rule set. If yes, stop and COREC(D) is a reduct.

3.3 Extracting and storing generic rules  As the number of items grows linearly, the number of the antecedents in the left-hand side of association rule grows exponentially. Though we can reduce the size of the subset of patterns by the two parameters, min_sup and min_conf, the search often becomes computationally intractable when we use naive methods. To solve this problem, we use generic rules composed of closed itemsets [6] only, rather than the total set of strong association rules.

The concept of closed itemset is based on the two following functions, f and g:  f (X)={i I | t X, i t} g (Y)={t T | i Y, i t}  Function f associates with X the items common to all objects t X and g associates with Y the objects related to all items i Y.

Definition 2. An itemset X is said to be closed if and only if  c (X)=f (g(X))= f g (X)=X Definition 3. An itemset g I is a generator of a  closed itemset X iff c(g)=X and g? I with g? g such that c(g?)=X.

Definition 4 (Min-max association rules) [6]. Let AR be the set of association rules extracted. An association rule R: l1 l2 AR is a min-max association rule iff R?: l1? l2? AR with sup(R?)=sup(R), conf (R?)=conf(R), l1? l1 and l2  l2?.

It is proved in [6] that we can only mine min-max association rules, and all other rules can be deduced from these rules. Furthermore, the rule with the form R: g c, where g is a generator, c is a closed itemset, and g c= , is a min-max association rule.

To efficiently discover the min-max association rule, we propose the following method based on subsume index [9].

Definition 5. The subsume index of item i is subsume(i)={j I | j i g(i) g(j)}.

Furthermore, the following properties are used for pruning.

Theorem 1. Let X be an itemset, X subsume(X) is a closed itemset [9].

Theorem 2. Let G be the set of generators of dataset D. Then for all A G, every proper subset B of A is a generator, i.e. B G; and for all X G, every proper superset Y of X is not a generator, i.e. Y G [6].

Algorithm 1. Discover generic classification rules Input:   dataset D, min_sup, min_conf Output:  associative classifier C  1) Scan database D once. Delete infrequent items; 2) for each frequent item i do 3) C (i subsume(i)\i); 4) for (k=2; Fk 1 ; k++) do 5) Gk=Candidate-Gen(Fk-1); 6) for each gen Gk do 7) C (gen subsume(gen)\gen); 8)          back (gen, subsume(gen), k);  Procedure back (gen, subsume, level) 9) for (k=1; k<level; k++) do 10) for each g Gk and g  subsume(gen) do 11)          if g subsume(gen)\g pass min_sup and  min_conf then 12) C (g subsume(gen)\g);  3.4 Performing classification  After a set of rules is selected for classification, Closed-AC is ready to classify new objects. Some methods such as those described in [2, 5] are based on the support-confidence order to classify a new object.

However, the confidence measure selection could be misleading, since it may identify a rule A B as an interesting one even though, the occurrence of A does not imply the occurrence of B [10]. In fact, the confidence can be deceiving since it is only an estimate of the conditional probability of itemset B given an itemset A and does not measure the actual strength of the implication between A and B.

To avoid the lacuna of using only confidence metric, we adopt the conviction introduced in [11]. The parameter of conviction is defined as  Conv(A B)=P(A)P( B)/P(A, B)           (3) Unlike confidence, conviction factors in both P(A)  and P(B) and always has a value of 1 when the relevant items are completely unrelated. Generally, conviction is truly a measure of implication because it  is directional, it is maximal for perfect implications, and it properly takes into account both P(A) and P(B).

Given two rules R1 and R2, R1 is said to precede R2, denoted R1>R2 if the followed condition is fulfilled:  Conv(R1)>Conv(R2) or Conv(R1)=Conv(R2) and Conf(R1)>Conf(R2) or Conv(R1)=Conv(R2) and Conf(R1)=Conf(R2) and sup(R1)>sup(R2)  4. Experimental results  We compare the accuracy of the proposed Closed- AC with other two associative classifiers CBA [2] and CMAR [5] on Chinese biological abstracts database, which is created by Shanghai Information Center of Life Sciences, Chinese Academy of Sciences. The database contains documents on the biological research in China, including that on general biology, cytology, genetics, biophysics, molecular biology, etc. All abstract have been categorized by experts and researchers of relevant fields.

Fig.2 Comparison of accuracies on Chinese biological abstracts database  Comparing the results in Fig.2, we observe that Closed-AC can achieve a better accuracy than CBA and CMAR on most experiments with different number of the abstracts. Due to the usage of all the strong rules, the accuracy of CBA and CMAR heavily depends on the number of instances and decreases dramatically when the number of the abstracts increasing.

5. Conclusions  Associative classification is a new method in the area of document classification. The expression of the     classification rule is easy and human-readable. To solve the problems of existing associative classification for text categorization, Closed-AC is proposed in this paper. The advantages of Closed-AC are as follows. On the one hand, rough set is used to dimension reduction. On the other hand, only generic rules composed of closed itemsets are used for classification. Experimental results show the proposed associative classifier is effective.

Acknowledgements  This work is supported by Beijing Municipal Education Commission (KM200710009006), and by Key Youth Research Project of North China University of Technology.

