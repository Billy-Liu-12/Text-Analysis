A Fast Distributed Mining Algorithm for Associiation Rules

Abstract  Association rule mining is an important task of data mining. In practice, being often interested in a subset of association rules, users only want to get rules that contain a specific item. Integrating the item constraints into the mining process can acquire more efficient algorithms.

This paper addresses the problem of distributed mining association rules with item constraints which are formalized boolean expression, and presents a fast algorithm called DMCA. Principles and implementation of the algorithm are discussed. Experiments prove efficiency of the algorithm.

I Introduction  Association rule mining, one of the important tasks of data mining, has recently attracted a lot of attention in data mining research community [l-31. The scope of the research has also been extended to maintenance of discovered association rules, parallel or distributed mining of association rules, efficient mining of association rules with item constraints, etc.

Many large databases are distributed in nature, most of present algorithms are not applicable to them due to memory sizes and processing power. A distributed mining algorithm is a better solution for the situation.

The high flexibility, the scalability, the low cost performance ratio, and the easy connectivity of a distributed system make it an ideal platform for mining association rules. Moreover, users are often interested in a subset of association rules, only want to get rules that contain a specific item. Integrating the item constraints into the mining algorithm can acquire more efficient algorithms. Therefore the research on distributed mining association rules with item constraints is necessary.

Previous studies have led to many different algorithms for different purposes, such as sequential  Honglian Li Northern China University of Technology  Beijing, China, 100041  algorithms AIS [2], Apriori [3], DHP [4], item constraints association rules mining algorithm [5], and parallel algorithms CD, DD and PDM [6]. Distributed algorithms are not comimon, mainly FDM [7] developed by David et al. The distributed mining algorithm of association rule with item constraints has not been seen yet.

In the paper we propose an efficient distributed data mining algorithm DMCA (Distributed Mining Association Rules with Item Constraints). A number of techniques are used in the algorithm to improve its speed and memory usage, such as generating candidate sets only satisfy given boolean expression, pruning them earlier by making usle of relationships between the global frequent set and the local frequent set, saving local counts in hash table to reduce data set scans, reducing the communication cost to O(n) by using hashing to specifl polling sites, etc.

Experimental results show that the algorithm is efficient.

The remainder of the paper is organized as follows.

The tasks of mining association rules with item constraints in sequential, as well as in distributed environments are introduced in Section 2. In Section 3, techniques and some important results are discussed.

Algorithm DMCA is presented in Section 4.

Experimental results are reported in Section 5 .  Our conclusions and future plan are presented in Sections 6.

2 Problem Definitiam  The section presents fornial descriptions about the main tasks of mining association rules and distributed mining association rules with item constraints.

2.1 Sequential Mining .Association Rules  Let I={ i, , i, ,... i, } be a set of items, DB be a database 0-7803-6583-6/001$10.00 0 2000 IEEE 1900    of transactions, where each transaction T consists of a set of items such that T c  I. Given an itemset X c I, a transaction T contains X if and only if X c T. The number of items that a itemset contains is called it?s size. An itemset of size k is called a k-itemset. The items that a itemset contains form a new itemset, which is called sub-itemset of the original one. For an itemset X, its support, denoted by a(X), is the percentage of transactions in DB which contain X,  and its support count, denoted by X.sup, is the number of transactions in DB containing X. An itemset X is large (or more precisely, frequently occurring) if its support is no less than a minimum support threshold (minsup).

An association rule is an implication of the form X = Y ,  where X,Y cl, and X n  Y = @ . The association rule X 3 Y holds in DB with confidence c if the probability of a transaction in DB which contains X also contains Y is c. The association rule X =j Y has support s in DB if the probability of a transaction in DB contains both X and Y is s. The task of mining association rules is to find all the association rules whose support is no less than a minimum support threshold (minsup) and whose confidence is no less than a minimum confidence threshold (minconf), i.e.

(r t X U Y >  3minsup and  hninconf.

a(X U Y )  d X )  The problem of mining association rules can be reduced to two subproblems: finding all large itemsets for a given minimum support threshold; generating association rules from the large itemsets found. Our research mainly focuses on how to develop efficient methods to solve the first subproblem.

2.2 Distributed Mining Association Rules with Item Constraints  Let B be a boolean expression over I. We assume without loss of generality that B is in a disjunctive normal form (DNF). That is, B is of the form B, v B, v . . . v B, , where each disjunct B, is of the form a,, A ar2 A ... urn.  When there are no taxonomies present, each element a, is either I, or 4, ,I, E I .  When there are taxonomies present, a, is ancestor ( I, > , descendant ( I, 1 , +incestor ( Iq >or 7descendant( I, >, I,, E I .The paper mainly   discusses items without taxonomies.

can make B true.

Definition 2.1 An itemset satisfies B if its subsets  In order to apply B efficiently to the mining process, we introduce a guided set of items S.

Definition 2.2 For given B, a guided set S is such that any itemset that satisfies B will contain at least one item from S.

For example, let the set of items I= { 1, 2, 3, 4, 5}, B = ( 1  A 2 )  v 3. So the sets (1, 3}, (2, 3) and ( 1 ,  2, 3 ,4 , 5 }  are all guided sets. If B = (1 A 2) v 3, the set { 1,2, 4, 5 }  is guided itemset. Notice that an itemset contains an item from S must not satisfy B.

An itemset contains at least one item from S is called selected itemset.

Definition 2.3 For given B, a rule X=Y is efficient,  0 ?XuY satisfies B; 0 ?XuY? has support greater than or equal to the  minimum support; 0 ?X 3 Y ? has confidence greater than or equal  to the minimum support.

Given a boolean expression B, the problem of  mining association rules with item constraints is to discover all rules that satisfy B and have support and confidence greater than or equal to the user-specified minimum support and minimum confidence, respectively. A solution of the problem can be divided into two steps: find all large itemsets L? that satisfy the set S, then based on them search all large itemsets LE that satisfy B.

Now we examine mining association rules in a distributed environment. Let DB be a database with D transactions. There are n sites S,, S,, ..., S,, in a distributed system DS and database DB is partitioned over the n sites into DB,,DB2,. . .,DB,,, respectively.

Assume the size of partition OB, be D, (i=l, .  . .n), Xsup and X.sup, be the support counts of an itemset X in DB and DB, respectively. X.sup is called the global support count of itemset X,  and X.sup, is the local support count of X at site SI. The formal definition of globally large and locally large itemsets is as follows.

Definition 2.4 Given a boolean expression B and a minimum support threshold minsup, S is guided set associated B, a selected itemset X is globally selected  if     large in a distributed system DS if X.sup2minsupxD; correspondingly, X is locally large at site SI if X. sup,2m insupx D,.

Definition 2.5 In DS (S,, S2,. . .S,,), for given boolean expression B, a rule ? X  a Y ? is globally efficient if,  0 ?XuY? is globally selected large ; 0 ?XuY? satisfies B; 0 the confidence of ? X  a Y ? is no less than the  minimum confidence threshold at the current level.

In the following portion, LE are all globally large itemsets in DB and satisfy B, Lf are all globally large k-itemsets in L and satisfy B. The essential task of a distributed algorithm of mining association rule with  item constraints is to find all globally large itemsets  L~ at multiple sites.

S  a,  3 Techniques for Distributed Mining Association Rules with Item Constraints  guided set of items is either I, or - 1 1 ~  for some items I, E I  Several techniques used in our algorithm DMCA are introduced in this section, including guided items S generating, candidate itemsets generating and pruning, and support counts computing, etc. The main notations used in this section are listed in Table 1.

GL:,,  CG,:,  I B I D, v D, v ... v D, (m disjuncts) I  gl-selected large k-itemsets at Si  selected candidate sets generated from GL;,,  L;,, Li  selected large k-itemsets at Si  satify B and globally large k-itemsets  3.1 Generating guided set  For a given expression B, there may be many different sets S such that any itemset that satisfies B contains an item from S. We would like to choose a set of items for S such that the sum of the supports of items in S is minimized. Because the sum of the supports of items is correlated with the sum of the supports of the large itemsets that contain these items, which is correlated  with the execution time.

We now show that we can generate S by choosing  one element aq from each disjunct D; in B, and adding either 1, or all the elemenits in I-{ 1, } to S,. based on whether a, is 1, or -1, respectively.

? Theorem 1 Let S be a set of items such that VDi E B ~ u ,  E Di[(a, = I, A I, E S )  Then any (non-empty) itemset that satisfies B will  Proof. let X be an itemiset that satisfies B. Since X satisfies B, there exists soime Di E B that is true for X.

From the theorem?s statement, there exists some ail E D; such that either ay = 1, and lge S or a,=- $and ( I - {1,}) S. If the former, we are done: since D; is true for X, 1, EX. If it is the latter, X must contain some item in I - { l,} since X does not contain lo and Xis  not an empty set. Since ( I  - { 1,}) c S, X contains an item from S.

v (a,  = 4, A ( I  - {Z,} c S )  contain an item in S.

3.2 Generation of Candidate Sets  Given Li , the set of all selected large k-itemsets, the candidate generation procedure must return a superset of the set of all selected large (el)-itemsets. Recall that unlike in the Apriori algorithm, not all subsets of candidate in Ci+l will be: in LL . While all subsets of a large selected itemset are large, they may not be selected itemsets. Hence the join produce of the Apriori algorithm will not generate all the candidates.

Therefore, modifying the produce, we sort the itemsets in LL, that all items in :S precede all items not in S; lexicographic ordering is used when two items are both in S or both not in S.

Lemma 1 If the ordering of items in itemsets is such  that all items in S precede all items not in S, the join  procedure of the Apriori algorithm applied to L; will generate a superset of Li+, ,  This lemma infers that the first item of any large selected itemset is always a selected item. Hence for any (el)-candidate X,  there exist two large selected k- subsets ofXwith the same first (k-I) items.

In a distributed environment, pruning the candidate itemsets based on the relation between the locally large     itemsets and globally large itemsets can improve algorithm?s efficiency. The definition of gl-selected large itemsets is as follows.

Definition 3.1 At site SI, if itemset X is both locally selected large and globally selected large, then X is called gl-selected large at the site.

The gl-selected large itemsets at a site form a basis for generating the site?s candidate itemsets.

Lemma 2 If itemset X is globally selected large, then there exists a site SI (1 Si*) such that X and all its subsets are gl- selected large at the site.

Proof see also [7].

Theorem 2 For each k (k>l), the set of all selected  large k-itemsets LL is a subset of CG; =  U:=, CGr:, . where CG,:, = Apriori_gen( GLS,k-I ).

Proof. For each itemset XE Li , it follows from  Lemma 2 that there exists a site SI (1 i 9) such that  all the size-(k-1) subsets of X are gl-selected large at the site. Hence X E  CGl:, . So the following formula is valid: Li ECG,? = U:=, CGr:, =U:=, Apriori_gen( GL:,k-I ).

Theorem 2 shows that CG; can be taken as the set of candidate sets for size-k large itemsets. Moreover, GL;,,-, c LL-, , so the numbers of candidate itemsets will be less, improving algorithm efficiency.

3.3 Pruning Candidate Sets  1) Constraint Pruning  one subset not in Li-, , then delete it from CG,:, .

2) Locally pruning  From lemma 1, for any XE CG,:, at sites SI, if

X.sup,<minsupxD,, then discard it from L ; , k ,  where L:,, denotes local selected large itemsets in CG,:, .

3 )  Global Pruning  In fact, the local support counts from other sites can also be used for pruning. At the end of each iteration, all local and global support counts of candidate set X are available. These local support counts can be broadcast together with the global support counts after a candidate set is found to be globally large. Using this information, some global pruning can be performed on  For any candidate itemset X E  CG,:, , if it contains  the candidate sets at a subsequent iteration. We introduce the following theorem.

Theorem 3 At each SI, suppose Xi s  a size-k selected candidate itemset at the k-th iteration, X.sup is the global support count of X, use maxsup,(X) to denote the minimum value of the local support counts of all the size-(k-1) subsets of X, i.e., masup,(X)= min{ Y.sup,lYcXand Iy1=k-l}, maxsup(X) is the sum of the maxsup,(X) over all sites, then the following formula holds:  sup I maxsup(~)= c? maxsup,(X) . r=l Proof. Because maxsup,(X) is the minimum value of  the local support counts of all the size-(k-1) subsets of X, based on the relation of subsets, it is an upper bound of Xsup,, i.e., Xsup, l  maxsup,(X); maxsup(X) is the sum of maxsup,(X) at all sites, hence maxsup(X) is an upper bound of X.sup.

The upper bound can be used for pruning, i.e., if maxsup(X)<minsupxD, then X cannot be a candidate itemset. This technique is called global pruning. It can be combined with local pruning to form different pruning strategies and different algorithms. Global pruning is a useful technique for reducing the number of candidate sets. its efficiency depends on the distribution of the local support counts.

3.4 Count Polling  In algorithm CD, the local support count of every candidate itemset is broadcast from every site to every other site. Therefore, the number of messages required for count exchange for each candidate itemset is O(n2), where n is the number of sites. Applying our count polling method, only O(n) messages are needed to collect all the support counts for every candidate itemset.

In general, few candidate itemsets are locally large at all sites. For each candidate itemset X,  the technique uses an assignment function, which could be a hash function on X, to assign X a polling site (assuming the assignment function is known to every site). The polling site assigned to X is independent of the sites at which X is founded to be locally large. Therefore, even if X is found to be locally large at more than one sites, it will still be sent to the same polling site. For each     candidate itemset X, its polling site is responsible to find out whether X is globally large. To achieve that purpose, the polling site of X has to broadcast the polling request for X, collect the local support counts, and compute the global support count. Since there is only one polling site for each candidate itemset X, the number of messages required for count exchange for X is reduced to O(n).

4 Algorithm for Distributed Mining Association Rules with Item Constraints  In this section, we will discuss the algorithm DMCA, which using a number of techniques including multiple encoded transaction tables, candidate set reduction and local pruning, etc.

4.1 Algorithm Schema  1) Generate a set of guided set S such that any itemset that satisfies B will contain at least one selected item;  2) Find globally selected I-itemset Li : collect all local large 1-itemsets L,,l at each site, then generate large I-itemsets F by looping, finally compute Li = Fx S;  (3) then F = F u X (4) G = F x S ( 5 )  for(k=2; Li-, # P I :  k-t-t){ (6)  (7)  (8) (9) if X.sup, >= minsup xD, then  (10) j = polling-site(9, insert(X  sup,) into L?lk ;  (12) send L?Jk to site 4; (13) f o r j =  1, ..., n d o  { receive L?lk; (14) for AllXE L?lk do{ (15)  CG,? = U:=, CG% = U:=, Apriori_gen( GLf,k-I ).

~ [ k ]  = get_locul_count(l3B,, CG,S ,k) 1;  for All X E T[k]  (1 1 )  while(i=l; L;,k #0 and-i<=n;i++)  if X E LP,[k] then insert X into LP,[k]; update

X.lurge-sites;} }  ( 1  6 )  for All XeLP,[k] do send-computing-request(@;  (1 7) reply-computing-requestl:T,[k]);  (1 8)  (19)  for All X E  LP,[k] do {  receiveX.sup, from the sites 4, where 4~ Xlurge-sites;  (20) X.sup= c;=,x.sup, ;  (21) if X.sup1mimupxD then  (22) (23) then insert Xinto G,[k]; } (24) broadcast G,[k];  (25)  receive G,[k] from All other sites 4,(j # i); (26) Li =U;=, Li,k ; divide .ci into GL:,, , (i=1, ..., n);  if g-upper-bound(.;ll) 2 minsup x D  3) For each k >= 2, generate CG,:k+, = apriori_gen( GL?, ) at each site;  4) For each X in CG,sk+, , collect its supports to generate Ls,k+l after local pruning, then send Ls,k+l and their supports to looping sites;  5 )  Collect the supports of all candidate itemsets at each looping site, then prune them based on constraint expression to get ;  globally large itemsets LE.

(27) L: =comp( Li )}  (28)rehm LE= u k L f  ; I  5 Experimental results  To examine the performance: of the proposed algorithm, DMCA is implemented arid tested on a distributed system. Several pes, n,ming NT system and connected by LAN, are used1 to perform the experiment.

6 )  Integrate all large itemsets of each size into  4.2 Algorithm DMCA The databases in the expleriment are composed of synthetic data. We set I, the total number of items, to 1000; L, the number of potentially large itemsets, to  B contains 10 items. Experimental results are displayed Output: LE, all large itemsets that satisfy B; in Figure 1 .  The figure indicates performance ( 1 )  fS =find(@; Ti[ l]=get_focal_count(DB,, 0,l);  difference between algorithm DMCA and FDM.

(2) for All X E T,[ 11  if X.sup, >= minsup xD, DMCA outperformed FDM by 40-45%. The reduction  in the numbers of candidate sets and execution time in  Input: { D B I ( ~  = l , . * * , n ) }  (the database partition at sites 200; D, the total number OfIransactions, to 100000 and {SI} ) ,  B and minsup;     DMCA is much significant than that in FDM.

Therefore the algorithm is efficient.

6 Conclusions  In this paper, we propose a distributed algorithm DMCA for mining association rules with item constraints. Experimental results show that the algorithm is efficient. Extension of our algorithm DMCA to data mining tasks such as generalized association rules, quantitative association rules, classification rules, etc., is an interesting issue for the future research. This paper only discusses item constraints with boolean expression, future research is to explore mining association rules with constraints expressed by more complicated forms, such as predicate forms, etc.

