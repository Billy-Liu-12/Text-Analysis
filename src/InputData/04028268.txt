

Abstract: In this paper, an Apriori algorithm is presented for  mining frequent patterns based on inverted list. Compared with traditional Apriori algorithm and FP-growth algorithm, this algorithm has better efficiency and wider application range. Aimed at reducing the defect of traditional Apriori algorithm, this algorithm avoids lots of redundant operations with inverted list. This algorithm only needs scan data set twice and don?t need joining and pruning operations.

Frequent item set is saved in each transaction frequent set TF,and insert next frequent single item one by one,then generate new possible frequent item set. In this way, lots of redundant operations can be reduced. The performance study shows that it is more efficient in both dense datasets and sparse datasets.

Keywords: Data mining; frequent patterns; Apriori; inverted list  1.  Introduction  The problem of mining frequent patterns is present as follows: Let I={I1, I2,?, Im,} be a set of lots of items, and T={T1, T2,?, Tn,} be a set of all transaction in data library.

Suppose that Ti ? I, if the set of items A is the subset of I.

The support of A is the number of transaction in D and A.

When the support of A is bigger than the number defined by user, A is called frequent patterns.

There are two kinds of algorithms for mining frequent patterns: Apriori algorithm and FP-growth algorithm.

Apriori algorithm is efficient for mining short frequent patterns. But when the length of frequent patterns is big, the efficiency of Apriori algorithm becomes bad. FP-growth algorithm doesn?t have this defect. Even frequent patterns is long, FP-growth algorithm is efficient. But when mining frequent patterns for sparse data such as commercial sales data, which has large amount transactions and short frequent patterns, we have to build lots of sparse FP-tree which share few prefix, so the efficiency of mining frequent patterns on such FP tree is awful.

This paper provides a new Apriori algorithm based on inverted list, which we called InList. The efficiency is better  than traditional Apriori algorithm and FP-growth algorithm for dense data and spare data, for long frequent patterns and short frequent patterns.

The structure of this paper is as follows: second section present traditional Apriori algorithm and it?s defect and give a example for step of InList. Third section present InList algorithm. Fourth section is experiments result comparison of algorithms. Followed is conclusion for this paper.

2.  Apriori algorithem and it?s defect, an example for InList  2.1. Step and defect of Apriori algorithm  2.1.1. Step of Apriori algorithm  1: Join all frequent items we have to produce new frequent candidate items.

2: Pruning frequent candidate items.

3: Scanning data set to verify every candidate whether  it is frequent, if it is, output it.

4: Repeating the step 1 to 3 until produce no new  frequent item.

Step 3 cost most of time in this algorithm. But there  are lots redundant operations in step 3, which bring bad efficiency to Apriori algorithm.

2.1.2. The defect of Apriori algorithm  We can draw a conclusion about the defect of tradition as follow:  1:Scanning data set many times,it is equal to the maximal length of all frequent items. When the length is bigger than 10,the time cost for scanning and I/O is large.

2:There are a great lot of candidate items by every joint, but only portion of them is frequent.

3:There are many redundant operation in every verification.

Maybe there are few frequent items in every transaction, but there are a great lot of candidate items. We      have to verify whether every candidate item is in every transaction. If it is in one transaction, add 1 to the amount of the candidate item. But most of verification is redundant.

For example, we have an frequent candidate item set as {I1I2,I1I3,??,I1In,I2I3,??,I2In,??IN-1In},we have to verify every candidate item in every transaction. In fact, if a transaction has no I1 and I2, we have no need to verify all candidate items which including I1 and I2.Some improved Apriori algorithm organize all candidate items as tree structure. For example, they organize {I1I2,I1I3,??,I1In} to a tree with I1 as root node and I2,I3,??,In as leaf node.

When they verified that a transaction has no a node, they will not verify it?s child node. The improved algorithm reduce some redundant operations, but at the same time, it bring the cost to build candidate item into tree structure.

When a transaction include I1I2I3I4I5, but don?t include I1I2I3I4I5I6 ,so the transaction don?t include any frequent item which have I1I2I3I4I5I6 as subset. So we have no need to verify the candidate item such as I1I2I3I4I5I6?In in this transaction. But there is still a  verification in every transaction scanning in Apriori algorithm. Even with tree structure, it has to verify six tree nodes (I1I2I3I4I5I6) and then find that this transaction don?t include I1I2I3I4I5I6. And in next verification, it will repeat this operation to the transaction again. When the depth and branch of tree is big, the time to build tree become bigger and the redundant operations reduced become fewer.

The idea of our InList algorithm is to assign a null Transaction Frequent Items Set TFi to every transaction, then insert every frequent single item into these TFi according to inverted list, produce new frequent items with frequent item saved in TFi .Then saved the new frequent items into TFi .When all frequent single items are inserted into TFi ,the algorithm is finished.

2.2.  Example and advantage of InList  2.2.1? Example of InLIST  example 1: Let a data set as table 1 and minimal support is 2.

Table 1:Data set Transaction Item set  T1 I1,I2,I3,I5 T2 I1,I2?I4 T3 I3,I5 T4 I1,I3,I4 T5 I2,I4 T6 I2,I5  At first, we gain invert list of items as table 2:   Table 2. Invert list of items item Transaction  NO I1 1?2?4 I2 1?2?5?6 I3 1?3?4 I4 2?4?5 I5 1?3?6  1. Read I1?insert I1 into  TF1?TF2?TF4. Because TF1? TF2?TF4 have no frequent item,after insertion they all include frequent item  I1.Now  TF1 is?I1??TF2  is?I1?? TF4 is?I1?.

2. Insert I2 into TF1?TF2?TF5?TF6.Now TF1 has frequent item I1,it make a new possible frequent item I1I2 .

The count of I1I2 is 1. TF2 also has frequent item I1,it add 1 to the count of I1I2. Now the count of I1I2 is 2. The count is equal to minimal support, so I1I2 is frequent item. There is no new possible frequent item after inset I2 into TF5,TF6.Now TF1 is(I1,I1I2,I2),TF2 is(I1,I1I2,I2),TF4 is(I1),TF5 is(I2),TF6 is(I2).

3. Insert I3 into TF1,TF3,TF4. TF1 make new possible frequent items I1I3,I1I2I3,I2I3,all count of them is 1. TF3 make no new possible frequent item. TF4 also has I1I3,so add 1 to the count of I1I3.Now the count of I1I3 is 2,so it is frequent item. After insertinon, the count of I1I2I3,I2I3 is 1,so they are not frequent item. Now,we can know that TF1 is(I1,I1I2,I2,I3,I1I3),TF2 is(I1,I1I2,I2),TF3 is(I3),TF4 is(I1,I3,I1I3),TF5 is(I2),TF6 is(I2).

We can insert I4,I5 to make new frequent items in same way.

2.2.2.  The advantage of InList algorithm  1:Only scan data set twice: scan data set for making inverted list one time and scan inverted list for make new frequent items one time.

2:Only add one frequent item one time,so we don?t need join operations. and all possible frequent item have no subset which is not frequent, so we don?t need pruning operations.

3:Save all frequent items into TFi, which can be used to make and verify new frequent items. Though this way, we avoid redundant operations in verification.

3.  The algorithm of mining frequent patterns based on inverted list      3.1.  The organization of frequent items  We organize all frequent items into tree structure as figure 1. Every node represents a frequent item with it?s father node and all it?s grandfather node (if it has). Node o in figure 1-a represents I1I2I3I4I5.

Figure 1-a    ??.

Figure 1-b          Figure 1-c   When we insert a new item such as I6, we save the count of new possible frequent item at acc, which is a member field of node at figure 1. For example, acc of node b in figure 1-a saves the count of I1I2I6. If the count is no less than minimal support, add a new ? node as a child of node b. Then we can output the frequent item I1I2I6. At first we output node ? ,which represents I6, then we output the parent node of ? node, which is b node and represents I2,then we output the parent node of b node, which is node a and represents I1. Because node a is the root node of the tree in figure 1-a, so we finish the process and get the frequent item I1I2I6.

3.2.  Choosing and processing of items to save  In the example of section 2, every TF save all frequent items. After step 3 in the example, TF1 is (I1,I1I2,I2,I3,I1I3),it includes all frequent items appeared in T1. A frequent item, which length is n, include 2n-1 frequent sub-item. When n is bigger than 20, we have to save startling large amount frequent items in every TF.

Definition 1: If item set Y is a sub-set of item set X, we call Y is closed item set of X when they satisfy the two conditions followed: 1.we can gain X though permute all items in Y,2.Y is the minimal item set which satisfy  condition 1.

Now we only save closed item set of all frequent items  in every TF. We can change TF1 into (I1I2,I1I3). I1 and I2 is sub-item of I1I2 and can gained by permute I1I2. I3 is sub-item of I1I3 and can gained by permute I1I3. In fact we save the pointer point to b and c.

When we insert a new frequent item, we can know the transaction NO where the item appears though the inverted list. Then we scan the TF of these transaction, and gain new possible frequent items by the frequent items saved in TF, and increase it?s count. After scanning of these TF, all possible frequent items transfer their count to their sub-item.

Definition 2 Showed as tree figure 1, the node added as the child of the node scanned by TF called scan-node.

There is an error in the above process. When we insert I4, TF1 make new possible frequent items I1I2I4 and I1I3I4, which all have the sub-item I1I4 and they all transfer their count to I1I4. In fact transaction 1 can only increase the count of I1I4 one time. So we gain a wrong count of I1I4.

Look back to the process of inserting I3, because there is no frequent item of I1I2I3 but frequent items of I1I2 and I1I3, which have same sub-item of I1 . One way to correct the error is to add a node of infrequent item of I1I2I3 ,which has sub-item of I1I2,I1I3,I2 and I3 . Now TF1 save the pointer of I1I2I3 and don?t save the pointers of I1I2 and I1I3. We will present the process of transferring count later.

We don?t add new node for all infrequent items to avoid making a huge tree. In fact, we add new node for infrequent items only when they satisfy the two conditions: 1.the count of the root node of the tree where the infrequent items is belonged is not less than minimal support. For example, after insert I3, the count of root node a is less than minimal support. So there is no frequent item of I1I3 and don?t add node for I1I2I3 and I1I3. TF includes the frequent items of I1I2 and I3. In fact we save the pointers of b and x.

In this way we omit the items that is globally frequent and locally infrequent in the tree. 2. The node is scan-node. The reason we add node of infrequent items is to save it to TF. If it is not scan-node, it will not be saved into TF. So we don?t add these unnecessary node.

3.3.  Finding sub-frequent-item  In order to transfer the count of one node to it?s sub-frequent-item, we need to find out it?s sub-frequent-item. It is obvious that the node which length is 1 has no sub-frequent-item, and node which length is 2 has two sub-frequent-items. We need to save pointers of those sub-frequent-items into the node in order to transfer count to them.

? a  ?b  ?d ?e  ?c  ?g  ?f  ?d  ?n  ?j ?k ?h  ?o ?l ?m  ?i  ?p  ?q ?r  ?s  ?w ?u ?v  ?t  ?x  ?y  ?a1 ?z      For a frequent item which length is n, the amount of sub-frequent-items is 2n-1. If n is large, the amount of sub-frequent-items is huge. It is not realizable to save all sub-frequent-items into the node, so we only save the sub-frequent-items which length is n-1. The sub-frequent-items which length is less than n-1 can be transferred layer by layer. For a frequent item which length is n, the amount of sub-frequent-items which length is n-1 is n. We don't need save the sub-frequent-items represented by parent node, which length is also n-1. So we only save n-1 sub-frequent-items. For example, we only save I1I2I4,I1I3I4 and I2I3I4 into the frequent item of I1I2I3I4, not include I1I2I3.

There are two cases for finding out sub-frequent-items.

1. the node is frequent item. For  example the node of o in figure 1-a, it save the count of I1I2I3I4I5I6 if we insert I6.Becaude the count is not less than minimal support and the node o had transferred it?s count to all other node in figure 1-a, the count of all other node in figure 1-a is not less than minimal support. It means that if we add a new ? node below o, we must add a new ? node below all other node. Now we will find a sub-frequent-items of ? below o, we only find all sub-frequent-items which length is 5. At first, the added node below node g, which is parent node of node o, is sub-frequent-item. It means that I1I2I3I4I6 is the sub-frequent-item of I1I2I3I4I5I6 . Then all added node below all sub-frequent-items of o is sub-frequent-items. The sub-frequent-items of o includes I1I2I3I5, I1I2I4I5, I1I3I4I5, I2I3I4I5. The added node below them are I1I2I3I5I6, I1I2I4I5I6, I1I3I4I5I6, I2I3I4I5I6. So the sub-frequent-items are I1I2I3I4I6, I1I2I3I5I6, I1I2I4I5I6, I1I3I4I5I6, I2I3I4I5I6.

2.when the item is infrequent item. For example the added ? node below o is infrequent item. Now it is uncertain that every node in figure 1-a is frequent item. We can?t simply regard all added node below the sub-frequent-items (l, m, n, w)of o as the sub-frequent-items of ? node, because the added node isn?t all frequent. If it is frequent, add it. Else consider the added node below sub-frequent-item of l, m, n, w node. In this way, continue until find sub-frequent-item.

3.4.  Transfer count  There are two cases for transferring. 1. The node is frequent node. For example, we transfer the count of o to it?s sub-frequent-items. There are only two kinds of nods in figure 1: ? node of or  parent node of ? node. So we only need to transfer to all ? node  and their parent node.

Firstly transfer the count to l, which is the first sub-frequent-item of o. secondly transfer the count to m(m transfer the count to i, it?s first sub-frequent-item),which is  the second sub-frequent-item. Then transfer the count to n(n transfer the count to j, which is the first sub-frequent-item of n, and transfer the count to k ,which is the second sub-frequent-item of n, and k also transfer the count to h which is the first sub-frequent-item of k),which is the third sub-frequent-item of o. In this way, o transfer the count to w.

Though w, the count is transferred to u,v,a1,t and z node. In the process, all ? node are visited. We add a step for all nodes to transfer the count to their parent node. We can realize it easily with recursive function.

2.If the node is not frequent. We need set a visited mark for all node had visited. When set the mark, we will not visited one node twice for same count. In this way, we resolve the error: the count will not transfer to I1I4 twice.

Definition 2: Not all sub-frequent-item of a infrequent item has same length. We call the path from infrequent item node to root node transfer path. It is obvious that it is unique for an infrequent item node. We call the amount of node from one sub-frequent-item node to transfer path (not include the node in transfer path) transfer length of the sub-frequent-item. When the sub-frequent-item  node is not in same tree with infrequent item node, the transfer length of the sub-frequent-item is the amount of node from the sub-frequent-item node to the root node of the tree it is in (not include root node), it is equal to the layer of the sub-frequent-item node.

We can get the transfer function as follow: Function: Transfer(pNode.t).

Step: 1?    PNode->parent->acc= PNode->parent->acc+t; 2?    If pNode is frequent 3?      Then call Transfer_Fren(pSubNode,t,i) for all  pSubNode of pNode //i is the position of pSubNode in //the sub-frequent-item of pNode.

4?    else 5?     Then call Transfer_In_Fren(pSubNode,t,i) for all  pSubNode of pNode //i is the transfer length of //pSubNode.

Function:Transfer_Fren(pSubNode,t,i) Step: 1.  pSubNode->acc= pSubNode->acc+t 2.  pSubNode->parent->acc= pSubNode->parent->acc+t 3. if (i= =1) 4?Return 5?else 6?{for?int m=1?m<i?m++? 7.   { 8.    call Transfer_Fren(pSubSubNode,t?m-1) for the m-th sub_frequent-item of pSubNode.

9. } 10. return      11. } Transfer_In_Fren(pSubNode,t,i)is alike to Transfer_Fren(pSubNode,t,i).  We just add judge  visited mark and set visited mark sentence.

3.5.  Other considerations  When we sort all single frequent items in increase order, we need process less node and branch in front insertion. In this way ,we reduce the time cost of algorithm.

So we sort all frequent items in increase order at first, then insert single frequent items one by one.

For example, there is a path of ????? . It includes 9 frequent items, that are ??????,??, ? ????. If we output frequent items after every insertion, we have to visit ? node 9 times and visit ? node 8 times, and so on. So we only output all frequent items after all insertion are finished. In this way, we only visited all node one time.

3.6.  InList--The Apriori algorithm of mining frequent patterns based on inverted list  We can gain the Apriori algorithm of mining frequent patterns based on inverted list as follow: Algorithm 1:InList-- the Apriori algorithm of mining frequent patterns based on inverted list.

Input : transaction set D?minimal support? Output:all frequent items Step: 1. Scan D one time to get inverted list and support  count of all single items .

2. Omit all single items which support is less than ? ?  and sort the left items in increase order.

3. Create a null transaction frequent items set (TF)for  all single frequent items.

4. Insert all single frequent items into TF according to  inverted list and make new possible frequent items, and increase it?s count.

5. Call Transfer(pNode,t) for all scan-node? 6. Judge the count of all node, if it is not less than?  or  it is scan-node?Add a new node and find out it?s sub-frequent-items.

7. Renew all pointer in TF.

8. Repeat all step from 4 to 7 until all single items are  inserted.

9. Output all frequent items.

4.  Experimental evaluation  4.1.  Environment and dataset characteristics  In this section, we will compare the efficiency of traditional Apriori, FP-growth and InList based on experiments. The programming language is C++ and STL, and the compile tools is Dev C++(4.0). All experiment are run on a PC with 500MHz Celeron CPU and 512M memory.

Our operation system is Windows 98.

The data set are publicly available from IBM Almaden (www.almaden.ibm.com). Mushroom is a dense data set, which can make frequent items which length is big.

Mushroom represents different kinds of mushroom and has 8124 transactions and 120 single items. T40I10D100K is an imitation of commercial sales data and is sparse data. It has 100,000 transactions and 999 single items.

4.2.  Experimental result  The experiment result on given data set are shown below. The ordinate of the figure is mining time with second as unit. And abscissa is support.

For dense data set, because the efficiency of Apriori algorithm is bad. So we don?t compare with it. We only provide the comparison with FP-growth. The experiment result is shown in figure 2-a.

Figure 2-a. MUSHROOM   Figure 2-b.T40I10D100K   From the result of figure 2-a, for dense data set ,the  efficiency of InList algorithm is better than FP-growth algorithm. And smaller the support is, more efficient the InList algorithm is. Because smaller the support is, more FP-tree and branch are. And there are huge amount recursive function call.

From the result of figure 2-b, for sparse data set, when the support is big, the performance of Apriori algorithm is good because the length of frequent items is short and the only need scan data set few times. When support become smaller, the length of frequent items became bigger, so the efficiency of Apriori algorithm decrease. The length of frequent items for sparse data set is commonly short and the degree of coupling of all items is very low. So when we have to build huge amount of FP-tree with lots of branch, which share few prefix. The efficiency of mining frequent   0.1   0.08  0.06   0.04 0.02  Time(s)  InList Apriori FP  0.1    0.08   0.06     0.04       Time(s)  InList  FP      items on this condition is awful. The efficiency is far worse than Apriori algorithm, and more worse than InList algorithm. When support is 0.04, it took more than 30 minutes to build FP-tree, and took more 3 hours in whole process while 230 seconds for Apriori algorithm and 67 seconds for InList algorithm.

5.  Conslusions  This paper provides an InList algorithm, that is a new Apriori algorithm for mining frequent patterns based on inverted list. It avoids lots redundant operations in traditional Apriori algorithm and increase it?s efficiency.

Compared with traditional Apriori algorithm and FP-growth algorithm, InList algorithm has better efficiency for dense dataset and sparse dataset.

