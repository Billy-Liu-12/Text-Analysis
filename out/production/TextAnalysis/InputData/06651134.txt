Energy Efficient Workflow Job Scheduling for Green Cloud

Abstract?The elastic resource provision, noninterfering re- source sharing and flexible customized configuration provided by the Cloud infrastructure has shed light on efficient execution of many scientific applications modeled as Directed Acyclic Graph (DAG) structured workflows. However, the energy cost on running the increasingly deployed Cloud data centers around the globe together with the amount of ??2 emissions have skyrocketed. In order to maintain sustainable Cloud computing facing with ever-increasing problem complexity and big data size, we propose an energy-efficient scientific workflow scheduling algorithm to minimize energy consumption and ??2 emission while satisfying certain Quality of Service (QoS).

Our multiple-step resource provision and allocation algorith- m applies Dynamic Voltage and Frequency Scaling (DVFS) technology to reduce energy consumption within acceptable performance bounds, and minimize the Virtual Machine (VM) overhead for further reduced energy consumption and higher resource utilization rate. The candidacy of multiple data cen- ters from the energy and performance efficiency perspectives is also evaluated. The simulation results show that our algorithm is able to achieve an average up to 30% of energy savings and increase the resource utilization rate for about 25% leading to higher profit and less ??2 emissions.

Keywords-Energy-efficient; Green Cloud Computing; Work- flow Scheduling; VM Allocation

I. INTRODUCTION  The flexible utility-oriented pay-as-you-go Cloud comput- ing model has demonstrated tremendous potential for both commercial and scientific users to access and deploy their applications anytime from anywhere at reasonable prices depending on their QoS specifications. The computing power of the Cloud environment is supplied by a collection of data centers that are typically installed with hundreds to thousands of servers which are built on virtualized compute and storage technologies. Meanwhile equally massive cool- ing systems are required to keep the servers within normal operating temperatures. However, these infrastructures con- sume tremendous amounts of energy. The cost of servers running and cooling systems has increased by 400% over the past decade, and such cost is expected to continue to rise [1]. Following the current usage and efficiency trends, energy consumption by data centers could nearly double in another five years to more than 100 billion kWh [2].

Besides the energy cost, considerable amount of ??2 emis-  sions produced by these data centers significantly contribute to the growing environmental issue of Global Warming.

Gartner estimated that the Information and Communication Technologies (ICT) industry generates about 2% of the total global ??2 emissions in 2007 [3]. Reducing energy consumption for modern data centers has been recognized as an ever increasingly important technique for operation cost, environment footprint and system reliability.

Therefore, we want to study the problem of supporting scientific applications modeled as DAG-structured work- flows over the Cloud. The dependency and parallelism embedded in a workflow requires that the tasks be dispatched to a group of distributed VMs in order to maximize the execution efficiency. Also the Cloud resource availability map is time-dependent due to on-demand and reservation instances. The startup, idle and shutdown time of VMs compose the overhead of resource utilization. The overhead could result in higher energy cost, and resource under- provisioning which will inevitably hurt the application per- formance [4]. Therefore, reducing the overhead to maximize resource utilization is always the Cloud provider?s desire.

Motivated by the above-mentioned challenges and promis- es, our paper proposes a resource provisioning and allocation scheme to strategically schedule the DAG-structured work- flow onto the underlying time-dependent Cloud infrastruc- ture. The key contributions of this paper are: (1) A realistic energy model based on various factors such as energy consumption, energy cost, and ??2 emission rate. (2) A novel DVFS based strategy to find the optimal frequency to run each task of a scientific workflow without affecting the performance. (3) VMs allocation strategy to achieve task consolidation and maximized resource utilization which eventually lead to enhanced system performance, reduced energy cost and ??2 emission.

The rest of the paper is organized as follows. Section II discusses the related works. Section III presents our green Cloud system architecture, and defines the energy model as well as the workflow scheduling model. Section IV presents the problem formulation and the algorithm design. Section V explains the experimental setup followed by the analysis of results. Conclusion and future work can be found in Section VI.

2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum  DOI 10.1109/IPDPSW.2013.19

II. RELATED WORKS  There are many research works addressing energy- efficient computation for either cluster servers or virtual- ized servers [5], [6], [7]. Technologies such as Dynamic Voltage and Frequency Scheduling (DVFS) and Dynamic Power Management (DPM) [8] were extensively studied and widely deployed for energy savings. Some limited research works [9], [10] studied on the energy-efficiency issue from an economic cost perspective. However, most of previous works focused on energy reduction within a single server instead of across multiple data centers whereas Cloud providers typically have multiple geographically located data centers. Furthermore, most of these previous works focused on reducing energy usage, not on the profit boost while reducing the carbon emissions. Recently, a few research work began to take the environment sustainability issue such as ??2 emission into consideration. Garg et al. [11] proposed energy-efficient scheduling policies on determining the mapping order of application/data center pairs along with DVFS strategy to either minimize the ??2 emission or maximize Cloud provide?s profit. However, their work did not consider complex workflow structure which were com- monly used by the scientific community, and virtualization mechanism was not considered.

There are several commonly used task scheduling policies in Cloud computing management systems such as Euca- lyptus [12], and Pegasus Workflow Manage System [13].

However, energy efficiency issue was not considered in these scheduling algorithms.

Our work differs from the aforementioned efforts in that we consider all of the following aspects: (i) scientific workflow applications as opposed to individual tasks are considered under a time-dependent Cloud environment; (ii) a bi-objective scheduling problem is formulated to meet both the QoS and energy reduction requirements. (iii) DVFS based technology is applied to each task in the workflow to lower the CPU frequency without affecting the execution deadline; (iv) multiple data centers are available for work- flow scheduling.



III. ANALYTICAL MODELS  A. Green Cloud System Architecture  There are basically five main entities involved in our system architecture:  1. Users: Submit their service requests (i.e., workflow ap- plications, QoS requirements, etc.) anytime from anywhere to the Cloud.

2. Green Service Selector: Acts as an interface between the users and the Cloud infrastructure to enable energy/??2- efficient Cloud services.

3. Local Scheduler: Performs the actual scheduling of workflows. Provides the Green Service Selector with Co- efficient Of Performance factor (COP) as the efficiency of  the cooling system [14], ??2 emission rate, CPU power- frequency relationship, and electricity cost.

4. Virtual Machines (VMs): Multiple independent VMs can be created and deployed on a single physical server.

5. Physical Machines (servers): Each data center consists of multiple physical machines with limited OS.

B. Green Cloud Energy Model  Table I PARAMETER OF A DATA CENTER ??  Parameters Definitions  [??,???, ??,???] CPU frequency range of CPU ? ?? Constant power consumption ?? Proportionality constant ???? Coefficient Of Performance factor ?? Electricity cost ($/kWh) ???2? ??2 emission rate (kg/kWh) ?? Executing price ($/hour)  We assume that a particular Cloud provider has ? data centers located at different places around the world. The related parameters of a data center ??, ? ? [1, ? ] are given in Tab. I. According to previous models [6], [11], executing a workflow ? at a data center ?? results the following:  (1) Energy consumption of CPU ? executing task ?? :  ???? = (?? + ??(? ? ? )  3)? ??? (??,???)? ??,?????? (1) where ? ?? is the frequency of CPU ? executing task ?? ,  and ??? (??,???) is the execution time of task ?? running on CPU ? at maximum frequency.

(2) Total Energy consumption  ?? = (1 +  ???? )  ??  ?=1  ???? (2)  (3) Energy cost ?? = ?? ? ?? (3)  (4) ??2 emission of data center ?? ??2?? = ?? ? ???2? (4)  C. Workflow Scheduling Model  We construct the workflow scheduling model in Fig. 1 to facilitate the scheduling problem. A workflow is constructed as a Directed Acyclic Graph (DAG) ?? = (??, ??) where ???? is the number of tasks and ???? is the number of de- pendency edges. We consider a general Cloud environment (i.e., a data center) ?? = (??, ??) where ???? is the number of servers and ???? is the number of network links.

The resource cost of a workflow includes the task execu- tion time, and overhead for VM startup, idle and shutdown time. The total resource cost for server ?? with allocated computing capacity ??,??,?? during time interval [??, ??] is calculated by Eq. 5 and the overhead cost is calculated by Eq. 6. Utilization Rate (UR) stated in Eq. 7 indicates the effective resource utilization of a workflow.

...

m3  m2  m1 ms  mt  ml  mn  ?  ?..?  ?  ?..?  Data Center Dependency?edge?  Layer?1 Layer?2 Layer?l-1 Layer?l  Schedule  Figure 1. Workflow Scheduling Model.

??? = ??,??,?? ? (?????? + ????? + ????? + ?????) (5) ????????? = ??,??,?? ? (?????? + ????? + ?????) (6)  ?? = 1?  ??  ?=1  ?????????  ??  ?=1  ???  (7)

IV. PROBLEM FORMULATION AND ALGORITHM DESIGN  A. Problem Formulation  The scheduling problem is defined as follows: Definition 1: Assume that a Cloud provider has ? data  centers at different geographical locations. Each data center has different electricity cost, ??2 emission rate and other energy related parameters. Cloud users submit jobs (i.e., a single task, pipeline, or DAG-structured workflow) to the Cloud with specified deadlines. For a particular workflow ? submitted by a user, our objective is to schedule the workflow to a chosen data center such that the energy cost and ??2 emission can be minimized for higher profit and less environmental footprint while still satisfying deadline requirement.

B. Algorithm Design  A five-step workflow scheduling algorithm, namely Energy-Aware Resource Efficient workflow Scheduling un- der Deadline constraint (EARES-D) is proposed. The com- plexity of this algorithm is ?(???? ? ????).

Step 1 Earliest Completion Time Estimation Phase: For all ? data centers, calculate the estimated earliest com- pletion time (???? ) for a workflow by assuming that all CPUs in each data center run at their maximum frequencies without considering actual VM allocation.

Step 2 DVFS Phase: Use DVFS technology to reduce energy consumption by scaling down the CPU frequency  under the deadline constraint and the optimal frequency for executing each task is also determined.

Step 3 Data Center Selection Phase: For each available data center that satisfies the deadline requirement, choose the best data center based on workflow completion time, energy cost and ??2 emission (???).

Step 4 Forward Workflow Scheduling Phase: A layer- based forward task scheduling is performed to the selected data center, and tasks are scheduled from the first layer to the last layer. Earliest completion time ??? is calculated. Each task is scheduled on a specific VM running at the desired frequency as calculated by Step 2. Go back to Step 3 to pick another data center if ??? exceeds the deadline constraint.

Step 3-4 is repeated until we find a data center that satisfies the deadline constraint or reject the user request.

Step 5 Backward Workflow Scheduling Phase: Based on the temporary mapping result from Step 4, the resource utilization rate (UR) is further improved by reusing VM and shrinking the idle time between tasks. In this step, a backward scheduling strategy is conducted to adjust the task mapping from the last layer to the first layer while the minimum ??? and deadline is still guaranteed.



V. EXPERIMENTAL RESULTS  A. Experimental Setup  We use open source Java-based CloudSim toolkit [15] to model our green Cloud infrastructure and evaluate our scheduling algorithm. Many Java classes have been adapted to accommodate our workflow application structure as well as the time-dependent Cloud resources.

1) Data Center Configuration: We model 8 different data centers with different configurations (with data center ID from 1 to 8). CPU power factors of different data centers are derived from Wang and Lu?s work [6], [11]. As current commercial CPUs only support discrete frequency levels, we consider discrete CPU frequencies in the range of [0.8, ??,???]. We assume that the COP value of each data center is 1.5. VM startup time is 100 seconds, and the shutdown time is 8 seconds based on [4].

2) Workflow Configuration: We experiment with 10 dif- ferent sizes of workflows (with workflow ID from 1 to 10) represented by a two-tuple (?, ????): (10, 21), (20, 35), (40, 88), (60, 119), (80, 158), (100, 212), (200, 398), (300, 601), (400, 784), (500, 1024).

B. Analysis of Results  Performance metrics of energy consumption, energy cost, and ??2 emission are used to examine the effect of the three phases. For each metric, we compare the following phases of our algorithm: earliest completion time estimation (???? ), DVFS (?? ??), forward workflow scheduling (???) and backward workflow scheduling (???).

On the left side of Fig. 2, the energy consumption decreases by up to 40% from ???? to ?? ?? due to     1 2 3 4 5 6 7 8       Energy Consumption VS Data Center (Workflow ID = 8)  Data Center ID  E ne  rg y  C on  su m  pt io  n (k  W h)     eECT DVFS FWS BWS  1 2 3 4 5 6 7 8 9 10         Energy Consumption VS Workflow Size (Data Center ID = 5)  Workflow ID  E ne  rg y  C on  su m  pt io  n (k  W h)     eECT DVFS FWS BWS  Figure 2. Effect of DVFS and scheduling policy on energy consumption.

DVFS technology, up to 12% from ?? ?? to ??? due to VM allocation policy with VM reuse, and up to 13% from ??? to ??? due to backward VM reuse strategy across different data centers. On the right side of Fig. 2, the energy consumption decreases by up to 11% from ???? to ?? ??, up to 11% from ?? ?? to ???, and up to 12% from ??? to ??? across different workflows. Similarly, the energy cost and ??2 emission also drop by each step.

From ??? to ???, the resource utilization rate increases about 20% to 25% for different workflows and data centers.



VI. CONCLUSIONS  In this paper, we propose a multi-step heuristic workflow scheduling algorithm, namely EARES-D to address the var- ious objectives including guaranteed QoS, energy efficiency and ??2 emission by a group of available data centers.

Thorough experiments are performed to demonstrate the efficiency of EARES-D . Energy consumptions, energy cost and ??2 emissions are decreased, and resource utilization rate is improved. Our future plan is to run our experiments on a local private Cloud, called Saluki Cloud established and managed by Eucalyptus with a few Beowulf clusters. We also would like to extend our model to consider possible VM migration, hibernation of some selected idle servers without affecting the response delay to future jobs.

