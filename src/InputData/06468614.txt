Mining Associative Decision Rules in Decision Tables

Abstract- There are many algorithms and approaches  developed to induce decision rules in  decision/information tables. Basically, these methods  share a common idea: reduction, including row  reduction, column reduction, and cell reduction. Most  solutions based on the rough set theory integrate these  three reductions in the above order, where column  reduction is performed by finding attribute reducts and  cell reduction is conducted via value reduction. Since  there may exist various attribute reducs, many efforts  have been put on seeking the best or optimal reduct in  the sense of accurate decisions. However, different  attribute reducts are only equivalent in the circumstance  of the given decision table. The decision rules that are  induced from different attribute reducts are not  replaceable each other for the coming objects in the  future. On the other hand, value reduction is to reduce  the decision rules to a logically equivalent minimal  subset of minimal length. Traditionally, the value reduct  has been searched through the attribute reduct. This  method may miss important decision rules. In this  paper, a novel method is presented to find associative  decision rules in a decision table by value reduction only  using the association rule mining technology. Value  reduction is conducted in a bottom-up fashion to induce  the decision rules without finding any attribute reducts.

Our method is described and demonstrated with an  illustrative example.

Keywords- Associative decision rules, rough set theory, data reduction, attribute reducts, association rule mining

I. INTRODUCTION  Rough set theory has been developed as an elegant and powerful methodology in extracting and minimizing decision rules from decision tables and has been extensively studied in the field and applied in real-life applications [4-15]. The essence of rough set theory is to reduce a given decision table small enough such that decision rules can be directly extracted. The reduction in rough set theory can be summarized as three aspects: row (tuple) reduction, column (attribute) reduction, and cell (value) reduction. Row reduction is only merging duplicate rows, attribute reduction is to fmd important attributes, and value reduction simplifies decision rules. Most algorithms and approaches to finding   decision rules follow these three steps in order. Since there may exist various attribute reducs, many efforts have been put on seeking the best or optimal reduct in the sense of accurate decisions. However, different attribute reducts are only equivalent in the circumstance of the given decision table. The decision rules that are induced from different attribute reducts are not replaceable each other for the new objects in the future. On the other hand, value reduction is to reduce the decision rules to a logically equivalent minimal subset of minimal length. Traditionally, the value reduct has been searched through the attribute reduct, which may miss important decision rules, although some authors have presented the other way to fmd value reduct first, and then attribute reducts. But very little effort has been made to find value reducts directly to induce decision rules without fmding any attribute reducts.

Unlike traditional rough set theory where decision rules are extracted from attribute reducts, we present an approach to extracting a set of associative decision rules by fmding value reduct directly in a decision table without fmding any attribute reducts. Our method uses the itemset concept exploited in mining association rules [1], [2], [3]. A bottom? up algorithm is proposed to generate item sets which are actually sub-decision tables of the original one.

The rest of this paper is organized as follows. In Section II, the traditional data reduction approach to finding decision rules in a decision table is illustrated with an example to distinguish attribute reduct and value reduct. The drawback of the traditional approach is analyzed and the related work is reviewed in Section III. Our bottom-up approach to finding value reduct without finding attribute reducts is presented and its implementation is considered in Section IV, Section V is the conclusion.



II. TRADITIONAL DATA REDUCTION TO FIND DECISION  RULES  Attribute reduction is to fmd attribute reducts. The central notions in this research are core, reduct and knowledge dependency [5]. An attribute reduct of a decision table is a subset of condition attributes that suffice to defme the decision attributes. More than one reduct for each decision table may exist. The intersection of all the possible reducts is called the core, which represents the most important information of the decision table. Finding all attribute reducts in a decision table is NP-hard [6] unfortunately, so the full power of rough set methodology may only be effective on clean and small sets of data.

Though approximation algorithms have been proposed to build reducts from a decision table either top-down or bottom-up [7], with gigabytes of data in modern database applications, direct applications of rough set methodology are prohibitively expensive.

Additionally, using any attribute reduct may still miss some important decision rules. To show this drawback of attribute reduction approach, let us consider an example.

A consistent decision table is shown in Table I, which contains five condition attributes Cl, C2, C3, C4, and C5, and one decision attribute D. For our convenience of discussion in the following, each row is labeled with an ID number.

TABLE I. AN DECISION TABLE  No. CI C2 C3 C4 C5 D  1 1 2 2 2 1 1  2 1 1 1 2 1 1  3 2 2 I 1 2 1  4 2 I 2 1 2 1 5 1 2 2 1 2 1 6 2 2 1 2 1 2  7 2 1 I 2 I 2  8 I I 2 2 I 2  9 2 1 2 2 1 3  10 1 2 1 2 1 3  TABLE II. DECISION RULE WITH REDUCTl  Rule CI C2 C3 C4 D  RI I 2 2 2 I  R2 I I I 2 1  R3 2 2 1 I I  R4 2 1 2 1 1 R5 1 2 2 1 1 R6 2 2 1 2 2  R7 2 I I 2 2  R8 I I 2 2 2  R9 2 1 2 2 3  RIO I 2 1 2 3  TABLE TIT. DECISION RULES WITH REDUCT2  Rule CI C2 C3 C5 D  RI 1 2 2 1 1  R2 1 1 I I 1  R3 2 2 I 2 I  R4 2 1 2 2 1 R5 1 2 2 2 1 R6 2 2 I I 2  R7 2 1 1 1 2  R8 1 1 2 1 2  R9 2 1 2 I 3  RIO 1 2 I I 3  One can simply verify that this decision table has two attribute reducts REDucn = {Cl, C2, C3, C4} and REDUCT2 = {Cl, C2, C3, C5}, where C4 and C5 are D? dispensible, but not both together. Table II and Table III show the reduced decision tables using attribute reducts REDucn and REDUCT2, respectively. Each row in Table II and Table III corresponds to a decision rule.

With an attribute reduct in hand, next is to do the value reduction by finding the value reduct to simplify each decision rule. Without loss of generality, we consider the first attribute reduct REDUCT 1 , and the corresponding  decision rules are shown in Table II.

First, find the equivalence class of the decision rule in  Table II. Let R;J denote the equivalence class of rules which are defined by the attribute Cj in row i in Table II, i = 1, 2, . . .  , 10, and j = 1, 2, 3, 4, 5. Namely R;jconsists of all rows (rules) that have the same value as row i in column j.

Therefore, considering row 1, we have  Rli = {I, 2, 5, 8, 1O}; R'2 = (I, 3, 5, 6, 10);  RI3 = (l, 4, 5, 8, 9); R'4 = {l, 2, 6, 7, 8, 9, 1O}.

Second, find the intersection of above equivalence classes:  R" (J R'2 (J R'3 (J R'4 = {I}.

Third, all subsets of these equivalence classes whose intersection is the same as the above intersection is considered as the value reduct of this row (row 1). One can verify that R'2' R'3' and R'4 meet this requirement. Therefore, the values in columns 2, 3 and 4 in this row constitute a value reduct of row 1, and the decision rule in row 1 can be simplified by dropping the component with the attribute Cl (column 1).

For some rows there may exist multiple value reducts. In this case, the corresponding rule can be simplified in different ways. To illustrate this, consider the rule in row 3.

R3, = (3, 4, 6, 7, 9); R32 = (I, 3, 5, 6, 10); R33 = (2, 3, 6, 7, 10); R34 = (3, 4, 5).

By intersecting the above equivalence classes, we have  R31 (J R32 (J R33 (J R34 = {3}.

It can be easily discovered that both the intersections of R31, R32 and R34 , and R33 and R34 are the same as the intersection of all the four equivalence classes. Therefore columns 1, 2, and 4 in row 3 constitute a value reduct of row 3, while the columns 3 and 4 constitute another value reduct of this row. That means that rule 3 can be simplified by either dropping the component of attribute 3 or dropping the components of attributes 1 and 2. Table IV summarizes the final value reducts of decision rules in Table II.

Similarly, if attribute reduct REDUCT2 shown in Table III is chosen, by repeating the above value reduction process, the decision rules in Table III can be simplified.

The final value reducs of these decision rules are summarized in Table V.

TABLE IV.

TABLE V.

SIMPLIFIED DECISION RULES IN TABLE II WITH V AUE REDUCTION  Rule Cl C2 C3 C4 D  Rl 2 2 2 1  R2 1 1 1 1  R3 2 2 1 1  R3 ' 1 1 1  R4 2 2 1 1 R4' 1 1 1 R5 2 2 1 1  R5' 1 1 1 R6 2 2 2 2  R7 2 1 1 2  R8 1 1 2 2  R9 2 2 2 3  RIO I 2 1 3  SIMPLIFIED DECISION RULES IN TABLE lIT WITH V AWE REDUCTION  Rule Cl C2 C3 C5 D  RI 2 2 I I  R2 1 1 1 1  R3 2 2 2 1  R3 ' 1 2 1  R4 1 2 1 R4' 2 2 2 1 R5 2 2 2 1  R5' 1 1 1 R6 2 I 1 2  R7 2 I I 2  R8 1 1 2 2  R9 2 2 1 3  RIO I 2 I 3  One can see from above example that no matter which attribute reduct is selected, some other decision rules will be missing. Even if they are equivalent in the given decision table, there is no general way to tell from which is more important. For example, consider two rules R3' in both  value reductions in Table IV and Table V. For a future instance that has missing C4 value, R3' in Table V can be applied, while if the instance has C5 value missing, then R3'  in Table IV can be applied to make decision.



III. RELATED WORK  The traditional method to find decision rules via attribute reduction and value reduction is elegant and powerful, but it has some drawbacks that are summarized as follows:  1) Generally, if multiple attribute reducts exist in a decision table, choosing any attribute reduct to induce the decision rules may cause other significant rules missing.

Unfortunately, finding all or minimum reducts is a NP-hard  problem [6].

2) Many algorithms to find optimal attribute reducts  have been developed, and the decision rules are merely induced from the only attribute reduct resulted in the algorithms. We argue that some very significant decision rules may not be extracted from this attribute reduct because the decision rules that are generated in this way are not evaluated with the same criteria.

3) Using traditional value reduction, the most significant concise rules may not be discovered because the value reduction is done only in terms of condition attributes which are considered and processed individually. For example, both attributes and value reducts in Table IV and Table V do not find rules:  C4 =1 -+ D = 1, and C5= 2-+D= 1.

Recently, some efforts have been conducted on the value reduction from various perspectives. The proposed methods can be summarized into two categories. The first one attempts to develop or improve efficient value reduction algorithms based on discenibility matrix [8, 10, 13], while the other one integrates attribute reduction and value reduction two steps together, either attribute reduction first and value reduction second [9, 11], or vice verse [12]. But none of them takes advantages of association rules mining technology and avoids the search of attribute reducts. The preliminary effort on this regard can be found in [14].

Association rule mining has been extensively studied in the field since the original Apriori algorithm was proposed [1], and more and more improved and extended algorithms have been developed [2], [17], [18]. Basically, the task of association rule mining is to find all relationships among items in a transactional database that satisfy some minimum support and minimum confidence constraints. One main drawback of traditional algorithms of mining association rules is that too many rules will be generated where most of them are trivial, spurious, and even useless. This is because there is no mining target predefined, and the rule generation is a blind-search [1]. In order to overcome this problem, some constraints have been enforced on the format of association rules, such as the rule template [18] where a rule pattern is predefined before the algorithm executes and only those rules that match the template will be discovered. A special rule template has been studied to constrain the association rules being classification rules where the right? side of rules must be a class label [15], [16], [19], [20]. This rule template is called class association rule or associative classification rule. With this template, an approach was proposed in [20] to construct a classifier, which consists of two steps. First the Apriori algorithm is adapted to find all class association rules with predetermined minimum support and threshold. In the second step, all class association rules are sorted in terms of their confidence and support, and are selected in sequence to find a small set of rules that has the    lowest classification error. The small set of class association rules selected in this fashion forms an associative classifier.

Since classification analysis is independent from association mining, one must convert each training instance of classification to a set of items that are represented as attribute-value pairs into order to apply the Apriori-like algorithm to discovering association rules. Even though the rule template is exploited in generating class association rules, the number of resulted rules is still prohibitive, especially in large databases or decision tables with many different values for each attribute. To reduce the number of possible attribute-value pairs, a method is attempted in [21] to generate attribute reducts from the original data using rough set theory, and then construct from the reducts generated classification rules with a rule-based system and neural networks.



IV. OUR METHOD - VALUE REDUCTION ONLY  For a given decision table, there may exist a large amount of decision rules. It is hardly possible to fmd all decision rules, not to mention, many decision rules are trivial or not significant. On the other hand, as discussed in previous section, most existing approaches to finding decision rules are first finding an attribute reduct and then simplifying the decision rules induced from the attribute reduct. This way the induction process is apparently biased and any significant rules that are not entailed by the chosen attribute reduct will not be discovered, even not be evaluated by any chance.

In this section, we present a method to mine decision rules in a decision table via value reduction only. The proposed method is based on the process of mining association rules. For this reason, the decision rules discovered in this way are called associative decision rules.

In our method, each cell in the decision table is considered as a <attribute, value> pair, and called an item.

Adopting the itemset idea from association rules mining [1], we come up a bottom up version of data reduction to find associative decision/classification rules. We present an approach to fmding value reduct in a decision table in a bottom-up fashion without fmding attribute reducts. Our method can discover concise and significant decision rules.

Consider a given decision table. As in [1], an itemset consists of encoded items of uniform length, say k, k=2, 3, ",. We distinguish condition items that are formed by condition colunms from decision items that are generated from decision attribute (column). Each k-item is constituted of k-l condition attributes and 1 decision attribute, thus called k-itemsets. All itemsets can be constructed iteratively.

This iteration will continue until either all items are exhausted or no more interesting item sets can be generated.

With this transformation, associative decision rules can be mined using association rules mining technology.

Let's revisit the example shown in Table I.

1) First iteration - mining I-condition associative  decision rules The simplest form of decision rule consists of one  condition and one decision. To find all decision rules of this kind, we consider 2-itemsets, where the first item is a <condition attribute, value> pair, while the second item is a <decision attribute, value> pair.

For each item, check to see whether there are conflicts or inconsistent itemsets. Two itemsets are inconsistent if  ? they have the same condition attribute with the same value and the same decision attribute but with different value; and  ? the equivalence classes induced from each of them  are not empty.

For example, consider two itemsets (<CI, 1>, <D, 1?  and (<C 1, 1>, <D, 2?. From Table I, one can verify that the first itemset covers rows 1, 2, and 5 (meaning the corresponding equivalent class is ( 1, 2, 5), while the second itemset has the coverage of (8). Therefore these two itemsets are inconsistent or conflicting.

If an itemset is consistent with any other itemsets, we can claim that this itemset constitutes an assoclatlve decision rule. Table VI lists all 2-itemsets (I-condition itemsets) and their coverage.

For each column in Table VI, the itemsets consistence can be checked. For example, in the first column {CI, D}, itemsets (<CI, 1>, <D, 1>), (<CI, 1>, <D, 2>), and (<CI, 1>, <D, 3? are inconsistent, since they have the same condition part but different decisions. One can see from Table VI, in the columns (C4, D), there is one itemset (<C4, 1>, <D, 1? is consistent with all other itemsets.

Similarly, in the columns {C5, D}, the itemset (<C5, 2>, <D, 1? is consistent with all other itemsets. Therefore, these two consistent 2-itemsets discovered in Table VI will be kept and used to filter out their supersets from being investigated in the following iterations.

2) Second iteration - mining 2-condition associative  decision rules In the second iteration, consider the 3-itemsets with two  condition items and one decision item in the form of (<Ci, Vi>, <Cj, Vj>, <D, d>), where i, j = 1, 2, 3, 4, 5, and i :;t j, {<Ci, Vi>, <D, d>} and {<Cj, Vj>, <D, d>} are inconsistent in previous iterations (or in Table VI in this case). The result is illustrated in Table VII, where all itemsets that contain the items <C4, 1> or <C5, 2> are excluded.

From Table VII, one can see that all itemsets are inconsistent, and therefore no associative decision rules can be extracted from 3-itemsets.

3) Third iteration - mining 3-condition associative  decision rules Similarly, in this iteration, all 4-itemsets with three  condition items and one decision item will be explored, excluding all supersets of consistent 2- and 3-itemsets discovered in the first two iterations. The result is illustrated in Table VIII.

{CI,D}  Value Coverage  1, 1 1,2,5 2, 1 3,4 2,2 6, 7 1,2 8 2,3 9 1,3 10  {CI,C2,D}  Value Coverage  1,2, 1 1,5 1, 1, 1 2 2,2, 1 3 2, 1, 1 4 2,2,2 6 2, 1,2 7 1, 1,2 8 2, 1,3 9 1,2,3 10  {C2,C4,DJ  Value Coverage  2,2, 1 1 1,2, 1 2 2,2,2 6 1,2,2 7,8 1,2,3 9 2,2,3 10  {Cl, C2, C3, OJ Value Coverage  1,2,2,1 1,5  1,1,1,1 2  2,2, 1, 1 3  2, 1, 2, 1 4  2,2,1,2 6  2,1,1,2 7  1,1,2,2 8  2, 1,2,3 9  1,2,1,3 10  {CI, C4, C5, DJ  Value Coverage  1,2, 1,1 1,2  2,2, 1,2 6, 7  1,2, 1,2 8  2,2, 1,3 3  1,2, 1,3 10  TABLE VI 2-1TEMSETS AND COVERAGE  {C2,D} {C3,D} {C4, D)  Value Coverage Value Coverage Value Coverage  2, 1 1,3,5 2, 1 1,2 2, 1 1,2 1, 1 2,4 1, 1 2, 3 1,1 3,4,5 2,2 6 1,2 6 2,2 6,7,8 1,2 7,8 2,2 8 2, 3 9, 10 1,3 9 2,3 9 2,3 10 1,3 10  TABLE VII 3-1TEMSETS AND COVERAGE  {CI,C3,D} {CI,C4,D} {CI,C5,D}  Value Coverage Value Coverage Value Coverage  1,2, 1 1,5 1,2, 1 1,2 1, 1, 1 1,2 1, 1, 1 2 2,2,2 6, 7 2, 1,2 6, 7 2, 1, 1 3 1,2,2 8 1, 1,2 8 2,2, 1 4 2,2,3 9 2, 1,3 9 2, 1,2 6, 7 1,2,3 10 1,  1,3 10 1,2,2 8 2,2,3 9 1, 1,3 10  {C2, C5, DJ {C3, C4, D) {C3, C5, DJ  Value Coverage Value Coverage Value Coverage  2, 1, 1 1 2,2, 1 1 2, 1, 1 1 1, 1, 1 2 1,2, 1 2 1, 1, 1 2 2, 1,2 6 1,2,2 6, 7 1, 1,2 6, 7 1, 1,2 7,8 2,2,2 8 2, 1,2 8 1, 1,3 9 2,2,3 9 2, 1,2 9 2, 1,3 10 1,2,3 10 1,  1,3 10  T ABLE VIII 4-ITEMSETS AND COVERAGE  {Cl, C2, C4, OJ {Cl, C2, C5, OJ {Cl, C3, C4, OJ Value Coverage Value Coverage Value Coverage  1,2,2,1 1 1,2, 1, 1 1 1,2,2,1 1  1, 1,2, 1 2 1, 1, 1, 1 2 1, 1,2, 1 2  2,2,2,2 6 2,2,1,2 6 2,1,2,2 6,7  2,1,2,2 7 2,1,1,2 7 1,2,2,2 8  1,1,2,2 8 1, 1, 1,2 8 2,2,2,3 9  2,1,2,3 9 2,1,1,3 9 1,1,2,3 10  1,2,2,3 10 1,2, 1,3 10  {C2, C3, C4, D) {C2, C3, C5, D J {C2, C4, C5, D)  Value Coverage Value Coverage Value Coverage  2,2,2,1 1 2,2,1,1 1 2,2, I, I I  I, 1,2, I I I, I, I, I 2 1,2, I, I 2  2, 1,2,2 6 2, I, 1,2 6 2,2, 1,2 6  I, 1,2,2 7 I, I, 1,2 7 1,2, 1,2 7,8  1,2,2,2 8 1,2,1,2 8 1,2,1,3 9  1,2,2,3 9 1,2, 1,3 9 2,2,1,3 10  2,1,2,3 10 2,1,1,3 10  {C5,D}  Value Coverage  1, 1 1,2 2,1 3,4,5  1,2 6,7,8 1,3 9, 10  {C2, C3, D)  Value Coverage  2,2, 1 1,5 1, 1, 1 2 2, 1, 1 3 1,2, 1 4 2, 1,2 6 1, 1,2 7 1,2,2 8 1,2,3 9 2, 1,3 10  {C4, C5, DJ  Value Coverage  2, 1, 1 1,2 2, 1,2 6,7,8 2, 1,3 9, 10  {Cl, C3, C5, OJ Value Coverage  1,2, 1, 1 1  1, 1, 1, 1 2  2,1,1,2 6,7  1, 2,1,2 8  2,2,1,3 9  1, 1, 1,3 10  {C3, C4, C5, DJ  Value Coverage  2,2, I, I I  1,2, I, I 2  1,2, 1,2 6, 7  2,2, 1,2 8  2,2,1,3 9  1,2, 1,3 10    It can be easily seen that any 5-itemset must be a superset of at least one consistent itemsets that were discovered in previous iterations. Therefore, the iteration terminates and all associative decision rules are mined.

Integrating the consistent item sets from Table VI, Table VII, and Table VIII, we have the final reduced decision table illustrated in Table IX, where each row is corresponding to a simplified concise decision rule.

No.

1 1      TABLE IX. THE FINAL REDUCED TABLE  CI  I  I   I        C2 C3 C4   2 2  1 1 1 1 1 2  2 I  2 2   1 2  2 2    2 2 2  2 2

V. CONCLUSION  C5       D   I  I           I  I  Traditional approaches to inducing decision rules from a decision table are based on the rough set theory by finding attribute reducts first and value reducts then. Unfortunately, generating decision rules from any attribute reducts may miss some important rules, and finding all attribute reducts is NP-hard. In this paper, we presented an approach to mine significant associative decision rules by finding value reducts directly without finding any attribute reducts. Our approach integrates the itemset idea of mining association rules and can be implemented with efficient association rules mining algorithms. The bottom-up constructing process of associative decision rules was described and illustrated with an example.

The approach proposed in this paper is still preliminary.

The further research on this topic will be conducted and focused on three aspects. The first one is to convert a decision table to items efficiently and effectively. The second one is to add the coverage of each item as its frequency, which can be employed to calculate support and confidence that are applied in mining associative decision rules. The third one is to adapt existing association rules mmmg algorithms and develop appropriate specific algorithms to our situation.

