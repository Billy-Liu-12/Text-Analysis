Improving Intrusion Detection Performance Using Rough Set Theory and  Association Rule Mining

Abstract  Intrusion Detection System has some defects, such  as signatures being generated manually, updating attack signatures difficultly and doing nothing in front of ultra data set. This paper presents a hybrid approaches for modeling IDS. Association rule mining and Rough set theory are combined as a hierarchical hybrid intelligent system model. The hybrid intrusion detection model combines association rule mining and rough set theory to improve detection accuracy and reduce false alarm, unreal alarm and computational complexity. Empirical results illustrate that the hybrid intrusion detection model can detect intrusion more accurately.

1. Introduction  This paper deals with intrusion detection and the  related issue of giving a good detection system.

Intrusion detection (ID) is a type of security management system for computers and networks. An intrusion detection system (IDS) inspects all inbound and outbound network activity and identifies suspicious patterns that may indicate a network or system detects attack from someone attempting to break into or compromise a system.

Several machine-learning paradigms including neural networks, linear genetic programming, support vector machines, Bayesian networks, multivariate adaptive regression splines, fuzzy inference systems,  rough set theory, etc. have been applied to the design of IDS [1]. Many data mining techniques, including discovering association rules, have been applied to intrusion detection [2]. Discovering association rules was introduced in [3]. These approaches build detection models by applying data-mining algorithms to large data sets of audit data. But sometimes, many unuseful rules are produced.

In this paper we investigate and evaluate the performance of association rule discovering, rough set theory and a hybrid system. The reason for using the hybrid approach is to improve the accuracy of the intrusion detection system when compared to using individual approaches. The hybrid approach combines the best results from the individual systems resulting in more accuracy.

In this paper, we will concentrate on using the ensemble of rough set theory and association rule discovering to achieve better classification accuracies.

The data we used in our experiments is KDD CUP 99 data sets [4]. It was developed for IDS evaluations by DARPA. We performed experiments to classify each of the five classes of patterns in the data. It is shown that using the hybrid system for classification gives good accuracies.

In the rest of the paper, a brief introduction to related work in the field of intrusion detection is given in Section 2. A brief introduction to the data we used is given in Section 3. In Section 4 the theoretical aspects of rough set theory, association rule discovery and a hybrid system are described. In Section 5 the experimental results of the hybrid system are presented.

In Section 6, we conclude our results.

2. Intrusion Detection   There are basically two main types of IDS being  0-7695-2674-8/06 $20.00  ? 2006    used today: Network based and Host based. Network Intrusion Detection System (NIDS) analyzes individual packets flowing through a network. The malicious packets that may be overlooked by a firewall simplistic filtering rules can be detected in the NIDS. While in a host-based system, the IDS examines the activity of each individual computer or host.

Intrusion detection technology has two main categories: anomaly detection and misuse detection.

Misuse detection is the most common approach to intrusion detection, which detects previously seen, known attacks by looking for attack signatures in attack database. The main disadvantage of misuse detection is that new attacks cannot be predicted or detected without coding them into the IDS attack database and has high false-alarm rate. In fact, these systems need to be updated frequently to detect new attacks as soon as they are discovered.

Anomaly detection is based on the assumption that misuse or intrusive behavior deviates from normal system use[5]. The main advantage of anomaly detection is that it can detect new attacks.

A review of the many alternative approaches to intrusion detection is available in [6].

3. Intrusion data set   The KDD Cup 1999 intrusion detection contest  data[4] is used in our experiments. The 1998 DARPA Intrusion Detection Evaluation program by MIT Lincoln Labs prepared this data. Lincoln labs acquired nine weeks of raw TCP dump data.  The raw data was processed into 24 attack types. These attacks belong to four main categories: probing, denial of service (DoS), user to root attacks (U2R) and remote to user attacks (R2U).

For each TCP/IP connection, 41 various quantitative and qualitative features plus one class label were extracted in the data set. Each record includes 9 basic Features, 13 Content Features, 9 Time-based features and 10 Host-based features. Basic Features were basic features to every network connection like duration of connection, service requested, and bytes transferred between source and destination machine etc. Content Features were like logged in flag, number of failed logins, hot indicators, etc. Time-based features were collected by observing various connections in ?two- second? time window with respect to current connection, such as SYN error rates, Rejection rates, number of different services requested etc. Host-based features were collected that were based on the past 100 connections similar to the one under consideration.

Experiments have two phases, namely, training phase and testing phase. In the training phase the  system builds a model using the training data to give maximum generalization accuracy (accuracy on unseen data). To find out the intrusion in the testing phase, the constructed model detects the test data. Besides the four different types of attacks mentioned above we also have to detect the normal class. Training data set in the paper contained 49,402 records, which were randomly generated from the KDD Cup 1999 10% training data set. Test data use 10% of test data sets of the KDD Cup 1999 data sets. Table 1 gives the distribution of data categories in the training and test data.

Table 1: Data distribution in training data and  test data  Training data Test data  Type Count Percent Count Percent  Normal 9768 19.77% 60592 19.48%  Probing 435 0.88% 4166 1.34%  DoS 39085 79.11% 231453 74.42%  U2R 3 0.01% 79 0.03%  R2L 111 0.22% 14740 4.74%   4. Hybrid system for Intrusion Detection  4.1. Rough set theory   Rough sets have been introduced as a tool to deal with inexact, uncertain or vague knowledge in many branches of artificial intelligence. The investigation of the rough set methodology for data mining is a challenging research area.

Information system is defined as S = (U, A?{d}, V, f), where U is a non-empty, finite set of objects called the universe, V is a set of values of attributes in A such that f:U?Va for any a?A, where Va is called the domain of a, and {d} includes decision attributes. The information system is also called a decision table.

Each non-empty subset B ? A determines an indiscernibility relation as follows:  RB={(x,y) ?U?U:a(x)=a(y) ? a?B}.

RB partitions U into a family of disjoint subsets U/RB called a partition of U:  U/RB={[x]B:x?U}, where [x]B denotes the equivalence class determined by x with respect to (w.r.t.) B, i.e.,  [x]B={y?U:(x,y) ?RB}.

0-7695-2674-8/06 $20.00  ? 2006    Given a set (or concept) X ? U, and an equivalence relationship RB ? A, one can define X?s lower and upper approximations:  Lower Approximation, R B(X)={x:(x?U)?([x]B ? X)}  Upper Approximation, R B(X)={x:(x?U)?([x]B?X?? )}.

The lower approximation R B(X) is the set of objects that belong to X with certainty, while the upper approximation R B(X) is the set of objects that possibly belong to X. The pair ( R B(X), R B(X))is referred to as the Pawlak rough set of X w.r.t. B.

4.2. Association rule mining  Association rule discovery, the area being studied  most actively[3][8][9][10], is a technique to investigate the possibility of simultaneous occurrence of the data.

Let I={i1, i2,..., im} be a set of literals, called items.

Let D be a set of transactions, where each transaction T is a set of items such that T ? I. An association rule is an implication of the form X?Y, where X?I, Y?I, X ?Y=? . Support and confidence are used to describe the ?interestingness? or ?goodness? of a rule as follows:  support(X?Y)=P(X?Y), confidence(X?Y)=P(Y | X),  or confidence(X?Y)= support(X?Y)/P(x).

The support of the rule X?Y is the proportion of transactions in D that contain X?Y. The confidence of the association rule X?Y is the proportion of transactions including both X and Y to all the transactions that include X. The existing association rule discovery techniques have discovered the association that may happen only among the data that satisfy the minimum support and minimum confidence set by the users. All item sets that have support greater than or equal to the user specified minimum support are generated.

That is, generating all frequent item sets according to minimum support value. Then generate all the rules that have minimum confidence in the following naive way: For a frequent item set B and any X ? B, let Y =B-X. If the rule X?Y satisfies the minimum confidence, it is a valid rule.

4.3. Hybrid rough set-association rule mining system    A hybrid intelligent system integrates different learning techniques. Different learning models produce better performance than the individual learning approaches by reducing their individual limitations and making using of their advantages. Figure 1 shows the architecture of the hybrid intelligent system with rough set theory and association rule discovery.

Phase 3  Phase 2  Data Reduction by RST  Reducted Data  Rule generation By Association Rule Discovery  Raw Data  Rule Selection  Rule Sets  Phase 1 Intrusion Detection  Data  Figure 1: The Hybrid Intelligent System with Rough Set Theory and Association Rule  Discovery.

4.3.1. Reduction by rough set theory. In the database, there are 31 quantitative attributes of 41 attributes, which result in large amount of data and high data dimensionality. The processing of ultra large data set is a novel area for the rough set methodology. One of the plausible approaches to tackle ultra large data is to reduce the data set horizontally, which means that a set of continuous values is partitioned into a finite number of intervals (categories). And this is common to the rough set community. The data-preprocessing unit discretizes the numerical attributes by an automatic discretization algorithm, Semi-naive algorithm[11].

Semi-naive algorithm has more logic to handle the case where value-neighboring objects belong to different decision classes.

For example, in our experiments the cuts of attribute Duration after discretization are  {0.5, 57, 62, 65, 72.5, 328.5, 337.5, 5007, 5064, 5065.5, 5195, 10017.5, 10228, 12503, 12553.5, 13901.5, 14191.5, 21204.5, 22625, 28847.5}.

4.3.2. Rule generation by discovering association rule. In the application of association rule mining, sometimes mined rules are not useful, especially in mining classification rules, i.e.,  Service (Http) ? Port (80).

Even if the confidence of the rule were 100%, such  rules are meaningless and lead to the number of detection rules becoming larger, which will lower detection efficiencies and effect detection accuracy.

0-7695-2674-8/06 $20.00  ? 2006    Restricting right-hand-side of association rules to the classification attribute can solve the problem. In this paper association rule mining is redefined under rough set theory frame as follows:  given a information system S = (U, A?{d}, V, f ), as defined in rough set theory, X?Y is called a association rule where X=?i <ai,vi>?ai?A?vi=f (ai) and Y?{d}.

The rule measurements of support and confidence are defined the same as before.

Figure 2 is the algorithm of classification rule mining by association rule discovery based on rough set theory. First Completion and Discretization are used to preprocess the data by rough set theory.

RuleGeneration(U) {  Completion (U); Discretization (U); F1={large 1-attrrule}; RULE1=genRules(F1); for (k=2; Fk-1?? ; k++) do  Ck=Candidate-gen(Fk-1); for each data record r ?U do  Cr=subset(Ck, r); for each candidate c ?Cr do  c.condCount++; if r.class=c.class then c.ruleCount++;  end end Fk={ c ?Ck | c.ruleCount ? minsupp} RULEk= genRules(Fk);  end RULE=?k RULEk;  } Candidate-gen(Fk-1) {  insert into Ck  select p.attr1, p.attr2, ?, q. attrk-1, p.class from p, q?Fk-1 where p.attr1= q.attr1, ?, p.attrk-2= q.attrk-2, p.class= q.class, p.attrk-1? q.attrk-1; for all c?Ck do  for all (k-1)-subsets s of c do if (s.class ?? ? s?Fk-1) then delete c from Ck;  } genRules(Fk) {for all p?Fk do  conf = support(p)/support(p.attr1, p.attr2, ?, p. attrk-1) ? if ( conf ? minconf ) then  Output the rule p.attr1, p.attr2, ?, p. attrk- 1?p.class, with confidence= conf and support =support(p);  end} Figure 2: The algorithm of classification rule  mining by association rule discovery.

4.3.3. Rule selection. When we apply the algorithm in Figure 2, we get complete set of association rules that satisfy the user-specified minimum support and minimum confidence conditions. In order to improve the detection efficiency, doing some rule selection as follows can reduce the number of the rules.If ? ??1 and ? ??2 are two rules with the same precedence, but the conclusions are not the same, then the rule with lower confidence will be discarded. This can avoid repeated and inconsistent rules.

If ?1 ?? and ?2 ?? have the same conclusions but different precedence, and at the same time there exits ?1 ? ?2??2 ?? is a redundant rule while ?1 ?? is more general and simple. So the rule ?2 ?? will be deleted from rule sets?  Besides the measurements of the support and confidence of association rules, we need a measure of dependent or correlated events, which is called Interest (correlation, lift) of a association rule defined as, lift (? ? ?)=P( ? | ?) / P(?) = P(?? ?) / ( P(?) P(?) ).

According to the value of lift (? ? ?), we can do some rule pruning as follows.

? and ? are independent events, if lift (? ? ?)=1 or P  (?? ?)= P (?) P (?), the rule will be pruned from the rule sets.

If the value of lift (? ? ?) is less than 1, ? and ? are negatively correlated. Even if the support or confidence of the rule were high, the rule would mislead the detection. So such rule will be pruned too.

Otherwise ? and ? are positively correlated; it means the occurrence of ? implies the occurrence of ?. The rule should be reserved in the rule sets.

5. Experiments  5.1. Experiments using rough set theory   The same data set described in Section 3 is being  used to test the performance of rough set theory. The Information system is first discretized by semi Naive algorithm, and then we use genetic algorithm [11] doing attribute reduction. Finally the decision table is obtained and classification rule can be extracted from the decision table. Table 2 gives the detection accuracy of experiments using rough set classification rules. In Table 2, 99.0% of the actual ?normal? data and 94.0% of the actual ?DoS? data are detected correctly. But the data of the rest classes are detected not well.

0-7695-2674-8/06 $20.00  ? 2006    Table 2: The detection accuracy of  experiments using rough set theory (Overall  Error =11%)  Class Accuracy (%) Normal 99.0% Probing 2.6% DoS 94.0% U2R 0% R2L 0%  5.2. Experiments using the hybrid system   The same data set described in Section 3 is being used for training and testing different experiments by the hybrid system with different minimum supports.

The Detection accuracy of the tests is showed in Table 3.

Table 3: The detection accuracy of three  experiments done with different minimum  supports by the hybrid system.

minsup=1% minsup=0.5% minsup=0.05% Class Accuracy(%) Accuracy(%) Accuracy(%)  Normal 99.50% 99.47% 99.47% Probing 50.77% 74.8% 74.8%  DoS 96.39% 96.6% 96.6% U2R 0% 2.53% 3.8% R2L 0.97% 0.73% 1.21%  The third row of Table 3 indicates that 99.50%,  99.47%, 99.47% of the actual ?normal? data points are detected correctly in the three tests with different minimum supports. But there also exits very bad results. The sixth row of Table 3 shows that only 0%, 2.53%, and 3.8% of the actual ?U2R? data points are detected correctly when minimum support is 1%, 0.5%, and 0.05% respectively in the tests. R2L?s cases are little better than ?U2R? data?s but not optimistic yet.

The reason maybe is that there are too few U2R data and R2L data, which are 3 and 111 connections, or 0.01% and 0.22% respectively in training data, to produce good enough detection rules.

Table 4 gives overall accuracy of the classification and rule count of each test. The second row of Table 4 indicates that three tests? overall accuracies are 91.8%, 92.3%, and 92.4% respectively. The last row of Table 4 indicates the final detection rule amount produced by rule selection of the hybrid system.

Table 4: The overall accuracy of the  classifications and rule count of each test.

minsup=1% minsup=0.5%minsup=0.05% Overall  accuracy 91.8% 92.3% 92.4%  Rules 74 158 104  We also can see that the detection results by the  hybrid system In Table 3 are better than those by rough set theory in Table 2. The rule amount of each test in Table 4 is rather less and applicable.

6. Conclusions   The importance of using hybrid system for modeling IDSs has been studied in this paper. A hybrid system combines the synergistic features of different intelligent information discovery methods without complex computation. Contributions of this paper are listed as follows.

It provides a hybrid system of rough set theory and association rule discovery.

The hybrid system outperforms both rough set theory and association rule mining in the important respect of classification accuracies for all the five classes.

It proposes a new definition and recognition of association rule mining under the frame of rough set theory.

