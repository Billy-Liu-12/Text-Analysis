MULTI-SENSES AND MULTI-DEPENDENCIES DISCOVERY AMONG

ABSTRACT  Word sense and word dependency benefit many applications. Manually constructed lexicon usually serves as a source for word sense and word dependency. However, there always requires very expensive work and extensive time consumption to compile it, simultaneously senses and relationships anlong words in these kinds of lexica are always missed. Automatically compilied lexica are under developing, the main obstacle is to discover multi-senses and multi-dependencies among words.

In this paper. we propose a new method combining an improved ISODATA Clustering algorithm with Association Rule mining to answer the question.

With the recursively clustering algorithm lower frequency senses are discovered. As well a approach for refinement is put forward to improve the precision. Experiments indicate that the approach presented here provides preferable outputs.

Keywords: Word Sense, Word Dependency, Clustering Algorithm  1. INTRODUCTION  With the development of software intelligence, electronic lexicon is becoming a more and more important tool for many applications, such as information retrieval, NLP(Natnra1 Language  0-7803-7902-0/03/$17.00@ 2003 IEEE  Processing) and machine leaning. Two main parts in lexicon are word senses and word dependencies.

Word senses are the meanings of the word, usually interpreted as a serial of synonyms or other similar words. Word dependencies are the relationships between words, such as subset of, hypemyms of, meronyms of and causal dependencies, etc.

Traditional thesaurus are often constructed manually e.g. WordNet [Miller et al.19901. It leads to very expensive work and extensive time consumption. At the same time, maintenance of these lexica is also difficult. For example, WordNet classifies retiring into four senses, zmnssertive, 1tnass7rming, receding, past, but misses the sense of alone. Neither could WordNet deal with the unknown word, e.g. LSI. an abbreviation for Latent Semantic Indexing.

Many researchers are working on automatically compiling lexica [Landauer et al.1990, Lin, 1998, Pereira et al. 19931. The work is based on the known Distributional Hypothesis [Harris, 19851, which supposed that words occurring in the same context tend to be similar. Accordingly, various methods to acquire senses are proposed, however rarely addresses the multi-senses discovery. Lin put forward cluster algorithm (CBC) to discover multi-senses [Lin, 1998, Patrick, 20021. But without rigorous refinement, these methods also lead to some frustrating output. For instance, Americnn is learned as a close word to retiring in their lexicon.

mailto:Ijz@keg.cs.tsiiighua,edu.cn   Relationship discovery is often defmed to fmd "interest" pattems for specific applications. Past work mainly focused on shopping data, such as market basket problem. Introduced into electronic lexicon, the "interest" pattems can be interpreted as: "word Y is very likely to be presented in the context  containingX, ;..,A', ".

In this paper. we propose a new approach to learn multi-senses and multi-dependencies among words.

Content of this paper is structured as followings.

Section 2 gives the survey of the related work.

Section 3 describes our method to acquire multi-dependencies and multi-senses among words.

The evaluation and experiment are given in Section 4 and 5 .  Finally, we conclude the paper with a discussion.

2. RELATED WORK  Andrei Mikheev and Steven Finch design KAWB to acquire knowledge from test [Mikheev et al.19971.

The workbench compromises a set of computational tools for uncovering intemal structure in natural language texts. Two main stages (i.e. text representation and text analysis) are included in their workbench to implement incremental process, including a rough automatic extraction, organization of semantic regularity, and a computer supported analysis of extracted data.

Alexander Maedche and Raphael Volz also put fonvard an Ontology Extraction and Maintenance Framework to accelerate ontology construction and simplifv the ontology maintenance [Alexander et al.20011. They employed a hierarchical cluster algorithm to acquire the word meanings, and adopted association rule algorithm to obtain relationships anlong words. Faure and Nedellec presented a cooperative machine learning system, ASIUM, which acquires taxonomic relations and sub-categorization frames of verbs based on syntactic input [Faure et al. 19981. Thc ASIUM systcm hierarchically clustcrs  nouns based on the verbs that they are syntactically related with and vice versa. Thus, with the cooperative operation, they extended the lexicon, the set of concepts, and the concept hierarchy. Hahn and Schnattinger introduced a methodology for the maintenance of domain specific taxonomies. They built up a framework Syndicate for natural language understanding [Hahn et al.20001. But in their methods, they failed to extract multi-senses among words. Landauer and Dumais employed the L S I (Latent Semantic Indexing) to acquire latent word senses [Z]. Nor did they address the multi-senses discovery. Dekang Lin and Patrick Pantel put forward clustering algorithm (CBC) to discover multi-senses from words [3,6]. They use a set of tight clusters called committee to define the center for the clusters, assign other words to their most close committee and remove the intersecting features from words after each cluster pass, and then re-cluster the new matrix to find lower frequency senses.

In this paper, we put forward a framework Concepts Network Leaming Workbench (CNLW). In this workbench we intend to construct a lexicon automatically with multiple word senses and multi-dependencies among words.

'  3. CNLW ARCHITECTURE  In this section, we elaborate the architecture of CNLW.

The systematic model can be described as a 4-tuple:  CNL W := (LexicalParser, DependencyMiner, SenseMiner, Re Jiner}  Where: LexiculPnrser parses the text corpora, segments  the sentences into words and removes the stop words, then constructs item-by-item matrix.

DependencyMiner mines dependencies among words.

SenseMiner is used to discovcr multi-sets     similarity words for each word and result in similar word sets with similarity value.

Rejner is a very important component, which compares the result of SenseMiner with WordNet, and refines the results.

CNLW architecture is depicted in figure 1.

I I  c Lexical Parser  lmtrix I I+-  b-l?t R e d i d i s p l a y Figure 1 : Shucture of CNLW  CN is the leaniing result, naniely concepts network.

We defuie CN as: CN := (Cs, Ds, WSS) , where Cs is  a set of words. Ds is a set of dependencies. Wss is the collection of senses for each word.

3.1. Multi-Dependencies Discovery Association rule mining is a powerful tool in data mining. It is usually used to mine item relationships for shopping baskets. An early approach, due to Agrawal, Imielinski and Swami [Agrawal et al.19931, was to find a set of items that occur together often (i.e. high support), and also have the property that one item often occurs in a transaction containing the other items (i.e. high confidence). Its measure for interest is based on the conditional probability.

After this mining processing, relationships can be discovered A relation can be described as:  R := ( I d ,  Anlecedeiit,Coiisequeiil, Support, Conjdence, Re lationship)  Where: Antecedent and Consequent are two word sets in  the relation, which can be interpreted as:  Antecedent = Consequent.

Support is the relation weight.

Conjdence stands for the reliability of the relation.

Relationship denotes the type of the relation.

For Example, as for the CRANFIELD document  Experiments -> Perform cone 65.3% sup: 7.2% Kinematic -> v e l o c i ~  conf 73.4% sup: 6.4% High, speed -> flight conf66.3% 5.8%  collection:  ......

Type of dependency between two word sets is  required to be specified manually. As for Chinese corpora, seventy four dependency categories derived from HowNet are used to choose. As for English corpora, dependency categories are based on FrameNet.

3.2. Multi-senses Discovery We employ an improved clustering algorithm to yield multi-senses for words. First, a word similarity matrix is generated according to original corpora.

Every value in the matrix indicates the similar degree between two words. Then multi-passes clustering is applied on the matrix. For each pass, an appropriate meaning for every word is yielded. Then corresponding features are removed from its feature vectors for the next clustering pass.

We use a classic similarity measure namely cosine coefficient to compute the similarity matrix.

c, = (w~,,, wi, ;.. wen) is the feature vector of word i ,  where wU is computed by TF*IDF. Similarity  between ci and c j  is defined as:     As for partitional clu$tering, seed selection is the key. Various selections would result in different outputs. We derive an idea from ISODATA algorithm to realize dynamic tuning K [Ravi et a1.19991. Based on the intersecting features removing, we improve the algorithm with character of multi-pass clustering.

Followings are the details of the algorithm.

Input: words similar matrix, minimum size of cluster: N, partitional and merging thresholds: y ,  9, maximum merging clusterj count in each L, maximum iterative times T, maximum senses thresholdMs. .

Step 1: K = J*% ;  Select K-seeds randomly.

Let these seeds as the centroids of clusters  Step 2: foreach(words i ) foreach(c1usterj)  compute sim(cml.,c,) ;I1 em, is the  centroid of clusterj  Step 3: foreach(c1usters) assign word to its closest cluster.

If (cozmtelements(cl~ster i)<M remove(chister I ) ; reclassify(words in i t ) ;  Step 4: Compute the new centroids for each clusters.

Step 5: Compute error for each cluster.

Compute distance for every two clusters  dis,, = Ikm, - eml II Step 6: foreach(cl2rsters)  if en; > y  partition the cluster i;  foreach(c1uster pii-wises)  if dis, < 7  add the painvise to merge list.

merge the first min(L,sizeof(list))  painvise clusier; Step 7: compute new cluster centroids.

if (iterative times >Tor no update to clusters) goto Step 8;  else goto Step 2; Step 8: foreach(c1usters)  remove(out1ier); labelByFeatures(c1uster);  RemoveIntersectingFeatures(word, cluster); foreach(word)  Step 9: if (maxfsenses) < Ms) Goto Step 1; else retum;  In Step 8, outliers for each cluster will be removed from the cluster by remove(out1ier). labelByFentures(j is to label cluster by its high scored features.

RemoveIntersectingFea~~s(j is to remove intersecting features from word vector. The ji lfeahtre score) indicating how much a feature contributes to the cluster. It is defined as follow:  Features with higher score are chosen for the cluster label, and then remove them from the feature matrix for next pass. After the clustering algorithm, lower frequency senses could also be found.

Follow is an example of the clustering result after one run:  (problem, case, solutions ... ...) (molecules, onygen, atoms ... ...) (large, measurements, small  ... ...) (basis, important, limited, considerable ... ...) (long, injiniteb, short, recently.. ...)  ISODATA is a solution for dynamic tuning K for clustering. But it doesn't address features scoring for clustcrs. In CBC, intersccting featurcs are also     removed from word to find multi-senses, the features for each clusters are predefined by the committee [3,6]. Contrarily, we define the features for clusters according to the clustering result, which provides an alternative method.

3.3. Refinement It is necessary to refine the result from the previous step. We compare the output of our system with the WordNet to improve the precision. WordNet is an electronic dictionary organized as a concept directed graph with nodes i.e. Synset, representing a set of synonymous words and arcs between nodes representing senses relationships such as: hypemym,  meronym, etc. The similarity between synset sI and  s, can be defined as [6]:  Where p(s )  is the probability of a randomly  chosen word referring to a synset or any synsets below it. s is the common hypemym concept of  both s, and sI  In this way, we compute similarity between words and elements in its each sense (represented by a word cluster). Similar value below a threshold 6' (usually set to 0.15) would be regarded as noise and .be removed from its sense. If word dose not exist in WordNet, keep it as a new word, e.g. LSZ.

4. EXPERIMENT  Derived from information retrieval community, precision and recall are used for the evaluation.

Adopting them to knowledge learning, we could get follo\vs:  Where:  wsA is the CN learned by this workbench  c, is the CN built manually  In our experiment, we compare output with WordNet to get precision and recall.

CNLW can deal with both English corpora and Chinese corpora. Our experiments are based on hvo English data sets: CRANFIELD documents from aeronautical system papers, Distribution 1 .O of the Reuters-21578 text categorization test collection.

As for CRANFIELD documents,,we focus on domain senses discover for words. Afier removing the stop words, the resulting test set consists of 8588 words, and the average number of feature is 670.3.

As for test collection of Reuters-21578, more common senses of words are concemed. After removing the stop words, the result consists of 12783 words with 722.6 average features.

Table 1 is the output of the dependencies discovering, table 2 is the sense mining result. Figure 3 shows the precision and recall for the senses learning.

Average Average Average  depend: confid- support  Encies  18765 6.73% 38.4% CRAN- FIELD  30849 2.4 6.41% 39.1% Reuters-  Table 1: Output of Dependency mining  Test Data count  10.8% 12.3 9.7%  Table 2: Output of senses discovey  In table 2, average similarity denotes average similarity between word and its senses.

30 35 40 45 50 55 60 65 70 75 80  Figure 2: Precision-Recall Evaluation Result  5. CONCLUSIONS  With the rapid development of WWW and computer technique, data and information have been growing so large that to deal with them manually is intractable. Knowledge learning, especially from unstructured data, becomes more important to the nex? generation of software. In this paper, a method to discover multi-dependencies and multi-senses among words is provided. With a multi-passes clustering algorithm, senses even lower frequency can be discovered.

Research and experiments show that this method can reduce the cost of building up the CN greatly.

However further work is still needed to provide a more practicable system. Based on this paper, we will improve on these aspects:  (1) Highly accurate and efficient approaches are still required to improve the performance of the learning processing.

(2) More practicable evaluation methods need to be developed.

