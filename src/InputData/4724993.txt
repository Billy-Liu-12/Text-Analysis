Extraction of Failure Graphs from Structured and Unstructured data

Abstract  Quality analysis in the automotive domain is up to now mainly focused on structured data obtained from repair vis- its, using for example association rules or decision trees on model families, model years and damage codes.

This work will outline a way to extract failure graphs from textual repair orders using taxonomy based concept recogni- tion, significant co-occurrences and graph clustering meth- ods. We will furthermore combine unstructured data with structured data and demonstrate the benefits of this method for root cause analysis in the automotive domain.

1. Introduction  The analysis of product failures can be seen as a complex procedure involving lots of different data sources and analysis methods. Main questions aris- ing while searching for root causes or while trying to achieve a deeper failure understanding are:  1. Which components are affected?

2. Which kinds of symptoms occur with which com- ponents?

3. Under which circumstances does the symptom take place?

Furthermore there is a lot of interest in which product is affected and from which production year. As it is obvious that a component may show several different symptoms, or one model may have different problems with different parts, it can be seen as a natural choice to model these relations as graphs.

Regular data analysis usually employs association rules or decision trees on structured data like damage codes or part numbers of replaced parts, connected to the data of the product itself. These methods normally  lack a more detailed symptom description, and nearly always there is no information about the conditions under which the symptoms show up.

This shortcoming can be avoided, if a textual failure description is used for data analysis and connected to the structured data. The human language can express symptoms, conditions and damage causes much more detailed and fine grained than any structured data can, although leading to a more time consuming and com- plex analysis. In this paper we will present a way to extract a set of interesting failure graphs from a collec- tion of repair order documents and their metadata. We will show how the combination of a taxonomy, signif- icant co-occurrences and graph clustering can be em- ployed to achieve a better failure understanding and a more accurate root cause analysis of unstructured and structured data.

We will evaluate the presented algorithms on repair order data from the automotive domain.

2. Related Work  As this paper describes a workflow of different modules and algorithms, related work can be found in several areas. While multilingual concept recognition is normally not adressed directly, there is some work in the field of knowledge bases like thesauri, of which WordNet ([12]) is probably the best known. There is also some work on multilingual knowledge bases like the one described in [16], but without a clear possibil- ity to disambiguate concepts in their context and map them across languages.

Graph clustering is well known for all applications which are not based on data vectors, but rather on a similarity (or distance) matrix between arbitrary ob- jects. Nice overviews can be found in [6], [3] and [2].

The clustering of word co-occurrence graphs suggests itself, and is for example described in [1] or [11], but is always based on words instead of extracted concepts   DOI 10.1109/ICMLA.2008.76     related to a taxonomy. Our proposed system is the first to our knowledge which is combining all those methods, and which is also including structured data.

3. Workflow  Our approach can be divided into several steps of a more complex workflow which is outlined below.

Input to the workflow is a set of multidimensional data points S = {x1, .., xn} with xi ? Fd, representing the structured (nominal scaled) data with their values in a d-dimensional feature space, and a set of text docu- ments T = t1, .., tn in (potentially different) languages with a bijective function f (xi) ? ti, mapping one text document to every data point.

1. The document collection T is transfered into a cleaned text collection T? by preprocessing every document ti ? T to a document t?i ? T? with im- proved textual quality (section 4).

2. By applying a taxonomy based text matcher, we can extract a set of language independent con- cepts Ci = {c1, ..., cn} out of each document t?i . With the help of the taxonomy synonyms as well as homonyms will be resolved (section 5).

3. The extracted concepts are combined with the structured data S leading to the data set S? = {x?1, .., x?n}. The feature space is extended by the concepts in the taxonomy, and is now F?.

4. The application of a significance measure (section 6) with a filtering step according to a given thresh- old ? leads to a set of significant co-occurring items which can also be written as a matrix CM, with  CMi, j = {  Sig(F?i , F ? j), if Sig(F  ? i , F ? j) > ?,  0 else (1)  5. This matrix CM can be seen as an adjacency ma- trix of a graph and can therefore be used as in- put for a graph clustering algorithm, which will create a clustering C = {C1, ...,Cn} of the feature space, grouping items which significantly occur together. These clusters contain the structured items, as well as the items of the taxonomy ob- tained from the analysis of the textual data (sec- tion 8).

6. The last step of the workflow is the visualization of the results with a graph layout algorithm.

Figure 1. General Workflow  4. Preprocessing  First step of the workflow is the text preprocessing, aiming at increased textual quality. Therefore the lan- guage is detected based on letter n-grams, using the NGramJ library which is theoretically based on ([5]).

After the language recognition several cleaning steps are performed, including context-sensitive replace- ment of abbreviations and context-sensitive spell- checking using neighbourhood word co-occurrences.

The cleaning steps are described in more detail in [13].

5. Taxonomy-based Concept Recognition  While the methods described in the following para- graphs can in principle be applied to the tokens of the document collection, we decided to base the system on concepts identified in the texts using a technical taxonomy. This leads to several advantages:  1. The concepts can be organized into top-level cate- gories (e.g. components, symptoms and conditions).

These categories can be used as constraints for filtering or visualization purposes (e.g. we can decline failure clusters which do not contain a component, or where symptoms are missing).

2. The complexity of language is reduced massively, due to the taxonomy which handles synonyms and ambiguities.

3. The multilingual taxonomy allows the analysis of texts in different languages and is therefore adding an additional layer of abstraction by hid- ing the language of the original text.

4. As the amount of concepts in the taxonomy is far smaller than the amount of words in the given language(s), the following processing steps can be accomplished much faster.

The taxonomy we use is a multilingual and polyhierar- chical knowledge base, which was derived from avail- able company sources and extended manually. The hierarchy is organized in terms of the semantics in- stead of the technical arrangement. To achieve multi- linguality we represent every word as a language in- dependent concept, and every concept may have more     than one parent (e.g. a radio fuse may be situated un- der radio system as well as under fuses). Every concept can be defined in several languages and with several synonyms per language. For example fuses is just the English word for a concept that is called Sicherungen in German and fusibles in French. The handling of ambi- guities is ensured by the optional specification of part- of-speech tags per word, and a context per synonym for each language. Therefore pump can be identified as a component if the word is used as a noun, or as an action when used as a verb.

Together with an efficient matching system, the tax- onomy is used for concept recognition as described in more detail in [14].

6. Significance Measures  Even with two items co-occurring together fre- quently, this might have no relevance - for example if both items are very frequent, they might co-occur so often just by chance. Therefore it is important to mea- sure the significance of the co-occurrence, that means to measure how probable the co-occurrence is to have happened just coincidental.

There are lots of different measures (described e.g. in [10, 9, 4]) to investigate how significant for example a symptom co-occurs with a component, of which we will use the t-Score in this work, which seemed most reasonable with respect to our application.

f (c1, c2) is the frequency stating how often the two items (from the taxonomy or the structured data) co- occur on a per document basis, f (c) is the frequency of an item in all documents, and n is the amount of doc- uments. The probability measures P can be calculated by normalizing the frequency values by the number of documents n (we will count every co-occurrence and all occurrences only once a document). The t- Score, which is originally formulated as a hypothesis test, calculates a normalized difference between the ex- pected and the observed mean of a sample, and can be transformed to calculate co-occurrence significances (see [10]):  Sig(c1, c2) = P(c1, c2) ? P(c1)P(c2)?  P(c1,c2) n  (2)  We discarded measures like the Dice coefficient or the Jaccard coefficient, because they tend to score co- occurrences down, when one of the two measured items occurs very seldom, and the other one is fre- quent ([4]). Furthermore we discarded the Mutual Information, because it tends to prefer seldom events (in contrast to the Local Mutual Information, which  behaves similar to the t-Score). These properties are inappropriate with regard to our task of quality anal- ysis. We are mainly interested in frequent events, but without ignoring seldom events (which might be an indicator for early warning).

7. Small World Property  Small world networks describe a special class of graphs, which are highly clustered, yet have a small shortest path between their nodes ([17]). These prop- erties are especially interesting for our goal of automo- tive data analysis:  1. As we are assuming to be able to find clusters of different failure topics in our data, it is evident that the co-occurrence graph should be highly clus- tered. This property will also be helpful in later workflow steps, in which the data will be clus- tered.

2. Graphs obtained from quality data covering re- pair issues have a small shortest path, because of common items which serve as strong hubs, like for example conditions (when cold) or symptoms (like rusty). As these items may co-occur significantly with lots of other items, they serve as shortcuts to other items, decreasing the size of the shortest paths.

To test our graphs for small world properties, we will use the definition proposed by [17]. In conclusion we will calculate the average shortest path L and the clustering coefficient CC of a given graph G(V,E). The average shortest path L is defined as the number of edges in the shortest path between any pair of vertices, averaged over all pairs of vertices ([17]).

For the calculation of the clustering coefficient CCi of a given vertex vi we will consider the ki vertices in its neighborhood Ni:  CCi = 2|{enm}|  ki(ki ? 1) vn, vm ? Ni, enm ? E (3)  The clustering coefficient of the whole graph G is sim- ply the average of all clustering coefficients of its ver- tices.

A given graph G is said to be small world, if its values for L are nearly as small as in a random graph Grandom but its clustering coefficient is remarkably higher, sim- ilar to a completely ordered graph Gordered (e.g. a ring lattice).

8. Graph Clustering  Graph Clustering tries to break a given graph into meaningful subgraphs, normally so that the vertices in the subgraphs are strongly connected to each other, yet having only few edges to other subgraphs (or edges with high weights within the subgraphs, and few edges with small weights to other subgraphs). Al- though some vector cluster algorithms can be applied for graphs too, there is also a set of so called graph clustering algorithms, which normally use methods and ideas from graph theory, like the minimum cut, or minimal spanning trees, to find clusters in a graph.

In order to find general problem groups in the co- occurrence graph of our data, we will apply and evaluate three partitional graph clustering algorithms.

These algorithms were chosen, because they have only few (or none) parameters, and find the amount of clus- ters automatically. We will use a stochastic transition matrix as input for all algorithms, were the edges of a node sum up to one. This is especially important to devalue hubs, which could otherwise glue the graph together.

8.1 Markov Cluster Algorithm  The Markov Cluster Algorithm (or MCL algorithm) is based on the idea of performing a random walk on the graph ([15]). A random walk which once enters a dense area in the graph, will most probably visit most of the vertices of the dense area before leaving it. This idea is realized by computing flows in the graph, which can be done by iteratively applying two operations  1. The expansion step, which computes the power of the transition matrix, thereby simulating random walks.

2. The inflation step, which sets the matrix to the Hadamard power (to strengthen strong flows, and to devalue weak flows by taking the power of each matrix entry), and then re-normalizing it back to a stochastic transition matrix.

This algorithm has especially the advantage of being deterministic, therefore always creating the same clus- tering of the graph. Unfortunately the algorithm isn?t very fast, having a runtime complexity of O(n3). Al- though the author describes possible optimizations of his algorithm ([15]), e.g. by pruning edges, we will use the base algorithm in our work.

8.2 Chinese Whispers  The Chinese Whispers algorithm is a very basic al- gorithm, which can be seen as a special version of the MCL algorithm, yet being more effective (O(|E|), which is especially fine for sparse graphs) and parameter-free ([2]).

After initially assigning each vertex in the graph a different class label, the algorithm will iteratively go through all vertices in a randomized order, and assign each vertex the predominant class label in its neigh- bourhood. The algorithm stops after a small amount of iterations, or if convergence is reached. The drawback of this algorithm is its randomized and indeterministic behavior.

8.3 Geometric MST Clustering  The Geometric MST Clustering (or GMC algorithm) is an algorithm which combines spectral partitioning with geometric clustering ([3]). Therefore it calculates the eigenvalues and eigenvectors from the normalized adjacency matrix (the stochastic transition matrix), and uses a small amount d of them to re-weight the edges of the graph. After that a minimum spanning tree is created, and the clustering is created by pruning edges of the MST. All possibilities (thresholds) for pruning edges are explored while trying to maximize a given quality measure.

9. Quality Measures  To measure the quality of a clusteringC, we will em- ploy two quality measures, which are based on graph theory. An overview of possible measures is for exam- ple given in [3].

The first quality measure which we will employ, is the performance, which counts the number of correctly interpreted node pairs. This is done by taking the sum of intra-cluster edges and non-adjacent nodes in different clusters, and normalizing it with the sum of all pairs of nodes. We reformulated this measure for weighted graphs, by using the sum of intra-cluster edge weights m(C) and by multiplying the sum of non- adjacent nodes and the sum of all pairs of nodes with the average edge weight a(G).

Performance(C) = m(C)+ a(G) ? {v,w}?E,v?Ci,w?Cj,i? j  a(G)0.5n(n? 1) (4)  The second measure is the coverage of a clustering, which is calculated by the ratio between intra-cluster     edge weights m(C) and the weights of all edges:  Coverage(C) = m(C)? ei j?E w(eij)  (5)  As coverage prefers clusterings with a small amount of clusters (the maximum is reached for one single clus- ter), we will change the formula to prefer clusterings with more clusters:  Coverage(C) = log (|C|) m(C)? ei j?E w(eij)  (6)  10. Evaluation and Experimental Results  The data used for our experiments is a dataset with approximately 10 000 repairs from our domain, including the model, the model year and the repair order text. We preprocessed the text as described above and extracted symptom descriptions, compo- nent names and conditions from the text using the taxonomy.

Afterwards we calculated the co-occurrence matrix of the taxonomy concepts and structured data (model name and model year) using the t-Score significance.

We increased the t-Score threshold from 0.1 to 2.8 to examine the small world properties for graphs of different density. Above a threshold of 2.8 the graph is not connected any more. For every threshold we removed the vertices without any edges.

0  0.5  1  1.5  2  2.5  3  A ve  ra ge  S ho  rt es  t P at  h L  t-Score Threshold  L(random) L(ordered)  L(graph)  Figure 2. The average shortest path of the co-occurrence graph, compared to a random graph and a ring lattice  Figures 2 and 3 show the average shortest path and the clustering coefficient of the calculated graphs for various t-Score thresholds. For every given thresh- old the graph fulfills the small world properties by having an average shortest path like a random graph  (the lines are nearly identical), but yet high cluster- ing coefficients. This proves, that beside the regu- lar word graphs also concept graphs fulfill the small world property, thereby making it easy to cluster them into subgraphs.

0.2  0.4  0.6  0.8   0  0.5  1  1.5  2  2.5  3  C lu  st er  in g  C oe  ffi ci  en t C  C  t-Score Threshold  CC(random) CC(ordered)  CC(graph)  Figure 3. The clustering coefficient of the co- occurrence graph, compared to a random graph and a ring lattice  The next two figures show the quality of the cluster- ing algorithms depending on the t-Score significance measure. For this evaluation we used a t-Score thresh- old above 1.5, which is reasonable with regard to our application, and which was verified manually. We stopped at a threshold of 5.0, above which the graph is to sparse. Figure 4 shows the results of the coverage quality measure, in which the GMC algorithm per- forms best. Although the Chinese Whisper algorithm is related to the MCL algorithm, it leads to worse re- sults with respect to coverage. For higher thresholds all three algorithms converge to similar coverage val- ues. This happens due to the internal clustering of the graph, which becomes more evident for higher thresh- olds.

0.5   1.5   1.5  2  2.5  3  3.5  4  4.5  5  C ov  er ag  e  t-Score Threshold  GMC-Clustering (CoverageQuality) Chinese Whispers Markov Clustering  Figure 4. Clustering results with the coverage quality measure and the t-Score significance measure     Figure 5 shows the performance results of the three clustering algorithms. While MCL performs similarly good with respect to performance, Chinese Whispers achieves better results for all t-Score thresholds. Al- though the GMC algorithm did best with respect to coverage, it performs significantly worse with regard to performance.

Regarding the results of both experiments, it can be stated that MCL shows the most stable results, al- though never achieving the best values. GMC and Chinese Whispers optimize only one of the two mea- sures, but with the shortcoming of worse values for the other quality measure. Chinese Whispers has the additional advantage of a fast runtime, but unfortu- nately creates an indeterministic clustering.

Therefore we conclude, that the MCL algorithm is a good choice for the clustering of concept graphs, com- bining good results with a deterministic outcome. Its shortcoming of a higher runtime complexity can be ne- glected, because the reduction of unstructured text to concepts of a taxonomy leads to much smaller graphs, which are easier to cluster.

0.2  0.4  0.6  0.8   1.5  2  2.5  3  3.5  4  4.5  5  P er  fo rm  an ce  t-Score Threshold  GMC-Clustering (Performance) Chinese Whispers Markov Clustering  Figure 5. Clustering results with the perfor- mance quality measure and the t-Score sig- nificance measure  10.1. Application  In this section we will show some examples from our domain. The following graphs were produced as described above, on the same dataset which was used for evaluation - a randomly chosen day of data, includ- ing repair order text, model name and model year.

The visualization is done by using the Fruchtermann- Reingold graph layout algorithm [7], provided by the Prefuse [8] library (the examples are re-arranged man- ually for better visibility). We furthermore pruned all but the 20 strongest edges per graph to not con- fuse the users with too much information. The width of the lines correlate with the significance of the co-  occurrence. Furthermore we assigned a weight to ev- ery cluster by summing the frequencies of all items in the cluster, to make it easy for a user to understand which problem groups are most important in the data.

Because of data confidentiality we altered model names and production years, picked some randomly chosen clusters and omitted the weights.

We used the Chinese Whispers algorithm for these ex- amples, with a t-Score threshold of 1.5.

Figure 6 shows a group of cigarette lighter problems (which is located in the ash tray). The cluster clearly shows, that the problems are caused by blown fuses.

Figure 7 shows a failure graph with brake problems, which are mainly connected to the models bbb and fff and the production year 200y. The last figure (8) is concerned with a set of repairs, in which the car did not start. The strong connections to the crankshaft show, that the starting problems are related with the crankshaft - in fact it turned out to be a problem with the crankshaft sensor.

The examples show that the outlined workflow can give a quick and intuitive insight into large data sets, helping the user to see and understand the most im- portant issues at one glance.

Figure 6. Example: Cigarette lighter problems  Figure 7. Example: Brake problems  10.2. Conclusions  We showed that a combination of knowledge-based concept recognition, co-occurrence significance and     Figure 8. Example: Starting problems  graph clustering can be applied to quality analysis on textual data. The empirical results proved, that con- cept co-occurrence graphs fulfill the small world prop- erties, and are therefore predestinated for clustering purposes. We evaluated three state-of-the-art graph clustering algorithms on automotive quality data, and conclude that the MCL algorithm is a good choice for concept graph clustering, combining good and stable clustering results with a deterministic outcome. Fi- nally we outlined a workflow from text preprocess- ing to visualization, which leads to high quality clus- ters, and can be used to visualize large amounts of data quickly and intuitively for a user. The usage of graphs is especially suited to visualize quality prob- lems, which consist of interrelated components, symp- toms and conditions.

10.3. Future Work  The future work will mainly try to investigate how the small world properties of the data can improve the clustering process. By pruning edges with small val- ues, even the natural clustering of the data itself may be enough to visualize the failure graphs. Further- more we will conduct further evaluations with respect to fuzzy graph clustering, allowing components to be part of several distinct failure graphs, or symptoms to occur with different components.

As the co-occurrence measures can be considered as a purely statistical way to find out how strong two concepts are connected to each other, we will also try to apply syntactic parsing on the textual data, to get a better fitting on symptoms and components, as they are normally related in a syntactic way.

