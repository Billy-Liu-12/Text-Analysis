Mining Non-Derivable Hypercliques

Abstract?Hypercliques have been successfully applied in a number of applications, e.g. clustering [1] and noise removal [2]. A hyperclique [3] is an itemset containing items that are strongly correlated with each other. Even though hypercliques have been shown to handle datasets with skewed support distribution and low support threshold well, they might still face problems for dense datasets and lower h-confidence threshold. In this paper, we propose a new pruning method based on combining hypercliques and Non-Derivable Itemsets (NDIs) [4] in order to substantially reduce the amount of generated hyperclique sets. Specifically, we propose a new collection of hypercliques, called Non-Derivable Hypercliques (NDHC), and present an efficient algorithm to mine these sets, called NDHCMiner. The proposed NDHC collection is a lossless representation of hypercliques, i.e., given the itemsets in NDHC, we can generate the complete collection of hypercliques and their support, without additional scanning of the dataset.

We experimentally compare NDHC with Hypercliques (HC), as well as another condensed representation of hypercliques, Maximal Hyperclique Patterns (MHP). Our experiments show that the NDHC collection offers substantial advantages over HC, and even MHP, especially for dense datasets and lower h-confidence threshold values.

Keywords-hyperclique; non-derivable itemset; frequent item- set mining; dense data; skewed support distribution

I. INTRODUCTION  Frequent itemset mining (FIM) has attracted substantial attention since the seminal paper by Agrawal and Srikant [5], introducing the Apriori, recognized today as one of the ten most influential data mining algorithms [6]. FIM aims to extract patterns or itemsets, sets of items that co-occur frequently in the dataset, based on a user-entered threshold of support, ?.

The frequent itemsets generated by an FIM algorithm may contain items that are weakly related to each other [3].

Furthermore, there might be many spurious combinations of items if a low ? threshold is used and the frequency distribution of items is skewed. Finally, with a high ? threshold value, strong affinity itemsets involving items with low support levels are missed.

To address these issues, a hyperclique pattern [3] was proposed as a new type of association pattern that contains items that are highly affiliated with each other. Specifically, the presence of an item in one transaction strongly implies  the presence of every other item that belongs to the same hyperclique. The h-confidence measure captures the strength of this association and is defined as the minimum confidence of all association rules containing an itemset. An itemset is a hyperclique if the h-confidence of this itemset is greater than ??, a user-specified minimum h-confidence threshold.

A hyperclique pattern is a maximal hyperclique pattern if no superset of this pattern is a hyperclique pattern [7].

Hypercliques can be generated at extremely low levels of support in dense datasets, and have been shown to be useful in a number of applications. For example, [2] used hypercliques to filter out noisy objects, in order to improve clustering performance.

Hypercliques have been shown to work well for large datasets and low support threshold, however they might still face problems for large dense datasets, e.g. census data, especially as the threshold ?? decreases. Dense datasets con- tain many strong correlations, and are typically characterized by items with high frequency and many frequent patterns [8].

For example, for the Pumsb dataset, ? = 0 and ?? = 0.7, over 3 million hypercliques are generated (see Section V).

This is a known problem for FIM algorithms, which per- form well with sparse data sets, such as market-basket data, but face problems for dense data [8]. To solve this issue, much work has been done toward obtaining Condensed representations of FIs (CFIs). The goal of these methods is to generate a smaller collection of representative sets, from which all FIs can be deduced. Many CFIs have been proposed, e.g. maximal, closed, ?-free, non-derivable (see [9] for a CFI survey).

In this paper, we propose Non-Derivable Hypercliques (NDHC), a condensed representation of Hypercliques based on Non-Derivable Itemsets (NDIs) [4], a well-known con- densed representation of FIs. NDIs have been shown to lead to large performance gains over FIs [10], and have recently been applied successfully to Outlier Detection [11].

Similar to NDIs, our proposed collection is a lossless representation of hypercliques, that is, all hypercliques and their individual support can be generated given the NDHC collection of sets and their support, without additional scans of the dataset, which leads to significant runtime advan- tages for certain datasets. We also present an algorithm,   DOI 10.1109/ICTAI.2011.80     NDHCMiner, to efficiently generate the NDHC itemsets, and discuss how to generate all hypecliques and their support from NDHC.

We compare our proposed representation, NDHC, with Hypercliques (HC), as well as with Maximal Hyperclique Patterns (MHP) [7], for several datasets and for different parameter values. Our experimental results show that the NDHC collection is overall significantly smaller than the HC collection, and grows much slower than HC as the h- confidence threshold ?? decreases. Moreover, NDHC offers significant advantages compared to HC and MHP for dense datasets such as census data and lower ?? values. Finally, we present runtime results for deriving all hypercliques and their support from NDHC.

The organization of this paper is as follows: Section II summarizes previous related work. In Section III, we describe Hypercliques (HC) and Non-Derivable Itemsets (NDI), and in Section IV we present our proposed col- lection of itemsets, Non-Derivable Hypercliques (NDHC), an efficient algorithm to mine NDHC sets, NDHCMiner, and discuss how to generate all hypercliques from NDHC.

Section V contains our experiments and results. Finally, in Section VI, we summarize our work and provide concluding remarks.



II. RELATED WORK  Hyperclique patterns were introduced in [3], and their properties and applications were discussed in [12]. The authors in [7] presented a hybrid approach for mining Maximal Hyperclique Patterns (MHP). An algorithm for mining Maximal Hyperclique Patterns in data sets containing quantitative attributes was presented in [13].

An algorithm for hierarchical clustering using hyper- cliques, Hierarchical Clustering with Pattern Preservation (HICAP), was proposed in [1]. A data cleaning method based on hypercliques, HCleaner, was proposed in [2] in order to remove noise with the goal to improve clustering or association rule performance; noise was defined as ir- relevant or weakly relevant objects to be removed before data analysis. An unsupervised clustering algorithm that se- lects constraints automatically based on Hyperclique patterns called HP-KMEANS was presented in [14].

The authors in [15] applied hypercliques to the identi- fication of functional modules in protein complexes. Hy- perclique patterns were used in [16] to detect unauthorized access to off-topic documents. Finally, the HICAP algorithm from [1] was compared with the bisecting K-means Cluster- ing with pAttern Preservation (K-CAP) algorithm in [17].



III. BACKGROUND  We consider a dataset ? which contains ? data points or transactions (rows). Let ? = {?1, ?2, . . . , ??} be a set of ? items in ?. Each transaction (row) ? in ? is a set of items such that ? ? ?. Given ? , a set of some items in  Table I EXAMPLE DATASET (? = 2, ?? = 0.3)  tid Items 1 ?, ? 2 ? 3 ?, ? 4 ?, ? 5 ?, ?, ? 6 ?, ?, ? 7 ? 8 ?, ?, ? 9 ? 10 ?, ?, ?, ?  ?, we say that ? contains ? if ? ? ? . The support of ? , supp(?), is the percentage of transactions in ? that contain ? . We say that ? is frequent if it appears at least ? times in ?, where ? is a user-defined threshold. The collection of frequent itemsets is denoted by ??:  ?? = {? ? ? ? ????(?) ? ?} A. Hypercliques (HC)  In this section, we present background information on hypercliques from [3].

The h-confidence of an itemset ? = {?1, ?2, . . . , ??} is defined as:  ?????(?) = ???{ ????(?1 ? ?2, . . . , ??), ????(?2 ? ?1, ?3, . . . , ??), . . . , ????(?? ? ?1, . . . , ???1) }  where ???? is the association rule confidence [5]. As shown in [3], the ????? above is equivalent to:  ?????(?) = ????({?1, ?2, . . . , ??)}) ???1???? { ????(??) }  Given a user-defined support threshold ? and a user- defined h-confidence threshold ??, itemset ? is a hyper- clique if it is frequent (i.e., ????(?) ? ?) and its h- confidence is at least ?? (i.e., ?????(?) ? ??). An itemset ? is a Maximal Hyperclique Pattern (MHP) if none of its supersets is a hyperclique pattern.

The collection of hypercliques is shown below:  ?? = {? ? ? ? ????(?) ? ? ? ?????(?) ? ??} H-confidence has three important properties, specifically  the anti-monotone property, the cross-support property, and the strong affinity property (see [12] for details). The anti- monotone property is used in this paper for mining hyper- cliques; it states that if itemset ? has h-confidence below the ?? threshold, so does every superset of ? .

Example 1: Given the example dataset in Table I with 10 transactions and items ?,?,?,?, let support ? equal to 2, and h-confidence threshold ?? equal to 0.3.

Itemset ? = {???} is a hyperclique itemset with ????(???) = 3 > ?, and ?????(???) = ????(???) ? max {????(?), ????(?), ????(?)} = 3/7 = 0.43 > ??.

Mining all Hypercliques (HC) can easily be implemented using an Apriori-like algorithm for mining ?? (all frequent itemsets). When ?? is 0, ?? is the same as ??. During the candidate generation phase, based on the anti-monotone property of the h-confidence measure, we can prune a candidate itemset of length ? if any of its (? ? 1)-length subsets is not a hyperclique pattern. Finally, after computing exact support and h-confidence for all candidate itemsets, we then prune candidates using the user specified support threshold ? and the h-confidence threshold ??.

B. Non-Derivable Itemsets (NDI)  In the following, we present background on the NDI representation from [10]. Calders et al. [10] showed that itemsets whose support can be deduced or derived from their subsets, i.e. derivable sets, can be pruned; this can dramatically reduce the total amount of sets generated.

Let a general itemset, ?, be a set of items and negations of items, e.g. ? = {???}. The support of ? in this case is the number of transactions where items ? and ? are present, while item ? is not present. We say that a general itemset ? = ? ? ? is based on itemset ? if ? = ? ? ? . The deduction rules in [10] are based on the inclusion-exclusion (IE) principle [18]. For example, using the IE principle we write the following for the support of another general itemset {???}: ????(???) = ????(?)?????(??)?????(??)+????(???).

Based on ????(???) ? 0, we can write the following: ????(???) ? ????(??) + ????(??)? ????(?).

The above is an upper bound on the support of set ???. [10] extended this concept to compute rules in order to derive the upper and lower bounds on the support of itemset ? based on its subsets. Let ??(?) and ??(?) be the lower and upper bounds on the support of itemset ?:  ??(?) = ???{??(?), ????? ???} ??(?) = ???{??(?), ????? ????}  where ??(?) denotes the summation shown below:  ??(?) = ?  ????? (?1)?????+1????(?).

Given a database ? and threshold ?, the NDI algorithm produces the condensed representation ??? below, which contains only the non-derivable frequent itemsets:  ??? = {? ? ? ? ????(?) ? ? ? ??(?) ?= ??(?)}  Example 2: Given the dataset and ? value in Table I.

Itemset ? = {???} is derivable, because its lower and upper support bounds are equal to 3, as shown below.

???????  ?????	 ??????????? ??????  ????? ????? ???? ????	 ????? ?????  ???? ??? ???? ????  ??? ??  ????  Figure 1. Generated itemsets for the example dataset with ? = 2, and ?? = 0.3: Non-Derivable Itemsets (NDI), Hypercliques (HC), and Non- derivable Hypercliques (NDHC) - shading denotes a frequent itemset  ????(???) ? 0 ? ????(??) = 4, ????(??) = 3, ????(??) = 3  ? ????(??) + ????(??)? ????(?) = 0 ? ????(??) + ????(??)? ????(?) = 3 ? ????(??) + ????(??)? ????(?) = 3 ? ????(??) + ????(??) + ????(??) ? ? ????(?)? ????(?)? ????(?) + + ????(?) = 5.

In order to find the frequent itemsets, the NDI algorithm uses Apriori-Gen, the candidate generation step of Apriori to generate candidate sets, and then prune infrequent candi- dates. If the lower and upper bounds are equal, then the item- set is derivable. If a set ? is NDI, i.e. ??(?) ?= ??(?), the algorithm needs to count the support of ?; if it is found that ????(?) = ??(?) or ????(?) = ??(?), all strict supersets of ? are derivable and they can be pruned. This process is repeated until candidate sets cannot be generated further.

Monotonicity: If itemset ? is derivable, then all its super- sets ? (? ? ?) are derivable.

Finally, the ??? collection is a condensed representation of ??, i.e. all sets in ?? and their individual support can be generated given the ??? collection, without scanning the dataset additional times.



IV. MINING NON-DERIVABLE HYPERCLIQUES (NDHC)  We propose to use the ideas behind Non-Derivable Item- sets (NDI) to considerably prune the Hyperclique (HC) collection. Specifically, we propose to compute bounds on the support of a hyperclique, as in the NDI context, and then prune derivable hypercliques. In the following, we define and discuss our proposed itemset collection, Non-Derivable Hypercliques (NDHC), and present algorithms to first mine NDHC sets, then to generate all hypercliques and their support given the NDHC collection.

A. Non-Derivable Hypercliques (NDHC)  Definition 1 An itemset ? is a non-derivable hyper- clique if ? is frequent (????(?) ? ?), and ? is a hyperclique (?????(?) ? ??), and ? is non-derivable (??(?) ?= ??(?)).

Given a dataset ?, support threshold ?, and h-confidence threshold ??, the collection of Non-Derivable Hypercliques ???? is defined below:  ???? =??? {? ? ? ? ????(?) ? ? ? ?????(?) ? ?? ? ??(?) ?= ??(?) }  Based on this definition and the definitions of NDIs and hypercliques, the NDHC collection preserves the monotonic- ity principle. In other words, if itemset ? is a non-derivable hyperclique, then every subset ? , ? ? ? , is also non- derivable and a hyperclique. If itemset ? is a derivable itemset with h-confidence less than ??, then all its supersets are also derivable and not hypercliques.

The NDHC collection will always be smaller than the NDI collection, and equal to NDI for ?? threshold equal to 0. Also, the NDHC collection will always be smaller than or equal to HC, as some of the hypercliques might be derivable.

Example 3: Given the dataset, ?, and ?? values in Table

I. Itemset ? = {???} is a derivable hyperclique, as shown in Examples 1 and 2. On the other hand, itemset ? = {???} is non-derivable with support bounds [2, 3], but it is not a hyperclique, with ?????(?) = 2/7 = 0.29 < ??.

Figure 1 depicts the Non-Derivable Itemsets (NDI), Hy- percliques (HC), and Non-derivable Hypercliques (NDHC) for the example dataset in Examples 1-3, with ? = 2, and ?? = 0.3.

In summary, the collections shown in Figure 1 for the example dataset are:  ?? = {?, ?, ?, ?, ??, ??, ??, ??, ??, ???, ???}, ??? = {?, ?, ?, ?, ??, ??, ??, ??, ??, ???}, ?? = {?, ?, ?, ?, ??, ??, ??, ??, ??, ???}, ???? = {?, ?, ?, ?, ??, ??, ??, ??, ??}.

B. NDHCMiner Algorithm  A na??ve method to mine Non-Derivable Hypercliques is to first mine all Non-Derivable Itemsets (???) and, in a second step, prune those with h-confidence less than ??.

However, there are some datasets for which a high number of NDIs is generated as shown in [19]. As both hypercliques and NDIs make use of the monotonicity principle, we are able to prune itemsets ? if any of their subsets ? either (a) are derivable, i.e. ??(?) = ??(?), or (b) have ?????(?) less than ??.

The algorithm for mining the NDHC sets, NDHCMiner, is shown in Algorithm 1. NDHCMiner generates candidates and computes support bounds as the NDI algorithm in [10] (steps 3-7). Instead of mining all NDIs and then calculating ?????(?) for each set ? in ??? , we calculate ?????(?)  Algorithm 1 NDHCMiner Algorithm Input: Database ?, support threshold ?, h-confidence  threshold ?? Output: ???? collection  1: Count support of single items 2: ???? ? single items with support ? ? 3: for itemset length ? ? 2..? do 4: ?? = Generate non-derivable candidates of length ?  with Apriori-Gen 5: for all candidates ? ? ?? do 6: Compute support bounds for ? , ??(?) and ??(?),  based on NDI deduction rules 7: if ??(?) ?= ??(?) and ??(?) ? ? then 8: // ? is a non-derivable itemset 9: Count ????(?)  10: Compute ?????(?) 11: ???? ? ? such that ????(?) ? ?  and ?????(?) ? ?? 12: else 13: Prune candidate ? 14: end if 15: end for 16: end for  (step 10) per Section III-A, right after we count ????(?) (step 9). Then, non-derivable itemsets with ????? less than ?? can be excluded from the NDHC collection at the same time as non-derivable itemsets that were found to be infrequent (step 11).

C. Deriving all hypercliques from NDHC  The complete set of hypercliques and the support of each hyperclique can be generated given the NDHC collection.

The process for deriving all hypercliques from NDHC is similar to generating all frequent itemsets, ??, and their support from the NDI collection presented in [10]. In our case, the itemsets that were pruned in Algorithm 1 were either infrequent, derivable, or had h-confidence less than ??.

The itemsets that we wish to generate to obtain the complete HC collection are those sets which are not contained in NDHC but are contained in HC, which are the derivable hypercliques.

Given non-derivable hyperclique ? in the NDHC collec- tion, we generate a superset itemset ? by adding an item ? to itemset ?: ? = {? ? {?}}. Then, we calculate support bounds, ??(?) and ??(?), using the same deduction rules as in generating NDIs. As in [10], we prune itemset ? if ??(?) < ?, i.e. ? is infrequent.

On the other hand, if ? is frequent, it is either a derivable itemset, or a non-derivable itemset that is not a hyperclique, otherwise it would have been included in the NDHC collec- tion. If it is derivable, i.e. ????(?) = ??(?) = ??(?), we compute its h-confidence ?????(?), and prune ? if     Table II DATASET DETAILS: NUMBER OF TRANSACTIONS (ROWS), AVERAGE  TRANSACTION LENGTH (COLUMNS), SINGLE DISTINCT ITEMS  Dataset Transactions Avg. Transaction Items length  Mushroom 8124 23 119 Pumsb 49046 74 2113 Pumsb? 49046 51.48 2089 Connect 67557 43 129 T40I10D100K 100000 40.61 942  ?????(?) < ??. The itemsets ? that are not pruned are the derivable hypercliques, and their support ????(?) is by definition equal to the support bounds. Finally, the complete HC collection is the union of NDHC and the derivable hypercliques.



V. EXPERIMENTS  A. Experimental Setup  We conducted all experiments on a Pentium 2.61 GHz processor with 2 GB RAM. We used the NDI code available online1, and implemented the rest in C++. For the compari- son with Hypercliques, we use the NDI code and set ?? to 0.

We note that our implementation does not include the cross- support pruning shown in [3]. For the Maximal Hyperclique Patterns (MHP), we used the MHP code provided from the authors of [16].

Our experiments were conducted using four real datasets (Mushroom, Pumsb, Pumsb*, and Connect), and one artifi- cially generated dataset (T40I10D100K). All datasets were obtained from the FIMI repository2 (see Table II for dataset details).

The Mushroom dataset contains characteristics from dif- ferent species of mushrooms. The Pumsb dataset is based on census data and is a benchmark set for evaluating the performance of FIM algorithms on dense data sets. One of its characteristics is the skewed support distribution, e.g. 81.5% of the items in this set have support less than 1%, while 0.95% of them have support greater than 90%. Pumsb? is the same dataset as Pumsb, except all items of 80% support or more have been removed, making it less dense. Finally, the Connect dataset contains different game positions.

B. Results  We ran several experiments with various values for sup- port threshold, ?, and the h-confidence threshold, ??.

1) Comparison with Hypercliques (HC): Table III con- tains the number of generated sets for the Hyperclique col- lection (HC) and the Non-Derivable Hyperclique collection (NDHC) for all datasets and various ? and ?? combinations.

Table IV contains runtime in seconds for the mining of  1http://www.adrem.ua.ac.be/goethals/software 2http://fimi.ua.ac.be/data/  Table III COMPARISON OF GENERATED SETS - HYPERCLIQUES (HC) VS.

NON-DERIVABLE HYPERCLIQUES (NDHC)  (a) T40I10D100K ? = 0.0025 ? = 0.005 ? = 0.01  ?? HC NDHC HC NDHC HC NDHC 0.30 1084 1084 1008 1008 860 860 0.20 2186 2144 2019 1978 2254 2212 0.15 6551 6183 6140 5773 6659 6291 0.10 55186 34353 52784 32048 17254 14524  (b) Mushroom ? = 0 ? = 0.1 ? = 0.2  ?? HC NDHC HC NDHC HC NDHC 0.7 197 160 103 83 88 68 0.5 485 327 375 236 335 208 0.3 5506 1316 5264 1122 4610 818 0.2 59274 3228 58302 2663 53624 1143 0.1 619693 9781 574473 4347 53664 1143  (c) Pumsb ? = 0 ? = 0.2 ? = 0.5  ?? HC NDHC HC NDHC HC NDHC 0.95 2617 2399 553 337 413 244 0.90 6500 3112 4430 1044 3709 910 0.80 179905 6821 177819 4737 174753 4484 0.70 - 11948 - 9831 - 9378 0.50 - 62340 - 60029 - 47764  (d) Pumsb?  ? = 0.02 ? = 0.04 ? = 0.06 ?? HC NDHC HC NDHC HC NDHC 0.90 1021 377 927 283 902 258 0.70 17339 878 17238 777 17213 752 0.60 36530 1806 36530 1704 36525 1675 0.50 436825 4820 436825 4718 436820 4687 0.30 - 42251 - 42129 - 42086  (e) Connect ? = 0.2 ? = 0.5 ? = 0.8  ?? HC NDHC HC NDHC HC NDHC 0.95 2579 197 2558 176 2548 166 0.90 28839 274 28818 253 28808 243 0.80 553386 452 553386 431 533884 348 0.70 - 689 - 668 - 348 0.20 - 7574 - 1397 - 348  hypercliques versus the NDHCMiner. A ??? symbol indi- cates that we had to stop execution of the corresponding algorithm due to the very large number of itemsets generated and extremely high execution time of the corresponding algorithm.

As this table shows, the NDHC collection is much smaller than the HC collection, especially for lower ?? values. The collections are close in number of sets for the artificial dataset, T40I10D100K, except for the lowest ?? value of 0.1 (see Table III(a)). Differences in number of sets are larger for the real datasets, as ?? decreases. For example, for the Mushroom dataset (see Tables III(b) and IV(a)), ? = 0, and ?? = 0.1, there are more than 600 thousands hypercliques generated in about 4 minutes versus less than 10 thousand non-derivable hypercliques generated in 2 seconds.

Table IV RUNTIME COMPARISON (IN SECONDS) - HYPERCLIQUE (HC) MINING  VS. NON-DERIVABLE HYPERCLIQUES (NDHC)  (a) Mushroom ? = 0 ? = 0.1 ? = 0.2  ?? HC NDHC HC NDHC HC NDHC 0.7 1 1 1 1 1 1 0.5 1 1 1 1 1 1 0.3 3 1 3 1 3 1 0.2 20 1 20 1 19 1 0.1 260 2 244 2 19 1  (b) Pumsb ? = 0 ? = 0.2 ? = 0.5  ?? HC NDHC HC NDHC HC NDHC 0.95 37 29 28 20 23 15 0.90 67 32 56 24 46 19 0.80 2775 50 2591 40 2653 33 0.70 - 70 - 60 - 53 0.50 - 362 - 329 - 212  (c) Connect ? = 0.2 ? = 0.5 ? = 0.8  ?? HC NDHC HC NDHC HC NDHC 0.95 39 8 37 6 34 5 0.90 324 9 323 8 318 5 0.80 - 10 - 8 3375 5 0.70 - 11 - 9 - 5 0.20 - 22 - 10 - 5  The gains become more pronounced for dense datasets and for relatively high ?? values. For example, for the Pumsb dataset (see Table III(c) and Table IV(b)), ? = 0 and ?? = 0.8, HC contains almost 180 thousand sets generated in 46 minutes versus less than 7 thousand NDHC sets generated in 50 seconds. As ?? becomes 0.7, there are more than 3 million hypercliques, versus less than 12 thousand non- derivable hypercliques generated in 71 seconds. We make similar observations for the Pumsb? and Connect datasets (see Table III(c)-(d)). For example, for the Connect dataset and ? = ?? = 0.8, HC mining takes 56 minutes versus 5 seconds for the NDHCMiner (see Table IV(c)). The NDHC collection grows in size faster (i.e. for higher ?? values) for the dense datasets, Pumsb and Connect (Table III(c) and (e)), than the other two real datasets.

Figures 2 and 3 contain a pictorial representation of generated sets in NDHC versus HC for the Mushroom and Pumsb dataset respectively, as ?? decreases for two ? values.

As these figures clearly show, the HC collection grows much faster than the NDHC collection as ?? decreases.

2) Comparison with Maximal Hyperclique Patterns (MHP) for dense datasets: Figures 4 and 5 depict the number of Non-Derivable Hypercliques (NDHC) versus the number of Maximal Hyperclique Patterns (MHP) for two dense datasets (Pumsb and Connect) as the h-confidence threshold ?? decreases, while the support threshold ? is 0.

As can be seen in these figures, the MHP collection increases in size much faster than NDHC for these dense  0.10.20.30.50.7     h c  G en  er at  ed S  et s  NDHC 0 NDHC 0.2 HC 0 HC 0.2  Figure 2. Non-Derivable Hypercliques (NDHC) versus Hypercliques (HC) for the Mushroom dataset as ?? decreases (? = 0 or 0.2)  0.500.700.800.900.95    G  en er  at ed  S et  s  h c  NDHC 0 NDHC 0.2 HC 0 HC 0.2  Figure 3. Non-Derivable Hypercliques (NDHC) versus Hypercliques (HC) for the Pumsb dataset as ?? decreases (? = 0 or 0.2)  datasets. For example, for the Pumsb dataset (see Figure 4), ? = 0, and ?? = 0.5, there are more than 290 thousand MHPs versus a little over 60 thousand NDHC.

The collections grow slower for the Connect dataset, but the difference between NDHC and MHP becomes larger as ?? decreases (see Figure 5): for this dataset, ? = 0 and ?? = 0.1, there are a little over 36 thousand NDHC and more than 240 thousand MHP.

It is also important to note that it is possible to derive all hypercliques and their individual support from the NDHC collection, while only hypercliques, not their support, can be generated from the MHP collection.

3) Deriving all Hypercliques and their support from the NDHC collection: Figures 6 and 7 show a runtime compar- ison between mining all hypercliques (HC) versus mining     0.50.7 0.8 0.9     G en  er at  ed S  et s  h c  NDHC MHP  Figure 4. Non-Derivable Hypercliques (NDHC) versus Maximal Hyper- cliques (MHP) for the Pumsb dataset (? = 0)  0.10.20.30.50.70.9     h c  G en  er at  ed S  et s  NDHC MHP  Figure 5. Non-Derivable Hypercliques (NDHC) versus Maximal Hyper- cliques (MHP) for the Connect dataset (? = 0)  the NDHC collection and then deriving the total HC col- lection (NDHC-DeriveAllHC), including hypercliques and their supports, for the Pumsb dataset (Figure 6) and for the Connect dataset (Figure 7), with ? = 0. The time to mine non-derivable hypercliques is included in the times shown in these figures.

In Figure 6, both algorithms that produce all hypercliques have similar runtime for ?? values higher than 0.85. As ?? decreases to 0.8, the runtime difference between the two algorithms becomes larger: 46 minutes for HC mining versus 1 minute for deriving all hypercliques from NDHC (NDHC- DeriveAllHC). We had to stop execution of the HC mining algorithm for ?? = 0.7, while the NDHC-DeriveAllHC algorithm finishes in under 95 minutes. It is noteworthy that the NDHCMiner algorithm finishes in 71 seconds for the  same dataset and parameter values.

As noted in the previous section, there are over 3 million  hypercliques versus under 12 thousand non-derivable hyper- cliques for the Pumsb dataset, ? = 0, and ?? = 0.7 (see Table II). The large gains in runtime is because we avoid additional scans of the dataset that would be needed to count the support of over 3 million derivable hypercliques; instead, we calculate the support of the derivable itemsets from their subsets.

Similar observations can be made for the Connect dataset in Figure 7, where the only runtime shown for HC is 324 seconds with ?? = 0.9. For example, for ?? = 0.75, NDHCMiner finishes in 12 seconds, NDHC-DeriveAllHC generates 1636534 hypercliques and their support in un- der 16 minutes, while execution of HC mining had to be terminated. For ?? = 0.7, execution of the NDHC- DeriveAllHC algorithm had to be terminated as well -more than 4 million hypercliques were generated before execution was terminated versus a total of 759 NDHC sets.



VI. CONCLUSION  Hypercliques (HC) [3] have been shown to find strong affinity itemsets at low support levels, and have been suc- cessful in applications such as clustering [1]. In this paper, we propose a new method to substantially reduce the size of the hyperclique collection, based on the Non-Derivable Itemsets (NDIs) [4], a condensed representation of Frequent Itemsets.

Our proposed representation of HC, called Non-Derivable Hypercliques (NDHC), is a lossless representation of HC, that is, all hypercliques and their individual support counts can be generated given the NDHC collection, without ad- ditional scans of the dataset. We present an algorithm to efficiently mine all NDHC sets, NDHCMiner, and discuss how to generate all hypercliques and their support from the NDHC sets.

Our experiments show that the NDHC collection presents significant advantages compared to HC, especially for dense datasets and lower h-confidence threshold values. Moreover, for dense datasets such as census data, the NDHC collection grows much slower than another hyperclique representation, Maximal Hyperclique Patterns (MHP), as the h-confidence threshold ?? decreases.

Future directions include utilizing non-derivable hyper- cliques in applications such as clustering, and further reduc- ing the size of the hyperclique collection.

