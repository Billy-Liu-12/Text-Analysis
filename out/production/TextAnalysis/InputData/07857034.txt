0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?While MapReduce is inherently designed for batch and high throughput processing workloads, there is an increasing demand for non-batch processes on big data, e.g., interactive jobs, real-time queries, and stream computations. Emerging Apache Spark fills in this gap, which can run on an established Hadoop cluster and take advantages of existing HDFS. As a result, the deployment model of Spark-on-YARN is widely applied by many industry leaders. However, we identify three key challenges to deploy Spark on YARN, inflexible reservation-based resource management, inter-task dependency blind scheduling, and the locality interference between Spark and MapReduce applications. The three challenges cause inefficient resource utilization and significant performance deterioration. We propose and develop a cross-platform resource scheduling middleware, iKayak, which aims to improve the resource utilization and application performance in multi-tenant Spark-on-YARN clusters. iKayak relies on three key mechanisms: reservation-aware executor placement to avoid long waiting for resource reservation, dependency-aware resource adjustment to exploit under-utilized resource occupied by reduce tasks, and cross-platform locality-aware task assignment to coordinate locality competition between Spark and MapReduce applications. We implement iKayak in YARN. Experimental results on a testbed show that iKayak can achieve 50% performance improvement for Spark applications and 19% performance improvement for MapReduce applications, compared to two popular Spark-on-YARN deployment models, i.e., YARN-client model and YARN-cluster model.

Index Terms?Spark-on-YARN, Resource Scheduling, Cross-platform, Application Performance, Reservation-aware Executor Place- ment, Dependency-aware Resource Adjustment, Locality-aware Task Assignment  F  1 INTRODUCTION In the past few years, MapReduce has revolutionized big data parallel and distributed processing. MapReduce has proven to be an effective platform to implement complex batch applications as diverse as sifting through system logs, running extract transform load operations, and computing web indexes. However, its one-pass com- putation model makes MapReduce a poor fit for low- latency applications and iterative computations, such as machine learning and graph algorithms. Recently, emerging Apache Spark [1] addresses such limitations by generalizing the MapReduce computation. Spark en- ables applications to reliably store the data in memory.

Each Spark application has multiple processes, called executors, running on the cluster to load related data in memory on its behalf even when it is not running any job. It allows applications to avoid costly disk accesses, which is the key to the high performance of Spark.

Thus, many of today?s big data deployments go be-  ? D. Cheng is with the Department of Computer Science, University of North Carolina at Charlotte, NC, 28223.

E-mail: dazhao.cheng@uncc.edu.

? X. Zhou are with the Department of Computer Science, University of Colorado, Colorado Springs, CO, 80918.

E-mail: xzhou@uccs.edu.

? P. Lama is with the Department of Computer Science, University of Texas at San Antonio, 1 UTSA Circle, San Antonio, TX 78249.

Email: palden.lama@utsa.edu.

? J. Wu and C. Jiang are with the Department of Computer Science & Technology, Tongji University, 1239 Siping Road, Shanghai, China.

E-mail: {wujun,cjjiang}@tongji.edu.cn.

? X. Zhou is the corresponding author.

yond MapReduce by integrating Spark for interactive and streaming computations. However, dividing a uni- fied resource pool into smaller pools for different appli- cations would lead to inefficient utilization of resources.

A single cluster manager with dynamic resource allo- cation may lead to better resource utilization. In par- ticular, enterprises prefer to deploy the emerging Spark applications on their existing Hadoop clusters in order to leverage the established cluster, access to existing HDFS dataset, and take advantage of Hadoop?s security environment. For example, eBay and Yahoo! [2] employ MapReduce to generate reports and answer historical queries, while deploying Spark at the same time to cal- culate key metrics in real-time. Hadoop YARN [3] is an emerging cross-platform cluster manager. It allows mul- tiple computing platforms to co-exist and share resource on a single cluster and benefit from its fine-grained resource management scheme. However, we find that there exists a semantic gap between the reservation- based resource scheduling policy [4] of YARN and the dynamic need of Spark applications, which causes inef- ficient resource utilization and poor application perfor- mance. Specifically, Spark-on-YARN raises several key challenges as follows.

First, the reservation-based resource scheduling policy of YARN makes tasks with high resource demand very hard to obtain the required resource in time. Unfortu- nately, Spark is such kind of applications. Spark is based on the multi-thread programming model. This charac- teristic could lead to a scenario that a single executor of Spark occupies a large amount of resource at one time.

Thus, an executor with high resource demand may have    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   to wait a long time for the resource reservation, leading to low resource utilization and poor performance. Even worse, starvation could happen for some jobs with very large resource demand (e.g., Spark streaming) when the required resource is to be reserved but cannot be satisfied in a long period of time.

Second, existing schedulers in YARN do not recognize the impact of dependency between map and reduce tasks. As the cluster resource is reserved and shared with Spark applications, MapReduce jobs that have already launched reduce tasks may not be able to have all their map tasks completed in time. Because the reduce tasks cannot execute their functions until all map tasks are completed, the launched reduce tasks will keep occupy- ing the resource and waiting for the completion of the map tasks. Thus, it incurs low utilization of the resource that is allocated to the reduce tasks.

Third, both MapReduce and Spark applications try to place tasks or executors alongside their related HDFS blocks for locality awareness, while they need to ne- gotiate with YARN for resource scheduling. In partic- ular, jobs could exhibit poor data locality on the nodes that co-host Spark executors and MapReduce tasks. For example, when a count task is computed by a Spark application, the same count dataset could be needed by a MapReduce job. However, the MapReduce job may not be able to obtain the resource on the node with local dataset if the node is mostly occupied by the Spark executor. Thus, the locality interference due to the multi- tenant competition hurts performance of both Spark and MapReduce applications.

In this work, we propose and develop a cross-platform resource scheduling middleware, iKayak, that aims to improve the resource utilization and application perfor- mance in multi-tenant Spark-on-YARN clusters. iKayak relies on three key designs that leverage time-varying resource demands of different applications, inter-task dependency between map and reduce tasks, and cross- platform locality awareness to tackle the aforementioned challenges, respectively. We first design and develop a reservation-aware executor placement mechanism to select efficient hosting nodes for Spark executors to achieve shorter reservation time. It aims to satisfy the high resource demands of executors in a timely man- ner. We then design and develop a dependency-aware resource adjustment mechanism to adaptively control the resource underutilized by reduce tasks. It allows map tasks to preempt resource from the reduce tasks.

We further design and develop a cross-platform task assignment mechanism to coordinate the locality aware- ness between MapReduce task assignment and Spark executor placement. It aims to increase local data ac- cess opportunities on the nodes that co-host Spark and MapReduce applications.

We implement iKayak on a 16-node Hadoop YARN cluster and evaluate its benefits using the Pur- due MapReduce Benchmark Suite (PUMA) and ?Big- DataBench? benchmark with datasets collected from real  Fig. 1. [Spark+MapReduce]-on-YARN.

applications. We compare the performance of iKayak with two popular Spark-on-YARN deployment models: YARN-client model and YARN-cluster model. Exper- imental results by running different workloads show that iKayak reduces the job completion time of Spark workloads by 50% and 30% compared to the two popular models, respectively. At the same time, it also reduces job completion time of MapReduce workloads by 14% and 19% while it increases the CPU utilization by 22% and 15% compared with the two models, respectively.

In the rest of paper, Section 2 gives case studies and motivations of Spark-on-YARN. Section 3 describes the design and development of iKayak. Section 4 gives implementation details. Section 5 presents the experi- mental results. Section 6 reviews related work. Section 7 concludes the paper.

2 MOTIVATIONS 2.1 Background  Reservation-based Scheduling in YARN. Compared to the first generation Hadoop, YARN adopts an fine- grained resource management, which means applica- tions can configure their required resources like CPU and memory for individual tasks when submit jobs.

If there are not sufficient resources available in the cluster as required, the ResourceManager will start to reserve resources on the selected node for the task. When other tasks on this node are completed, more available resources can be assigned to the upcoming task. Until the reservation is satisfied, the released resources on this node will not allow to be allocated to other applica- tions. During the reservation process, the new released resources are accumulated but not allowed to use, which provides a good resource isolation capability but causes ineffective resource utilization.

In-Memory Computing with Spark. Spark is an up- and-coming big-data analytics solution developed by using highly efficient in-memory computing. It allows applications to explicitly cache a dataset in memory so that applications can access data from memory instead of disk, which can dramatically improve the perfor- mance. Compared to MapReduce, Spark utilizes mul- tiple threads instead of multiple processes to achieve parallelism on a single node, avoiding the memory over- head of several JVMs but leading individual executors    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   Fig. 2. Ineffective resource utilization.

occupying a large amount of resources at one time. The resource demand (i.e., the number of executors and the resource request of each executor) of a specific Spark application should be configured by users when the job is submitted to the cluster. Spark can run in two popular modes: Spark-on-Mesos and Spark-on-YARN. Spark-on- Mesos [5] adopts the ?all-or-nothing? resource manage- ment policy (e.g., allocating enough resource at one time or denying the request), which could cause starvations especially when the cluster is busy. However, YARN will reserve the resource instead of denying application to avoid such starvation.

Spark-on-YARN. In Spark-on-YARN deployment mode, Spark applications are similar to MapReduce ?jobs?. As shown in Figure 1, MapReduce runs each task in its own process. When a task completes, the process goes away. In Spark, many tasks can run concurrently in a single process (i.e., executor), and this process sticks around for the lifetime of the Spark application, even when no jobs are running. Thus, MapReduce does not suffer from the reservation-based scheduling policy since the reservation periods of most map/reduce tasks are very short due to their small resource demands. How- ever, Spark applications with huge resource demands of individual containers can suffer from too long resource reservation period under the current resource scheduling policy of YARN.

2.2 Case Study and Challenges  Although Spark-on-YARN provides good integration with YARN?s cluster-wide resource management poli- cies, it incurs significant challenges due to the different characteristics of the two programming modes. To illus- trate the inefficiency caused by the hybrid deployment, we conduct a case study on a (Spark+MapReduce)-on- YARN cluster composed of six machines. We configured each slave node with 8 cores and 12 GB Memory, and the block size is set to 256MB in the experiment. In the exper- iment, a representative MapReduce application from the PUMA benchmark [6], i.e., WordCount, is executed with 30 GB input data. Another representative Spark applica- tion, i.e., Logistic Regression [7], is executed with 10 GB input data from Wikipedia. The resource requirements of map and reduce tasks are configured to 2 cores and  Fig. 3. Underutilized reduce tasks.

2GB memory. The resource requirement of individual executors is configured to 6 cores and 8GB memory. We measured the different task completion times for various applications and observed the resource utilizations on the hybrid platform as follows.

Ineffective Resource Utilization. Figure 2 shows the details of the resource scheduling diagram on a slave node during a short time window. When a Spark execu- tor request is submitted at 20th second, ResourceManager finds there is no enough available resource on the node.

Then ResourceManager starts to reserve resources for the submitted Spark application and reject to assign other MapReduce tasks to the selected node. Figure 2 shows there exists a large amount of idle resource after Map 1 and Map 2 are completed. This is because the resource (i.e., 4 cores) released by Map 1 and Map 2 is less than the demand of the executor (i.e., 6 cores). The executor eventually starts to be executed on the node until Reduce 2 completed, when the resource accumulation on the node satisfies the demand of the Spark executor. During this resource scheduling process, we find the reservation- based resource allocation policy is apparently ineffective and causes low resource utilization, leading poor per- formance of Spark applications as well. Such ineffective resource scheduling could be even worse if there are straggler tasks appeared on the selected nodes which typically occupy the resource for a long time period.

Underutilized Reduce Tasks. In the second case, we first run the MapReduce application solely and then run the MapReduce and Spark applications together on the cluster. Figure 3 compares the tasks execution differences of MapReduce application under the two different sce- narios. It shows that the mix deployment significantly increases the number of map waves for the MapReduce application (i.e., from 5 waves to 10 waves) and delays the reduce tasks (i.e., from 120s to 300s). This is due to the fact that half of the cluster resource is allocated for the Spark application compared to the sole MapReduce deployment. In this case, we find the time of resource occupied by the reduce tasks is almost three times of the time spent in the sole MapReduce deployment while they complete the same task. Apparently, the resource reserved for reduce tasks in the hybrid environment is over-reserved than its need, leading significant under- utilization of reduce tasks. Such phenomenon could be    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   even worse when the reduce tasks are configured to complete in multiple waves.

Locality-missing Assignment. We further observed the locality awareness of the task assignment in both MapReduce and Spark executions. We find that the map tasks with data locality achieve 95% of the total completed map tasks when the MapReduce application runs solely in the dedicated cluster. However, the local map task rate is reduced to 68% when the MapReduce application is co-hosted with the Spark application. This is due to the fact that Spark executors typically occupy a large amount of resource on individual nodes, which sig- nificantly obstruct the locality-aware map task accesses for MapReduce application. For example, there are two nodes (i.e., M1 and M2) in the cluster and each node has 4 cores. Spark executor requires 3 cores and each map task requires 1 core. The data storage on M1 and M2 node is different with each other and then map tasks can be divided into two types, i.e., M1-local tasks and M2-local tasks. When the Spark executor occupies a big container (i.e., 3 cores) on the node M1, there is only 1 core for MapReduce task assignment. For map tasks, there are more free resources (i.e., 4 cores) on M2 node than that (i.e., 1 core) on M1 node. As MapReduce adopts a ?pull-based? task assignment policy, some M1- local tasks are scheduled to M2 node. In this case, M1- local tasks that are assigned on the M2 node miss the locality awareness due to the competition from the Spark application.

HDFS usually has three data replicas in the cluster, which may improve the data locality. For example, there may exist another node having the same data file with M1 node, which provides the substitutes with locality for M1-local tasks. However, as the cluster size and the application quantity expand, such multi-replicas are still too limited and hard to avoid such locality-missing task assignment. In particular, both Spark and MapReduce apply Delay Scheduling [8] to schedule tasks based on the locality preference, i.e. they try to schedule tasks onto nodes with local data. So such locality awareness competition between Spark and MapReduce will make they both suffer from lack of locality awareness even there are multiple replicas in the cluster.

2.3 Opportunities  The new feature (i.e., reservation-based resource scheduling) of YARN fits well to deal with the small size MapReduce tasks due to its fine-grained resource management capability. However, running Spark on YARN incurs significant performance deterioration (e.g., inefficient resource utilization, dependency- blind resource allocation and locality-missing task assignment) due to its special characteristic, i.e., huge resource demand of individual executors. While Spark- on-YARN poses the above challenges, it also opens up new opportunities. Intuitively, we find three strategies to take the aforementioned three challenges respectively.

Fig. 4. Overview of iKayak.

? Spark executors should be placed on the suitable nodes to satisfy their resource reservations in time, and avoid too long time waiting and low resource utilization.

? The resources allocated to reduce tasks should be re-balanced based on their time-varying demands to avoid the resource wasting when any Spark applications is submitted into the cluster.

? The task assignment of both Spark and MapRe- duce applications should be optimized to reduce the locality-aware competition while considering the locality awareness of both Spark and MapReduce applications.

All these motivate us to develop a holistic resource scheduling approach to improve the cluster utilization and the application performance in Spark-on-YARN. In next section, we focus on exploiting these opportunities to optimize the resource scheduling for Spark-on-YARN.

3 IKAYAK DESIGN iKayak is a cross-platform resource scheduling middle- ware that aims to optimize the resource management in Spark-on-YARN clusters. It takes advantage of the different resource demand characteristics of Spark and MapReduce applications, and dynamically optimizes re- source scheduling while considering the data locality for both applications. Figure 4 shows the architecture of iKayak. It has three main components: reservation-aware executor placement, dependency-aware resource adjust- ment, and locality-aware task assignment coordination.

We briefly describe their major features.

? Reservation-aware executor placement adaptively places Spark executors on efficient hosting nodes that can satisfy high resource demand of individual executors in a timely manner.

? Dependency-aware resource adjustment dynami- cally exploits the resource allocated to reduce tasks to mitigate low resource utilization due to resource over provisioning, especially when reduce tasks are idle and waiting for the intermediate data from map tasks.

? Locality-aware task assignment coordination im- proves the data locality awareness of both Spark and MapReduce applications so that they can share the limited local data access opportunities on the multi- tenant cluster nodes.

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   3.1 Reservation-aware Executor Placement  When a Spark job is submitted to the cluster, the place- ment of executors is crucial to the application perfor- mance and cluster utilization. We propose and develop a reservation-aware executor placement mechanism to mitigate the inefficient resource utilization issue due to high resource demand of Spark. The key insight is that Spark executors should be placed on nodes that either have enough available resource or host small map tasks.

Note that reduce tasks usually occupy resource for much longer time than map tasks. If executors have to wait for the released resource from reduce tasks, they should be placed on the nodes that host reduce tasks approaching completion.

Spark provides two default executor placement sched- ulers, i.e., SpreadOut and Non-SpreadOut [9], to place executors on multiple nodes in the cluster. Both of them have no capability to place executors in a resource reservation aware manner. When running Spark on YARN, each Spark executor runs in a YARN container.

The Spark ApplicationMaster is responsible to negotiate resource requests with YARN and find a set of effi- cient hosts to place and run the executors. However, YARN does not have the capability to recognize effi- cient worker nodes for Spark applications because it was originally designed for MapReduce computations.

MapReduce schedules a container and fires up a JVM for each task, while Spark hosts multiple tasks within the same container for multi-threading. Correspondingly, the resource demand of an individual container for Spark is much more than that of an individual task for MapReduce. So the default ?pull? based task scheduling approach for MapReduce jobs does not fit well for the executor placement of Spark applications.

3.1.1 Resource Reservation Analysis Resource reserved for Spark executors may come from three sources on a selected node n, i.e., available free resource (Rnfree), running map tasks (R  n map), and running  reduce tasks (Rnreduce). We define an efficient worker node for hosting a Spark application as the node that can timely satisfy the resource demand of individual executors of Spark. We analyze the characteristics of the three sources as follows.

? The free resource on the selected node can be used for executors immediately after reservation, which is the most efficient source for executor resource reservation.

? The resource from running map tasks can be used for executors in a very short time, i.e., several sec- onds or a couple of minutes. This is due to the fact that map tasks are light weight and generally small. They will release the occupied resource after completion.

? The resource from running reduce tasks can be used for executors in an uncertain time, which is dependent on the reduce task execution progress.

Algorithm 1 Reservation-aware Executor Placement.

1: repeat 2: if Any Spark executor requests resource: Rexecutor  then 3: Evaluate Rnfree, Rnmap, Rnreduce, n ? N 4: if Rnfree ? Rexecutor then 5: best = argmax  n [Rnfree]  6: Select the node with maximum free resource 7: end if 8: if Rnfree < Rexecutor ? [Rnfree +Rnmap] then 9: best = argmax  n [Rnfree +R  n map]  10: Select the node with maximum map tasks 11: end if 12: if [Rnfree +R  n map] < Rexecutor then  13: Select the node with minimum waiting time 14: end if 15: Place executor on the selected node 16: end if 17: until Satisfy the demand of Spark  We develop an effective executor placement algorithm based on the resource reservation awareness of each possible destination node in the cluster. The algorithm aims to make the executor run as soon as possible once it is placed on the selected worker node. As shown in Algorithm 1, it first selects the efficient worker node with enough free resource to satisfy the demand of individual executors. If there is not enough free resource on the node, iKayak selects the efficient worker node that hosts enough running map tasks to place executors. If there is still not enough resource, iKayak finally selects a node that hosts reduce tasks approaching completion.

The first two scenarios are straight forward so that we focus on analysis of the third scenario. When most MapReduce workloads in the cluster are reduce-heavy workloads, Spark executors may have to wait the re- source released by reduce tasks to satisfy their demands.

In this scenario, iKayak aims to find the destination nodes with the reduce tasks approaching completion so that the Spark application does not have to wait for long time. Spark executor is not simply assigned to the first resource slot freed by reduce tasks since it is no guarantee that other reduce tasks on this machine can complete in a short time. In order to to find which reduce tasks are approaching completion, it is necessary to predict the execution progress and remaining time of reduce tasks.

3.1.2 Estimating Reduce Remaining Time Although there are many existing prediction approaches for MapReduce execution, we develop a self-adaptive fuzzy model to predict a reduce task?s remaining execu- tion time based on its input size and resource allocation.

The fuzzy model is often used to capture the complex re- lationship between resource allocations and a task?s fine- grained execution progress. However, a task?s progress can be affected by many factors. First, reduce task progress is not uniform at different execution phases,    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   e.g., shuffle, sort and reduce phases. Second, even within the same phase, data skew among tasks leads to differ- ent task execution speed at different intervals. Finally, co-running tasks may unpredictably interfere with a task?s execution, making the mapping of resource to task progress variable. Many existing prediction approaches do not consider the multi-tenant interferences from other co-existing applications. Therefore, we design an online self-adaptive fuzzy model based on real-time measure- ments.

Fuzzy Model. The reduce task remaining execution time (i.e., y(t)) in the control interval t is represented as the input-output NARX type (Nonlinear Auto Regres- sive model with eXogenous inputs),  y(t) = F (u(t), d, ?(t)). (1)  F is the relationship between the input variables and the output variable. The input variables are the current resource allocation u(t), the job input size d, and the regression vector ?(t). Here, u(t) represents the resource allocation for the reduce task. The regression vector ?(t) contains a number of lagged outputs and inputs of the previous control periods. It is represented as  ?(t) = [(y(t? 1), y(t? 2), ? ? ? , y(t? ny)), (u(t), u(t? 1), ? ? ? , u(t? nu))]T  (2)  where ny and nu are the number of lagged values for outputs and inputs, respectively. Let ? denote the number of elements in the regression vector ?(t), that is,  ? = ny + nu. (3)  F is the rule-based fuzzy model that consists of Takagi- Sugeno rules [10]. A rule Fj is represented as  Fj : IF ?1(t) is ?j,1, ?2(t) is ?j,2, ? ? ? , and ??(t) is ?j,? u(t) is ?j,?+1 and dj is ?j,?+2  THEN yj(t) = ?j?(t) + ?ju(t) + ?jdj + ?j .

(4)  Here, ?j is the antecedent fuzzy set of the jth rule, which is composed of a series of subsets: ?j,1,?j,2, ? ? ? ,?j,?+2.

?j , ?j and ?j are parameters, and ?j is the offset. Their values are obtained by offline training. Each fuzzy rule characterizes the nonlinear relationship between allo- cated resources and performance for a specific reduce task type.

Online Self-Learning. Due to the dynamics of MapRe- duce job behaviors (e.g., data skews, different phases and multi-tenant interferences), we design an online self- learning module to adapt the fuzzy model. It aims to minimize the prediction error of the fuzzy model e(t), which is the error between actual measured job progress and predicted value.

If e(t) 6= 0, we apply a recursive least squares (RLS) method to adapt the parameters of the current fuzzy rule. The technique updates the model parameters as new measurements are sampled from the runtime sys-  Algorithm 2 Executor Placement with Reduce Tasks.

1: if [Rnfree +R  n map] < Rexecutor then  2: repeat 3: Evaluate ynm, n ? N,m ?M by Eq.(1) 4: Sort remaining times: ynm, n ? N,m ?M 5: /*yn1 ,? . . . ,? ynx ,? . . . ,? ynM*/ 6: if  ?m=x m=1 u  n m ? [Rexecutor ? (Rnfree +Rnmap)] then  7: Waitn = argmax m?x  [ynm]  8: end if 9: until Obtain Waitn, n ? N  10: end if 11: best = argmin  n?N [Waitn]  12: Place executor on the selected node  tem. It applies exponentially decaying weights on the sampled data so that higher weights are assigned to more recent observations.

We express the fuzzy model output in Eq.(1) as follow:  y(t) = ?(t)X + e(t) (5)  where e(t) is the error between the actual output and predicted output. ?(t) = [?T1 , ?T2 , .., ?T? ] is a vector composed of the model parameters. X = [?1X(t), ?2X(t), .., ??X(t)] where ?j is the normalized degree of fulfillment or firing strength of jth rule and X(t) = [?(t)T , u(t)]. The parameter vector ?(t) is esti- mated so that the error function in Eq.(6) is minimized.

We apply both the current error e(t) and the previous error e(t? 1) to estimate the parameter vector,  Error = e(t)2 + ?e(t? 1)2. (6)  Here ? is called the discount factor as it gives higher weights on more recent samples in the optimization. It determines in what manner the current prediction error and old errors affect the update of parameter estimation.

3.1.3 Executor Placement with Reduce Tasks Based on the estimated remaining time of reduce tasks on different nodes, we use Algorithm 2 to select the hosting node for executor placement. As shown in Al- gorithm 2, we assume that reduce tasks m, (m ? M ), are running on nodes n, (n ? N ). We first evaluate the remaining execution times (i.e., ynm) of reduce tasks on each node based on Eq.(1). We then sort the remain- ing execution times of reduce tasks within each node, and calculate the minimum accumulated resource (i.e.,?m=x m=1 u  n m) from reduce tasks on the nth node to satisfy  the executor?s demand. We obtain the corresponding es- timated waiting time (Waitn) to accumulate the required resources from the nth node. Finally, the best candidate to place the executor is the node with the minimum waiting time.

3.2 Dependency-aware Resource Adjustment We identify the dependency in Spark-on-YARN at two levels, i.e., application level and task level. At the ap- plication level, there exists apparent dependency, i.e.,    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers       100 75 50 25 10     Jo b  ex ec  ut io  n sl  ow d  ow n  T hr  ou gh  pu t (  /m in  )  MapReduce proportion (%)  Job slow down Map tasks  Fig. 5. Impact of workload proportion.

resource competition between Spark and MapReduce applications in the multi-tenant cluster. We measured various MapReduce job completion times and through- puts while giving different input data proportions be- tween the Spark and MapReduce applications. Three MapReduce applications from the PUMA benchmark [6], i.e., WordCount, Terasort and Grep, with various input data sizes, were run on the cluster. Another three repre- sentative Spark applications from the BigDataBench [7] benchmark, i.e., K-means, PageRank and Index, with the input data sets from Wikipedia, were run on the cluster too. We quantified the slowdown of MapReduce job execution by comparing various job completion times to the job completion time achieved in the dedicated cluster with sufficient resources (i.e., 100%). Figure 5 shows that MapReduce execution slows down as its workload proportion decreases in the cluster. It also shows that the throughput of completed map tasks is significantly lower as MapReduce workload decreases in the cluster. As a result, at the task level, the delayed map task executions further affect the resource demands of the reduce tasks due to the dependency between the map and reduce tasks. We develop a dependency-aware resource adjustment mechanism to dynamically control the resource allocated to reduce tasks.

We want to exploit the resource allocated to reduce tasks after a MapReduce job starts execution, when the cluster resource allocated to the MapReduce workload changes because of co-hosting Spark applications in the cluster. We define a delay factor ? = RSparkRSpark+RMR to represent the delay of map task execution caused by the dynamic resource allocation in the multi-tenant cluster environment. Here, RSpark and RMR represent the resource allocations for Spark and MapReduce ap- plications, respectively. The delay factor is event driven by application submissions.

We use JVM suspend function to suspend the execu- tion of reduce tasks when their allocated resource is not fully utilized due to the delay of their map task execution. The length of suspension is dependent on the delay factor ?. Let T represent the control interval. The suspension length is calculated as Tsuspend = ?v ? T , where v is weight parameter. A larger ? means there is more resource occupied by Spark applications than that occupied by MapReduce applications so that the suspen-  sion time of reduce tasks should be longer. During the suspension period, reduce task executions are suspended and their CPU resource is released. Memory resource cannot be released immediately because the related data of the process is still kept in memory to avoid data loss when the reduce task is resumed. Note that the related data of the suspended reduce JVM can be swapped out by the operating system with overhead [11]. Thus, we use Tsuspend = [?v ? ?]? T to determine the suspension length, where ? is a threshold value representing the suspension overhead.

When reduce tasks of a job are suspended, iKayak utilizes the released resource to assign map tasks of the same job to the node for improving resource utilization.

Firstly, more intermediate data produced by the map tasks can be used to accelerate reduce tasks, since reduce tasks need the intermediate data for execution. Secondly, many map tasks are CPU intensive so that the resource released by suspended reduce tasks can be utilized by map tasks more efficiently.

3.3 Locality-aware Assignment Coordination Both Spark and MapReduce applications prefer to assign executors and tasks together with their data blocks to achieve local data access. Spark applications are greedy of the data locality from two aspects. Firstly, executors stick around for the lifetime of the application, even when no jobs are running. Secondly, unlike MapReduce tasks, individual Spark executors typically occupy a large amount of resource on the selected worker node.

Correspondingly, there are often locality interference when co-hosting Spark executors and MapReduce tasks.

The performance of Spark degrades significantly if there is not enough memory to store the data of a job [1].

Thus, the resource configuration of Spark executors is more inflexible compared to that of MapReduce tasks.

As map tasks are more sensitive of the data local- ity than reduce tasks, iKayak focuses on the locality coordination of map tasks for MapReduce workloads.

When the local data access opportunity is limited for map tasks, ResourceManager has to make a choice between allowing remote data access and allocating less resource than its demand but with local data access.

For example, there are 2 cpu cores and 2GB memory available on the M1 node. The resource demand of individual MapReduce tasks is configured as 2 cpu cores and 2GB memory. There are two M1 local map tasks have to be assigned at the moment. There are two potential solutions for task assignment. Solution a is to assign only one local map task on the node M1 while allowing another task to be assigned on the node M2 without data locality. Solution b is to shrink the resource demands of both map tasks to (1 cpu core, 1GB memory) and then they both can be assigned on M1 node with local data access. Correspondingly, the containers for the two map tasks have to be proportionally shrank when they are initialized. However, shrinking the resource allocation for tasks definitely deteriorates performance.

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers    1.5   R+100% L+100% L+80% L+60% L+40%  M ap  c om  pl et  io n  tim e  Data locality and resource configuration  Normalized time  Fig. 6. Data locality and resource configuration  We explore the performance impact of data locality and resource configuration for individual map tasks. Fig- ure 6 compares the map task completion time achieved with remote data access and that achieved with local data access and different resource configurations. In this case study, map tasks are configured with 2 cores and 2 GB memory resource demand (presented as < 2core, 2G >). ?R+100%? means map tasks rely on remote data access and have 100% resource, i.e., < 2core, 2G >.

?L+100%? means map tasks rely on local data access and have 100% resource, i.e., < 2core, 2G >. It shows that map tasks that have local data access with 100% resource configuration have the shortest completion time. It also demonstrates that map tasks (i.e., L+80%) with less resource configuration than its demand but with local data access have better performance than map tasks (i.e., R+100%) that have full amount of needed resource but have to do remote data access.

To coordinate the locality awareness, we develop a per-task configuration approach [12] for iKayak that al- locates different amount of resource to map tasks instead of maintaining the identical resource configuration of containers for all map tasks. As shown in Algorithm 3, iKayak periodically checks the resource status of each node by heartbeat connections. If there is any free re- source available in a node, ResourceManager assigns map tasks with data locality to the node. Suppose there are Prequested M-local map task assignments requested by ApplicationManager. The node M has Rfree free resource and each task requests Rdemandmap resource. At the beginning, iKayak accepts all map tasks. If there is not enough resource to satisfy all of the map tasks at the moment, iKayak gradually reduces the number of accepted tasks and shrinks the resource configuration of individual tasks. Eventually iKayak accepts Paccepted M-local map tasks on the node M with the shrunk resource configuration Rshrinkmap . To avoid performance deterioration, we set a shrinking ratio ? to prevent iKayak from shrinking the resource configuration of individual tasks too much. The shrinking ratio ? of map tasks is defined as ? = NlocalNlocal+Nremote , where Nlocal and Nremote represent the number of local and remote map task access respectively.

Algorithm 3 Map task shrinking algorithm.

1: repeat 2: if Any free resource is available on node Mn then 3: Paccepted = Prequested /*accepts all requested  tasks*/ 4: if Rnfree < p?Rdemandmap then 5: repeat 6: /*Shrink resource allocation of map tasks*/ 7: Rshrinkmap =  Rnfree Paccepted  8: Reduce the number of accepted map tasks.

9: until Rshrinkmap ? ??Rdemandmap  10: end if 11: Accept Paccepted M-local map tasks.

12: Assign tasks with new configuration: Rshrinkmap .

13: end if 14: until All running jobs completed  4 IMPLEMENTATION 4.1 YARN Modification We implemented iKayak by modifying classes ResourceManager, ApplicationManager and LaunchTaskAction based on Hadoop version 2.6.0.

We added a new interface exePlaceto implement the reservation-aware executor placement algorithm.

When an application submits its register request to ApplicationMaster of Spark, it will call the method exePlace to allocate the resource for Executors. The method exePlace is responsible for allocating the available resource to applications, which is event-driven by any application submissions or cluster resource changes.

Additionally, we added another new interface taskRes, which is used to specify the resource configuration of individual tasks while assigning them to slave nodes.

Each executor placement and task resource allocation is tagged with its corresponding AttemptTaskID. During job execution, we created a method taskAnalyzer to collect the status of completed tasks by using TaskCounter and TaskReport. The resource utilizations and the execution times of tasks are reported by TaskTrackers via heartbeat connection periodically.

4.2 Experiment Setup We evaluate iKayak on a cluster composed of 3 T110 (8- core CPUs and 16 GB RAM), 2 T420 (24-core CPUs and 32 GB RAM), 1 T320 (12-core CPUs and 24 GB RAM), 2 T620 (24-core CPUs and 32 GB RAM), and 8 Dell desktops (8-core CPUs and 16 GB RAM). Each machine has 1 TB hard disk. The master node is hosted on one Dell desktop in the cluster. The servers are connected with Gigabit Ethernet. The block size of HDFS is set to 256 MB and the number of replicas of HDFS is set to 3. iKayak is based on the version 2.6.0 of Hadoop implementation and 1.4.0 of Spark implementation. We implement iKayak on the Master node of the cluster.

The suspension control interval (T ) is set to 5 minutes, which is a trade-off between the map task and the reduce    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers    0.3  0.6  0.9  1.2  1.5  K-means PageRank Index  N o  rm a  liz e  d j o  b c  o m  p le  ti o  n t  im e  Workload types  YARN-client YARN-cluster  iKayak  (a) Spark completion time.

0.3  0.6  0.9  1.2  1.5  Wordcount Grep Terasort  N or  m al  iz ed  jo b  co m  pl et  io n  tim e  Workload types  YARN-client YARN-cluster  iKayak  (b) MapReduce completion time.

Desktop T110 T420 T620 T320  C P  U u  ti liz  a ti o n (  % )  YARN-client YARN-cluster  iKayak Dynamic R.A.

(c) Various CPU utilizations.

Fig. 7. Effectiveness of iKayak for Spark, MapReduce and the cluster resource utilization.

task completion time. The suspension parameter v and the overhead (?) are empirically determined to be 1 and 0.15 in the experiment.

TABLE 1 MapReduce Parameter configuration.

Parameter Wordcount Grep Terasort io.sort.factor 10 10 10  io.sort.mb 100mb 200mb 150mb io.sort.record.percent 0.35 0.25 0.15 io.sort.spill.percent 0.8 0.6 0.75  io.file.buffer.size 4k 16k 32k mapred.child.java.opts 200 300 250  4.3 Real-world Workloads  To understand the effectiveness of iKayak in a pro- duction environment, we used a synthetic workload, ?MicroSoft-Derived (MSD)?, which models the produc- tion workload of 174,000 jobs in Microsoft datacenter in a single month in 2011 [13] as MapReduce applica- tions. MSD mimics the distribution characteristics of the production jobs by running Wordcount, Terasort and Grep applications from the PUMA benchmark [6] with various input data sizes. It is a scaled-down version of the workload studied in [13] since our cluster is significantly smaller. We scale down the workload in two ways: we reduce the overall number of jobs to 87, and eliminate the largest 10% of jobs and the smallest 20% of jobs. We  TABLE 2 Resource configurations of MapReduce.

Task Wordcount Grep Terasort  Map 1core, 1GB 1core, 2GB 1core, 2GB Reduce 1core, 1GB 1core, 2GB 1core, 2GB  TABLE 3 Resource configurations of Spark.

K-means PageRank Index  4core, 4GB 2core, 8GB 4core, 6GB  used a set of representative Spark applications from the ?BigDataBench [7]? benchmark, i.e., K-means, PageRank and Index, with the input data sets from Wikipedia. We mixed MapReduce workload and Spark workload at a ratio of 1:1 in the experiments, which is based on the workload analysis from industry [14], [2].

As shown in Table 1, we set the Hadoop configurations according to the rules recommended by Cloudera [15].

We used the same configurations for evaluating various approaches in the experiment. In YARN, there is no ?slot? which is the building block in the old versions, and the system no longer distinguishes map and reduce tasks when allocating resources. Instead, each task spec- ifies a resource request in the form of < 1core, 2GB > (i.e., requesting 1 cpu core and 2GB memory), and it will be assigned to a node with sufficient capacity. We con- figured the resource demands of MapReduce workloads (as shown in Table 2) and Spark workloads (as shown in Table 3) based the previous studies [16], [1] and our experimental experiences.

5 EXPERIMENTAL EVALUATION 5.1 Effectiveness of iKayak We compare the performance of iKayak with two rep- resentative deployment models for running Spark on YARN clusters [17]: YARN-cluster model and YARN- client model.

Reducing Spark job completion time. Figure 7(a) shows iKayak significantly improves the job completion times of K-means, PageRank and Index workloads about 50% and 30% compared with YARN-client model and YARN-cluster model, respectively. Spark jobs perform better than MapReduce jobs due to two factors. Firstly, as Spark is in-memory computing application, Spark jobs are more sensitive of the data locality than MapReduce jobs. The performance of Spark degrades significantly if there is not enough memory to store the data of a job.

So our work focuses on shrinking the resource configu- rations of map tasks for MapReduce workloads while maintaining sufficient resource for Spark applications.

Secondly, it benefits from the capability of iKayak that can adaptively search efficient worker nodes to place executors based on the reservation time awareness. The results also reveal that iKayak is more effective for CPU    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers    0.3  0.6  0.9  1.2  1.5  K-means PageRank Index  N o rm  a liz  e d j o b c  o m  p le  ti o n t im  e  Workload types  Non-SpreadOut SpreadOut  iKayak  (a) Reduce job completion time.

K-means PageRank Index  R es  er va  tio n  tim e  (m in  )  Workload types  Nonsparedout Sparedout  iKayak  (b) Reduce reservation time.

K-means PageRank Index  C P  U u  til iz  at io  n (%  )  Workload types  Nonsparedout Sparedout  iKayak  (c) Increase CPU utilization.

Fig. 8. Effectiveness of the reservation-aware executor placement for Spark applications.

intensive workload (i.e., K-means and Index) than I/O intensive workload (i.e., PageRank). This is due to the fact that the memory configurations of the machines in the cluster are relatively limited. The reservation time of I/O intensive workload is longer than that of CPU intensive workload.

Reducing MapReduce job completion time. Fig- ure 7(b) shows iKayak improves the overall job comple- tion time of MSD workload by 14% and 19% compared with YARN-client mode and YARN-cluster model, re- spectively. This is due to the fact that iKayak is more flex- ible about the resource management and task scheduling of MapReduce applications in a task dependence and locality aware manner. On the other hand, Figure 7(b) shows that YARN-client model achieves better perfor- mance than YARN-cluster model does. This is because that YARN-client model hosts only one Spark applica- tion at one time. It allocates more cluster resources to MapReduce applications than YARN-cluster model does.

Increasing cluster resource utilization. Figure 7(c) shows the CPU utilizations of the various type machines that resulted from different cluster resource scheduling approaches. It demonstrates iKayak improves the over- all CPU utilization of all type machines in the cluster by 22% and 15% compared with YARN-client model and YARN-cluster model, respectively. Note that iKayak achieves a lower utilization improvement on T420 and T620 machines compared to others in the cluster. This is due to its relatively lower memory resource availability and powerful CPU. Thus, it is hard to fully utilize the CPU especially for memory intensive computing.

We further compare the CPU utilization under the dy- namic resource allocation (Dynamic R.A.) policy of Spark and the proposed iKayak approach. The coarse-grained dynamic resource allocation policy improves the CPU utilization by 16% and 9% compared with YARN-client model and YARN-cluster model, respectively. The results demonstrate that the fine-grained resource allocation of iKayak provides higher resource utilization than the dynamic allocation policy does.

5.2 Benefit of Executor Placement Figure 8 compares the job completion times and CPU uti- lizations achieved by different executor placement strate-  gies (i.e., SpreadOut, Nonspreadout and iKayak) while running the three Spark workloads on the cluster. The result demonstrates the proposed reservation-aware ex- ecutor placement approach significantly reduces the job completion times and reservation times of the Spark jobs while increasing the CPU utilization of the hosting machines.

Reducing job completion times. Figure 8(a) shows iKayak significantly reduces the job completion times of K-means, PageRank and Index workloads up to 55% and 37% compared with NonspreadOut policy and Spread- Out policy, respectively. It can automatically select the destination machines to host Spark executors, which aims to avoid the unnecessary waiting time for resource reservation. In particular, the reservation-aware executor placement mechanism effectively reduces the opportu- nity of placing executors on the worker nodes that host long running reduce tasks. The result also shows that NonspreadOut approach achieves better performance than SpreadOut policy in terms of job completion time.

This is due to the fact that NonspreadOut encourages executors to be placed on a subset of machines in the cluster so that it can reduce the intermediate data traffic to improve performance.

Reducing reservation times. Figure 8(b) shows the reservation-aware executor placement mechanism of iKayak significantly reduces the reservation times of K- means, PageRank and Index workloads up to 67% and 53% compared with NonspreadOut policy and SpreadOut policy, respectively. It also illustrates that the reservation time of the executors with fewer resource demands (i.e., K-means) is apparently less than that of the executors with more resource demands (i.e., PageRank and Index).

This is due to the fact that small resource demand can be satisfied in time by the current reservation based resource management scheme in YARN. The result also reveals that SpreadOut approach achieves better perfor- mance than NonspreadOut policy in terms of reservation time. The reason is that the centralized deployment of NonspreadOut causes more executors to be co-hosted with reduce tasks than SpreadOut does, leading long time to wait for resource reservation.

Increasing CPU utilization. Figure 8(c) shows the ex- ecutor placement mechanism slightly increases the CPU    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers       j1 j2 j3 j4 j5 j6 j7 j8 j9 j10 j11 j12  R e  m a  in in  g t  im e  ( m  in )  Task ID  Record time Prediction error Predicted time  (a) Prediction error.

10 20 30  P e  rc e  n t  o f  d e  la y (  % )  RRTE NRMSE (%)  Job Execution Delay  (b) Impact on job execution.

Fig. 9. Accuracy of task remaining time estimation.

utilizations (i.e., 10%) for the three different workloads since most of Spark workloads are memory intensive application and have limited impact on CPU resource.

On the other hand, SpreadOut incus higher CPU uti- lization than NonspreadOut does. This is due to the fact that SpreadOut policy tries to spread out all executors on the whole cluster. It leads executors to be located on most nodes in the cluster, which results in higher CPU utilization than NonspreadOut.

5.3 Accuracy of Remaining Time Estimation iKayak applies fuzzy models to estimate the reduce task remaining times while selecting efficient hosting machines for Spark executors. To evaluate the accuracy of the fuzzy models, we compare the error between the predicted reduce task remaining times and the actual re- maining times. The accuracy is measured by the normal- ized root mean square error (NRMSE), a standard metric for deviation. Three MapReduce applications from the PUMA benchmark [6], i.e., Terasort, Grep and WordCount, each with 300 GB input data, were run on the four different machines (i.e., T420, T320, T620 and T110) in the cluster. Accordingly, we recorded the actual remaining times and predicted remaining times of 12 reduce tasks as shown in Figure 9(a). It shows that the prediction was quite accurate, with on average 7.8% NRMSE. Figure 9(b) shows that as the remaining time prediction error is increased from 10% to 35%, the Spark job execution times increased from 7.5% to 34% compared to the execution times achieved with iKayak?s real prediction accuracy.

This is due to the fact that inaccurate remaining time estimation may incur additional Spark executor waiting time for resource reservation. This observation confirms that the reservation-aware executor placement, and the estimation accuracy of reduce task remaining time are critical to reducing Spark job execution times, and the executor placement waiting times.

5.4 Benefit of Resource Adjustment Figure 10 depicts the throughput of MapReduce tasks achieved by iKayak and stock YARN in the experiment.

We turn off other features of iKayak except the re- source adjustment mechanism. The result demonstrates that the resource adjustment mechanism of iKayak ef- fectively increases the throughput of map tasks and      Wordcount Grep Terasort  C om  pl et  ed ta  sk s  (/ ho  ur )  Workloads on Desktop  iKayak Stock YARN  (a) Throughout of Map tasks.

0.5   1.5  Wordcount Grep TerasortN or  m al  iz ed  c om  pl et  io n  tim e  Workloads on Desktop  iKayak Stock YARN  (b) Reduce completion time.

Fig. 10. Effectiveness of reduce adjustment.

slightly reduces the completion times of reduce tasks.

Figure 10(a) shows that the throughput of map tasks on the Desktop machine is significantly increased about 17% compared to stock YARN since the dependence-aware resource adjustment mechanism gives more resource to map executions. The resource adjustment mechanism periodically suspends the reduce tasks when there are not enough intermediate data to process and releases the redundant resource to the related map tasks.

The reduce suspending provides more cluster resource for the corresponding map tasks. It further produces more intermediate data for their reduce tasks. Thus, suspending reduce tasks indeed accelerates these reduce task executions in this case. Figure 10(b) demonstrates that the suspending policy has reduced the reduce com- pletion time 8% than stock YARN does on the Desktop machine. This is due to the fact that the resource allo- cated to reduce tasks are over provisioning and would be idle if there are not enough intermediate data to feed.

5.5 Benefit of Locality Coordination Figure 11 demonstrates that the proposed locality-aware task assignment coordination module effectively im- proves the local data access rate of MapReduce tasks and accordingly reduces job completion times. Figure 11(a) compares the local data access rates of map tasks achieved by the locality-aware coordination of iKayak and YARN-cluster model in the experiment. The result demonstrates that iKayak increases the local data ac- cess by 27% compared to YARN-cluster model for the workload Wordcount, Grep and Terasort. This is due to the fact that the coordination mechanism enforces map tasks to be assigned on the hosting nodes with local data access when MapReduce competes the locality awareness with Spark applications. Thus, the coordina- tion mechanism allows more local map executions than the YARN-cluster deployment.

As described before, shrinking the resource allocations for map tasks to achieve local data access deteriorates individual map task performance. Figure 11(b) shows the average map task completion time increases slightly by 9% when apply the locality-aware coordinator. How- ever, the performance deterioration of individual tasks contributes more opportunities to run map tasks in par- allel (i.e., increasing map ?slots?). When shrinking some tasks to prefer data locality, there are more resources    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers       Wordcount Grep Terasort  Lo ca  l a cc  es s  ra te  ( %  )  Workloads  iKayak YARN-cluster  (a) Local access rate.

Wordcount Grep Terasort  A ve  ra ge  c om  pl et  io n  tim e  (s )  Workloads  iKayak YARN-cluster  (b) Map task completion time.

0.5   1.5  Wordcount Grep TerasortN or  m al  iz ed  jo b  co m  pl et  io n  tim e  Workloads on Desktop  iKayak YARN-cluster  (c) MapReduce job completion time.

Fig. 11. Impact of cross-platform Locality-aware Assignment Coordination (LAC).

0.9   1.1  1.2  0.5 1 1.5 2  N o  rm a  liz e  d J  C T  Suspension parameter (v)  Normalized JCT  (a) Impact of v.

0.9   1.1  1.2  0  0.05  0.1  0.15  0.2  0.25  0.3  N o  rm a  liz e  d J  C T  Suspension overhead (?)  Normalized JCT  (b) Impact of ?.

Fig. 12. Sensitivity of suspension parameter tuning.

on other slave machines in the cluster will be freed.

Those available resources can be used to run more other tasks in parallel than without task shrinking. Increasing parallelism of map execution does not only compensate the performance losses of the shrunk map tasks, but also significantly improve job level performance. The whole job execution progress will be speeded up correspond- ingly. Figure 11(c) demonstrates the job completion times of Wordcount, Grep and Terasort are effectively reduced when apply the locality-aware task assignment coordi- nation mechanism.

5.6 Impact of parameters and configurations  We change the values of the suspension weight param- eter v and the overhead ? to study their impact on the performance improvement in terms of job completion time. Figure 12(a) shows that the job completion time ini- tially decreases as the suspension parameter v increases.

However, increasing the v further leads to performance degradation. This tells that a very large suspension pa- rameter v may lead to job completion time deterioration.

A large overhead ? (e.g., 2) leads significant performance deterioration due to the instability of suspension control.

Thus, we empirically set the suspension parameter v to 1 in the experiment. Figure 12(b) shows that tuning the suspension overhead ? has the similar phenomenon with tuning the parameter v. We empirically set the overhead ? to 0.15 without affecting system stability. It is a tradeoff between the reduce task suspension time cost and the job completion time.

We further explore the effectiveness of the proposed iKayak under the various configurations (i.e. applying      0 0.5 1  J o  b c  o m  p le  ti o  n t  im e  ( m  in )  early start percentage  Wordcount-iKayak Wordcount-YARN  Terasort-iKayak Terasort-YARN  (a) Impact of early start.

N-N N-D D-N D-D  L o  c a  l a  c c e  s s r  a te  ( %  )  Delay scheduling configuration  Spark MapReduce  (b) Impact of delay scheduling.

Fig. 13. Impact of early start and delay scheduling.

early start and delay scheduling) in the experiment. We firstly compared the performance improvements under the different early start percentages. Figure 13(a) shows the job completion times changed as we altered the values of the early start percentage under the differ- ent approaches, i.e., iKayak and original YARN. The result demonstrates that iKayak outperforms the original YARN about 7% in terms of job completion time when tuning the early stage percentage. It shows that IO in- tensive workload (e.g., terasort) prefers small early start percentages since the intermediate data from map tasks can be directly shuffled to save the bandwidth. Non- IO intensive workload (e.g., wordcount) prefers a large early start percentage because it prevents reduce tasks to occupy the reduce slots without execution. We then compared the local data access rates while applying dif- ferent scheduling policies (i.e., Delay (D) and Non-delay (N)) on MapReduce and Spark, respectively. The results in Figure 13(b) shows that the delay scheduling increases the local access rate 12%-15% if only one application (i.e., Spark or MapReduce) adopts it. However, when both Spark and MapReduce apply the delay scheduling, the local access rates are similar with the scenario that both of them do not apply the delay scheduling.

5.7 iKayak Overhead and Scalability  The overhead of iKayak mainly comes from the time required to perform the executor placement algorithm, the time required to suspend and resume reduce tasks, and the time required to reconfigure individual map tasks. Experimental results show that the overhead of the executor placement algorithm is between 120 ms to 150    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2669964, IEEE Transactions on Computers   ms, which is very small compared to Spark job execution time. We also measure the suspension overhead in each control interval. iKayak took on average of 2.7 seconds to suspend and resume a reduce task. The overhead is relatively negligible compared to the 5-minute control interval. Finally, the overhead of the task-level reconfig- uration is quite stable (i.e., 45-55 ms) and independent of the testbed size. iKayak is scalable and applicable to larger scale clusters. The revisions on YARN components do not affect other computing systems.

6 RELATED WORK Big-data resource management: Modern big-data clus- ters apply a diverse mix of resource managers, e.g., YARN, Corona, Omega and Mesos. These different com- puting frameworks have inherently different scheduling needs. YARN [3], the second generation of Hadoop, added a resource management layer in Hadoop. It al- lows different applications to be allocated with different number of task containers. Corona [18] is developed by Facebook and provides more flexibilities to manage the cluster resources based on the different resource demands of workloads. Omega [19], a new parallel scheduler architecture built around shared state, using lock-free optimistic concurrency control, to achieve both implementation extensibility and performance scalabil- ity. Mesos [5] abstracts CPU, memory, storage, and other compute resources away from machines, enabling fault- tolerant and elastic distributed systems to easily be built and run. Among these Big-data resource managers, YARN is the only one that supports security and lever- ages the existing HDFS dataset at the same time.

MapReduce optimization: There are growing interests on MapReduce performance optimization with various techniques, e.g., resource provisioning [20], job schedul- ing [21] and self-tuning configuration [12]. Rao et al.

proposed a new MapReduce framework, Sailfish [22], to improve performance by aggregating intermediate data. Jinda et al. [23] proposed a new data layout, coined Trojan Layout, that internally organizes data blocks into attribute groups in order to improve data access times. Dittrich et al. proposed Hadoop++ [24], a new index and join technique to improve runtime of MapReduce jobs. They are able to schedule incoming MapReduce jobs to data block replicas with the most suitable Trojan Layout. Recently, a few studies start to explore that how to optimize Hadoop configuration to improve MapReduce performance. Herodotou et al.

proposed Starfish [25], an optimization framework that hierarchically optimizes from MapReduce jobs to work flows by searching for good parameter configurations.

None of these approaches consider to optimize MapRe- duce performance by dynamic job scheduling in the Spark-on-YARN environment.

Task scheduling: Many prior studies have shown that MapReduce performance can be significantly improved by various scheduling techniques [21]. The default FIFO  Scheduler in Hadoop implementation may not work well since a long job can exclusively take the comput- ing resource on the cluster, and cause large delays for other jobs. This is the reason that many schedulers, e.g., Capacity Scheduler, Fair Scheduler, can share resources among multiple jobs. Recently, a few studies [20] start to optimize the performance of MapReduce jobs with respect to their performance goals. Wolf et al. described FLEX [21], a flexible and intelligent allocation scheme for MapReduce workloads. It is proposed as an add-on module that worked synergistically with Fair Scheduler to provide performance guarantees. Curino et al. de- signed a reservation-based scheduling [20], that provides support for prioritized MapReduce scheduling of the jobs with various deadlines. Our work differs from these efforts in that we consider running Spark and MapRe- duce applications together on YARN clusters.

7 CONCLUSIONS AND FUTURE WORK We observed that running Spark and MapReduce on YARN clusters incurs significant performance deteriora- tion due to the semantic gap between the reservation- based resource allocation scheme of YARN and the dynamic application demands. Therefore, we design and develop a cross-platform resource scheduling mid- dleware, iKayak, that aims to improve the cluster re- source utilization as well as application performance for Spark-on-YARN deployment. It relies on three key mechanisms, i.e., reservation-aware executor placement, dependency-aware resource adjustment and locality- aware assignment coordination. iKayak leverages time- varying resource demands of different applications, inter-task dependency between map and reduce tasks, and cross-platform locality awareness to tackle the afore- mentioned challenges. Experimental results show that iKayak achieves up to 50% performance improvement for Spark applications and 19% performance improve- ment for MapReduce applications compared to the two popular Spark on YARN deploy models, i.e., YARN- client model and YARN-cluster model. In future work, we will explore more cross-platform resource scheduling approaches while deploying other processing paradigms (e.g., Shark, Pig, Storm and Hive) on Hadoop YARN.

ACKNOWLEDGEMENT This research was supported in part by U.S. NSF research grant CNS-1422119.

