Association Rules Mining and Theirs Principal Analysis Component Based on  Probability and Statistics Estimate Model

Abstract   Currently those algorithms to mine association rules only pay attention to one aspect of efficiency or accuracy or correlativity respectively, even they ignore mining principal factors among all the correlativity. Thus, there seems a paradox among efficiency, accuracy and correlativity. In order to resolve to this conflict, a novel algorithm based on Probability estimate and principal component analysis is proposed to mine the association rules from database with the high correlativity and the high confidence. Probability estimate reduce the times of database scanning so as to increase efficiency and accuracy, and principal component analysis helps us to know which factors have most influence to event rate so as to distinguish correlativity. Experimental results have demonstrated that our algorithms are efficient accurate and correlativity.

1. Introduction   Association rule mining is an important embranchment of the artificial intelligence. Data mining has developed fast recently, it is used in the medical treatment, exploration, building and so on. In the future, it will be widely used in the military field, for example, it can be used to examine the failure of the military equipment, warfare simulator, the assessment of battlefield situation, target identification, weapons distribution, command decisions, and the judgment of military threat and so on.

The most typical usage of association rules, in the military field, is that it can be used to examine the failure of the military equipment. During the examination, one fault character will usually lead to many failures. So during the diagnosis process, not only do we need the default judge rules of fault in each subsystem, but also we need the rules of transmission of failure information and interaction. Mining the association rules from a great deal of failures, we can get available association rules between failure and cause or get available association rules  between one failure and another failure. And then we can provide important reference for eliminating the failure quickly and completely. From figure 1, we can know the hierarchy of missile weapon system fault characters [1].

Figure 1  Hierarchy of missile weapon system fault  characters   There is a bottleneck question in classical Apriori algorithm, which is frequently repeated scanning the transaction database [2][3]. So we have proposed an algorithm based on an optimized probability and statistics estimate model, which has an obvious improvement in efficiency. However, there are many fault characters lead to a failure of the military equipment. And we can use principal component analysis to know which fault characters have most influence to a failure.

This paper is organized as follows: Section 2 presents our improved Apriori algorithm which has an obvious improvement in efficiency. In section 3, we propose an algorithm based on principal component analysis to know which factors have most influence to event rate so as to distinguish correlativity. Section 4 gives out the experimental results. And we compare Apriori algorithm  Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.171   Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.171   Second International Symposium on Intelligent Information Technology Application  DOI 10.1109/IITA.2008.171     with our algorithm. Finally, conclusions and future work will be illustrated in Section 5.

2. Our algorithm based on probability model  2.1. Our improved Apriori algorithm   The most crucial step in the Apriori algorithm is finding frequent itemsets [4]. The Apriori algorithm finds frequent itemsets by scanning the database frequently so that it consumes too much time. It may also lead to a large number of itemsets by JOIN operation, which also consumes too much time [5][6]. In order to solve these issues, we present an algorithm based on probability and statistics estimate model which has an obvious improvement in efficiency.

Our improved algorithm only needs to scan the database three times. Firstly, it finds frequent 1-itemsets and 2-itemsets and their supports by scanning the database two times. Secondly, it estimates the candidate frequent 3-itemsets, ?, n-itemsets (n is the number of the longest itemsets) by k-itemsets probability algorithm. And then it can get the all supports of all candidate frequent itemsets by scanning the database thirdly. Finally, it can get frequent itemsets and association rules through comparing them with minimum support and confidence.

This method can greatly decrease the times of scanning the database, and can also save the time and space.

2.2. K-itemsets probability algorithm   Suppose independent probabilities of each attribute  item 1 2, , , nK K K  in Database are 1 2, , , nP P P .

Simultaneous happening probability of arbitrary two attributes items iK , jK is ijP . According to discussion above, in positive correlation, we have  ( ) ( ) ( ) min{ ( ), ( )}P B P A P AB P A P B? ?  [7].

Then, we can deduce:  1 1 1(1 ) ( )ij j i j j i j i jP a P a PP a P PP PP= + ? = ? + .           (1) We can compute 1a by following steps: (1) Scanning mined database at the first time: To get  each probability of 1-itemset; (2) Scanning mined database at the second time: To  get each probability of 2-itemset; (3) Get 1ia : Calculate every 1ia by expressions (1) and  then average to get 1a .

To estimate the 3-itemsets, we only take the situation  in positive correlation into account, and we have ( ) ( ) ( ) min{ ( ), ( ), ( )}P C P AB P ABC P A P B P C? ? .

Then we can deduce:  1 1  1 1  1 1  (1 )  (1 )  (1 )  ijk ij ij ik  ijk ij ij jk  ijk ik ik jk  P a P a P P  P a P a P P  P a P a P P  ? ? = + ? ? ? ?? = + ?? ?  ??? = + ???  (2)  According to total probability formula:  ( ) ( | ) ( )  n  i i i  P X P X Y P Y =  =? , we can get:  1 2 3 1 2 3( 1)ijk ijk ijk ijkP k P k P k P k k k? ?? ???= + + + + = .         (3) Where 10 1k? ?  20 1k? ?  30 1k? ? .We use  1 2 3 1/ 3k k k= = =  in the calculation.

We generalize an K-itemsets recurrence formula:  1 2 1 1 2 1 2  1 2 1 1 1 2 1  1 2 1 2 1 2 1 1 2 2  1 2 3 2 3 2 3 1 2  1 2 1  ( ) ( ) ( ) min( ( ), ( ), , ( )) ( ) ( ) (1 ) ( ) ( ) ( ) ( ) (1 ) ( ) ( )  ( ) ( ) (1 ) ( ) ( ) ( )  k k k k  k k k k k k  k k k k k k  PAA A PA PAA A PA PA PA PAA A a PA a PA PAA A PAA A a PA a PA PAA A  PAAA aPA a PA PAA PAA a  ?  ? ? ?  ? ? ? ? ? ?  ? ? = + ? = + ?  = + ? = 2 1 2 1( ) (1 ) ( ) ( )PA a PA PA+ ?  (4)   3. Principal component analysis   However, there are many fault characters lead to a failure of the military equipment. And usually we want to know which fault characters have most influence to an failure. Then we can use principal component analysis helps us to know which factors have most influence to event rate so as to distinguish correlativity.

Using our improved Apriori algorithm, we can get a linear model. After we get linear model, we can use principal component analysis to know which factors have most influence to an event rate and this influence is how much. Formally, we have  0 , ~ ( , )N? ? ?= + +Y 1 X? ? 0 I ,                (5)  where Y is a 1N ? vector, 0? is an unknown, 1 is an N dimensional column vector those elements equal to 1, and ? is an unknown vector.

11 12 1  21 22 2  1 2  ( )  p  p  n n np  x x x x x x  x x x  ? ? ? ? ? ?= = ? ? ? ?? ? ? ?  1 2 pX x , x , , x  (6)  where X has already been standardization ( jij jx x s? ).

An arbitrary line combination for independent variable  is: 2  n  j j  c =  = =?1 1 2 2 p p,z c x + c x + + c x  then we  compute singular value of X matrix, namely, eigenvalue of TX X , which is expressed by 1 2 p? ? ?? ? ? .

And these eigenvalue correspond to orthogonal characteristic vector 1 2 p? ,? , ,? , the kth principal component is kz = X? . If 1 0r p? ?+ = = ? , we delete , ,r+1 r+2 pz , z z . And accumulative contribution  rate is 1 1  pr  i i i i  ? ? = = ? ? , thus, we are able to know which  selected principal components have influence to event rate and this influence is how much .

Sometimes, if we only take accumulative contribution rate into account, that is not enough, so, we also take contribution, which is principal components toward initial data, into account. And we use coefficient of correlation  sum of squares to express: 2   ( , ) n  i i i j  r? =  =? z x , where , , r1 2z , z z  is selected principal components, and  cov( , )( , ) ( ) ( )  i i i i  i i  r D D  = z xz x z x  .

When accumulative contribution rate is more than 85% (we choose this threshold), and , , r1 2z , z z have contribution to all the ix (namely, 0i? ? ), selecting  , , r1 2z , z z is ok. Q is written as ( ) p p?= 1 2 pQ ? ,? , ,? , where Q is orthogonal  matrix, and Z = XQ , TZ Z = ?  ( ?  is diagonal matrix). Z is substituend into Y .

0 0? ?= + + = + + TY 1 ZQ ? ? 1 Z? ?                     (7)  Matrix Q and ? are partitioned  as 1 2Q = (Q ,Q ) , ? ? ? ? ? ?    ? ? =  ? , where 1Q is p r?  matrix, 1? is r dimensional column vector. And principal  components estimation is 1 ? ? ? ?? ? ? ?  ?? =  , where 1?  is a  sample mean. Then 1 T  1? = Q Q ? , however, when eigenvalue is approximate to 1, there is a jump, which has negative influence to stability of solution [8]. In order to solve this problem, please bring Yang Hu?s paper as reference.

4. Experiment   We have already achieved the Apriori algorithm which  is in the environment of VC++ 6.0, and we also program with VC++ 6.0 to realize our algorithms. Then we compared them in the same condition. The test condition is: P4 2.0 CPU; 512M DDR; 80G HD and Windows xp- sp2 professional OS (Figure 2 is the interface of program under our experimental environment). And the test data that we use in the experiment is the failure detection simulation notes of certain system equipment. The number of the total failure detection simulation notes is 142 728. From table 1, we can know attributes of fault characters. From table 2, we can see the part of the test data. The minimum support is 0.2, the degree of confidence is 0.3. In the experiment, the failure detection simulation notes are different parts of the all notes, and the numbers of them are: 1.0?104; 2.0?104; 3.0?104; 4.0?104; 5.0?104; 6.0?104; 7.0?104; 8.0?104; 9.0?104; 1.0?105; 1.2?105 and 1.4?105. From figure 3, we can know the comparative result of Apriori algorithm and this algorithm in the execution speed. And from figure 4, we can know the comparative result of Apriori algorithm and this algorithm in the quantity of rule sets.

Table 1  Attributes of fault characters  Event ID Event Name Attribute Abbreviation  1 Trace radar target  a1 Mutation a2 Continuous lost frame a3 Four times error a4 Five times error a5 Six times error a6 Seven times error a7 False alarm  2 Appropriated firepower a8 Abnormal time a9 Appropriated error     Event ID Event Name Attribute Abbreviation a10 Can not appropriate  3 Command of launching control a11 Abnormal time a12 Control information error a13 Can not launch   Table 2  Part of the test data  Number a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 a12 a13 1 1 0 0 0 1 0 1 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 1 1 0 1 0 3 1 1 0 0 0 1 0 0 0 1 0 1 0 4 0 0 0 0 0 0 0 1 1 0 0 0 0 5 0 1 1 0 0 0 1 0 0 0 0 0 0 6 0 1 0 0 1 0 1 0 0 0 0 0 1 7 0 1 0 1 0 0 0 0 0 0 0 0 1 8 0 1 1 0 0 0 0 1 0 0 1 0 0 9 1 1 0 1 0 0 0 1 0 0 1 0 0  10 0 0 1 0 0 0 0 1 0 0 0 0 0 ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ??  Note: From a1 to a13 represent the fault characters which are described in the table 1.

Figure 2  Interface of program   From the experimental result, we can easily find that  under the same conditions, the executive time of this algorithm is much less than that of Apriori algorithm. And the time curve of this algorithm is always under the time curve of Apriori algorithm. When the notes in data sets are increasing, the executive time of Apriori algorithm increases much more rapidly than that of this algorithm.

This algorithm is much efficient. Because if we want to get all frequent itemsets, we should use Apriori algorithm to scan the database n times (n is the number of the longest itemsets). But if we use this algorithm, we only need scan the database three times. And the diffusibility of this algorithm is better than the diffusibility of Apriori algorithm. With the increase of items and notes, the difference will be more and more obvious. And in the experiment, we have found that it decreases I/O operation of computer and it only needs less memory.

Figure 3  Compare of implementation time     Figure 4  Compare of rules   In order to estimate parameter 1a  in the formula (2)  (4), we can use sample mean to estimate it.

The results of principal component analysis are demonstrated below (Table 3): After mining principal factor, we can know fault type and main fault, even contribution rate clearly.

Table 3  Principal factor for fault  Fault Type Fault Cause (n-itemsets) Main Fault Contribution  Rate Radar Fault {a1,a2,a3,a7} a1,a2 52%+35% Radar Fault {a1,a2,a5} a1,a2 58%+32% Radar Fault {a2,a4,a7} a2 73% Radar Fault {a1,a2,a6,a7} a1,a2 56%+37%  Fault of Appropriated  Firepower {a8,a9,a10} a8 75%  Fault Command of Launching  Control  {a11,a12,a13} a11 84%   5. Conclusions   In this paper, our probability and models that are based  on probability estimate and principal component analysis are proposed to mine the association rules from database.

Our algorithm takes both efficiency accuracy and correlativity into account and it is proved and validated by experiment.

Sometimes, we need to know which factors have the most influence to an event rate and how much this influence is. Therefore, we mine principal factors by principal component analysis in order to distinguish main correlativity in the association rules data mining.

In future, we will research those issues in mass and spatial database.

Acknowledgement   It is a project supported by Wuhan Digital Engineering  Institute.

