Nonfuzzy Classification using Rules annotated with Weight of Evidence from  Statistical Data

Abstract?To design and develop a nonfuzzy classification paradigm from a statistical data set. The event association patterns of different orders are detected which provides a probabilistic inference mechanism to achieve flexible classification and prediction. To detect significant event associations, residual analysis in statistics is used. Patterns are detected and rules are generated based on the deviations of the observed patterns from a default model. The discriminative power of each rule generated is described using Weight of Evidence (WOE) statistic. Classification decisions are made using WOE based estimation of the relative likelihoods of each possible labeling. Estimates are calculated by using the set of rules triggered by matching input values. Experimental results are discussed towards the end of the paper.

Keywords-Maximum marginal entropy; Event generation; Residual analysis; Pattern discovery; Nonfuzzy

I. INTRODUCTION One of the basic tasks of data analysis is to automatically  uncover the qualitative and quantitative patterns in a data set.

When a large database is given, discovering the inherent patterns and regularities becomes a challenging task, especially when no domain knowledge is available or when the domain knowledge is too weak [1]. Because of the size of the dataset, it is almost impossible for a decision maker to manually abstract from it the useful patterns. Hence, it is desirable for automatic pattern discovery tools to support various types of decision-making tasks.

Consider a set D of M observations or samples obtained from a database. Each observation is described by N attributes. Thus, for any distinct attribute, there is a domain of possible values which can be numeric, boolean, and/or symbolic. Here, all the possible values are assumed to be discrete. The purpose of pattern discovery is to find the relations among the attributes and/or among their values.

Those events whose occurrences are significantly different from those based on a ?random? model are extracted. Thus, the co-occurrence of events detected may reflect the nature of the data set and provide useful information for future inferences and reasoning.

To design a pattern discovery system, some specific requirements must be addressed [2].

The discovery process and the representation of the patterns should be transparent and easily interpretable.

To account for probabilistic uncertainty in the data, the description of patterns should have a probabilistic indication of statistical relevance.

As real data is often noisy, the discovery system must be able to filter random noise from useful information.

To alleviate an often-vexatious data-preprocessing step, the discovery process should not be sensitive to monotonic, linear scaling of the data.

In order to meet these objectives, past methods have often sacrificed interpretability. A pattern discovery method is presented that aims at meeting the above challenges under a single framework, while remaining fairly transparent. The discovery method is based on the statistically guided construction of events in the sample space.

A. Theoretical background The fundamental concepts and definitions are presented  follows.

Consider a data set D containing M samples. Every  sample is described in terms of N attributes, each of which can assume values in a corresponding discrete finite alphabet.

1) Feature: A feature, X, is a random variable which, in general, may be a nominal, ordinal, or numerical attribute or characteristic of an object or process.

2) Event: An event is the elemental construct in the present formulation of pattern discovery. An event is just a subspace of the continuous sample space.

3) Primary Event: A primary event of a random variable Xi is a realization of Xi which takes on a value ?a? from Di.

i a  4) Compound Event: A compound event associated with the variable set Xs is a set of primary events. A ?k? compound event is made up of k primary events of k distinctive variables. Every sample in the data set is an N compound event.

DOI 10.1109/ICETET.2010.114    DOI 10.1109/ICETET.2010.114    DOI 10.1109/ICETET.2010.114    DOI 10.1109/ICETET.2010.114     5) Observed Occurrence: The observed occurrence is the actual occurrence of the compound event Xs in data set D.

6) Expected Occurrence: The expected occurrence of a compound event Xs in data set D is its expected total under the assumption that the variables in Xs are mutually independent. The expected occurrence of Xs is denoted as Ex.

7) Pattern: If a compound event Xsj passes the statistical significance test T then Xsj is a significant pattern, or, simply, a pattern, of order |s|.



II. DISCRETIZATION BY PARTITIONING In order to function in a continuous domain the pattern  discovery algorithm has been adapted to use discretized data.

The approach for event construction is to partition the continuous sample space into nonoverlapping sections [3].

Figure 1. Observed data events and MME partitioning  The following are the reasons for partitioning The continuous sample must be discretized because the pattern discovery cannot work on continuous data values.

Each event can be described probabilistically i.e. the probability of each event can be estimated.

A. Maximum marginal entropy partitioning The marginal maximum entropy criterion is derived from  Information Theory and prescribes a method of partitioning the sample space [4]. The subspace of interest is segmented in a way, which approximately maximizes the overall entropy H, of the partition. If the subspace is partitioned into J events, then the entropy of the partition is expressed as  J H = -  Pj log Pj                                                       (2) j=1  The approximate nature of the method is due to the fact that partitioning is not performed in the full dimensionality, but rather marginally along each dimension.

1)  Marginal Maximum Entropy (MME) Criterion:  To approximately maximize the entropy of a d-dimensional partition maximize the entropy of each one-dimensional (marginal) partition [17]. Thus discretization is performed via independent analysis of the density of feature values along each dimension using marginal maximum entropy partitioning in which the values observed in each input feature is divided among Q quantization bins, such that each bin contains the same number of values as shown in Fig. 1.

B. Choice of Q value The choice of partition size Q is governed by the trade-  off between maximizing statistical significance of the resulting events while minimizing computational complexity [5].

Too low value of Q will cause coarse partition and hence exact specific and true patterns cannot be identified.

Too high value of Q will cause sparse number of samples in each partition and hence the confidence of the rules identified cannot be asserted.

Q is chosen such that each d-dimensional event has at least the minimum requisite number of data points for statistical testing.

C. Algorithm For a subspace , with  as the threshold sample size  and vj as the volume of the event in a d-dimensional partition, the simple recursive partitioning with MME criterion is:  Choose a partition size Qo where Qo >= 2 Call Partition( ,Qo) Procedure Partition(S,Q) Partition S into Qd events according to the MME  criterion For each event Ej, j=1?.Qd  If nj  , Estimate probability density as pj = nj / Nvj  Else Choose a partition size Qj Call Partition(Ej,Qj) End if  End For End Partition

III. PATTERN DISCOVERY An event in a dataset is said to be a statistically  significant pattern if there is significant deviation difference between its expected and observed occurrence [6][13].

A positive pattern is one where the observed occurrence is much more than the expected occurrence.

A negative pattern is one where the observed occurrence is much less than the expected occurrence.

This deviation is measured in terms of residuals Simple residual Standard residual Adjusted residual  For an event Xi ex indicates the number of occurrences of xlm expected from the observation of a training set of known size under an assumed model of uniform random chance.

ox is the observed number of occurrences of xlm  in a training data set.

A. Simple Residual Simple Residual is just the absolute difference between  the expected and observed occurrence  sx = ox - ex                                                                              (3)  But it does not show the actual deviation, as it is not scalable. It depends on the size of the input dataset.

B. Standard residual Standard residual is defined as the ratio of the simple  residual and the square root of the expected occurrence. The standardized residual provides a normalization of the simple residual  zx = ox ? ex / ex                                                                    (4)  But it does not have a unit standard deviation.

C. Adjusted residual Adjusted residual is defined is the ratio of the standard  deviation and its variance  dx =zx  / vx                                                                                 (5)  The benefit of the adjusted residual is that it scales the space of the standardized residual into that of a normal deviate with unit standard deviation. Using the resulting N(0, 1) space, we can easily calculate observed deviation from expectation[7].

It is important to note that the expectation value expected value cannot fall to zero under the assumed model used here.

This is because it is equivalent to a linear scale of the number of available training examples, as it is produced by dividing the available training examples among the number of quanta.

The only way a zero value could therefore be produced is by having no observed values for some feature. If this were to occur (i.e. if all the values for a given column in the training data were missing) the adjusted residual would be undefined, however in this pathological case there is no discovery system that would be able to proceed. It suffices to proceed under the assumption that there will be a non-zero number of examples available for every input feature.

D. Pattern discovery by residual analysis To select significant events, the adjusted residual is  used as it provides an N (0, 1) distributed z statistic (i.e., a statistic drawn from a normal distribution with zero mean and unit standard deviation). The value of the adjusted residual defines the relative significance of the associated event. The pattern discovery algorithm determines an event to be significant if the adjusted residual is greater than a threshold value [14].

E. Logic A compound event is said to be a pattern if the absolute value of the adjusted residual is greater than the threshold value. If the value is positive it is a positive pattern else it is a negative pattern. The testing can be formalized as follows  H0 : x is random H1 : x is significant  The decision rule has the following form:  ( |dx|  >  threshold ) => H1 F. Selecting the threshold value  The threshold value determines the statistical significance the compound event has to pass to be considered as a pattern. So the selection of the threshold H is very crucial. It depends on the confidence support [8].

For a confidence level of 95 percent, H is chosen as 1.96  Figure 2.  Region of significance and threshold determination  G. Elimination and validation of test cases As a precaution against recording spurious patterns when  is low, the pattern discovery classifier implemented will not consider as patterns any events for which the expected number of occurrences is less than a cut off [9]. This is desired in order to prevent the discovery of high-order patterns caused by the occurrence of only one or two instances of a high-order event (instances that were possibly generated by chance). The existence of such patterns may diminish classifier performance, especially when the total number of features, M, is quite large. The use of a spurious pattern would then corrupt the evaluation of any remaining features in the induction of the correct class label value.

So a cut off ? ? is decided. Only when expected occurrence of an event is greater than or equal to the cut off,     the compound event is eligible for residual analysis. Only when  ex The test is valid.

This has two purposes  Elimination of spurious candidate patterns early in the pattern discovery phase.

Avoiding exhaustive search of all combinations of primary events.

Usually the value for is taken as any integer greater than 3. For extremely stringent cases a large cut of value can be assumed.

H. Algorithm for pattern discovery by residual analysis Procedure: PatternDiscovery() Input: Data Set Output: {Pattern, Label} Begin Procedure  Let Order=2 While Order<=N, Do For all compound events xi of order, Do  Calculate R, adjusted residual If |R|>Threshold  Output xi as a rule End if  End for Let Order = Order + 1 End while  End Procedure

I. Output of pattern discovery The algorithm takes as input the given dataset and  outputs a set of patterns by calculating the residual analysis and comparing with the threshold.

It outputs a set of patterns with statistical significance.

If the pattern contains a label it becomes a rule

IV. WEIGHT OF EVIDENCE In order to measure the discriminative power of a pattern,  ?weight of evidence? statistic or WOE is used [11].

Definition: Let (Y=yk) represent the label portion of a given pattern xlm, the remaining portion (consisting of the input feature values) is referred to as x*l. The mutual information between these two components can be calculated using:  (6)  A WOE in favor of or against a particular labeling yk Y can be calculated as  (7) or  (8)  WOE thereby provides a measure of how discriminative a pattern x*l is in relation to a label yk and gives us a measure of the relative probability of the co-occurrence of x*l and yk (i.e., the ?odds? of labeling correctly).

The domain of WOE values is [ 1   1], where 1 indicates those patterns (x*l) that never occur in the training data with the specific class label yk; 1 indicates patterns, which only occur with the specific class label yk. These ?1 valued WOE patterns are the most descriptive relationships found in the training data set as any non-infinite WOE indicates a pattern for which conflicting labels have been observed.



V. NONFUZZY CLASSIFICATION WOE-based support for each yk (possible class label) is  evaluated in turn by considering the highest order pattern with the greatest adjusted residual from the set of all patterns occurring in an input data vector to be classified, and accumulating the WOE of this pattern in support of the associated label. All features of the input data vector matching this pattern are then excluded from further consideration for this yk, and the next-highest order occurring pattern is considered [12]. This continues until no patterns match the remaining input data vector, or all the features in the input data vector are excluded. This ?independent? method of selecting patterns attempts to accumulate their WOE in way, which estimates the accumulation of the probabilities of independent random variables. Once this is repeated to consider each yk, the label with the highest accrued WOE is assumed as the highest likelihood match and this class label is assigned to the input feature vector. The classification is done using independent rule firing.

A. Independent Rule Firing The algorithm proceeds through the following steps:  1) Produce a list L of all input variables 2) Repeat steps 3-9 for each classification label.

3) Set search order o to be N, the number of inputs to  the system 4) Place all rules of order o whose precedents exist in  the list L into a list of matches, M. If this search fails, repeat after setting o = o  1  5) If o = 1 has been reached, and no matches have been found, then stop  6) Find the rule Rmax in M which has the highest adjusted residual  7) Add the WOE of Rmax to the WOE support of the label  8) Remove from L all the variables matching the precedents of rule Rmax  9) If L is empty, then stop 10) The class label with the highest accrued WOE is  assigned to the input feature vector     The occurrence of a pattern is assumed to consume the information associated with the primary events describing the pattern. For this reason, each input feature can support at most one rule firing, in order to maintain the assumption of statistical independence of the input features [15] [16].



VI. EXPERIMENTAL RESULTS The pattern discovery and classification modules were  successfully implemented. A large sample dataset was used as input and the rules were got as output. The modules were successfully tested on various datasets and results were obtained.

TABLE I. INPUT DATA SET  A training dataset D is used as input to the algorithm [10].

D consists of six input features with five attributes ?Red?, ?Blue?, ?Green?, ?Hue?, ?Saturation? and a label ?Texture?.

1500 record sets were considered  A. Parameters The three important parameters, which had to be decided  on, are Q ? number of quantization bins H ? threshold level for statistical significance  ? valid significance test cut-off  1) Q value: The value of Q was a tradeoff between the classification precision and the computational complexity.

Taking all the parameters into consideration, the value of Q was fixed as 5.

2) H value: The threshold level was chosen to be a standard value of 1.96 which corresponds to 95 percent confidence level which ensures a reasonable assertion to make a strong suggestion in decision support system.

3)  value: The valid significance test cut off was decided based on the size of the database. To eliminate the spurious patterns early in pattern discovery phase, and to avoid exhaustive search of the data set, an optimum value of was chosen as 10.

B. Quantization The numeric data of all the 5 input features were  discretized using maximum marginal entropy partitioning.

Each sample space was divided into quantization bins along each margin.

The value of Q was chosen as 5.

The input data was distributed over the 5 quantized bins in such a way that the entropy was maximized.

The number of sample data in each bin was equal.

1) Quantized bins  TABLE II. QUANTIZED BINS  C. Events generated After quantization the input sample space is partitioned  into events each of which can be determined probabilistically.

Primary events are the basic building blocks A :  [a1] [a2] [a3] [a4] [a5] B :  [b1] [b2] [b3] [b4] [b5] C :  [c1] [c2] [c3] [c4] [c5] D :  [d1] [d2] [d3] [d4] [d5] E :  [e1] [e2] [e3] [e4] [e5] The primary events are connected to form higher order  events.

Second order [a1, b1] [a2, b2]??.

Third order [a1, a2, a3]   [b1, b2, b3] ? And so on?  D. Rules generated 303 rules were generated. Each rule mapped an event to a  label value and the following are some of the rules generated IF (Green 0.0...3.55556) and (Hue 0.0...8.22222) THEN  Window IF (Blue 0.0...7.33333) and (Green 3.55556...16.8889)  THEN Cement IF (Blue 0.0...7.33333) and (Hue 8.33333...23.6667)  THEN Cement  IF (Red 0.0...5.44444) and (Blue 7.33333...20.0) THEN Foliage     IF (Red 0.0...5.44444) and (Green 3.55556...16.8889) THEN Foliage  No of rules generated.. 303  E. Weight of Evidence Weight of Evidence for each rule is calculated IF (Green 0.0...3.55556) and (Hue 0.0...8.22222) THEN  Window WOE : 1.5279335293281375 IF (Blue 0.0...7.33333) and (Green 3.55556...16.8889)  THEN Cement WOE : 7.067363981745421 IF (Blue 0.0...7.33333) and (Hue 8.33333...23.6667)  THEN Cement WOE : Infinity  IF (Red 0.0...5.44444) and (Blue 7.33333...20.0) THEN Foliage  WOE : 5.495014176104029 IF (Red 0.0...5.44444) and (Green 3.55556...16.8889)  THEN Foliage WOE : 4.35485123066857  F. Classification Same training data was used as input to the classification  algorithm. Of the 1500 input sets, 949 were correctly labeled.

26.3333,35.2222,25.6667,35.2222,-2.04915,Brickface, Miss : LABEL  :  Window 13.1111,17.8889,21.5556,21.5556,2.69011,Cement, Hit : LABEL  :  Cement 0,1.33333,0,1.33333,-2.0944,Foliage, Hit : LABEL  :  Foliage 6.55556,6.44444,11.5556,11.5556,2.09315,Cement, Hit : LABEL  :  Cement 0.777778,3,0,3,-1.82221,Grass, Miss : LABEL  :  Foliage 0.444444,6.44444,1.44444,6.44444,-2.26954,Foliage, Miss : LABEL  :  Path  115.111,142.222,121.333,142.222,-2.33375,Path, Hit : LABEL  :  Path 51.8889,72.4444,49.6667,72.4444,-1.99159,Brickface, Hit : LABEL  :  Brickface 7.44444,7.11111,14.6667,14.6667,2.06945,Cement, Hit : LABEL  :  Cement 41,57.2222,39.5556,57.2222,-2.01072,Brickface, Hit : LABEL  :  Brickface No of hits : 949  CONCLUSION Hence a framework for nonfuzzy classification is  proposed and proved experimentally. This model identifies statistically significant event associations inherent in a given  data set, using residual analysis to test whether or not a pattern candidate is significantly deviated from a default log- linear model. The generated rules annotated with weight of evidence are fired independently to classify the input feature vector.

In scenarios where labeling requires further statistical augmentation, a deterministic means of confidence could be derived. The confidence measure can be further combined with the label to supplement the decision analogous to multiple votes. The proposed non-fuzzy model leveraging such weighted classifiers increases the accuracy of decision- making.

