Memory System Characterization of Big Data Workloads

Abstract?Two recent trends that have emerged include (1) Rapid growth in big data technologies with new types of computing models to handle unstructured data, such as map- reduce and noSQL (2) A growing focus on the memory sub- system for performance and power optimizations, particularly with emerging memory technologies offering different char- acteristics from conventional DRAM (bandwidths, read/write asymmetries).

This paper examines how these trends may intersect by characterizing the memory access patterns of various Hadoop and noSQL big data workloads. Using memory DIMM traces collected using special hardware, we analyze the spatial and temporal reference patterns to bring out several insights related to memory and platform usages, such as memory footprints, read-write ratios, bandwidths, latencies, etc. We develop an analysis methodology to understand how conventional opti- mizations such as caching, prediction, and prefetching may apply to these workloads, and discuss the implications on software and system design.

Keywords-big data, memory characterization

I. INTRODUCTION  The massive information explosion over the past decade has resulted in zetabytes of data [1] being created each year, with most of this data being in the form of files, videos, logs, documents, images, etc (unstructured formats). Data continues to grow at an exponential pace, for example, in 2010, the world created half as much data as it had in all previous years combined [2]. With such data growth, it becomes challenging for conventional computing models to handle such large volumes. Big Data analytics [3] [4] [5] [6] has emerged as the solution to parse, analyze and extract meaningful information from these large volumes of unstructured data. Extracting this information provides opportunities and insights in a variety of fronts- from making more intelligent business decisions, understanding trends in usages and markets, detecting frauds and anomalies, etc- with many of these being possible real-time. As a result of these advantages, big data processing is becoming increas- ingly popular. Two primary big data computing models have emerged: (1) Hadoop-based computing, and (2) NoSQL- based computing, and the two are among the fastest growing software segments in modern computer systems.

On the other hand, computing systems themselves have seen a shift in optimizations; unlike a decade earlier when most optimizations were primarily on the processor, there has been more focus on the overall platform, and particularly the memory subsystem for performance and power improve- ments. Recent studies have shown memory to be a first- order consideration for performance, and sometimes even the dominant consumer of power in a computer system. This  emphasis becomes even more important with recent trends in emerging memory technologies [7] [8], that are expected to offer different characteristics from conventional DRAM- such as higher latencies, differing capacities, persistence, etc.

In order to have software run efficiently using such technolo- gies, it becomes critical to characterize and understand the memory usages.While various studies have been performed on memory characterization of workloads [9] [10] [11] [12] over the past decade, unfortunately, most of them focus on the SPEC benchmark suites and traditional computing models. Very few studies have examined memory behaviors of big data workloads- and these are mostly specific to an optimization, such as TLB improvements [13] [14].

This paper addresses this gap by providing a detailed char- acterization of the spatial and temporal memory references of various big data workloads. We analyze various building blocks of big data operations such as sort, wordcount, aggregations, and joins on Hadoop, and building indexes to process data on a NoSQL data store. We characterize the memory behavior by monitoring various metrics such as memory latencies, first, second and last level processor cache miss rates, code and data TLB miss rates, peak memory bandwidths, etc. We examine the impact of Hadoop compression both on performance and the spatial patterns.

Using specially designed hardware, we are able to observe and trace the memory reference patterns of all the workloads at the DIMM level with precise timing information. These traces provide us the unique ability to obtain insights based on the spatial and temporal references, such as memory foot- prints, and the spatial histograms of the memory references over the footprints.

This paper also examines the potential for big data workloads to be tolerant to the higher latencies expected in emerging memory technologies. The classic mechanisms for doing this are caching in a faster memory tier, and predicting future memory references to prefetch sections of memory. We examine the cacheability of big data workloads by running the memory traces through a cache simulator with different cache sizes. An interesting insight is that we observe is that many of the workloads operate on only a small subset of their spatial footprint at a time. As a result, we find that a cache that is less than 0.1% the size of the footprint can provide a hit rate as high as 40% of all the memory references. For prefetchability, we observe that using existing prefetcher schemes to predict the precise     next memory reference is a hard problem at the memory DIMM level due to mixing of different streams from differ- ent processor cores, and this mixed stream getting further interleaved across different memory ranks for performance.

Clearly, more sophisticated algorithms and techniques are required if prefetching is to be transparent to the application at the lower levels of the memory hierarchy. In order to examine the potential for the same, we use signal processing techniques: entropy and trend analysis (correlation with known signals) to bring out insights related to the memory patterns.

We believe this is the first paper to examine the design space for memory architectures running big data workloads, by analyzing spatial patterns using DIMM traces and pro- viding a detailed memory characterization. The study brings out a wealth of insights for system and software design.

The experiments are performed on a 4 node cluster, using 2 socket servers with Intel Xeon E5 processors, with each node configured with 128GB of DDR3 memory, and 2TB of SSD storage. (We intentionally selected fast storage and large memory capacities: with the price for flash and non-volatile media continuing to drop, we chose this configuration to understand forward looking usages). The remainder of this paper is organized as follows: Section II describes the related work. Section III describes the various workloads used in this paper. Section IV describes the experimental methodology used, and Section V presents the results and observations, with Section VI concluding the paper and discussing future work.



II. RELATED WORK A. Memory system characterization  There have been several papers discussing memory sys- tem characterization of enterprise workloads, over the past decade. Barroso et al. [9] characterize the memory references of various commercial workloads. Domain specific charac- terization studies include memory characterization of paral- lel data mining workloads [15], of the ECperf benchmark [16], of memcached [17], and of the SPEC CPU2000 and CPU2006 benchmark suites [10] [11] [12]. Particularly note- worthy is the work of Shao et al. [12], where statistical mea- sures are used for memory characterization. The common focus of all these works is using instrumentation techniques and platform monitoring to understand how specific work- loads use memory. With emerging memory technologies [8] [18] [19] [20] having different properties from conventional DRAM that has been used for the past decade, this type of characterization focus becomes particularly important.

B. Big Data workloads  Various researchers have proposed benchmarks and work- loads representative of big data usages; the common focus of all these benchmarks is they deal with processing unstruc- tured data, typically using Hadoop or NoSQL. The Hibench  suite developed by Intel [21] consists of several Hadoop workloads such as sort, wordcount, hive aggregation, etc.

that are proxies for big data usages in the real world. In this paper, we use several Hadoop workloads from the HiBench suite. Another class of data stores to handle unstructured data are NoSQL databases [22], which are specialized for query and search operations. They differ from conventional databases in that they typically do not offer transactional guarantees, and this is a trade-off made in exchange for very fast retrieval.

Recent studies have also proposed characterizing and understanding these big data usage cases. These can be classified as follows ? Implications on system design and architecture: A study  from IBM research [23] examines how big data work- loads may be suited for the IBM POWER architecture.

Chang et al. [24] examine the implications of big data analytics on system design.

? Modeling big data workloads: Yang et al. [25] propose using statistics-based techniques for modeling map reduce. Atikoglu et al. [26] model and analyze the behavior of a key-value store (memcached).

? Performance characterization of big data workloads: Ren et al. characterize the behavior of a production Hadoop cluster, using a specific case study [27]. Issa et al. [28] present power and performance characterization of Hadoop with memcached.

Very few studies focus on understanding the memory characteristics of big data workloads. Noteworthy among these are that of Basu et al. [14], which focuses on page- table and virtual memory related optimizations for big data workloads. Jia et al. [13] present a characterization of L1, L2, and LLC cache misses observed for a Hadoop workload cluster. Both of these studies focus on characterization at the virtual memory and cache hierarchy, as opposed to the DRAM level.

C. Contributions  The following are the unique contributions of this paper: ? We believe this is the first study analyzing the memory  reference patterns of big data workloads. Using hard- ware memory traces at the DIMM level, we are able to analyze references to physical memory.

? We introduce various metrics to qualitatively and quan- titatively characterize the memory reference patterns, and we discuss the implications of these metrics for future system design.



III. WORKLOADS  We use several Hadoop workloads, from the HiBench workload suite [21] and a NoSQL datastore that builds in- dexes from text documents. We use a performance-optimized Hadoop configuration in our experiments. Since Hadoop has a compression codec for both input and output data, all the     Hadoop workloads are examined with and without use of compression. The following is a brief description of the various workloads used:  A. Sort  Sort is a good proxy of a common type of big data opera- tion, that requires transforming data from one representation to another. In this case, the workload sorts its text input data, which is generated using the Hadoop RandomTextWriter example. In our setup, we sort a total of 96GB dataset in HDFS, using a 4 node cluster, with 24GB dataset/node.

B. WordCount  Word count also represents a common big data operation: extracting a small amount of interesting data from a large dataset, or a ?needle in haystack search?. In this case, the workload counts the number of occurrences of each word in the input data set. The data set is generated using the Hadoop RandomTextWriter example. In our setup, we perform wordcount on a 128GB dataset in HDFS, distributed between the 4 nodes as 32GB per node.

C. Hive Join  The Hive join workload approximates a complex analytic query, representative of typical OLAP workloads. It com- putes the average and the sum for each group by joining two different tables. The join task consists of two sub-tasks that perform a complex calculation on two data sets. In the first part of the task, each system must find the sourceIP that generated the most revenue within a particular date range. Once these intermediate records are generated, the system must then calculate the average pageRank of all the pages visited during this interval. The data set generated approximates web-server logs with hyperlinks following the Zipfian distribution. In this case, we simulated nearly 130 million user visits, to nearly 18 million pages.

D. Hive Aggregation  Hive aggregation approximates a complex analytic query, representative of typical OLAP workloads by computing the inlink count for each document in the dataset, a task that is often used as a component of PageRank calculations. The first step is to read each document and search for all the URLs that appear in the contents. The second step is then, for each unique URL, to count the number of unique pages that reference that particular URL across the entire set of documents. It is this type of task that the map-reduce is believed to be commonly used for.

E. NoSQL indexing  The NoSQL workload uses a NoSQL data store to build indexes, from 240GB of text files, distributed across the 4 nodes. This type of computation is heavy in regular expression comparisons, and is a very common big data use case.



IV. EXPERIMENTAL METHODOLOGY  In this section, we discuss the experimental methodology used in the paper. The experimental methodology is focused on the following objectives: (1) providing insights about how big data applications use the memory subsystem (Section IV-A) (2) examining the latency tolerance of big data work- loads (since emerging memory technologies have higher latencies than DRAM). The latency tolerance is examined by understanding the potential for classic techniques to hide latency: cacheability in a faster tier (Sections IV-B, IV-C, and IV-D) and prefetching into a faster tier (IV-D and IV-E).

