Bayesian Based Subgroup Discovery

Abstract?Data Mining is concerned with extraction of interesting patterns or knowledge from huge amounts of Data.

Generally data mining tasks are either predictive or descriptive.

Classification falls under predictive induction while clustering and association rule mining fall under descriptive induction.

Subgroup discovery is a task at the intersection of supervised learning and descriptive induction. In subgroup discovery we want to uncover individual patterns in data with a given property of interest. We want to find subgroups that cover a large population and are statistically different. The main application areas of subgroup discovery are exploration and descriptive induction, where the user wants to find the overview of dependencies between a target and many explaining variables.

Many techniques have been proposed for discovering subgroups and some of these techniques are based on classification. But none of the techniques uses Bayesian networks for the generation of subgroups. Our contributions include a technique for the discovery of subgroups where the subgroups are generated using Bayesian networks.

Keywords- Subgroup Discovery, Bayesian Networks

I.  INTRODUCTION  Generally Data Mining tasks are divided into two broad categories: Predictive and Descriptive. Descriptive mining is aimed at characterizing the general properties of the data in the databases [1]. Predictive Mining performs inference on the current data in order to make predictions [1]. There are some tasks in data mining that can be categorized as both supervised (we have labelled instances in the training data) and descriptive data mining. One of these tasks is subgroup discovery.

Classification, a branch of predicative data mining, is the process of finding a model that describes and distinguishes the classes of objects, for the purpose of being able to use the model to predict the class of objects whose class label is unknown [1]. C4.5, CART, K Nearest Neighbours, Na?ve Bayes and K2 are some of the well-known algorithms that are being used for the purpose of classification.

One of the well-known techniques of classification is Bayesian Networks. These networks are directed acyclic graphs that effectively represent the joint probability over a set of random variables. The vertices in the graph represent random variables while the edges represent direct correlation between the variables. ?More precisely, the network encodes the following conditional independence statements: each  variable is independent of its non-descendants in the graph given the state of its parents? as quoted from [2]. Using this independence assumption the number of parameters required to characterize the joint probability distribution can be reduced. In other words the number of parameters required to compute the posterior probability given the evidence is greatly reduced.

Each node in the graph has an associated Probability distribution. These distributions give us the probabilities of all possible values of the node given all possible values of the parents. Using the independence statements encoded in the network, the joint distribution is uniquely determined by these local conditional distributions [2]. Figure 1 shows a sample Bayesian network.

Figure 1.  A sample Bayesian network  Subgroup Discovery is a task at the intersection of Predictive and Descriptive data mining. Subgroup Discovery is defined as: ?Given a population of individuals and a specific property of individuals that we are interested in, find population subgroups that are statistically ?most interesting?, e.g. are as large as possible and have the most unusual distributional characteristics with respect to the property of interest.? [8].

In subgroup discovery we want to uncover properties of selected target population. Target population is the part of the population with a given property of interest (Class) [4]. In this paper we propose a scheme for subgroup discovery that is based on Bayesian Networks. There are a number of techniques that have been proposed for subgroup discovery. Some of them     such as CN2-SD [6] proposed by Lavrac et al; and Apriori-SD also proposed by Lavrac et al that is based on or adapted from classification algorithms. Other works in this domain include EXPLORA by Klosgen [8], MIDOS by Wrobel [9] and SD- Map by Aztmuller [10]. Some used beam search to discover subgroups while others applied expert guided search strategies which involve input from human experts to generate affective subgroups. Most of these techniques use the ROC or the TP/FP space to evaluate the quality of their subgroups. As we shall see in Section III that none of the techniques uses Bayesian Networks to generate rules that represent out subgroups.

The remainder of this paper is organized as follows. Section two briefly introduces Bayesian Networks. Section three explains subgroup discovery. Section four outlines the techniques that have been previously proposed focusing mainly on techniques based on classification. In Section five we discuss our proposed technique. In Section six we demonstrate the technique using an example.



II. BAYESIAN NETWORK  A Bayesian network is a natural way of representing knowledge. The network represents dependency/causal relationships which exist in the real world. Heckerman has given four advantages to emphasize the usefulness of Bayesian Networks. Firstly, Bayesian Networks can readily handle incomplete data by the use of dependency relationships [3].

Secondly, they allow us to learn causal relationships. These relationships help us firstly in understanding the problem domain and also they can help us make predictions in the presence of interventions [3]. Thirdly, Bayesian Networks combined with Bayesian Statistics facilitate the combination of prior knowledge and data [3]. Lastly, they avoid over fitting of Data [3]. Bayesian networks give surprisingly good results in a large number of domains.

Bayesian network is a directed acyclic graph (DAG). The edges in the DAG represent dependency relationships or direct correlation between the nodes. If there is a node from node A to node B in a Bayesian network then node A is called the parent or ancestor of node B while node B is referred to as the child or descendant of node A. For example in Figure 1, ?Gas? and ?Jewelry? have one common ancestor or parent ?Fraud?.

While ?Gas? has two children or descendants ?Gas? and ?Jewelry?. In such a case ?Gas? and ?Jewelry? are also called siblings because they have a common parent. The independence assumption that a Bayesian network is based upon states that nodes are independent of each other given the value of their parents. For example, if the value of ?Fraud? is known then ?Gas? and ?Jewelry? are independent of each other.

Before we can use a Bayesian network for the purpose of classification, we need to learn it from data. Learning a Bayesian network involves two tasks: learning the structure of the network and, learning the conditional probability tables.

Various methods have been proposed for the learning of Bayesian networks. The one we will discuss here which is also the algorithm we have used in our proposed scheme is K2 [11].

K2 [11] is an algorithm proposed by Cooper et al, for the purpose of learning a Bayesian network from a database of records. The algorithm accomplishes this task by searching for  the most probable structure of a network given a database D.

The algorithm learns the most probable Bayesian network by iteratively adding parents which most increase the probability of the resulting structure. To find the most probable network the probabilities of structures need to be compared e.g. given two network structures Bsi and Bsj and a Database D of records, the probabilities P(Bsi|D) and P(Bsj|D) can be computed and compared using equation 1 given below [11]:  (1)    Cooper et al introduce an efficient formula for the calculation of P(Bs, D) based on four assumptions. Firstly the variables are assumed to be discrete. If the variables are continuous, then they will need to be discretized. Second, those cases occur independently is given a Bayes model. The third assumption made by the algorithm is that there are no missing values and lastly that the density function f(Bp|Bs) is uniform where Bp is a vector whose values denote the conditional probability assignment associated with structure Bs. Based on these assumptions Cooper et al [11] derive equation 2 as given below for the computation of P(Bs, D).

(2)    TABLE I.  PARAMETERS USED IN EQUATION 2 [11]  c : P(Bs), Prior probability of the structure.

i : Set of parents of node xi.

qi : | ?i |.

?i : List of all possible instantiations of the parents of  xi in the database D.

Ri : | Vi |.

Vi : List of all possible values of the attribute xi Nijk : Number of cases in D in which the attribute xi is  instantiated with its kth value and the parents of xi in i are instantiated with the j  th instantiation in ?i.

Nij : The number of instances in the database in which the parents of xi in i are instantiated with the j  th instantiation in ?i.

Table 1 above describes each of the parameters used in equation 2. To simplify the computation, Cooper et al. [11] assumes that the prior probability of all the networks is the same. This means that all the networks are equally likely; therefore to maximize P(Bs, D) it would be enough to find the parent set that maximizes the second inner product of equation 2. Therefore to increase the computational complexity of the algorithm, it is assumed that we are initially provided with an ordering of the nodes such that if node A precedes node B in the ordering then:       (3)  There will not be an edge from A to A in the graph.

Secondly a limit on the number of parents of any given node may be applied. And lastly P( i xi) and P( j xj) are independent when i  j. Using these assumptions decreases the computational complexity and makes the algorithm feasible.

K2 begins with the first node in the ordering and assumes that it has no parents, and then incrementally adds those parents which maximize the probability of the resulting structure. The algorithm stops when the addition of parents does not lead to further increase in probability. The following function, which is the inner product from equations 2 and 3, is used in the algorithm [11] since this is the product we want to maximize.

Figure 2 that follows is the K2 algorithm proposed by Cooper et al. The algorithm uses another function Pred(xi) that returns the nodes which precede xi in the node ordering.

Figure 2.  The K2 Algorithm [11]  The K2 algorithm shown in Figure 2 constructs the structure of the Bayesian Network for us. Next the conditional probability table needs to be learned for the data. Let ijk denotes the conditional probability P(xi=vik | i = wij) i.e. the probability that xi has value v for some k from 1 to ri, given that the parents of x are instantiated as wij. Then taking into account of the four assumptions mentioned earlier, the expected value of ijk is given by [11]:    (5)    For a detailed discussion on the derivation of the equations used in K2, please refer to the paper by cooper et al [11].



III. SUBGROUP DISCOVERY  ?The main application of subgroup discovery is exploration and descriptive induction, where the user wants to find the overview of dependencies between a target and many explaining variables? as pointed out by Atzmueller [12].

Further Atzmueller points out in [12] that ?the deviations of the subgroup from the performance of the general population are usually not simply due to statistical fluctuations, but are caused by local factors. Identifying these factors helps to understand the data in general and has a huge impact, e.g., on diagnostic, preventive or therapeutic issues concerning medical questions.

Thus subgroup discovery has become more important in the medical domain lately.?  The three main components of a subgroup discovery algorithm are the description language, the quality function and the search strategy. The description language is the way we represent the discovered subgroups. Mainly conjunctive languages are used. The quality function is used to evaluate the interestingness of the subgroups. A number of quality functions have been proposed e.g. the binomial test proposed by Klosgen et al [8] and the ratio of the true positives and false positives as proposed by Gamberger et al [5]. The search strategy defines how we go about discovering the subgroups. Commonly a beam search strategy is used because of its efficiency [8].

Subgroup Discovery is a task at the intersection of Predictive and Descriptive data mining. Subgroup Discovery is defined as: ?Given a population of individuals and a specific property of individuals that we are interested in, find population subgroups that are statistically ?most interesting?, e.g. are as large as possible and have the most unusual distributional characteristics with respect to the property of interest [8].?  In subgroup discovery we want to uncover properties of selected target population. Target population is the part of the population with a given property of interest (class) [4]. We need labelled data to identify the target population, and therefore in this sense subgroup discovery is similar to supervised learning. But subgroup discovery is also similar to descriptive data mining in the sense that we want to uncover individual interesting patterns in Data [4].

(4)  Procedure K2;  Inputs:  A set of n nodes.

An ordering on the nodes.

An upper-bound u on the number of parents of a node.

A database D containing m cases.

Output: For each node, a list of the parents of the nodes   For i = 1 to n do xi = ?; Pold = g(i, i); (Computed using equation 4) OKToProceed = true; While OKToProceed and | i| < u do Let z be the node in Pred(xi) - i that maximizes  g(i, i ?{z});  Pnew = g(i, i ?{z}); If Pnew > Pold then Pold = Pnew;  i = i ? {z}; Else OKToProceed = false; End (while) Write (?Node:?, xi, ?Parents of this node: ?, i) End (for); End (K2);     In subgroup discovery we focus on discovering subgroups, which are represented by rules, which are simple and cover large proportions of the training data. The simplicity of the rules makes them more understandable and actionable [5].



IV. RELATED WORK IN SUBGROUP DISCOVERY  A number of methods have been proposed for the task of subgroup discovery. In this section we list down some of the methods proposed so far. In addition to these we will briefly discuss CN2-SD and Apriori-SD for two reasons. The first reason is that both of them are based on classification techniques, and secondly we will be using parts of the methods in our proposed scheme.

The first method proposed for subgroup discovery was EXPLORA by Klosgen [8]. Decision trees are used to extract rules in this scheme. Criteria such as evidence, generality, redundancy and simplicity are used to measure the interestingness of subgroups. MIDOS proposed by Wrobel [9] was another method used for subgroup discovery by applying EXPLORA to multi-relational databases. The goal is to discover subgroups of the target relation which have unusual statistical distributions with respect to the complete population.

The quality measure uses a combination of unusualness and size.

SubgorupMiner was proposed by Klosgen [13] by extending EXPLORA and MIDOS. This method uses decision rules and interactive search in the space of solutions. It caters for the use of very large databases through database integration.

This algorithm uses binomial test as the quality function to find out how different a discovered subgroup is from the original population based on the statistical distribution.

SD proposed by Gamberger et al [14] is a method that uses expert guidance instead of the quality function to judge the interestingness of subgroups. SD-Map proposed by Atzmueller [10] is an exhaustive subgroup discovery algorithm. It uses the GP-Growth method for the mining of association rules with adaptations for the purpose of subgroup discovery.

The CN2-SD proposed by Lavrac et al [4] is a subgroup discovery method developed by modifying the CN2 classification rule learner. The CN2 algorithm consists of two main procedures. The bottom level search procedure performs beam search in order to find a single rule, and the top level control procedure repeatedly executes the bottom level search to induce a rule set. The bottom level search procedure uses classification accuracy as a heuristic function [4]:     (6)    Two different top level control procedures may be used in CN2. Procedure 1 induces an ordered list of rules while Procedure 2 induces an un-ordered list of rules. The difference between the two procedures is that procedure 1 removes all covered examples from the training set before each new iteration while procedure 2 removes examples belonging to that class instead of removing all covered examples.

To adapt CN2 for the purpose of subgroup discovery the author suggests the use of a weighted covering algorithm.

Covered positive examples in this case are not deleted from the current training set. Instead, a count is maintained of how many times the example has been covered. This count is used to assign weights to each example according to a weighting scheme. And then the weights are used in computing the search heuristics of weighted relative accuracy to give a quality function that is feasible for the purpose of subgroup discovery.

Two example weighting schemes have been described [4].

One that uses additive weights:  (7)    And the other one which uses multiplicative weights:  , (8)    where i in both the equations refers to the number of times an example has been covered. The weighted relative accuracy of a rule [4] is given by the following equation:     (9)  where P(Cond) is used as a weight to trade off generality of a rule while P(Class|Cond) ? P(Class) is the relative accuracy which tells us how statistically different a subgroup is from the original population. The example weights are used in the weighted relative accuracy search heuristic to provide us with the quality function to evaluate subgroups:    (10)    where  is the sum of weights of all examples, (Cond) is the sum of the weights of all covered examples and  (Class Cond) is the sum of the weights of all correctly covered examples.

The Apriori-SD [7] algorithm proposed by Lavrac et al uses the same weighted relative accuracy search heuristic to judge the quality of subgroups along with a threshold value k. This value determines the number of times an example is covered by rules before it is removed from the training set. The algorithm first of all generates classification rules using the Apriori-C algorithm. It then applies the quality function to find the best rule. It then modifies the weights of the covered examples, removes the rule from the initial rule set and includes the rule in the result set. The same procedure is repeated until either the rule set becomes empty or training set becomes empty.

Figure 3.  Architecture of the Proposed Scheme

V. PROPOSED SCHEME  The scheme that we are proposing here is similar in structure to the Apriori-SD algorithm. We propose using the same weighted relative accuracy heuristic as used in both CN2- SD and Apriori-SD. Our scheme differs from these algorithms in the way that rules are generated as an additional advantage for understanding the subgroup relations. The overall architecture of the proposed scheme is presented in Figure 3.

Our proposed scheme consists of three main steps. Step 1, the learning of the Bayesian Network using the K2 algorithm.

Step 2, the generation of rules from the Bayesian Network and lastly ordering the rules according to their quality. And step 3, ordering the rules.

Step 1. Generation of Bayesian Network: A Bayesian Network is generated by using the K2 algorithm.

Step 2. Rule Set Generation: Rules are generated from the Bayesian network that was constructed in Step 1. From the Bayesian network all connected pairs of nodes are picked up, and each pair will be used to generate rules e.g. we get the following pair of nodes:   Figure 4.  Example pair of nodes from a hypothetical Bayesian network.

Furthermore we assume that the node (attribute) ?Tired? can take on two values ?Yes? and ?No? and ?Work Hours? can also take on two values ?>5? and ?<5?. Since each node can take on 2 values, there are four possible rules that can be generated as the example shown below:  Rule 1: Work Hours < 5 then Tired = ?Yes? Rule 2: Work Hours > 5 then Tired = ?Yes? Rule 3: Work Hours < 5 then Tired = ?No? Rule 4: Work Hours > 5 then Tired = ?No?    In this way we will generate rules for all pairs of connected nodes. The list that we get will be called 1-List since it contains only one condition in the antecedent. Now we can use the 1- List to merge rules which have the same consequent and non- conflicting antecedants. In this way we can acquire 2-List and so forth. At the end of this step we will obtain a list of rules representing subgroups of the population. Now we want to rank and sort these rules by their qualities.

Step 3. Ordering the Rules: Once we have the rules list, the rules can then be ordered according to their qualities. The steps to order the rules are as follow: First apply the Weighted relative accuracy search function on all the rules. Secondly, pick the rule with the highest value. Then remove the rule from the rule list and include it in the result set. Lastly modify the weights of all the covered examples. Repeat this process until the whole rule list is traversed. Alternatively if we want only the Top-K subgroups then we will repeat these steps K times.

At the end we will get an ordered list of rules in the result set.



VI. DEMONSTRATION BY EXPERIMENTAL DATASET  A. Generation of Bayesian Network A Bayesian Network was generated by using the  ?experimenter? application in Weka (Waikato Environment for Knowledge Analysis) ? a non-commercial Data Mining Software in Java, by The University of Waikato, New Zealand.

A small sample weather dataset that has 14 records, 4 attributes and 1 class is used in the experiment. The same dataset has been popularly used as illustrative examples in numerous data mining textbooks. The following steps are to generate the Bayesian Network:  Tired Work Hours            Figure 5.  Bayesian Network derived from the Weather Dataset  B. Generation of rules from the Bayesian Network For each pair of connected vertices, rules are generated.

From the above Bayesian Network the following rules were generated, as shown from Table 2 to Table 6.

For simplicity we have omitted the rule merging step as that would make the rule list extremely large. The objective here is to validate our concept. So we have restricted the rule size. But the rule merging step can be incorporated in the actual scheme.

TABLE II.  RULES FROM PLAY  OUTLOOK  Play = ?Yes?  Outlook = ?Sunny?  Play = ?No?  Outlook = ?Sunny?  Play = ?Yes?  Outlook = ?Overcast?  Play = ?No?  Outlook = ?Overcast?  Play = ?Yes?  Outlook = ?Rainy?  Play = ?No?  Outlook = ?Rainy?  TABLE III.  RULES FROM PLAY  TEMPERATURE  Play = ?Yes?  Temperature = ?Hot?  Play = ?No?  Temperature = ?Hot?  Play = ?Yes?  Temperature = ?Mild?  Play = ?No?  Temperature = ?Mild?  Play = ?Yes?  Temperature = ?Cool?  Play = ?No?  Temperature = ?Cool?  TABLE IV.  RULES FROM PLAY  WINDY  Play = ?Yes?  Windy = ?True?  Play = ?No?  Windy = ?True?  Play = ?Yes?  Windy = ?False?  Play = ?No?  Windy = ?False?  TABLE V.  RULES FROM PLAY  HUMIDITY  Play = ?Yes?  Humidity = ?High?  Play = ?No?  Humidity = ?High?  Play = ?Yes?  Humidity = ?Normal?  Play = ?No?  Humidity = ?Normal?  TABLE VI.  RULES FROM TEMPERATURE  HUMIDITY  Temperature = ?Hot?  Humidity = ?High?  Temperature = ?Mild?  Humidity = ?High?  Temperature = ?Cool?  Humidity = ?High?  Temperature = ?Hot?  Humidity = ?Normal?  Temperature = ?Mild?  Humidity = ?Normal?  Temperature = ?Cool?  Humidity = ?Normal?   C. Calculation of Weighted Relative Accuracy for each rule The weighted relative accuracy of each rule is calculated  using (10) given above. The additive weighting scheme proposed by the Lavrac [4] is used, w(ej, i)=1/(i+1) where i is the number of times a record of the dataset is covered. After each round of iteration, the weights of covered examples are adjusted by using this scheme. The weighted relative accuracies of the rules are listed below, from Table 7 to Table 11. The last rule in Table 10 has the highest weighted relative accuracy value and therefore this rule is removed from this list and added to the list of subgroups.

1. Open the ?Explorer? application in Weka 2. Open the Dataset File.

3. Select all the attributes.

4. Go to the ?Classify? Tab 5. Under the ?Classifier? heading click  ?choose? 6. In the window that opens choose:  Classifiers Bayes Bayesnet 7. Set the following options:  ? initAsNaiveBayes: False ? maxNrOfParents: 10000 ? markovBlanketClassifier: True ? randomOrder: False ? Leave all other options as they are  8. Click on start to generate network.

9. Visualize the generated the graph.

TABLE VII.  WRACC OF RULES FROM PLAY  OUTLOOK  Play  Outlook WRAcc  Play = ?Yes?  Outlook = ?Sunny? -0.086734694  Play = ?No?  Outlook = ?Sunny? 0.086734694  Play = ?Yes?  Outlook = ?Overcast? 0.102040816  Play = ?No?  Outlook = ?Overcast? -0.102040816  Play = ?Yes?  Outlook = ?Rainy? -0.015306122  Play = ?No?  Outlook = ?Rainy? 0.015306122  TABLE VIII.  WRACC OF RULES FROM PLAY  TEMPERATURE  Play  Temperature WRAcc  Play = ?Yes?  Temperature = ?Hot? -0.040816327  Play = ?No?  Temperature = ?Hot? 0.040816327  Play = ?Yes?  Temperature = ?Mild? 0.010204082  Play = ?No?  Temperature = ?Mild? -0.010204082  Play = ?Yes?  Temperature = ?Cool? 0.030612245  Play = ?No?  Temperature = ?Cool? -0.030612245  TABLE IX.  WRACC OF RULES FROM PLAY  WINDY  Play  Windy WRAcc  Play = ?Yes?  Windy = ?True? -0.06122449 Play = ?No?  Windy = ?True? 0.06122449 Play = ?Yes?  Windy = ?False? 0.06122449 Play = ?No?  Windy = ?False? -0.06122449  TABLE X.  WRACC OF RULES FROM PLAY  HUMIDITY  Play  Humidity WRAcc  Play = ?Yes?  Humidity = ?High? -0.107142857 Play = ?No?  Humidity = ?High? 0.107142857 Play = ?Yes?  Humidity = ?Normal? 0.107142857 Play = ?No?  Humidity = ?Normal? -0.107142857  TABLE XI.  RULES FROM TEMPERATURE  HUMIDITY  Temperature  Humidity WRAcc  Temperature = ?Hot?  Humidity = ?High? 0.071428571 Temperature = ?Mild?  Humidity = ?High? 0.071428571 Temperature = ?Cool?  Humidity = ?High? -0.142857143 Temperature = ?Hot?  Humidity = ?Normal? -0.071428571 Temperature = ?Mild?  Humidity = ?Normal? -0.071428571 Temperature = ?Cool?  Humidity = ?Normal? 0.142857143      D. Weights Adjustment The weights of the covered examples are adjusted using the  additive weighting scheme. Tables 12 and 13 show the initial weights and the weights after the first iteration respectively.

TABLE XII.  INITTIAL DATASET WEIGHTS  Outlook Temp Humidity Windy Play Weight  1 sunny hot High FALSE No 1  2 sunny hot High TRUE No 1  3 overcast hot High FALSE Yes 1  4 rainy mild High FALSE Yes 1  5 rainy cool normal FALSE Yes 1  6 rainy cool normal TRUE No 1  7 overcast cool normal TRUE Yes 1  8 sunny mild High FALSE No 1  9 sunny cool normal FALSE Yes 1  10 rainy mild normal FALSE Yes 1  11 sunny mild normal TRUE Yes 1  12 overcast mild High TRUE Yes 1  13 overcast hot normal FALSE Yes 1  14 rainy mild High TRUE No 1  TABLE XIII.  DATASET WEIGHTS AFTER FIRST ITERATION  Outlook Temp Humidity Windy Play Weight  1 sunny Hot High FALSE No 1  2 sunny Hot High TRUE No 1  3 overcast Hot High FALSE Yes 1  4 rainy mild High FALSE Yes 1  5 rainy cool Normal FALSE Yes 0.5  6 rainy cool Normal TRUE No 0.5  7 overcast cool Normal TRUE Yes 0.5  8 sunny mild High FALSE No 1  9 sunny cool Normal FALSE Yes 0.5  10 rainy mild Normal FALSE Yes 1  11 sunny mild Normal TRUE Yes 1  12 overcast mild High TRUE Yes 1  13 overcast Hot Normal FALSE Yes 1  14 rainy mild High TRUE No 1   The shaded rows in Table 11 indicate the covered examples.

You can see that their weights have been adjusted. This marks the end of the first iteration. We have identified out first subgroup: Temperature = ?Cool?  Humidity = ?Normal?.

The process repeats until the required number of subgroups is found.



VII. CONCLUSION AND FUTURE WORK  Subgroup discovery is referring to discovery of relations among attributes and predicted classes. This problem has been generally studied in the past with techniques enabled by statistics, datamining and human experts. It is considered as a hybrid between predictive modeling and descriptive analysis, where subgroups of similar data are formed pertaining to predicting a particular target class. In this paper, we proposed a novel method for subgroup discovery by using Bayesian network. By this method, probabilities of likenesses for each relation can be explicitly calculated (that has an advantage of white-box modeling) and each subgroup can be represented by rules that are readily interpretable for modeling the relations.

We verified this method by using a small example and reasonable results are produced preliminarily. In the future, we would be working on constructing a software prototype that implements this new scheme. We will be putting it to work on actual datasets in large volume so that their results can be further evaluated and compared against other subgroup discovery algorithms for benchmarking. We are also working on evaluating the performance of various quality functions (other than WRAcc) for the purpose of subgroup discovery.

Considering the good results that Bayesian network has achieved in classification, it is believed that it can yield good results in subgroup discovery scheme as well.

