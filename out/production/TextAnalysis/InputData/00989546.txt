Mining Mutually Dependent Patterns  Sheng Ma and Joseph L. Hellerstein

Abstract-In some domains, such as isolating problems in computer net- works and discovering stock market irregularities, there is more interest in patterns consisting of infrequent, but highly correlated items rather than patterns that occur frequently (as defined by minsup, the minimum sup- port level). Herein, we describe the m-pattern, a new pattern that is defined in terms of minp, the minimum probability of mutual dependence of items in the pattern. We show that all infrequent m-pattern can be discovered by an efficient algorithm that makes use of: (a) a linear algorithm to qualify an m-pattern; (b) an effective technique for candidate pruning based on a necessary condition for the presence of an m-pattern; and (e) a level-wise search for m-pattern discovery (which is possible because m-patterns are downward closed). Further, we consider frequent m-patterns, which are defined in terms of both minp and minsup. Using synthetic data, we study the scalability of our algorithm. Then, we apply our algorithm to data from a production computer network both to show the m-patterns present and to contrast with frequent patterns. We show that when minp= 0, our al- gorithm is equivalent to finding frequent patterns. However, with a larger minp, our algorithm yields a modest number of highly correlated items, which makes it possible to mine for infrequent but highly correlated item- sets. To date, many actionable m-patterns have been discovered in produc- tion systems.



I. INTRODUCTION Data mining aims to discover useful patterns in large data sets.

To date, much emphasis has been placed on finding frequent pat- terns in the setting of the market basket data[2][3]. Market bas- ket data consists of transactions, each of which is a set of items purchased together. An association rule relates two itemsets, E1 and Ez. Such a rule indicates that E1 is strongly dependent on E2, which is denoted by El + Ez. Directly discovering all such rules is computationally intractable because the large number of possible itemsets. To avoid this difficulty, Agrawal et.al.[2] pro- posed finding all frequent association rules. Here, ?frequent? places an additional requirement, and enables the two-step al- gorithm [2][4] for discovering all frequent itemsets followed by (exhaustively) computing association rules of the frequent item- sets. The efficiency gain is achieved by exploring the down- ward closeness property of the frequent itemsets. However, this diminishes quickly when searching for infrequent itemsets, in which the minimum support minsup has to be set very low.

Some real world applications requires the discovery of infre- quent, but highly correlated patterns. CohenIS] discussed this and analyzed pair-wise patterns for news articles and web logs.

Other such applications include problem detection in computer networks, intrusion detection in computer systems, and fraud detection in financial systems. In these applications, normal be- havior dominates; abnormal behavior, such as failures and in- trusions, is rare. Thus, it is interesting in discovering infrequent patterns that relate to problem situations. Consider the exam- ple in [ 101 in wliich the following events are generated from a router: network interface card failure, unreachable destination, and ?cold start? trap. The last event indicates that the router has failed and restarted. If these events commonly occur together, then the first two events may provide advance warning of when  the third will occur.

One naive approach to mining for infrequent patterns is to  discover all frequent patterns (e.g., using A-priori) with very low minimum support (minsup) ,  and then use post-processing to discover significant patterns. However, this is impractical.

Specifically, to find infrequent patterns, the minimum support in A-priori must be set very low, which in turn results in long processing time as well as many irrelevant patterns that must be examined. Irrelevant patterns (i.e. pattern occurring sim- ply by chances) are particularly problematic in real applications with a wide disparity in the frequency of items. As an exam- ple in event data, some events have a very high probability of occurrence, such as the ?connection closed? events issued by network hubs that support the dynamic host configuration pro- tocol (DHCP). As a result, there are a large number of patterns that have ?connection closed? in them even though this event has little correlation with the rest of events.

The foregoing motivates us to develop the m-pattern. An m-pattern is an itemset for which any two subsets are strongly dependent with each other characterized by a minimum depen- dence probability of at least m i n p  (a user-specified parameter), where 0 5 m i n p  5 1. That is, if some items in an m-pattern occurs, then it is very likely that the other items in the item- set occur as well. Here, m i n p  plays a similar role to that of mincon f, but requires for any two subsets.

An m-pattern differs from a frequent association rule[2][4][3] and the correlated pattern[7] in three major aspects.

An m-pattern requires mutual dependence. An association rule only requires one-way dependence. For example, an as- sociation rule might specify either diaper + beer or beer + diaper; while an m-pattern is present only if diaper e beer.

A correlated pattern refers to an itemset whose items are not in- dependent determined by a statistical test. Thus, i t  is a much ?weaker? correlation compared to both association rules and m- patterns. However, because of this, i t  is not feasible to discover infrequent patterns. We note that our m-pattern definition ex- cludes the situation that some two subset pairs, but not all, of an itemset are strongly dependent. This strong dependence require- ment provides many nice properties that enables us to efficiently discover all m-patterns, even if their supports are very low.

An m-pattern requires that any two subsets of the itemset be mutually dependent. Neither an association rule nor a correlated pattern has such a property. As we will show later, such a prop- erty enables directly, efficiently discovering all m-patterns.

An m-pattern does not have a condition of minimum support (although this can be added). This makes i t  possible to find all infrequent m-patterns. In contrast, only frequent associa- tion rules and correlated rules can be efficiently discovered in practice.

To provide more insight into m-patterns and how they differ  0-7695-1 119-8/01 $17.00 0 2001 IEEE 409    TI: (a. b. c. d. e 1, 9) R: (d, r,9) T3: (a. b. d. 9) TI: (a. d. 9) TS: (f, g 1 T6: (e, 1.9) V: {e, t )  Fig. 1 .  Illustrative transaction data  from frequent associations, an example is presented. Figure 1 displays data for transactions that are listed in the left side of the figure. The i-th transaction is denoted by Ti,  and symbols a through g represent items. Also, there is a table that displays support levels for patterns of length one and another table for patterns of length two. For example, the support level for {a, b} is 2, since a and b are contained in TI  and T3. Frequent itemsets have a number of occurrences that exceeds minsup. For exam- ple, let minsup = 3. Then, {a,  d }  and {a ,  g} are frequent item- sets since both have a support level of 3. In contrast, m-patterns are defined by mutual dependence using the threshold minp. As we show later, if minp = 0.5, then { a ,  b} is an m-pattern, but {a ,g }  is not. Intuitively, since g occurs in every transaction, g + a, although {a ,  g} occurs frequently and a + g.

How can we use m-patterns? As we will show in Section 5, m-patterns are common in event data as a result of the exten- sive interdependencies introduced by logical and physical inter- connections of network elements. These dependencies manifest themselves as a set of correlated events (or alarms) when a prob- lem arises. For example, when a link of a local area network (LAN) goes down, all hosts connected to the link generates "lost connection" events. Clearly, it is important to quickly pinpoint problem causes before there are wide spread service disruptions.

Discovering an m-pattern may provide a way to characterize or anticipate problem situations. We refer to [IO] for more detailed explanations.

To efficiently discover all m-patterns for given minp, we need to resolve two non-trivial issues. First, the definition of m-pattern requires any two subsets of an itemset to be mutually dependent. On the surface, checking whether an itemset is a valid m-pattern requires exponential computational complexity in respect to the size of pattern. This makes it very inefficient.

Second, the search space is exponentially large in respect to the number of distinct items. Examining and qualifying each of the candidate is computationally intractable. This is the same issue as that of discovering all frequent itemset.

Herein, we resolve these efficiency issues by exploiting spe- cial properties of m-patterns. We demonstrate analytically that examining a small number of two subset pairs is sufficient for qualifying an itemset as a m-pattern. This results in a simple linear algorithm. Further, we show that an m-pattern has the downward closure property, and thus it can be searched effi-  ciently through a level-wise search strategy. Last, we establish a necessary condition for qualifying m-patterns. This necessary condition provides an upper bound that can be used to further prune candidates effectively.

To explore the relationship with frequent patterns, we define frequent m-patterns in terms of both minp and minsup. We show that when minp= 0, frequent m-patterns reduce to fre- quent patterns; While, with a larger minp, our algorithm yields a modest number of highly correlated items, which makes it possi- ble to mine for relatively infrequent but highly correlated item- sets. Consequently, we can explore the tradeoff between the strength of mutual dependence and the occurrence frequency of patterns.

A. Related work  Much prior work is relevant to this paper. Agrawal et.a1.[2][4][3] developed an elegant algorithm called Apriori that finds all frequent patterns in a level-wise manner. Since then, many techniques have been proposed to improve the algorithm's efficiency by minimizing the number of data scans needed, memory required, etc [1][6][18]. In this paper, we propose mu- tual dependence patterns. We demonstrate that m-patterns can be discovered, even if they occur infrequently.

Much work has been done for finding different types of pat- terns (e.g., [16], [ l l ] ,  [5], [12]). However, most these efforts require that frequent itemsets be discovered first; rules are de- rived in a separate post-processing step. As discussed before, this approach cannot find infrequent patterns without examining a huge number of candidates.

Closely related to our work, Cohen et a1.[8] discussed the needs for discovering infrequent, but highly correlated item- sets. However, they defined a symetric similarity measure of two items and their algorithm can only discover itemsets containing only two items. Herein, we define the mutual depedency of an itemset with any length. We develop an efficient level-wise algo- rithm that discovers all infrequent, but highly correlated patterns with any length. Brin et.a1.[7] defined correlated patterns, i.e.

itemsets that are not independent based on a chi-squared test.

The condition used for a correlated pattern is rather weak in that correlated patterns are upward closed. That is, any super set of a correlated pattern is a correlated pattern. To avoid a potential explosion of patterns, the authors focus on frequent, correlated patterns, as specified by minsup. In contrast, m-patterns cap- ture mutual dependency without using minsup by using minp.

As we show later, m-patterns are downward closed, which al- lows for the efficient discovery of infrequent m-patterns.

B. Our contributions  Our main contributions can be summarized in the following.

( 1 )  We define a new type of patterns called m-pattern that cap- tures the mutual dependence of items. We demonstrate that the major advantage of m-pattern is its ability to discover infre- quent, but mutually dependent patterns, We demonstrate empir- ically the significance (e.g. actionable, interesting, unexpected) of m-patterns in system management applications ( 2 )  We develop an efficient algorithm that can check whether an itemset is a qualified m-pattern in a linear time. This is not obvious because the definition of m-patterns requires that any     two subset pairs are dependent with each other. This leads to exponential computational complexity in respect to the size of a patterns.

(3) We derive an upper bound property of an m-pattern. This can be used to effectively eliminate the search space.

(4) We derive an algorithm for efficiently discovering all m- patterns. Our algorithm integrates (2) and (3) above with the well-known level-wise search strategy.

(5 )  We show that the m-pattern condition defined by m i n p can be used together with the Occurrence frequence defined by minsup.  This allows to explore the tradeoff between the strength of mutual dependence and the occurrence frequency of patterns.

C. Organization of this paper The remainder of this paper is organized as follows. Section 2  defines m-patterns and related concepts. Section 3 presents our algorithm for finding m-patterns. Section 4 discusses extensions to the basic algorithm to handle frequent m-patterns. Section 5 provides empirical assessments of our algorithms. Our conclu- sions are contained in Section 6.

11. DEFINITIONS OF M-PATTERNS AND RELATED CONCEPTS  This section formalizes m-patterns and related concepts. We begin by generalizing the notion of a qualified pattern, a concept that is usually associated with support levels. We then specify what is meant by mutual dependence, a concept that we formal- ize in terms of the empirical distribution of the transaction data to be mined. With this background, we then define m-patterns.

We begin by generalizing what is meant by a qualified pat- tern. Let I be a collection of distinct items, and let the itemset E be a non-empty subset of I. As usual, the transaction data, D = { D i } E 1 ,  is a collection of N transactions, where the i-th transaction D ,  c I.

We define a qualification function ~ D ( E )  to be fD : E + { t rue , fa l se} ,  where fu(E) = t rue  iff E is a qualified f-pattern in D .  For example, let f u  be the qualification func- tion for a frequent pattern with minimum support level minsup.

Then, the qualification for frequent pattern:  fo(E) = true iff suppor tu (E)  2 minsup ,  (1)  where suppor tu (E)  = I{DI E DIE Dt}I is the number of occurrences of E in data D. Is1 represents the cardinality of a sets.

Now we formalize what is meant by dependence (known as rule confidence in association rules) and mutual dependence.

Let E1 and E2 be two itemsets. El + E2 is the union of E1 and Ez. The empirical probability of the occurrence of E is defined by  pu(E) = s u p p m t ~ ( E ) / N ,  (2) where N is a normalizationconstant. For the market basket data, N is the number of transactions in D. Under a mild assumption, the empirical probability PD( .) approaches the real probability P( . )  as the number of observations becomes large.

The dependence of itemset El on itemset E2 is quantified by the empirical conditional probability. Denoted by PD(E1 I Ez) , the empirical conditional probability is computed by  Using Equation 2, we can further obtain  The above equation provides another interpretation of the con- ditional probability: the ratio of the number of transactions in which E1 and E2 occur together to the number of transac- tions in which E2 occurs. Clearly, a large PD(ElIE2) indi- cates that E1 depends more strongly on Ez. In an extreme case, PD(ElIE2) = 1 implies that whenever E2 occurs, E1 also occurs. Note that PD(E1IEz) characterizes a one-way de- pendence. In order to have a mutual dependence, we must also examine p~(EzlE1).

Definition I :  Two non-empty itemsets El, E2 are signifi- cantly mutually dependent for a given D and a minimum de- pendence threshold 0 < m i n p  < l, iff PD(EIIE~) 2 m i n p and PD(EzlE1) 2 minp.

With this background, we define an m-pattern to be an itemset for which any subset is significantly mutually dependent on all other subsets. That is, if any subset of an m-pattern is present, the remaining items occur with high probability. This is formal- ized as follows:  Definition2: A nonempty itemset E is an m-pattern with minimum mutual dependence threshold minp ,  iff  PD(ElIE2) 2 m i n p  ( 5 )  holds for any non-empty two subsets E1 and E2 of E, where 0 5 m i n p  < 1.

Note that this definition does not refer to a support level.

Thus, unlike frequent associations, m-patterns will be discov- ered regardless of the frequency of their occurrences. However, as we note later, it is quite reasonable to consider frequent m- patterns that use both m i n p  and minsup.

We now return to the example in Figure 1 to illustrate why { a , d }  is an m-pattern but {a ,g }  is not assuming m i n p  = 0.5. M-patterns are defined by mutual dependence, which re- quires that all pairs of subsets be significantly mutually depen- dent above the m i n p  threshold. For the itemset E = {a, b}, we compute two conditional probabilities: Po( (a} l (b})  =  2 / 2 .  Therefore, {a ,  b} is a m-pattern with m i n p  = 0.5. How- ever, {a ,g }  is not an m-pattern since PD({a}I{g})  = 3/7 < 0.5.

suppmtD((a,  b } ) / s u p p ~ t D ( { a } )  = 2/3 and PD({b)l{a})  =  111. ALGORITHM FOR MINING M-PATTERNS  This section develops an efficient algorithm for discovering all m-patterns. Efficiency is obtained by addressing three is- sues: (1) how to test (qualify) that an itemset is an m-pattern; (2) how to exploit a level-wise search; and (3) how to prune the search space using a necessary condition for the presence of an m-pattern.

A. ESJiciently Qualifying an M-Pattern  The definition of m-patterns (Equation 5) can be used to test whether an itemset is an m-pattern. However, doing so means computing the pairwise conditional probability of all subsets of a candidate m-pattern. Assuming that El, E2 are disjoint and observing that these sets are chosen so that an item is in at most one, then the number of conditional probabilities that must be computed is 0(3'),  where k is the length of E. (If El, E2 are not disjoint, the computational complexity is 0(4'").) This means that the direct application of the definition to qualify an m-pattern is computationally intractable even for modest sized values of k .

To qualify an m-pattern efficiently, we derive the follow- ing equivalent definition of a m-pattern. Roughly speaking, it shows that we just need to compute a linear number of condi- tional probabilities rather than to compute all pairwise condi- tional probabilities of m-pattern subsets.

Property I :  (Equivalent definition of m-patterns) An itemset E is an m-pattern as in Definition 2, iff  PD(E - {a}l{a}) L minp (6)  for every item a in E.

Proof: We first prove the necessary condition. That is, we  must show that if E is an m-pattern, then PD(E - { a } ~ { u } )  2 minp for any item a E E. But this follows by letting E1 = E - {a}, E2 = { U } ,  and using Definition 2.

Now, we prove that if PD(E - {a} l {a} )  2 minp for any a E E, then E is m-pattern. Let El and E2 represent any two non-empty subsets of E. Let a E Ea. Since support~({a}) 2 support~(E2), we obtain Po({a})  2 by using Equa- tion 2. Similarly, since E1 + E2 is a subset of E, Po (El + E2) 2 Po (E). Therefore, we obtain  + E~)/PD(E~) Z PD(E)/PD({~}) (7) = PD(E - { u } l { u } )  2 minp.

Since E1 and E2 are any two subsets of E, we have proven that E is an m-pattern by Definition 2. 0  The above property allows us to qualify an m-pattern in linear time O(k) .  The specifics are described in the algorithm below:  Algorithm: isMPattern Input: itemset E, the supports for E, the support for each item in E, and minp Output: {true, false} (I) For each item a in E; (2) If SupportD(E)/SuppurtD({a}) < minp (3) Return false (4) End for (5 )  Return true  B. Level-Wise Searching  Now, we develop an efficient algorithm for discovering all m- patterns. Similar to frequent itemset discovery, the number of all potential m-patterns is huge, on the order of n '", where n is the number of distinct items, and k is the maximum length of an itemset. It is not uncommon that n can to be 1,000 or more.

Thus, it is computationally intractable even for modest sized k .

We note that a downward closure (See [7] for more discussion of downward closure.) can lead to an efficient search algorithm.

Now, we demonstrate that m-patterns have such a property.

Definition 3: An itemset E is said to be downward closed un- der a qualification function fo( .) if the following is true: if the qualification condition f ~ (  E) holds, then fo( E') holds for any E' c E.

Put differently, if ~D(E') does not hold for a subset E', then fo(E) cannot be true. With the downward closure, we can greatly reduce the number of itemsets that must be exam- ined. For example, frequent itemsets are downward closed since support~(E) 5 supportD(E'). By taking advantage of this property, Agrawal et.a1.[2][3] devised a level-wise algorithm called A-priori that discovers all frequent itemsets with a sup- port of at least minsup. In specific, starting with k = 1, A- priori explores the candidate space iteratively in level wise by first finding all frequent itemsets of size k ,  and then using this knowledge to construct the candidates of size k + 1 based on the downward closure property. Since then, much work has been de- veloped to improve the algorithm by exploring different search strategies (see [1][6][9], and references therein). All these work is built on the downward closure property of frequent itemsets.

Herein, we first show the downward closure property of m- patterns. We then focus on the level-wise algorithm, although all other algorithms can be applied equivalently.

Property 2: (Downward closure property) M-patterns are downward closed.

Proof let E be an itemset and E' be a subset of E. We need to prove that if E is an m-pattern, then E' must be an m-pattern.

Let El and E2 be any two non-empty subsets of E'. Clearly, E1 and E2 are also subsets of E because E' C E. Since E is an m-pattern, we obtain PD(E1IE2) 2 minp (Equation 5).

Therefore, E' is an m-pattern. 0  Since m-patterns are downward closed, we use a level-wise algorithm for discovering all m-patterns.

Algorithm: DiscoverMPatterns Input: minp and data D Output: all qualified m-patterns { L k } (1 )  L1 = {{a} laE q;c2 = {{a,b}la ,bE I } (2) Scan D to count the occurrences of each pattern in L 1 and c2 (3) k = 2 (4) Compute the qualified candidate set: Lk = {U E ck lish/lPattern(v) = true} (5) Compute the new candidate set Ck+l based on Lk (6) If Ck+l is empty, output { L k }  and terminate (7) Scan D and count the occurrence of each pattern Y E Ckfl (8) k = k + 1; go back to (4)  We note that all itemsets with size 1 are qualified m-patterns by our definition. The set of candidate itemsets of size 2 are thus the set of all combinations of two items. Steps 1 and 2 treat this special situation. Then, the algorithm iteratively searches the candidate space in a level-wise manner. Step 4 finds all m- patterns of length k by using isMPattern. Step 5 constructs Ck+l, the candidate itemsets for level k + 1, based on L k .  the qualified m-patterns found in level k. This can be accomplished by the join operation followed by the pruning done in the A- priori algorithm [ 2 ] .  If no more candidates can be generated,     step 6 terminates and outputs all m-patterns found. Otherwise, Step 7 scans the data to count the occurrences of each candidate in Ck+l. Steps 4 through 8 are repeated until no more qualified candidates are found.

Implementing this algorithm requires keeping a counter for each candidate pattern in Ck. As each market basket in D is examined, the counter is increased by one if the candidate is a subset of the transaction. We refer to [2] for further implemen- tation details.

This algorithm needs k-1 data scans. The complexity of this algorithm is linearly dependent on the length of data, but is ex- ponentially dependent on the size of the longest m-pattern. In practice, the algorithm converges quickly, especially when pat- terns are not very long.

C. Pruning Candidate M-Patterns The most time-consuming step in the above algorithm is  counting the occurrences of candidates. Clearly, the smaller C k , the faster the algorithm. Therefore, we should prune candidates as much as possible before scanning the data and counting. Be- low, we show how this can be done by making use of a necessary condition for the presence of an m-pattern.

Property 3: (Upper bound property) Let E? be a non-empty subset of E .  Let an item a E E?. Then,  (2) PD(E - { ~ } ) / P D ( { . } )  5 PD(E - E ? ) / b ( E ? ) .

Proof: Recall that PD(E - {a } l {a } )  = PD(E)/PD({~})  (Equation3). Since PD(E) 5 P D ( E - { u } ) ,  the first conclusion holds. Further, since a E E?, PD(E - { U } )  5 PD(E - E?) and PD({u})  2 PD(E?), the second inequality holds as well. 0  The above property provides an upper-bound for the empirical conditional probability PD(E - {a } l {a } ) .  Moreover, it also proves that PD(E - {a))/P~({a)) is the tightest upper bound among possible upper bounds.

Using Property.3, we easily obtain a necessary condition for qualifying an m-pattern.

Property 4: Neeessary condition If E is an m-pattern with minp, then  (1) PD(E - {a } l {a } )  5 pD(E - { a } ) / p D ( { a ) ) .

supportu(E - {a})/supportD({a}) 2 minp, for any item a (8)  Proof This follows by combining Equation 5, 4, and Property 3. 0  We note that the above necessary condition only depends on the support of patterns found at level 1 and level k - 1 (this is different from Property 1 which is dependent on support (E) computed by an additional data scan.). Thus, this condition can be used to prune candidates in Step 5, thereby reducing the num- ber of candidates for which counting is done in Step 7. We sum- marize the pruning algorithm as follows.

Input: a set of candidate patterns Ck Output: a set of pruned candidate patterns (1) For each pattern E in Ck (2) For each item a in E (3)  Algorithm: Pruning  If supportD(E - {a})/supportD({a}) < minp (4) (5) Goto(,v;  Ck = Ck - E  (6) Endfor (7) End for (8) Return CI,  D. Algorithm for Mining M-Patterns Putting the above described algorithms together, we obtain Algorithm: DiscoverMPatternsWithPruning  Input: minp and data D Output: all qualified m-patterns { L k } (1 )  Li = { { a } l ~ E  I } ; C 2 = { { a , b } l a , b E I } (2 )  Scan D to count the occurrences of each pattern in L 1 and c2 (3) k = 2 (4) Compute the qualified candidate set: Lk = {v E CklisMPattern(v) = true} (5a)  Construct the new candidate set C k + l  based on L k  by the downward closure property (56) Prune C k + l  based on Property 4 using the Pruning algo- rithm.

(6) if C k + 1  is empty, output { L k }  and terminate (7) Scan D and count the occurrence of each pattern v E C k + l (8) k = k + 1; go back to (4)  We note that the level-wise algorithm for m-patterns and that for the frequent association have two common steps: construct- ing the next level candidate patterns based on the previously qualified patterns (Step 5a) and counting occurrences of candi- dates (Step 7). However, these algorithms differ in several ways.

In particular, our algorithm for m-pattern discovery: (a) requires a different treatment for the first and the second levels (Steps 1 to 3); (b) takes advantage of an extra pruning step (Step 5b); and (c) employs a different algorithm to qualify a candidate pattern (Step 4)  E. Extensions  Here, we consider a couple of extensions to our algorithm for discovering m-patterns.

First, note that the definitions and results thus far presented make no assumption about how transactions in D are obtained.

Thus, if items have a timestamp (e.g. temporal event data. See [15][ 141 for the detailed definition), transactions can be con- structed using windowing schemes as in[15]. Doing so allows us to discover temporal m-patterns I .

Second, we show how further performance gain can be ob- tained by partitioning items. We note that Property 3 can be used to partition the search space. To illustrate this, consider E = {a, b}. By the property 3, E may be an m-pattern, if both P u ( { a } ) I p ~ ( { b } )  2 minp and Pu({b))/Pu({a)) L minp.

By Equation 2,  we obtain  supportu({ b})*minp 5  support^( { a } )  5 supportD({ b})/minp.

(9)  Extending this to other items, we can partition items so that the above equation will not hold for items in two different partitions.

In this way, a potential m-pattern can only be a subset of items in one and only one partition. Consequently, the original problem is divided into several sub-problems, each of which relates to  ?Some cares are needed to deal with the overlapped windows. We refer to [ 151 for more implementation details.

one partition of items. This reduces the search space as we only need to consider candidates within a partition. Further, we can solve these sub-problems in parallel.



IV. FREQUENT M-PATTERNS  This section shows that the concepts of frequent itemsets and m-patterns can be combined to develop an algorithm that dis- covers frequent m-patterns. A frequent m-pattern is defined in terms of both minp and minsup. That is, a frequent m-pattern is significantly mutually dependent with the dependence thresh- old minp and has support threshold minsup.

How do frequent m-patterns compare with frequent associa- tion rules? A frequent association rule with the form E1 + Ez requires two conditions: ( I )  suppmt~(E1  + Ez) 2 minsup; and ( 2 )  PD(EzlE1) 2 mincon f. We note that the second con- dition does not have the closure property, and thus it is com- putationally intractable to consider (2) alone. In contrast, m- patterns can be discovered efficiently since downward closure holds. However, mutual dependency, which is required by m- patterns, is a stronger condition than association, as required by association rules.

We now formalize the notion of frequent m-patterns.

DeJinition4: E is said to be a frequent m-pattern with  minsup and minp, iff E is an m-pattern with minp, and the number of occurrences of E is no less than minsup.

We note that the frequent m-pattern is a more general pattern than the frequent itemset and the m-pattern. In that, when minsup = 0, the frequent m-pattern reduces to the m-pattern.

When minp = 0, the frequent m-pattern becomes the frequent itemsets.

We can mine all frequent m-patterns efficiently. The key insight is that frequent m-patterns are downward closed. We demonstrate this by showing a much stronger result. (1171 dis- cusses this in a slightly different form).

Property 5: (Conjunction and disjunction of downward closure properties) Let boolean functions f l ( . )  and f 2  (.) be two qualification functions of an item set such that fl(.) and fa(.) are both downward closed. Then, the qualification func- tion f,(E) A f2(E)  is downward closed, where ?A? represents the ?and? operation. And f l ( E )  V f z (E)  is also downward closed, V is the ?or? operation.

Proof: Let E be a nonempty itemset, E? C E. Assume that f l ( E )  A f z ( E )  holds. Since fl(E) and f z (E)  both hold, we know that both fl(E?) and fz(E?) are true since f l  and fa are downward closed. Therefore, we obtain f l  (E?) A fa( E?) is true.

Similarly, assume that f l  ( E )  V f~ ( E )  holds. Then, at least one of fl(E) and f i ( E )  is true and so f l (E?)  V fz(E?) is true. 0  The foregoing allows us to construct a level-wise algorithms for frequent m-patterns by modifying Step 7 of the m-pattern mining algorithms. For a frequent m-pattern, the qualified pat- terns at level k is Lk = {U E CklisMPattern(v) = true and supportu(v) > minsup}. The remaining steps of the al- gorithm are unchanged.

v. EXPERIMENTAL RESULTS This section assesses our algorithms for discovering m-  patterns. Two kinds of assessments are presented. The first eval- uates the performance of our algorithm using synthetic trans-  ,?  I I a 100 3 5 -  2- 2- a0 350 1- 1- - ss0  rnfNmb..d*.?.LDn.,? *-?..?d.

Fig. 2. Average nm time in second vs. the number of transactions in 1,ooO.

action data. The second studies our algorithm using real data collected from a production computer network.

A. Synthetic data  We begin by using synthetic data to study the scalability and efficiency of our algorithm for discovering m-patterns. The syn- thetic data are constructed by first generating items randomly and uniformly, and then adding instances of patterns into ran- domly selected transactions. Thus, the synthetic data are spec- ified by the following parameters: the number of transactions, the number of distinct items, the average number of random items per transaction, the number of patterns and their length, and the noise to single ratio (NSR). Here, the NSR for an item in a pattern is defined by the ratio between the number of ran- dom instances to the number of the item instances in the pattern.

Throughout, the number of distinct items is 1000; the number of patterns is I O  with length 5; the average number of random items in a transaction is 20; and the NSR is 5.

We assess scalability by varying the number of transactions.

We compare the level-wise algorithm for mining frequent pat- tern itemsets with our DiscoverMPatternsWithPruning algo- rithm for mining m-patterns. The values we choose for minsup for frequent patterns and minp for m-pattern are set2 so that there is no false positive above level 2. An experiment con- sists of 5 runs done with different random seeds. Figure 2 plots the average CPU time against the total number of transactions (in ten thousands). The results for frequent itemsets are des- ignated by the ?*? markers and those for m-patterns by the ?+? marker. We see that the two curves are almost indistinguishable, although the curve for frequent patterns is just below that for m-patterns. It is somewhat surprising that m-pattern discovery is so efficient since qualifying an m-pattern requires k compar- isons where as frequent itemset only requires one comparison.

This suggests that a linear algorithm for qualifying m-patterns is sufficiently fast. Indeed, we see that both algorithms scale linearly as the number of transactions increases.

Now, we study the effect of minp and the benefits provided by the Pruning algorithm. Here, the number of transaction is fixed at 50,000. The results are plotted in Figure 3. The x-axis is minp, and the y-axis is the CPU seconds required to discover m-patterns. The line with the ?+? markers are the results for DiscoverMPatterns, and the line with the ?*? markers is for DiscoverMPatternsWithPruning. Note that for larger values  2 0 u r  results are not sensitive to the specific values used for rninsup and minp.

,.- t  - Fig. 3. Average run time vs. minp  of minp (e.g., minp > 7%), there is little impact on CPU con- sumption. This is because minp is sufficiently large compared to the fraction of ?noise? transactions. However, when minp is small, pruning provides significant benefits. In fact, when minp is 6.5%, a typical run generates about 3730 candidates at the third level. With pruning, the number of candidates reduces to 2550.

B. Production Data  This section applies our algorithms for discovering m- patterns in data from a production computer network. Here, our evaluation criteria are more subjective than the last section in that we must rely on the operations staff to detect whether we have false positives or false negatives.

Two temporal data sets are considered. The first was collected from an insurance company that has events from over two thou- sand network elements (e.g., routers, hubs, and servers). The second was obtained from an outsourcing center that supports multiple application servers across a large geographical region.

Events in the second data set are mostly server-oriented (e.g. the CPU utilization of a server is above a threshold), and those in the first relate largely to network events (e.g. ?link down? events).

Each data set consists of a series of records describing events received by a network console. An event has three attributes of interest here: host name, which is the source of the event; alarm type, which specifies what happened (e.g., a connection was lost, port up); and the time when the event message was received at the network console. We preprocess these data to convert events into items, where an item is a distinct pair of host and alarm type. The first data set contains approximately 70,000 events for which there are over 2,000 distinct items during a two- week period. The second data set contains over 100,000 events for which there are over 3,000 distinct items across three weeks.

We apply our algorithm for m-pattern discovery to both data sets, and compare the results to those for mining frequent item- sets. We fix minsup to be 3 so as to eliminate a pattern with only one or two instances, and we vary minp. Our results are reported in Figures 4 and 5 for data sets 1 and 2, respectively.

These figures plot the total number of m-patterns (the solid line) and the number of border m-patterns (the dashed line) against minp. Here, a border pattern refers to a pattern that is not a sub- set of any other pattern. The x-axis is minp, and the y-axis is the number of m-patterns discovered on a log scale. Clearly, minp provides a very effective way to select the strongest patterns in that the number of m-patters discovered drops dramatically as  14. 0.1 0.. 0 .  0.. 0.. 0.. J.

Fig. 4. M-patterns of the first data set. ?-?: the number of m-patterns in the log scale; ?..?: the number of border m-patterns in the log scale; x-axis is minp  Fig. 5. M-patterns of the second data set. *?-?: the number of m-patterns in the log scale; ?..?: the number of border m-patterns in the log scale; x-axis is minp  minp increases. Many of these patterns have very low support levels. For example, we found 59 border m-patterns with length from 2 to 5 in the first data set when minp = 0.7. Half of these patterns have support levels below 10.

To compare with frequent patterns, it suffices to set minp = 0 since the algorithm reduces to mining frequent patterns. Figure 6 reports frequent patterns found in the first data. Here the x-axis is minsup, and the y-axis is the log of the number of patterns found. Note that the number of frequent patterns is huge-in ex- cess of 1 ,%veri when when minsup is 20. Examining the frequent patterns closely, we find that most are related to items that occur frequently, not necessarily items that are causally re- lated. This is not surprise since the marginal distribution of items in our data is highly skewed. Indeed, a small set of items account for over 50% of total events, and consequently, these items tend to appear in many frequent patterns.

Beyond the quality of the results produced by mining for fre-  t no U .a  man-  Fig. 6. Frequent patterns of the first data set. ?-?: the number of frequent patterns in the log scale; ?..?: the number of border frequent patterns in the log scale; x-axis is minp     quent itemsets, there is an issue with scalability as well. In Fig- ures 4 and 5 ,  minp 2 0.1 and minsup = 3. Suppose we have minp = 0 and minsup = 3 so that we are mining for frequent itemsets, but with a very low support threshold. When we at- tempt to run this case, more than 30k candidates are generated at the third level. Not only does this result in very large compu- tation time, we ultimately run out of memory and so are unable to process the data.

We reviewed the m-pattern found with the operations staff.

Many patterns are related to installation errors (e.g. a wrong parameter setting of a monitoring agent) and redundant events (e.g. 11 events are generated to signal a single problem). In addition, a couple of correlations were discovered that are being studied for incorporating into event correlation rules for the real- time monitoring. We emphasize that over half of the m-patterns discovered have very low support levels.

Why are m-patterns common in these data? One reason is a result of physical dependence that manifests itself as a set of events when a problem arises. For example, when a local area network (LAN) fails, ail hosts connected to the LAN gen- erate ?lost connection? events. Further, the same hosts generate these events if the same failure occurs. This results in the mu- tual dependence of these events. This observation suggests that m-patterns can be used to construct signatures for problematic situations.

A second cause of m-patterns is redundant information. For example, a device may generate an event to report a problem it detects. However, there may also be several management agents that monitor the same device and report the same problem. This results in an m-pattern consisting of redundant events. Identify- ing such m-patterns can aid in constructing filtering rules that re- move redundant events. More details and insights can be found in [13][10].



VI. CONCLUSION  Motivated by the need to discover infrequent, but strongly correlated patterns, we propose a new pattern, a mutual depen- dence pattern or a m-pattern. M-patterns are defined in terms of minp, the minimum probability of mutual occurrence of items in the pattern. In contrast to one-way dependence as in asso- ciation rules, an m-pattern is characterized by a strong mutual dependency between any two of its subsets. That is, if any part of an itemset occurs, the other part is very likely to occur as well. Our results suggest that such strong mutual dependencies are common in computer networks, such as due to interrelated components that are impacted by the same failure.

We develop an efficient algorithm for discovering m-patterns.

This is accomplished in three steps. First, we develop a linear algorithm to qualify an m-pattern based on an equivalence we prove. Second, we show that a level-wise search can be used for m-pattern discovery, a technique that is possible since we prove that m-patterns are downward closed. Last, we develop an effec- tive technique for candidate pruning by establishing a necessary condition for the presence of an m-pattern. A significant impact of the resulting algorithm is that it discovers strongly correlated itemsets that may occur with low support levels, something that is difficult to do with existing mining algorithms.

Using synthetic data, we demonstrate that our algorithm scales well as the data set increases in size. We also show that the pruning algorithm provides considerable benefit, especially for small values of minp.

We apply our algorithm to data collected from two produc- tion computer networks. The results show that there are many m-patterns, many of which of have very low support levels (e.g., fewer than 10 occurrences). Attempting to discover these pat- terns using A-priori requires a very small value for support lev- els, which results in an explosion of candidates that overruns the memory of the computer we used.

We further develop frequent m-patterns that are defined in terms of both minsup and minp. We show that this is a more general pattern. That is, when minp = 0, this pattern is equiv- alent to frequent itemsets, and when minsup = 0, frequent m- patterns become m-patterns.

