Incremental updating Algorithm Based on Artificial Immune System   For Mining Association Rules

Abstract   A new algorithm is developed based on artificial  immune system and on the improved model for association rules mining. The algorithm is both scalable and efficient in discovering significant relationships in weighted settings as illustrated by experiments performed on web usage datasets. We propose a strategy for maintaining association rules in dynamic databases. This method uses weighting technique to highlight new data. The experiments have shown that our approach is efficient and promising.

1. Introduction   Traditional association rules mining (ARM) model assumes that items have the same significance without taking account of their weight/attributes within a transaction or within the whole item space. But this is not always the case. In order to tackle this problem, we made adaptation on the traditional association rule mining model under the ?significant ? weighted support? metric framework instead of the ?large ? support? framework used in previous works.

Due to the completeness nature of algorithms for mining association rules (such as the Apriori algorithm [1]), the number of discovered rules can be so large that browsing the rule set and finding interesting rules from it can be quite difficult for the user. This algorithm will be faced with tremendous searching space, so has lower efficiency. Artificial immune systems are highly distributed systems based on the principles of the natural system. It offers powerful and robust information processing capabilities for solving complex problems. Its ability of self/non-self- identification provides a good solution to Web usage mining based on association rules. So in this paper, we present a new algorithm, association rules mining based on artificial immune algorithm.

2. Weighted association rules mining   In this paper, the idea of replacing the support with significance is proposed in [2] and it argue that a ?weighted downward closure property? can be retained by using weighted support. It also proves that this property is valid under the ?weighted support ? significant? framework.

In order to make use of the weight in the mining process, several new concepts have been adapted.

Support is used in association rule mining. In weighted association rule mining (WARM), itemsets are no longer simply counted as they appear in a transaction.

This change of counting mechanism makes it necessary to adapt traditional support to weighted support. The goal of using weighted support is to make use of the weight in the mining process and prioritize the selection of target itemsets according to their significance in the dataset, rather than their frequency alone.

Let { }niiiI ,,, 21 ???= be a set of distinct items and W be a set of non-negative real numbers. A pair (x, w) is called a weighted item where Ix ?  is an item and Ww ?  is the weight associated with x. A transaction is a set of weighted items, each of which may appear in multiple transactions with different weights.

Definition-1 Weighted attributes: weighting attributes  ( )kaaaA ,,, 21 ???  are variables selected to calculate weights. Depending on the domain, there could be any variable ranging from item?s price in a supermarket domain to visitor page dwelling time in a web log mining domain.

There are two types of weights ? the item weight and the itemset weight: Definition-2 Item weight: Item weight is a value attached to an item representing its significance. We denote it as  ( )iw . In the web log mining setting where each item is a page visited in a click-stream/transaction, the weight can be related to a users average dwelling time on that  0-7695-2645-4/06 $20.00  ? 2006    page.

Definition-3 Itemset weight: Based on the item weight ( )iw , the weight of an itemset, denoted as ( )isw , can be derived from the weights of its enclosing items.

One simple way is to calculate the average value of the item weights, denoted as:  ( ) ( )  is  iw  isw  is  k k?  == 1  Also bear in mind that an item weight is a special itemset weight when the itemset has only one item.

Definition-4 Transaction weight: Transaction weight is a type of itemset weight. It is a value attached to each of the transactions. Usually the higher a transaction weight, the more it contributes to the mining result.

The transaction weight ( kt ) can be derived from weights of the items presented in the transaction. One may formulate it easily as the average weight of the items presented in the transaction.

( ) ( )( )  k  t  i k t  iitemweight tweight  k  ? == 1  An itemset is denoted large if its support is above a predefined minimum support threshold. In the WARM context, we say an itemset is significant if its weighted support is above a pre-defined minimum weighted support threshold. This method may be more meaningful than only specifying relatively arbitrary support threshold.

3. Presentation of algorithm   Through the inspiration by artificial immune system[3], the original transactions and candidate model can be considered as chromosome. The chromosome is constituted by gene (attributes).

Therefore the transactions in the database will be regarded as antigens, the candidate model as antibody.

In our algorithm, there are the following definitions: (1) Antibody: The antibodies are specific proteins that  recognize and bind to another protein. As the antigens, antibodies are also used to represent a certain data item. So according to our need, antibody population are those transactions that are randomly selected from dataset.

(2) Antigen: Each antigen in the AIS represented an individual data item. In our application, after initialising the antibody population, the rest transactions of dataset are regarded as antigens.

(3) Stimulation level: Each antibody can recognize a set of related antigens, each of which differs  slightly in shape. The stimulation level of an antibody dictates whether the antibody survives. It represents the intensity of combining antigen and antibody. The higher stimulation an antibody has, the stronger intensity their combination will have.

The stimulation level is defined as follow.

( ) ( )  ( )?  ?  =  ?  == antigens  k k  antigenantibodyantigens  k k  i  antigenweight  antigenweight antibodysl  ki   &   The weight of antigen )( kantigenweight can be calculated by using the formula given in Definition-4.

(4) Cloning and mutation: When antibodies on a B-cell  bind with an antigen, the B-cell becomes activated and begins to proliferate. New B-cell clones are produced that are an exact copy of the parent B- cell, but then undergo somatic hypermutation and produce antibodies that are specific to the invading antigen. The number of clone is relevant with the affinity: ( )ii slke = , where ie is the number of clone, isl is the affinity of iantibody , k  is a constant value to control the number of antibodies that participate in cloning operation. The antibodies produced by cloning must be mutated.

Cloning_ And_ Mutation (antibodies set)  For each antibody in the population {  For ( i=0;i< ? ?slk ? ; i++) {    Generate and mutate new Clone; If ( the new clone is identical to any  antibody in the population) Eliminate the new Clone; Else Add the new Clone to the population; } }  As for mutation operation, we adopt random point mutation, and change the value of these points.

(5) Memory cell: The immune system maintains a  memory of its encounters with antigens The B- cells, in addition to proliferating or differentiating into plasma cells, can differentiate into long-lived B memory cells. During our algorithm performing, the outstanding individual that their stimulation is larger than the minimal threshold will be preserved as a memory cell. If the memory cells are overflow, the antibody with lower stimulation level will be remove from memory cells. So we can easily mining association rules from memory cell.

The procedure of the association rules mining based on artificial immune algorithm can be described as follows.

AIS_Based_Association_Rule_Mining_Algorithm  0-7695-2645-4/06 $20.00  ? 2006    Initialize the antibody population; Load antigen population; While ( Termination condition not met ) { Present antigens set; Calculate antibody stimulation level; Select antibody with high stimulation level and  store their digital strings and stimulation as memory cells;  If ( memory cells are overflow ) Remove the antibody with lower stimulation  level; If ( Termination condition not met ) Cloning_And_Mutation ( antibodies set); } Generate association rules; The minimum stimulation level, the number of  memory cells and the value of k  need to be specified by user.

4. Mining incremental data sets   Given the original database DB, minsupp is the minimum support threshold. For efficiency, we reuse the information from the old frequent pages and not frequent pages as they are done in our proposed algorithm. After some updates, the increment database db is added into the original database DB. Let 1w and  2w be the weights of DB and db, respectively, 121 =+ ww . Then for any association rule YX ? ,  we define its support and confidence as ( ) ( ) ( )YXwspwYXwspwYXnewSPw ??+??=? 2211  ( ) ( )  ( )?  ?  =  ??  ==? setntransactio  k k  tYXsetntransactio  k k  tweight  tweight YXwsp  k  _   )(&_   where ( )YXwsp ?1 and ( )YXwsp ?2  are the support of YX ? in DB and db, respectively.

( )YXnewSPw ?   is the support of YX ? in dbDB ? . The association rule YX ? can be  extracted as a valid rule in dbDB ? only if it has both support and confidence larger than minsupp and minconf respectively.

Any pagesets X can be divided into the following four groups: Group 1: the X is significant in both DB and db.

Group 2: the X is significant in DB but not significant in db.

Group 3: the X is not significant in DB but significant in db.

Group 4: the X is neither significant in DB nor db.

In order to make full use of the result of DB, we  execute our algorithm in transaction set db, and obtain the 1-pagesets and all association rule. First, let?s select the 1-pagesets in db that its support value satisfies the minimum support threshold and compare them with 1- pagesets in DB. Given in Group 4, then X cannot be used to generate any rule because their supports are less than the minimum support threshold. Thus, X is indifferent. In Group 1, X is certainly significant in  dbDB ? , its support ( ) ( ) ( )XwspwXwspwXnewSPw 2211 ?+?= . In Group  2, ( )Xwsp1  is known, and the support of X in db also is available, if  ( ) ( ) pXwspwXwspw supmin2211 >=?+? , then X is valid, else it will be deleted. Meanwhile, delete all pagesets in DB that contain X. In Group 3, ( )Xwsp2 is known, by searching the running result in DB we also obtain the support ( )Xwsp1 , if  ( ) ( ) pXwspwXwspw supmin2211 >=?+? , then X is valid, else it will be deleted in db. Continually, utilize above method to deal with remained rules. At last, if  ( )YXconf w ?  satisfy the minimum confidence threshold, then this rule is the association rule in  dbDB ? .

5. Experiment and results   We present our experience in applying association rule technique to the web data set maintained by Guangxi University. In this paper, we concentrate on Web usage mining that discovers user navigation patterns from a large collection of Guangxi University Web access logs. Its Web site mainly presents the basic conditions of Guangxi University. The data set is composed of 2880 transactions and 322 pages. The weight of page is visitor page dwelling time. We provide a concrete example to illustrate this in Figure 1.

In Figure 2, three different minimum weighted supports ranging from 0.03 to 0.05 are used for both cases.

Transaction Page and its weight   1 (P65,3.75) (P86,2.0) (P233,0.5) (P255,0.5) (P303,0.5) (P304,0.5) (P319,20)  2 (P65,0.5) (P112,0.5) (P255,0.33) 3 (P10,0.5) (P17,0.5) (P86,0.857)  (P162,2.0) (P236,0.5) (P255,2.0) (P320,0.5)  4 (P3,1.25) (P86,4.0) (P105,0.5) (P178,0.5) (P194,0.5) (P279,2.0)  5 (P65,0.5) (P68,0.833) (P246,2.0)  0-7695-2645-4/06 $20.00  ? 2006    6 (P65,0.5) (P74,1.5) (P178,0.5) (P223,6) (P246,0.5)   Transactions Transaction  weight weight( kt )  1 3.964 2 0.443 3 0.980 4 1.458 5 1.111 6 1.800  Figure 1 The lattice of transaction with weights Page 65, 86, 233, 255, 303, 304 and 319 appear in  transaction 1 in Figure 1, therefore the weight of transaction 1 is  )( 1tweight =(3.75+2.0+0.5+0.5+0.5+0.5+20)/7=3.964.

Min support Number of  significant itemset  Max length of itemsets  0.03 167 5 0.04 115 5 0.05 86 4  Figure 2  significant itemsets for different support We also mined these data sets with the Apriori  algorithm. The running results are illustrated as follows. After first running, 139 frequent itemsets are generated in Apriori algorithm, and 167 significant itemsets are generated in weight model, where 139 frequent itemsets are both in the two models. It means that significant itemsets generated in weight model contain all frequent itemsets generated in Apriori algorithm. 28 significant itemsets only in weight model have high weight. And the general results are presented in Figure 3.

Min support=0.03  Algorithm Number of significant itemset  Max length of itemsets  Our algorithm 167 5 Apriori 139 5  Figure3  significant itemsets for different algorithm For mining incremental data set, we added the  increment datasets db, and the size of each increment db is 5%, 10%, 20% and 30% of the size of the original database respectively. We performed experiments with different updates on our weighting model. Let min_supp=0.03, min_conf=0.5, and the weights of old and new itemsets be 7.01 =w and 3.02 =w respectively. Our method gets a significant improvement by only scanning the new data sets once, while the Apriori algorithm is least efficient because it needs to scan for the candidate items in the old and new datasets. As we have mentioned, effects of  weights is mainly critical. Because the new data in db can partly reflect the novel properties of the data in some context, we can exploit this novelty of data to assign weights.

6. Conclusions   The experiments show that the mining results in the weighted setting conform to the expected hypothesis.

The experiments also show that we studied an efficient, fast, incremental updating technique for maintenance of the association rules discovered by web usage mining. The developed method strives to determine the promising itemsets and hopeless itemsets in the incremental portion and reduce the size of the candidate sets to be searched against the original large data set. The idea of weighted methods really reflects the novelty of dynamic data. Our association rules mining based on AIS algorithm could effectively find out satisfied and interested association rules, and make search more efficient.

7. References  [1] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proc. 1993 ACM-SIGMOD Int. Conf.

Management of data, 207-216,May 1993.

[2] Feng Tao, "Mining Binary Relationships from Transaction Data in Weighted Settings" PhD Thesis, School of Computer Science, Queen's University Belfast, UK, 2003.

[3] Liang Mei-lian, Liang Jia-rong, Guo Chen.

Association rule mining algorithm based on artificial immune system [J]. Computer Applications, 2004.08.

