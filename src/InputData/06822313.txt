QoS-aware Resource Provisioning for Big Data Processing in Cloud Computing Environment

Abstract?Nowadays, the quantity of collected data from many different sources is increasing dramatically. As the traditional on-hand computing resources are not sufficient enough to handle Big data, deploying the processing services into clouds is becoming an inevitable trend. For QoS (quality of service)-aware Big data processing, a specially designed cloud resource allocation approach is required. Presently, it is challenging to incorporate the comprehensive QoS demand of Big data with cloud while minimizing the total cost. In order to solve this problem, a general problem formulation is established in this paper. By giving certain assumptions, we prove that the reduction of resource waste has a direct relation with cost minimization. Based on that, we develop efficient heuristic algorithms with tuning parameters to find cost minimized dynamic resource allocation solutions for the above-mentioned problem. In paper, we study and test the workload of Big data by running a group of typical Big data jobs, i.e., video surveillance services, on Amazon Cloud EC2.

Then we create a large simulation scenario and compare our proposed method with other approaches.

Keywords-Big data; resource allocation; cloud computing;

I. INTRODUCTION  In recent years, Big data has attracted a lot of attention from academia, industry as well as government [1]. Today handling Big data is a crucial research issue since it relates directly to public safety, human health, science development and economic growth. Many studies [2], [1], [3], [4], [5], [6], [7], [8] and [9] are emerging now-a-days to explore the possibility of using cloud computing paradigm for Big data processing. Those works are driven by a fact that the Big data processing requires scalable and parallel computing resources rather than using on-hand database management tools or traditional data processing applications [2]. In [2], Ji et al. presented a survey on the big data processing in the Cloud, and conclude that the conventional cloud resource management approaches are not suitable for big data processing. Similarly, in [9] and [10], the authors claim that the high volume, high velocity and high variety of big data introduce new challenging issues in the context of Cloud computing.

Generally, a cloud provider need to create various VMs with heterogeneous capacities to handle various Big data tasks such as processing, storing, searching, sharing, analysis and visualization. Each task also has its unique allocation  deadline specified in the QoS requirement. Fulfilling the resource and allocation time requirements to guarantee QoS is important for cloud operator, but still not enough. It is also the responsibility of a cloud operator to reduce the average service waiting time and long-term service cost. Unless these goals are achieved, the cloud based Big data processing cannot be truly applicable. Hence, the cloud resources need to be managed with the concerns of online allocation, load balance, joint task placement and cost reduction while meeting the rigid QoS requirement of big data processing jobs.

Most of the existing resource management solutions [2], [1], [3], [4], [5], [6], [7], [11], [3] and [9] for cloud systems, concentrates on the efficiency of computational resource usage. They hardly consider the multiple VM re- source dimensions (i.e. CPU, memory, disk I/O and network bandwidth) and overall resource utilization in the resource allocation problem which are very important for handling Big data tasks. Besides, the workload parameters may not be known or perfectly predictable because many tasks in Big data systems are driven by unexpected events. Consequently, the entire allocation problem should be formulated and controlled as a dynamic process.

In this paper, we tackle the aforementioned challeng- ing allocation problem in cloud based Big data system.

Specifically, we develop a cost effective and dynamic VM allocation model to handle Big data tasks. We suggest and prove that the overall resource utilization of cloud resources directly indicates the long-term service cost. Several exper- iments are conducted to validate the efficiency of our pro- posed allocation model in cloud based Big data processing platform. These experiments were conducted for different request patterns of Big data tasks in various environments.

We compare our proposed algorithm with other algorithms, and present results as well as explanations in this paper.

The rest of the paper is organized as follows: Section 2 presents the related work. Section 3 provides the detailed de- scription of the problem formulation for resource allocation.

Section 4 presents the online allocation and dynamic control situation. Section 5 shows the performance evaluation of the proposed allocation and finally section 6 concludes the paper.

DOI 10.1109/CSCI.2014.103    DOI 10.1109/CSCI.2014.103    DOI 10.1109/CSCI.2014.103    DOI 10.1109/CSCI.2014.103

II. PROBLEM FORMULATION  In this paper, we consider the cloud-based resource man- agement for big data tasks as a time-slotted task alloca- tion problem with time slots of equal length indexed by t = 0, 1, .... The actual duration of a time slot is related to the specific application (e.g., every few seconds or minutes for big data application). The major challenges regarding resource management in the proposed system are briefly described as follows.

? Regular allocation: We use A(t), t = 0, 1, ... to denote the task allocation results by the end of each time slot. We consider that each task has its own QoS requirements regarding resources and time. The resource requirement of a task is fulfilled by assigning it to a virtual machine having enough CPU, memory, disk I/O and bandwidth capacity. Meanwhile, each VM holding a specific task must be allocated before the task allocation deadline specified in the QoS requirement of that task.

? Emergent task arrival: If any emergent task arrives, the allocation decision for that task will be made immediately to handle real-time big data processing.

Same as regular task, a virtual machine having enough CPU, memory, disk I/O and bandwidth resources will be created for each emergent task. Since there is no waiting time for emergent task allocation, only long- term cost reduction is considered while allocating a VM holding emergent task.

? Loop feedback control: As the regular allocation algorithm pursues long-term optimization goals in a dynamic environment, we propose to use a threshold variable in our on-line allocation process. Meanwhile, we define long time slots T = 0, 1, ... of equal length where the threshold value is adjusted by the end of every long time slot using a proportional-integral- derivative controller (PID controller).

? VM migration: VM migration can be triggered by two types of events: server overloaded event and server underloaded event. We also consider long-term cost reduction as the optimization goal for VM migration.

In the following, the modeling details of the servers and tasks are provided. We summarize the key notations in I.

A. Physical servers and tasks  The cloud resources consists of np physical servers defined as P = {p1, p2, ..., pnp}. In order to describe a physical server pi(1 ? i ? np) in general, we use ci, mi, si and bi to represent its CPU processing capability (expressed in millions of instructions per second- MIPS), memory space (expressed in MB), disk I/O (expressed in MB/s) and net- work bandwidth (expressed in KB/s), respectively. At time t, let fci(t), fmi(t), fsi(t), and fbi(t) be the percentage of free CPU processing capability, memory space, disk I/O and  Table I LIST OF NOTATIONS  Parameters  P set of physical servers T (t) set of arrived big data tasks at time t V (t) set of VMs at time t prj(t) service waiting time of task tj /VM vj at  time t A(t) allocation results at time t C long term cost RW long term resource waste fci(t) percentage of free CPU capacity on physical  server pi at time t fmi(t) percentage of free memory capacity on  physical server pi at time t fsi(t) percentage of free disk I/O capacity on  physical server pi at time t fbi(t) percentage of free bandwidth capacity on  physical server pi at time t cvi(t) overall resource situation on physical server  pi at time t rcij percentage of task tj /VM vj ?s CPU require-  ment on physical server pi rmij percentage of task tj /VM vj ?s memory re-  quirement on physical server pi rsij percentage of task tj /VM vj ?s disk I/O  requirement on physical server pi rbij percentage of task tj /VM vj ?s bandwidth  requirement on physical server pi  network bandwidth, respectively. If a physical server pi joins a server group G where the memory or disk I/O is shared in a distributed way among the servers in G, the definitions of mi, si, fmi(t) and fsi(t) are changed to represent the total resources and free resources of the server group rather than those of the individual server.

At time t, denote the set of arrived big data tasks by T (t) =  { t1, t2, ..., tnt(t)  } where nt(t) is the total number of  arrived tasks from time 0 to time t. For a task tj(1 ? j ? nt(t)), let wtj , adj and stj be the waiting time, allocation deadline and service time of that task, respectively. The waiting time is counted when the task arrives and finally fixed when it is allocated on a physical machine. For any emergent task, it?s allocation deadline should be specified as a distinguished value so that the allocation decision on this task will be made immediately. Meanwhile, the waiting time of any emergent task must be 0. In some cases, stj may not be known or predictable before the processing of tj is accomplished. Since each task may require a heterogeneous running environment, a corresponding virtual machine need to be created and deployed on a physical machine for processing. Let vj be the virtual machine for task tj .

As we mentioned before, we use a np-by-nt(t) matrix A(t) to represent the allocation results at time t where the elements are binary. For any aij(t), aij(t) = 1 means that the virtual machine vj has been allocated on the physical machine pi, and vice versa. Let rcij , rmij , rsij and rbij be vj?s resource requirements on pi regarding CPU capability, memory space, disk I/O and network bandwidth, respectively     and all in percentage form. We assume that the overhead of VM creation and maintainance is also included in the re- source requirements, which means rcij , rmij , rsij and rbij are the overall resource requirements for running vj on pi.

In pratical design, the values of rcij , rmij , rsij and rbij are acquired from user-supplied information, experimental data, benchmarking, application profiling or other techniques. The following conditions must be satisfied before allocating vj to pi.

rcij ? fci(t? 1) rmij ? fmi(t? 1) rsij ? fsi(t? 1) rbij ? fbi(t? 1)  (1)  The above conditions are crucial for the QoS guarantees in big data since the lack of processing capability, memory space, disk I/O or network bandwidth may cause severe QoS degradation. After allcating vj to pi, the free resources of pi are calculated by using (2).

fci(t? 1)? fci(t? 1)? rcij fmi(t? 1)? fmi(t? 1)? rmij fsi(t? 1)? fsi(t? 1)? rsij fbi(t? 1)? fbi(t? 1)? rbij  (2)  When all allocation decisions have been made by the end of time slot t, the following updates are performed.

fci(t)? fci(t? 1) fmi(t)? fmi(t? 1) fsi(t)? fsi(t? 1) fbi(t)? fbi(t? 1)  ?i (3)  B. Optimization goal  The cost reduction is a major concern while operating a cloud-based big data system. In cloud environment, the cost is commonly associated with the number of active physical servers [11]. To be more specific, the long-term cost can be defined as the cumulative running time of all active physical servers if we assume the servers? energy consumption is homogeneous. Let yi(t) be the binary variable indicating whether a physical server pi is active at time t (yi(t) = 1) or not (yi(t) = 0). The long term cost C is then expressed as  C? = lim t??   t  np?  i=1  t?1?  ?=0  yi(?) (4)  Intuitively, the waste of resource causes the increase of long-term cost C. Thus, we investigate the impacts of overall resource waste on the long-term cost reduction in Proposition 1.

Proposition 1: Assume the physical servers are homoge- neous where  c? = c? m? = m? s? = s? b? = b?  &  rc?j = rc?j rm?j = rm?j rs?j = rs?j rs?j = rs?j  ? Phsical Server p?, p? ? VM vj  (5)  We define the long-term resource waste function RW as  RW = lim t??   t  np?  i=1  t?1?  ?=0  (fci(?) + fmi(?) + fsi(?) + fbi(?))?yi(?) (6)  For any task set T (t), there always exists a positive corre- lation between RW and C.

Based on the above definitions, we set up the optimization goal as to find an on-line allocation method that solves the following problem:  min A(t),t=0,1,2,...

C =  t  np?  i=1  t?1?  ?=0  yi(?) (7)  s.t., fci(t), fmi(t), fsi(t), fbi(t) ? 0 ?t, i (8)  ?pi : fci(t) ? rcij fmi(t) ? rmij fsi(t) ? rsij fbi(t) ? rbij  ?t, j (9)  t ? adj |j, np?  i=1  aij(t) = 0 ?t (10)  where the constraints (8) and (9) guarantee the resource sufficiency of single server and entire cloud, respectively.

(10) shows that all tasks must be allocated before their allo- cation deadline. If (5) is satisfied, we can use the following optimization goal to replace (7) according to Proposition 1.

min A(t),t=0,1,2,...

RW (11)  We assume that throughout this paper the problem (5), (7)-(11) is feasible unless otherwise stated.

C. Problem analysis  The optimization problem we proposed above should be solved by using an online allocation approach without knowing and perfectly predicting the workload parameters.

The cloud-based big data system is a complex dynamical system because of the following reasons. At any time t, future task set T (t+?), ? > 0 is not able to be obtained. For the allocated tasks, the stj may not be known or perfectly predictable before the processing of tj is accomplished. It is also possible that the resource requirements rcij , rmij , rsij and rbij may differ from their original settings during running time. Therefore, the possible solutions can only achieve near-optimal results for this allocation problem.

The computational complexity of the online allocation al- gorithm is another important issue that need to be analyzed.

To this end, we consider a possible workload pattern where the allocation decision made at any time t is independent with the previous allocation status. Then we are able to analyze the complexity of the problem in Proposition 2.

Proposition 2: Assume the allocation deadline and service time of any task satisfies  adj = t, stj = 1|tj ? T (t)? T (t? 1) ?t (12) meaning all tasks must be allocated in the same time slots when they arrive, and the processing of any task can be finished within next time slot. By taking this assumption, we can prove that the allocation problem at any time t is a NP-complete problem.

Proof: The optimization goal of the allocation is  min A(t),t=0,1,2,...

C? = lim t??   t  np?  i=1  t?1?  ?=0  yi(?) (13)  Furthermore, the allocation decision made at time t is independent with previous and future decisions. Thus, the allocation problem at time t can be again simplified as  min  np?  i=1  yi(t) (14)  The above problem can be mapped to the multi- dimensional bin-packing problem [12]. The goal of this problem is to map several items, where each item represents a tuple containing its dimensions, into the smallest number of bins as possible. As the multi-dimensional bin-packing problem is a well-known NP-complete problem, it is proved that the allocation problem in our senario is also a NP- complete problem at any time t. More specifically, the size of this NP-complete problem depends on the number of arrived tasks during t ? 1 to t, which can be expressed as nt(t)? nt(t? 1).



III. ONLINE ALLOCATION AND DYNAMIC CONTROL  In this section, we first describe the key parameters used in our online allocation process. After that, the entire online allocation process are presented. The updating method for dynamic controller is explained at the end of this section.

A. Key parameters  As we proved in Proposition 1, the long-term service cost of cloud is related to the overall resource usage. By taking this as the starting point, we define the following parameters to represent the overall resource utilization con- dition of physical server. Given a physical server pi, The first parameter is the mean of resource usage ?i(t) at time t.

?i(t) = fci(t) + fmi(t) + fsi(t) + fbi(t)  (15)  ?i(t) provides a direct view showing how efficiently the resources of pi are utilized at time t. However, there is another parameter which indicates the overall resource  utilization condition implicitly. We use ?i(t) to denote the situation of resource balance.

temp1 = (?i(t)? fci(t))2 temp2 = (?i(t)? fmi(t))2 temp3 = (?i(t)? fsi(t))2 temp4 = (?i(t)? fbi(t))2 ?i(t) =  ? temp1+temp2+temp3+temp4   (16)  It can be seen intuitively that a better overall resource utilization will occur when ?i(t) increases and ?i(t) in- creases. Thus, we combine these two parameters into a single parameter cvi(t). The definition of cvi(t) is  cvi(t) = ?i(t)? ?i(t) (17) Since the actual value of the cvi(t) is independent of the  unit in which the measurement has been taken, it also has the potential to be applied on heterogeneous or inconsistent physical servers where (5) is not fully satisfied.

B. Online allocation  We first introduce Min-Min heuristic algorithm [12] and modify it by changing the metric and adding a threshold.

Based on Min-Min heuristic, we explain our proposed online allocation process step-by-step.

The metric in Min-Min heuristics are defined as cvi(t).

We also introduce a dynamic threshold value ?(t). If a candidate allocation aij(t) = 1 satisfies cvi(t) ? ?(t), it is considered as an approved allocation. At time t, a regular task tj can not be allocated when there does not exist any physical server pi satisfies cvi(t) ? ?(t).

The original Min-Min attempts to map as many VMs as possible to the first choice resource. Among all VM/server pairs, the modified Min-Min heuristic selects the one that produces the overall minimum metric value. If the VM/resource pair is approved according to the threshold value, the VM in that pair will be allocated to the cor- responding server. This process continuously repeats until all of the VMs have been allocated or until no VM/server pair has a value below the threshold. If any VM cannot be allocated, the corresponding task will be put into the waiting list for future allocation.

We provide the detailed steps of allocation process as follows.

Step 1: The allocation process starts at time t = 0. The ini- tial threshold value is defined as ?(0) = +?, which means the threshold is not adopted. For any physical machine pi, we have fci(0) = fmi(0) = fsi(0) = fbi(0) = 100%.

The task set is an empty set, i.e., T (0) = ?. The initial selected heuristic is Min-Min. In fact, the initial selection of threshold value is not an important issue. It will be changed at the end of first long-period T = 1.

Step 2: During a short time slot t, the urgent allocation requests may arrive. To handle them, we use the modified Min-Min and remove the constraint of threshold from it. As     each urgent request is executed individually and immedi- ately, the actual metric is cvi(t).

Step 3: At the end of a short time slot t, the regular allocation requests need to be handled. The modified Min- Min heuristic with threshold value is used.

Step 4: Repeat Step 2 and Step 3 until the end of a long- period T . If the time is also the end of a short time slot, finish Step 3 before starting this step. According to the pre-defined conditions, choose the VMs that need to be migrated first.

Try to allocate the VMs use modified Min-Min heuristic, and check whether the condition is eliminated or not. If so, then execute the allocation.

Step 5: At this step, the threshold value for next long- period are determined. The method is to adopt different threshold values in each heuristic to find the offline alloca- tion results for the past period. We use (11) to judge which is the optimal threshold value The optimal threshold value passes PID controller which generates the threshold value that can be used in next long-period.



IV. PERFORMANCE EVALUATION  In this section, we first explain the simulation setup and then illustrate the results from the simulations in details.

A. Setup  To generate the big data workload, we focus on four typical video surveillance paradigms: video streaming and monitoring, face detection, video encoding/transcoding and video storage. Video streaming and monitoring tasks repre- sents the basic functionality of cloud-based video surveil- lance system. To understand the characteristics of our video surveillance workloads, we analyze their runtime statistics collected while running the applications on AMAZON cloud EC2. We rent a M1 Small VM having 1 Intel(R) Xeon(R) E5430 @2.66GHz CPU unit, 1 CPU core, 1.7GiB memory, 1Gbps bandwidth and 30G hard drive with Microsoft Server 2008 Base 64-bit. We use the performance monitor of Windows to record the resource utilization of CPU, memory, storage and network bandwidth. We download videos from PETS (Performance Evaluation of Tracking and Surveil- lance) where each video consists 1000-5000 frames taken at 1000-5000 time instants. Each frame is a 720 by 576 JPEG photo. We use open-source software for face detection and video encoding. The following are the observed results of our workload test in cloud environment. Table II clearly illustrates the inter-workload diversity of workload.

Table II SUMMARY OF WORKLOAD  Workload CPU/Memory CPU/Disk I/O CPU/Net Face detection 72.13% 184.57% 37.63% Monitoring 59.89% 136.06% 24.42% Storage 4.76% 1.37% 9.22% Transcoding 250.06% 765.97% 564.29%  To create static and dynamic workload, we generate the aforementioned four groups of tasks with different arrival rates. The arrival rate of each service request is fixed in the static workload. For dynamic workolad, the task arrival rate of each service request is changed every 100 time- slots. The Total duration of task arrival is between time- slot 0 and 1000. 30% of service request has be allocated immediately, and the allocation deadline of the remaining tasks is randomly generated from 0 to 1000 time-slots after arriving to the system. The service time of each task is randomly generated from 50 to 1000. The physical servers are homogeneous and sufficient. The exact resource requirements of each task is generated based on Table II  B. Simulation Results  In our simulation, we implemented five approaches: Fisrt Come First Serve (FCFS), single-resouce-Min- Min heuristic (CPU), multi-resource-Min-Min heuristic (CPU+memory+disk I/O+bandwidth), Min-Min heuristic with proposed metric, and Min-Min heuristic with proposed metric plus dynamic threshold. We first present the results of total cost in terms of cumulative machine hours in Figures 1 and 2. As we can see from these figures, FCFS has the  Figure 1. Total cost of running static workload  worst performance among five approaches. Single-resource- Min-Min heuristic is slightly better than FCFS in static environment, but also has the worst performance in dynamic environment. The multi-resource-Min-Min heuristic, since it considers overall resource utilization, performs better than FCFS and single-resource-Min-Min heuristic. With proposed metric, the efficiency of Min-Min heuristic again increases.

However, the improvement is not magnificent without apply- ing dynamic threshold. The best approach is the Min-Min with proposed metric plus dynamic threshold, which saves more than 20% cost comparing with other approaches. Now we provide the resource utilization of CPU, memory, disk I/O and bandwidth while running the dynamic workload.

The percentage value in each sub-figure represents how much resource on the physical machines have been reserved by the VMs averagely. As we can see from Figure 3, our proposed appraoch can fully utilize all resources from time-slot 100 to 900 while other approaches only maintain     Figure 2. Total cost of running dynamic workload  high utilization from time-slot 100 to 400. Thus, this figure clearly indicates the efficiency of proposed approach.

Figure 3. Realtime resource utilizations of running dynamic workload. (a) CPU. (b) memory. (c) disk I/O. (d) bandwidth

V. CONCLUSION  In this paper, we provide a dynamic cloud resource provisioning method to handle QoS-aware big data process- ing. We address the challenging issue of incorporating the comprehensive QoS demand of big data with cloud while minimizing the total cost. We prove that the reduction of resoure waste has a direct relation with cost minimization.

Thus, we propose an efficient metric with modified Min- Min heuristic algorithm by adding threshold value. Our ap- proach can find cost minimized dynamic resource allocation solutions for the above mentioned problem. Our simulation results verify the efficiency of our proposed approach in both static and dynamic workload environment.

