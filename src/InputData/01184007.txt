Generating an informative cover for association rules

Abstract  Mining associarion rules may generare a large num- bers of rules making rhe rpsulrs hard fo analyze manually.

Pasquier er al. have discussed rhe generalion of Guigues- Duquenne-Luxenburger basis (GD-L basis). Using a sim- ilar appmach. we inrmduce a new rule of inference and define rhe norion of associarion rules cover as a minimal ser of rules rhar are non-redundanr wirh respecr ro rhis new rule of inference. Our experimenral results (obtained us- ing borh synrheric and real data sets) show rhar our covers are smaller rhan the GD-L  basis and they are computed in rime rhar is comparable ro the classic Apriori algorirhm for generaring rules.

1 Introduction  The number of association rules generated by data min- ing algorithms can easily overwhelm a human analyst. To address this problem several methods were proposed. Our paper continues the line of research from [71 by introducing a new rule of inference for association rules and by defining the concept of a cover of the association rules as a minimal set of rules that are non-redundant with respect to this new inference rule.

We use the terminology and notations of [91. Let T = (T, H, p )  be a table, where T is the name of the table, H = AI . . . A ,  is the heading of the table, and p E {0, l}". We assume that each attribute have (0, l} as its domain. The projection of a tuple on a set L C_ H is denoted by t [L] .

The tuple over I that consists of Is is denoted by 11. An iremser I is a set of items I C_ H. The supparr of I, denoted by supp(I), is given by supp(1) = v u .  In addition, supp(0) is defined to be 1. The closure of an itemset I is cl(1) = { A ,  E H ( i f t [ I ]  = 11 then t[Ai] = 1). An itemset is called closed if it is equal to its closure.

An associarion rule X + Y consists of two disjoint non-  Dan Simovici University of Massachusetts at Boston,  Department of Computer Science, Boston, Massachusetts 02125, USA  dsim@cs.umb.edu  empty itemsets X and Y, called anrecadenr and cansequenr and denoted by antc(r) and cons(r), respectively. We refer to the items of a rule T by i t e m s ( r )  = antc(r) U C O ~ S ( T ) .  The supporr of the association rule is the support of i tems(rl. The confidence of the association rule is the . .

ratio:  supp( i t e m s  ( T ) ) supp(antc(r)) . COnf(T) =  If conf(r) = 1, then T is called an exact association rule and denoted by X + Y; otherwise, r is called an approx.

imarive association rule (see [7]). Given a table T ,  a mini- mum support value minsupp, and a minimum confidence value minconf, we seek to generate all valid association rules (cf. [8]), that is, all rules with support greater or equal to minsupp and confidence greater or equal to minconf.

To deal with the usual large number of association rules it is preferable to generate only the association rules that cannot be inferred from other rules by using N ~ S  of infer- ence. A minimal set of such association rules was called basis in [71. To avoid confusion, we mention here that the single word "rule" will only be used in the sense of an as- sociation rule and will never be used to denote an inference rule. The Guigues-Duquenne basis for exact rules and the henburger  basis for approximative rules are introduced in [7]; together they form a basis for the valid rules.

The Guigues-Duquenne basis is a minimal set of ex- act rules from which the complete set of exact rules can be deduced using as following two inference rules: X + Y, W + Z t  XW + Y Z , a n d X  + Y, Y + Z k A'+ Z. The Guigues-Duquenne basis does not allow us to infer the support of the rules and in fact. by ignoring the support values, the first inference rule can lead to rules that have in- ferior support compared to the rules used in its generation.

The Luxenburger basis is a minimal set of approximative rules from which the complete set of approximative rules can be deduced using the two properties introduced in [61: ( I )  the association rule X + Y has the same support and confidence as the rule c l ( X )  + c l ( Y ) ,  and (2) for any three closed itemsets X ,  Y, and Z,  such that X C_ Y Z,  0-7695-1754-4/02 $17.00 Q 2002 IEEE 597  mailto:laur@cs.umb.edu mailto:dsim@cs.umb.edu   the confidence of the rule X + Z is equal to the product of the confidences of the rules X --t Y and Y -i 2, and its support is equal to the support of the rule Y + Z Both these properties can be regarded as new inference rules and they permit the inference of both the support and confidence of the resulting rules. Together, the Guigues-Duquenne ba- sis and the Luxenburger basis, provide a minimal basis for rules, which we will denote as the GD-L basis.

Next, we introduce a new rule of inference for rules.

Theorem 1.1 Ler r, r' be hvo rules such that i terns(#) C_ i terns(r)  and supp(antc(r ' ) )  5 s u p p ( a n t c ( r ) ) .

Then, supp(r') 2 supp(r)  andconf(r ' )  2 c o n f ( r ) .

This justifies the introduction of the inference rule:  r, i tems(+)  C_ i t e m s ( r ) , supp(antc(r ' ) )  5 s u p p ( a n t c ( r ) )  r'  Definition 1.2 If for two rules, r1 and r2. it is possible to infer TS from r1 using Theorem 1. I ,  then we say that rule r2 is covered by rule TI (and that q covers ~ p ) .  and we write rI + r2. The coverage relarion + consists of all ordered pairs of rules ( r t , r z ) .  such that rI + r2. If r1 + rz and rz + T I ,  lhrn TI, ~2 arc said 10 be equipotent. n Because of Definition I .2, we will also refer to the property of Theorem 1.1 as the coverage rule.

Theorem 1.3 The rules rl and r2 are equiporenr if and only$iterns(rl) = iterns(r2)and supp(antc(r1) )  = SUPP( an t 4 . 2 )  ).

Equipotent rules are interchangeable from the point of view of the coverage relation. that is, if rt and r2 are equipo- tent and T I  + ~g for some TQ. then TS < ' 3 .

2 Covers for association rules  Theorem 1 .I suggests the following definition:  Definition 2.1 Let S be the set of all valid rules extracted from a table r. A cover o fX  is a minimal set C 2, such that any rule from 2 - e is covered by a rule in e. A rule   Theorem2.2 Ler e be a cover of a set of rules X ex- rracred fmm a rable r. If r is a Ccover rule, then for any T' E 31 such rhar i terns(r) = i terns(r'), we have supp(r) 5 SUPP(T') and c o n f ( r )  5 c o n f ( r ' )  and rhere is no r1 E X such rhar a n t c ( r )  = antc(r1) and cons(r )  C cons(T1). Furrhec if rI,r2 E e, rhen iterns(r1) # i terns(r2) .

belonging to e is called a Ccover rule.

Definition 2.3 An informurive cover is a cover where for each cover rule T there is no equipotent rule r' such that antc(r ' ]  c a n t c ( r ) .  U  T h w r e m  2.4 Ler t be on informarive cover of a ser of rules X extracredfmm a rable r. l f ~  is a Ccover rule, rhen rhere is no valid rule r' such rhar i terns(#) = i terns(r) and a n t c ( r ' )  c a n t c ( r ) .

Note that it is possible to have an informative cover rule randavalidruler',suchthatitems(r) = iterns(r') and jantc(r)l > jantc(r ' ) l ,  as the next example shows.

A cover summarizes the set of valid rules in a similar way in which the large itemsets summarize the set of fre- quent itemsets [2]. A cover can also be used to simplify the presentation of rules to users: initially, only cover rules could he shown to a user, then the user could select a cover rule r and retrieve a subset of all rules covered by r ,  and then the process could be repeated. In this manner, the user could guide his search for rules without being overwhelmed by their number. A similar type of rule exploration has been proposed in [ 5 ] ,  in the context of the so called direction ser- ring rules.

The following pseudocode describes an algorithm for generating an informative cover for the set of valid rules.

Algorithm 2.5 (The CoverRules Algorithm) Let R be a queue that will contain frequent itemsets and let C be the set of rules in which we will place the cover rules.

I Initialize R by enqueuing into it all maximal frequent itemsets, in decreasing order of their size. C is 0.

2 If R is empty, then output C and exit; else extract an  3 For all strict non-empty subsets 1, of I, with i = 1.. .21' - 2, soned primarily by their support values (decreasingly) and secondarily by their cardinality (in- creasingly), do:  itemset I from R.

3.1 If the rule I ,  + I - I, is valid, then add it to C if it is not covered by a rule already in C. Go to step 2.

3.2 If i = 1 and 111 > 2, then add to Reach  subset of I that h a s  size /I1 - 1 and that is not already included in an itemset from R. Continue step 3.

Algorithm CoverRules starts from the set of maximal frequent itemsets and examines them in decreasing order of their cardinalities (steps 1-2). For each such itemset I. we search for a subset S having maximum support, such that S -i I - S is a valid association rule (step 3). Such a rule is a candidate cover rule and, once found, the search stops and the rule is added to the set of cover rules c if it is not     covered by one of the rules of C (step 3.1). During the ex- amination of each subset S of I. we may encounter some subsets such that they cannot be used as an antecedent of a rule based on the items of I. For these subsets, we will have to verify whether they can be antecedents of rules based on subsets of I. This is why, in step 3.2 of the algorithm, we add to R all the subsets of 1. Those subsets that are al- ready included in an itemset of R, however, do not need to be added. Step 3.2 needs to be performed only once, so we perform it if the first subset examined in step 3 cannot be used as an antecedent. The collection R is a queue because we want to examine the maximal frequent itemsets in de- creasing order of their size before we examine their subsets (added in step 3.2). We examine these itemsets in decreas- ing order of their size because a rule whose set of items is larger cannot be covered by a rule whose set of items is smaller. This ensures that a cover rule added to C cannot be covered by another cover rule that we may discover later.

Each time that we intend to add a rule to C, however, we still need to check whether that rule can be covered by one of the rules already in 6.

The strategy of examining first the maximal frequent itemsets and then their subsets, in  decreasing order of their size, guarantees that the set of rules that we generate is min- imal. Step 3.2 guarantees that all valid rules can be inferred from the rules in set C. Together, these ensure that the re- sulting set C is a minimal set of rules from which all valid rules can be inferred, and thus, C is a cover. The cover is in- formative because, in step 3, for subsets with same support, we examine first those with smaller cardinality.

3 Experimental results  The optimized version of CoverRules implemented in Java is available in ARtool [4]. We tested CoverRules on several databases. In a first experiment, presented in Ta- ble l ,  we executed the algorithm on the Mushroom database from the UCI Repositoly of Machine Learning Databases [3]. Note that the UCI repository contains two versions of the Mushroom database. We have used the version contain- ing fewer rows, which was used in the experiments of [7].

Mushrwm dnahhase (minsupp = 3G-X)  70si 453 47Y  Table 1. Results for Mushroom database  The cover contains fewer rules than the GD-L basis and its size decreases as m i n c o n f  is lowered (see also Fig. I ) .

This may seem surprising at first, but is due to the fact that, as minconf is lowered, more valid rules exist, the re-  93% 1m 5G-X MI Minimum EonBdenmce  Figure 1. Comparative Results on Mushroom Database  dundancy of these rules is greater, and thus they can be sum- marized better. In fact, forminconf  = 30%, the size ofthe cover is identical to the number of maximal frequent item- sets existing in the mushroom database (for m i n s u p p  = 30% there are 78 such niaximal frequent itemsets), and this happens because all rules that can be generated using sub- sets of a maximal frequent itemset are valid. In this case, the cover size is one order of magnitude smaller than the size of the GD-L basis, and three orders of magnitude smaller than the total number of valid rules.

For m i n c o n f  = 30%, all cover rules have the item ve i l -  type = p a r t i a l  as antecedent. Interestingly.

this item is common to all the mushrooms described in the database, so its support value is 1 - the maximum possi- ble support value. By looking at a cover rule separately, the fact that the rule has the most frequent item as antecedent might make us think that the rule is trivial. Knowing that this is a cover rule, however, its antecedent being the most frequent item takes new meaning because it implies that any rule that we can build from the items of the cover rule will be a valid association rule. Usually, the most frequent items are known to the users of the database, so a cover rule hav- ing such an item as antecedent can be easily interpreted, even without the help of the computer.

In the case of the Mushroom database, the CoverRules algorithm is about as fast as the Apriori ap-genrules proce- dure for generating all valid rules, which was described in [ I ] .  Both algorithms finished their processing in a couple of seconds, so we do not include their detailed time results here.

To test the on synthetic data we generated database SPARSE with 100.000 transactions of average size IO,  hav- ing I00 items, and containing 300 patterns of average size 5 .

We mined SPARSE f o r m i n s u p p =  5% and discovered 207 maximal frequent itemsets. For all our experiments on this     database, the times taken by CoverRules and ap-genrules were well below 1 second so we omit them again. The num- ber of rules discovered and the corresponding cover size are presented in Table 2  50?F 4ff.

I Y x7  1x6 xxo nno x8o __  - = 5 2 J CWCr   I24 I Y 6 1Y4 1Y4 1Y4  - -  -  Table 2. Results for SPARSE database  Note that the cover size increases initially as m i n c o n f decreases. This happens because the database is sparse.

so the redundancy is poor and rules that are discovered when the confidence threshold is lowered do not necessar- ily allow the inference of rules with higher confidence. For minconf = 10%. we obtain all valid rules and lowering the confidence threshold funher does not bring any new rules.

In fact. the 194 cover rules correspond to the maximal large itemsets that have cardinality greater than one, since there are 13 such maximal frequent itemsets of size one.

For our final experiment, we generated a dense synthetic database, which we will call DENSE, with 100,000 transac- tions of average size IS, having 100 items. and containing 100 patterns of average size IO. Our strategy for obtaining dense synthetic databases consists of choosing fewer and longer patterns. We mined this database for m i n s u p p  = 5% and we obtained 3,182 maximal frequent itemsets. For this experiment, the times taken by the CoverRules and ap-genrules algorithms became noticeable and we include them in Table 3.

Again, for this dense database, the cover size generally tends to decrease as we lower the confidence threshold. All valid rules are discovered for confidence 5%. so lowering  DENSEdrmbru (minsupp. 5%) minconf Valid m l e ~  spgenrulr, Cover Co~erRula  Tlme(ucondrJ Tlrnr(wconds)  5111Y1  54% 603x61 6483 40C 630706 6506 301- 656724 5496 104 20?x 682076 5674 10% 703373 3416 5% 703924 52 3181 37 IC 703Y24 52 31x1 37  Table 3. Results for DENSE database  m i n c o n f  further does not result in more rules. There is only one maximum frequent itemset of size one, which ac- counts for the difference between the number of maximal frequent itemsets and the cover size obtained in this case.

The time taken by the rule generation algorithms is more significative and allows us to notice that CoverRules?s per- formance tends to improve with the lowering of the confi- dence threshold, while ap-genrules tends to take more time as minconf is decreased. ap-genrules runs initially faster than CoverRules, which performs better for lower values of m i n c o n f .  These results, however, do not include the time necessary to output the generated rules. The space re- quirements of ap-genrules are more significant than those of CoverRules, and in some experiments we had to increase the memory available to the lava Virtual Machine so that ap-genrules would not run out of memory.

As expected, the performance of CoverRules, as well as that of ap-genrules, slows down when the databases are denser, and when the number of maximal frequent itemsets increases. The performance of the algorithms varies differ- ently with the change of rn inconf .  For dense databases, the size of the cover is one-two orders of magnitude smaller than the number of valid rules and shows the tendency of getting smaller as the redundancy in the generated rules in- creases.

