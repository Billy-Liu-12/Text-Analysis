All Silicon Data Center, the Energy Perspectives

Abstract ? The energy consumption related to data centers is ever increasing, despite the significant efficiency improvements in the involved technologies and the economic crisis. Pervasive adoption of virtualization, decreasing of PUE as well as new features related to energy-efficiency are not able to stop this growth trend, mainly sustained from the explosion of cloud services in both the fixed and mobile ICT markets. One of the most important factors that contributes to the increase in the data center energy demand is the evolution of data utilization in cloud environments. Big data support is one of the main driver of this process,  new data sources continuously produce huge amounts of mostly unstructured data, that should be stored in large centralized storage systems to be accessed by the cloud users and analyzed by the companies in real time.

This has a disruptive effect for the energy consumption of the data center hosting the above storage resources, that?s why the data need to be stored and analyzed in IOPS intensive style.

Flash memory technologies can play a key role in such scenario, since they can reduce dramatically both the storage footprint and energy consumption and increase applications performances bringing appreciable economic benefits. This fundamental efficiency and performance improvement for storage architectures can?t be supported by legacy technologies because they are not designed to use flash technologies but rather they are based on energy-hungry legacy mechanical disks. That is limit the energy efficiency and performance delivered by solid state disks (SSD) installed in legacy storage systems, opening the door to the development of new technologies that will be able to fully exploit the potential of flash disks.  The aim of this work is presenting a brief survey about the potential of flash technologies for improving the energy efficiency of data center storage systems supporting the incoming Big Data services.



I. DATA CENTER EVOLUTION The evolution of data center is one of the basic  technological challenges characterizing the ICT arena in the last years. Despite the emergency of several technologies aiming at bringing efficiency in the data center infrastructures, its energy consumption continues to be a really growing trend. First of all, the improvements in semiconductor technology, with the following effects in hardware miniaturization have not found the expected correspondence in a reduced power consumption. In fact, while miniaturization has drastically reduced the per-unit power consumption, it has allowed at the same time a larger number of logic ports to be assembled into the same physical space, with the obvious effect of increasing simultaneously both the performances and, power utilization [20].

Analogously, the introduction of new more energy-efficient  devices is not enough to achieve significant improvements, since as their total cost of ownership (TCO) continuously reduces, their usage demand increases consequently, so that the benefits achieved are rapidly overcome by a greater energy consumption and concomitant GHG emissions. This is known as rebound effect or as the Jevons (technological progress that increases the efficiency with which a resource is used tends to increase, rather than decrease, the rate of consumption of that resource) paradox or Kazzoom/Brookes postulate [21]). According to a recently issued Gartner?s report, there are some specific developments that are driving such growth in the data center energy demand: - Hyper data centers: very big data centers, consolidating  large quantities of computing and storage, typically focused on delivering cloud services or HPC on a large scale. They will absorb the 17% of physical servers/storage within 2015 starting from the 11% of today.

- HDV: the Hosted Desktop Virtualization, within 2015 the 16,7% of physical servers will be dedicated to HDV.

- Big Data: some strong increasing phenomena such as the adoption of mobile lightweight devices for private life and work usage, with the related cloud-empowered mobile-centric applications and interfaces, are further boosting the data center growth, under the pressure of the huge volumes of data to be processed centrally

II. DATA CENTER POWER CONSUMPTION According with Datacenter Dynamic [3], the total  estimated data center energy consumption for 2011 is about 31GW with an increment of 19% foreseen for 2012. The analysis shows also the effect of some fundamental dynamics related, country by country, to both the ICT markets status and the energy sources.

The global energy demand is however increasing despite PUE (Power Usage Effectiveness) improvements. The PUE represents the efficiency of a data center facility expressed as the ratio between the total amount of power used and the power delivered to the equipment, thus assessing the fraction of energy consumption due to, the HVAC (Heating Ventilation and Air Conditioning), the UPS (Uninterruptible Power Supply) subsystems and the lighting facilities. Of course, this growing trend in energy demand has also an impact on CO2 and other GHG emissions, introducing side effects on the environment that contribute to pollution and micro-climate changes. This is another fundamental factor that drives the evolution of data center architectures towards   DOI 10.1109/WAINA.2013.186     a more energy-conscious and hence eco-sustainable evolution.



III. THE TECHNOLOGIES INNOVATION IMPACT Figure 1. shows the EPA (Environmental Protection  Agency) forecast about data center energy consumption presented in the report to US Congress on Server and data center Energy Efficiency of August 2nd 2007 [2]. Such report emphasized that in 2006 data centers consumed the 1,5% of the total amount of US electricity, with a predicted growth trend that should double in 2010. That prediction was related mainly to the growth of the servers forecast whose demand in 2006 was the 68% of the total amount of energy.

Figure 1.  EPA Forecat and Koomey?s update  A study from J.G. Koomey [1], published in the 2011, claims that the EPA forecast doesn?t have never become actual. The report [1] shows that the actual growth of data center energy consumption is about 36% (1,7% - 1,2% of total use) in USA and 56% (1,1% - 1,5% of total use) worldwide, instead of doubling as it did from 2000 to 2005.

According to [1], the difference depends on both the economic crisis and the technological evolution, mostly related to the rapid adoption of virtualization practices, introduced by the industries and service providers in order to improve the efficiency of their computing and storage facilities.

In fact, due to the massive virtualization usage growth of the number of physical servers deployed in modern data centers is slowing down despite an explosion of the application services delivered. The biggest services providers have also been able to reduce drastically the infrastructure?s electricity usage compared to in-house data centers through the use of cloud computing technologies.

It?s very interesting to observe the improvement of the PUE occurred in the recent past. In fact, starting from the typical reference value of 2 assumed by both Koomey [1] and the EPA report [2], a couple of recent surveys, issued form EPA and Up Time Institute conducted in 2011, show an average PUE of about 1,8-1,9 ranging between 1,3 (for large installations) and 3,6. A last remark concerns storage and communication, Koomey [1] asserts that the energy consumption related to communication and storage has been estimated by considering the same growing ratio experienced by servers. Instead, there is practical evidence that the needs for storage continues to grow more rapidly than the one characterizing computing power. The total power used by the  storage devices is primarily related to the number of drive spindles, not just to the amount of data they hold. Thus, considering that the data density of storage devices is also growing rapidly (doubling every year, according to Grochowski and Halem [4]), the relationship between the total storage capacity and the associated energy usage is not so immediate. Accordingly, in order to correctly investigate the data center energy consumption we have to consider the role of storage systems into the overall infrastructure economy.



IV. STORAGE EVOLUTION In one of the last data dynamics reports from Forbes [16]  regarding Big Data dynamics, IDC confirms the trend claiming that during 2010 the barrier of Zettabytes of data has been cracked and in the 2015 it?s expected to get close to 8 Zettabytes [5].

The growth trend characterizing the storage market does not follow the typical ICT evolution dynamics observed in the last years. In fact, respect to the past experiences there are different sources and types of data, and there are different tasks and objectives related to data utilization that bring new and challenging requirements for the storage features and performance. The main drivers of such evolution is certainly the constantly increasing usage in many field of science [22][23][24] (e.g., genomics, complex physics) or every day?s life (e.g., meteorology, finance) of ?Big Data? - data sets whose size is beyond the management and processing ability of commonly used hardware and software technologies ? currently ranging from a few dozen terabytes to many petabytes of data in a single data set. Of course, the explosion of Big Data is not a part of EPA and Koomey analysis, who asserts that the storage-related energy consumption is approximately 17% of the total energy demand (18,2-24,2 BKWh/year, PUE 1,82-1,91).

Furthermore, a Intel internal white paper [17] reports that PUE is expected to attain 1,25 (81% of power delivered to the IT equipment).

Consequently, reducing the power consumption associated to the storage system will be one of the energy efficiency mainstreams in the next years. In the recent past the adverse effects introduced by the growth in the storage data volumes has been countered with the introduction of several new features in storage devices aiming at reducing the data space occupied on disks, (e.g., through compression or de-duplication), or promoting a better usage of disk hardware characteristics (e.g., through multi-tiering, by using the best storage flavor for each stored data) and improving the energy efficiency (e.g., through spindle down techniques). Those features have demonstrated the ability to improve the storage efficiency from several points of view, by limiting the explosion of disk space and containing the energy consumption.

A. The SSD Opportunities One of the better opportunities related to storage  evolution and efficiency, is the one offered by the introduction of Solid State Disks (SSD). In the recent past,  Koomey?s range     the adoption of SSD technology has been limited by the cost, the size and the reliability of the available implementations.

However, recently, a strong boost to the adoption of SSD technology is coming from the emerging requirements of modern applications as well as the wide adoption of mobile devices and, of course, by the significant improvements achieved in the SSD technology Error! Reference source not found.[9]. Nevertheless, the legacy storage architectures have been historically designed to use HDD devices and then are not able to fully exploit the power of SSD devices. In particular, there are several limitation coming from the integration of SSD in legacy storage systems :  ? Performance limitations: legacy storage doesn?t  scale in terms of real I/O performance, by wasting most of the SSD advantages.

? Performance sustainability, legacy storage doesn?t manage write cliff effect so can?t grant performance in steady state. [18][19]  ? Disks lifetime: the use of legacy RAID processes reduces the lifetime of SSD.

? Density: legacy storage architectures adopt SSDs in their own form factor, limiting the storage density per rack unit.

Several startup companies were born in the recent past with the aim of creating new storage architectures focused on SSD. Emerging storage architectures try to address the previous limitations by using specifically designed devices tailored on SSDs characteristics. For instance, devices from Violin Memory (www.violin-memory.com), leader in virtualization and database benchmarks, are able to guarantee 1 million of I/O operations per second (IOPS), with few tens of microseconds of latency and 99.9999% of availability.

From the economic side, in order to understand the real cost of SSDs we need to consider their TCO and ROI [7][8] and not just the ratio cost/capacity of SSDs and HDDs. The TCO/ROI are tightly related to the emerging storage requirements that are coming from business and applications:  ? I/O Performance, that is more IOPS and lower latency.

? Concurrent workload and performance predictability.

? Scalability.

Figure 2. shows the relation between applications and their performance requirements [9]. Moreover performance requirements are changing with the adoption of highly virtualized architectures; it is estimated that the cloud environments require the 7-8% of total storage capacity in SSD technology [9]. The current approach of data center storage designers is based on the storage multi-tiering, that?s why the cost of SSD is not yet comparable to the cost of HDD, and, in order reduce the storage costs, the mostly accessed data are dynamically moved, in a manual or automatic way, in the best performing storage layer. There are several studies on modeling the multi-tiered storage systems aiming at predicting the storage composition in  advance, in order to balance its costs and performance [10][11].

Figure 2.  Application storage requirements, SNIA curtesy  B. Estimations From a Rreal World Use Case Concerning the benefits that SSD technology can  introduce, there are several elements to be considered, and in order to give an idea about the overall potential and effects we present the following  real use case.

We consider a typical application well suited for exploiting the power of SSD: the Virtualized Desktop Infrastructure (VDI). It?s well known that VDI is an hard scaling application, mostly for the current performance limitation in legacy storage systems. In our real case we suppose to design an architecture for a 5.000 hosts Virtual Desktop infrastructure by using a typical configuration for the desktop based on Windows 7 - 2GB RAM - 30GB user data. According with the well-known Citrix best practices, we have to consider to put on performance storage swap file and write cache file that we can estimate as 6GB/user [12].

There is a lot of literature about VDI storage dimensioning, we consider here a basic choice in order to give just an idea about the available alternatives. A better choice can just result from a more detailed analysis on the specific requirements of the target environment. The IOPS requirements depend on several constraints (e.g., RAM allocations, boot time, logon/logoff time, applications requirements, etc.), in the Table I are reported the typical IOPS and IO requirement related to Windows 7 [12].

TABLE I: Windows 7 characteristics  Activity IOPS/IO (R/W)  Boot IO 12k Boot + 12k AV agent (80/20)  Logon-Logoff IO 5k (20/80)  Steady State IOPS 25 (80/20) ? Power User [12]  AV Scan (AVIRA) IO 300k (90/10)    In order to properly tune the storage performance (SLA) we assume the following values, reported in Table II:  TABLE II: Physical PC performance requirements     Assumption Requirement  Boot+Scan 2 minutes  Systems data 6GB swap + cache + headroom (20%)  AV Scan (AVIRA) Background maximum 4h  HDD SAS 15K RPM 180 IOPS  RAID (write penalty) 10(2) System Data, 5(4) User Data  Concurrency Boot-log storm 50%, Steady State 100%    The reference calculations for a correct dimensioning are:  ????? ?? ???? ? ????????  ???????? ? ?? ??   ????? ? ??????? !"????? ? ?#???	??$?%& ? ?????""??%  ???????    Under the above conditions, in the case of 5.000 users we have the following storage requirements:  TABLE III:VDI Storage requirements for 5.000 users  Storage Sizing 5.000 users storage requirements  User Data 150TB NAS system RAID 5  System Data 60TB, 30TB usable RAID 10  IOPS  Boot-log ~115 IOPS/vDesk (>580k IOPS)  Steady State 45 IOPS  AV Scan 20 IOPS    In order to address the above requirements, we consider two alternate design options: the first one based on Violin Memory all-flash solutions and the second one based on storage market leader EMC2 VNX 7500, high end device of the midrange series.

The Violin Memory solution uses a couple of device 6232 series with 64TB raw and about 500K IOPS for devices.

Figure 3.  Violin Memory 6232 Performance Report, Violin Curtesy  During the more stressful storage activities, where the read operations are predominant, Violin guarantees from 400k to over 600k IOPS. During the steady state, where the write operations are predominant, it ensures almost 300k  IOPS. In order to give an idea of the difference between SSD and HDD, we going to consider a EMC2 VNX 7500 completely filled with 300GB SAS disks. To address the aforementioned capacity requirements the VNX system needs to be filled by 200 HDDs. On the other hand, to address the performance requirements the VNX needs to be filled by about 3.200 HDDs, that means using 4 VNX 7500, three of them completely filled. The Table IV show the dramatic differences in both power consumption and space occupation between the solutions considered (the results would be the same if we consider other manufacturers instead of  EMC2).

TABLE IV: Performance comparison between the two solutions  EMC Violin Difference  Space 840RU 4RU -99,3%  Power Consumption  38,5KW 4KW -89,6%  IOPS 580K 1M +72%  Cooling 168,400BTU/h 5,800BTU/7h -96,5%    The benefit on energy demand and cooling following the introduction of the new flash array technologies is really impressive. Notice that in order to satisfy both the I/O performance requirement and the capacity one it is necessary to oversize the HDD storage from 60TB to 960TB. To get the same total cost of disk acquisition, the price/GB ratio of SSDs should be 15 times the one of HDDs. Currently MLC flash costs approximately 5.9 $/GB [13] whereas high performance enterprise HDDs cost about 2 $/GB. The ratio between the two values is about 3, so very far from the 15 needed to achieve the same total cost.

It?s also evident that an HDD-based solution is not comparable (much worse) in terms of I/O performance (latency and IOPS).

Of course, according to multi-tiering practices, we could introduce an SSDs isle within a VNX legacy storage device.

It that case, the EMC2 guidelines [14] suggest a building block design composed by a couple of SSDs plus about 15 HDDs SAS 15K RPM for each 1.000 Virtual Desktops.

Hence, a project of 5.000 VDI requires five building blocks.

We have to observe that such dimensioning doesn?t consider the write penalty and takes into account only the I/O performance during normal operations, without caring of the additional load due to boot/log storm and Anti-Virus.

A bit of skepticism come from the assumption that most of the I/O of normal users, mainly composed by write operations, can benefit on multi-level cache (DRAM+SSD) boosting effects. Anyway, is clear is that the performance of the VNX, like all similar midrange legacy storage products, is constrained by its own basic architecture. Basing on VNX scalability of 1.000 drives, we can assume on that the upper limit of its performance capacity is 180K IOPS. To address 6GB of system memory VNX has to be filled with 300 SSD     units of 200GB each one in a RAID10 configuration. That requires a couple of VNX 7500 systems (4-5KW power demand and a 13.000 BTU/h cooling demand). But it is not enough to get 580K IOPS, resulting in 4 VNX 7500 equipped with 300 SSDs of 200GB each, (6KW power consumption and 19.300 BTU/h cooling demand), and a total space occupation of 56 rack units in 4 racks (Table V).

TABLE V: Performance comparison SSD solutions  Indicators Comparison  VNX7500 SSD Violin 6232 Difference Space (RU) 56 6 -90%  Power Consumption (KW)  6 4 -33%  IOPS (K)  580 1000 +72%  Cooling (BTU/h) 19.300 5.800 -70%    The effects on the boot phase, that may have a stronger impact on the overall infrastructure performance than what it seems at a first glance, can become more clear from the following Figure 4. based on Violin 6000 flash-empowered system [15]. The figure shows that by using a Violin 6616 (SLC), that has the same performance of two 6232 (MLC), the boot time for a Windows 7 desktop is kept close to 22 seconds from 1.000 to 3.000 desktop units until it grows to 36 seconds when reaching 10.000 desktop units (traffic characterization:  boot/login 100% random ? 80% write). It?s quite clear that the IOPS/latency performance matters.

Figure 4.  Violin all-flash, VDI scalability  So, if we need a 1 million of IOPS performance we will have the following comparison:  TABLE VI: Performance comparison with 1M IOPS requirement  1M  IOPS Disks Space (RU/Rack)  Power (KW)  Cooling (KBTU/h)  2 Violin 6232 NA 6/1 4 5,8  7 VNX 7500 5555 HDD 1129/20  66,3 290  7 VNX 7500 285 SSD 76/6  5,5 23,4    The Table VI shows the other side of the scenario, the one related to pure performance. It?s clear that legacy technologies cannot compete with new Violin all-flash  because of the architectural bottlenecks that limit their ability to scale. Thus even if we use just SSDs disks we need to add a large number of devices in order to scale the performance, that means more space, energy and cooling.

In our comparison we are not considering some typical issues related to legacy storage technologies integrated with SSDs like the ?write cliff? or the disk lifetime [6]. These are other two important points that limit dramatically the performance of legacy storage even when they host SSDs.

Finally, according to [15] there is another key element to differentiate the technologies. Under a real mixed workload, typical in a virtualized data center and called I/O Blender, Violin doesn?t change his behavior. It scales-up/out in linear style as the VMs grow. The legacy storage solution are very sensible to the workload mix so much that a complete redefinition of the storage system organization/architecture could be needed to address new requirements, even through dedicated solutions.



V. CONCLUSIONS In this work we have observed how the energy  consumption of the data center,  specifically from the storage point of view, is currently huge and continuously increasing despite the economic crisis and the technological improvements in energy efficiency. The emerging business models, based on real time data analysis and the ongoing adoption of virtualization are changing the way of defining the requirements for storage systems which are more and more based on I/O sustainable performances. This emphasize the limitations of the legacy infrastructures in terms of performance and flexibility, requiring oversized and  (in the critical cases) dedicated solutions. Resulting in higher operational costs including increase in environmental costs.

In this emerging scenario, the significant reduction of the SSD technology costs and the improvements on its reliability are fostering the massive adoption of flash technologies. We have seen like SSD, integrated in brand new architectures can provide strategic benefits in terms of:  ? increased application performance  ? independence on workload variability and improved ability to share platforms  ? improved scaling  This in turn can dramatically reduce environment cost power, cooling, and footprint.

We have also explained that to really appreciate the potential benefits coming from the evolution of solid state technologies is necessary to adopt new architectures specifically designed on the base of the solid state device characteristics such as Violin Memory.

That is what is clearly emerged by the case presented in this paper: benefits more than 90% in terms of power, cooling and footprint, and more than 70% in terms of performance.

Furthermore, we can observe from Figure 5. another interesting phenomenon: flash technology is able to drain data not just from high performance HDD but also from DRAM. This is the case of emerging in-memory database architectures, like SAP HANA or SAS analytics, designed to reduce the latency time of the legacy storage architecture.

The use of SSD technologies in these scenarios allows us to deliver suitable performance at lower price than the ones that characterize DRAM-based solutions.

Figure 5.  SSD position in storage  Finally, we observe that the adoption of new brand solutions such as Violin Memory brings energy efficiency also because storage inefficiency implies not only storage but also computing overprovisioning (the ?big iron? trend).

Everything let predict a massive adoption of Flash technologies in the coming years.

