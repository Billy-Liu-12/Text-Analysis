2009 Second Asia-Pacific Conference on Computational Intelligence and Industrial Applications

Abstract-The associations between different modalities of Web images could be very useful for Web image retrieval. In this paper, we investigate the multi-modal associations between two basic modalities of web images, i.e, keyword and visual feature clusters, by data mining technique. The association rule crosses two modalities, in which the antecedent is a single keyword and the consequent is several visual feature clusters. A customized mining process is provided to mine such special multi-modal association rules. The multi-modal association rules are obtained offline based on the existing inverted file and utilized online to automatically integrate the keyword and visual features for web image retrieval. The experiments are carried out in a prototype system for Web image retrieval, and the results show the effectiveness of the mined multi-modal association rules.

Index Terms-Multi-Modal Mining, Web Image Retrieval, Association Rule

I. INTRODUCTION  The images on the Web are generally exposed as part of Web contents. Both text and image content can contain useful information that should be exploited in retrieving Web images [1]. They bring information that are complementary and that can disambiguate each other [2]. Most of the past Web image retrieval systems, such as WebMars [3], ImageRover [4], Cortina [5], and Atlas WISE [6], etc., integrate text and image visual features together by the relevance feedback (RF) mechanism, which adds the user's burden because the user must interact with the system for feedback after the initial retrieval.

Due to the laziness of web users [7-9] and for the sake of convenience, it is valuable to automatically associate and integrate the two modalities, i.e. keyword and visual feature, for web image retrieval. In this paper, we explore a special multi-modal association between keywords and visual feature clusters in Web image retrieval by data mining technique. The association rule crosses two modalities of web image, i.e.

textual feature space and visual feature space. It is obtained offline based on the existing inverted file and utilized online to automatically integrate keyword and visual feature for Web image retrieval. The integrated query based on the multi-modal association rules remarkably improves the retrieval precision and has relatively fast response time.

Wei Zhan Hubei Electric Power Information and  Communication Center, Wuhan, 430077, P. R. China.

E-mail: carloszhan@163.com  In the rest of this paper: section 2 discusses the related works. The multi-modal association rule is mined in Section 3.

Section 4 describes the experiment. Section 5 presents our conclusion and future work.



II. RELATED WORK  To automatically integrate the textual and visual information of Web images, previous works mainly include pseudo-RF method and online clustering method. The pseudo- RF method [10, 11] always assumes the top-N images are the relevant ones for feedback. Obviously, its performance strongly relies on the quality of top-N images that are used for feedback.

Meanwhile, the online clustering method [12, 13] clusters the initial text-based retrieval results based on visual feature online.

It is based on majority rule. In [13], the system clusters the first keyword-based retrieval results by the visual features of image and thus automatic re-rank the results set. As mentioned in [13], these methods can not improve the retrieval effectiveness when only a few correct images are contained within top-N results. Moreover, these online clustering methods remarkably increase the response time of the query. A recent Web image retrieval systems, Cortina [5], uses the association between the keyword and MPEG-7 visual features in the interactive RF process. However, the extremely low support of the association rule maybe reduces its usability.

Motivated by the success of data mining in CBIR and search engine, our approach mines the multi-modal association rules between keyword and visual clusters from the prospective of data mining. The rule is applied online to automatically realize the integration.



III. MULTI-MoDAL ASSOCIATION RULES MINING  The multi-modal association rule mining is done as follows:  A. Construct the transaction database D and the basic candidate 2-itemsets.

We are interested with the association between keyword and visual feature clusters. Therefore, only the itemset containing at least one keyword and one visual feature cluster is considered. Firstly, we must construct the basic 2-itemsets based on the inverted file.

PACIIA 2009    The existing inverted file in Web image retrieval relates the keywords to their associated images, i.e.

(1)  where Qj is one keyword and ~,i is one image that contains the keyword Qj.

Assume each image has h different visual features, i.e.

j=(!J, 12, ... , !h}. For each ~,i' it corresponds to two visual feature clusters over a feature type, i.e.

. count(Qj,{Cj Ii =1,...,m}) conf(Qj =>{C, 11 =1,...,m}) = (10)  MAX (count(Qj' C/? 1:::;/:::;111  C. Frequent Itemsets Mining  Based on A-priori algorithm and the above formulas, a customized frequent itemsets mining algorithm is given by Table 1. Each superset of frequent itemsets with cardinality K belongs to a set of candidate itemsets of size K, i.e. SK. It must be noted that the itemset is special and the mining process starts from 2-itemsets.

(2) TABLE!. THE FREQUENT ITEMSETS MINING ALGORITHM  Therefore, we construct N, transactions for keyword Qj in D. Similarly, transactions for other keywords can be added into D. Thus, the database D is constructed.

At the same time, according to (1) and (2), there are 2h*~ 2-itemsets for keyword Qj, i.e.

where Cfi,l and C{i,2 is the cluster that the ~,i belongs to and  the next closest over feature f We select two clusters for each image over each visual feature type to increase the frequency of the visual feature clusters and the number of co-occurrences of keyword and visual feature clusters.

According to (1) and (2), a transaction is defined as  SOME EXAMPLESTABLE II.

multi-modal association rule support confidence  (Great Wall, C11, C34, C65) 10.8% 32.1%  (car, C22, C56) 14.4% 56.7%  (Great Wall, C65) 21.8% 72.8%

IV. EXPERIMENTS  D. Identify the strong multi-modal association rules  We set min_count, min_supp and min_confto be 300, 10% and 30% respectively in practice to get the strong association rules like (Qj,{Cili=], ...,m}). The min_count is used to filter out the keywords with only a few images in the inverted file.

Table 2 shows some examples for the multi-modal association rules. The C11 represents the visual feature cluster with ID 11, and the rest C34, C65, C22, can be deduced by analogy.

Input:: Transaction Database D; Minimal Support Threshold min_supp; Minimal Count Threshold mincount;  Output: Frequent Itemset L 1. L] ={Qj Icount(Q?min_count}; 2. L] = LjU{Cj Icount(CJ>min_count}; 3. L2={(Qj,CJIQj E L],C j E L],supp(Qj,CJ>min_supp};  4. Jor(K=3;LK _1;f.r/J;K++)do 5. SK = {s, Is; is K - itemset with only one keyword  in it and a combination of frequent sets from LK - 1} ;  6. for(VTED) and (Vs k ESK)do  7. if (sk ~ T) do  8. assume the only keyword that Sk contains is Qj; 9. supp(sJ=supp(sJ +llcount(Qj); 10. end if 11. endfor 12. LK ={ s, Isupp(sJ>min_supp}; 13. endfor  14. L= U L K  ;  K"?2  15. return L;  (8)  (3)  (5)  (6)  (7)  supp(Q)= Icount(Q)IIIDI  supp(CJ= Icount(CJIIIDI  count(Qj ,c, ) conf(Q? =>C j ) =----==----  J MAX(count(Qj' C/? /  supp(Qj=>CJ =count(Qj,CJlcount(Q)  Similarly, we can get the 2-itemsets for the other keywords.

By filtering out the duplicate 2-itemsets, the candidate 2- item sets are successfully constructed.

B. Define the support-confidence  If we directly utilize standard support-confidence for the multi-modal association rules, their support is extremely low, which will affect the rules mining lately. To overcome this drawback, we defme the support and confidence of the rule as follows:  where count(A) is the count of itemsets that contain A in D. IDI is the count of all itemsets in D. Similarly,  { . _ _ count(Qj' {C, Ii =1,...,m}) (9)  supp(Qj => C, 11-1,...,m})-------::.....----- count(Qj)  A. Experimental setup  To evaluate the effectiveness of the multi-modal association rules, they are applied in our web image retrieval system, VAST [14], with a database of about 1,000,000 Web     images that are crawled starting from the website ..www.enature.com...

car, Great Wall, lion, tiger, seabeach, butterfly, bird, snow, sky, sunset, tulip, elephant, lake, baby, bear, boat, cat, cloud, dog, fish, frog, fox, horse, house, mountain , river, tree, rose, the Imperial Palace, Yellow River  The application of the rules is in a simple way currently.

The user enters a query keyword , and gets the initial retrieval results. For the top-K initial retrieved images, if existing multi- modal association rules that contain query keyword , the images in the cluster with the largest corifidence ranks first, then subsequently the images in the cluster with the second largest corifidence, and the same is valid for the rest.

The keywords in VAST is obtained from multiple sources on the web page that contains the image, including image filename and URL, ALT text, surrounding text, image hyperlinks , anchor text, etc. For the visual features, we use watershed algorithm to segment the image and then adopt c- means algorithm to merge the small regions [15]. The number of the final segmented regions is set to be 6. For each region, we obtain its average L*U*V color and three-level wavelet texture. We use EMD (Earth Mover Distance) to compute the similarity of two images. The distance metric of two regions is Euclidean distance .

The K-Means algorithm is chose to cluster each visual feature. 100 clusters are created over each feature type initially.

For the big cluster which is larger than a threshold T (currently be set to 20,000), we partition it into smaller clusters of about 10,000 images per cluster. In addition , 30 query keywords shown in Table 3 are selected for experiments.

0.80.4 0.6 Recall  --+- Keyword Only -+- Pseudo-RF _ Onl ine Clustering  ~ Our Method  COM PARISON OF INCREASED RESPONSE TIME  0.2 0 '------ -----'-- - -----'--- - ---"---- - ---'---- ----1 o  TABLE IV.

0.8  0.2  g 0.6 '(i.j '13 (j)a:: 0.4  method our method I'seudo-RF Online clustering vs. scope (ms) (ms) (ms) Top- l Oa 86.2 20.2 189.3 Top-200 112.3 34.6 316.2 Top-300 134.1 47.5 456.8 Top-SaO 173.6 72.8 671.1  Top-IOOO 278.7 143.2 1293.7  Figure I. Average Precision/R ecall graph  C. Response time  To evaluate the response time, we performed experiments on Red Hat Advanced Server 2.6.9-34.EL, ltanium 2 with 1000 MHz and 2G RAM system.

Table 4 shows the comparison of the increased response time against top-K retrieved scope for the three methods that integrate text and visual feature for retrieval. The increase response time is equal to the response time of the corresponding method minus the response time of keyword only method. The keyword only method is not considered because it does not use the visual feature.

From Table 4, it can be seen that the association rules method is faster than the online clustering method while slower than the pseudo-RF method . Furthermore , for the different scope of the top-K initial retrieved images, our method and the Pseudo-RF method rise slowly relatively while the online clustering method increases fast. In fact, the online clustering method consumes much time on the clustering process online . Our method makes the clustering process and the mining process done offline, which does not add much response time to the system.

In all, our method is superior to the online clustering method on both retrieval precision and response time, and significantly outperforms the pseudo-RF method on retrieval precision.



V. CONCLUSIONS AND F UTURE WORK  We use data mining technology in our VAST Web image retrieval system to discover the multi-modal association rule between keyword and image visual feature clusters. The multi- modal association rule not only improves the retrieval  Q UERY KEYWORD S FOR EXPERIMENTTABLE III.

Top-K (K=IOO in our experiments) images from results are gathered for experiments. The evaluation measure is Precision & Recall. Precision = number of relevant retrieved images/total number of retrieved images, Recall = number of relevant retrieved images/number of all relevant images.

Four methods have been used for comparison, namely the keyword-only method, the pseudo-RF method, the online clustering method, and our method (i.e. multi-modal association rules method) . In the pseudo- RF method, top- I0 images are used for feedback . In the online clustering method, we adopt the centroid-of-largest-cluster mode, which is the best reported in [13].

B. Top-K retrieval precision Figure I shows the average precision/recall graphs for the  30 query keywords in Table 3. The online clustering method is better than pseudo-RF method, and our method is better than the online clustering method. The keyword only method is the worst. The results confirm that it is necessary to combine the two modalities of web images. Moreover, the reason that our method outperforms the others maybe lies in that our method clusters the images over each feature type separately based on the whole inverted index for keyword and automatically select the most appropriate visual feature clusters to associate with keyword.

performance, but also achieves relatively faster query response time. In addition, it is easy to merge the multi-modal association rule into current text-based image search engines because they are all based on the inverted file.

Furthermore, we would try other more effective clustering and visual feature extraction algorithm for the preparation of data mining, and further combine the multi-modal association rules with other methods (such as long-term relevance feedback learning) for better retrieval performance. The association between text semantic clusters and image visual feature clusters will be further investigated.

