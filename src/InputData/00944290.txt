Discovering auantitative associations in databases

Abstract  In this paper we introduce a technique for mining association rules fiom quantitative data tables. The proposed method integrates the fuzzy set concept and the apriori algorithm. In this algorithm, the design of the membership functions avoids discriminating the importance level of the points.

Additionally, our method incorporates the bias direction of an item fiom the center of a membership function region. Also, the method emphasizes the distinction between three important parameters: the support of a rule, its strength, and its confidence. It avoids missing the distinction between small number of occurrences with high support intersections and large number of occurrences with low support intersections.

Introduction  The goal of data mining process in data tables is to discover interesting rules of the type A-B, which state association between the items A and B. for this purpose, Agrawal et a1 designed some algorithms [I].

The data mining process was divided into two phases: in the first one, itemsets (sets of attributes), whose support (number of occurrences in the database) is above a certain threshold (called minimum support) are considered as "large itemsets" and move over to the next stage.

The second stage of the data mining process is the formation of association rules fiom the large itemsets. Then the confidence (percentage of transactions having B given that they also having A) is computed for every association rule. Those with confidence level above a certain threshold (called minimum confidence) are displayed as interesting association rules.

Minimum S U D D O ~ ~  criterion  Many papers were writen about fmding boolean association rules [ 1, 8, 101 and quantitative association rules [2,3,4,5,6,7,9]. In [1,2,3,4,9,10] the support threshold plays a major role in the judgement weather an itemset is a large one.

Assume two data points, A and B, as depicted in Fig, 1. The intersection support of the itemset AB, as applied in [2,3], is very low. Thus, the summation of the support from the whole database may be less than the support threshold, even if the attributes A and B occur many times in the same environments, meaning they are strongly associated. Even methods that overcome this problem [5,6] do not distinguish between the two sides of the membership function apex, as long as the points are not in the area of overlapping between two membership function regions (see Fig. 2). These methods cause identical association rules, when attribute B occurs at bl or b.

Algorithms for mininp auantitative laree itemsets  In this section, an algorithm based on the apriori data mining algorithm is described to discover large itemsets. Fuzzy sets are used to handle quantitative values, as described in [2]. Our algorithm is applied with some differences.

Notation  n - m -  number of transactions in the database.

number of items (attributes) in the database.

0-7803-7U78-3/0u$10.00 (C)#N)l IEEE. Page: 423    di - the i-th transaction.

Ij - the j-th attribute.

Iij - the value of 1, for di.

pijk - Rjk - nUm(Rjk) - C, - c, - 4? ,- f,? - L, - 1, - num(Il, ..., Is) - numR(Ij) -  the membership grade of 1, in the region k.

the k-th fuzzy region of the attribute I,.

the set of candidate itemsets with r attributes.

candidate itemset with r attributes.

the membership value of di in region s,.

the fuzzy value of the itemset c, in the transaction di.

the set of large itemsets with r items.

a large itemset with r items.

number of occurrences of the attribute region Rjk in the whole database, where p,jk >O.

the occurrences number ofthe itemset (Il, ..., Is).

the number of the membership hnction regions for the attribute I,.

The quantitative values are first transformed into a set of membership grades, by using predefined membership functions. Every membership grade represents the agreement of a quantitative value with a linguistic term. In order to avoid discriminating the importance level of data, each point must have membership grade of 1 in one membership function; Thus, the membership functions of each attribute produce a continuous line of p=l . Additionally, in order to diagnose the bias direction of an item fiom the center of a membership hnction region, almost each point get another membership grade which is lower than 1 in other membership functions region. Thus, each end of membership function region is touching, close to, or slightly overlapping an end of another membership function (except the outside regions, of course) (Fig. 3).

By this mechanism, as point ?a? moves right, M e r  fiom the center of the region ?middle?, it gets a higher value of the label ?middle-high?, additionally to the value 1 of the label ?middle?.

The algorithm Step 1:  Step 2:  Step 3:  step 4: Step 5:  Step 6:  a.

b.

C.

For each transaction i For each attribute j  1:=(hj1/Rjl+pij&27.. . +pijk&) Where the superscript f denotes fuzzy set.

For each attribute region Rik, count the number of occurrences, where whole database. The output is num(Rjk).

p+O, in the  For each attribute region R,k, check weather its frequency num(R,k) is larger than or equal to the predefmed number of occurrences minnum. If Rik satisfies the above condition, put it in the set of large 1-itemsets L,. That is: LI={RjklnUm(Rjk)~mi,  lSjlm, llklnumR(Ij)}.

r-1 (r is the number of items that composed the large itemsets in the current stage).

Generate the candidate set fiom L, in a way similar to that in the apriori algorithm, that is, the algorithm frs t  joins 1, and l,, assuming that r-1 items in the two itemsets are the same, and the other one is different. It then keeps in CWI the itemsets, having all sub-itemsets of r items existing in L, (step 6 in [3]). Here, every item in an itemset is a fuzzy region.

Note that two regions belonging to the same attribute can not exist simultaneously in the same itemset.

For each newly formed candidate itemset cWl in CHI, that is composed of the items (sl,  For each transaction di calculate its intersection fuzzy value as: &+l,?=fl?nf;n.. .nfWl?, where 4? is the membership value of the region s, in di.

Calculate the frequency of cWI on the transactions, where qc,+l)?>O. num(cWl) is output.

If the fiequency of the itemset is larger than or equal to the predefined number of Occurrences minnum, put it in the set of large el-itemsets Ll.

s2,. . . $,+I), do the following substeps: . .  .

Page: 424    Step 7: Step 8:  a.

b.

Step 9: Step 10:  Step 11 :  ExamDle  If kl is empty, than go to the next step. Otherwise, set r=r+l, and go to step 5.

For each large itemset I,, r22, do the following substeps: Calculate its support as: sup(l,)=Cf[l,)?.

Calculate its strength as: str(lr)= sup(l,)/num(l,).

For each large itemset I,, r22, generate the possible association rules as in [ 11.

For each association rule sl, s2.. . s,,=>s,,+~. . . s,, calculate its confidence as: num(sl, SZ.. .S,,S,,+~. . .s,)/num(sl, s2.. &). if the confidence is higher than the predefined threshold minconf, then output the rule as an association rule.

For each association rule s I ,  s2. ..s,,=>s,,+,.. .s,, record its strength as str(s1, s2...sn,s,,+~. . .sr), and its support as sup(],).

In this section, an example is given to illustrate the improvements of the proposed algorithm, compared with previous methods. The database of the example is a part of the database that is used in [3], and transactions No. 1 and No. 3 were changed, in order to illustrate the advantage of the proposed method.

Examples for the differences between our method and previous methods are given.

The data set of students course grades, consisting of 10 transactions, is shown in Table 1.

Table 1 : students course scores in the example  Each case consists of three course scores whose initials appear above. Each course is an attribute.

Each attribute has five fuzzy regions: low, middle and high, as in [3] (but the membership functions are different), and two additional fuzzy regions: low-middle and middle-high, that have been added according to our method (Fig.4). Thus, five membership grades belong to each course score. These values are shown in Table 2, as a result of step 1. The fiequency of each is shown in the bottom line, as a result of step 2. The regions are identified by their initials.

Table 2: The membership grades of the data from Table 1 and their occurrences.

0-7803-7078-3/Ol/$lOAM (C)U)ol IEEE Page: 425    Step 3:  Step4: r-1 Step 5:  Step 6:  Assuming that in this example minnum is 3. All the fuzzy regions are put in L1, except OOPh.

Generate the candidate set Cr+l fiom L,. as an example, the itemsets OOP,DBl, OOP,DBI,, OOP,,,I,DBI, OOP,,,t,DBI,, are generated.

a. The intersection fuzzy values of the four itemsets above are shown in Table 3. Here the  b. The frequencies of the four itemsets above are shown on the bottom lines of table 3.

c. Since the frequency of all four itemsets above are larger than or equal 3, they are all put  min operator is used for intersection.

in the set of large 2-itemsets Lz.

The results from this step, for the example itemsets, are shown in Table 3.

- ~  Table 3: The intersection fuzzy values and summarizing parameters for itemsets in L2.

Step 7:  Step 8:  Step 9:  Step 10:  Step 11:  L2 is not empty. Step 5 generates, for example, the candidate itemset OOP,,,I,DB~STI in Cs (note that not all the itemsets in Cz and are detailed in the example). By step 6, the frequency of this itemset is 3, which states it as a large itemset.

The supports of the four above itemsets in are shown in the bottom lines of table 3, as well as their strength. The support of the itemset OOPmhDB~,ST1 is 2.8 and its strength is 0.93.

From the large itemset OOPdDBt,, the association rule OOPd=>DBh is generated. From the large itemset OOPdDBl the association rule OOPd=>DB1 is generated. The itemset 00PdDB&T1 generates, for example, the association rule OOP~=>DBI,,,STI.

The confidence of the rule 00Pd=>DB1, is 1. The confidence of the rule OOPd=>DBt is 0.75. The confidence of the rule OOPd=>DBhST, is 0.75. Suppose that minconf=0.7, these three rules are output as association rules.

The support of the rule OOPh=>DBh is 3.8 and its strength is 0.95. The support of the rule OOPd=>DB, is 2 and its strength is 0.67. It can be seen, that the bias of the rule OOPd=>DBl, to the left (with respect to the itemset DBh), formed the rule OOPd=>DBt, whose support is much lower than the support of the main rule that conects OOP,,,h to DB,. Also, it can be shown that there is no rule in the right side of DBh.

The support ofthe rule OOPd=>DBhSTI is 2.8, and its strength is 0.93.

It can be seen, that the three rules above would not be identified by some of the previous methods.

Also, our method found the bias of the main rule.

0-7803-7078-3/0U$l0.00 (C)U#)l IEEE. Page: 426    Conclusions  The suggested rules mining method avoids the missing of association rules that might happen as a consequence of arbitrary membership functions design. Also, our method considers the bias direction of an association f?om the main-rule regions.

The proposed algorithm distinguishes between three important parameters: the total support of the rule, the mean level of agreement (the support intersection) between the items in the itemset (the ?strength? of the rule), and the part of the relevant database, which fidfils the association rule (the confidence of the rule). These three parameters are instead of the integration of the individual supports and confidence calculation afterwards. Thus the distinction between small number of occurrences with high agreement and large number of occurrences with bad agreement do not disappear.

1.

2.

3.

4.

5 .

6.

7.

8.

9.

10.

R. Agrawal, T. hielinski and A. Swami: Mining Association Rules between Sets of Items in Large Databases. Proceeding of ACM SIGMOD, 207-216. Washington, D.C, 1993.

T.P. Hong, C.S. Kuo and S.C. Chi: A Fuzzy Data Mining Algorithm for Quantitative Values.

1999 Third Intemational Conference on Knowledge-Based Intelligent Information Engineering Systems. Proceedings. IEEE 1999, pp. 480-3.

T.P. Hong, C.S. Kuo and S.C. Chi: Mining Association Rules fiom Quantitative Data. Intelligent Data Analysis, vo1.3, no.5, nov. 1999, pp363-376.

Qiang Wei and Guoqing Chen: Mining Generalized Association Rules with Fuzzy Taxonomic Society - NAFIPS (cat. No. 99TH8397). IEEE. 1999, pp. 477-8 1. Piscataway, NJ, USA.

K.C.C. Chan and W.H. Au: Mining Fuzzy Association Rules. In Proc. of the 6* ACM Int?l Conf. on Information and Knowledge Management, Las Vegas, Nevada, Nov. 1997, pp.209-2 15.

W.H. Au and K.C.C. Chan: FARM: A Data Mining System for Discovering Fuzzy Association Rules. FUZZ-IEEE, 99. 1999 IEEE International Fuzzy Systems. Conference Proceedings.

Vazirgiannis M. and Halkidi M.: Uncertainty handling in the data mining process with fuzzy logic. Ninth IEEE Intemational Conference on Fuzzy Systems. FUZZ-IEEE 2000 (cat. No.

00CH37063). IEEE. Part vol. 1,2000, pp.393-8 vol. 1. Piscataway, NJ, USA.

A. Savasere, E. Omiecinski and S. Navathe: Mining for Strong Negative Associations in a Large Database of Customer Transactions. ICDE 1998, pp. 494-502.@@@ Chan Man Kuok, Ada Fu and Man Hon Wong: Mining Fuzzy Association Rules in Databases.

Sigmod Record, vol. 27, no. 1, March 1998, pp. 41-6.

R. Feldman, Auman Y. and Kloesgen W. Maximal Association Rules: A New Tool for Mining for Keywords co-occurrences in Document Collection. Proceeding of the KDD conference, pp.

IEEE. Part vol. 3, 1999, pp 1217-22 vol. 3.

167- 170, 1997.

