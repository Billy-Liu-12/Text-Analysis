Bipolar Pattern Association Using A Recurrent Winner Take All  Network

Abstract  A neural network for  heteroassociative (or autoas- sociative) pattern recognition of input bipolar binary vectors is  proposed. By  combining the advantages of feedforward and recurrent techniques f o r  heteroassocia- t ion, a simple network with guaranteed error correction is found. The heart of the network i s  based on a new, recurrent method of performing the Winner Take All function. The analysis of this network leads to  design rules which guarantee its performance. The network is tested on a character recognition problem utilizing the entire IBM CGA character set .

1 Introduction  This paper presents a recurrent neural network for implementing the Winner Take All (WTA) function.

By incorporating this network into a previously pro- posed neural network for bipolar pattern association [l], error correcting pattern associations can be per- formed easily and quickly. The resulting heteroasso- ciative memory combines the recurrent concepts of the Hopfield network with feedforward strategies for bipo- lar heteroassociation. The final network is fast, re- quires no supervision, and is simple to implement even for a large number of stored patterns.

The heteroassociate problem treated is similar to that examined by Hao, Tan, and Vandewalle [l]. In fact, many of their elegant and useful techniques are incorporated into the design. To summarize, the prob- lem is the following: Let an arbitrary set of well-defined associations be given by (ui + ri), i = 1, ..., m, where ui is a bipolar vector of dimension k, and .zi is a vec- tor (not necessarily bipolar) of dimension 1. Since the input vectors are bipolar, and therefore have entries  Bogdan M. Wilamowski Dept. of Electrical Engr.

University of Wyoming  Laramie, WY 82071 wilam@uwyo.edu  U uni olar negons  JTAGrossberg layer Figure 1. The neural network topology.

consisting only of fl, these are binary associations.

For many applications, the input vector may be cor- rupted with noise. In the worst case, this may produce bit errors, although the proposed network will tolerate both analog and digital (bit error) noise sources. This paper will develop methods of designing the network such that the output vector will always be that asso- ciation which corresponds to the minimum Hamming distance from the input vector. This error correcting ability, in particular, is extended greatly beyond that possible with Hao, Tan, and Vandewalle?s network.

2 The Network Topology  The network, which consists of three parts, is de- picted in Figure l .  The first layer, which is motivated by the concepts originally proposed by Kohonen [2], performs the task of roughly identifying which stored input pattern is closest (in the binary, Hamming dis- tance sense) to the k-dimensional input vector, U. This is accomplished by configuring the input weights (Wi) so that, if the ith stored pattern is the closest to the input, then the ith element of the m-dimensional vector n is the maximumelement of n. Since both the input  0-7803-4122-8/97 $10.0001997 IEEE 123 1  mailto:uwyo.edu mailto:wilam@uwyo.edu   Figure 2. The neural network topology, with hard logic neurons added to eliminate possible noise.

and the stored patterns are binary, the inner product $U equals k - 2HD,i, where ND,j is the Hamming distance (or number of bit errors) between the input and the ith stored input pattern. Consequently, if the jfh stored pattern is closest (in the Hamming distance sense) to the input, then the the inner product UTU will be maximized when i = j .  For this reason, if n is a vector of containing the inner products between the input and all stored input patterns, i.e.

T T  T T  n = [ul U u2 U . . .u,u] , then the ith element of n will be maximum when the ith stored input pattern is the closest to the input pattern.

The relationship (1) can be realized by forming the weighting matrix  T T  wi = [UT U; ... UmJ , Note that this has two extremely useful properties: (1) since the patterns are bipolar, so is Wi, thus storage of the weighting matrix is simpler than in the case of Hopfield networks; and (2) the weighting matrix does not need to be calculated, as it follows directly from the stored input patterns.

To facilitate the description of the second layer?s op- eration, an easier to understand version will first be considered. This network (Figure 2) contains an ex- tra layer of unipolar, hard logic neurons. They are used strictly to convert the vector o to a purely bi- nary output vector, Oh,.  By using a large gain, a, and a steep slope on unipolar neuron?s activation func- tion, the ?winner? can be made very close to one and the ?losers? can be made very close to zero. Conse- quently, the hard logic neurons will not be necessary for many applications. However, the network will be analyzed using the configuration shown in Figure 2 to avoid asymptotic arguments which may obscure the main points of the paper.

The WTA function inputs a vector, and then out- puts which element of that vector is maximum. In a  neural network, the output neuron corresponding to the maximum elemeat of the input vector fires. In other words, if a vector n E ?Xmxl is presented to the network, then the unipolar, hard logic output vector of the network, Oh, E ?Xmx?, will contain a single one which corresponds to the maximum element of n. All the other elements of Oh1 will be zero. The nonlinear function, f h , ( * ) ,  which implements the hard logic neu- rons has the following characteristic:  (3)  If the input to the ith neuron is x = ni - t z ,  where ni is the ith element of n,  and t 2  is a threshold, then the ith hard logic neuron?s output, ohi, is  (4)  First consider the case where the first layer of unipo- lar neurons is not present, but connections are made di- rectly from n to the hard logic neurons. Then the WTA function can be implemented by increasing the neuron threshold, t 2 ,  until only a single hard logic neuron fires.

While this is effective, it requires ?supervision? in the sense that some mechanism must be used to increase the threshold, while the output vector (oh,) is moni- tored. When Oh1 has only a single one, then t 2  is held constant. If a new input pattern is presented, then t z must be reset to a low value, and then the process is repeated. In essence, a search procedure which finds the value of t z  corresponding to a single neuron fired must be implemented.

Rather than using a supervised search procedure, the same function will be performed by adding the first layer of unipolar neurons, along with the feedback (shown in Figure 2) which modifies the first layer?s threshold, t l .  By using the results from the next section, the recurrent WTA network will automati- cally perform the search in a highly parallelized man- ner which is simple to implement with circuit technol- ogy. Consequently, the recurrent WTA network can be viewed as a fast and efficient method of finding the maximum of an m-dimensional vector. If m increases, the the only modification required is the addition of another unipolar neuron. Since the network grows so slowly with increases in m, this means that it is prac- tical for performing parallelized, large scale searches-a functionality which is important for pattern association and many other applications.

The final layer is termed a Grossberg layer due to its similarity to the networks typically trained using Grossberg?s outstar learning rule [3]. The input to the     layer, OhI, will be an m-dimensional vector consisting of all zeros, except for a single element equal to 1. That element will be the ith element if the ith stored input pattern is the closest match to the network input, U.

As a result, the I-dimensional network output vector, z, will equal the ith column of WO. If  WO = [Zl % 2 ? . ? Z * ]  ( 5 ) then the correct pattern association (i.e. uj + z i )  will be performed. Again, note that the output weights, WO, can be analog, bipolar binary, unipolar binary, etc. Consequently, considerable freedom is available to choose a format suited to a particular implementation.

Furthermore, WO is derived directly from the output stored patterns by using ( 5 ) .  Finally, note that if error correcting autoassociation is desired, then WO = v.

This can be used to further reduce the weight storage requirements, when an autoassociative network is de- sired.

1/2+ k / A ID]-  I  3 Design of the Recurrent WTA Net- work  1 7 -- ?1  I I  c  The previous section delineated a method whereby error correcting bipolar heteroassociation can be ele- gantly performed. Most of this network was originally developed in [l], with the notable exception of the re- current WTA network. Since [l] does not use recur- rence in its WTA network, its error correcting ability is diminished. This section will show that, by correctly designing the recurrent WTA network, the correct as- sociation will always be made.

Only the recurrent WTA portion of the network, will be considered in this section since it alone remains to be designed. Due to space limitations, only brief sketchs of the proofs are presented. The proofs appear in [4].

Theorem 1 The hard logic neuron output vector, OhI, will have exactly one neuron on if the following condi- t ions hold:  1. The maximum element of the vector n is at least e greater than the next highest response.

2. The unipolar neuron activation function, f l ( x )  has a slope such that, i f  f o r  some 21 < 2, f 1 ( q )  = T(1 + k / a )  and 2 2  = X I  - e ,  then f l ( x 2 )  5 ?1, where  A n  appropriate activation function is depicted in Figure 3.

9. f l ( 0 )  = 1/2.

4 .  Any  input pattern, U, has at least one stored pat- tern match with more than one-half of the bits matching.

5. The threshold of the hard logic neurons, t2 ,  equals t 2  = i(l+ k / a )  where Q > 0 .

Corollary 2 The recurrent WTA network will imple- ment the W T A  function if it i s  designed t o  satisfy the conditions imposed b y  Theorem 1.

Discussion: The preceding theorems provide rules which will en-  sure that the recurrent WTA network will perform properly. Although these design rules impose several conditions, the conditions are rather mild and can eas- ily be implemented. For example, most of the con- ditions are concerned with the design of the unipolar Iayer?s activation function. In short, these rules guar- antee that its slope in the transition region is suffi- ciently steep to preclude the possibility that multiple neurons will be activated.

The feedback gain, a, depends greatly on the man- ner in which the network is implemented. A larger value of a will cause a faster rate of convergence, so ideally a should be large. Often, Q may be determined by the properties of the neurons themselves.

The minimum separation between responses, E ,  is e = 2 if no input vector (U) is expected to match more than one of the stored input patterns equally. However, since U is often noisy, the only way guarantee this is to put constraints on the allowable noise levels and/or the allowable stored input patterns. While these kinds of constraints can be employed with this neural network, another approach can also be used when the stored input patterns are close together and/or there are high levels of input noise. If more than one stored input pattern matches the input (U) with an identical number of matching bits, then it is impossible to tell which of the identical matches to output. If noise is present on     the vector n ,  then one of the matching patterns can be randomly selected by choosing a value G: c which is less .

than the separation between elements of n caused by that noise. In this case, c will be much smaller than 2.

The hard logic layer of unipolar neurons is com- pletely determined, as the threshold is the only de- sign parameter, and it is given by the formula t 2  = +(1+ +).

4 Experimental Results  In order to test the neural network, it has been ap- plied to a simple character recognition problem. IBM?s CGA graphic representation of the ASCII character set consists of 256 characters. Each of these characters is represented in an 8 by 7 pixel array. Consequently, a total of k = 56 bits are used to represent each char- acter?s graphic pattern. All m = 256 members of the CGA ASCII set are stored in the matrix Wi , so Wj is a 256 x 56 bipolar binary matrix. Three output matrices, WO, may be useful. First, a WO which outputs art error corrected version of the input image can be found by letting WO := v. In this case, the network is used for autoassociation. Note that, since WO contains the same information as Wi, some implementations of the network may be able to use the same memory locations to store both WO and Wi. Second, a single analog out- put ( I  = 1) can represent the 256 levels in a highly concise, analog form.

During the simulation, a random character from the ASCII set is chosen. Next, many of its 56 bits are ran- domly determined to introduce noise. A large feedback gain (a = 1000) has been used. Uniformly distributed noise between f0.5 has been added to the vector n in order to separate identical responses. Since Theorem 1 gives constraints on the minimum slope needed for fl(-), an activation function with a sharper slope has been used, with gain = 30: fl(z) = l+e--ga,nXI 1  4.1 Autaassociative Experiments  Figure 4 demonstrates some of the results found for autoassociation. In each Figure, the ASCII character originally chosen is depicted in the first column. The second column depicts that character after the random bits are added to it. This noisy image forms the input, U, to the neural network. The third column shows the neural network?s output after convergence. In all cases, the network converges to the ASCII character with the smallest Hamming distance from the noisy input char- acter.

Figure 4. Three ASCII characters recognized by the neural network. The top row has 11 bit errors.

The middle row also has 11 bit errors, but the next best match has only one more bit error. The bottom row contains 13 bit errors.

5 Conclusion  A new, recurrent neural network has been proposed which allows for very simple storage of patterns, and the memory capacity is large. The input weights have only binary values, in contrast to many traditional as- sociative memories, where integer values are required.

Consequently, the network is simple and cost effective to implement.

By designing the recurrent WTA portion of the net- work according to the rules developed herein, it will be guaranteed to produce an output which is closest (in the Hamming distance sense) to the input pattern.

