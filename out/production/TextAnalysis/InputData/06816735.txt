A Crowd-Based Route Recommendation System?CrowdPlanner

Abstract? Route recommendation service has become a big business in industry since traveling is now an important part of our daily life. We can travel to unknown places by simply typing in our destination and then following recommendation service?s guidance, that a pleasant trip desires them to provide a good route. However, previous research shows that even the routes recommended by the big-thumb service providers can deviate significantly from the routes travelled by experienced drivers since the many latent factors affect drivers? preferences and it is hard for a single route recommendation algorithm to model all of them. In this demo we will present the CrowPlanner system to leverage crowds? knowledge to improve the recommendation quality. It requests human workers to evaluate candidates routes recommended by different sources and methods, and determines the best route based on the feedbacks of these workers. In this demo, we first introduce the core component of our system for smart question generation, and then show several real route recommendation cases and the feedback of users.



I. INTRODUCTION  Traveling is an important part of our daily life. Thanks to the route recommendation services powered by mainstream searching engines and GPS modules in our cars, we can travel to unknown places by simply typing in our start place and destination and then following their guidance. A pleasant trip desires them to provide good route, since our trip entirely replies on them. With the increasing number of users who rely on these map services to travel, a natural question arises: are these routes always good enough to be the best choice when people travel? According to [1], the similarities between the routes recommended by a big thumb map company and the routes drivers prefer decrease with the growth of the trip length, that the similarities are below 50% when the lengths of the trips beyonds 20 kilometers. The actual cause of this phenomenon is that there are too many latent factors which affect drivers? preferences, such as the number of turns, traffic lights, speed bump and so on, affecting the quality of a recommended route. That is to say, drivers? preferences are the ultimate criterion to judge the quality of a route, i.e., given a source and a destination, route A is regarded as a better choice than route B if more drivers prefer to drive through A with some reasons.

With the observation and awareness of the differences between recommended routes of web services and human preferences, and the differences results provided by different human preference detection algorithms, a route evaluating process is necessary, which rates the confidence level of a route to be best, for the purpose of giving users the best  Fig. 1. System Overview  routes. In this paper, we propose a CrowdPlanner system which combines the strengths of computers and human brains to recommend the best route with respect to the knowledge of experienced drivers. Instead of proposing a new route recommendation algorithm, our work provides some candidate routes and requests workers (some experienced drivers) to make a choice among them.

It is a non-trivial task to ask workers to evaluate these routes.

First, it is not easy to automatically generate a user-friendly task. Since the human?s knowledge of routes are quite different from computers, that is human remember routes discretely while computer process routes continuously. Besides, as the performance of system largely relies on the quality of an answer given by each worker, how to choose a set of suitable workers for a given task is another problem we need to solve.

CrowdPlanner tackles the above challenges using two carefully designed core components. It can generate user friendly tasks to suitable workers.



II. SYSTEM OVERVIEW  CrowdPlanner, as Figure 1 shown, is a two-layer system (mobile client layer and server layer) which give the ver- ified best routes to users. The workflow of CrowdPlanner is as follows: the traditional route recommendation module (TR) firstly processes user?s request by evaluating the quality of candidate routes obtained from external sources such as map services and historical trajectory mining; the crowd- based route recommendation module (CR) will generate a crowdsourcing task when the TR module can not determine the quality of candidate routes, and recommend the most promising route based on the feedbacks of human workers of the system.

1) Traditional Route Recommendation Module: Just as the Fig. 2(a) shown, when a client user submit a recommendation  Historical trajectories  Map web services  M  Route Generation  Verified truth  Reuse truth Control logic  Route Evaluation  Worker Selection  Task Generation  Rewarding  Early stop  Route request Response  a task  Assign task  TR  CR     request, which specifies she wants to get the best route from ?Nanjing Confucius Temple? to ?Nanjing Railway Station? in fifteen minutes and she awards the request for five coins (the virtual currency of CrowdPlanner), the control logic component of the traditional route evaluating module will handle it.

(a) A Route Recommenda- tion Request  (b) Recommended routes from dif- ferent sources  Fig. 2. A Route Evaluating Task  Control Logic component: Once a route plan request is handed to control logic, it will invoke reuse truth component to match the request to the verified routes (truth) between two places at her departure time. If the new coming request is a hit of the truth, the system will return result immediately.

Otherwise the component will invoke the route evaluation component to automatically generate some candidate routes and evaluate the qualities of these candidate routes using the verified truth.

Route evaluation component: The evaluating routes com- ponent evaluates the routes using computer power and it provides a efficient way to reduce the cost of CrowdPlanner, since it can largely reduce the amount of publishing evaluating tasks. The component will firstly build up a candidate routes set by invoking route generation component. If mining routes agree to each other and the frequency of these mining routes is high enough (bigger than a threshold) or these mining routes agree to a web service provided route, the mining routes will be regarded as the best recommended route, which will also be added into truth database with a time tag. If a best recommended route can not be decided, then the system will assign each candidate route a confidence score, which is generated by the verified truths and illustrates the possibility of the route to be best recommended. A route with the highest confidence score and the score is bigger than a threshold will be regarded to be the best recommended and system return it to the requester; otherwise the logic control will invoke the crowd-based route recommendation module to assign a route evaluating task to some suitable workers.

Route generation component: Route generation compo- nent generates two types of candidate routes, that one type of candidates is provided by web services and the other type of candidates is generated from historical trajectories by using  several well known algorithms, i.e., MRP [2], LDR [1] and MFP [7].

2) Crowd Route Recommendation Module: The crowd- based route recommendation module will take over the routes evaluating job while the traditional route recommendation module cannot provide a high confidence best route. The crowd-based route recommendation module will assign a eval- uating task, well designed by task generation component, to some target workers selected by worker selection component.

Task generation component: As we know, questions have different difficulties, for example, binary choice questions are easier than multiple choice questions. It?s a common sense that people prefer to answer simple questions. So in order to attract more people to response to the evaluating tasks, we need task generation component to generate concrete but simple tasks.

Worker selection component: The crowd evaluating pro- cess is a both time costly and manpower costly procedure.

In order to reduce the time cost and manpower cost, we use worker selecting component to assign a task to some most suitable workers, i.e., people lives near the destination.

Early stop component: We do not to need to wait for all the feedbacks of the assigned workers to decide the best route among all the candidate routes. Some already collected feedbacks can give us a best route with a confidence higher than a threshold, in this case the answer can be regarded as the best route.

Reward workers component: The workers denote their effort to choose their own best route in a task, thus the worker should be rewarded by some virtual currencies of our system, which can be used when they submit a best route recommendation request.



III. TECHNICAL BACKGROUND  If the system cannot automatically evaluate these candidate routes, it will generate a route evaluation task and assign it to some eligible workers.

A. Task Generation  The task format is one of the key factors which can affect the complexity of a task. We cannot simply generate a question by asking a human to describe the best route in a turn-by- turn manner, since this is not the natural way for human to memorize and describe a route. As a more user-friendly alternative, we may provide several pictures, showing several candidate routes on a map, as a multiple-choice question for workers to answer. However, even when all the routes have been visualized on a map, it is still effort-demanding to tell the subtle differences between candidate routes, especially on mobile devices with relatively small resolutions. To make the question easier to answer, we observe that it is natural for humans to utilize significant locations, i.e., landmarks, to help describe a route in high-level, whereas a computer sees a route as a sequence of continuous roads segments. Thus, we choose to proactively present the differences in candidate routes to the workers using landmarks in explicit manner, instead of asking them to find out. Thus, we design each task as a     series of binary questions, such as ?do you prefer the route passing landmark A at 2:00pm??, and each question relating to a landmark can discriminate some of the candidate routes from the others. Originally, the candidate routes are continuous paths, in order to obtain the landmark-based route from a raw path, we employ trajectory calibration [11] proposed in our previous research to rewrite the continuous paths into landmark-based routes.

Next we will present in brief our task generation process, which can be divided into three phases. The first phase infers the significance of each landmark which indicates people?s familiarity with it. The second phase utilizes a set of most significant landmarks to summarize the differences among the candidate routes. The third phase generates the final task by ordering the questions in a smart way so that the expected number of issued questions is as small as possible.

1) Inferring Landmark Significance: It is common sense that landmarks have different significances to people. For in- stance, the Nanjing Railway station is well known in Nanjing, but the Longpan road, where the Nanjing Railway station is located, is only known by people living in the local area.

In this work, we utilize the online check-in records from a popular location-based social network and trajectories of vehicles to infer the significance of landmarks. By regarding the travellers as authorities, landmarks as hubs, and check- ins/visits as hyperlinks, we can adopt a HITS-like algorithm such as [12] to infer the significance of a landmark.

2) Landmark Selection: Although any landmark can be used to generate a question, not all of them are suitable for the purpose of generating easy questions for a certain candidate route set R?. First, the selected landmark set L should be discriminative to the candidate routes R?, which ensures that the difference between any two routes can be presented. Second, the landmarks of L should have high significances, so that more people can answer the question accurately. Third, in order to reduce the workload of workers, the selected landmark set L should be as small as possible. Therefore the problem of landmark selection is to find a small set of highly significant landmarks which are discriminative to all the candidate routes.

It can be formally represented as an optimization problem which is a non-trivial task. To speed up this process, we propose a greedy algorithm, called GreedySelect to enumerate all the possible landmark combinations in a smart order.

3) Question Ordering: In the previous step we select a set of significant landmarks that will be used for generating questions. However, presenting those question to workers with random order is unwise because of the following two reasons: 1) it is not necessary to ask all the questions in most cases. For example, there is no need to present the landmarks on the paths that are not favored by the work based her previous answers.

2) each time we ask a question, we expect to obtain the most informative feedback, which is more likely to identify the final answer. This implies that 1) the next question to ask depends on the answer of the previous question, so the question order is a hierarchical structure; 2) the informativeness of a question (landmark) l is proportional to people?s familiarity with the  landmark (the significance of the landmark), and how many routes the landmark can discriminate (the information gain if we ask the question). In order to get more information after each question, we employ the Iterative Dichotomiser 3 (ID3) algorithm [10], which recursively selects the question with the largest informativeness as the next question, to find the optimal question order. .

B. Worker Selection  Some Crowdsourcing platforms such as AMT and Crowd- Flower give workers the freedom to choose any questions.

However this may cause some problems, such as many work- ers choose to answer the same question while some other questions are not picked by anyone; workers have to view all the questions before they choose; workers may answer questions that they are not familiar with. CrowdPlanner avoids these problems by designing a dedicated component to assign each task to a set of eligible workers. In order to judge whether a worker is eligible for a task, many aspects of the worker have to be taken into consideration, i.e., number of outstanding tasks, worker?s response time and familiarity with a certain area.

1) Number Of Outstanding Tasks: First, in order to balance the workload and reduce the response time, we use a threshold to restrict the maximum number of outstanding tasks for each worker.

2) Response Time: Each task has a user-specified response time, within which an answer must be returned. We use exponential distribution to model the probability of a worker to answer a question within a certain time. Based on it, if the probability of a worker to respond a task within time t is less than the threshold, we will not assign the task to him.

3) Familiarity Score: In order to improve the accuracy of the route evaluation task, we should select workers who are highly familiar with the landmarks relating to a given task.

Before selecting worker, we should start with measuring the familiarity score of each worker about each landmark.

Measuring Familiarity Score People usually have the best knowledge for areas where they live or visit frequently. Ac- cordingly, the familiarity score f lw to estimate the knowledge of a worker w about a landmark l is decided by two factors: (1) worker?s profile information, including her home address, work place and familiar suburbs, which can be collected during her registration to the system, and (2) history of worker?s ac- complished tasks around this area. Besides, workers who have similar profile information or have answered several similar questions are highly possible to share the similar knowledge.

For example, if a worker w1 has high familiarity score with l1, l2 and l3 and another worker w2 living nearby has high familiarity score with l1 and l2, w2 is also likely to be familiar with l3 though w2 has not answered any question relating to l3.

Here, we employ Probabilistic Matrix Factorization (PMF) [8] to predict familiarity scores of workers on landmarks using the latent similarity between workers  Finding Top-k Eligible Workers The top-k eligible work- ers are workers who has the best knowledge of the task.

We cannot simply add up a worker?s accumulated familiarity scores on all the landmarks of a task, since it may lead to biased result in worker selection. Thus not only the famil- iarity score of landmarks of a task, but also the coverage of knowledge, should be taken into consideration. The process is quite similar to rated voting system [9], in which the wining opinion is chosen according to the voters preferences score of an opinion and the number of voters preferring the opinion.

In our system, we can treat each landmark of a task as a voter and each worker as an opinion. Applying the weighted voting algorithm, we can find the top-k eligible workers.



IV. DEMONSTRATION  In this demonstration, CrowdPlanner will engage our demonstration audience to submit their route recommendation request and evaluate routes themselves. The data sets used in the demo are generated by taxis and private cars in Beijing and Nanjing (big cities of China). We get two POI datasets of the Beijing and Nanjing cities from a reliable third-party company in China. We invoke route recommendation API of a big thumb company to provide three candidate routes, and use three popular route mining algorithms, MPR, LDR and MFP, to provide three popular routes between two places. Tens of volunteers have played with the application, so their familiarity score of landmarks have already been measured. Besides, five smartphones will be used during the demonstration, one for requesters to submit a route recommendation request and the other four for workers to evaluate the task. These four smartphones have been assigned with different familiarity scores in different regions.

We start the demonstration by briefly explaining the work- flow of our system. Then, we let our audience submit a route recommendation request by specifying her start location, destination, leaving time (15 min later by default) and rewards on the request smartphone. For example shown in Fig. 2(a), a user submits a recommendation request, which specifies that she wants to travel from ?Nanjing Confucius Temple? to ?Nanjing Railway Station? fifteen minutes later (about 1:48am) and she awards the request for five coins.

After receiving the request, the server matches the request to the verified routes and generating four candidates routes, as shown in Fig. 2(b), by invoking web services and popular route mining algorithms. If the automatic evaluation is of high confidence, our audience will see the recommendation result displayed immediately on the request smartphone; otherwise, a route evaluation task will be sent to one or more labor smartphone based on their familarity scores. For instance, Fig. 3 illustrates the evaluation task of Fig. 2(a) on a mobile client, where four candidate routes from Nanjing Confucius temple to Nanjing railway station are shown in red and blue lines. The first binary question is ?do you prefer to go pass ?Xinjiekou? from Nanjing Confucius temple to Nanjing railway station at 1:48??. Since Xinjiekou is one of the busiest commercial districts of Nanjing. Experienced traveller may not prefer to pass it, which excludes the two red routes. The second question is whether the route should pass ?Jiuhuashan Tunnel?.

As shown in Fig. 3(b), ?Jiuhuashan Tunnel? is the most famous tunnel under the Xuanwu Lake, which is the major difference between the two routes left. From this showcase we can see that CrowdPlanner can select the most significant landmarks to distinguish the candidate routes and ask questions in the most effective order. Also, we will show the profile information and task history of workers, to whom the task is assigned. In this way, we will show our audience the power of worker selecting.

