IMPACT 2009

Abstract-Clustering techniques have been used by many intelligent software agents in order to retrieve, filter, and categorize documents available on the World Wide Web. Clustering is also useful in extracting salient features of related web documents to automatically formulate queries and search for other similar documents on the Web. Traditional clustering algorithms either use a priori knowledge of document structures to define a distance or similarity among these documents, or use probabilistic techniques such as Bayesian classification. Many of these traditional algorithms, however, falter when the dimensionality of the feature space becomes high relative to the size of the document space. In this paper, we introduce two new clustering algorithms that can effectively cluster documents, even in the presence of a very high dimensional feature space. These clustering techniques which are based on generalizations of graph partitioning, do not require pre-specified ad hoc distance functions, and are capable of automatically discovering document similarities or associations. We conduct several experiments on real Web data using various feature selection heuristics, and compare our clustering schemes to standard distance-based techniques, such as hierarchical agglomeration clustering, and Bayesian classification methods, AutoClass.

1. INTRODUCTION    The World Wide Web is a vast resource of information and services that continues to grow rapidly. Powerful search engines have been developed to aid in locating unfamiliar documents by category, contents, or subject.

Relying on large indexes to documents located on the web, search engines determine the URLs of those documents satisfying a user's query. Often queries return inconsistent search results, with document referrals that meet the search criteria but are of no interest to the user.

While it may not be currently feasible to extract in full the meaning of an HTML document, intelligent software agents have been developed which extract semantic features from the words or structure of an HTML document. These extracted features are then employed to classify and categorize the documents.

Clustering offers the advantage that a priori knowledge of categories is not needed, so the categorization process is unsupervised. The results of clustering could then be used to automatically formulate queries and search for other similar documents on the Web, or to organize bookmark files, or to construct a user profile.

In this paper, we present two new clustering algorithms based on graph partitioning and compare their performance against more traditional clustering algorithms used in information retrieval. Traditional clustering algorithms either define a distance or similarity among documents, or use probabilistic techniques such as Bayesian classification. Many of these algorithms, however, break down as the size of the document space, and hence, the dimensionality of the corresponding feature space increases. High dimensionality is characteristic of the type of information retrieval applications which are used to filter and categorize hypertext documents on the World Wide Web. In contrast, our partitioning-based algorithms do not rely on pre-specified or ad hoc notions of distance, and they perform well in the presence of a high dimensional space.

2. CLUSTERING METHODS   Most of the existing approaches to document clustering are based on either probabilistic methods, or distance and similarity measures (see [9]). Distance-based methods such as k-means analysis, hierarchical clustering [12] and nearest-neighbor clustering [15] use a selected set of words (features) appearing in different documents as the dimensions. Each such feature vector, representing a document, can be viewed as a point in this multi-dimensional space.

There are a number of problems with clustering in a   IBM Rectangle    IMPACT 2009   multi-dimensional space using traditional distance- or probability-based methods. First, it is not trivial to define a distance measure in this space. Feature vectors must be scaled to avoid skewing the result by different document lengths or possibly by how common a word is across many documents. Techniques such as TFIDF [20] have been proposed precisely to deal with some of these problems.

Second, the number of different words in all the documents can be very large. Distance-based schemes generally require the calculation of the mean of document clusters, which are often chosen initially at random. In a high dimensional space, the cluster means will do a poor job at separating documents. We have found that hierarchical agglomeration clustering (HAC) [7], based on distances between sample cluster means, does a poor job on our examples. Similarly, probabilistic methods such as Bayesian classification used in AutoClass [6, 21] do not perform well when the size of the feature space is much larger than the size of the sample set or may depend on the independence of the underlying features. Web documents suffer from both high dimensionality and high correlation among the feature values. We have found AutoClass has performed poorly on our examples.

3. ASSOCIATION RULE HYPERGRAPH  PARTITIONING (ARHP)   In [11], a new method was proposed for clustering related items in transaction-based databases, such as supermarket bar code data, using association rules and hypergraph partitioning This method first finds set of items that occur frequently together in transactions using association rule discovery methods [1]. These frequent item sets are then used to group items into hypergraph edges, and a hypergraph partitioning algorithm [13] is used to find the item clusters. The similarity among items is captured implicitly by the frequent item sets.

In document clustering, each document corresponds to an item and each possible feature corresponds to a transaction. The association rule discovery algorithm is used to find sets of documents with many features in common (frequent item sets). Each frequent item sets must satisfy a user specified minimum support criteria which specifies a threshold on the minimum number of features common among documents in the set. A hypergraph [3] H = (V;E) is formed with vertices V consisting of the documents and hyperedges E representing the frequent item sets. Hyperedges are edges that can connect more than 2 vertices. To each hyperedge we associate a weight, which is calculated as the average confidence of all the association rules  involving the related documents of the hyperedge, where each individual confidence level is the conditional probability that a feature occurs in a document or group of documents given that it occurs in the remaining documents in that hyperedge.

Next, a hypergraph partitioning algorithm is used to partition the hypergraph such that the weight of the hyperedges that are cut by the partitioning is minimized.

We are essentially minimizing the relations that are violated by partitioning the documents into different clusters. Similarly, this method can be applied to word clustering. In this setting, each word would correspond to an item and each document would correspond to a transaction. This method uses the Apriori algorithm [1] which has been shown to be very efficient in finding frequent item sets and HMETIS [13] which can partition very large hypergraphs (of size > 100K nodes) in minutes on personal computers. Furthermore, the support criteria on frequent item sets can be used to filter out documents that are less likely to be related to other documents in each cluster.

4. PRINCIPAL COMPONENT ANALYSIS (PCA)  PARTITIONING ALGORITHM  In the principal component algorithm, each document is represented by a feature vector of word frequencies, scaled to unit length. TFIDF scaling could be used, but simple scaling to unit length was found to achieve better clustering results in less time, at least on the data sets we tried. The algorithm proceeds by cutting the entire space of documents with a hyperplane passing through the overall arithmetic mean of the documents, and normal to the principal direction (direction of maximum variance) for the document set. The documents are split into two separate groups by means of the hyperplane in a manner similar to a linear discriminant function, then each group is further split in the same manner. This is repeated as many times as desired, resulting in a tree-like hierarchy. The leaves of the tree are document clusters, each with a computed mean and principal direction. We use a scatter value, measuring the average distance from the documents in a cluster to the mean [7], to determine the next cluster to split at each stage.

The definition of the hyperplane is based on principal component analysis, similar to the Hotelling or Karhunen-Loeve Transformation [7]. We compute the principal direction as the leading eigenvector of the sample covariance matrix. This is the most expensive part, for which we use a fast Lanczos-based singular value solver [10].

