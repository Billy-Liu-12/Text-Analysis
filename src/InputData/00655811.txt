Mining Association Rules: Anti-Skew Algorithms ?

Abstract Mining association rules among items in a large  database has been recognized as one of the most impor- tant data mining problems. All proposed approaches for this problem require scanning the entire database at least or almost twice in the worst case. In this pa- per we propose several techniques which overcome the problem of data skew in the basket data. These tech- niques reduce the maximum number of scans to less than 2, and in most cases ?nd all association rules in about 1 scan. Our algorithms employ prior knowledge collected during the mining process and/or via sam- pling, to further reduce the number of candidate item- sets and identify false candidate itemsets at an earlier stage.

1 Introduction Recently, data mining has attracted much attention  in the database community [3, 4]. One of the most in- vestigated topics in data mining is the problem of dis- covering association rules over basket data [1, 6, 8, 9].

Basket data, which typically contains items purchased by a customer, are collected at the point-of-sales sys- tem in a retail business. Each basket data de?nes a transaction. With the advance of bar-code technol- ogy, businesses can e?ectively gather a vast amount of basket data. However, to e?ectively extract use- ful knowledge over such basket data to assist decision making is a major challenge.

Mining association rules over basket data was ?rst introduced in [1]. Since then, association rules have been applied to other databases as well, for examples, telecommunication alarm data and university course enrollment data [6]. An association rule has three parts: the head, the body, and the con?dence of the rule. Both the head and the body are a set of items.

For example, if 90% of the customers who purchase A and B, also purchase C and D, then it can be repre- sented as an association rule as follows.

fA;Bg ! fC;Dg with confidence = 90%  In order to generate reliable results, the size of the database must be large [9]. Thus, an e?cient algo-  ?This research is based upon work supported by a Massive Digital Data System (MDDS) e?ort sponsored by the Advanced  Research and DevelopmentCommittee of the CommunityMan- agement Sta?.

rithm to discover the association rules must try to re- duce I/O as much as possible. Many algorithms have been proposed in the literature [1, 6, 8, 9]. Among them, the Partition algorithm [8] and the Sampling algorithm [9] require the least amount of I/O to ac- complish the task. Both the Partition algorithm and the Sampling algorithm require two complete scans over the database in the worst case and one complete scan in the best case. Also, a variation of the Partition algorithm called SPINC [7] requires 2n?1  n scans in the  worst case, where n is the number of partitions.

In this paper we propose and investigate a family  of algorithms called Anti-Skew Counting Partition Al- gorithms (AS-CPA). Like SPINC, AS-CPA only needs to scan the database once in the best case, and 2n?1  n  times in the worst case. Several techniques have been incorporated into AS-CPA to reduce the possibility of worst case behavior. In case a random sample can be drawn, a Random Sampling version of AS-CPA (de- noted as RSAS-CPA) outperforms the Sampling algo- rithm whenever the Sampling algorithm fails to ?nd all association rules during the ?rst scan. RSAS-CPA has the same best case performance (i.e., 1 scan) as the Sampling algorithm. In case a random sample can not be drawn without scanning the database from the beginning, a Sequential Sampling version of AS-CPA (denoted as SSAS-CPA) can be applied. It is impor- tant to note that our techniques employ prior knowl- edge to remove false candidates in an earlier stage which in turn reduces the CPU and memory overhead.

The rest of this paper is organized as follows. Sec- tion 2 gives a formal description of the problem, and describes previous work. Sections 3 and 4 present AS-CPA and Sampling AS-CPA, respectively. Sec- tion 5 gives a performance comparison among these algorithms, and Section 6 concludes this paper.

2 Mining Association Rules This section gives a formal description of the prob-  lem of mining association rules over basket data, mostly based on the description in [1].

Let I = fi1; i2; : : : ; img be a set of literals called items. Let D be a set of transactions. Each transac- tion is a non-empty subset of I, and is associated with a unique identi?er called TID.

A set of items is called an itemset. An itemset with k items in it, is called a k-itemset. Each itemset X ? I has a measurement of statistical signi?cance    in D, called support, where support(X;D) equals the fraction of transactions in D containing X.

An association rule is an implication of the form X ) Y , where X;Y ? I, and X \ Y = ;. Each rule X ) Y has a measurement of strength, called con?-  dence, where confidence(X ) Y ) = support(X[Y;D) support(X;D)  .

The problem of mining association rules is to ?nd all the association rules of the form X ) Y that sat- isfy the following two conditions:  1. support(X [ Y;D) ? min support  2. confidence(X ) Y ) ? min confidence  Here, min confidence and min support are user speci?ed. This problem can be decomposed into two subproblems:  1. Find all the itemsets that have support above min support. These itemsets are called the large itemsets.

2. For each large itemset X and any Y ? X, check if rule (X ? Y ) ) Y has con?dence above min confidence.

After ?nding all large itemsets, the second subprob- lem can be solved in a straightforward manner. How- ever, ?nding all large itemsets is not trivial because the size of D and the number of subsets of I can be very large. Thus, most previous work focuses on how to ?nd all the large itemsets e?ciently. The rest of this paper will also focus on this subproblem only.

Several algorithms have been proposed on ?nding large itemsets [1, 6, 8, 9]. All previous approaches to ?nding large itemsets construct a set of candidate itemsets, and verify if a candidate itemset is a large itemset. Since the size of the database can be very large, most algorithms aim at reducing the number of scans needed over the entire database and the number of candidate itemsets.

2.1 Level-Wise Algorithms The idea behind level-wise algorithms is based on  the following observation. If X is a large itemset, then any non-empty subset of X is also a large itemset.

Let Lk be the set of all large k-itemsets, and Ck be the set of candidate k-itemsets. Level-wise algorithms [4, 6] use Lk to generate Ck+1 so that Ck+1 is a su- perset of Lk+1. Then, the algorithms construct Lk+1 by scanning the database once to check if c is a large itemset, for each c 2 Ck+1. The same process repeats for the next level until no large itemset is found. As a result, if the largest large itemset is a j-itemset, then the level-wise algorithms need to scan the database at most (j + 1) times.

2.2 Partition Algorithm The idea behind the Partition algorithm [8] is based  on lemma 1.

Lemma 1 If X is a large itemset in database D, which is divided into n partitions p1; p2; : : : ; pn, then X must be a large itemset in at least one of the n par- titions.

Proof: Prove by contrapositive. Assume that X is not a large itemset in any of the n partitions, and prove that X must be a small itemset in D. 2  The Partition algorithmdivides D into n partitions, and processes one partition in main memory at a time.

The algorithm ?rst scans partition pi, for i = 1 to n, to ?nd the set of all local large itemsets in pi, denoted as Lpi . Then, by taking the union of Lpi for i = 1 to n, a set of candidate itemsets over D is constructed, denoted as CG. Based on Lemma 1, CG is a super- set of the set of all large itemsets in D. Finally, the algorithm scans each partition for the second time to calculate the support of each itemset inCG and to ?nd out which candidate itemsets are really large itemsets in D. Thus, only two scans are needed to ?nd all the large itemsets in D. Only one scan is needed if all partitions have identical local large itemsets.

SPINC is a variation of the Partition algorithmpro- posed in [7]. Instead of constructing CG by taking the union of Lpi for i = 1 to n at the end of the ?rst scan as in the Partition algorithm, it constructs CG  incrementally by adding Lpi to CG whenever Lpi is available. SPINC starts counting the number of oc- currences for each candidate itemset c 2 CG as soon as c is added to CG. Thus, after the ?rst scan, if SPINC added the last candidate itemset to CG when processing partition pj, then it only needs to scan up to partition pj?1 for the second scan. Thus, in the best case only 1 scan is needed, and in the worst case 2n?1 n  scans are needed.

2.3 Sampling Algorithm  Using sampling to assist ?nding all association rules has been discussed in [6, 9, 10]. Among them, the Sampling algorithm proposed by [9] has the best per- formance. The Sampling algorithm [9] ?rst takes a random sample of the database D, and ?nds the set of large itemsets (denoted as S) in the sample using a smaller min support than the one the user speci- ?ed. Then, the algorithm calculates the set of item- sets, which are in the Bd?(S). Bd?(S) is the set of minimal itemsets X ? I that X is not in S [9]. The algorithm scans D to check if c is a large itemset in D, for each itemset c 2 S [Bd?(S).

If there is no large itemset in Bd?(S), the algo- rithm has found all the large itemsets. Otherwise, the algorithm constructs a set of candidate itemsets (denoted as CG) by expanding the negative border of S [ Bd?(S) recursively until the negative border is empty. Finally, the algorithm scans D for the second time to check if c is a large itemset for each c 2 CG. In the best case, this algorithm needs only 1 scan over D.

In the worst case, two scans are needed. It should be noted that the number of candidate itemsets generated for the second scan can be very large if a bad sample was drawn for the ?rst scan. As a result, the sec- ond scan can be very ine?cient. Also, using a smaller min support for the sample and expanding with the negative border could result in a large CG, making the ?rst scan very ine?cient.

3 Anti-Skew Counting Partition Algo-  rithms One of the major problems of the Partition algo-  rithm (and the SPINC algorithm as well) is data skew, which refers to the irregularity of data distribution over the entire database D. An example from [8] is severe weather conditions causing the sales of some items to increase rapidly only for a short period of time. Data skew can cause both algorithms to gener- ate many false candidate itemsets. One way to over- come data skew is to increase the randomness of data across all partitions [8]. However, this conicts with the goal of exploiting sequential I/O to speed up read- ing the database.

Even without data skew, unless each item is dis- tributed rather uniformly over D, and the size of each partition is large enough to capture this uniformness, the chance of a local large itemset (i.e., a candidate itemset) being a global large itemset can be small, and the number of candidate itemsets generated can be very large. It is obvious that the above problems can be mitigated with large partitions. However, this conicts with the main idea of the Partition algorithm: processing one partition in memory at a time to avoid multiple scans over D from disk.

In what follows, we propose a variation of the SPINC algorithm, called Anti-Skew Counting Parti- tion Algorithm (AS-CPA), that makes use of the cu- mulative count of each candidate itemset to achieve the illusion of a large partition. Like the Partition Algorithm, AS-CPA also divides the database D into n disjoint partitions, and processes one partition at a time. A partition is a subset of transactions contained in D. The notation shown in Table 1 is partly based on the notation in [8]. An itemset x is a local large item- set in partition pi if support(x; pi) ? min support. An itemset x is a global large itemset if support(x;D) ? min support.

Like the SPINC algorithm, AS-CPA adds Li to CG  as soon as Li is available during the ?rst scan. AS- CPA starts counting the number of occurrences for each candidate itemset c as soon as c is added to CG.

Each candidate itemset c 2 CG has two attributes: c:age contains the partition number when c was added to CG, and c:count contains the number of occurrences of c since c was added to CG. At the end of the ?rst scan, we already have the number of occurrences over D for each candidate itemset c 2 CG with c:age = 1, thus we can check if c is a large itemset immediately.

After that, we remove those itemsets with age = 1 from CG. Then, we start the second scan on partition p1, and count the number of occurrence over p1 for each itemset c 2 CG and add it to c:count. At this moment, we have the the number of occurrences over D for each candidate itemset c 2 CG with c:age = 2, thus we can check if c is a large itemset and remove c from CG. Repeat the same procedure for partitions p2; p3; : : : ; pi until C  G is empty.

So far, the description of AS-CPA is just the same  as SPINC. The main feature that separates AS-CPA from SPINC is that AS-CPA provides several e?ec-  DBi Set of all 1-itemsets in partition pi and their tidlists  Li1 Set of all local large 1-itemsets in parti- tion pi  Li Set of all local large itemsets (not includ- ing 1-itemsets) in partition pi  L1:::i1 Set of all local large 1-itemsets inS j=1:::i pj  Bi1 Set of all better local large 1-itemsets in partition pi  Bi Set of all better local large itemsets (not including 1-itemsets) in partition pi  CG Set of all global candidate itemsets LG Set of all global large itemsets  Table 1: Notation  tive techniques to ?lter out false candidate itemsets at an earlier stage. This reduces the probability of worse case behavior. We discuss these techniques in the following three subsections.

3.1 Early Local Pruning  To generate the set Li of all local large itemsets in a partition pi, both the Partition algorithm and SPINC ?rst ?nd the set of all local large 1-itemsets in pi and use a level-wise algorithm to ?nd all the local large itemsets in pi. With early local pruning, we take a similar but more e?cient approach.

The idea of early local pruning is as follows. When reading a partition pi to generate L  i 1, we record and  accumulate the number of occurrences for each item.

Thus, after reading partition pi, we know both the number of occurrences for each item in partition pi, and the number of occurrences for each item in the big partition  S j=1;2;:::;i pj. With this information, we  know both the set Li1 of all local large 1-itemsets in pi, and the set L  1:::i 1 of all local large 1-itemsets in the  big partition S j=1;2;:::;i pj . We de?ne B  i 1 as the set of  all better local large 1-itemsets in partition pi, where Bi1 = L  i 1 \ L  1:::i 1 . With early local pruning, we use  Bi1 instead of L i 1 to start the level-wise algorithm to  construct the set Bi of all better local large itemsets in partition pi. Since the size of B  i 1 is usually smaller  than that of Li1, B i can be constructed faster than Li.

With Partition algorithm, Lemma 1 ensures that CG =  S i=1;2;:::;nL  pi is a superset of the set LG of  all global large itemsets. With early local pruning, Lemma 2 shows that CG =  S i=1;2;:::;nB  pi is also a  superset of LG.

Lemma 2 Let the database D be divided into n par- titions, p1; p2; : : : ; pn. Let C  G = S i=1;2;:::;nB  i. Then,  CG is a superset of LG.

Proof: prove by mathematical induction. We denote the set of all global large itemsets of a database D as LG(D).

Base case: D contains only 1 partition, Lemma 2 is true (trivial).

Induction Hypothesis: if D =  S i=1;2;:::;n pi, thenS  i=1;2;:::;nB i is a superset of LG(D). That is,  LG(D) ? S i=1;2;:::;nB  i  Induction Step: Consider a new database D  = D [ pn+1, and prove that  S i=1;2;:::;n;(n+1)B  i is a su-  perset of LG(D  ). Consider D  as a database with 2 partitions: D and pn+1. According to Lemma 1 and the Induction Hypothesis, we have,  LG(D  ) ? LG(D) [ Ln+1  ? S i=1;2;:::;nB  i [ Ln+1  (case 1) Ln+11 ? L 1:::(n+1) 1 :  Bn+11 = L n+1 1 \ L  1:::(n+1) 1 = L  n+1  =) Bn+1 = Ln+1  =) LG(D  ) ? S i=1;2;:::;n;(n+1)B  i  (case 2) Ln+11 is not a subset of L 1:::(n+1) 1 : there exists  some 1-itemset l 2 Ln+11 but not in L 1:::(n+1) 1 .

Since l is not in L 1:::(n+1) 1 , any superset of l is not  in LG(D  ). Thus, we can remove all such l from  Ln+11 , and eventually L n+1 1 will become a subset  of L 1:::(n+1) 1 ; case 2 degrades to case 1. 2  3.2 First Global Anti-Skew The ?rst global anti-skew technique aims at remov-  ing possibly false global candidate itemsets during the ?rst scan over the database D. It can be used with or without the early local pruning technique. In what fol- lows, we ?rst discuss how it is used without the early local pruning technique, and then discuss how it can be used with the early local pruning technique.

Lemma 3 Let the database D be divided into n par- titions, p1; p2; : : : ; pn. Assume that itemset x is not a local large itemset in either of the following two big partitions:  S j=1;2;:::;k?1 pj , and  S j=k;k+1;k+2;:::;i pj.

If x is a large itemset over D, x must be a local large itemset in at least one of the following partitions: pi+1; pi+2; : : : ; pn.

Proof: Lemma 3 is the direct result of Lemma 1. 2 Without early local pruning, CG is de?ned asS i=1;2;:::;n L  pi as in the Partition algorithm, and the  ?rst global anti-skew technique is based on Lemma 3. Note that for each c 2 CG, c is not a local large itemset in  S j=1;2;:::;c:age?1 pj because c was added  to CG when processing partition pc:age. During the ?rst scan, after processing partition pi, for each item- set c 2 CG, c:count contains the number of occur- rences of c in  S j=c:age;c:age+1;:::;i pj . Thus, we can  check if c is a local large itemset in the big partition BIG =  S j=c:age;c:age+1;:::;i pj. If c is not a local large  itemset in BIG, then c is a possibly false candidate itemset, and the ?rst global anti-skew technique re- moves c from CG. If c is really a global large item- set in D, according to Lemma 3, c must be a local  large itemset in at least one of the following partitions: pi+1; pi+2; : : : ; pn, and thus c will be added back to C  G  again.

Notice here we can also remove any superset s  of c from CG when we remove c from CG. The reason is as follows. Since c was added to CG  when processing partition pc:age, c is not a local large itemset in  S j=1;2;:::;c:age?1 pj. This, together  with the fact that c is not a local large itemset inS j=c:age;c:age+1;:::;i pj, indicates that c is not a local  large itemset in S j=1;2;:::;i pj, according to Lemma 1.

Since s is a superset of c, s is not a local large itemset in S j=1;2;:::;i pj either. If we remove s from C  G when  processing partition pi but s is actually a global large itemset in D, according to Lemma 1, s must be a local large itemset in at least one of the following partitions: pi+1; pi+2; : : : ; pn, and thus s will be added back to C  G  again.

To avoid removing and adding the same itemset re-  peatedly and to reduce the added CPU overhead of the ?rst global anti-skew technique, this procedure is applied to each c 2 CG just once as soon as c is con- sidered old enough. That is, if k partitions are consid- ered large enough to capture the uniformness of the database D, then when c:count contains the number of occurrences of c in k partitions, this procedure is applied to c, i.e. after processing partition pc:age+k?1.

With early local pruning, CG is de?ned asS i=1;2;:::;nB  pi , and the ?rst anti-skew technique is  based on Lemma 4 below.

Lemma 4 Let the database D be divided into n partitions, p1; p2; : : : ; pn. Assume that itemset x is not a better local large itemset in either of the following two big partitions:  S j=1;2;:::;k?1 pj, andS  j=k;k+1;k+2;:::;i pj . If x is a large itemset over D,  x must be a better local large itemset in at least one of the following partitions: pi+1; pi+2; : : : ; pn.

Proof: According to Lemma 2, if c 2 CG is a global large itemset in D, then c must be a better local large itemset in at least one of the partitions of D. Lemma 4 is the direct result of Lemma 2. 2  3.3 Second Global Anti-Skew The second global anti-skew technique is used to re-  move false global candidate itemsets during the second scan. It can be used with or without the earlier local pruning and the ?rst global anti-skew techniques. In what follows, we ?rst show how the second anti-skew technique is used without the earlier local pruning and the ?rst anti-skew techniques. Then, we show how to use the second anti-skew technique with the ?rst anti- skew technique.

Lemma 5 Let the database D be divided into n partitions, p1; p2; : : : ; pn. If itemset x is not a local large itemset in any of the following parti- tions: p1; p2; : : : ; pk?1, and  S j=k;k+1;k+2;:::;n pj [S  j=1;2;3;:::;i pj, where i < k, then x is not a large item-  set in D.

Proof: Since x is not a local large itemset in any of the following partitions: p1; p2; : : : ; pk?1, x is not a local large itemset in  S j=i+1;i+2;:::;k?1 pj . Imagine there  are only two partitions in D: S j=k;k+1;k+2;:::;n pj [S  j=1;2;3;:::;i pj and S j=i+1;i+2;:::;k?1 pj. Lemma 5 is  the direct result of Lemma 1. 2 During the second scan, after processing partition  pi, for each itemset c 2 C G, c:count contains the  number of occurrences of c in S j=c:age;c:age+1;:::;n pj [S  j=1;2;:::;i pj . Since c is added to C G when processing  partition pc:age, c is not a local large itemset in any of the following partitions: p1; p2; : : : ; pc:age?1. Here, we assume the ?rst anti-skew and early local prun- ing techniques are not used. According to Lemma 5, if c is not a large itemset in  S j=c:age;c:age+1;:::;n pj [S  j=1;2;:::;i pj , c is not a large itemset in D, and thus  we can remove c and all supersets of c from CG.

A less restricted version of Lemma 5 is needed in  order to support the combination of the ?rst and sec- ond anti-skew techniques. With ?rst anti-skew tech- nique, a candidate itemset c can be added to and re- moved from CG many times so that c might be a lo- cal large itemset in one of the following partitions: p1; p2; : : : ; pc:age?1. Thus, we can no longer apply Lemma 5 to support the second anti-skew technique.

Lemma 6 shows that both anti-skew techniques can coexist.

Lemma 6 Let the database D be divided into n par- titions, p1; p2; : : : ; pn. Assume that, at the end of the ?rst scan, an itemset c 2 CG has c:age = k. If c is not a local large itemset in  S j=k;k+1;k+2;:::;n pj [S  j=1;2;3;:::;i pj, where i < k, then c is not a large item-  set in D.

The proof for Lemma 6 can be found in [5]. Both Lemmas 5 and 6 are based on Lemma 1. Since Lemma 1 to local large itemsets is just the same as Lemma 2 to better local large itemsets, we can derive from Lemma 2 in a similar fashion to prove that the second anti-skew technique can be used with the early local pruning technique as well.

3.4 Algorithms AS-CPA is a family of algorithms. Each uses one or  more techniques to improve its performance, as shown in Table 2. Here we only list the algorithm for AS- CPA-12E. AS-CPA has adopted some procedures from their counterparts in the Partition algorithm [8]. As in [2, 8], we assume that a transaction in the database is in the form of < TID; ij ; ik; : : : ; ix >, and the items in a transaction are in lexicographic order.

Name early local  pruning  1st global  anti-skew  2nd global  anti-skew  AS-CPA-12 no yes yes  AS-CPA-2 no no yes  AS-CPA-12E yes yes yes  AS-CPA-2E yes no yes  Table 2: Algorithms  procedure AS-CPA-12E  1. P = partition database(D)  2. n = Number of partitions  3. m = Number of transactions in D 4. k = number of partitions to be considered as a big parti-  tion 5. for i = 1 to n 6. m[i] = Number of transactions in partition pi  7. CG = ; 8. for i = 1 to n begin // phase I  9. acc[i] = m[i]  10. for j = 1 to (i? 1)  11. acc[j] = acc[j] +m[i]  12. DBi = read and transpose(pi 2 P )  13. add count(CG; DBi)  14. Bi1 = gen better large 1-itemsets(DB i)  15. Bi = gen large itemsets(Bi1)  16. for each itemset l 2 Bi ? CG begin  17. l:age = i  18. l:count = jl:tidlistj  19. CG = CG [ flg  20. end  21. CG = ?rst anti skew(CG; acc[1::i])  22. end  23. CG = second anti skew(CG; acc[i; n]) // phase II  24. LG = fc 2 CGjc:age == 1g  25. CG = CG ? LG  26. i = 1  27. while (CG 6= ;) begin  28. DBi = read and transpose(pi 2 P )  29. add count(CG; DBi)  30. for j = (i+ 1) to n  31. acc[j] = acc[j] +m[i]  32. i = i+ 1  33. CG = second anti skew(CG; acc[i; n])  34. LG = LG [ fc 2 CGjc:age == ig  35. CG = CG ? fc 2 CGjc:age == ig  36. end  37. return LG  Initially, the database D is partitioned into n par- titions by executing procedure partition database, and CG is empty. During the phase I, the algorithm processes one partition at a time for all partitions.

When processing partition pi, the content of pi is read and transformed into DBi, which contains a tidlist data structure for each item in pi. Here, the tidlist of an item x contains the TID of all the transac- tions in partition pi that contains x. Thus, the length of the tidlist of an item x, denoted as jx:tidlistj, equals to the number of occurrences of x over par- tition pi. With DB  i, the algorithm then calls proce- dure add count to count the number of occurrences for each itemset c 2 CG, and add it to c:count. 1  1The tidlist data structure is proposed in [8]. By taking the  intersection of the tidlist of all items in an itemset, the length of the intersection gives the number of occurrences of the itemset.

In AS-CPA-12E, we use the early local pruning tech- nique, thus we also accumulate the number of oc- currences for each item and construct the set L1:::i1 of all local large 1-itemsets in  S j=1;2;:::;i pj in proce-  dure add count. Then, the algorithm calls procedure gen better large 1-itemsets to generate the set Bi1 of all the better local large 1-itemsets and their asso- ciated tidlist. Again, due to the early local pruning technique, the Bi1 generated here is L  i 1\L  1:::i 1 . At this  stage, the content of DBi is no longer needed.

procedure add count(CG; DBi)  1. L1:::i  = ;  2. for each x 2 I begin  3. x:count = x:count + jDBix:tidlistj  4. if x:count acc[1]  ? min support  5. L1:::i1 = L 1:::i 1 [ fxg  6. end  7. for each c 2 CG begin  8. num = number of occurrences of c in partition pi 9. c:count = c:count+ num  10. end  procedure gen better large 1-itemsets(DBi)  1. L11 = ;  2. for each x 2 I begin  3. if jDBi  x :tidlistj  acc[i] ? min support  4. Li1 = L i 1 [ fxg  5. end  6. return Li1 \ L 1:::i  The algorithm then calls procedure gen large itemsets to generate the set of all the lo- cal better large itemsets in partition pi, denoted as B  i.

Then, the algorithm starts to add itemsets from Bi to CG. When adding an itemset l from Bi to CG, we di?erentiate those in Bi that are already in CG (i.e., old candidate itemsets) and those in Bi that are not yet in CG (i.e., new candidate itemsets). If l is a old candidate, do nothing since its number of occurrences in partition pi has been already added to l:count when calling procedure add count. If l is a new candidate, we assign partition number i to l:age, assign the num- ber of occurrences of l in pi to l:count, and add l toC  G.

Then, the algorithm calls procedure ?rst anti skew to remove possibly false candidate itemsets and their supersets in CG.

procedure ?rst anti skew (CG; acc[i::j])  1. for each c 2 CG with c:age == i? k + 1 begin  2. if c:count acc[c:age]  < min support  3. CG = CG ? fs 2 CGjc ? sg  4. end  The same process repeats for the rest of the par- titions, then the algorithm enters the phase II. Note that, at this moment, for each candidate itemset c 2  CG, c:count contains the number of occurrences of c from partition pc:age to partition pn.

During the phase II, the algorithm calls procedure second anti skew to remove some false candidate itemsets in CG. Then, the algorithm moves each c 2 CG with c:age = 1 to LG. Then, the algorithm reads in partition 1, and updates the count for each itemset c 2 CG. The same process repeats until CG  is empty.

procedure second anti skew (CG; acc[i::j])  1. for each c 2 CG begin  2. if c:count acc[c:age]  < min support  3. CG = CG ? fs 2 CGjc ? sg  4. end  Note that, the only complex procedure called by CPA is gen large itemsets. Since AS-CPA uses the same tidlist data structure to count the number of occurrences of an itemset as in Partition algorithm, the same counterpart in Partition algorithm can be used in CPA with very little modi?cation. Please refer [8] for gen large itemsets.

4 Sampling Anti-Skew Counting Par-  tition Algorithm In this section, we present a sequential sampling  version of AS-CPA called SSAS-CPA, that uses the ?rst few partitions as a sample to locate all large item- sets at an earlier stage. With SSAS-CPA, the user also speci?es how many partitions are used as the sample.

If the ?rst k partitions are used as sample, then SSAS- CPA uses a min support smaller than the one the user speci?ed to determine the local large itemsets for those partitions. The rest of the algorithm is just the same as AS-CPA.

The idea of using a smallermin support for the sam- ple was proposed in the Sampling algorithm [9]. How- ever, instead of calculating the negative border of S to check if all large itemsets have been found at the end of the ?rst scan as in [9], SSAS-CPA simply executes procedure second anti skew and checks if there are any itemsets in CG with age greater than 1. The Sampling algorithm in [9] needs a second complete scan over D if it ?nds any large itemsets not in S. SSAS-CPA needs a second partial scan over D if it ?nds any large itemsets with age greater than 1.

There are several advantages in SSAS-CPA. First, the sampling phase of SSAS-CPA is actually part of the ?rst scan, and thus it does not add extra overhead like the Sampling algorithm [9]. Second, in some cases, a random sample cannot be drawn without scanning the database D from the beginning. An example for this kind of database is a database with variant length transactions, and there is no delimiter between trans- actions. In these cases, a random sampling approach could require an extra scan over the database, and thus a sequential sampling approach like SSAS-CPA is a better way to go. In case a random sample can be drawn without too much overhead, we can sim- ply use algorithm AS-CPA but ?rst add all local large    jDj number of transactions  jT j average number of items per transaction  jIj average number of items of maximal potentially  large itemsets  jLj number of maximal potentially large itemsets  N number of items  Table 3: Parameters  Name jT j jIj jDj N  T5.I2.100K 5 2 100K 1000  T10.I4.100K 10 4 100K 1000  Table 4: Parameter Settings  itemsets (with age = 1) found in the sample to CG.

We denote the random sampling version of AS-CPA as RSAS-CPA. The algorithm for RSAS-CPA can be found in [5].

5 Performance Results We have implemented the various AS-CPA algo-  rithms and examined their performance using a stan- dard set of synthetic test data [2]. Since the sec- ond global anti-skew technique detects false candidate itemsets with little overhead, it is treated as a required part of AS-CPA. We treat the ?rst global anti-skew and the early local pruning techniques as the optional parts of AS-CPA. We also apply the early local prun- ing technique to SPINC, and the resulting algorithm is denoted as SPINC-E. Below we briey examine some of the results.

The results of two data sets are discussed here. Ta- ble 3 shows the notation for each parameter, and Ta- ble 4 shows the name and parameter setting for each data set. All algorithms in this experiment divides the database into 5 partitions.

Tables 5 and 6 show number of scans for the vari- ous algorithms with each data set. The columns are labeled with the min support value. We were not able to actually implement the Sampling algorithm as the sampling code is proprietary, so we have shown a value of 1+. This is because the best behavior for the Sam- pling algorithm is one scan. The worst is 2. The actual number of scans required depends on the sampling as well as the data. In [9] experiments were repeated 100 times for each test database and the number of scans was reported as the average of these. The values plot- ted in this work were slightly above 1. The overall percent of retrievals with misses was 0.0038. When a miss occurs the database must be read again com- pletely. This requires another scan. In addition, the Sampling algorithm requires extra I/O to do the sam- pling. Experiments done in [9] use a sample size from 20% to 80% of the database. Thus coming up with an exact number here is di?cult. As can be seen, though, the sequential sampling AS-CPA algorithms has about the same performance as the Sampling al- gorithm based on number of scans. Certainly further work with more data and real data (rather than syn-  0.0025 0.0050 0.0075 0.0100  Partition 2 2 2 1  SPINC 1:8 1:8 1:8 1  AS-CPA-2 1:8 1:8 1:2 1  AS-CPA-12 1:8 1:8 1:6 1  SPINC-E 1:8 1:8 1:8 1  AS-CPA-2E 1:8 1:8 1:2 1  AS-CPA-12E 1:8 1:8 1:6 1  SSAS-CPA-2 1 1 1 1  SSAS-CPA-2E 1 1 1 1  Sampling 1+ 1+ 1+ 1+  Table 5: Number of Scans for T5.I2.D100K  0.0025 0.0050 0.0075 0.0100  Partition 2 2 2 2  SPINC 1:8 1:8 1:8 1:8  AS-CPA-2 1:8 1:8 1:4 1:2  AS-CPA-12 1:8 1:8 1:8 1:2  SPINC-E 1:8 1:8 1:8 1:8  AS-CPA-2E 1:8 1:8 1:4 1:2  AS-CPA-12E 1:8 1:8 1:8 1:2  SSAS-CPA-2 1 1 1 1  SSAS-CPA-2E 1 1 1 1  Sampling 1+ 1+ 1+ 1+  Table 6: Number of Scans for T10.I4.D100K  thetic data) will help to determine the best overall approach. The SSAS-CPA algorithms, however, do appear to be quite promising.

Figure 1 shows the number of global candidate itemsets generated by the various algorithms with the two data sets. The early local pruning versions of AS- CPA are not shown in Figure 1 since their number of global candidate itemsets is about the same as their counterpart without early local pruning. The early local pruning technique aims at reducing the number of local candidate itemsets, not the global candidate itemsets.

During the ?rst phase (scanning the 5 partitions the ?rst time), AS-CPA-12 generates the smallest num- ber of global candidate itemset. During the second phase, AS-CPA-2 has the smallest. Certainly since SPINC generates a smaller global candidate itemset size than the regular Partition algorithm our anti- skew approaches are also better than this. Com- pared with non-samplingalgorithms, SSAS-CPA-2 has a much large number of global candidate itemsets. Al- though not shown, we know that the Sampling algo- rithm would have a much larger number of global can- didate itemsets due to the fact that it adds those in the negative border. Notice that for these test data SSAS-CPA only had one scan in most case, but less candidate itemsets were generated.

Using the number of global candidate itemsets as a performance measure allows us to estimate CPU and memory utilization of the various algorithms. Cer- tainly more work is needed to investigate these fur- ther.

0 1 2 3 4 5  N um  be r  of C  an di  da te  I te  m se  ts  Partition Number  T5.I2.D100K, min_support = 0.0025  SPINC  1 2 3 4  AS-CPA-12 AS-CPA-2  SSAS-CPA-2             0 1 2 3 4 5  N um  be r  of C  an di  da te  I te  m se  ts  Partition Number  T5.I2.D100K, min_support = 0.0075  AS-CPA-2 SPINC  SSAS-CPA-2  1 2 3 4  AS-CPA-12        0 1 2 3 4 5  N um  be r  of C  an di  da te  I te  m se  ts  Partition Number  T10.I4.D100K, min_support = 0.0025  AS-CPA-12 AS-CPA-2  SPINC SSAS-CPA-2  1 2 3 4           0 1 2 3 4 5  N um  be r  of C  an di  da te  I te  m se  ts  Partition Number  T10.I4.D100K, min_support = 0.0050  AS-CPA-12 AS-CPA-2  SPINC SSAS-CPA-2  1 2 3 4  Figure 1: Number of Global Candidate Itemsets.

6 Conclusions and Future Work In this paper, we propose several techniques to  speed up the process of ?nding association rules among basket data. Our techniques employ prior or accumulated knowledge of the data processed, to prune false candidates at an early stage. Future work includes further performance studies of AS-CPA, early identi?cation of possibly large itemsets before scan- ning, parallel version of AS-CPA on a share-nothing architecture, and rule maintenance with AS-CPA after adding data to or removing data from the database.

