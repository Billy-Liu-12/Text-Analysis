<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2529 July,  2004 Budapest, Hungary

Abstract: We examine the performance of compact fuzzy rule- based classification systems that consist of a small number of simple fuzzy rules with high comprehensibility. Those fuzzy systems are designed in a heuristic manner using rule selection criteria. In this paper, we first describe fuzzy rule-based classification. Next we describe heuristic rule selection criteria using the terminology in data mining: cnnfinence and sripport.

A small number of fuzzy rules are extracted from numerical data based on each rule selection criterim. Then we examine the classification performance of extracted fuzzy rules through computational experiments on a number of benchmark data sets from the UCI MC Repository. Our results should be viewed as the lowest benchmark performance of fuzzy rule- based classification systems because fuzzy rules are extracted using a simple heuristic method with no optimization or tuning procedures. Nevertheless our results on some data sets are comparable to reported results by the C4.5 algorithm in the literature.



I. INTRODUCTION In the design of fuzzy rule-based systems, accuracy maximization has been mainly discussed in the literature. In the area of filzzy classification [l], various approaches have been proposed for iinproving the acciuacy of fuzzy rule- based classification systems by tuning fuzzy rules. Those approaches are often based on fuzzy-neuro learning [2]-[3] and evolutionary optimization [4]. Some of recent studies on the design of fuzzy rule-based systems emphasize the importance of comprehensibility of fuuy  rules as well as their accuracy [5]-[8]. That is, the comprehensibility- accuracy tradeoff has bcen discussed (see [9]-[ 1 11 for various issues on the comprehensibility-accuracy tradeoff).

In this paper, we examine thc classification perfonnancr of compact fuzzy rule-based systems that consist of a small Takashi Yamamoto Department of Industrial Engineering Osaka Prefecture University Sakai, Osaka 599-853 1, Japan E-mail: yama@ie.osakafu-u.zic.jp number of simple fuzzy rules with high comprehensibility.

Those fiizzy systems are designed in a heuristic manner using rule selection criteria. Performance examination is executed on a number of benchmark data sets from the UCI ML Repository (http://www.ics.uci.edu/-mleam?). In our computational experiments, each fwzy rule is evaluated by heuristic rule selection criteria. ?4 pre-specified number of the best fuzzy rules with respect to each criterion are extracted from numerical data. The classification performance of extracted fuzzy rules is evaluated using the ten-fold cross-validation ( 1 0-CV) technique. A number of heuristic rule selection criteria are examined in this paper.

This paper is organized as follows. In Section 11, we describe fuzzy rule-based classification. In Section 111, we explain heuristic rule selection criteria using the terminology in data mining: coq/Xence and suppurt. In Section IV, we  examine the performance of f u y  rules extracted from numerical data using each rule selection criterion. Our results are compared with reported results [12]-[13] by the C4.5 algorithm [ 141. Finally Section V concludes this paper.

11. FUZZY RULE BASED CLASSIFICATION We use fuzzy rules of the following type for an n-    We use fuzzy rules of the following type for an n- dimensional pattem classification problem [ 151.

Rule R, : If xI is Aql and ... and x,, is Ay,, then Class C, with CF,, (1) where x = (,XI ,..., x n )  is an n-dimensional pattem vector, Ay, is an antecedent fuzzy set (i.e., linguistic value such as small and large) for the i-th attribute. Cy is a consequent class. and CFq is a rule weight (i.e., certainty grade). The fuzzy rule R,  in ( I )  can be viewed as a fuzzy association rule ?LAq  =$ Class C, ? where A, = (A , ] ,  ..., A y l r )  .

First we explain how the consequent class Cc, of the 0-7803-8353-2104/$20.00  2004 IEEE 161 FUZZ-IEEE 2004 fuzzy rule R, in (1) is specified from numerical data. Let us assume that we have n2 labeled patterns x p  = (xpl  ,..., xpU,n), p = 1,2, ..., m from Mclasses (i.e., we have an n-dimensional M-class problem). We define the compatibility grade of each training pattern x p  with the antecedent part A, of the fiiu;y rule R, using the product operator as pAQ("~)=pFcny , (Xp l ) '~UAq:! ( ."p2) '  ' p A q n ( X p n )  (2) where p,4 .( . ) is the membership function of the antecedent fuzzy set AYj . The fuzzy conditional probability Pr(C1ass h 1 A4)  of each class for the antecedent part A, is nwnerically approximated as follows [ 161: 4' Pr(Class h I Ay)  = ,z P A 4 ( X P )  c P*&amp;). (3) xpcClass h I:, I p d  In The right-hand side of (3) is often referred to as the confidence of the fuzzy association rule " A, 3 Class h '' in the field of fuzzy data mining [ 171-[ 181: c(Aq 3 class h)  = PA,, ( x p )  1 kiA4 C x p )  . (4) x p  ?Class h The consequent class Cy of the fuzzy rule R ,  is specified by identifying the class with the maximum fuzzy conditional probability (i.e., maximum confidence) as c(A, 3 Class Cq)  = m a  @(A, 3 Class h ) }  . ( 5 ) hl.1.2, ...,:U Before explaining the specification of the rule weight CF, , we describe hzzy  reasoning for classifying new pattems by fuzzy rules of the fonn in (1). Let S be a set of fuzzy rules. A new pattern x/, is classified by a single winner rule R ,  , which is chosen from the rule set S as The winner rule R,v has the maximum product of the compatibility grade and the rule weight in S.

Using the concept of fuzzy conditional probabilities, Berg et al. [16] theoretically showed that the following specification of the rule weight CF, is appropriate for two- class pattern classification problems when we use the single winner-based method: - CF, = c(Ay 3 Class Cy) - c(Ay Class Cy&gt;. (7) where C,  is h e  complementary class of C, in two-class problems (e.g., is Class 2 when C,  is Class 1). It was also empirically demonstrated in [ 191 that better results were obtained from the definition in (7) than the direct use of the confidence (i.e., CF, = c(Aq z Class Cy) ).

There are several alternative definitions of rule weights - as extensions of (7) in the case of multi-class problems. In  this paper, we use the definition in (7) with the following specification of the second term for multi-class problems: h f Cq We use the definition of rule weights in (7)-(8) because good results were obtained from this definition in our preliminary computational experiments [ 191.

111. HEURISTIC RULE EXTRACTION Let us assume that we have K antecedent fuzzy sets for each attribute. In this case. the number of combinations of the antecedent fuzzy sets is K" for n-dimensional problems.

For two-dimensional problems (i.e., n = 2 ), we usually use    For two-dimensional problems (i.e., n = 2 ), we usually use all the K 2  combinations for generating fuzzy rules.

Generated f w  rules have high comprehensibility. The comprehensibility of furzy rules, however, is degraded when they are applied to high-dimensional problems due to the following two difficulties. One is the exponential increase in the number of fuzzy rules. It is a troublesome task for human users to manually examine thousands of fuzzy rules. The other is the increase in the number of antecedent conditions.

It is not easy for human users to intuitively understand fuzzy rules with tens of antecedent conditions.

Our idea for avoiding these two difficulties is to choose only a small number of short fuzzy rules. The number of antecedent conditions in each fuuy  nile is referred to as the rule length. For generating short fuzzy rules, we use don 't care as an additional antecedent fuzzy set. Thus the number of combinations of antecedent fuzzy sets is ( K  i I )" .  Our task is to find a small number of short fuzzy rules with high classification accuracy from such a huge number of possible combinations of antecedent fuzzy sets. In this section, we show several heuristic rule selection criteria.

We extract a pre-specified number of fuzzy rules for each class in a simple greedy manner. That is, the best L fuzzy rules with respect to a heuristic rule selection criterion are extracted for each class. In our former work [8] .  we examined three rule evaluation criteria: the confidence, the support. and their product. In the same manner as the confidence of the fuzzy association rule "A', z Class h " in (4), its support is defined as follows: where M is the number of given training pattems. In this paper, we examine two composite criteria of the confidence and the support in addition to their product. One is the 25-29 July, 2004 Budapest, Hungary support criterion with the minimum confidence level where the rule selection is performed using the support criterion from fuzzy rules whose confidence values satisfy a pre- specified minimum level. The other is the confidence criterion with the minimum support level.

We also examine two heuristic rule selection criteria used in iterative f u ~ z y  GBML algorithms 1201-[23].

Gomalez &amp; Perez [20] proposed an iterative algorithm called SLAVE where a single fuzy  rule was extracted in its single iteration. The SLAVE algorithm was modified in [22] by including a tiebreak mechanism that favors more general rules. While a somewhat complicated general formulation was shown in 1201, [22] ,  the rule selection criterion used in their computational experiments can be written as n+(R)-tz-(R) where n+(R) and n-(R) are the number of positive and negative examples, respectively. We use the following fuzzified version of this criterion: = F A y ( x y ) -  f l A q ( x p )  .

x, &amp;lass Cq x p  crClnss c; (10) Since the rule selection is not an?ected by the linear  transformation of the rule selection criterion (i.e., since the same fuzzy nile is chosen after the linear transformation), we rewrite (10) by dividing the right-hand side by in as - . fsLAVE(R4) = s(A, Class Cy.) - s(A, 3 Class C,) , (1 1 ) where :%I h = l h#Cq s(A,, 3 Class Cy) = .$(A, 3 class h)  . (12) On the other hand, the following criterion was used in h e  iterative fuzzy GBML algorithm in Castro et al. [23] : where I Class C, I and /Class cl are the number of training patterns in Class Cy and the other classes.

respectively. We use the following fuzzified version of (13): IClass Cy I    IClass Cy I 4) This criterion can be rewritten by removing the constant terms in the numerator and the denominator as , ~ c ~ ~ ~ ~ ~ ( R ~ )  = s(A, 3 Class Cy) It should be noted that we can remove [Class C, I and 1 Class a from the denominator of (14) because the consequent class Cy is pre-speciiied in our heuristic rule extraction (i.e., because the best L fuzzy rules are extracted lor each class).

For some rule selection criteria, the selection of the best L fuzzy rules for each class can be efiiciently performed using the well-known Apriori algorithm [24] in the field of data mining. For example, when A, cB, holds for antecedent fuzzy sets, the follou ing relation holds for fuzzy association rules in the case of the support criterion: s(A, 3 Class Cy) 2 s(B, 3 Class Cy), (16) where the inclusion relation for the fuzzy vectors A, and B, is defined by their elements as A, c B, e Aqi c B,, for Vi.

The inequality in (16) means that the support of a fuzzy rule does not increase when we add an additional condition to its antecedent part. For example, the inequality condition in (16) holds for A, = (small, small, don?t cave) and B, = (small, don ?f care, don ?t care). If the support of the fuzzy rule ?B, 3 Class Cy ?- is not high, we do not have to examine m y  fizzy rule ?-A, 3 Class C, ? satisfying the inclusion relation A, cB, . Thus we can efficiently perform the fuzzy rule extraction using the support criterion.

This search technique can be also utilized for the product criterion, the SLAVE criterion, and the Castro criterion using the following relations: c(R,).  .s(Rq) 5 s(A, 9 Class Cy), fsLAL~u(R,) 5 s(A, 3 Class Cy) , (18) (19) jkastm(R,) I s(Ay Clas  C,) x . (20) /class C, 1 m On the other hand, the confidence has no monotonicity similar to (16). That is, the confidence tends to increase as a new condition is added to the antecedent part (i.e., as the rule length increases). Thus the confidence criterion usually needs the restriction on the rule length.

1V. COMPUTATIONAL EXPERIMENTS We examined the performance of each rule selection criterion through computational experiments on benchmark data sets with many continuous attributes in the UCI ML FUZZ-IEEE 2004  Repository. Table 1 summarizes data sets used in this paper.

Some data sets in Table 1 have missing values. We did not use incomplete patterns with missing values. Data sets with missing values are marked by "*" in the third column of Table 1 where inconiplete patterns with missing values are not included in the number of patterns. All attribute values were linearly transformed in real numbers in the unit interval [0, 11 before the heuristic rule extraction was performed.

TABLE 1 DATA FETS USED IN OUR COtv4PUTATIONAL EXPFRIMF~NTS.

Data set Attributes Pattems Classes Brcast W 9 683* 2 Diabetes 8 768 &amp;. 7 Glass 9 214 6 Heart C 13 297* 5 Hepatitis 19 80* 2 Iris 4 150 3 Liver 6 345 - 7 Page blocks 10 5.473 5 Segnentation 18 2,310 7 Sonar 60 208 2    Sonar 60 208 2 Vehiclc 18 846 4 Vowel 10 990 1 1 Wine 13 178 3 Yeast 8 1,484 10 We can use arbitrary given fuzzy partitions for each attribute in our fuuy  rule extraction. In our computational experiments, we simultaneously used four fuzzy partitions in Fig. 1 for all attributes because we did not know an appropriate granularity of the fuzzy partition for each attribute. In addition to the 14 fuzzy sets in Fig. 1, we also used don 't care as an additional antecedent f u z y  set. Thus the total number of possible combinations of the antecedent fuzzy sets is 15" for an n-dimensional problem.

0.0 ""Dooa. 0.0 1 .o ~~~ 0.0 1 .o Fig. I .  Four fuzzy partitions used in our computational cxperimcnts.

Our task is to find a small number of comprehensible fuzzy rules with high classification performance. In our computational experiments on the 60-dimensional sonar data, we only examined fuzzy rules with two or less antecedent conditions. For the other data sets, we examined fuzzy rules with three or less antecedent conditions. The restriction on the number of antecedent conditions is for finding short fhzzy rules as well as for decreasing CPU time.

We extracted L fuzzy rules ( L  = 1,2, ...I for each class using one of the above-mentioned seven rule selection criteria (the confidence, the support. their product, the confidence with the minimum support level, the support with the minimum confidence level, the SLAVE criterion, and the Castro criterion) where several values of the minimum support and confidence levels were examined. The heuristic rule extraction was performed for each class in a simple greedy manner. That is, the best L fuzzy rules were found for each class. There were many cases where multiple fuzzy rules had the same best value of a rule selection criterion. In those cases, the tiebreak was performed using the following two-step procedure. The first tiebreak criterion was the number of antecedent conditions. The fuzzy rule with the least antecedent conditions was chosen. This tiebreak criterion is to favor shorter fuzzy rules. When a single fuzzy rule could not be chosen by the first tiebreak criterion, we used the total area of the antecedent fuzzy sets of each fuzzy rule as the second tiebreak criterion. The total area was calculated by sitnply summing up the area of the triangular membership function of each antecedent f k z y  set in each fuzzy rule. The fuzzy rule with the largest total area was chosen. The second tiebreak criterion is to favor more general fuzzy rules that cover larger subspaces of the pattern space. When multiple fuzzy rules still had the same best evaluation with respect to all the three criteria (i.e., the primary rule selection criterion and the two tiebreak criteria).

a single rule was randomly selected among those best rules.

For evaluating the classification performance on test data, we used the ten-fold cross-validation (1 0-CV) technique.

Since the classification rate estimated by the IO-CV technique usually depends on the data partition into ten subsets, we executed the whole IO-CV procedure ten times using different data partitions (i.e., IO x 10-CV). Average classification rates on test data are summarized in Table 2 where ten fuzzy rules were extracted for each class.

In Table 2, we also cited some reported results of the C4.5 algorithm 1141 in the literature. Quinlan [12] proposed a modified version (R.8 in Table 2) of his C4.5 algorithm (R.7 in Table 2) f'or appropriately handling continuous attributes. He evaluated the performance of each version by ten iterations of the whole 10-CV procedure. Elomaa &amp; Rousu [13] proposed an optimal discretization method of continuous attributes into multiple intervals. They examined the performance of six variants of the C4.5 algorithm, which 25-29 July. 2004 Budapest, Hungary 9.2 51.5 57.1 43.5 42.6 42.4 72.6 73.9 70.4 74.8 73.6 71.6 74.4 64.2    6.8 50.2 55.7 22.5 22.5 22.5 58.u 54.6 50.4 51.9 52.1 70.9 72.9 70.4 18.4 47.4 49.2 20.8 20.8 20.8 52.4 46.0 42.2 48.8 47.7 - - 58.8 55.3 85.8 92.2 92.4 89.0 89.2 90.2 92.9 93.4 92.5 93.4 - - 91.2 were implemented using three discretization methods (i.e., binary discretization, greedy multisplitting, and optitnal mnultisplitting) and two evaluation functions (i.e., gain ratio and balanced gain). The performance of each variant was examined by ten iterations of the whole IO-CV procedure.

We show in the last two columns in Table 2 the worst and best results among the six variants reported in [ 131.

TABLE 11 ICATION RATES OF I'UZLY RUL6S A h U  SOME REPOKTED RESULTS USIWO THE C4.5 ALGORITHM. TEN FUZZY RULES WERE EXTRACTED FOR WCH CLASS USING A HEURISTIC RULE EVALC'ATION CRTTERIOA;.

75.4 73.0 63.1 94.4 c with s-level s with c-level Q 1121 ER [ 131 0.1 0.2 0.3 0.6 0.7 0.8 CA R.7 R.8 min max Data sct c s Breast W Diabetes Glass Hcart C Hepatitis Iris Livcr Pagc Scginent Sonar vehlclc Voacl Wine Yeast 2.2 53.7 55.0 39.6 39.6 39.6 47.1 37.9 28.1 52.7 53.7 - - 56.6 I 57.6 1 Average 21.4 66.7 70.4 54.3 53.7 55.4 71.5 70.7 66.4 70.8 68.8 - - 71.1 75.0 TABLE 111 last three criteria: the support with the minimum confidence Castro criterion (CA) for many data sets. On the other hand, the confidence (c) and the confidence with the minimum support level (e with s-level) were inferior to the other criteria AVER4GE CLASSIFICATION RATES OF FLZZY RULES WHEN ONLY A 91hCiLE level (' c-level). the SLAVE (sL) and the FLZZY RULL WAS LXTRACTLD r OR EACH CLASS s with c-level 0.6 0.7 0.8 SL CA Data set cs Rrcast W 9 3 4  902  9 0 9  9 2 7  935  94.3 for many data sets in Table 2.

Diabctcs 65 I 66 I 73.4 73.1 73 4 7J.o G l a s  52.1 50.4 57.8 29.7 53 9 5 1  1 TABLE IV Heart C Hcpatitis Iris Liver Pagc Scgmen t Sonar Vehicle Vowcl Wine 42.1 52.5 45.9 42.2 38.4 41.3 77.1 83.8 83.8 82.8 81.3 64.1 - 95.6 90.1 94.5 94 7 95.6 94.5 - 58.0 52.0 44.8 19.0 58.0 54.2 89.8 89.8 89.8 !3&amp; 90.1 71.9 - 82.4 82.2 80.8 80.5 80.8 67.5 53.2 733 72.5 56.1 72.5 72.4 45.3 47.4 43.3 31.6 41.7 44.2 43.5 41.4 35.3 33.3 40.1 43.6 88.5 84.6 87.1 89.0 88.7 83.7 Y t X t  53.7 44.5 35.0 22.5 50.7 50.2 Average 67.1 67.7 66.8 59.8 68.5 64.8 In Table 2, we highlight the cell corresponding to the best result for each data set. Moreover the best result among the seven rule selection criteria is shown by boldface with    seven rule selection criteria is shown by boldface with underline for each data set. From Table 2, we can see that our simple heuristic rule extraction procedure is comparable to the (24.5 algorithm for soma data sets (e.g., Breast W, Diabetes, etc.). Among the seven rule selection criteria, good results were obtained from the product criterion (cs) and the AVERACF Cl ASSlFICATlON RATES OF K7ZY RULES WEEN ON1.Y TWO FLIZZY RULES WERE EXTRACTED FOR EACII CI.ASS.

s with c-level 0.6 0.7 0.8 SL CA Dataset cs BrcastW 93.0 89.7 91.7 91.9 93.1 94.1 Diabctcs Glass Hcart C Hepatitis Iris Livcr Pagc Segment Sonar Vehicle Vowel Wine Y e a t Avcrnce 65.1 67.4 73.3 73.5 71.3 74,o 53.6 51.2 58.9 31.3 54.4 52.1 43.5 53.2 49.3 43.7 38.8 42.0 77.0 83,8 838 82.9 81.9 64.1 95.1 95.J 95.3 95.1 95.4 95.1 - 58.0 55.6 50.1 23.6 57.9 56.3 89.8 89.8 90.0 90.1 90.1 74.9 80.6 82.9 81.1 80.4 80.5 68.0 52.3 73.2 72.2 61.0 72.6 72.5 48.7 51.2 45.6 36.4 44.3 47.5 45.4 45.5 39.3 36.7 41.7 45.8 89.2 86.3 88.6 92.8 92.5 87.5 54.7 45.2 35.5 14.2 51.3 51.6 67.5 69.3 68.2 61.7 09.0 66.1 In Table 3, only a single fuzzy d e  was extracted for each FUZZ-/E?? 2004 class. Due to the page limitation, we show a part of our experimental results. From the comparison between Table 2 and Table 3, we can see that our experimental results by a single rule for each class are still comparble to the C4.5 algorithm for some data sets (i.e., Breast W, Diabetes, Heart C, Hepatics, Iris and Sonar). In Table 4, two fiizzy rules were extracted for each class. It is interesting to observe that the classification perkormance did not always monotonically increase with the number of fizzy rules (e.g., see Breast W) in Table 3 and Table 4. This observation suggests that OUT experimental results may be improved by searching for good cornbinations of f i q  rules extracted by our heuristic procedure.



V. CONCLUDING REMARKS We examined the classification performance of a sinall number of simple fuzzy rules that were extracted from numerical data in a greedy manner based on heuristic rule selection criteria.

We did not intend to propose a new approach to the design of fiizzy nile-based classification systems. Our intention was to provide the lowest benchmark performance (i.e.. lower bound of the perfomtance) of f m y  rule-based classification systems. We demonstrated that very simple fuzzy rule-based systems had high classification ability for some data sets (e.g., see Table 3 where only a single rule was extracted for each class). Our experiinental results can be referred as benchmarks in future studies on the tradeoff between the accuracy and the comprehensibility of fiwy rule-based classification systems.

Our idea of extracting hzzy rules using heuristic rule selection criteria can be also utilized as a pre-screening procedure for genetic rule selection. First, a pre-specified number of promising candidate rules are generated by heuristic    number of promising candidate rules are generated by heuristic rule selection. Then optimal combinations of candidate rules are searched for by evolutionary optimization techniques. While we used the product criterion in our former study [SI, we can use other criteria for the pre-screening of candidate fuzzy rules.

