Discovery of Interesting Association Rules Based on Web Usage Mining

AbstractMining of association rules is an important  research topic in web usage mining?The purpose of this  paper is to research how to dig interesting association rules  effectively from the Web logs after been preprocessed. Firstly,  using the FP-growth algorithm for processing the web log  records, obtaining a set of frequent access patterns, then using  the combination of browse interestingness and site topology  interestingness of association rules for web mining, discovering  a new pattern to provide valuable data for the site  construction .

Keywords-Web usage mining; FP-growth algorithm; Browse  interestingness; Interesting association rule

I. INTRODUCTION  With the growth of explosive Internet information, the  data mined from the web useful information, which has  gradually become a hot research .Generally, according to  the different subjects, Web mining  can be divided into  three categoriesWeb content mining, Web structure  mining and Web usage mining. Web log mining is a way of  Web usage mining. Web access patterns mined from Web  logs are interesting and useful knowledge in practice.

Examples of applications of such knowledge include  improving designs of web sites, analyzing system  performance to understand user's reaction and motivation,  build adaptive web sites[1-2].

The current web log mining mainly has focused on the  user browsing pattern mining, however, in this paper, the  FP-tree algorithm is applied to the web log records, adding  the user's interest degree and redefining the best access time  of the user's parameters and topology. Firstly, using the  FP-growth algorithm [3]for dealing with web logging to get  a frequent access pattern set, and then using a combination  of user access and site topology Interestingness of  association rules for web mining, finally getting the  association rules of web logs. So this can take into account    the frequency of user's browsing, the user's browsing  interest degree and site topology to provide valuable data  for optimizing the construction site.



II. WEB USAGE MINING  Web usage mining refers to the automatic discovery  and associated data collected or generated as a result of user  interactions with Web resources on one or more Web  sites .The goal is to capture, model, and analyze the  behavioral patterns and the profiles of users interacting of a  Web site. The discovered patterns are usually represented as  collections of pages, objects, or resources that are  frequently accessed by groups of users with common needs  or interests[4].

In web mining, data can be collected at the server side,  client-side, proxy servers , or obtained from an  organization's database. In web mining, there are many  kinds of data that can be used , classifying such data as  following types:  gContent: The real data in the web pages,ie, the data  the web page was designed to convey to the users. This  usually consists of , but is not limited to, text and graphics.

gStructure: Data which describes the organization of  the content. This can be representative as a tree structure,  where the tag becomes the root of the tree.

gUsage: Data that describes the pattern of usage of  Web pages such as IP address, page references, and the data  and time of access.

g User Profile: Data that provides demographic  information about users of the Web site. This includes  registration data and customer profile information.

Web server logs record the each page request  information of the user access to the site , there are two  kinds of log record format: Common log format (CLF) and  the Expansion of Common log format (ECLF).

Through the web server log mining, obtaining the  user's access patterns, further analyzing the log law to  improve the site's organizational structure.



III. DISCOVERY of Interesting Association Rules  A. Data Preprocessing  The purpose of Data Preprocessing is to change a web  data mining into reliable data, including four phases: data  cleaning, user identification, session identification as well    as fragments identification [2]  1) Data Cleaning: Data cleaning is usually site-specific,  and involves tasks such as, removing extraneous references  to embedded objects that may not be important for the  purpose of analysis.

2) User Identification: The analysis of Web usage does  not require knowledge about a users identity. However, it  is necessary to distinguish among different users. Using a  similar modification of the paper as web log records, and as  is shown in table 1 after data preprocessing [3]. Identifying   978-0-7695-4136-5/10 $26.00  2010 IEEE  DOI 10.1109/MEDIACOM.2010.82   the user's browsing path  A1-B1-C1-C2-D3-C3-D4, A1-B2 -C4-B3-D4-B2, A1-B3  3) Session identification: Session identification is the  process of segmenting the user activity record of each user  into sessions, each representing a single visit to the site.

After session identification, the ideal heuristic can  re-construct the exact sequence of user navigation during a  session. According to Figure 1, setting the topological span  to four and adding the user to access the situation and the  session identfied, the user's access path:  A1-B1-C1-B1-C2-D3, B1-C3-D4, A1-B2- C4-B2-A1-B3,  A1-B1-C3-D4, A1-B2, A1-B3.

4) Fragments identification: The purpose of fragments  identification is to find the user session meaningful access  path.

Fig.1 The image of site's topology  TABLE I     THE WEB LOG DATA RECORD AFTER DATA CLEATING    B. The analysis of user behavior of based on FP-tree  After data preprocessing, using association rules for  dealing the data mining.The following is a formal statement  of the problem:Let L={i1,i2,...,im} be a set of literals,  called items.Let D be a set of transactions,where each  transaction T is a set of items such that T? L. Associated  with each tranction is a unique identifier, called TID. We  say that a transaction T contains X, a set of some items in L,  if X ?T.  An asstion rule is an implication of the form X    ==> Y ,where X? L,Y ?L,and X ==>Y= ? .The rule X  ==>Y holds in the transaction set D with confidence c if c%  of transaction in D that contain X also contain Y. The rule X  ==>Y has support s in e the transaction sed D if s% of  transaction in D contain X?Y[3] .

Define 1:  Best Access Time(BAT): Generally, the best access time  means to meet the vast majority of users within a certain  period of time required to read all the information page of  time, which usually relate to the amount of information  about the page. However, in this paper, the best time means  that the time required for page to view the Web site by the  topology of the shortest starting to reach the page and all  the path segments completely.

Define 2:  Browse Interestingness(BI ): Defined Pj as users set the  page j of the level of interest and Countij as user from the  previous page i to page j's visits, Timeij as user from page i  to page j required to browse time (unit: seconds) Timej that  user j page is best viewed time, the definition of the user's  browsing interest:  Pj=[support(AUB)  7LPHAB]/ [support(A)*TimeB]  A frequent-pattern tree can be designed as follows.

Definition 3:  FP-tree: A frequent-pattern tree (or FP-tree in short) is a  tree structure defined below.

Num IP Address Date Method/URL/Protocol Referred Agent  1 123.456.78.9 20/March/2010 09:05:51 Get/A1 html Http/1.00 - MSIE/6.0(Windows NT,5.1)  2 123.456.78.9 20/March/2010 09:06:15 Get/B1 html Http/1.00 A1 html MSIE/6.0(Windows  NT,5.1)  3 123.456.78.9 20/March/2010 09:09:36 Get/C1 html Http/1.00 B1 html MSIE/6.0(Windows  NT,5.1)  4 123.456.78.9 20/March/2010 09:09:56 Get/C2 html Http/1.00 B1 html MSIE/6.0(Windows  NT,5.1)  5 209.456.78.2 20/March/2010 09:10:36 Get/A1 html Http/1.00 - MSIE/6.0(Windows NT,5.1)  6 123.456.78.9 20/March/2010 09:12:19 Get/D3 html Http/1.00 C2 html MSIE/6.0(Windows  NT,5.1)  7 209.456.78.2 20/March/2010 09:13:09 Get/B2 html Http/1.00 - MSIE/6.0(Windows NT,5.1)  8 209.456.78.2 20/March/2010 09:14:17 Get/C4 html Http/1.00 B2 html MSIE/6.0(Windows  NT,5.1)  9 209.456.78.2 20/March/2010 09:16:18 Get/B3 html Http/1.00 - MSIE/6.0(Windows NT,5.1)    10 123.456.78.9 20/March/2010 09:13:51 Get/C3 html Http/1.00 B1 html MSIE/6.0(Windows  NT,5.1)  11 209.456.78.2 20/March/2010 09:19:30 Get/D4 html Http/1.00 C3 html MSIE/6.0(Windows  NT,5.1)  12 209.456.78.3 20/March/2010 09:20:29 Get/A1 html Http/1.00 - MSIE/6.0(Windows NT,5.1)  13 209.456.78.2 20/March/2010 09:25:11 Get/B2 html Http/1.00 - MSIE/6.0(Windows NT,5.1)  14 209.456.79.3 20/March/2010 09:27:29 Get/B3 html Http/1.00 A1 html MSIE/6.0(Windows  NT,5.1)  15 123.456.78.9 20/March/2010 09:33:55 Get/D4 html Http/1.00 C3 html MSIE/6.0(Windows  NT,5.1)   It consists of one root labeled as null, a set of  item-prefix subtrees as the children of the root, and a  frequent-item-header table.

Each node in the item-prefix subtree consists of three  fields: item-name, count, and node-link, where item-name  registers which item this node represents, count registers the  number of transactions represented by the portion of the  path reaching this node, and node-link links to the next  node in the FP-tree carrying the same item-name, or null if  there is none.

Each entry in the frequent-item-header table consists of  two fields, item-name and head of node-link (a pointer  pointing to the first node in the FP-tree carrying the  item-name).

Based on this definition, we have the following FP-tree  construction algorithm.

Algorithm 1 :(FP-tree construction).

Input: A transaction database DB and a minimum  support threshold  Output: FP-tree, the frequent-pattern tree of DB.

Method: The FP-tree is constructed as follows.

1). Scan the transaction database DB once. Collect F,  the set of frequent items, and the support of each frequent  item. Sort F in support-descending order as F List, the list  of frequent items.

TABLE? THE TRANSACTION DATABASE  TID UID Page Access Sequence Ordered Access Sequence  1 1 {A1,B1,C1} {A1,B1}  2 1 {B1,C2,D3} {B1}  3 1 {B1,C3,D4} {B1,C3,D4}    4 2 {A1,B2,C4} {A1,B2}  5 2 {A1,B3} {A1,B3}  6 2 {A1,B1,C3,D4} {A1,B1,C3,D4}  7 2 {A1,B2} {A1,B2}  8 3 {A1,B3} {A1,B3}  If the minimum support threshold is 2, so L = ((A1: 6),  (B1: 4), (B2: 2), (B3: 2), (C3: 2), (D4: 2)), the second  component of each element represents the first component  represents the degree of support items. Therefore, the only  frequently visited pages of data mining is frequent access to  the page below shows the best access time and  the best  access time of frequently accessed pages topology.

TABLE.?  THE BEST ACCESS TIME(BAT)  OF  THE  FREQUENTLY  ACCESSED  PAGES    Fig.2. The time of website's topology  2). Create the root of an FP-treeT and label it as  null. For each transaction Trans in DB do the following.

Select the frequent items in Trans and sort them  according to the order of FList. Let the sorted frequent-item  list in Trans be [p | P], where p is the first element and P is  the remaining list. Call insert tree([p | P], T ).The function  insert tree([p | P], T ) is performed as follows. If T has a  child N such that N.item-name = p.item-name, then  increment  Ns count by 1; else create a new node N, with  its count initialized to 1, its parent link linked to T , and its  node-link linked to the nodes with the same item-name via  the node-link structure. If P is nonempty, call insert tree(P,  N) recursively. Therefore, constructing a Fp-tree according  to Table 2 as follow      Fig.3. FP-Tree  Based on the above lemmas and properties, we have  the following algorithm for mining frequent patterns using  FP-tree.

Algorithm 2 ?(FP-growth: Mining frequent patterns  with FP-tree by pattern fragment growth).

Input: A database DB, represented by FP-tree  constructed according to Algorithm 1, and a minimum  support threshold ? .

Output? The complete set of frequent patterns.

Method:  FP-growth(FP-tree, null).

Procedure FP-growth(Tree, ?)  {  (1) if Tree contains a single prefix path // Mining  single prefix-path FP-tree  (2) then {  (3) let P be the single prefix-path part of Tree;   (4) let Q be the multipath part with the top branching  node replaced by a null root;  (5) for each combination (denoted as ?) of the nodes in  the path P do  (6) generate pattern   ?    with support =  minimum support of nodes in ;  (7) let freq pattern set(P) be the set of patterns so  generated; }  (8) else let Q be Tree;  (9) for each item ai in Q do {                    //  Mining multipath FP-tree  (10) generate pattern  = ai ?  with support =  ai .support;  (11) construct ?s conditional pattern-base and then ?s  conditional FP-tree Tree? ;  (12) if Tree? ??

(13) then call FP-growth(Tree?, ?);  (14) let freq pattern set(Q) be the set of patterns so  generated; }  (15) return(freq pattern set(P) ? freq pattern set(Q)  ? (freq pattern set(P)hfreq pattern  set(Q))) }  Using the FP-growth algorithm, getting the frequent  item sets F of the database D:  {  {A1:6},{B1:4},{B2:2},{B3:2},{C3:2},{D4:2},{A1,B1:  2},{A1,B2:2},{A1,B3:2},{B1,C3:2},{B1,D4:2},{C3,D4:2}  {B1,C3,D4:2}}  C. The process of association rules  1) According to the frequent item sets  above ,continuing the  association rule mining  If A ? B = ?  and A ? ? B is also a frequent set .Then  calculate the path degree of credibility:  P = [support (AUB) * TimeAB] / [support (A) * TimeB]  2) If P> minconf, then having the association rules A1    => B and store it.

3) Traversing all the elements of F, then output all the  association rules.

Else return to 1).

The interest in the site combined with the previous  degree of experience, setting the minconf as 0.30, the final  rules by the association is.

A1=>B1;B1=>C3;C3=>D4; B1=>D4; (B1,C3)=>D4.

Association rules can be drawn from the above the  correlation between pages, understanding the user interface  on the site who is interested in, mining the user preferred  path to provide more valuable data to the construction site.



IV.  CONCLUSIONS  In this paper, using the mining association rules those  based on FP-Grow algorithm, combining with interest  measure and website users to view the topology of the user  the best access time, all the data mining are based on the  web use log data source, analysising the users behavior to  find the user s browsing mode to provide valuable data for  the optimization of the site.

