An Adaptive Method for Mining Frequent Itemsets Efficiently: An Improved Header Tree Method

Abstract?Data mining has become an important field and has been applied extensively across many different areas. Mining frequent itemsets from a transaction database is crucial for min- ing association rules. FP-growth algorithm has been widely used for frequent pattern mining and it is one of the most important algorithm proposed to efficiently mine association rules because it can dramatically improve the performance compared to the Apriori algorithm. Many investigations have proved that FP- growth method outperforms the method of Apriori-like candidate generation. The performance of the FP-growth method depends on many factors; the data structures, recursive creation of pattern trees, searching, sorting, insertion and many more. In all of the algorithms which are using fp-tree, a header table is used with sorted items. Header table is an important data structure in the mining process. The main datastructure (frequent trees) is created with the use of the header table. In this paper we suggest a new Binary Search Header Three (BSHT) and an Improved Header Tree mining (IHT-growth) to improve the performance of the frequent pattern mining. Experimental results show that the mining with BSHT is efficient for frequent pattern mining.



I. INTRODUCTION  The process which has considered as an important aspect of data mining is discovering association rules among the large number of item sets. A lot of algorithms and techniques has been developed in this area. Among the existing techniques the frequent pattern growth (FP-growth) method proposed by Han et al.[1] is the most efficient and feasible approach. It mines the frequent item set by using a frequent pattern tree (FP-tree ) with a header table and avoids costly candidate generation.

FP-tree is a data structure that is used for storing frequent patterns (or itemsets) in association rule mining. It therefore achieves much better performance and efficiency than Apriori- like algorithms. It scans the database only twice.

Frequent itemset mining is an important problem in the data mining area. It was first introduced by Agrawal et al.[6] in the context of transaction databases. The problem of frequent- itemsets mining can be defined as follows. A transaction database is a database containing a set of transactions and each transaction is associated with a unique transaction id. Let D ={ t1 , t2, ......., tN} be a transaction database and I ={ i1,i2,....in, } be the set of items appearing in D, where ti (i? [1,N]) is a transaction and ti ? I.Each subset of I is called an itemset. If an itemset contains k items, then the itemset is called a k-itemset. The support of itemset l in database D is defined as the percentage of transactions in D containing l, that is, supportD(l) =|{t|t? D and l?t}|/|D|.ie..|l|/|D|. If supportD(l)?min sup, where min sup(min sup?[0,1]) is a  user-specified minimum support threshold, then l is called a frequent itemset in D. Given a transaction database and a min- imum support threshold, the task of frequent itemset mining is to find all frequent itemsets in the transaction database. An association rule is a conditional implication among itemsets , X ?Y , where X?I, Y? I, X ?= ?, Y?= ? and X?Y=?. The confidence of the association rule, given as sup(X?Y)/sup(X), is the conditional probability among X and Y, such that the appearance of X in ti implies the appearance of Y in ti.Mining the association rules that have support and confidence greater than the min sup and the user-defined minimum confidence is the problem of association rule mining.

Discovering frequent itemsets is the important and time consuming step in the process of mining association rules. A huge number of candidate itemsets are generated during the mining process. The step of rule construction is direct and less expensive. Therefore, most researchers concentrate on the first phase for finding frequent itemsets. In addition to the problem of the large number of candidates, this algorithm also demands an efficient data structure to store frequent itemsets for further processing .Agrawal and Srikant (1994) [6] proposed the Apriori algorithm for mining frequent itemsets. Apriori uses a candidate generation method, such that the frequent k-itemset in one iteration can be used to construct candidate (k + 1)- itemsets for the next iteration. Apriori terminates its process when no new candidate itemsets can be generated. Various methods are proposed by researches as the improvement of Apriori. However, those methods cannot avoid scanning the database many times to verify frequent itemsets.

Han et al.[1] introduced the FP-growth algorithm without generating candidate itemsets to mine frequent itemsets. FP- growth uses a Frequent Pattern tree(FP-tree)with a header table for efficiently mining association rules. In this method the FP- tree captures the content of the transaction database and stores all the transactions(only frequent items) in a compressed form.

It scans the database only twice . In addition to FP-tree, another data structure called header table, is used to find frequent itemsets quickly by traversing the tree. The header table stores frequent 1-itemsets in decreasing order of their frequencies.

Each item in the header table points to the first occurrence of the corresponding node in FP-tree. All nodes with similar items in FP-tree are connected by using a link between them.

After completing the FP-tree construction , the next step of the FP-growth algorithm is to mine frequent patterns from FP-tree.

It starts the mining from the least frequent item to the most frequent item. It traverses from the leaf nodes of the FP-tree to     the root. Paths with the same prefix item in the FP-tree are used to construct the conditional FP-tree and its header table. Using the conditional FP-tree, the algorithm can generate frequent itemsets with the same prefix. According to the experimental results, FP-growth is faster than the Apriori algorithm and several other methods of mining frequent itemsets[4]. Our challenge is to improve the efficiency of the algorithm. In this paper, we propose a new improved header table and an efficient way of transaction processing to construct FP-tree.



II. RELATED WORK  Frequent pattern mining extracts patterns with supports higher than or equal to a minimum support threshold, and many mining methods have been introduced , but Apriori[7] and FP-growth[1] are still regarded as the popular algorithms.

Apriori is the oldest conventional mining algorithm, and it generates candidate patterns in advance, and then confirms whether the candidates are actually frequent patterns by scan- ning a database. Apriori scan the database as much as the maximum length among frequent patterns. FPgrowth solved the problem of unwanted scan , it scans a database only twice.

It uses a tree structure, called FP-tree, which can prevent the algorithm from generating candidate patterns. FP-tree consists of a tree for storing the items in a transactional database and a header table containing item names, supports, and node links, link is pointing to the corresponding first item in the tree.

Fp-tree is composed of nodes, where each of them includes an item name, a support, a parent pointer, a child pointer, and a node link. The node link is a pointer that connects all nodes with the same item to each other. Since the FP- growth[1] algorithm was proposed, various algorithms have been published on the basis of the algorithm[2]. Gwangbumet al.,[2] recommended an improved tree structure to imple- ment an outstanding frequent pattern mining technique by introducing a new tree structure called Linear Prefix Tree (LP-Tree).LP tree is composed of array forms to minimize pointers between nodes. Yuh-JiuanTsay et al.,[3] suggested a novel method, the Frequent Items Ultrametrictrees(FIUT) for mining frequent itemset. The FIU-tree structure is used to enhance the efficiency in obtaining frequent itemsets. Fan- Chen Tseng[5] presents an adaptive mechanism to select and use a suitable data structure among two pattern list structures to mine frequent itemsets. The two methods are the Frequent Pattern List (FPL) for sparse databases, and the Transaction Pattern List (TPL) for dense databases .The selection criteria is depending on database densities , they give a method to calculate the database density. Ke-Chung Lin et al., [4] proposed an improved frequent pattern(IFP) growth method for mining association rules. Authors of the paper pointed out that the proposed algorithm requires less memory and shows better performance in comparison with FP-tree based algorithms. In the paper, the authors propose a new IFP-growth (Improved FP-growth)algorithm to improve the performance of FP-growth. First, the IFPgrowth employs an address-table structure to lower the complexity of searching in each node in an FP-tree. It also uses a hybrid FP-tree mining method to reduce the need for rebuilding conditional FP-trees[4]. The IFP growth use additional memory for holding an address table for each node . This results in lack of memory to store those additional data. Address table is containing item name and pointer to its child . IFP growth does not reduce the size of  trees since it still uses the original-tree-based structures with additional address tables. As a result, we need to develop a new algorithm to reduce the running time without increasing the memory usage to improve fundamental performance of the mining algorithm. Consequently, we propose a novel algorithm by introducing some new methods to process the transactions for satisfying both runtime and memory efficiency. In our algorithm ,its runtime and memory performances are more outstanding than those of the above mentioned methods due to its efficient transaction processing with the use of BST(binary search tree) data structures.



III. THE IMPROVED HEADER TABLE  Various factors are affecting the performance of the mining process of FP-tree algorithm, the insertion of transactions into the tree, searching for items, traversing in fp-tree , data structures , node structure and pruning are only few of them.

The mining process starts by scanning the transaction database to find out the frequent 1-itemsets. During this process the datastructure used to store all the items is also effect the running time of the mining process. Total number of items in a transaction database may not be known in an earlier stage.

To use the simple data structure array, we should know the number of items earlier because array is a static datastructure.

To calculate the number of items we have to scan the database one more time. FP-growth and other related methods already use two scans to create the tree. To improve the efficiency, the researchers are trying to reduce the database scan. So here we can use a dynamic data structure ,Binary Search Tree (BST) then only we can add items during the scan. The BST is frequently using to increase the support count of each item or to add a new item if it is not added earlier. In a dynamic linear data structure like linked list , we can apply only linear search. If the list grows in size, the number of comparisons required to find a target item in both worst and average cases grows linearly in the case of a linear datastructure. Let the total number of items in I(set I=i1,i2,i3.in) is n and the number of items in transaction T is m. While we are processing the transaction T we have to search in the datastructure m*n times in worst case scenario. This will happen for all the transactions.

BST is created only if the number of items is not known.

A. Binary Search Tree  Binary search is a more efficient algorithm than linear search, but it can be applied only on a sorted list ,most commonly using on arrays. The procedure of binary search will be started by checking the middle (approximately) item in the linear list with the target item . If it is not same as the target item and the target is smaller than the middle item, the target item must be in the first half of the list(if the list is sorted in ascending order). If the middle item is smaller than the target, the target must be in the last half of the list. Therefore, one unsuccessful comparison reduces the search space by half .The search continues by checking the middle item in the remaining half of the list. If it?s not the target, the search narrows to the half of the remaining part of the list that the target could be in. The splitting process continues until the target is located or the remaining list consists of only one item. If that item is not the target, then it?s not in the list. A binary search tree (BST), is a tree like data structure where each internal node     Fig. 1. (a)Table1.The transaction database(b)BST after inserting transaction- 1.

Fig. 2. (a)BST after inserting transaction-2.(b).BST after inserting all transactions.

stores an element such that the elements stored in the left sub tree of the node are less than or equal to the element of the node and elements stored in the right sub tree of the node are greater than or equal to the element of the node. Each node has no more than two child nodes. The left sub-tree contains only nodes with keys less than the parent node; the right sub-tree contains only nodes with keys greater than the parent node.

The main advantage of binary search trees is that it remains ordered, which provides a faster search than many other data structures.

B. The header tree  After getting the support count of each item ,a header table is created by using the frequent items only. This time we have a prior knowledge about the number of items. Therefore we can create array here. But to use a static datastructure the availability of continuous free memory is an issue. Here again a binary search tree is used as a header tree instead of header table called Binary Search Header Tree(BSHT) . Each node of the BSHT includes item name , support count ,link to the next node and link to the corresponding item in the fptree. If we are using BST while counting the item frequency we can prune the infrequent items and can reuse the same BST as BSHT with a slight modification. The BST is created earlier by considering item names as keys. In BSHT also the item names are used  as keys. The structure of a binary search tree depends on the order of items . In this implementation the number of items is available. So to get the maximum efficiency here we created the BSHT after sorting the items in frequency descending order to get the most frequent item in the root of BSHT. Then the second scan starts for constructing the tree by inserting each transaction one by one. Before inserting a transaction in to the tree the infrequent items have to be removed and the remaining items should be sorted in descending order of the frequency .

In earlier algorithms to remove the infrequent items we should search the header table to verify whether the item is there or not. The header table is the set of items sorted in descending order of their frequency. Therefore, only the basic linear search can be applied. Another issue is that to sort the remaining items in the transaction, each item?s position in the header table should be considered to avoid the conflicts between the items with same frequency . Here also only a linear search is possible. Linear search is the simplest search algorithm ,it checks the first item, then the second item, then the third item and so on until you find the target item or reaches the end of the list. Its worst case cost is proportional to the number of elements in the list. Therefore, if the list has more than a few elements, other methods will be faster. In general, for a list of length n, the worst case is n comparisons. The number of comparisons to find a target increases linearly as the size of the list increases. Suppose that we want to add a transaction T with n items i1, i2, . . . , in into an FP-tree, and the number of items in the header table is m the header table have to be visited n times , in each search it must verify m nodes. Therefore, in the worst-case scenario, the complexity of processing one transaction before insertion into fp-tree is n*m. Therefore we suggest the new data structure BSHT instead of header table to avoid this time consuming searches.

Construct the BSHT with the frequent items same as the structure of binary search tree, each node contains item name, support count, link to children and link to the corresponding node in fptree. Item names are using as keys. Therefore, a binary search can apply. Binary search is more efficient than a linear search. Here is the procedure to apply this method. First sort the frequent items in descending order of their frequency, then construct the BSHT by taking each item from the top of the sorted list. Put the first item on the root node, insert the next item as the left child of the root if its name is less than the name of the root, otherwise insert as the right child.

Insert all the items to the BSHT to apply a fast binary search.

To process a transaction T , construct two arrays with the same size equal to the number of items in the transaction. Fill the first array with the items in the transaction and second array with zeros. Find the frequency of each item from the header tree by using a binary search method and put the value in corresponding location of the second array. Remove the items with zero frequency and sort all other items based on their frequency which is stored in the second array. During the mining process the fp-growth method and its variations recursively creating the conditional fp tree and also a header table for each conditional fp-tree. But in IHT-growth method a BSHT and conditional fp-tree are created recursively.

To implement the binary search on a linked list structure is more complex than a linear search. We can use only a dynamic datastructure to store the items in the transaction database because during the scan new items have to be added     Fig. 3. header tree with fptree.

Fig. 4. fp tree with header table  dynamically. So to get the advantages of arrays, the advantages of dynamic datastructure and to implement binary search we used here the binary search tree. So the efficiency of the frequent mining has improved drastically.Table1 in Figure 1(a) shows an example database and sets the min-sup as 2. Fig.

1(b),Fig.2(a) and (b) illustrates the binary search tree for table 1 after inserting the first , second and all transactions respectively.

IV. IHT ALGORITHM.

Each node contains item name , frequency and link to the next node. During the second scan we can reuse the same BST as the header tree, but we have to do two things. First, include another field in the nodes as link to point to the corresponding first node of the item in the fptree, second, delete all infrequent nodes from the BST by using the BST node removal process.

We can use array or BST to use as a header table. Both of them perform efficiently and we can apply a binary search on both the data structure. Fig.3 illustrates the header tree with     Fig. 5. header tree constructed from sorted items.

fp tree for table 1.

third column of Table1 contains the sorted transactions of transaction database . Fig.4 represents the fptree with header table with items sorted in the order of item names. In this section we introduce another way to construct a more efficient header tree by using another method. After the first scan we will get all frequent items with its frequency count. Sort the items in the order of its frequency and construct the header tree based on this list. The root of the header tree constructed by using this method contains the highest frequent item and all other items with highest frequency are stored around the root.

The items with highest frequency will be searched more than less frequent items, so by using this method we can reduce the searching time again. Here we implemented this method.

Figure-5 contains the sorted items in their frequency order and shows the header tree constructed by this method with the sorted items.

We included four algorithms here to mine frequent item- sets. The first algorithm is used to construct the BST by scanning the database once. In all other fptree algorithms the first scan is used to find the 1-frequent items. So we have to know the number of items earlier to use an array. We can use the first algorithm if we dont know the number of items .

But in our experimental results we used the second algorithm because we already know the number of items. The second algorithm is used to construct the BSHT.

The third algorithm is used to construct the fptree . This process is time consuming. In our algorithm we used the BSHT to sort the transactions. In the 4th algorithm the actual mining process is conducting. The mining will be started from the least frequent item. The process is as follows. From the fptree find all the paths which includes the current item x. We set the support of each identified path with the support of x in that particular path. Each path is considering as a transaction. Then the process will be called recursively. The new paths will be used as dataset in algorithm 2, then construct the fptree of x, and finally call the 4th algorithm to find the frequent items of x. If more than one path is there in the fptree of x again all the procedure will be recursively called.

Fig. 6. dataset.

Fig. 7. Runtime test(Connect).

Fig. 8. Runtime test(Retail).

Fig. 9. Runtime test(T10L4D1000K ).

V. PERFORMANCE EVALUATION.

In this section we present some experimental results by comparing our algorithm IHT-growth with other two algorithms - FP-growth[1] and IFP-growth[4]. The FP- growth algorithm was obtained from the FIMI website, http://fimi.ua.ac.be/src/ and IFP-growth algorithm was down- loaded from the website www.sciencedirect.com. We used one synthetic dataset and two real datasets in the experiments. The real datasets are retail and Connect , and the synthetic dataset is T10L4D1000K. All the datasets were downloaded from FIMI04 websitehttp://fimi.cs.helsinki.fi.data. Connect dataset contains network connection data and it is a dense dataset.

Retail is a sparse dataset , consists of product sales data from retail stores. Each transaction in the Retai dataset represents purchase information from one consumer at a time. The details of the datasets are presented in figure 6.

The programming language used to implement all the three algorithms is java and run in 3.3 GHz Intel processor, 4 Gbyte memory and Windows 7 32bit OS.

Figs. 7?9 show results of runtime experiments regarding the real and synthetic datasets shown in figure 6. In these figures, we can observe that IHT-growth outperforms the others in all of the cases. IHT-growth uses the proposed header tree structure to store the 1-frequent items instead of the older header table to minimize access times to search items. As a result, its advantages have a positive effect on reducing runtime in whole experiments. Especially in the case of Retail dataset the difference of runtime between our algorithm and the others is much more than the other datasets .

In all experiments FP-growth shows the worst performance.

Note that IHT-growth method can be used with any fptree mining to improve its efficiency. We suggested here two methods to implement the new algorithm. If we dont know the number of items in advance, the First method is suggested.

In this experiments we used the second method. By using the second method we can create a more efficient BSHTree because all the items with highest frequency will be appeared on the top of the BSHTree. By using this method the run time can be improved by minimizing the searching time of items while sorting out the transactions.



VI. CONCLUSION  This study proposes an efficient transaction processing method during the transaction scanning time. By applying the efficient binary search tree, the mining time drastically reduced. The new transaction sorting method is also improves the performance of mining. The experimental results show that our algorithm outperforms the fp-growth and IFP-growth two well known and widely used algorithms. Here we used a BSHTree after the first scan to store the items with support count. While using the BSHTree we used the actual names of items as keys. This method can be applied to improve the mining process with any frequent itemset mining algorithm which is using a header table.

