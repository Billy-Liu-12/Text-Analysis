A Simple but Effective Stream Maximal Frequent Itemset Mining Algorithm

Abstract?Maximal frequent itemsets are one of several con- densed representations of frequent itemsets, which store most of the information contained in frequent itemsets using less space, thus being more suitable for stream mining. This paper focuses on mining maximal frequent itemsets approximately over a stream landmark model. We separate the continuously arriving transactions into sections, and the mining results are indexed by an extended direct update tree; thus, a simple but effective algorithm named SMIS is proposed. In our algorithm, we employ the Chernoff Bound to perform the maximal frequent itemset mining in a false negative manner, which can reduce the memory cost, as well guarantee our algorithm conducting with an incremental fashion. Our experimental results on two synthetic datasets and two real world datasets show that SMIS achieves much reduced memory cost in comparison with the state-of-the- art algorithm with a 100 percent precision.

Keywords-false negative; maximal frequent itemset; stream

I. INTRODUCTION  Frequent itemset mining is a traditional and important problem in data mining. An itemset is frequent if its support is not less than a threshold specified by users. Traditional frequent itemset mining approaches have mainly considered the problem of mining static transaction databases. In these methods, transactions are stored in secondary storage so that multiple scans over data can be performed. Three kinds of frequent itemset mining approaches over static databases have been proposed: reading-based[3], writing-based[15], and pointer-based[18]. [14] presented a comprehensive survey of frequent itemset mining and discussed research directions.

Many methods focusing on frequent itemset mining over a stream have been proposed. [13] proposed FP-Stream to mine frequent itemsets, which was efficient when the average transaction length was small; [22] used lossy counting to mine frequent itemsets; [7],[8], and [9] focused on mining the recent itemsets, which used a regression parameter to adjust and reflect the importance of recent transactions; [27] presented the FTP-DS method to compress each frequent itemset; [10] and [1] separately focused on multiple-level frequent itemset mining and semi-structure stream mining; [12] proposed a group testing technique, and [17] proposed a hash technique to improve frequent itemset mining; [16] proposed an in- core mining algorithm to speed up the runtime when distinct items are huge or minimum support is low; [19] presented two methods separately based on the average time stamps and  frequency-changing points of patterns to estimate the approx- imate supports of frequent itemsets; [5] focused on mining a stream over a flexible sliding window; [20] was a block-based stream mining algorithm with DSTree structure; [23] used a verification technique to mine frequent itemsets over a stream when the sliding window is large; [11] reviewed the main tech- niques of frequent itemset mining algorithms over data streams and classified them into categories to be separately addressed.

Given these algorithms, the runtime could be reduced, but the mining results were huge when the minimum support was low; consequently, the condensed representations of frequent itemsets including closed itemsets[25], maximal itemsets[31], free itemsets[4], approximate k-sets[2], weighted itemsets[28], and non-derivable itemsets[6] were proposed; in addition, [26] focused on discovering a minimal set of unexpected itemsets.

The concept of maximal frequent itemsets(MFI) was first proposed in 1998, an itemset is maximal frequent itemset if its support is frequent and it is not covered by other frequent itemsets. We will discuss the details in Section 2. Maximal frequent itemsets are one of the condensed representations, which only store the non-redundant cover of frequent itemsets, resulting in a space cost reduction.

Many maximal frequent itemset mining algorithms were proposed to improve the performance. The main considera- tions focused on developing new data retrieving method, new data pruning strategy and new data structure. Yang used di- rected graphs in [35] to obtain maximal frequent itemsets and proved that maximal frequent itemset mining is a ?p problem.

The basic maximal frequent itemset mining method is based on the a priori property of the itemset. The implementations were separated into two types: One type is an improvement of the a priori mining method, a breadth first search[32], with utilizing data pruning, nevertheless, the candidate results are huge when an itemset is large; a further optimization was the bottom-up method, which counted the weight from the largest itemset to avoid superset checking, also, the efficiency was low when the threshold was small. Another one used depth first search[33] to prune most of the redundant candidate results, which, generally, is better than the first type. In these algorithms, many optimized strategies were proposed[34][31]: The candidate group has a head itemset and a tail itemset, which can quickly built different candidate itemsets; the super- itemset pruning could immediately locate the right frequent   DOI 10.1109/CIS.2011.281     itemset; the global itemset pruning deleted all the subitemsets according to the sorted itemsets; the item dynamic sort strategy built heuristic rules to directly obtain the itemsets with high support, which was extended by a further pruning based on tail itemset; the local check strategy got the related maximal frequent itemsets with the current itemset.

Many stream mining algorithms for maximal frequent item- sets were proposed to improve the performance. [21] pro- posed an increasing algorithm estDec+ based on CP-tree structure, which compressed several itemsets into one node according to their frequencies; thus, the memory cost can be flexibly handled by merging or splitting nodes. Furthermore, employing an isFI-forest data structure to maintain itemsets, [30] presented DSM-MFI algorithm to mine maximal frequent itemsets. Moreover, considering maximal frequent itemset is one of the condensed representation, [24] proposed INSTANT algorithm, which stored itemsets with frequencies under a specified minimum support, and compared them to the new transactions to obtain new itemsets; Plus, [29] presented an improved method estMax, which predicted the type of itemsets with their defined maximal life circle, resulting in advanced pruning.

Motivation: In [29], estMax used a significant support to increase the mining range of frequent itemsets, that is, some false positive itemsets are obtained as the potential frequent itemsets. The problem is, the significant support is decided by the minimum support, which may result in a lot of useless itemsets being stored in memory. Nevertheless, maximal frequent itemset mining is to reduce the memory cost. That is, estMax can be further improved based on a false negative method.

In this paper, we address this problem and propose a Chernoff Bound based method named SMIS on a stream landmark model. The main contributions are as follows:  1) First, Although stream maximal frequent itemset mining have been proposed a lot, they are all false positive methods. We will use a false negative method to obtain the maximal frequent itemsets over stream, which is never considered before to our best knowledge.

2) Second, we split the stream data into blocks and build data synopsis, which is indexed with an extended direct update tree, through which we can raise the computing efficiency.

3) Third, our algorithm is space efficient. It prune the redundant infrequent itemsets, which results in our al- gorithm being memory efficient; plus, our algorithm is 100 percent precise.

4) Finally, we evaluate our algorithm on two synthetic and two real-life datasets in comparison to the state-of-the- art maximal frequent itemset mining method estMax.

The experimental results show that SMIS is effective and efficient.

The rest of this paper is organized as follows: In Section 2 we define the mining problem with presenting the preliminar- ies of frequent itemsets, maximal frequent itemsets, and Cher- noff Bound method. Section 3 presents our method based on  TABLE I SIMPLE DATABASE  id itemsets 1 a b c d e f g 2 a b c d e f 3 a b c d e 4 a b c d 5 a b c 6 a b 7 a  Chernoff Bound in detail. Section 4 evaluates the performance of our algorithm with experimental results. Finally, Section 5 concludes this paper.



II. PRELIMINARIES AND PROBLEM STATEMENT  A brief review of frequent itemset and maximal frequent itemset is presented in this section; also, a tool of the false negative method, the Chernoff Bound, is introduced. Based on the concepts, we introduce the problem addressed in this paper.

A. Preliminaries  1) Frequent Itemsets: Given a set of distinct items ? = {?1, ?2, ? ? ? , ??} where ??? = ? denotes the size of ?, a subset ? ? ? is called an itemset; suppose ??? = ?, we call ? a k-itemset. A concise expression of itemset ? = {?1, ?2, ? ? ? , ??} is ?1?2 ? ? ???. A database ? = {?1, ?2, ? ? ? , ??} is a collection wherein each transaction is a subset of ?, namely an itemset. Each transaction ??(? = 1 ? ? ? ?) is related to an id, i.e., the id of ?? is i. The absolute support (AS) of an itemset ?, also called the weight of ? , is the number of transactions which cover ?, denoted ?(?,?(?)), ?(?) in short, is {?? ??? ? ? ? ? ? ?}; the relative support (RS) of an itemset ? is the ratio of AS with respect to ???, denoted ??(?) =  ?(?) ? . Given a relative minimum support  ?(0 ? ? ? 1), itemset ? is frequent if ??(?) ? ?.

2) Maximal Frequent Itemsets: A maximal itemset is a  largest itemset in a database ?, that is, it is not covered by other itemsets. A maximal frequent itemset is both maximal and frequent in ?.

Definition 1. Given an relative support ?, an itemset ? is maximal frequent itemset if it is frequent and it is not covered by other frequent itemsets, denoted ??(?) ? ? ? ???(? ? ? ? ??(?) ? ?).

Example 1. Given a simple database ? as shown in Tab.I and an absolute support 5, the frequent itemsets are {?, ?, ?, ??, ??, ??, ???}, nevertheless, the maximal frequent itemsets are {abc}. As can be seen, the maximal frequent itemsets are much less than the frequent itemsets, which is much adaptive for the stream mining.

3) Chernoff Bound: Given n independent value ?1, ?2, ? ? ? , ??, ??+1, ? ? ? according with Bernoulli trial and probable value p, ??[?? = 1] = ? and ??[?? = 0] = 1 ? ? is satisfied. Let r be the actual value of ??[?? = 1], then the expected value of r is  np, ?? > 0, ??{?? ? ??? ? ???} ? 2?????  2 . Let     W indow 1  W indow 2  B1 B2 B3 B4 B5  Time Line  W indow 3  Fig. 1. A running example of landmark model  ? = ?? , ??{?? ? ?? ? ??} ? 2? ????2  2 . Let ? = ??; thus,  ??{?? ? ?? ? ?} ? 2????  2? . Let ? = 2? ???2  2? , then the average probable value of ?? = 1 beyond the range of [? ? ?, ? + ?] with a probability less than ?, in which ? =  ? 2???(2/?)  ? . That is to say, for a sequence of transactions ? = {?1, ?2, ? ? ? , ??, ??+1, ? ? ? , ??} where n is the number of first arrived transactions and ? << ? . The actual support ??(?,?(?)) for itemset ? is within the range of [??(?,?(?)) ? ?,??(?,?(?)) + ?] with probability larger than 1? ?.

The FDPM algorithm[36] substituted p with the minimum support ?, the rationale is that ? is the minimum support, which is satisfied represents all the other higher supports are also satisfied. For an instance, suppose ? = 0.1, ? = 0.1, and ? = 0.01, according to Chernoff Bound, ? ? 5991, i.e., for 5991 transactions, the actual support of itemset ? is between 0.09 and 0.11 with probability larger than 0.9. [37] try to mine all the results with one stream scan: The stream is split into sections, and the frequent itemsets from the previous section will be used as the candidate itemsets of the next section, Chernoff Bound is also used to guarantee the precision. As can be seen, both algorithms aim to obtain the frequent itemsets.

We will prove that the Chernoff Bound method can also be effectively used in maximal frequent itemset mining based on our definition.

B. Problem Definition  We choose the landmark model in this paper because it can reflect the whole characteristic of a stream, i.e., the problem addressed in this paper is to dynamically generate maximal frequent itemsets from all arrived transactions. Fig. 1 is an example: Once a group of new transactions arrive, they will be processed and the new results will be output immediately.



III. SMIS METHOD  As can be seen from our addressing problem, we can get a simple but effective method to obtain the maximal frequent itemsets over stream using Chernoff Bound. In Algo.1, we will employ FDPM method to mine the frequent itemsets, and use a traditional maximal frequent itemset mining method to get the results for each arriving sections. in this algorithm, we store all the frequent itemsets and potential frequent itemsets of the existing transactions, which may consume much more memory and cannot achieve our destination, i.e., using maxi- mal itemsets to save the memory cost; otherwise, if we only  Algorithm 1 SMIS Function Require: ?1: Existing frequent itemsets and potential fre-  quent itemsets; ?1: The number of arrived transactions; ??1: Existing maximal frequent itemsets; ?2: New frequent itemsets and potential frequent itemsets; ?2: The number of new arriving transactions; ?: Minimum support; ?: Probability; ?: False negative parameter;  1: ?2 = 2+2??(2/?)  ? ;[36] 2: for each ?2 new arriving transactions do 3: obtain ?2; 4: ?1 = ?1 + ?2; 5: ?1 = ?1 ? ?2; 6: ? =  ? 2???(2/?)  ?1 ;  7: prune the new infrequent itemsets from ?1; 8: obtain ??1 from ?1 w.r.t. a traditional maximal frequent  itemset mining algorithm on demand;  TABLE II DATA CHARACTERISTICS  DataSet nr. avg. min. max. nr. trans.

of trans. trans. trans. of corr.

trans. length length length items T10I4D100K 100 000 10.1 1 29 870 86.1 T40I10D100K 100 000 39.6 4 77 942 23.8  KOSARAK 990 002 7.1 1 2497 36 841 5188.8 MUSHROOM 8 124 23 23 23 119 5.2  store the maximal frequent itemsets instead of all frequent itemsets, we cannot guarantee the algorithm performing with an incremental fashion since some itemsets information is missed.

To speed up itemsets comparison and results output, we will build the index on the 3-tuples in ?1. Since our aim is to reduce the memory cost, the index will be as simple as possible. Consequently, we use an extended lexicographical ordered direct update tree(EDIU tree) rather than a traditional prefix tree or an enumeration tree. In our EDIU tree, the root node is the itemset includes all distinct items, and all the descend nodes are the subsets. We link each itemset with their subsets, which are sorted by their lexicographical orders. The advantages of our index are as follows. First, the EDIU tree is simple, only the proper itemsets are stored, the only added information is the pointers between itemsets. Second, when we need to find or compare itemsets, the redundant computing will be ignored. Third, when we prune some itemsets, which is a frequent operation especially in our algorithm, the pruning efficiency is high since most itemsets can be deleted in a cascaded matter.



IV. EXPERIMENTAL RESULTS  All experiments were implemented with ? + +, compiled with Visual Studio 2005 running on Windows XP and executed on a Core2 DUO CPU 2.0GHz PC with 2GB RAM.

10-4  10-3  10-2  10-3 10-2 10-1 100  R un  ni ng  T im  e C  os t(  S )  Mimimum Support  T10I4D100K  SMIS Method estMax  (a) T10I4D100K  10-4  10-3  10-2  10-1   10-3 10-2 10-1 100  R un  ni ng  T im  e C  os t(  S )  Mimimum Support  T40I10D100K  SMIS Method estMax  (b) T40I10D100K  10-4  10-3  10-2  10-3 10-2 10-1 100  R un  ni ng  T im  e C  os t(  S )  Mimimum Support  KOSARAK  SMIS Method estMax  (c) KOSARAK  10-3  10-2  10-1    10-3 10-2 10-1 100  R un  ni ng  T im  e C  os t(  S )  Mimimum Support  MUSHROOM  SMIS Method estMax  (d) MUSHROOM  Fig. 2. Running time cost VS. Minimum support  We used 2 synthetic datasets and 2 real-life datasets, which are well-known benchmarks for frequent itemset mining. The T10I4D100K and T40I10D100K datasets are generated with the IBM synthetic data generator. The KOSARAK dataset contains the click-stream data of a Hungarian online news portal. The MUSHROOM dataset contains characteristics from different species of mushrooms. The data characteristics are summarized in Tab. II.

The estMax algorithm in [29] is a state-of-the-art method for mining maximal frequent itemsets over stream; thus, we use it as the evaluated method for comparison. In estMax, without loss of generality, we configure the fixed parameter ???? = 0.1 and ???? = 0.01; In our algorithm, we configure the Chernoff Bound parameter as a fixed value, that is, ? = 0.1, which denotes a 10 percent probability for mistaken deleting the actual maximal frequent itemsets.

A. Running Time Cost and Memory Cost Evaluation  As shown in Fig.2, when the minimum support decreases, the running time cost of these three algorithms increase over all datasets. Our algorithm is much better than estMax in runtime cost, the reason is that our algorithm prunes some useless computing, as well employs an EDIU tree to index itemsets. As shown in Fig.3, our algorithm reaches to a little lower memory cost than estMax. That is because our method prune some infrequent itemsets when the new transactions arrive.

B. Precision and Recall  Our algorithm and estMax are both approximate methods, but with employing the Chernoff Bound method, our algorithm is much more efficiency in memory cost. Since the error comparison of frequent itemsets has been conducted in FDPM algorithm, here we use precision and recall to present the approximation of the maximal frequent itemsets, which are defined as follows: For an actual collection ? and a computed  10-1     10-3 10-2 10-1 100  M em  or y  C os  t( M  )  Mimimum Support  T10I4D100K  SMIS Method extMax  (a) T10I4D100K     10-3 10-2 10-1 100  M em  or y  C os  t( M  )  Mimimum Support  T40I10D100K  SMIS extMax  (b) T40I10D100K    10-3 10-2 10-1 100  M em  or y  C os  t( M  )  Mimimum Support  KOSARAK  SMIS Method extMax  (c) KOSARAK      10-3 10-2 10-1 100  M em  or y  C os  t( M  )  Mimimum Support  MUSHROOM  SMIS extMax  (d) MUSHROOM  Fig. 3. Memory Cost VS. Minimum support    0.07 0.05 0.03 0.01  P R  E C  IS IO  N (%  )  Mimimum Support  T10I4D100K  SMIS Method estMax  (a) T10I4D100K    0.07 0.05 0.03 0.01  P R  E C  IS IO  N (%  )  Mimimum Support  T40I10D100K  SMIS Mehtod estMax  (b) T40I10D100K    0.07 0.05 0.03 0.01  P R  E C  IS IO  N (%  )  Mimimum Support  KOSARAK  SMIS Method estMax  (c) KOSARAK    0.07 0.05 0.03  P R  E C  IS IO  N (%  )  Mimimum Support  MUSHROOM  SMIS Method estMax  (d) MUSHROOM  Fig. 4. Precision of maximal frequent itemsets VS. Minimum support  one ??, the precision of ?? is ? = ??? ?  ?? , and the recall of ? ?  is ? = ??? ?  ? . The larger the precision and recall ,the closer between ? and ??; as an example, when ? = ??, ? = ? = 1.

As shown in Fig.4, the precisions of all the two algorithms are 100% over different datasets, that is because they both can obtain the true maximal frequent itemsets, nevertheless, our algorithm obtain the true results based on a least number of frequent itemsets. As shown in Fig.5, even though we set a probability of 10 percent to be the wrong results, the experimental results are much better. Since our method stores all the un-maximal frequent itemsets, it can obtain all the maximal frequent itemsets; thus, the recall of both algorithm are 100% over different datasets.

0.07 0.05 0.03 0.01  R E  C A  LL (%  )  Mimimum Support  T10I4D100K  SMIS Method estMax  (a) T10I4D100K    0.07 0.05 0.03 0.01  R E  C A  LL (%  )  Mimimum Support  T40I10D100K  SMIS Method estMax  (b) T40I10D100K    0.07 0.05 0.03 0.01  R E  C A  LL (%  )  Mimimum Support  KOSARAK  SMIS Method estMax  (c) KOSARAK    0.07 0.05 0.03  R E  C A  LL (%  )  Mimimum Support  MUSHROOM  SMIS Method estMax  (d) MUSHROOM  Fig. 5. Recall of maximal frequent itemsets VS. Minimum support

V. CONCLUSIONS  In this paper we considered a problem, which is how to mine maximal frequent itemset over stream using a false negative method, and then proposed our method SMIS. In our algorithm, we used Chernoff Bound to prune the infrequent itemsets, which can still guarantee that our algorithm was able to perform in an incremental manner. Furthermore, we employed an extended direct update tree to index the itemsets, which can raise the computing efficiency. Our experimental re- sults showed that our algorithm was more efficient in memory cost and running time cost in comparison with the state-of- the-art maximal frequent itemset mining algorithm.

