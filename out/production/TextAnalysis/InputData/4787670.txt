Index Terms - Data mining rule mining classifier, instance- centric, prune.

Abstract-- Classification is an important task in both data mining and machine learning communities. In this paper we present a method to build a categorization system that merges association rule mining task with the classification problem.A classifier is built by applying a learning method to a training set of objects. In this case the learning method is represented by the association rule mining and used for building classification models.A good text classifier is a classifier that efficiently categorizes large sets of text documents in a specified time and with an acceptable accuracy, But most of the learning approaches with new technique text ,automatic categorization method are coming from machine learning research using association rule mining in the data-mining field which proves to be efficient and effective for construction of effective classifiers.

We focus on two major problems: (1) finding the best term association rules in a textual database by generating and pruning; and (2) using the rules to build a text classifier. In addition, training as well as classification is fast.The main idea behind this approach is to discover strong patterns that are associated with the class labels._ The problem of discovering all association rules from a set of transactions consists of generating the rules that have a support and confidence greater than given thresholds. These rules are called strong rules.. The pruning methods eliminate the specific rules and keep only those that are more general and with high confidence, and prune unnecessary rules by database coverage.I have presented an association rule-based classifier with all categories (ARC- AC) algorithm for building the classifier and association rule based classifier by category (ARC-BC) that considers categories one at a time. The algorithm assumes a transaction-based model for the training document. The introduction of the dominance factory allowed multi-class categorization..

A Set of Classical Factors for Rule Mining in Pruning Methods  M.Ravichandranl, P. Sengottuvelan2, A. Shanmugam3  Lecturerl , Research Scholar/Lecturer 2, Principal3  Department of Information Technology, Bannari Amman Institute of Technology,  Sathyamangalam -638401, India.

E-mail: mraveen2007~gmai1.com.sengottuvelan~rediffinai1.com.dras_bit~yahoo.co.in  classification algorithms perform very well in classifying both categorical databases and sparse high dimensional databases such as those arising in the context of document classification.

The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minimum support (called minsup) and minimum confidence (called minconf) respectively. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database.

The algorithm incorporates buffer management and novel estimation and pruning techniques. This paper introduces the problem of mining a large collection of basket data type transactions for association rules between sets of items with some minimum specified confidence. Association rules identify the set of items that are most often purchased with another set of items.

Due to this heuristic rule induction process and the sequential covering framework, the final set of discovered rules are not guaranteed to be the best possible. For example, due to the removal of some training instances, the information gain is computed based on the incomplete information; thus, the variable (or literal) chosen by these algorithms to extend the current rule will be no longer the globally optimal one. Moreover, for multi-class problems, these algorithms need to be applied multiple times, each time mining the rules for one class. If the training database is large and contains many classes, the algorithms will be inefficient.

Since the introduction of association rule mining, many association-based classifiers have been proposed. Some typical examples like CBA and CMAR adopt efficient association rule mining algorithms (e.g., Apriori and FP- growth) to first mine a large number of high-confidence rules satisfying a user-specified minimum support and confidence thresholds and then use various sequential-covering-based schemes to select from them a set of high-quality rules to be used for classification.

Most of these schemes defer the selection step only after a large intermediate set of high-confidence rules have been identified, they tend to achieve somewhat better accuracy than the heuristic rule induction schemes. However, the     drawback of these approaches is that the number of initial rules is usually extremely large, significantly increasing the rule discovery and selection time.

A proposed algorithm of this thesis, overcome the problems of both the rule-induction-based and the association-rule-based algorithms. It directly mines for each training instance one of the highest confidence classification rules that it supports and satisfies a user-specified minimum support constraint, and builds the classification model from the union of these rules over the entire set of instances.

The proposed system mines the classification rules for all the classes simultaneously and directly mines the final set of classification rules by pushing deeply some effective pruning methods into the projection based frequent item set mining framework, to achieve high computational efficiency. All these pruning methods preserve the completeness of the resulting rule-set in the sense that they only remove from consideration rules that are guaranteed not to be of high quality.

In this work we present a new classification method for text that takes advantage of association rule mining in the learning phase and makes the following contributions: First, a new technique for text categorization that makes no assumption of term independence is proposed.. Second, it is fast during both training and categorization phases. Third, the classifier that is built using our approach can be read, understood and modified by humans. Experiments show that the effectiveness of the classifier can be improved by manually fine tuning the classification rules generated during the training phase. The resulting classifier is able to perform both single-class classification, by which each document is assigned a unique class label, and multiple-class classification It can be applied in addition to images or any other database that can be modelled as a transactional database [2].



II. LITERATURE REVIEW  The association based classifiers adopt another approach to find the set of classification rules. They first use some efficient association rule mining algorithms to discover the complete (or a large intermediate) set of association rules, from which the final set of classification rules can be chosen based on various types of sequential database covering techniques. Some typical examples of association-based (or Emerging Pattern-based) methods include CBA [12], CAEP [7], CMAR [11], ARCBC [2], and DeEPs [10].

In contrast to the rule-induction-based algorithms, HARMONY does not apply any heuristic pruning methods and the sequential database covering approach. Instead, it follows an instance-centric framework and mines the covering rules with the highest confidence for each instance, which can achieve better accuracy. At the same time, by maintaining the currently best rules for each training instance and pushing deeply several effective pruning methods into the projection-based frequent itemset mining framework [9], [1],  [8], HARMONY directly mines the final set of classification rules, which avoids the time consuming rule generation and selection process used in several association-based classifiers [12], [11], [5].

The idea of directly mining a set of high confidence rules is similar to those in [3], [6]. The author of [3] investigated a brute-force technique for mining the set of high-confidence classification rules, and proposed several effective pruning strategies to control the combinatorial explosion in the number of rule candidates. The FARMER algorithm [6] finds the interesting rule groups for micro array databases. It mines the rules in a row enumeration space, and fully exploits some pruning methods to prune the search space based on the user- specified constraints like minimum support, confidence, and chi-square.

Unlike [3], [6], HARMONY does not need the user to specify the minimum confidence and/or chisquare. Instead, it mines for each training instance one of the highest confidence frequent rules that it covers. In addition, by maintaining the currently best classification rules for each instance, HARMONY is able to incorporate some new pruning methods under the unpromising item (or conditional database) pruning framework.

It has been proven very effective in pushing deeply the length decreasing support constraint or tough block constraints into closed itemset mining [15], [8]. In addition, recently we noticed that a similar approach [5] to this research was independently proposed at the same time frame, and showed its high accuracy in classifying gene expression data.

Two classes of algorithms that are directly related to this work are the traditional rule-induction-based methods and the other is the recently proposed association rule based methods.

Both of these classes share the same idea of trying to find a set of classification rules to build the model. The rule- induction-based classifiers like C4.5 [13], FOIL [14], RIPPER [4], and CPAR [15] use various heuristics such as information gain (including Foil gain) and gini index to identify the best variable (or literal) by which to grow the current rule, and many of them follow a sequential database covering paradigm to speed up rule induction.



III. SYSTEM MODEL  The system model adapt the traditional projection-based frequent item set mining framework to efficiently enumerate the classification rules, then focus on how to push deeply some effective pruning methods into the rule enumeration framework and give the whole algorithm  A. Association rules composing  Algorithm Pruning the set of association rules    Input The set of association rules that were found in the association rule mining phase (S) and the training text collection(D)  Output A set of rules used in the classification process.

Method  (1) sort the roles according to Definition I (2) fomch rule in1he set S (3) find all those rules that are more specific  according to (Dffinition 2) (4) prune those that have lower confidence (5) anew set ofrules S~ is generated (6) fOrfach rule Rin the set S' (7) go over D and find those transactions that are  coveredby the rule R (8) ifRclassifies correctly at least one transaction (9) select R (10) remove those cases that were coveredby R  A.l. Associative classifiers  A new method that builds associative general classifiers has been proposed . In this case the learning method is represented by the association rule mining. The main idea behind this approach is to discover strong patterns that are associated with the class labels and further with these patterns a classifier is built and new objects are categorized in the proper classes.

Two such models were presented in the literature: CMAR [11] and CBA [12]. Although both of them proved to be effective and achieve high accuracy on relatively small VCI datasets [16], they have some limitations. Both models perform only single-class classification and were not implemented for text categorization. So to implement, multiple class classification.

B. Classification Rule Enumeration  Assume the min sup is 3 and the lexicographical ordering is the default ordering scheme, the f list computed is {a, b, c, d, e}. The algorithm applies the divide-and-conquer method plus the depth-first search strategy. It first mines the rules whose body contains item 'a', then mines the rules whose body contains 'b' but no 'a', ... , and finally mines the rules whose body contains only 'e'.

The projection-based item set enumeration framework has been widely used in many frequent item set mining algorithms and will be used as the basis in enumerating the classification rules. Given a training database TrDB and a minimum support min sup, it first computes the frequent items by scanning TrDB once, and sorts them to get a list of  frequent items (denoted by f list) according to a certain ordering scheme.

In mining the rules with item 'a', item 'a' is treated as the current prefix, and its conditional database (denoted by TrDBla) is built and the divide-and-conquer method is applied recursively with the depth-first search strategy. To build conditional database TrDBla, HARMONY first identifies the instances in TrDB containing 'a' and removes the infrequent items, then sorts the left items in each instance according to the flist order, finally TrDBla is built. Following the divide-and-conquer method, the proposed algorithm first mines the rules with prefix 'ab', then mines rules with prefix 'ac' but no 'b', and finally mines rules with prefix 'ae' but no 'b' nor 'c'.

During the mining process, when it gets a new prefix, it will generate a set of classification rules with respect to. the training instances covered by the prefix. For each training instance, it always maintains one of its currently highest confidence rules mined so far. It computes the covering rules according to the class distribution with respect to the prefix P.

C. Ordering ofthe Local Items  To mine the highest confidence covering rules as quickly as possible, a good heuristic is to sort the local frequent items in their maximum confidence descending order.

The widely used entropy to some extent measures the purity of a cluster of instances. If the entropy of the set of instances containing P is small, it is highly possible to generate some high confidence rules with body P{xj}. Thus another good ordering heuristic is to rank the set of local frequent items in their entropy ascending order. In the above rule enumeration process, we used the lexicographical ordering as an illustration to sort the set of local frequent items in order to get the f list. Many frequent item set mining algorithms either adopt item support descending order or support ascending order as the ordering scheme. However, because we are interested in the highest confidence rules w.r.t. the training instances, both the support descending order and ascending order may not be the most efficient and effective ways. As a result, we propose the following three new ordering schemes as the alternatives.

D. Search Space Pruning  Unlike the association-based algorithms, it directly mines the final set of classification rules. By maintaining the current highest confidence among the covering rules for each training instance during the mining process, some effective pruning methods can be proposed to improve the algorithm efficiency.



IV. METHODOLOGY  A. Classification Rule Generation  Enumerate the classification rules under the divide-and- conquer and depth first search paradigm, and proposed several pruning methods to speed up the enumeration of the    ??  i Fig: 2. Output Result-2 ""Edt_F_ToaIo""", ???????????,.

Ollldl. o? Lit YII0'~-.Jl *- .... ? 'Q.~~~  Specifically, given a training database TrDB and a minimum support threshold min sup, the problem of this study is to find one of the highest confidence frequent covering rules for each of the training instances in TrDB, and build a classifier from these classification rules. The input training database must be in the form that is consistent; otherwise, the training database should be first converted to that form. For example, a numerical database should be first discretized into a categorical one in order to use to build the model.

label that are correctly classified) for the classes with low average rule confidences, while a small dominant factor can lead to low precisions (i.e., the percentage of predicted instances for the given class label that are correctly classified) for the classes with high average rule confidences. To overcome this problem, the proposed algorithm adopts a weighted dominant factor-based method.

v. EXPERIMENTAL EVALUATION The experimentation is done to obtain an accurate and  efficient rule-based classifier with good scalability, which should be able to overcome the problems of both the traditional rule based and the recently proposed association- based classifiers. Instead of using the sequential database covering to select the rules, our solution mines a set of high quality rules in an instance-centric manner and can assure that at least one of the highest confidence frequent covering rules (if there is any) w.r.t. any training instance is included in the final result set of classification rules.

Fig: 1. Output Result-l  highest confidence covering rules. By integrating the pruning methods with the rule enumeration, we get the classification rule generation algorithm.

The algorithm first initializes the highest confidence classification rules w.r.t. each training instance to empty, then enumerates the classification rules by calling subroutine ruleminer(0, TrDB). Subroutine rule miner takes as input a prefix itemset pi and its corresponding conditional database cdb. For each conditional instance, it checks if a classification rule with higher confidence can be computed from the current prefix pi, if so, it replaces the corresponding instance's current highest confidence rule with the new rule. It then finds the frequent local items by scanning cdb, prunes invalid items based on the support equivalence item pruning method and the unpromising item pruning method. If the set of valid local items is empty or the whole conditional database cdb can be pruned based on the unpromising conditional database pruning method, it returns directly.

Otherwise, it sorts the left frequent local items according to the correlation coefficient ascending order, and grows the current prefix, builds the conditional database for the new prefix, and recursively calls itself to mine the highest confidence rules from the new prefix.

B. Building the Classification Model  After the set of highest confidence covering rules have been mined, it will be straightforward to build the classification model. HARMONY first groups the set of highest confidence covering rules into k groups according to their rule heads (i.e., class labels), where k is the total number of distinct class labels in the training database.

Within the same group of rules, the algorithm sorts the rules in their confidence descending order, and for the rules with the same confidence, sorts them in support descending order.

In this way, it prefers the rules with higher confidence and the rules with higher support if the confidence is the same.

C. New Instance Classification  After the classification model, CM, has been built, it can be used to classify a new test instance, ti, using the New Instance Classification algorithm. It first computes a score w.r.t. ti for each group of rules in CM, and predicts for ti a class label or a set of class labels if the underlying classification is a multi-class multi-label problem (i.e., each instance can be associated with several class labels).

D. Multi-class multi-label classification  The dominant factor-based method was proposed to predict the class labels for a multi-class multi-label classification problem. However, in many imbalanced classification problems, the average confidence of each group of classification rules may be quite different from each other, this uniform dominant factor based method will not work well. A large dominant factor may lead to low recalls (i.e., the percentage of the total test instances for the given class    In addition, although this study mainly focuses on mining anyone of the highest confidence frequent covering rules for each training instance, it is straightforward to revise to mine the complete set of the highest confidence frequent covering rules or K highest confidence frequent covering rules for each training instance. Implemented the proposed algorithm in Java and performed a thorough experimental study(Fig: 1, Fig: 2, Fig: 3). We first evaluated the system as a frequent item set mining algorithm to show the effectiveness of the pruning methods, the algorithm efficiency and scalability.

Then we compared it with some well-known classifiers on both categorical and text databases. All the experiments except the scalability test were performed on a 2.6GHz Windows machine with 1GB memory.

Fig: 3. Output Result-3 ,  ?  Many previous studies used some small databases to evaluate both the accuracy and efficiency of a classifier. For example, most of the databases used only contain several hundred instances, which means the test databases contain too few test instances (i.e., only a few tens) if the IO-fold cross validation is adopted to evaluate the classification accuracy.

Fig: 4. Output Result-4  ?0110.' 0 ltI.o:~- *'- e lWo .':a..~'70l.iJ  .~n~!i,~?" .. ~ (:~~~~'t'  ~,A>.

~"~fl (;'1;";'., :....;". ...

\ ..t;~~"-i2.'.:'D~~r;..':~'~': ./  In this thesis, main focus is made on relatively large databases (by large, we mean the database should contain no fewer than I000 instances), although it report the comparison results for some small databases.



VI. CONCLUSION This paper introduced a new technique for text  categorization .It employs the use of association rules. Our study provides evidence that association rule mining can be used for the construction of fast and effective classifiers for automatic text categorization. We have presented an association rule-based algorithm for building the classifier: ARC-BC that considers categories one at a time. The algorithm assumes a transaction-based model for the training document set. The experimental results show that the associations rule based classifier performs well and its effectiveness is comparable to most well-known text classifiers. In the case of ARC-BC, when new documents are presented for retraining, only the concerned categories are adjusted and the rules could be incrementally updated.

The proposed algorithm is an instance-centric classification rule mining paradigm and designed an accurate classifier. Several effective pruning methods and search strategies have also been proposed, which can be pushed deeply into the projection-based frequent item set enumeration framework. The performance study shows that the proposed algorithm has high accuracy and efficiency in comparison with many well known classifiers for both the categorical data and the high dimensional text data. It also has good scalability in terms of the base size.

