Research on the FP Growth Algorithm about Association Rule Mining

Abstract?For large databases, the research on improving the mining performance and precision is necessary, so many focuses of today on association rule mining are about new mining theories, algorithms and improvement to old methods.

Association rules mining is a function of data mining research domain and arise many researchers interest to design a high efficient algorithm to mine association rules from transaction database. Generally all the frequent item sets discovery from the database in the process of association rule mining shares of larger, the price is also spending more. This paper introduces an improved aprior algorithm so called FP-growth algorithm that will help resolve two neck-bottle problems of traditional apriori algorithm and has more efficiency than original one. In theoretic research, An anatomy of two representative arithmetics of the Apriori and the FP Growth explains the mining process of frequent patterns item set. The constructing method of FP tree structure is provided and how it affects association rule mining is discussed. Experimental results show that the algorithm has higher mining efficiency in execution time, memory usage and CPU utilization than most current ones like Apriori.

Keywords-Data mining; Association rule mining; FP growth method

I. INTRODUCTION Data mining is a rapidly expanding field in many  disciplines, ranging from remote sensing to geographical information systems, computer cartography, environmental assessment and planning. The collected data far exceeds people's ability to analyze it. Thus, new and efficient methods are needed to discover knowledge from large spatial databases. Attribute data mining methods were extended to applying in data mining.

Rule mining is a powerful technique used to discover interesting associations between attributes contained in a database [1]. Association rules can have one or several output attributes and an output attribute from one rule can be used as the input of another rule. Association rules are thus useful, both for obtaining an idea of what concept structures exist in the data (as with unsupervised clustering) and for model creation. In the second instance, the rules generated provide the underlying concepts used in the construction of decision trees and even neural networks (although this is carried out by the automated learning process).In data mining, one of the most classic and novel algorithm for association rule is Apriori[2], If the frequencies of items vary a great deal, Apriori will encounter two problems:(1)If minsup is set too high, those rules that involve rare items  will not be found. (2)To find rules that involve both frequent and rare items, minsup has to be set very low. This may cause combinatorial explosion because those frequent items will be associated with one another in all possible ways.

Most of the previous studies, such as[3,4], adopt an Apriori_like approach, because the Apriori heuristic achieves good performance by reducing the size of candidate set. However, candidate set is still costly, especially when there are a mass of patterns or long patterns.

In our works, we developed FP-growth algorithm for solving two neck-bottle problems of traditional apriori algorithm.



II. ASSOCIATION MINING Association rule mining searches for relationships  between items in a dataset. It finds association, correlation, or causal structures among sets of items or objects in transaction databases, relational databases, and other information repositories.To mine an association rule, a database of transactions is needed. And each transaction is a list of items. Then, apply mining algorithm to find the association rule.

The final association rule is in the following form:A->B Which means customer buys A also tends to buy  B.There are no restrictions on the number of items in the head or body of the rule.

That is, the association rule can also be the following form:  A, C->B, D Which means customer buys A and C also tends to buy  B and D.

To mine an association rule, basic concepts of support  and confidence are needed.

To explain the support and confidence, think of the rule:  X -> Y ?1 Support, s, is the probability that a transaction  contains {X, Y} ?2 Confidence, c, is the conditional probability that a  transaction having {X} also contains Z Also, minimum support and minimum confidence is  needed to eliminate the unimportant association rules. Such that the association rule is hold when it is greater than the minimum support and minimum confidence  Example:      2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.177   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.177   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.177   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.177     Transaction ID Items 1 A, B, C 2 A, C 3 A, D 4 B, E, F   Let the minimum support and minimum confidences  both are 50%, we have the following association rule: A -> C (50%, 66.6%) C -> A (50%, 100%) For A->C, the 66.6% means that customer buys A also  have 66.6% chance tends to buy C.

With C->A, customer buys C is 100% tends to buy A.

Equation for support and confidence: Support (P->Q) = Probability (P?Q) Confidence (P->Q) = Probability (Q/P) Think the transaction database in the previous page, for  rule A->C: Support ({A, C}) = 2 / 4 * 100% = 50% Confidence = Support ({A, C})/ Support ({A}) = 50% /  75% = 66.6%

III. APRIORI ALGORITHM The Apriori Algorithm proposed by Agrawal et. al. in  1994, finds frequent items in a given data set using the anti- monotone constraint [5,6]. Apriori algorithm is an influential algorithm for mining frequent itemsets for Boolean association rules. This algorithm contains a number of passes over the database. During pass k, the algorithm finds the set of frequent itemsets Lk of length k that satisfy the minimum support requirement. The algorithm terminates when Lk is empty. A pruning step eliminates any candidate, which has a smaller subset. The pseudo code for Apriori Algorithm is following:  Ck: candidate itemset of size k Lk: frequent itemset of size k L1 = {frequent items}; For (k=1; Lk != null; k++) do begin Ck+1 = candidates generated from Lk; For each transaction t in database do Increment the count of all candidates in Ck+1 that are contained in t Lk+1 = candidates in Ck+1with min_support End Return Lk; In the above example, a frequent itemset with size 3 if  finally mined. And its support is higher than the minimum  support.With the Apriori algorithm, only frequent itemsets satisfy minimum support threshold can be generated.



IV. FREQUENT PATTERN (FP) GROWTH METHOD Apriori needs n+1 scans, where n is the length of the  longest pattern?we can use frequent pattern (FP) growth method to reduce the number of scans of the entire database, d to find the frequent itemsets using only two scans of database.

Procedure FP-growth 1) IF Tree contains a single path P  THEN 2) FOR all combination (denoted as?) to the nodes  in the path P DO 3) generate patter ???with support=minsupport of  nodes in?; 4) ELSE FOR all ?i in the header of Tree DO  BEGIN 5) generate patter ?=?i??with  support=?i.support; 6) onstruct conditional pattern base and generate  Tree?; 7) IF Tree??? THEN CALL FP-growth (Tree?,  ?); 8) END?  Example: Consider a small database with six items as shown in Table 1, Figure 1 gives the main execution process of FP algorithm. This is illustrated as follows:  Table 1 1) Scan databse and find items with frequency  greater then or equal to a threshold.

2) Order the frequent items in decreasing  order,{B5?C4?A3?D3?E3} 3) Construct a tree which has only the root  TID Itemset  A?B?C?D?F B?C?E A?B?D A?B?F C?D?F  6 E?F  4) Scan database again; for each sample: a) add the items from the sample to the existing  tree, using only the frequent items (i.e. items discovered in step 1.)  b) repeat a. until all samples have been process     n u l l  B 2  C 2  A 1  D 1  T 2 :  E 1  n u l l  B 1  C 1  A 1  D 1  T 1 : n u l l  B 3  C 3  A 2  D 1  T 3 :  E 1  E 1  n u l l  B 4  C 3  A 2  D 1  T 4 :  E 1  E 1  D 1  E 1  I te m            S C P B 5 C 4 A 3 D 3 E 3  n u l l  B 5  C 4  A 3  D 2  T 5 :  E 1  E 1  D 1  E 1  Fig.1 the execution process of FP algorithm

V. EXPERIMENTS In this section we performed a set of experiments to  evaluate the effectiveness of the frequent pattern (FP) growth method, The experimental dataset consists of two kinds of data whose records are set to 10K and 100K . For each experiment, Fig.2 show the total runtime, According to the experimental results, the FP growth method is faster than Apriori Algorithm because FP scans the database at most twice, wherease in Apriori this is not known in advance and may be quite large. From the Fig.3, we can see that the FP based approach is quite effective in reducing memory.

Moreover, Let the most original minimum support are 10% ? 20% and 30%, respectively, , the capabilities of FP algorithms are shown in Fig. 4.

100 200 300 400 500 600 800 1000  Number  of  i t emsequences  Ti me  (S EC  ON D)  Apr i or i FP   Fig. 2 the total runtime            10 20 30 40 50 60 80 100  Number  of  r ead r equest s i n di f f er ent  dat abase (KB)  Apr i or i FP    Fig.3 memory space usage       20 40 60 80 100mi nsup=10% mi nsup=20% mi nsup=30%   Fig.4 original minimum support

VI. CONCLUSIONS In conclusion, this paper analyzes application  architecture of data mining algorithm and Association rule mining, creates new mining theoretic models, and designs a new algorithms based on such theories. This technique has been implemented in some parallel compiling system and can get better results for some practical programs.

