An Efficient Association Rule Mining Algorithm In  Distributed Databases

Abstract-This paper describes the alarm correlation in comm- unication networks based on data mining. A direct application of sequential algorithms to distributed databases is not effective, because it requires a large amount of communication overhead. In our study, an efficient algorithm, EDMA, is proposed. It minimizes the number of candidate sets and exchange messages by local and global pruning. In local sites, it runs the application based on the improved algorithm-CMatrix, which is used to calculate local support counts. By numbering the global frequent itemsets generated at the end of k-th iteration from 1 to m, the algorithm codes every candidate (k+1)-itemset into a pair of those number formed as-(x,y) to compress the context transmitted and query corresponding support counts in CMatrix. Our solution also reduces the size of average transactions and datasets that leads to reduction of scan time. The performance study shows that EDMA has superior running efficiency, lower communication cost and stronger scalability than direct application of a sequential algorithm in distributed databases.

1. INTRODUCTION  1.1 Background Association rule mining is an active data mining research  area and most ARM algorithms cater to a centralized environment. However, adapting centralized data mining to discover useful patterns in distributed database isn't always feasible because merging data sets from different sites incurs huge network communication costs. Therefore, our research is to develop a distributed algorithm for geographically distributed data sets that reduces communication costs.

1.2 Association Rules Let I = {i1, i2,? , im} be a set of literals, called items. Let D  be a set of transactions where each transaction T is a set of items such that ? T?D, T ? I. The transaction T supports an itemset A if it supports every element a?A. An association rule is an implication of the form A =>B, where A? I, B? I and A? B= ? . The rule A=>B has two properties, support and confidence. When s% of transactions in D contain A? B, the support of it is s%. If some of the transactions in D contain A, and c% also contain B, then the confidence in the rule is c%.

The problem of mining association rules is to generate all association rules that have support and confidence are larger than the user-specified minimum support and minimum confidence respectively. We also call them strong rules to distinguish from the weak ones.

It has been shown that the problem of discovering association rules can be reduced to two sub-steps: ? Find all frequent itemsets for a predetermined support ? Generate the association rules from the frequent itemsets  As creating association rules is straightforward, the most crucial factor that affects the performance is to find efficient method to resolve the first problem.

This paper is organized as follows: Section 2 summarizes the related work in distributed ARM. Section 3 introduces a CMatrix to test support counts and investigates the definitions and methodology of EDMA. Section 4 shows the experimental results. The last section concludes the paper.

2. RELATED WORK  Most existing parallel and distributed ARM algorithms are based on a kernel that employs the well-known Apriori algorithm. Directly adapting an Apriori algorithm will not significantly improve performance over frequent itemsets generation or overall distributed ARM performance. The main challenges include work-load balancing, synchronization, communication minimization, finding good data layout, data decomposition, and disk I/O minimization, which is especially important for DARM. In distributed mining, synchronization is implicit in message passing, so the goal becomes communication optimization. Data decomposition is very important for distributed memory. Therefore, the main challenge for obtaining good performance on distributed mining is to find a good data decomposition among the nodes for good load balancing, and to minimize communication.

Distributed ARM algorithms aim to generate rules from different data sets spread over various geographical sites; hence, they require external communications throughout the entire process. They must reduce communication costs so that generating global association rules costs less than combining the participating sites' data sets into a centralized site. There are several famous DARM algorithms described following.

2.1 The Count Distribution (CD) algorithm CD algorithm uses the sequential Apriori algorithm in a  parallel environment and assumes datasets are horizontally partitioned among different sites. At each iteration, it generates the candidate sets at every site by applying the Apriori-gen function on the set of frequent itemsets found at the previous iteration. Every site then computes the local support counts of all these candidate sets and broadcasts them to all the other sites. Subsequently, all the sites can find the globally frequent itemsets for that iteration, and then proceed to the next iteration.

This algorithm has a simple communication scheme for count exchange. However, it also has the similar problems of higher number of candidate sets and larger amount of communication overhead. It does not use the memory of the system effectively.

2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.33   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.33   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.33   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.33   2008 Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2008.33     The algorithm's communication overhead is O (|C|*n) at each phase, where |C| and n are the size of candidate itemsets and number of datasets, respectively.

2.2 The Fast Distributed Mining algorithm FDM generates fewer candidates than CD, and use effective  pruning techniques to minimize the messages for the support exchange step. In each site, FDM finds the local support counts and prunes all infrequent local support counts. After completing local pruning, instead of broadcasting the local counts of all candidates as in CD, they send the local counts to polling site. FDM's message optimization techniques require some functions to determine the polling site, which could cause extra computational cost when each site has numerous local frequent itemsets. Furthermore, each polling site must send a request to remote sites other than the originator site to find an itemset's global support counts, increasing message size when numerous remote sites exist. FDM's main advantage over CD is that it reduces the communication overhead to O (|Cp|*n), where |Cp| and n are potentially frequent candidate itemsets and the number of sites, respectively. When different sites have nonhomogeneous data sets, the number of disjoint candidate itemsets among them is frequent, and FDM generates fewer candidate itemsets compared to CD.

3.  OUR NEW ALGORITHM?EDMA  3.1 Problem Statement The distributed database in our model is a horizontally  partitioned database, which means the database schema of all the partitions are the same. However, distributed database also has an intrinsic data skewness property. The distribution of the itemsets in different partitions are not identical, and many items occur more frequently in some partitions than the others.

As a result, many itemsets may be large locally at some sites but not necessarily in the other sites. This skewness property poses a new requirement in the design of mining algorithm.

Let DB be a database with D transactions. Assume that there are n sites n sites S1,S2,?,Sn in a distributed system and the database DB is partitioned over the n sites into{DB1, DB2,?,DBn},respectively. Let the size of the partitions DBi be Di, for i=1,2,..., n. For a given itemset x, let X.count and X.counti be the respective support counts of X in DB and DBi.

We will call X.count the global support count and X.counti the local support count. For a given minimum support s, X is globally frequent if X.count ? s*D; correspondingly, X is locally frequent at site Si, if X.counti ? s*Di. The essential task of a distributed ARM algorithm is to find the globally frequent itemsets L.

3.2 CMatrix Definition A CMatrix is an object-by-variable compressed structure,  which represents the transaction database as a binary matrix where the rows represent transactions and the columns represent alarms. Each entry aij of it is stored in one bit and set to 1, if alarm j is existed in transaction I and 0, otherwise. This means the numbers of '1' per column equals to the support count of corresponding alarm. We need once and only once scan of every partition database to convert each of them to its  local CMatrix. Denoting CMi as the i-th column of a CMatrix, which is called meta-vector. An m*n CMatrix is the data of m transactions on n alarms.

When local sites calculate locally frequent 1-itemsets or center site queries candidate sets to search globally frequent 1- itemsets, we can easily obtain all their support counts by accessing their meta-vector. However, if the size of itemset is lager than 1, it becomes more complex. As every k- itemset was generated of two (k-1)-itemset by using Apriori_gen function. If we know the support counts of those two (k-1)- itemset, then from the meaning of meta-vector and the property of boolean algorithm, we get the meta-vector of a k-itemset Vk=Vk-1,1& Vk-1,2 where Vk-1,1 and Vk-1,2 represent the meta- vectors of its two size k-1 subset, respectively. Since EDMA is an iteration algorithm, we mine globally frequent itemsets pass by pass, that is to say, when we count supports of each k-itemset belongs to CHk we have already obtained the support counts of all its subsets, which are available for computing meta-vectors of k-itemsets with the above analysis. Therefore, we just read the local CMatrix to find support counts instead of scanning the partition databases time after time, which will save a lot of memory.

The following instance is described to illustrate construction and application of CMatrix. The transaction database is shown in Table1.

Table 1  transaction database D              As the method refered above, we easily get a CMatrix of the  transaction database D. The CMatrix C8*5 will be represented as:             Then we can obtain the support count of 'A' by accumulating  the numbers of '1' in the first column while others in corresponding columns. After that we may obtain the support count of any itemsets by joining its two subsets? meta-vector together. Take 'AC' for example, it was generated by 'A' and 'C'.

we first calculate its meta-vector like:  8*5  1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0  C  ? ? ? ? ? ? ? ? ? ? ? ?= ? ? ? ? ? ? ? ? ? ?? ? ? ?                 Then counting the numbers of '1' in VAC, and finally get the  support of AC is 3. It can be seen that using CMatrix to calculate itemset?s support count is convenient and efficient.

3.3 Proposed Algorithm in Detail In this paper, we developed an efficient association rule  mining algorithm in distributed databases called EDMA.

We have found that many candidate sets generated by  applying the Apriori-gen function are not needed in the search of frequent itemsets. In fact, there is a natural and effective method for every site to generate its own set of candidate sets, which is typically much smaller than the set of all the candidate sets. Following that, every site only needs to find the frequent itemsets among these candidate sets. The following lemma is described to illustrate the above observations.

For a site Si, if an itemset X is both locally and globally frequent at site Si, we say that X is heavy at site Si. We use HLi to denote the set of heavy itemsets at site Si. In DMA, the heavy itemsets at each site play an important role in the generation of candidate sets.

LEMMA If X?Lk, (Lk is globally frequent k-itemset), then there exists a site Si, such that X and all its subsets are heavy at site Si.

PROOF. If X.counti<s*Di for all i=1,2,?,n, then X.count<s*D, and X cannot be globally frequent. Therefore, X must be locally frequent at some site Si. Hence X is heavy at site Si. For the monotonic subset relationship, we know all the subsets of X must be locally frequent at Si. Moreover, since X is globally frequent, that all its subsets should also be globally frequent.

Hence, all the subsets are heavy at site Si.

We give a straightforward adaptation of Apriori. That is, in the k-th iteration and at each site Si, let ikCH be the set of candidates sets generated by applying Apriori-gen on 1  i kHL ? .

Namely:  1_ ( ) i i k kCH Apriori gen HL ?=      (1)  Hence, ikCH is generated from 1 i kHL ? , which is only a subset  of Lk-1. According to the Lemma, for every frequent itemset X?Lk, there exists a site Si, such that all the size-(k-1) subsets of X are heavy at site Si; hence, X? ikCH for some site Si.

Therefore  1 1 1_ ( ) n i n i  k i k i kL CH Apriori gen HL= = ?? =? ? (2)  Since every 1 i kHL ? is a subset of Lk-1, the number of candidate  sets in CHk is in general smaller than that in Ck. In EDMA, we generate a set of candidate sets ikCH for each site Si in each iteration. It can be seen that this set of candidate sets is typically much smaller than that in a direct application of Apriori-gen on Lk.

For example, consider three sites. After the first iteration, the global frequent 1-itemset is {A, B, C, D, E, F, G}. A, B, and C are locally frequent and heavy at site S1, B, C, and D are locally frequent and heavy at site S2, and E, F, and G are locally frequent and heavy at site S3. Subsequently, in the second pass, EDMA generates candidate itemsets from those frequent 1-itemsets that are globally and locally frequent at each site. Site support counts of candidate itemsets at site S1 are {AB, AC, BC}; S2 is {BC, BD, CD}; and S3 is {EF, EG, FG}. On the hand, if we straight use all frequent 1-itemset to generate candidate 2-itemsets, the number is 27C =21.

Therefore, we reduce the total number of candidate itemset generation from 21 to 9. In fact, this technique reduces significantly the number of candidate itemsets when numbers of disjoint itemsets among different sites are high.

In order to find the globally frequent itemsets, subsequent to the generation of CHk, support count exchange should be done.

However, we have observed that some candidate sets in CHk can be pruned away by using some local information before the count exchange starts. From the lemma, if X is a globally frequent k-itemset, then there must exist a site Si, X must be locally frequent at site Si. Therefore, a site Si can prune away those candidates in CHk which are not locally frequent at Si.

For describe easily, we use ikLL to denote those candidate sets in ikCH which are locally frequent at site Si.

It should be noticed that: although X is not locally frequent in site Si, it may still be a candidate set at some other site.

Therefore, when center site sends requests to site Si, we need to scan the partition DBi again to find the local support counts of

X. In order to avoid doing two scans, EDMA is required to find the support counts by one scan on the partition. This is possible because the heavy sets for candidate set generation are available to all the sites at the end of each iteration. Since  j kHL ? and  j kCH (j=1,?,n),are available at Si, Si can compute  all these candidate sets before the scan for their local support counts starts. In other words, every site only needs to scan its partition once to find the local support counts of the itemsets in CHk. With this technique, all the support counts required for local pruning and count exchange can be found in a single scan of the partition and the number of scan in EDMA is minimized.

If X is a k-itemset, with respect to each site Si(1 ? i ? n), we use maxsup(X)i to denote the minimum value of the local support counts of all the size (k-1) subsets of X, i.e., maxsup (X)i= min{Y.counti IY? X and Y = k-1). It follows from the subset relationship that maxsup(X)i is an upper bound of the local support count X.counti. Hence, the sum of these upper bounds over all partitions, denoted by maxsup(X), is an upper  1 0 0 1 1 1 1 1 1 0 0 0  & & 0 0 0 1 1 1 0 1 0 0 1 0  AC A cV V V  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?= = =? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ? ? ? ? ? ? ? ?     bound of its global support count X.count. Note that maxsup(X) can be computed at every site at the beginning of the k-th iteration. Since maxsup (X) is an upper bound, it can be used for pruning, if maxsup(X)< s*D, then X cannot be a candidate set. We call this technique global pruning.

Follows from the above discussion, in k-th iteration, EDMA does some improvements of local site and computes frequent k-itemsets at each site Si according to the following procedure.

?  While flagi=true, get heavy itemsets found at site Si in the  k-1 iteration 1 i kHL ? =Lk-1 ? 1  i kLL ? , then generate the  candidate sets 1_ ( ) i i k kCH Apriori gen HL ?= .

?  For each X? ikCH , using the number that center site numbered Lk-1 to code it into a pair of number formed as-(x,y), where 1 ? x,y ? 1kL ? . Then calculate

X.maxcountl = 1,  .sup max sup( ) n  j i  j j i  X X = ?  + ? , prune away those X which X.maxcountl <s*D.

?  Read CMatrixi to compute the local support count of the  remainder X?  n i k  i CH  = ? . When X is locally frequent at  site Si, put it in 1 i kCH =  i kLL ; the remaining candidate sets  are stored in 2 i kCH . If 1  i kCH = ? ,set flagi=false.

?  Send the candidate sets in ikLL to center sites to collect their global support counts.

?  Once flag turns to false, set flagi=false.

?  while flag=true, if Si receives a count request of itemset X  from center site, it read CMatrixi again to obtain support counts of X and sends it back to center site; else it receives globally frequent itemsets and their support counts.

In the sequential Apriori algorithm, each site broadcasts all its candidate sets to others requires O((n-1)* C ) messages in  total, where C and n are the number of candidate itemsets and the number of sites, respectively. In EDMA a site Si only needs O( C ) messages exchange to determine all its heavy sets. In fact, very few candidate sets are locally frequent at all the sites because of the data skewness property, which causes EDMA uses much less messages to generate globally frequent itemsets.

We define the center sites differ from polling site in FDM, which is responsible for collecting the support counts of X? ikCH and find all X?Lk. In the k-th iteration, after local prune phase has been done at site Si, EDMA uses the following procedure to complete support count calculation at center site.

?  Si acts as a home site of its candidate sets, Si then send all  the candidate sets in ikLL and their local support counts to the center site.

?  Center site receives all ikLL sent to it from the partition sites. When 1  n i i kLL=? = ? ,set flag=false. For every  candidate set X? 1n ii kLL=? , it finds the list of originating sites from which X is being sent.

?  If all the partition sites are in the list of X, which means X  is frequent at all sites, put X in Lk and computes its  globally counts X.count=  .

n  i  i X count  = ? ; else calculate

X.maxcountg =    ,prune away those X which X.maxcountg<s*D, then  broadcast the remainders? count requests to the other sites  not on the list to collect the support counts.

?  Because all candidates X?  n i k  i CH  = ? has been scanned,  the local support counts of it which count requests are sent can easily be achieved. Center site receives them back and  adds together. If X.count=  .

n  i  i X count  = ? ? s*D, put it  also in Lk.

?  Center Site then numbering all the X?Lk from 1 to m  (m= kL ).From the knowledge that k-itemset X is frequent only when its k size-(k-1) subsets are frequent, if kL <k+1, set flag=false. It represents another global pruning.

?   Finally, when flag=true, it broadcasts the globally frequent itemsets together with their global support counts to all the sites and find the heavy itemsets in each site Si.

Some might argue that this optimization could cause a bottleneck in the center site when we increase the total number of participating sites. We argue that the center site receives the same level number of candidate itemsets from all local sites that a particular site would receive in the CD algorithm. For example, for three sites, each with five candidate itemsets, each site in the CD algorithm will send its own 5 candidate itemsets (1?5=5) to other sites and will receive 10 candidate itemsets (2?5=10) from other sites. Our proposed technique's center site will receive 15 candidate itemsets (3?5=15) from all local sites.

Furthermore, in most situations, the center site sends fewer frequent itemsets to the local sites by global pruning.

4. PERFORMANCE STUDY OF EDMA  4.1  Generation of synthetic data In our research, we used synthetic databases to evaluate the performance of different algorithms. Fig.1 shows the topology of our network with the alarm data is generated as following:  Alarm occurs in two of the blue node randomly as well as  . arg _ . arg _  .sup min(maxsup( ) , * 1)n nm m X l e sites n X l e sites  X X s D ? ?  + ?? ?               Figure 1    Topology of our experimental network the alarm level and alarm type  Alarms of the blue node cause corresponding alarms in same level of all neighboring nodes and will not be transmitted further  After generating alarms, it comes to the problem of alarm?s synchronization which was settled by setting time window and slip length. The attributes of the alarm that reflect the faults were picked out to form an item of an alarm transaction and the redundant alarms were got rid of by alarm compressing. The alarm?s pretreatment was carried out to transform the alarm database into alarm transaction database which make ready the data for alarm association rules distributed mining.

4.2 Experimental Results We have extensively studied our algorithm's performance to  confirm its effectiveness. We compare the performance of our EDMA with the CD algorithm and consider the superiority of it with FDM. The algorithm was implemented using Java.

We first fix the number of sites. The aim is to perform the comparison with respect to different support thresholds and database sizes. We partition the original data set into four partitions. To reduce the dependency among different partitions, each one contains only 25 percent of the original data set's transactions. So, the number of identical transactions among different partitions is very low.

In Fig.2, the execution time of EDMA, FDM and CD for a database of size 10K transactions are plotted against the support thresholds. We experienced, EDMA is about 4% to 67% faster than CD or FDM, and the difference decreases as the support increases.

In this experiment, we have also compared these algorithms on a series of databases from 1K to 10K transactions. In Fig.3, the execution time in EDMA is compared to that in CD and FDM, for the threshold s=0.3. The result shows that in terms of candidate sets and scan time reduction, the improvement of EDMA is very steady in all cases, and the difference increases as the size increases.

In the second experiment, the threshold and database size are fixed, and performance of the three algorithms are compared with respect to different number of sites from 3 to 7. In Fig.4 and Fig.5, the ratios of average number of candidate sets per sites and the ratios of total message sizes are compared. A reduction of candidate sets number about 75% to 95% and 40% to 78% are witnessed in EDMA compare to CD and FDM, respectively; while a corresponding reduction of message size about 83% to 92% and 47% to 66% among them. Both show EDMA is faster and more scalable. All of that because we expected EDMA uses fewer time to compute support counts              Figure 2  execution time with different minimum support               Figure 3  execution time with different database size                Figure 4  candidate sets reduction of EDMA/CD and EDMA/FDM                Figure 5  message size reduction of EDMA/CD and EDMA/FDM  than FDM. On the other hand, CD broadcasts its candidate support counts to all other sites and subsequently receives support counts of others, broadcasting numerous messages when we increase number of sites.

5. CONCLUSION  Distributed ARM algorithms must reduce communication costs so that generating global association rules costs less than combining the participating sites' datasets into a centralized site. We have developed an efficient algorithm for mining association rules in distributed databases.

Reduces the size of message exchanges by novel local and global pruning.

Reduces the time of scan partition databases to get support counts by using a compressed matrix-CMatrix, which is very effective in increasing the performance.

Founds a center site to manage every the message exchanges to obtain all globally frequent itemsets, only O(n) messages are needed for support count exchange. This is much less than a straight adaptation of Apriori, which requires O(n2) messages for support count exchange.

Furthermore, EDMA can be applied to the mining of association rules in a large centralized database by partitioning the database to the nodes of a distributed system. This is particularly useful if the data set is too large for sequential mining.

In the future, as in our communication network the users? concentration on different alarms is various, which makes how to decide the weight of each alarm to be further considered.

