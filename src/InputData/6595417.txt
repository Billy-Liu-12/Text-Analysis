Minimal-redundancy-maximal-relevance feature selection using different relevance measures for

Abstract?Omics refers to a field of study in biology such as genomics, proteomics, and metabolomics. Investigating funda- mental biological problems based on omics data would increase our understanding of bio-systems as a whole. However, omics data is characterized with high-dimensionality and unbalance between features and samples, which poses big challenges for classical statistical analysis and machine learning methods. This paper studies a minimal-redundancy-maximal-relevance (MRMR) fea- ture selection for omics data classification using three different relevance evaluation measures including mutual information (MI), correlation coefficient (CC), and maximal information coefficient (MIC). A linear forward search method is used to search the optimal feature subset. The experimental results on five real-world omics datasets indicate that MRMR feature selection with CC is more robust to obtain better (or competitive) classification accuracy than the other two measures.



I. INTRODUCTION  With the rapid development of new technologies for inves- tigating biological activity on a global basis in experimen- tal samples, numerous of new -omics signatures have been developed to predict disease progression [1]. Omics data is characterized with thousands of variables/features but with only a small number of samples available for analysis. This makes learning from such data an arduous task under the effect of curse of dimensionality. Furthermore, omics data often contains a large number of irrelevant and redundant features, which tend to affect the speed and accuracy of most learning algorithms. Feature selection is widely used to address these problems by identifying relevant features and eliminating  This work was supported in part by the National Natural Science Foundation of China Joint Fund with Guangdong, under Key Project U1201256, in part by the National Natural Science Foundation of China, under Grants 61171125 and 61001185, in part by the NSFC-RS joint project under Grant 61211130120, in part by the Fok Ying-Tung Education Foundation, Guangdong Natural Science Foundation, under Grant S2012010009545, in part by Scientific Research Foundation for the Returned Overseas Chinese Scholars, Ministry of Education of China, under Grand 20111568, in part by Science Foundation of Shenzhen City under Grants JC201105170650A and KQC201108300045A, and in part by the Shenzhen City Foundation for Distinguished Young Scientists. All correspondence should be addressed to Prof. Zhen Ji (Email: jizhen@szu.edu.cn, Tel.: +86 755 26557413).

irrelevant/redundant features, so that the learning accuracy and understanding of the results could be improved [2].

Feature selection normally consists of an evaluation function and a search strategy for obtaining the optimal feature subset.

The evaluation function measures the discriminating ability of a feature or a feature subset on distinguishing the different sample classes. Depending on whether the learning algorithm is involved in the feature evaluation, feature selection methods could be grouped into filter and wrapper methods [3].

Filter methods, independent on the learning algorithm, usu- ally evaluate the goodness of features based on their intrinsic statistical characteristic, while wrapper methods directly use the performance of the learning algorithm as feature goodness evaluation criterion. Filter methods are computationally more efficient and could be more preferable to handling high- dimensional omics data than wrapper methods. Nevertheless, most filter methods work by ranking features individually based on their relevance to the learning target and then select the top ranked ones; they do not take into account the relation between features and therefore are inadequate to deal with redundant features.

A few information theory based filter evaluation criteria have been proposed to take care both feature relevance and redundancy [4]. The minimal-redundancy-maximal-relevance (MRMR) criterion [5] represents one of such efforts to maxi- mizing feature relevance and minimizing redundancy simulta- neously. Particularly, MRMR evaluates feature relevance and redundancy based on mutual information. The max-relevance condition is used to identify features which have the largest de- pendency on the target class label, while the min-redundancy criterion is used to select mutually exclusive features.

Feature selection with MRMR criterion and incremental search has been shown to have promising performance on gene expression data [5], [6]. MRMR is simply yet efficient in handling both irrelevant and redundant features. In this work, we study MRMR based feature selection with three different feature relevance measures, including maximal information coefficient (MIC), mutual information (MI), and correlation     coefficient (CC), for omics data classification. A linear forward search is introduced to accelerate the search of optimal feature subset. The classification accuracies based on these three crite- ria and linear forward search method are compared using five omics datasets. The experimental results demonstrate that CC is more robust to attain better (or competitive) classification accuracy than the other measures.

The rest of this paper is organized as follows. Section II introduces the MRMR criterion, the three feature relevance evaluation measures, and the linear forward search method.

Section III presents the experimental results of classification accuracy on seven omics datasets and finally Section IV concludes this study.



II. METHODOLOGY  In this section, we describe the definition of MRMR, MI, CC, and MIC, followed by the introduction of linear forward search method.

A. Max-Relevance-Min-Redundancy  Feature evaluation function focusing on relation between features and the target class tends to involve redundant fea- tures, which could impede the understanding of the learn- ing results and sometimes deteriorate the learning accuracy.

MRMR feature selection criterion was proposed by [5] to resolve the problem by evaluating both feature redundancy and relevance at the meantime. Particularly, max-relevance, denoted as maxD(S, c), refers to maximizing the relevance of a feature subset S to the class label c. In [5], the relevance of a feature subset is defined as:  D(S, c) =  |S| ? fi?S  ?(fi, c) (1)  where ?(fi, c) denotes the relevance of a feature fi to c. ? could be estimated with any correlation measures. In [5], ? was calculated based on MI. In this work, we also study the other two measures CC and MIC.

Feature redundancy is defined based on pair-wise feature de- pendence. If two relevant features highly depend on each other, the class-discriminative power would not change too much if one of them were removed. Min-redundancy minR(S) is used to select a feature subset of mutually exclusively features. The redundancy of feature subset R(S) is defined as follows:  R(S) =  |S|2 ?  fi,fj?S ?(fi, fj) (2)  MRMR is defined as the simple operator maximizing D and minimizing R simultaneously. In [5], incremental search method was used to find the near-optimal features. Given a feature subset Sm?1 of m? 1 selected feature, the task is to select the m-th feature that optimizes the following criterion:  max fj /?Sm?1  [?(fj , c)? 1 m? 1  ? fi?Sm?1  ?(fi, fj)] (3)  B. Mutual Information MI is defined based on variable entropy. In information  theory, entropy is a measure of the uncertainty associated with a random variable [7]. Given a variables X of discrete random values in alphabet ?, if the probability distribution of X is p(x), then the entropy of X can be defined as:  H(X) = ? ? x??  p(x) log p(x) (4)  where the base of log is 2. The conditional entropy of two discrete random variables X and Y with their joint probability distribution p(x, y) is:  H(X|Y ) = ? ? x??  ? y??  p (x, y) log p(x|y) (5)  MI measures the amount of information that one random variable contains another random variable. It is defined as:  I(X;Y ) = H(X)?H(X|Y ) =  ? x??  ? y??  p(x, y) log p(x, y)  p(x)p(y)  The larger the MI between two random variables, the more the two variable are closely related, or vice versa.

C. Correlation Coefficient Beside MI, the other measure of feature relevance could  be CC which is one of the most commonly used correlation measure in various areas. Given a pair of variables (X,Y ), the correlation coefficient r(X,Y ) is defined as follows:  r(X,Y ) =  ? i  (xi ? xi)(yi ? yi)?? i  (xi ? xi)2 ??  i  (yi ? yi)2 (6)  where xi is the mean of X , and yi is the mean of Y . The value of r lies between -1 and 1. If X and Y are completely independent, r equals to zero.

D. Maximal Information Coefficient MIC was proposed by David N.Reshef et al [8] as a  novel measure for identifying relationships between pairs of variables in large data sets. MIC was shown to be able to capture a wide range of variable associations which cannot be identified by other measures including MI and CC.

The idea of MIC is that if a relationship exists between two variables, then a grid can be drawn on the scatterplot of the two variables that partitions the data to encapsulate that relationship. Let (X,Y ) be a finite set of ordered two- variable data. The X-value and Y -value can be partitioned into ? bins and ? bins respectively, such that a ?-by-? grid G could be formed. (X,Y )|G denotes the distribution induced by the points in (X,Y ) on the cells of G. For a fixed (X,Y ), different grids G lead to different distributions (X,Y )|G.

Before introducing MIC, the definition of characteristic matrix M(X,Y ) of (X,Y ) is given as follows:  M(X,Y )(?,?) = I?((X,Y ), ?, ?) logmin{?, ?} (7)  2013 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB) 247    where I?((X,Y ), ?, ?) = max I((X,Y )|G) defines the max- imal mutual information of (X,Y )|G over all grids G with ? columns and ? rows. Based on M(X,Y ), the MIC of (X,Y ) with sample size n is defined as:  MIC(X,Y ) = max ??<B(n)  {M(X,Y )?,?} (8)  where B(n) is function of sample size n, which imposes an upper bounds on the sizes of grids for searching the MIC value. In this paper, B(n) = n0.4 is used.

E. Linear Forward Search  Omics data usually contains a large number of features, it is necessary to use search techniques that could be scaled up to large feature size. Peng [5] used an incremental search for selecting optimal feature subset. In their work, at each incremental step, all the excluded features were search to find the one that could optimize (3) and then added this feature to the selected subset. This method could be computationally expensive when the number of features is extremely large. In this work, we use a linear forward search [9] to proceed search of features. Particularly, linear forward search firstly rank all the features in descending order based on their relevance to the class label. Afterward, a forward search is conducted along the ranked feature list with each step searching within a fixed window. It could be seen as a variant of incremental search, with the running time independent on the size of features but the size of window. The outline of MRMR feature selection based on linear forward search is described in Algorithm 1  Algorithm 1 MRMR Feature Selection with Linear Forward Search  1: INPUT: the whole feature set F , class label c, the number of selected feature m, the size of search window W , and the relevance measure ?(?) ?{MI,CC,MIC}.

2: OUTPUT: selected feature subset S.

3: BEGIN 4: Rank all features in F in descending order based on their  relevance to c, i.e., ?(fi, c).

5: Select the top ranked feature f0 as the first feature of the  candidate feature subset S.

6: while |S| < m do 7: Search the top ranked W features in F ?S and identify  the feature fi that optimizes (3).

8: Move fi from F to S.

9: end while  10: END

III. RESULTS The performance of the MRMR feature selection based on  three different criterions, i.e., MIC, MI, and CC is evalu- ated on five real-world omics datasets including Colorectal [10], LiverM [11], Pancreatic [12], Central Nervous System (CNS) [13], Leukemia [14] data. Among them, Colorectal and Pancreatic are Mass Spectrometry Time-Of-Flight (MS- TOF) proteomic data. They are preprocessed according to  TABLE I SUMMARY OF DATASETS  Dataset #Features #Instances #Classes Colorectal [10] 779 112 2  LiverM [11] 114 126 2 Pancreatic [12] 664 181 2  CNS [13] 7129 60 2 Leukemia [14] 7129 72 2  TABLE II AVERAGE CLASSIFICATION ACCURACIES OF MIC, CC, MI ON THE FIVE  DATASETS WITH CLASSIFIER 1NN.

#Selected features MIC MI CC 20 76.76 78.49 75.8 40 79.05 79.69 80.29 60 78.67 78.92 80.61 80 80.43 81.21 80.72 100 82.01 81.27 82.79  Average 79.38 79.92 80.04  [15]. LiverM is a Ion Molecule Reaction MS (IMR-MS) metabolomic data collected in [11]. Leukemia and CNS are two widely used microarray gene expression data. All datasets have binary classes, e.g., disease and normal. The information of the datasets are summarized in Table I.

In our empirical study, the linear forward search method is set to use a fixed window of size 50. The number of selected features are increased from 20 to 100 with internal 20. The classification accuracies of the selected feature subsets are evaluated using Support Vector Machine (SVM) [16] and 1- Nearest-Neighbour (1NN) [17] classifiers. The classification accuracies are reported over ten independent runs of 10-fold cross-validation.

The average classification accuracies and error bars of the MRMR feature selection using different relevance measures on the five omics datasets are depicted in Figs 1-2. The average classification accuracy of using all features is also included as a baseline for comparison. The overall average values and ranks of classification accuracies of MRMR with MIC, MI, and CC are also reported in Tables II-V. MRMR with MI measure could be seen as a equivalent to that of [5], except that a more efficient feature space search method is used in this work. MRMR with MI is competitive to MRMR with CC, but the later performs more robustly. In David N. Reshef?s work [8], MIC was shown to have better discriminative ability than CC and MI for identifying various variable relationship types such as linear, cubic, exponential, sinusoidal, categorical and periodic. Nevertheless MIC applied to the feature selection on omics data is comparable to the CC and MI in terms of classification accuracy. There is no obviously dominant measure on all feature numbers, but the results summarized in Tables II-V show that MRMR with CC measure obtains slightly better average classification accuracy than MI and MIC.

Comparing the classification accuracy of 1NN and SVM, we can see that the feature selection algorithms based on SVM classifier perform better than 1NN classifier. When using 1NN, feature selection helps more to improve classification accuracy  248 2013 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)    0 20 40 60 80 100 120            Number of selected features (a) Colorectal  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )     0 20 40 60 80 100 120         Number of selected features (b) LiverM  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120            Number of selected features (c) Pancreatic  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120             Number of selected features (d) CNS  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120            Number of selected features (e) Leukemia  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      MIC  MI  CC  All Features  Fig. 1. Comparison of feature classification accuracies of MIC, CC, MI, All features using 1NN.

TABLE III AVERAGE CLASSIFICATION ACCURACIES OF MIC, CC, MI ON THE FIVE  DATASETS WITH CLASSIFIER SVM.

#Selected features MIC MI CC 20 79.13 78.27 78.83 40 82.5 80.82 83.25 60 83.18 83.07 85.21 80 84.66 85.53 85.3  100 85.11 87.78 86.1 Average 82.92 83.09 83.74  TABLE IV AVERAGE ACCURACY RANKS OF MIC, CC, MI ON THE FIVE DATASETS  WITH CLASSIFIER 1NN.

#Selected features MIC MI CC 20 2 1.6 2.4 40 2.8 1.8 1.4 60 2.2 2.2 1.6 80 2.6 1.8 1.6 100 1.6 2.4 2  Average 2.24 1.96 1.8  with respect to using all features, which suggests that 1NN is more sensitive to irrelevant/redundant features. SVM is more  TABLE V AVERAGE ACCURACY RANKS OF MIC, CC, MI ON THE FIVE DATASETS  WITH CLASSIFIER SVM.

#Selected features MIC MI CC 20 2 2.2 1.8 40 2.4 2.2 1.4 60 2 2.4 1.6 80 2.4 1.2 2.4 100 3 1.4 1.6  Average 2.36 1.88 1.76  resistant to irrelevant/redundant features for it can manage to obtain better accuracy using all features than using feature selection.



IV. CONCLUSIONS  In this paper, we studied an MRMR feature selection based on three relevance evaluation measures, i.e., MI, CC, and MIC for omics data classification. A linear forward search was also introduced to search the desirable features. The experimental results on five real-world omics datasets demonstrate that MRMR with CC is more reliable on revealing the information  2013 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB) 249    0 20 40 60 80 100 120         Number of selected features (a) Colorectal  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )     0 20 40 60 80 100 120            Number of selected features (b) LiverM  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120         Number of selected features (c) Pancreatic  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120            Number of selected features (d) CNS  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      0 20 40 60 80 100 120         Number of selected features (e) Leukemia  C la  ss ifi  ca tio  n A  cc u  ra cy  ( %  )      MIC  MI  CC  All Features  Fig. 2. Comparison of feature classification accuracies of MIC, CC, MI, All features using SVM.

of feature?s relationship, thereby resulting in better (or com- petitive) classification accuracy than the other two criteria.

