Efficient Learning from Explanation of Prediction Errors in Streaming Data

Abstract?Streaming data from different kinds of sensors contributes to Big Data in a significant way. Recognizing the norms and abnormalities in such spatiotemporal data is a chal- lenging problem. We present a general-purpose biologically- plausible computational model, called SELP, for learning the norms or invariances as features in an unsupervised and online manner from explanations of saliencies or surprises in the data.

Given streaming data, this model runs a relentless cycle of Surprise ? Explain ? Learn ? Predict involving the real external world and its internal model, and hence the name.

The key characteristic of the model is its efficiency, crucial for streaming Big Data applications, which stems from two functionalities exploited at each sampling instant ? it operates on the change in the state of data between consecutive sampling instants as opposed to the entire state of data, and it learns only from surprise or prediction error to update its internal state as opposed to learning from the entire input. The former allows the model to concentrate its computational resources on spatial regions of the data changing most frequently and ignore others, while the latter allows it to concentrate on those instants of time when its prediction is erroneous and ignore others.

The model is implemented in a neural network architecture.

We show the performance of the network in learning and retaining sequences of handwritten numerals. When exposed to natural videos acquired by a camera mounted on a cat?s head, the neurons learn receptive fields resembling simple cells in the primary visual cortex. The model leads to an agent- dependent framework for mining streaming data where the agent interprets and learns from the data in order to update its internal model.

Keywords-surprise; salience; explain; learn; predict; genera- tive model; predictive coding

I. INTRODUCTION  Sensors that stream data round-the-clock are becoming more ubiquitous. Storing all the data and developing algo- rithms that can mine the stored data are highly impractical for such sensors. The biological brain faces a similar prob- lem. Perceptual organs in a biological organism are typically exposed to data in the environment around the clock. The perceptual system learns the norms in such data so that only the salient events may be attended to and learned from. Such character is missing but imperative in smart artificial sensors that generate streaming data.

The biological perceptual system operates as a generative prediction machine [1]?[4]. The brain builds an internal model of the external world using which it thinks [5], [6].

The goal of the generative brain is to explain (or reconstruct) the external input using the causes in the internal model such that the internal model remains grounded to the external world. The goal of the predictive brain is to generate the input for the future instants which allows the agent to act proactively in the external world. If the predictions are correct, explanation of the input at those instants is not necessary, thereby saving valuable computational resources.

If the predictions are incorrect, the internal model is updated to account for those errors through the process of learning.

This idea is referred to as predictive coding.

The perceptual cortices are hierarchically organized with reciprocal connections [7]. It has been proposed that higher- level areas predict lower-level activity while lower areas feedforward the prediction errors, thereby removing the predictable or redundant information in the input [1]. It is claimed that predictive coding is employed in different parts of the brain, such as retina [8], [9], visual pathway [10]?[12], auditory pathway [12]?[16], mirror neuron system [17], and even in multisensory areas [18]. Formally, the objective of predictive coding is to minimize the following error function:  E(X) ? 1 ?X ? X??22 (1)  where X is the input, X? is its prediction and ?.?p denotes the ?p-norm. The input is assumed to have been generated from a sparse linear combination of causes (or features or nonorthogonal bases) where ?sparse? refers to a small subset of causes activated to explain the input (see [19]). If D is the dictionary of causes, reconstruction of the input is X = D? ? where ? are the sparse coefficients. Further, the predicted coefficients in any layer is given by ??(t) = T ? ?(t ? 1) where T is a transition matrix and t denotes time. Then, the objective can be rewritten as:  E(X|D,T ) ? 1 ?X ?X?22 +   ??? ???22 (2)  subject to a sparsity constraint, such as ???0 ? n where n is a small integer compared to the number of causes in D.

The first term in equ. 2 is the reconstruction error minimized by updating D while the second term is the prediction error minimized by updating T . The predicted input at any time t is X? = D ? ??. The goal is to estimate D and T from       streaming data by minimizing E , which can be accomplished in different ways.

Rao and Ballard [1], [20] minimized E by estimating the coefficients, ?, using the Kalman filter at each instant. Their approach was inspired by the minimum description length (MDL) principle that advocates the largest compression of data [21], [22]. Their implementation exhibited unstable convergence behavior which would be detrimental to scaling their model to an arbitrary number of layers. Jehee et al.

[23] used matching pursuit [24] instead of Kalman filter, and showed faster convergence with respect to the MDL metric and the possibility of a straightforward hierarchical implementation. As their objective function, Chalasani and Principe [25] considered the ?1-norm of the second term in equ. 2 and an ?1 sparsity constraint on ?. They observed that, due to the presence of the non-smooth second term, the objective function can be minimized using computationally expensive methods, such as dynamic programming, homo- topy and Bayesian sparse coding. For efficiency purposes, they approximated the non-smooth term and minimized the objective using the fast iterative shrinkage thresholding algorithm [26]. Spratling?s PC/BC model [27] is a nonlinear model of predictive coding where the prediction error is computed by division, instead of subtraction, leading to interesting properties. Friston [28] proposed a general frame- work of predictive coding considering all the higher-order moments in a continuous time dynamic model. None of these models, except Rao?s [20] and Chalasani and Principe?s [25], is capable of learning sparse invariant representations from temporal data (e.g., image sequences) that is helpful for recognition and prediction; rest of the models learn from spatial data and use prediction in space only.

Predictive coding is a very efficient paradigm for learning from streaming sensory data as it learns selectively in time.

It learns only when a prediction error or surprise occurs, unlike traditional learning algorithms that continue to learn from data all the time. However, predictive coding models, like traditional learning algorithms, learn from data in the entire space. Our contribution in this paper is to extend the predictive coding model for learning from spatiotemporal data selectively in time and space. At any instant, our model, called SELP, will operate on input which is the change in the state of data between the current and last sampling instants as opposed to the entire state of data. This will allow the model to concentrate on spatial regions of most significant changes and ignore those with minimal changes, thereby allowing all computational resources to be deployed for learning from a small but most interesting space of the data at any instant. The reconstruction and prediction objectives in equ. 2 will be minimized independently of each other and in online manner. What follows is an agent- dependent framework for mining streaming data where the agent interprets and learns from the data efficiently in order to update its internal model. This article lays out a                                                    Explain  Surprise  Predict  Learn Updated neuronal connections  Activations of lower layer neurons  Activations of higher layer neurons  Expected input  Real-world input (varying in space and time)  Figure 1: The SELP model relentlessly executes this cycle on streaming data. The explanation cycle, that runs on a faster time scale than the prediction cycle, is also shown.

computational framework for learning from streaming data in such an agent, implemented as the SELP model.

The rest of the article is organized as follows. In the next section, we describe the SELP model including the neural architecture and algorithms for learning from space- and time-varying data. In Section III, we present the results from deploying the model to synthesized and real world data.

Finally, we end with conclusions.



II. THE SELP MODEL  SELP is a general-purpose biologically-plausible com- putational model for learning the norms or invariances as features from space- and time-varying data in an unsuper- vised and online manner from saliencies or surprises in the data. Given streaming data, this model learns invariances using four functions ? detect any unexpected or Salient event, Explain the salient event, Learn from its explanation, Predict or expect the future events ? and hence the name.

SELP runs a relentless cycle of Surprise ? Explain ? Learn? Predict involving the real external world and its internal model (see Fig. 1). It samples the environment (i.e.

data stream) at regular intervals of time. To maintain a true correspondence between the data and its internal model, the stimulus obtained at any sampled instant must be explained.

By learning from explanation, the internal model is kept up-to-date with the nonstationary data distribution of the external world. An accurate internal model facilitates correct predictions (or expectations), thereby allowing SELP to act proactively and efficiently in the real world. When an expec- tation is violated, surprise occurs. Thus, the four functions are tightly coupled together. We will implement the SELP model in a two-layered neural network architecture.

A. Neural Architecture  The architecture consists of two of layers of neurons ? the input layer L0 and the first layer L1. The feedforward     connections connect each neuron in L0 to each neuron in L1 (see Fig. 2a). The feedforward weights to each L1 neuron encode a unique spatial pattern as a feature. All feedforward weights together encode the dictionary D of features or causes. An L1 neuron is connected to all the neurons in the same layer by lateral connections (see Fig. 2b). The lateral connections form a first-order directed Markov graph encoding the temporal correlations among the neuronal activations. All lateral weights together encode the transition matrix T . Each L1 neuron is also connected to each L0 neuron using top-down connections (not shown in Fig. 2) that are assumed to reciprocate the feedforward connections [7]. The top-down in conjunction with lateral connections predict the activations of L0 neurons for the next sampling instant.

Input Layer L0  Weights W(0,1)  Layer L1  .  .  . .  .  .

(a) Feedforward connections        Weights W(1,1)  Layer L1           (b) Lateral connections  Figure 2: The neural network architecture used to implement the SELP model.

The L0 neurons sample the data at regular intervals of time. At each sampling instant, the L0 neurons are activated by the difference between the actual and predicted change in state of data. The goal of computations is to build an internal model of the external world, as accurately as possible, i.e.

to estimate D and T by minimizing E in equ. 2. Over time, each L1 neuron gets tuned to a unique feature that, along with other features, can reconstruct many different inputs.

Functionally, L1 neurons embody an overcomplete set of filters, all of which are applied to the input data. These filters (or features or nonorthogonal bases) are the causes using which an input is explained by the model.

At each sampling instant, the SELP model predicts the change in the data. If I(t) is the state of the data at time t, the input to the model at t is:  ?I(t) = I(t)? I(t? 1) (3) The model?s predicted input is ?I?(t) = I?(t) ? I(t ? 1).

Hence, the predicted state of the data I?(t) can be computed given the predicted input and the state of the data at the last time instant.

B. Neuron  Activation at time t of the ith neuron in L1 is:  A (1) i (t) =  ? j  A (0) j (t)?W (0,1)ji (t) (4)  where W (k,?)ji (t) is the weight or strength of connection from the jth neuron in Lk to the ith neuron in L? at time t, and A(0)j is the activation of j  th neuron in L0. In matrix  notation, A(1)i (t) = A (0)(t) ? W (0,1)i (t) where ? is the dot  product operator and W (0,1)i are the feedforward weights leading to the ith neuron in L1. Thus, activation of a L1 neuron denotes the strength of the feature, encoded by the feedforward weights leading to it, in the input.

C. Surprise  A surprise is evoked when the actual input does not match the model?s expected or predicted input. The L0 neurons are activated as a direct response to surprise which is computed as follows:  A(0)(t) = ?I(t)??I?(t) (5) Note that this is the same as I(t)? I?(t), i.e. the difference between the actual and predicted states of data. It has been conjectured that responding to a class of saliency is a basic function of individual neurons [29]?[32]. Meyer and Olson [33] have shown that inferotemporal neurons respond much more strongly to unpredicted transitions than to predicted ones. Gill et al. [30] have shown that auditory neurons encode stimulus surprise as opposed to intensity or intensity changes.

D. Explain  At any sampling instant, the final activation A(1) of L1 neurons is the sum of predicted activations A?(1) and the activations A  (1) required to explain the surprise or  prediction error. In general, explanation may be construed as constructing a story or composite hypothesis that best accounts for the surprise. Here we consider the simplest case where explanation is construed as reconstruction of the surprise A(0) using the learned features and activations A  (1) .

For reconstruction, we minimize the following loss function:  Erecon(A(0)|W (0,1)) ? 1 ?A(0) ?W (0,1) ?A(1)?22 (6)  subject to ?A(1)?0 ? n, where ?A(1)?0 ? #{i : A(1)i ?= 0}, n is a positive integer, and each column of W (0,1) is a feature that has been normalized to have unit norm. The condition on A  (1) constrains the maximum number of features used in  the reconstruction, thereby allowing a way to induce sparsity.

In section III, we will show the difference this sparsity makes in explanation and learning.

If n is greater than or equal to the number of available features (i.e., number of columns in W (0,1)), the L1 neurons? activations may be computed using ordinary least squares in closed form as: A  (1) = (W (0,1)T ?W (0,1))?1 ?W (0,1)T ?  A(0), where MT denotes transpose of matrix M . If n is less than the number of features, reconstruction in the model is     achieved by an iterative process, called the explanation cycle (see Fig. 1), that operates at a faster time scale than the prediction cycle. Algorithmically, the behavior of neurons is similar to matching pursuit [24] which is a greedy forward selection algorithm that includes at each iteration the feature most correlated with the current residual. The algorithm starts with an empty list and the input (i.e., prediction error) as the residual. At each step, the feature for the maximally activated (absolute values of activations are considered) or winner L1 neuron is included in the list, and the residual is updated as the difference between the input and the winner neuron?s feature times its activation (see equ. 4).

Mathematically, the residual R at any iteration h of the explanation cycle is given as:  R(h+ 1) = R(h)? (R(h) ? f)? f (7)  where ? is the dot product operator and f is that feature from W (0,1) which has maximum absolute dot product with R(h).

Initially, R(0) is the input. This procedure continues until n features have been used. From the perspective of neuronal dynamics, this process may be conceived as recurrent lateral excitation and inhibition among neurons that eventually settle for a stable state occurring at a fixed point, as shown in [34], [35]. While in [34], the winner is only chosen ? a case of extreme sparsity, in our model, a sparse set of winner neurons participate in reconstructing the input.

Therefore, the activation A(1) of L1 neurons at time t is:  A(1)(t) =  ??? ??  A?(1)(t), if there is no surprise i.e., A(0)(t) = 0  A?(1)(t) +A (1)  (t), otherwise (8)  If there is no surprise, explanation is not required and the SELP cycle turns fast. If the surprise is large, the explanation might require longer time. Thus, the speed of execution of the SELP cycle is data dependent.

E. Predict  At any sampling instant, the model predicts the activations of L1 neurons for the next instant, as follows:  A?(1)(t+ 1) = W (1,1)(t)?A(1)(t) (9)  where W (1,1) is the transition matrix encoded by the lateral weights in L1. Then, the predicted change in input is:  ?I?(t+ 1) = W (0,1)(t)? A?(1)(t+ 1) (10)  The predicted input may be computed as: I?(t+1) = I(t)+ ?I?(t+ 1), though it is not required to be computed in the SELP model.

F. Learn  There are two sets of weights ? W (0,1) and W (1,1) ? to learn in the SELP model. The former is learned to minimize the reconstruction error while the latter to minimize predic- tion error. Keeping the L1 activations fixed, the feedforward weight update rule is obtained by applying stochastic gradi- ent descent on the reconstruction loss function (equ. 6).

?W (0,1)(t) = ??(A(0)(t)?W (0,1)(t)?A(1)(t))?A(1)(t)T (11)  where ? is the learning rate, 0 < ? < 1. After each update, the feedforward weights to each neuron are normalized to have unit norm. The prediction loss function is:  Epred(A(1)|W (1,1)) ? 1 ?A(1) ? A?(1)?22 (12)  where A?(1) is computed as in equ. 9. The lateral weight update rule, obtained by applying stochastic gradient descent on the prediction loss function, is:  ?W (1,1)(t) = ? ? (A(1)(t)?W (1,1)(t)?A(1)(t? 1)) ?A(1)(t? 1)T (13)  where ? is the learning rate, 0 < ? < 1. After each update, lateral weights to each neuron are normalized to have unit norm. Thus, learning in the SELP model amounts to minimizing the explanation/reconstruction and prediction errors in conjunction. Stochastic gradient descent allows the weights to be updated in an online manner.



III. EXPERIMENTAL RESULTS  Since SELP learns only from changes in input over time, it is a much more efficient model than the other predictive coding models. SELP was deployed for learning to predict from two kinds of data ? a synthesized sequence composed of handwritten numerals, and natural data obtained from a video camera mounted on a cat?s head. The goal of our experimentation with the former data set was to recreate the results in Fig. 10 of [20], thereby showcasing the capability of SELP to learn from prediction error in spatiotemporal data. With the latter data set, our goal was to show that SELP is capable of learning simple cell-like RF properties from natural videos, and qualitatively compare these features with those in Fig. 2(d) of [1]. In the above two experiments, SELP?s performance is at par with other predictive coding models, despite learning from changes in input and hence being much more efficient by design.

The weights were learned with ?(t) = ?(t ? 1)/(1 + t/107), ?(0) = 10?2, and ? = 10?3. Initially, ? is chosen smaller than ? so that the feedforward weights get time to stabilize before temporal correlations between them is learned. Once they are stabilized after a number     of presentations, the value of ? becomes larger than ? to learn lateral weights based on temporal correlations. Both the feedforward and lateral weight matrices were initialized to random values in (0, 1) and each column was normalized to have unit norm.

A. Synthesized data  The MNIST data set [36] contains 60,000 images of ten handwritten numerals {0, 1, ...9}, each of size 28?28 pixels.

The SELP model was shown a sequence of ten numerals ?0, 1, 2, 3, 4, 5, 6, 7, 8, 9?, each randomly chosen from a class in the data set. The data was not preprocessed. There were 282 neurons in L0 and 20 neurons in L1. We simulated the model with no sparsity (i.e., n = 20 in equ. 6) and with high sparsity (only 20% features were used, i.e. n = 4). Features learned by the L1 neurons in both cases are shown in Fig.

3. Qualitatively, the features do not improve much after the sequence has been presented 106 times as compared to 102  presentations. Also, the degree of sparsity does not influence the quality of the features significantly.

However, when the model was used for prediction, the observations were quite different. As shown in Fig. 4, the predictions become significantly stronger when the data was presented longer. This is expected given the nature of learn- ing rates, ? and ?. The model learned more quickly in the non-sparse case compared to the highly sparse case (compare second and fourth rows in Fig. 4). This is because more resources in the form of lateral connections are available in the non-sparse case which can encode more information.

At t=13, when the data presented is ?5? instead of ?2?, the surprise that evoked has not been experienced by the model before. Still it predicts quite well in the non-sparse case after being trained for 102 iterations but fails to do so well after being trained for 106 iterations. This is because, the feedforward weights and temporal correlations become so strongly encoded after 106 iterations that it becomes difficult to respond correctly to an unknown surprise. Finally, the effect of this unknown surprise lingers much longer in the highly sparse case as compared to the non-sparse case, as more resources in the latter allow for stronger encoding which does not alter easily. These results are in accordance with the experiments with non-sparse case in [20].

B. Natural data  The model was deployed for learning visual features from spatiotemporal data in an unsupervised and online manner. The data consisted of 17 videos recorded at different natural locations with a CCD camera mounted on a cat?s head exploring its environment [37]. These videos provided a continuous stream of stimuli similar to what the cat?s visual system is naturally exposed to, preserving its temporal structure. The same catcam videos were used in [34], [38] for evaluating models on learning complex cell receptive field properties. Each frame (320?240 pixels) was converted  Figure 3: Twenty features were learned from a sequence of ten handwritten numerals ?0, 1, 2, 3, 4, 5, 6, 7, 8, 9? from the MNIST data set. The features learned after 102 and 106  iterations are shown in top and bottom rows respectively.

The features learned with no sparsity and high sparsity are shown in left and right columns respectively.

to grayscale. Unlike in [1], the data was not subject to any preprocessing. Spatiotemporal voxels of size 10 ? 10 pixels spanning over the entire duration of a video were extracted at fixed points from a 9?11 grid, sampled every 25 pixels. These 99 voxels from each video formed our stimuli, consisting of 5.3 million patches from the 17 videos. Thirty features learned in the feedforward weights are shown in Fig. 5. Such localized and oriented Gabor-like filters and elongated edge-detectors have been observed in macaque primary visual cortex, and have been reported to be learned by computational models such as SAILnet [39] and SSC [40]. Qualitatively, our features are similar to those learned from natural images by the predictive coding model in [1].



IV. CONCLUSIONS  Since noteworthy events happen only occasionally, it is not necessary to store or analyze the data generated at every instant of time. Rather, it is imperative for a smart sensor to learn the norms in such data so that only the abnormal (or salient) events may be attended to and learned from.

We presented the SELP model that runs a relentless cycle of Surprise ? Explain ? Learn ? Predict involving the external world and its internal model in order to learn norms or invariances from real world spatiotemporal data.

We implemented the four functions in a two-layered neural network leading to interesting behaviors. The feedforward weights learned feature dictionaries with different degrees of sparsity while the lateral weights learned temporal correla- tions in an unsupervised and online manner from space- and     Figure 4: The SELP model was trained with a sequence of ten handwritten numerals ?0, 1, 2, 3, 4, 5, 6, 7, 8, 9? from the MNIST data set. 20 features were learned (shown in Fig. 3). The top (or first) row shows the data at t=1 through 27 consecutive instants. The second and third rows show the prediction of the model after training with no sparsity for 102 and 106 iterations respectively. The fourth and fifth rows show the prediction of the model after training with high sparsity for 102 and 106 iterations respectively. Compare to Fig. 10 in [20].

time-varying data. We deployed the model for learning from synthesized and natural videos with no preprocessing. The model?s behavior in retaining the sequences and responding to unknown surprises varied depending on the degree of sparsity and number of presentations of the same data.

The key characteristic of the model is its efficiency, crucial for streaming Big Data applications. The efficiency stems from two functionalities exploited at each sampling instant ? it operates on the change in the state of data between consecutive sampling instants as opposed to the entire state of data, and it learns only from surprise or prediction error to update its internal state as opposed to learning from the entire input. The former allows the model to concentrate its computational resources on spatial regions of the data changing most frequently and ignore others, while the latter allows it to concentrate on those instants of time when its prediction is erroneous and ignore others. This leads to an agent-dependent framework for mining streaming data where the agent interprets and learns from the data efficiently in order to update its internal model. This article laid out a computational framework for learning from streaming data in such an agent, implemented as the SELP model.

