A Simulation Study on Map/Reduce Framework in Wireless DataCenter Environment

Abstract?Big data has different characteristics from the traditional data. It has large volume and various type and requires real-time processing. To process the big data, cluster based parallel and distributed computing model, Map/Reduce has been introduced. The Map/Reduce is basically designed for the cluster where the processing units are connected to each other with wired network, called data center. However, in the future, the wireless data center would be introduced since it has low installation and management complexity and cost. To introduce the wireless data center, we need to answer the fundamental question, ?How does Map/Reduce framework would operate in the wireless data center?? To answer the question, in this paper, we perform simulation study of the Map/Reduce application in the wireless environment. We introduce the basic simulation model and the results of the simulation study to verify the potential of the wireless data center.



I. INTRODUCTION  With the emergence of the smart phone and social net- working service, a huge volume of data is being created by the end users and the system such as UCC data, SNS feed and comment data of end users and various kind of system log data.

Moreover, with the emergence of the IT convergence, the IT system is being applied to various industries (such as power industry and medical industry), and a lot of raw data is being produced (for example, power metering data and patient data of hospitals). This kind of data is now called big data [1], [2].

The big data can be characterized by three main features - volume, velocity, variety [3], [4]. Volume means the size of the big data. Big data usually has a size of terabytes to petabytes per day. Velocity means that the big data is produced rapidly and requires almost real-time processing. Variety means that the big data can be either structured data (such as metering and log data) or unstructured data (such as user feed data in social networking).

Through the big data, various information can be retrieved.

Power consumption pattern, life style of each user, and social issues can be extracted from the big data. The information is very valuable and important since, it can bring much benefits to the service providers who provides user-specific services.

Therefore, how to process the big data rapidly and precisely is a very important issue. However, the big data has different characteristics from the traditional data. Therefore, the data cannot be processed in a traditional manner in which single server with multi-core CPU handles all the data [5], [6].

????? ????  ???????  ???????  ?  ???????  ??? ??????  ??????????????  ??????????????  ?  ??????????????  ?????? ??????  ????????  ????????  ????????????? ?????????  ????????????  ?????????? ???????????  ?????????? ???????????  ????????? ???????????????????  Fig. 1: Overview of the Map/Reduce operation  Map/Reduce [7] is an outstanding paradigm for han- dling the big data. It is a distributed and parallel processing framework to process the big data efficiently and reliably.

Basically, the model was motivated by the map and reduce function in functional programming (language), and it is applied to the computing environment where the processing units (servers) are tightly clustered. Instead of using super computer which is too much expensive, system operators can process big data efficiently with cheap commercial servers through the Map/Reduce. Hadoop is a representative open- source Map/Reduce framework.

The Map/Reduce framework basically operates on top of TCP/IP protocol stack and is employed in the wired server cluster (data center). However, in the future, the wireless data center would/should be constructed since the wireless network has cheaper installation and management cost. A lot of researches have been conducted and would be conducted to construct the wireless data center network [8], [9]. In such paradigm, we need to consider the basic question, ?How does Map/Reduce framework would operate in the wireless data center??     To answer the question, in this paper, we perform a simulation study of the Map/Reduce framework. We pro- pose the simulation model for the Map/Reduce framework based on the OPNET simulator which is a popular network simulator and verify the operation of the Map/Reduce in the environment of the wireless data center. The IEEE 802.11 based wireless data center is assumed in our simulation study.

This paper is organized as follows. In section II, we shortly introduce the basic concept of the Map/Reduce model and the OPNET network simulator. In section III, we discuss some issues of Map/Reduce in the wireless data center and describe the modeling of the Map/Reduce and its implementation. In section IV, we perform the simulation study with the simulation model which is devised in section III and discuss the results of the simulation study. Finally, we conclude our paper with section V.



II. PRELIMINARY  In this section, we briefly introduce the concept of Map/Reduce and OPNET network simulator.

A. Map/Reduce  Map/Reduce is a simplified data processing model on large clusters. The model has been first proposed by Google [7] and now widely used in many cloud computing frame- work such as Hadoop and nutch. The Map/Reduce model consists of a master and workers. The master is a special user program which splits the job (input file) into pieces and assigns of map and reduce tasks to workers. The map worker performs map operation which reads input data from the pieces, parses the key/value pairs of the input data and generates intermediate key/value pairs of the data. The reduce worker retrieves intermediate file from the map worker and processes the intermediate file to generate output key/value pairs according to user defined function. Fig. 1 represents the overview of the Map/Reduce operation.

Basically, the scheduling problem in the Map/Reduce is treated as a homogeneous task scheduling problem. The input file is divided into pieces of fixed size and assigned to the workers with FIFO scheduling policy. All the tasks are queued and assigned to the workers sequentially. In the Hadoop framework, a worker is scored by the master whenever the worker finishs its task in the deadline and the score is used to determine the target worker [10]. If we assume that each worker has constant and fixed processing time, the scheduling problem can be formulated as follows:  minimize X  T  subject to Ci ?Xi ? T {for all i=1, . . . ,N} N?  n=1  Xi = K  X ? 0  (1)  where N is the number of worker, K is the number of task, C is the processing time of each worker, X is the number of assigned task to each worker, and T is the maximum finish time among all the workers. Since the Map/Reduce  (a) WLAN node model of OPNET  (b) TCP process model of OPNET  Fig. 2: The node and process model of OPNET  is parallel and distributed processing model, all the tasks should be finished before it step into next stage. Therefore, the scheduling of the Map/Reduce tries to minimize the finish time of the last task. The finish time of the last task is denoted by T in Eq. (1).

However, since processing time (including transmission time) of each server shows variation in real system, finding the optimal scheduling of the map and reduce task is difficult. A lot of scheduling algorithm has been proposed to enhance the performance of the Map/Reduce trying to find sub-optimal solution of the scheduling problem [10], [11].

B. OPNET  OPNET is a popular network simulator. It provides so- phisticated GUI to configure and perform network simulations easily. The most outstanding feature of the OPNET simu- lator is a hierarchical representation of the network model.

Node model represents a physical network device, such as a server and router. Each node model consists of process model which represents network protocol [12]. The process model is implemented with Finite State Machine (FSM). Through the FSM, the network system can be modeled precisely and     ?????? ??????  ???????????????  ???????????????????  ?????????? ??????????????  ??????????????  ???????????????  ?????????? ?????????????  ??????????????  Fig. 3: Operation Scenario of Map/Reduce application  unambiguously. Fig. 2 represents the node model of the typical wireless lan enabled work station and the process model of TCP. As shown in Fig. 2, the typical work station has TCP/IP protocol stack with WLAN (IEEE 802.11) MAC. Moreover, it has application protocol process which can be modified by the user to perform simulation of specific application.



III. SIMULATION MODEL OF MAP-REDUCE IN WIRELESS DATA CENTER  In this section, we discuss the issues of the Map/Reduce application in the wireless data center environment. Then, we introduce the simulation model of the Map/Reduce application and the wireless data center. Moreover, we describe the model implementation in the OPNET simulator.

A. Issues of Map/Reduce in Wireless Data Center  In the Map/Reduce software framework has a basic assumption of homogeneity of nodes and tasks. The Hadoop scheduler assumes that all the node have same processing per- formance (homogeneity of node) and the tasks are processed at a constant rate since all the tasks have the same length (homogeneity of task). It means that in the problem (1), Ci (for all i = 1, . . . , N ) is a same constant value, and the FIFO scheduling can guarantee optimal solution. Moreover, the Hadoop scheduler checks the progress of each task, and if the progress of a specific task shows large difference with other tasks, the task is treated as a speculative task. If the scheduler detects a speculative task, it tries to migrate or reassign the task to another worker.

However, in the practical environment, the basic assump- tion of the Hadoop scheduler is broken down. In the general data center, a virtual machine performs a role of Map/Reduce worker. It leads the heterogeneity of nodes and causes a serious impact to the Hadoop scheduler since the heterogeneity would create too many speculative tasks. The speculative tasks can lower the entire performance of the Map/Reduce [10].

In the wireless data center, the performance degradation would be much more serious since the fundamental limitation of the wireless media [13] can intensify the heterogeneity of the node performance. The network performance of the wireless network is lower than wired network in perspective  (a) Process model for master of Map/Reduce application  (b) Process model for worker of Map/Reduce application  Fig. 4: Process model for Map/Reduce application in OPNET  of throughput, delay and jitter. It makes the variation of a processing cost of the Map/Reduce much more difficult to handle. In the problem (1), Ci has much more heterogeneity and time-variant characteristic in wireless data center. The impact of the break down of Map/Reduce scheduling would be much more serious in wireless environment. Therefore, the basic philosophy of Map/Reduce operation should be modified in the wireless data center. In the later section, how much the performance of Map/Reduce can be degraded in wireless data center would be described through the simulation study.

B. Simulation Model of the Map/Reduce Operation  To simulate and verify the impact of wireless data center on Map/Reduce, we model the Map/Reduce operation as a simple application. The Map/Reduce application shows different characteristics compared to the traditional network (simulation) application such as HTTP, FTP, and CBR. It re- quires sequential information exchange between the master and workers. Therefore, in this paper, we simplified the operation of the Map/Reduce application to implement the application as a simulation model. Fig. 3 represents the basic operational scenario of Map/Reduce application.

To perform Map/Reduce application, each processing unit (server) register to master as a worker. The master en- queues the worker to the idle worker list. The master contin- uously checks for the remaining (map or reduce) tasks. If there is a remaining task, then the master dequeues an idle worker     from the idle worker list and assigns the task to the idle worker. If any worker finishes its task, it reports to the master.

During the processing time, each worker periodically reports the progress of its task and the master detects the speculative task. If speculative task is detected, the master reassigns the task to another worker. To detect the speculative task, the Hadoop framework uses a progress score and a speculative task threshold. The progress scores is defined as a ratio of expected processing time and elapsed time ( elapsed timeexpected time ). If a progress score of any task is larger than the average progress score (i.e. (progress score ? average progress score) > speculative task threshold), then the task is detected as a speculative task.

C. Simulation Model Implementation in OPNET  To perform simulation study, we implemented the Map/Reduce application on the OPNET simulator. As we mentioned in section II-B, the application process of OPNET can be modified or implemented to represent a specific appli- cation. Therefore, we implemented Map/Reduce application model as a application process of OPNET simulator. Fig. 4 represents the Map/Reduce application process which we implemented.

The process model of the Map/Reduce master consists of IDLE, Recv Conn Request, Send PKT, Recv PKT states. The master stays in IDLE state and move to state Recv Conn Request when it receives TCP SYN packet from the worker and tries to establish TCP connection with the worker. The Recv PKT state handles the received applica- tion packet such as REGISTER WORKER and JOB FINISH which are defined in section III-B. If the master receives REGISTER WOREKR or JOB FINISH packet, the worker is added to the idle_worker_list. If there exists a remaining job and idle worker, the master enters into the Send PKT state and assign a job to a idle worker.

The worker application consists of OPEN, IDLE, JOB_RECEIVE states. The worker send TCP SYN packet in OPEN state. When the TCP connection is established, the worker send REGISTER WORKER packet to the master and enters into IDLE state. When the worker receives a job assignment, it enters into the JOB_RECEIVE state. The worker stays in the state until the job is finished and it returns to the IDLE state sending the JOB FINISH packet to the master. In the JOB RECEIVE state, the worker periodically reports the progress of the assigned job by employing a timer.

As we mentioned in section II, most Map/Reduce frame- works employs FIFO scheduler. Therefore, we implemented the FIFO scheduler which assigns first task to first registered worker in the process model of the master application. The Map/Reduce application is terminated when all the map and reduce jobs are finished.



IV. SIMULATION STUDY OF MAP-REDUCE IN WIRELESS DATA CENTER  In the previous sections, we discussed the issues of Map/Reduce in wireless data center environment and de- scribed the simulation model of the Map/Reduce applica- tion. Based on the discussion and description, we perform the simulation study to verify the performance issues of the  TABLE I: Simulation Configuration  Scenario 1 Scenario 2  Input Size 800 MB  Map task size 800 KB 80 KB  Reduce task size Exponential(1.5 ? Map task size)  Transmission Rate  11 Mbps  Processing Time Exponential(0.6s) Exponential(0.06s)  Speculative Job Threshold  150%  Map/Reduce application in wireless data center environment.

In our simulation study, we assume that the wireless data center consists of servers which have wireless network capability. Ev- ery server communicates with each other with omnidirectional antenna and a same subchannel of the ISM band.

A. Simulation Configuration  In our simulation study, we assume the wireless data center to consis of 21 processing units (server). 1 processing unit acts as a master and 20 processing units act as workers. Moreover, we employs logical topology where the distance between each server is ignored, since we are interested in protocol operation and we cannot reflect the various physical data center design.

All the servers has IEEE 802.11 WLAN interface and the Map/Reduce application is operated on the top of TCP/IP, IEEE 802.11 WLAN protocol stack.

Table I represents the Map/Reduce application parameters which are employed in simulation study. The wireless data cen- ter forms very dense wireless network, and the dense network can suffer from a lot of collisions. In a general situation, the collision probability is proportional to the number of nodes.

However, in the Map/Reduce application, the probability is proportional to the task size since the number of transmission is determined by the task size. Therefore, the task size can be an important factor of the Map/Reduce application. To in- vestigate the impact of the task size, we define two simulation scenarios with different task size. Moreover, we assume that the mean processing time is the same as the transmission time, therefore, the ideal performance gain is 206 = 3.33 (since data center has 20 processing units, triplication of time per task is required - transmission, processing and transmission, and double number of task is required - map and reduce). The processing time of each job follows exponential distribution, and if the processing time is larger than 150% of its mean value, then the job is treated as a speculative job.

B. Simulation Results  Table II represents the simulation result of each scenario.

The scenario 1 outperformed the scenario 2, however, both scenario has worse results than ideal performance gain 3.33.

From the results, we can notice that in the wireless environ- ment, the configuration of the Map/Reduce application can make important impact on the performance of the application.

Moreover, the portion of false-positive speculative tasks, which are detected as a speculative task but are not speculative task,     0 50 100 150 200 250 300    time  (s)  M a p  E la  p se  d T  im e  (a) Map Elapsed Time of Scenario 1  0 5 10 15 20 25 30  0.2  0.4  0.6  0.8   x  F (x  )      Map Elapsed Time  Exponential Distribution (?=0.6)  (b) CDF of the Map Elapsed Time of Scenario 1  Fig. 5: Map Elapsed Time of Scenario 1  TABLE II: Simulation Results  No Map/Reduce Scenario 1 Scenario 2  Elapsed Time 10m 6m 18s 9m 31s  Performance Gain - 1.58 1.05  False-Positive Speculative Task - 78% 77%  is very high in both scenarios. It means that the network perfor- mance can cause the Map/Reduce performance degradation, since the low performance network not only lengthens the processing cost (processing time+transmission time) but also produces the additional overhead which causes false-positive speculative task.

Fig. 5 represents a map elapsed time (job transmission time + job processing time) and CDF of the map elapsed time of the scenario 1. Also, Fig. 6 represents a reduce elapsed time and CDF of the reduce elapsed time. As we mentioned above, we assumed that the job processing time is exponentially distributed. Therefore, we can notice that the elapsed time follows the exponential distribution. However, we can be also notified that the simulated elapsed time has a little larger value than theoretical exponential distribution value. The difference represents the impact of the network performance, especially larger delay due to the collisions and retransmission.

Fig. 7 shows the map elapsed time of the scenario 2 and Fig. 8 represents the reduce elapsed time and its cdf of the  0 100 200 300 400      time  (s)  R ed  u ce  E la  p se  d T  im e  (a) Reduce Elapsed Time of Scenario 1  0 10 20 30 40  0.2  0.4  0.6  0.8   x  F (x  )      Reduce Elapsed Time  Exponential Distribution (?=1.8)  (b) CDF of the Reduce Elapsed Time of Scenario 1  Fig. 6: Reduce Elapsed Time of Scenario 1  scenario 2. The notable point is that the map and elapsed time is densely distributed around two parts. The concentrated distribution means among the all possible collision and retrans- mission states, two states have higher probability in the steady state. The impact of the collision and retransmission is much more notable in scenario 2 than scenario 1, since scenario 2 has much smaller processing and transmission time. Both scenario 1 and 2 has same backoff time, however, scenario 2 has much smaller processing and transmission time. Therefore, the relative backoff time (backoff time/transmission time) is much larger in scenario 2, and it caused the notable distribution.

?From the simulation results, we can acquire a blueprint for the wireless data center, especially Map/Reduce application in wireless data center. The wireless data center is a very dense network, and the system parameters of the Map/Reduce can make an important impact on the system performance.

Moreover, the wireless data center has high heterogeneity in the perspective of network performance due to the natural limitation of the wireless medium. Therefore, to perform the Map/Reduce in the wireless data center, a scheduling mechanism which considers the high heterogeneity of the network performance should be designed and implemented.



V. CONCLUSION  In this paper, we firstly introduced the issues of Map/Reduce programming model in the wireless data center environment. The wireless data center has different charac- teristics from the wired data center. Especially, because of     0 100 200 300 400 500 600      time  (s)  M a  p E  la p  se d  T im  e  (a) Map Elapsed Time of Scenario 2  0 1 2 3 4 5  0.2  0.4  0.6  0.8   x  F (x  )      Map Elapsed Time  Exponential Distribution (?=0.18)  (b) CDF of the Map Elapsed Time of Scenario 2  Fig. 7: Map Elapsed Time of Scenario 2  the fundamental limitation of the wireless medium, it has heterogeneous transmission time. It can leads the performance degradation of Map/Reduce application. To simulate the degradation, we modeled and implemented the Map/Reduce application into OPNET simulator. The simulation study shows the performance degradation of Map/Reduce application in wireless data center precisely. Through the simulation study, we can acquire an insight to design and construct the wire- less data center and to perform Map/Reduce application in wireless data center.

