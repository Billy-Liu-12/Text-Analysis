Improving Frequent Patterns Mining by LFP

Abstract?Frequent patterns mining is the focused research topic  in association rule analysis. Most of the previous studies adopt  Apriori-like algorithms or lattice-theoretic approaches which  generate-and-test candidates. However, there are extremely  invalidated candidate generations in the exponential search  space. In this paper, we systematically explore the search space of  frequent patterns mining and present a local frequent pruning  (LFP) strategy based on local frequent property.  LFP can be  used in all Apriori-like algorithms. With a little more memory  overhead, proposed pruning strategy can prune invalidated  search space and effectively decrease the total number of  infrequent candidate generation. For effectiveness testing reason,  we optimize MAFIA and SPAM and present the improved  algorithms, MAFIA+ and SPAM+. A comprehensive  performance experiments study shows that LFP can improve  performance by a factor of 10 on small datasets and better than 30% to 50% on reasonably large datasets.

Keywords- data mining, association rule analysis, frequent  patterns mining, pruning strategy.



I. INTRODUCTION  Frequent patterns mining is a task of discovering frequent patterns shared among a large number of objects in a given database [1, 2]. It has attracted considerable attention from database practitioners and researches because of its broad applications in many areas such as analysis of market data, discovering of Web access patterns in Web-log, extraction of Motifs from DNA sequences, etc. Frequent patterns mining is divided into two related sub-problems, which are frequent itemsets mining and frequent sequences mining. Frequent itemsets mining is to find all itemsets that frequently occurred together, while each sequence is an ordered list of itemsets and the order is very important for mining sequential patterns. The search space of frequent patterns mining is quite challenging and its search space is extreme complex and massive. For example, with N different items there are O(2N) potentially frequent itemsets and O(Nk) potentially frequent sequences of length k. So, effective pruning strategy is very important for performance optimization of mining algorithms. Can we find some effective pruning strategies that may help algorithms to generate as fewer infrequent candidates as possible? This is the motivation of this paper.

In this paper, by systematically explore the search space of frequent patterns mining, we present a pruning strategy LFP (Local Frequent Property). LFP can be used in all Apriori-like algorithms. With a little more memory cost, LFP can prune  invalidate search space effectively. Thus, a large number infrequent candidates need not to be generated. In order to test performance improvement, we optimize MAFIA [4] and SPAM [3], which are two fastest algorithms for mining frequent patterns. A comprehensive performance study shows that LFP can improve performance of original algorithms by a factor of 10 on small datasets and better than 30% to 50% for reasonably large datasets.

The rest of the paper is organized as follows. Section 2 introduces the basic concepts related to the frequent patterns mining problem.  Section 3 discusses the related works. The proposed pruning strategy is discussed in section 4. A comprehensive experimental study is presented in Section 5.

Finally, conclusions can be found in section 6.



II. PROBLEM STATEMENT  Let I={i1, i2,?,in} be a set of n distinct items comprising the alphabet. An itemset IX ?  is a non-empty collection of items.

Without loss of generality, we assume that items in an itemset are sorted in lexicographic order and denoted as i1i2?ik. A  sequence s is an ordered list of itemsets, denoted as X1 X2 Xm, where Xi is an itemset. The number of items of an  itemset is called its length. An itemset with length of k is called a k-itemset. Similarly, the number of instances of items in a sequence is called the length of the sequence. Let |Xi| refer to the number of items in itemset Xi, a sequence with length k is  called k-sequence, where k= |Xi|. For example, c ab a is a 4-sequence. The size of a sequence, denoted as size(s), is the  number of itemsets that it contains. For example, if s = X1 X2 Xm, then size(s) is m.

Given two itemsets Xa ? Xb, then Xa is a sub-itemset or sub- pattern of Xb, denoted as Xa ? Xb. A sequence sa= X1 X2  Xm is said to be contained in another sequence sb= Y1 Y2 Yn if and only if exists i1, i2, , im, such that 1  i1<i2< <im n, and X1 ? Yi1, X2 ? Yi2, ,Xm ? Yim. If sa is contained in sb,  then sa is a sub-pattern of sb and sb is a super-  pattern of sa. This relationship is denoted as sa ? sb.

Additionally, pattern Pa=X1X2?Xm is a prefix sub-pattern of Pb=X1X2?XmYm+1?Yn.

A pattern can be extended into each of its super-pattern by adding items. Given a pattern p=X1X2?Xm and an itemset X,  Xp  means p concatenates X. This concatenation is called  prefix pattern extensions. It can be itemset extension, denoted  This work was supported by National Natural Science Foundation of China under grant No. 90612016 and grant No. 60473095.

Corresponding author: MA Zhixin is with School of Information Science and Engineering, Lanzhou University, 730000, China     as Xp i  or sequence extension, denoted as Xp s . For  example, with itemset extensions, ab can extended into abefg, and the extension path is ab  i e  i f  i g. Similarly, with  sequence extension path dcaab iss , itemset ab can also  be extended into a sequence ab a cd.

The database D for frequent pattern mining consists of a collection of input patterns. Each input pattern has a unique identifier called pattern-id (pid) and the pattern. Given a pattern p, if it is a sub-pattern of an input pattern, pattern p is supported by the input pattern. The support count of pattern p, denoted as sup(p), is the total number of input patterns in database D that  support p. The support of p, denoted as )( p? , is the percentage of patterns in D that contain p. Without loss of generality, we use the support count for describing the pruning strategy while using the support to present the experimental results in the remaining of this paper. Given a user-specified threshold min_sup, we say that a pattern p is frequent if sup(p) is greater than or equal to min_sup. A pattern p is frequent and there is no proper super-pattern of p with the same support, i.e., not exists p? such that   'pp ? and sup(p) = sup(p?), we call p a frequent closed pattern. A frequent pattern p is maximal if it is not a sub-pattern of any other frequent pattern. The (closed/maximal) frequent patterns mining problem is to find a complete set of (closed/maximal) frequent patterns for an input database D, with a specified minimum support threshold min_sup.

For example, Fig.1 shows an input databases Di which is used to mining frequent itemsets. Transaction input database Di has 6 items, which are a, b, c, d, e, and f. Suppose minimal threshold min_sup=3. The complete set of all frequent itemset FI = {a:4, b:6, c:4, d:4, e:5, ab:4, ad:3, ae:4, bc:4, bd:4, be:5, ce:3, de:3, abd:3, abe:3, ade:3, bce:3, bde:3, abde:3}, consists of 19 itemsets. All closed frequent itemsets FCI = {b:6,bc:4, bd:4, be:5, abe:4, bce:3, abde:3}. And all maximal frequent itemsets MFI={bce:3, abde:3}.



III. RELATED WORKS  Since the frequent patterns mining problem was presented by R. Agrawal, et al [1, 2], many algorithms have been proposed, such as APRIORI [1], CHARM [10], FP-Growth [5], MAFIA [4], SPADE [9], SPAM [3], GSP [7], CloSpan [8], etc.

Most of these algorithms adopt an Apriori-like method: generates a candidate pattern by extending currently frequent pattern and then test the candidate. During this process, many infrequent patterns are generated. In order to improve the performance, several pruning strategies are proposed, which include Apriori property (AP) [1, 9], Parent Equivalence Pruning (PEP) [4, 8], frequent head union tail (FHUT) [4],  head union tail is a sub-pattern of a maximal frequent pattern (MHUT) [4], and so on. They are described as follows.

AP states that any sub-pattern of a frequent pattern is frequent, too. And any super-pattern of an infrequent pattern is also infrequent. This is a basic pruning strategy, which is used in almost all methods.

PEP pruning search space by comparing support of the parent (current frequent pattern) with that of a tail (item selected to extend parent). If all input patterns that support parent also support the child, this guarantees that any frequent super-pattern p of the parent which not has the child has a frequent super-pattern that contains the child. Thus, the space that has the parent but has no the child can be pruned.

FHUT states that if a tail extends current frequent prefix and get a new frequent pattern, i.e., head unions tail is frequent, we never have to explore any sub-pattern of HUT and thus can prune out the sub-space rooted at head.

MHUT is similar with HUT. It says that if head unions tail is a sub-pattern of a known frequent pattern, any sub-pattern of HUT is also frequent. Thus this sub-space can pruned out.

All pruning strategies can improve performance effectively.

Unfortunately, they are complex and couple tightly with their enumerating methods and data structures, and unlike our LFP strategy, they can not be shared by several algorithms.

A. Algorithm MAFIA and SPAM  MAFIA [4] and SPAM [3] are two fastest algorithms to mine frequent patterns, which are taken as examples for demonstrating the proposed pruning strategy and comparing performance. This section describes them in more details.

MAFIA was proposed by J. Ayres, et al. It can find closed, maximal or frequent itemsets in a given input transaction database. From current node C, each candidate item is used to extend the node. If a new frequent itemset is gotten, MAFIA recursively goes on this depth first traversal. During the frequent itemsets enumeration process, MAFIA adopts strategy PEP, FHUT and MFUT to prune out invalidate candidates.

SPAM algorithm is based on the lexicographic sequence tree and can make either depth or width first traversal. Taking  current frequent sequent node n=(s1 sk), S-step extension candidate items set Sn and I-step extension candidate items set In as input parameters, SPAM generates a new sequence by either S-step or I-step. And this process recursively goes on until no more extension can be performed. When traversing the lexicographic sequence enumeration tree, SPAM uses S-step and I-step pruning strategies to optimize the performance.



IV. LFP PRUNING STRATEGY  In this section, we introduce the local frequent property pruning strategy, and demonstrate how to optimize MAFIA and SPAM with this strategy.

A. Pattern  extension item set  In lexicographic enumeration space [4], each node has a corresponding pattern. A pattern is generated from its parent by  Figure 1. Example input pattern database  Input database Di  1,    a  b  d  e  f  2,    b  c   e  3,    a  b  d  e  4,    a  b  c  e  f  5,    a  b  c  d  e  6,    b  c  d  Input database Ds  1,  a b c d e  2,  a bc d  3,  a b e  4,  dc e  5,  b bc d e     one pattern extension. If a pattern is infrequent, all patterns in its sub-space must be infrequent, too. Thus, only extensions that start from frequent nodes may generate frequent patterns.

Each frequent pattern has a candidate extension item set, which includes any items that may generate a new frequent pattern by pattern extension. In all Apriori-like algorithms, if some invalidated items in candidate sets can be pruned out, their performance can be effectively improved.

B. Proposed local frequent property strategy (LFP)  Lexicographic enumeration tree describes the search space of Apriori-like methods. If the number of invalidate nodes is decreased, the performance of these methods may be effectively improved. Local frequent property (LFP) is such a strategy to prune out infrequent nodes.

LEMMA 1: Given a frequent pattern p and its candidate extension item set C. Suppose item a is the last item of pattern  p. For each item b in C and any pattern p?, if ba  is infrequent, pattern bpp ' must not be frequent.

Proof: Suppose pattern bpp ' is frequent. According  to Apriori property, any sub-pattern of bpp ' must be  frequent. Specially, sub-pattern ba  is frequent too. This conflicts with that ba  is infrequent. Thus pattern  bpp ' is infrequent.

Local property pruning strategy (abbr. LFP): Given a frequent pattern p and its pattern-extension candidate set C.

Item a is the last item of p and item b is a candidate. If ba  is infrequent, item b can be pruned out. The reason is that any pattern bpp ' must be infrequent. Additionally, for any  pattern p* whose has p as a prefix, the candidate set C* of p* must be sub-set of C.

For example, consider the input database Ds in Fig.1 and let  min_sup = 2. Suppose the current node is ( b) and its candidate set is {b, c, d, e}. Without pruning, possible sequence  extensions are (a b b), (a b c), (a b d) and (a b e).

Because (b b) is infrequent, sequence ( b b) must be infrequent and can be pruned out.

Fig.2 shows pseudo-code of LFP strategy. Parameter a is the item to prune extension candidate set Ci, and ext is the pattern extension type (itemset extension or sequence extension). Line 3 ~ 5 delete all candidate items, with which frequent 2-pattern can not be generated, from candidate set.

C. Use LFP to improve MAFIA and SPAM  In order to test performance improvement of local frequent pruning strategy, we optimize algorithms MAFIA and SPAM.

Fig.3 describes the optimized algorithms, named MAFIA+ and  SPAM+, which also show that strategy LFP can be shared by all Apriori-like methods. Comparing with MAFIA and SPAM, there are only three additional lines of code (line 7 in MAFIA+, line 2 and 8 in SPAM+), which call LFP strategy. All other codes are as same as that in MAFIA and SPAM respectively.

Both MAFIA+ and SPAM+ are all divided into two steps: generating frequent 2-patterns and recursively generating frequent k-patterns (k>2). Frequent 2-patterns are gotten by one step of frequent pattern-extensions and stored in a list. In the second step, MAFIA+ and SPAM+ recursively enumerating candidates by sharing frequent 2-pattern list.

Note that frequent 2-patterns, which are used by LFP strategy, are not generated in above optimized algorithms. Each 2-pattern can be generated by one pattern-extension. Having generated all frequent 2-patterns, MAFIA+ and SPAM+ start their recursive searching procedures from level two nodes in lexicographic enumeration tree, while MAFIA and SPAM start from root node (level 0).



V. PERFORMANCE STUDY  To test the performance improvement of LFP strategy, an extensive set of experiments were performed upon an Intel Pentium 4 CPU 1.6GHz PC with 256MB main memory, running Microsoft Windows 2003 server. Source codes of MAFIA and SPAM were downloaded from Himalaya Data Mining Tools (http://sourceforge.net/projects/himalaya-tools/) and modified with proposed strategy. Same as MAFIA and SPAM, all synthetic datasets are downloaded from FIMI?04 dataset repository or generated by using the IBM AssocGen program. The experiments include two parts: performance  1) DFS-SPAM+(node n = (s1 sk), CSn, CIn, ia)  2)  Sn?= LFP(ia, CSn, s-extension);  3)  For each (i in Sn?)  4)       If ((s1 sk i) is frequent )  5)           Stmp = Stmp  {i};  6)   For each (i in Stemp)  7)     DFS-SPAM ((s1 sk i), Stmp, items in Stmp greater than i, i);  8)   In?=LFP(ia, CIn, i-extension);  9)   For each (i in In?)  10)     If ((s1 sk i) is frequent)  11)          Itmp = Itmp  {i};  12)  For each (i in Itmp)  13)   DFS-SPAMSPIP ((s1 ski), Stmp, items in Itmp greater than i, i);  14) END of DFS-SPAM+;  (b) Pseudo-code of SPAM+ algorithm  1) MAFIA+ (C, MFI, IsHUT)  2)    HUT = C.pattern U C.candidate;  3)    If HUT is in MFI Return;  4)    For each item i in C.freq_candidate {  5)       IsHUT = whether i is the first item in the tail  6)       newNode = C U {i};  7)       newNode.candidate = LFP(i, C. freq_candidate, i-extension);  8)       MAFIA(newNode, MFI, IsHUT) }  9)       If (IsHUT and all extensions are frequent)  10)           Stop search and go back up subtree;  11)       If (C is a leaf and C.pattern is not in MFI)  12)           Add C.pattern to MFI;  13) End of MAFIA+;  (a) Pseudo-code of MAFIA+ algorithm  Figure 3. Pseudo-code of MAFIA+ and SPAM+  1) LFP(a, Ci, ext)  2) C = {};  3) For each i in Ci Do  4)       If pattern a ext i is frequent Then  5)             C = C U {i};  6) Return C; Figure 2. Pseudo-code of LFP strategy     comparison with FHUT, MHUT and PEP upon primary MAFIA algorithm for mining frequent itemsets, and performance comparing with and without LFP strategy upon SPAM for mining frequent sequences.

A. Performance comparison with strategies in MAFIA  In this section we study performance improvement of strategy LFP by comparing it with other strategies adopted by MAFIA, which are PEP, FHUT and MFUT. The chosen datasets include T40I10D100K and connect4. T40I10D100K is a sparse dataset. Connect4 is a dense datasets and has much more long maximal patterns. For each test, only one strategy is effective while others are turned off. In this way we can investigate the impact of different strategies on the running time of algorithm MAFIA. Fig.4 shows the experiment results, which clearly depicts that on sparse datasets strategy LFP is better than FHUT and MHUT, performance increased by a factor of 30%. On dense datasets LFP is similar as FHUT and MHUT, but not as fast as PEP. The reasons are on sparse datasets, frequent 2-patterns are only a small percent of all 2- itemsets, thus LFP can prune out infrequent pattern-extensions effectively. When datasets are dense, almost all 2-itemsets are frequent ones, only a few of invalidate itemsets can be pruned.

B. Performance comparison with SPAM  We next study the impact of LFP on mining frequent sequences. Firstly, a set of experiments were performed for studying the performance of the SPAM+ by comparing it with SPAM. The generated datasets include one small database (Fig.5 (a)), two medium-sized ones (Fig.5 (b) and (c)) and a large one (Fig.5 (d)). The experiments show that SPAM+ outperforms SPAM by a factor of about 10 on small datasets.

Although, there is no distinct magnitude difference between SPAM+ and SPAM on reasonably large datasets, the running time of SPAM+ is decreased by 30% to 50%. In small sequence databases, frequent 2-sequences are only a small portion. When SPAM+ traverses the enumeration space, a mass of infrequent extensions are pruned. On the other side, in large database, a majority of 2-sequences are frequent ones. As SPAM+ traverses the enumeration space, only few branches are pruned out. This is the primary reason why LFP strategy performs effectively in small databases while not so effectively in large ones.



VI. CONCLUSION  With systemically exploring the searching space of frequent pattern mining, a new pruning strategy is proposed, which is  local frequent property (LFP) and can used in all Apriori-like algorithms and lattice-theoretic approaches. With a little more memory overhead, proposed pruning strategy can prune out invalidated search space and effectively decrease the total number of infrequent candidate generating. For effectiveness testing reason, we optimized MAFIA and SPAM by using proposed pruning strategy and presented the improved algorithms, MAFIA+ and SPAM+. A comprehensive performance experiments study shows that LFP strategy can improve performance of SPAM by a factor of 10 on small sparse datasets and better than 30% to 50% on reasonably large datasets.

