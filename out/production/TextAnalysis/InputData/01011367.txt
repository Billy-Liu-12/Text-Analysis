Mining Diabetes Database With Decision Trees and Association Rules

Abstract Searching for new rules and new knowledge in problem areas, where very little or almost  none previous knowledge is present, can be a very long and demanding process. In our research we addressed the problem of finding new knowledge in the form of rules in the diabetes database using a combination of decision trees and association rules. The first question we wanted to answer was, if there are significant differences in sets of rules both approaches produce, and how rules, produced by decision trees behave, after being a subject of filtering and reduction, normally used in association rule approaches. In order to accomplish that, we had to make some modifications to both the decision tree approach and association rule approach. From the first results we can conclude, that the sets of rules, built by decision trees are much smaller than the sets created by association rules. We could also establish, that filtering and reduction did not effect the rules derived from decision trees in the same scale as association rules.

1. Introduction Searching for new rules and new knowledge in problem areas, where very little or almost  none previous knowledge is present, can be a very long and demanding process. A great help can be sets of similar already solved cases from the filed. The size of such set which is at researcher's disposal, can vary from very few to immense number of cases. The results of research often rely on expertise, intuition, 'detective' capabilities and even pure luck of researchers working on the problem. A great leap forward in search for knowledge can be achieved by using data mining techniques, such as association rules or decision trees.

Association rules are more appropriate when we are searching for completely new rules, where decision trees are more convenient in 'goal oriented' cases, where we are looking for rules with fixed consequence or outcome, like some specific diagnosis, decision, or some other action.

In our research we addressed the problem of finding new knowledge in the form of rules in the diabetes database using a combination of decision trees and association rules. In order to accomplish that, we had to make some modifications to both the decision tree approach and association rule approach. The first question we wanted to answer was, if there are significant differences in sets of rules both approaches produce, and how rules, produced by decision trees behave, after being a subject of filtering and reduction, normally used in association rule approaches.

2. Diabetes database Diabetes database was collected in Osaka Medical College Hospital. The original database  had 1251 cases, each described by 60 attributes. After due consideration, the data set was preprocessed and the original number of attributes was reduced to 22; 4 continuous attributes and 18 discrete attributes.

Proceedings of the 15 th IEEE Symposium on Computer-Based Medical Systems (CBMS 2002)    To satisfy the demands of both association rules and decision trees, we had to use discretization techniques in order to map the values of 4 continuous attributes into a discrete space. In case of association rules, we used the same discretization for every continuous attribute as in each case of decision trees. These techniques can be divided into three groups: equidistant discretization, threshold discretization and dynamic discretization. The use of different techniques is also a reason, why we got different number of rules on the data set with the same consequence.

3. Association rule mining Association rule mining is one of the popular methods in data mining. The following is a  formal statement of the problem [1]. Let },...,{ 21 miiiI = be a set of items and D  be a set of transactions, where each transaction T  is a set of items such that IT ? . An association rule is an implication of the form YX ? , where IX ? , IY ? , and ?=? YX . The rule  YX ?  holds in the transaction with confidence c if c% of transactions in D  that contain X also contain Y . The rule YX ? has support s in the transaction set D  if s% of transactions in D  contain YX ? . Given a set of transactions D , the problem of mining association rules is to generate all association rules that have certain user-specified minimum support (minsup) and confidence (minconf).

3.1. Filtering redundant rules  Association rule mining often produces a huge number of rules that are very difficult to be analyzed by users. We introduce simple two filtering methods.

Filter 1:                  XYYYR m ?,...,,: 21                               )1(: mkXYR kk ??? Let R be an association rule that has m )2( ?m  attributes in the antecedent. Let Rk be an  association rule that as the same consequent and the kth attribute )1( mk ??  in the antecedent of R. Then, we adopt all rules that satisfy the following condition.

MinconfRConRConf k ??< )()( ?  where ?  is a threshold given by user.

Filter 2:                  XYYYR mm ?,...,,: 21                               XYYYYR mmm ?++ 1211 ,,...,,: Let Rm be an association rule that has m (m>=1) attributes in the antecedent and Rm+1 be an  association rule that has one additional attribute than Rm. Then we adopt the rule Rm+1 only if confidence of Rm+1 greater equal than that of Rm.

3.2. Rule pruning  When Filter 2 is applied to the rules that derived from association rule mining and a Rule R is eliminated, it is assured that at least one of the ancestor rules (that have less attributes in antecedent than the R) remain in the results. The reason is that if R has greater support than minsup, all ancestor rules of R also have greater support than minsup. And since R is eliminated by Filter 2, one of the ancestor rules has greater confidence than R. However, when Filter 2 is applied to the rules that derived from decision tree, it is not assured that one of the ancestor rules remain in the results. Therefore applying Filter 2 to the rules from decision tree is not reasonable.

Instead of applying Filter 2, we introduce the following rule pruning method.

Rule Pruning:          XYYYR kk ?,...,,: 21                                XYYYYR mkm ?,...,,...,,: 21  Proceedings of the 15 th IEEE Symposium on Computer-Based Medical Systems (CBMS 2002)    Let mR be an association rule that has m attributes in the antecedent and Rk )1( mk <? be an association rule that has less attribute than Rm (ancestor rule of Rm). If Rk has greater confidence than that of Rm , replace Rm with Rk.

If same rules are produced, such the rules are made into one rule. If same rules are produced, such the rules are made into one rule. This reduces the number of rules as well as making rules simpler.

4. Decision trees Inductive inference is the process of moving from concrete examples to general models,  where the goal is to learn how to classify objects by analysing a set of already solved cases whose classes are known. Instances are typically represented as attribute-value vectors.

Learning input consists of a set of such vectors, each belonging to a known class, and the output consists of a mapping from attribute values to classes. This mapping should accurately classify both the given instances and other unseen instances.

A decision tree [2, 3] is a formalism for expressing mappings from attribute values to classes and consists of tests or attribute nodes linked to two or more subtrees and leafs or decision nodes labelled with a class which indicates the decision. Because of the very simple representation of accumulated knowledge they also give us the explanation of the decision, and that is essential in medical applications.

The tool we used is called MtDeciT2.0. It basically follows the same principles as many other decision tree building tools, but it also implements different extensions [4, 5]. One of those extensions is called dynamic discretization of continuous attributes, which was used in our experiments with success.

4.1. Dynamic discretization of continuous attributes  Because of the nature of decision trees, all continuous attributes must be mapped into a set of discrete values.

In MtDeciT 2.0 tool we implemented an algorithm for finding subintervals [5], where we consider the distribution of training objects and where there are more than two subintervals possible. The approach is called dynamic discretization of continuous attributes, since the subintervals are determined dynamically during the process of building the decision tree. This technique first splits the interval into many subintervals, so that every training object?s value has its own subinterval. In the second step it merges together smaller subintervals that are labelled with the same outcome into larger subintervals. In comparison to other approaches the dynamic discretization returns more ?natural? subintervals, which results in better and smaller decision trees.

In general we differentiate between two types of dynamic discretization: General dynamic discretization, and Nodal dynamic discretization.

General dynamic discretization uses all available training objects for the definition of subintervals. That is why we perform the general dynamic discretization before we start building the decision tree. All the subintervals of all attributes are memorised in order to be used later in the process of building the decision tree.

Nodal dynamic discretization performs the definition of subintervals for all continuous attributes that are available in the current node of the decision tree. Only those training objects, that came in the current node are used for setting the subintervals of the continuous attributes.

Proceedings of the 15 th IEEE Symposium on Computer-Based Medical Systems (CBMS 2002)    4.2. Modifications of decision trees  To be able of making comparisons between decision trees and association rules, we had to make a few modifications to classic decision tree approaches.

? First we had to produce a set of rules instead of a decision tree. Originally a decision tree tool produces a tree in the form as seen in the figure 1.

Figure 1: A sample decision tree  With a simple recursive walk through this decision tree, we get a set of rules (figure 2), which represent exactly the same knowledge as source decision tree. On this set of rules we can perform filtering and/or pruning, as on any other original association rule set.

Figure 2: Set of rules, derived from a sample decision tree  ? The second modification was the selection of training set. In building decision trees it is a common practice to split the data set into training and testing set. This can be done manually or automatically by using n-fold cross evaluation.  To be able to make comparison with association rules, which use the entire data set as basis for rule search, we had to build our decision trees using the entire data set.

? The third difference to classical decision trees was the selection of outcome or consequence.

Usually we deal with only one possible discrete outcome. But when searching for new knowledge in a form of association rules, this limitation would narrow down the search space in such way, that the use of decision trees would not be justified. To avoid this limitation, we decided to build decision trees for every single candidate attribute from the set of all reasonable outcomes. The set of attributes has been selected by the experts from the clinical field of diabetes. Each single attribute from this set could represent an interesting consequence of the rule.

5. Testing  The tests we describe in the following sections are only the first insight into comparison of decision trees and association rules. What we observed was the number of rules derived from different approaches. These rules were than further filtered and reduced.

Neuropathy(Autonomic) | |____[No]  NoNoNoNo | |____[Yes]  YesYesYesYes | |____[Unknown]  Hypertension | |____[No]  DiseaseDuration |    | |    |____[1.0 .. 21.5]  Age |    |    | |    |    |____[8.0 .. 76.0]  NoNoNoNo |    |    | |    |    |____[76.0 .. 100.0]  YesYesYesYes |    | |    |____[21.5 .. 65.0]  YesYesYesYes | |____[Yes]  YesYesYesYes  1. IFIFIFIF Neuropathy(Autonomic) ISISISIS [No] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [No]  2. IFIFIFIF Neuropathy(Autonomic) ISISISIS [Yes] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [Yes]  3. IFIFIFIF Neuropathy(Autonomic) ISISISIS [Unknown] ANDANDANDAND Hypertension ISISISIS [No] ANDANDANDAND DiseaseDuration ISISISIS  [1.0 .. 21.5] ANDANDANDAND Age ISISISIS [8.0 .. 76.0] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [No]  4. IFIFIFIF Neuropathy(Autonomic) ISISISIS [Unknown] ANDANDANDAND Hypertension ISISISIS [No] ANDANDANDAND DiseaseDuration ISISISIS  [1.0 .. 21.5] ANDANDANDAND Age ISISISIS [76.0 .. 100.0] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [Yes]  5. IFIFIFIF Neuropathy(Autonomic) ISISISIS [Unknown] ANDANDANDAND Hypertension ISISISIS [No] ANDANDANDAND DiseaseDuration ISISISIS [21.5 .. 65.0] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [Yes]  6. IFIFIFIF Neuropathy(Autonomic) ISISISIS [Unknown] ANDANDANDAND Hypertension ISISISIS [Yes] THENTHENTHENTHEN Neuropathy(Peripheral) ISISISIS [Yes]  Proceedings of the 15 th IEEE Symposium on Computer-Based Medical Systems (CBMS 2002)    From the 22 attributes, the following 5 were selected as possible targets: ? AngiopathLimbs ? CerebralAngiopathy ? Nephropathy ? Neuropathy(Autonomic) ? Neuropathy(Peripheral)  When one of these attributes was selected as a target (for instance AngiopathLimbs), the remaining 21attributes were used as input or description attributes.

5.1. Decision trees  Focusing on selected target attributes, we have tried to build compact, generalising decision trees with high accuracy, sensitivity and specificity on the training set. Varying the value for prepruning tolerance, using different types of discretization methods, and varying parameters of discretization methods were the tools we have used in building decision trees. The overall accuracy of the selected decision trees varied form 65,5% in case of target attribute Nephropathy, to 99,9% in case of target attribute Neuropathy(Autonomic).

These decision trees were then transformed in the set of rules, which were then a subject for further filtering and reduction, using the same approach as for association rules.

5.2. Association rules  For comparison with decision trees, only those rules were selected, where the consequence attribute was from the set of preselected 5 attributes. The rule size was limited only to rules with less than five attributes.

6. First results  In table 1 there you can see how many rules were produced using different approaches and different settings. The minimum confidence for both decision trees and association rules was set to 90%, the minimum support for association rules was 20%.

Table 1: Number of rules per approach  Decision Trees Association Rules nf f1-90 pr pr-f1-90 nf f1-90 f2 f2-f1-90 pr pr-f1-90  AngiopathLimbs 5% DC20-2  55 19 52 24 2417 292 1288 216 1262 210  CerebralAngiopathy 2% TC  94 35 87 37 6739 1698 3667 1232 3422 1159  4% DC20-2 55 22 52 26 2474 449 1190 285 1169 280 4% TC 43 23 41 25 3942 847 2167 618 2066 597  Nephropathy 40% DC40-2  171 142 171 142 0 0 0 0 0 0  40% TC 6 1 6 1 0 0 0 0 0 0 Neuropathy(Autonomic)  6% DC10-2 119 66 117 92 1391 955 968 814 942 795  Neuropathy(Peripheral) 15% DC40-2  19 8 19 19 67 67 55 55 55 55  25% DC40-2 6 1 6 1 58 58 50 50 50 50 25% TC 6 1 6 1 59 59 51 51 51 51  We divided the sets of rules according to the target attribute (column 1). In the first column there are also settings of the selected decision trees. The first number is the prepruning  Proceedings of the 15 th IEEE Symposium on Computer-Based Medical Systems (CBMS 2002)    tolerance, followed by the type of discretization method  (TC for threshold, DC for nodal dynamic discretization). In case of nodal dynamic discretization, further numbers represent the discretization tolerance and factor z [5].

The following abbreviations are also used in the table 1.

? nf: (no filtering: including the rule that has less confidence than minimum confidence) ? f1-90: (Filter 1: ? = 90%) ? pr: pruning ? pr-f1-90: (pruning + f1-90) ? f2: (Filter 2) ? f2-f1-90: (Filter 2 + f1-90)  7. Discussion and conclusion As we see in table 1, we have built different decision trees and converted them in sets of rules.

Those rules were than further reduced and filtered with approaches, normally used in association rule approaches. Our goal was to find out, how many rules will be generated, how many rules will remain after the filtering and reduction and how many rules will be generated, using association rules approach on the same database.

To be honest, we did not expect such large differences in the number of rules. As you can see in table 1, the sets of rules, built by decision trees were much smaller than results of association rules. We can also establish, that filtering and reduction did not effect the rules derived from decision trees in the same scale as association rules.

From this we can conclude, that decision trees represent their knowledge in more compact and optimised way. Even after reduction and filtering, the number of rules ratios  between approaches varied from 1:3 to 1:24.

There was only one exception. In case of Nephropathy the association rules generated no rules within the limitations (rule size <=4, minimum support 20%, minimum confidence 90%).

Whether the limitations were to hard or there is no useful knowledge for that target attribute, is to be established in further research.

The results of the first comparison opened some more questions. Why are the numbers of rules so different for both approaches? Is quantity a guarantee for quality? Is it worth using reduction and filtering on decision tree rules? Which rules contain better knowledge? Which rules actually make sense to an expert from the field of diabetes? Is there any new knowledge contained in those rules?

To answer them, we have a lot of work to do. We will have to perform numerous different tests on different data sets, to get insight into the real reasons for the differences we discovered.

Only than we hope, we will be able to tell, which approach is more suitable for data mining.

8. References [1] Agrawal, R., Imielinski, T., and Swami, A. ?Mining association rules between sets of items in large database,?  In Proc. of the ACM SIGMOD Int?l Conf. on Management of Data, pp. 207-216, 1993.

[2] J. R. Quinlan: C4.5: Programs for machine learning. Morgan Kaufmann publishers, San Mateo, CA, 1993.

[3] Stuart J. Russel, Peter Norvig, et al.: Artificial intelligence: a modern approach. Englewood cliffs, Prentice-Hall  (1995): 525-562.

[4] Zorman Milan, Hleb ?pela, ?progar Matej: Advanced tool for building decision trees MtDeciT 2.0. In:  USA. Las Vegas: CSREA, (1999), book. 1: 315-318.

[5] Zorman Milan, Kokol Peter. Dynamic discretization of continuous attributes for building decision trees. In: Fyfe C. (ed.). Proceedings of the second ICSC symposium on engineering of intelligent systems, June 27-30, 2000, University of Paisley, Scotland, U.K.: EIS 2000. Wetaskiwin; Z?rich: ICSC Academic Press, (2000): 252-257.

