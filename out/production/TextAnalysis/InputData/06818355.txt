User-Driven Refinement of Imprecise Queries Bahar Qarabaqi

Abstract?With the advent of big data everywhere, the need for new techniques for exploratory search in large databases is magnified. The focus of this work is on less technical users who often query a database through a limited interface. We propose user-driven query refinement as an interactive process to aid users create and refine query conditions. This process is characterized by three key challenges: (1) dealing with incomplete and imprecise query conditions, (2) keeping user effort low, and (3) guaranteeing interactive system response time. We address the first two challenges with a probability-based framework that guides the user to the most important query conditions. To recover from input errors, we introduce the notion of sensitivity and propose efficient algorithms for identifying the most sensitive user-specified query conditions, i.e., those conditions that had the greatest influence on the query results. For the third challenge, we develop techniques to estimate the required probabilities within a given hard realtime limit.



I. INTRODUCTION  Our goal is to make databases more accessible to a broad spectrum of users, ranging from SQL experts to non-technical users querying them through form-based interfaces. While an expert with full SQL access will usually find her way around a database, less technical users working through limited interfaces often have difficulty finding what they are looking for. Hence, for databases to be relevant for these users, they have to provide functionality for exploratory search.

Consider a large crowd-sourced database of bird observa- tions. Each record has attributes describing the properties of the bird (e.g., species, size) and the observation event (e.g., location, habitat features). An organization such as the Cornell Lab of Ornithology would like to leverage this database to help casual bird watchers identify the species of a bird they observed. Consider user Amy who is sure about some attributes, e.g., her GPS device reliably recorded location. She does not know others, e.g., the bird?s belly color. Then there will be many more attributes whose values she recalls with varying degrees of uncertainty, e.g., she thinks the bird?s wing was mostly red, but maybe it was more of a brownish tone.

Assume Amy initially only fills in values she is certain about. Unfortunately, these are not selective enough and she receives an overwhelming list of possible matches, where none of the top-10 species displayed looks like the one she observed.

Trying to narrow the search further, she has to decide which of the less certain attribute values to enter. Without help from the database, this is a tedious process of trial-and-error. If she selects only ?red? for wing color, but the species is actually rufous, the correct answer might be eliminated. If she selects too many possible colors, only few of the irrelevant records might be eliminated. Or maybe the wing color selection does not even matter much.

Similar issues arise in many other applications. In general, our work targets the challenge of helping a database user fine-tune selection conditions for exploratory analysis. In these applications the query is not pre-determined, but the goal is to find a query that returns a result that is ?good enough?.

We make several contributions aimed at improving support for exploratory search in databases. First, exploratory search usually involves uncertainty; not only in the data (which moti- vated probabilistic database research [1]) but also in the query.

To deal with uncertainty in user-provided query conditions, we propose a probability-based framework that makes this uncertainty explicit. Hence, ranking of result records based on their probability is inherent in exploratory search. Our second contribution helps the user judge the potential risk of specifying a condition she is not very confident about.

Specifying such a condition might provide useful information for improving the query result. However, if getting it just slightly wrong might significantly change the result, then it might be safer to not enter it at all. To provide this kind of risk-estimation functionality, we introduce the novel notion of sensitivity of a condition and prove structural properties that allow its efficient computation. While sensitivity allows the user to quantify the risk of a query constraint, our third contribution aims at quantifying the benefit of a constraint by identifying the best new conditions to be added in order to improve result quality. As a user-driven process, query refinement should be interactive. Unfortunately, computation time tends to be high when dealing with large databases and imprecise queries and data. We therefore propose fast approxi- mate estimation techniques that guarantee to deliver results within a given response time threshold.

The following sections present an overview of our ap- proach [2].



II. DATA MODEL AND FRAMEWORK  We introduce the data model and propose a probability- based framework for exploratory search in databases.

A. Data Model  The database manages some entities of interest, each of them described by a set of attributes {X1, X2, . . . , Xm} and an entity-identifier Y . For simplicity and without loss of generality, assume that data is stored in a relational view R with schema {X1, X2, . . . , Xm, Y }. (Y does not need to be a key of R.) In the bird example, the entities of interest are the different bird species. Attribute Y is the species name. The Xi describe various properties of a bird and observation event, e.g., hasWingColorRed and obsLongitude.

B. Probabilistic Query Framework  The user wants to find entities y ? Y of interest and expresses her preferences by specifying constraints on some of the attributes Xi. This corresponds to query SELECT Y FROM R WHERE condition(X1, X2, . . . , Xm) in a relational database. To incorporate uncertainty, our framework supports constraints that are probability distributions in the condition.

More precisely, for some attribute Xi, let Ai be the set of all possible probability distributions over the values in the domain of Xi. When interacting with the system, the user specifies some probability distribution ai ? Ai.

Given such probability distributions for some of the Xi, we support exploratory search by doing the following:  1) Estimate for each entity y ? Y the probability that it is what the user is looking for and present a list of the top-ranked entities based on these probabilities.

2) Estimate the sensitivity of each Xi for which the user specified a distribution ai and present these Xi and their sensitivity scores in decreasing order.

3) Estimate how much each of the remaining unspecified Xj would help improve the query result quality if the user provided a distribution aj for it. Present these attributes Xj and their quality-improvement score in decreasing order to the user.

Entity Probability: Given data set D and assuming the user specified distributions a1, . . . , ak for attributes X1, . . . , Xk, the desired probability for y ? Y is  Pr(Y = y |A1 = a1, . . . , Ak = ak, D),  written more compactly as Pr(Y |A1, . . . , Ak, D). Seeing pos- sible results, the user might explicitly reject some of them. This also affects the entity probability. Let y?j denote that entity yj was rejected and assume the user rejected entities y1, . . . , yl.

The entity probability then becomes  Pr(Y |A1, . . . , Ak, y?1, . . . , y?l, D).



III. RESULT RANKING  Given an imprecise query, we want to present a ranked list of entities. Due to the probabilistic nature of the query conditions, each entity y ? Y is a query result with probability Pr(Y |A1, . . . , Ak, y?1, . . . , y?l, D). We can use it to simply rank by entity probability.

We also propose ranking by effort-adjusted entity prob- ability, which takes user effort into account. It is moti- vated by the fact that the user has to invest some time to look at each of the top-ranked entities to decide which of them are of interest. Let ?j denote the user effort required for deciding about the relevance of entity yj . The effort- adjusted probability of entity yi ? Y is defined as Pr(Y = yi |A1, . . . , Ak, y?1, . . . , y?l, D)/?i with the following property: (proof omitted due to space constraints)  Lemma 1: Assume the user is looking for a single entity of interest by exploring the ranked list of entities one-by-one from top to bottom, until this entity is found. Expected user effort then is minimized if the entities are ranked in decreasing order of their effort-adjusted probability.



IV. SENSITIVITY ANALYSIS  We propose the notion of sensitivity of a user-provided condition for an attribute Xi. Intuitively, user response ai for attribute Xi has high sensitivity, if the current result ranking would change ?significantly? if the user were to change her current answer ?a little?. To illustrate the use of sensitivity analysis, suppose that Amy can exclude some values of an observed attribute with certainty, but is not certain about its exact value. If the system tells her that any alternative input in the limited space of probability distributions, would result in only minor changes of the species ranking, she can be con- fident about having entered the distribution information, even if her probability numbers are a bit ?off?. On the other hand, if even minor changes, could dramatically change the species ranking, then adding this information is very risky. Sensitivity complements other approaches that identify possibly incorrect input by looking for unusual values or outliers [3], [4], [5].

A. Definition and Naive Algorithm  The sensitivity of user-provided condition ai for attribute Xi depends on how much a change of ai would affect the current result ranking. Let Lp and Lq be two entity rankings.

For each entity y ? Y , let ?p(y) and ?q(y) denote y?s rank in the first and second ranking, respectively. To measure how different the rank of each entity is between the two rankings, we use the popular Minkowski distance:  dst(Lp, Lq) =  ??? y?Y |?p(y)? ?q(y)|d  ??1/d .

Definition 1: Let Lp be the current entity ranking and X  be an attribute for which the user has specified a condition. Let A denote the set of all alternative conditions the user considers for attribute X . The sensitivity of the current ranking Lp to attribute X for a set of possible conditions A is defined as the maximum difference dst(Lp, Lq) between Lp and any other ranking Lq that could be obtained if the user were to change the current condition for attribute X to any other value in A.

Naive algorithm for computing sensitivity: When comput- ing the sensitivity of attribute Xi, try every single condition in Ai: compute the entity ranking based on this modified condition to find the ranking with the maximum distance from the current ranking Lp.

B. Efficient Sensitivity Algorithm  We proved that the maximum ranking difference can only be achieved by a distribution ?on the edge? of the space of possible alternative inputs considered: (The proof is based on structural properties of the ranking distance and is omitted due to space constraints.)  Theorem 1: Let p be the current user input for attribute X . Let q and r be two other possible responses from A, such that q = p+ ? ? (r? p) for some 0 < ? < 1. And let Lp, Lq , and Lr, respectively, denote the rankings obtained for these responses (while keeping all other responses constant). Then, if categories are ranked based on their probability or effort- adjusted probability, it holds that dst(Lp, Lq) ? dst(Lp, Lr).

Fig. 1. Ranking distance for a 3- valued attribute and current distribu- tion (0.4, 0.2)  Fig. 2. Ranking distance for a binary attribute and current distribution (0.6)  Efficient algorithm: Based on Theorem 1, when searching for the distribution that maximizes the ranking distance to the current ranking, we only need to consider the ?boundaries? of the data space. This often eliminates the vast majority of possible distributions in A, dramatically reducing the cost of sensitivity analysis compared to the naive algorithm. For instance, to compute the sensitivity of a 3-valued attribute, the naive algorithm would have to consider the entire colored triangle in Figure 1. Thanks to Theorem 1, we only need to consider the triangle edges. The implications of Theorem 1 are particularly powerful for binary attributes where we only need to check the rankings for the lower and upper extreme of the range of possible probability values considered. (See Figure 2)

V. RECOMMENDING ADDITIONAL CONDITIONS  The goal is to recommend currently unspecified attributes that have the greatest potential for improving result quality. In- tuitively, we estimate how beneficial each currently unspecified attribute is for improving the result quality. Since the user?s future input a for attribute X is not known, we model it as a random variable A. We can estimate the probability that A takes on a specific value a from all the information gathered so far, i.e.,  Pr(A = a |A1, . . . , Ak, y?1, . . . , y?l, D).

For each value a, we can then compute  Pr(Y |A = a,A1, . . . , Ak, y?1, . . . , y?l, D)  and use it to create an entity ranking as discussed in Section III.

Assume we have a function F that returns by how much the entity ranking improves with the additional information A = a, compared to the current ranking without knowing A. Using this function F , we can then compute the expected improvement in entity ranking over random variable A. While entropy and other purity measures are widely used to measure how well ?winning? cases are separated from ?losing? ones and are intuitive candidates for function F , they do not help the user decide if adding another condition is ?worth the effort?. If it costs the user an effort of ?X to decide about and then specify a condition for attribute X , then this investment should result in savings of future effort of at least ?X . We measure these future savings in terms of the reduction of expected effort for going through the ranked list of entities.



VI. PROBABILITY ESTIMATION  We make use of Pr(Y = y |A1, . . . , Ak, y?1, . . . , y?l, D) and Pr(A = a |A1, . . . , Ak, y?1, . . . , y?l, D) in our methods. In principle, we can leverage the vast body of previous work on classification and prediction techniques to estimate these probabilities [6], [7], [8], [9]. We can in theory leverage almost any classification technique using the following basic approach for estimating Pr(Y |A1, . . . , Ak, y?1, . . . , y?l, D): Using data set D, we train a classification model M(X1, X2, . . . , Xk) that predicts the probability of each entity y ? Y ?{y1, . . . , yl} for a given input vector (x1, x2, . . . , xk) ? X1 ?X2 ? ? ? ? ?Xk.

Then we sample a sufficiently large number of points from the joint distribution over A1, . . . , Ak. For each such vector (x1, x2, . . . , xk), M(x1, x2, . . . , xk) returns the desired prob- abilities. We return the average of these probabilities for each entity. The same approach can also be used for estimating Pr(A |A1, . . . , Ak, y?1, . . . , y?l, D).

While conceptually straightforward, making the above probability-estimation algorithm work in practice is very dif- ficult due to the requirement of guaranteeing interactive re- sponse time.

A. Solution: Bagged Tree Ensembles  We propose a solution based on bagged decision tree ensembles [10] due to several reasons. First, trees can handle any attribute type and missing values. Bagged trees are robust against noise and overfitting and have been shown to return ex- cellent probabilities ?out-of-the-box? [8]. Second, due to their structure of splitting on an attribute at a time, trees can easily compute expectations like Pr(Y |A1, . . . , Ak, y?1, . . . , y?l, D) in a single pass through the tree. This is achieved by partitioning the ?weight? according to the potentially imprecise user- provided distribution or the training data distribution (when the split attribute value has not been specified at all). Hence there is no need to sample from A1, . . . , Ak. Third, a tree trained for input (X1, . . . , Xm) can be used to make predictions for any subset of these attributes. And fourth, the comparably simple index-like structure makes tree cost predictable and tuneable based on the number of nodes accessed. This is crucial for guaranteeing interactive response time.

In addition to predictability, we can tune bagged tree access cost very accurately as well. Assume the ensemble consists of T trees, each with at most L levels. By using only t < T of these trees or limiting access to the top l < L levels of each tree, we can reduce cost proportional to the reduction in the number of nodes accessed. As the user reveals more attribute value distributions, the initial limits for t and l can be increased. Since this tree ensemble is capable of adapting to the time threshold, we refer to it as an Adaptive Tree. The term Full Tree refers to the full bagged ensemble with t = T and l = L.



VII. EXPERIMENTS  In our experiments we are using a data set provided by the Cornell Lab of Ornithology. It contains 2 million observations of 372 common bird species in North America. Each record was generated as follows. First, an observation record is sampled uniformly at random from the real eBird data set [11], a data set containing actual observations reported by citizen     0 200 400 600 800 1000         Number of Samples  M ax  im um  D is  ta nc  e      Efficient Sensitivity Analyzer Naive Sensitivity Analyzer  Fig. 3. Naive vs. efficient sensitivity estimation algorithm  0 50 100 150 200     Number of Specified Attributes  Ti m  e (s  )      Full Tree Adaptive Tree  Fig. 4. Response time: Uncertainty=0  0 50 100 150 200     Number of Specified Attributes  Ti m  e (s  )      Full Tree Adaptive Tree  Fig. 5. Response time: Uncertainty=0.5  scientists. Unfortunately, while eBird reports the species of the observed bird, it does not contain individual bird properties like color, size, etc. These individual features were added by sampling from a feature distribution that was carefully defined by the domain experts for every single species.

A. Sensitivity Analysis  In this experiment we explore how much difference it makes to use the efficient algorithm for sensitivity estimation instead of the naive one. For each attribute, we randomly select 1,000 sample points and plot the maximum ranking distance found versus the number of samples explored so far. The goal is to find a good estimation of the maximum distance with as few samples as possible. Figure 3 shows this experiment for attribute ?size?, which has 9 values. After sampling only 27 points, the efficient algorithm has already reached a ceiling, while the naive one keeps improving with more samples. Still, even after 1000 samples for the naive algorithm, the maximum distance found by the efficient algorithm is 78% greater.

B. Additional Condition Recommendation  In this experiment we explore how well our condition rec- ommendation framework improves the entity ranking quality.

We compare the Full Tree (F) to the Adaptive Tree (A) to explore the effect of potentially less accurate probability esti- mates in the Adaptive Tree. The other competitor is Random (R) which picks a random attribute.

In Table I the first three columns report ?(y), the rank of the true entity. The second and third sets of three columns report the probability of the true entity and the expected user effort, respectively. The first row shows the initial values based on the data set D only. The values are updated as conditions are added one by one in the next rows. The experiment is repeated for varying degrees of uncertainty in the user response. Uncertainty value x means that with probability x the user will specify an imprecise constraint. The value of x in Table I is 0.5. (Other uncertainty values show the same trend and are not presented due to space constraints.) The Full and the Adaptive Trees clearly outperform the Random approach, providing strong evidence that a careful selection of attributes has a great impact on the query refinement process. Although the same number of conditions are specified in the Random approach, it can at best only slightly improve the initial rank.

TABLE I. RANK OF THE ENTITY INITIALLY AT POSITION 100 AS VALUES FOR MORE ATTRIBUTES ARE SPECIFIED. (PROBABILITY OF  IMPRECISE INPUT: 0.5)  ?(y) Pr(Y |A1, . . . , Ak, D) Expected Effort F A R F A R F A R  100 100 100 0.002 0.002 0.002 38.1 38.1 38.1 75 76 100 0.003 0.003 0.002 35.3 35.3 39.1 54 54 100 0.006 0.006 0.002 30.6 30.7 40.1 42 42 100 0.008 0.008 0.002 30.6 30.6 41.1 26 26 100 0.012 0.012 0.002 24.2 26.5 42.1 19 19 100 0.020 0.020 0.002 20.6 21.1 43.1 9 17 100 0.031 0.020 0.002 18.9 21.8 44.1 4 17 100 0.062 0.020 0.002 18.4 22.0 45.1 3 18 100 0.125 0.020 0.002 16.8 21.3 46.1 3 17 100 0.125 0.020 0.002 17.6 21.4 47.1 3 16 100 0.125 0.020 0.002 17.4 21.6 48.1  C. Interactive Response Time  An important feature of our framework is to guarantee interactive response times. In this experiment we discuss representative results, comparing an Adaptive Tree to a Full Tree. Bagged tree ensembles (represented by the Full Tree results here) have been shown to provide excellent probability estimates, but might be too slow for large data sets.

We set the response time threshold to 5 seconds. Figures 4 and 5 show how system response time varies as more user- provided conditions are added with varying degrees of uncer- tainty. The x-axis reports the number of attributes specified so far and the y-axis reports the system response time. The experiment shows that the Adaptive Tree virtually guarantees our system to respond within the set realtime limit, no matter how uncertain the user provided data is. It initially tracks the 5 sec threshold and at some point mirrors the behavior of the Full Tree when it converged to full size as user input results in pruning of tree branches.



VIII. FUTURE WORK  Fast and accurate probability estimation is central to the interactive query refinement problem, therefore the main thrust of our future work is to identify other probability estimation approaches. Another direction for future work is to make the methods scalable to big crowd-sourced data.

