DDSN: Duplicate Detection to Reduce Both Storage and Bandwidth Consumption

Abstract?As highly centralized storage facilities are gaining popularity, duplicate detection becomes a critical problem.

Traditional methods focus on reducing the storage space consumption; however, for network storage system with remote clients, the network overhead cannot be ignored, especially when the system is accessed over WAN. We propose a new duplicate detection method and implement a network file system prototype called DDSN based on this new method. It can reach the same performance in terms of storage space consumption as the state-of-the-art sliding blocking method.

Meanwhile, our method overcomes its drawback that the whole file needs to be transmitted over the network, and therefore saves massive bandwidth for duplicate data. Experiments confirm the effectiveness of the proposed method.

Keywords: Duplicate Detection, Network File System

I. INTRODUCTION  Duplicate detection methods aim to identify the duplicates or near-duplicates from massive data, usually in the scenario of highly centralized physical storage facilities like cloud storage. Despite the ever-growing capacity of hard disks and the processing speed of network switches, there is still a significant benefit of having duplicate detection methods deployed. They can contribute to reducing the consumption of storage space and network bandwidth, particularly for the storage of archival data, software distributions, electronic mails, etc.

Recently, many duplicate detection methods have been proposed. The whole file content hashing algorithm [1] can identify completely identical files. To detect replicas in smaller granularities, the fixed-size blocking method [2], [9] divides the file into fixed-size data blocks and stores the same block only once. However, the insertion or dele- tion of even a small portion of the file at the beginning would cause a shift in the file content and therefore render this method inapplicable. To overcome this drawback, the content chunking method [3], [4] uses variable-sized data chunks and content-aware boundaries. The sliding blocking algorithm [6], which is derived from rsync [7], a well-known remote file synchronization algorithm, is also insensitive to  ? Corresponding Author  the modification position by sliding a fixed-size window to identify every overlapping segment.

The aforementioned methods focus on reducing the stor- age consumption. In a remote storage system, however, reducing network bandwidth consumption is also highly important, especially when the system is deployed over a wide area network (WAN). Based on [6] and [10], the sliding blocking method is able to detect more duplicate data than the other algorithms, and is therefore the best performing method in the literature with respect to storage consumption.

But it does not reduce data transmission over networks. In contrast, the other methods can save network bandwidth to various degrees, but have worse performance in terms of storage consumption.

We propose a new duplicate detection method for net- worked file systems, and implement a prototype called DDSN based on it to evaluate its performance. DDSN is comparable to the state-of-the-art sliding blocking method in terms of storage consumption, and overcomes its major drawback which requires the whole file to be transmitted over the network. As such, DDSN enjoys the best of both worlds, achieving better performance with respect to storage and bandwidth consumption at the same time.

To exploit cross-file similarities, DDSN divides files into fixed-size blocks to find the shared portions. Despite up- loading or updating, clients would partition files in their local caches into non-overlapping fixed-size mini-blocks and calculate a ?rolling? checksum for each mini-block.

Afterwards, these checksums are transmitted to the server.

Some bytes at the beginning of each mini-block, called floating bytes, are also transmitted. At the server, data are organized by fixed-size blocks while files are presented by the tree structure consisting of pointers. Each block is made up of eight mini-blocks, which owns a ?rolling? checksum as the key and a short address as the value. These key- value pairs are maintained as the index in the memory. If eight mini-blocks? checksums of the server could correspond to eight sequential checksums from clients and their values could compose one whole block address, we can assert that this block exists at the server and know its exact block      address. Moreover, a range of new ?rolling? checksums can be computed at the server with floating bytes to detect more mini-blocks that are possible matches. In the meantime, the interval of the index for mini-blocks at the server equals to the size of floating bytes. Therefore, we can detect the duplicate block which slides even only one byte.

The rest of this paper is organized as follows. Section II introduces existing duplicate detection methods. Section III proposes a new perspective that considers both storage and network. Section IV presents the design of DDSN. Exper- imental evaluation is presented in Section V. Section VI provides an overview of related work. Section VII concludes this paper.



II. DUPLICATE DETECTION METHODS  Four different methods are frequently used to detect duplicate data in a storage system.

A. Whole File Content Hashing  The whole file content hashing method is the simplest one to detect duplicate data at the granularity of file. Using this method, every file owns a signature. This signature is a collision-resistant cryptographic hash value of the entire content of the common store file. SIS [1] is an implementa- tion of this method. This component explores the content of whole files and forges links with semantics of copies rather than stores the same content several times.

B. Fixed Size Blocking  The fixed size blocking method can find the identical data in smaller granularity. This algorithm divides every file into non-overlapping fixed-sized blocks. The SHA-1 or MD5 value of each block is computed and maintained as metadata. When subsequent blocks come, their hash values are calculated and looked up in the metadata to detect the duplicates. The major problem with this method is that it is very sensitive to the modification position. If an original file exists and a small number of bytes are inserted or deleted at the beginning of the derived file, all the blocks? boundaries would be re-measured, and these new blocks have exactly distinct signatures. None of the blocks of the derived file would be detected.

C. Content Chunking  To overcome the drawback of the fixed size blocking method, a variable-sized chunking method based on file con- tent is proposed. It means that the definition of boundaries, or the so-called breakpoints, lies in data content rather than data position. For example, we can divide the text file into lines according to the breakpoint wrap character instead of the fixed count of characters. Apparently, wrap characters could not be used for all types of files. Rabin fingerprint [8] is usually used to locate chunk breakpoints. It is efficient to compute fingerprints on a sliding window. When the  low-order n bits of the sliding window?s fingerprint match a constant value, this window constitutes a breakpoint; otherwise, this window rolls along the file and continues to compute. The portion between two breakpoints is defined as one chunk. Under the assumption that file contents are completely random, the expected chunk size is 2n bytes.

Using this method, inserting or deleting a small number of bytes, affects only one or two chunks. The remaining chunks survive and are expected to be detected as duplication data.

D. Sliding Blocking  The sliding blocking method is inspired by the famous remote file synchronization algorithm, rsync. Like the fixed size blocking algorithm, this method also divides every file into non-overlapping fixed-sized blocks, but it maintains a weak ?rolling? checksum and a strong checksum for each block. The weak checksum allows legal collision; however, the combination of weak and strong checksums should be unique to identify each block. When new file comes, a block-sized sliding window would be used to calculate the checksum of every overlapping block-sized segment of the new file. If the weak ?rolling? checksum of the sliding window exists in the metadata of storage system, the strong one would be calculated to compare again. If the strong one equals too, we can assert that a duplicate block is found.

Otherwise the sliding window rolls one byte along the new file and continues to compute. This iterative computation requires that the weak ?rolling? checksum be fast to calculate over a sliding window besides being uniformly distributed.

The segment between two matched blocks is regarded as new data and stored in the storage system.

IBM uses this method for email storage [6]. In practice, users do not look up old emails very often, but all emails must be saved in case of occasional requests. Moreover, many emails have similar or identical contents and attach- ments. The sliding blocking method performs prominently in this scenario.

E. Performance Comparison  Since the performance of duplicate detection methods is sensitive to choice of data sets, only the same real-world data sets could reflect the difference. [6] chooses six data sets, including emails and personal files, to evaluate the three cross-file duplicate detection methods. The result is that the simple fixed size blocking, content chunking, and sliding blocking methods detect 12.6%, 17.1%, and 26.8% of duplicate data respectively. [10] also makes an experiment about the effectiveness of these basic methods. Web pages, source files and individual files make up its data sets. This time three methods find out 61.3%, 69.0%, and 73.1% duplicate data respectively.

Beyond these figures, we would like to stress the dis- tinction of these cross-file duplicate detection methods. The fixed size blocking method is sensitive to the modification     position of a file. It just works for the blocks which are before the point of modification. The content chunking and sliding blocking methods can solve this problem. The mod- ification usually affects the surrounding chunks or blocks.

The content chunking algorithm uses variable-sized chunks while the sliding blocking algorithm chooses fixed-sized blocks. Assuming that modification happens randomly in files, big chunks would be affected more easily than small chunks. In other words, the amount of affected data is larger than that of fixed-sized methods. The sliding blocking algorithm thus could get a better result than the content chunking method.



III. TARGETING BOTH STORAGE AND NETWORK  Most of above methods just emphasize the performance of reducing the storage consumption. As a network file system, bandwidth usage is also a significant factor. If the storage system runs over WAN, the transmission of mass data usually dominates the bandwidth usage and causes unacceptable delays. To save bandwidth, LBFS [3] uses a large, persistent file cache at each client. LBFS assumes that clients have enough cache space to contain a user?s entire working set of files. In the LBFS?s model, if the client uploads a file in its cache, the server would try to find as many identical chunks as possible. At last, only unmatched data would be transmitted to the server. Similarly, when a client reads a new version of the file to overwrite the existing one in its cache, the server also only sends the changed chunks. Today, this model is gaining more and more popularity. Even some so-called cloud storage applications, like Dropbox, iCloud, and SugarSync, adopt this model.

The duplicate detection methods should be reviewed under the economical perspective with consideration given to both storage and network. The whole file content hashing, fixed size blocking, and sliding blocking methods are all proposed for the low-bandwidth model. The hash value of every file, block or chunk is transmitted to detect duplicate data firstly and only unmatched data would be transmitted over the network at last. In this manner, they can save bandwidth to various degrees. But the sliding blocking method, which has the best performance on reducing storage space, can not save any bandwidth because that the ?rolling? checksum needs every byte of the file. Therefore, we would like to propose a new method that has the same storage performance with the sliding blocking algorithm, and better capability of cutting down the network transmission cost.



IV. DESIGN  DDSN is designed as a network file system providing traditional file system semantics, while saving the storage space and the bandwidth usage between clients and sever.

Just as in LBFS[3], DDSN assumes that every client has its own disk cache space to hold a user?s full working set of files. At the server end, an entire logical storage device,  maybe implemented by clusters or RAID, offers storage space. This architecture means that the file is stored as a whole at clients but partitioned at the server end. One tree structure, derived from Venti, represents the file content using block pointers. When two files have the same blocks, the relative pointers in their trees would have the same addresses. Therefore, we can reduce the amount of data by storing the duplicate data once.

DDSN uses close-to-open consistency. Once users add a new file or a modified file into their caches, clients would communicate with the server immediately to maintain the file consistency. If this file is shared by other clients, these clients also need to update their versions to keep the file consistency. Therefore, users always have the new content.

Despite uploading or updating, some metadata of the file would be transmitted to the server firstly to distinguish those blocks which exist at the server. Only the new data would be transmitted between clients and the server.

A. Indexing  The index is the most important part for any de- duplication storage system. DDSN needs to index all the files at the server end to detect duplicate data to avoid the waste of storage space and bandwidth. In DDSN, files are organized by some fixed-size blocks and fragments.

The fixed size blocking method provides a straightforward way to index fixed-size blocks. For example, Venti uses the SHA-1 algorithm to generate 160-bit hash values to identify each data block. The shortcoming of the fixed size blocking method is very obvious. This method is very sensitive to the modification positions of files, since it only indexes the blocks which slide by the fixed size. The sliding blocking method, which is the front-runner on reducing the consump- tion of storage space, uses a similar indexing method, but it requires the entire new file at the server. We propose a new indexing mechanism to overcome the drawbacks of the above methods. Refraining from transmitting the whole file over the network, we can only make use of some digest information, such as block signatures and file signatures, to detect replicas. Therefore, we must index every fixed-size block at the server that even slides by only one byte, in order to get the same storage space consumption performance of the sliding blocking method.

To quantize the parameters and for ease of exposition, we assume that the block size is 8 KBytes and the server end uses 32-bit (4-byte) addresses. Apart from this, the 160-bit (20-byte) SHA-1 hash value is regarded as the key of the block address. Formula 1 presents the computation of index size. Suppose that we index all the fixed-size blocks in- cluding the overlapping ones. It means that SlidingLength equals one byte. Therefore, the index size is about 20-24 times as large as the data size. Of course such a large index is too cumbersome, so we attempt to shrink it.

IndexSize = SizeofHashItem?NumberofBlocks = SizeofHashItem? TotalDataSize  SlidingLength  (1)  We divide the 8-KByte block into 8 identical mini-blocks and number them from 0 to 7 in sequence. Each mini- block has its own 16-bit (2-byte) short address. In these 16 bits, the first three bits imply the mini-block number while the rest bits are the real physical address. Figure 1 presents how these 16-bit mini-block addresses compose one 32-bit block address. Besides everything else, each mini- block has a 32-bit ?rolling? checksum, derived from the rsync and the sliding blocking method, as its key. This 32-bit checksum is not strong enough and allows legal collision; therefore, our new index is constituted by <key, set (the mini-block address)>. In the sequel, we prove that the combination of weak checksums and the sequence of mini- blocks can uniquely identify the block. According to the previous method of computation, the size of the new index is around 2 times as large as the data size. Although this index size is much smaller than the previous one, it is still too large. In DDSN, we try to reduce the index size further to place it into memory for accelerating the lookup.

32 bits  16 bits  Figure 1: Block and Mini-block Addresses  B. Duplicate Detection  When the user puts in the cache a new file or saves the existing file after modification, the duplicate detection algorithm starts execution. The file is divided into mini- blocks to compute the ?rolling? checksums. These check- sums are transmitted to the server immediately. The server gets the first set of the mini-block addresses according to the first checksum and the second set depending on the second checksum. Then a join connection is executed on these two sets. Figure 1 illustrates this join operation: the mini-block addresses in the second set must follow those in the first set by the sequential mini-block number and have nine identical bits with the following. If eight sequential mini-block addresses can be joined successfully, we can get the whole block physical address. Otherwise, this process would repeat and the second checksum is chosen as the connection instead of the first one.

The eight sequential mini-blocks can start with any mini- block. Assuming that the first mini-block is number 3, so the order of eight mini-blocks is 3, 4,...1, 2. The block address must be composed by the order from 0 to 7 and the number 3 is kept as the offset within the block. In fact the addresses of mini-blocks from number 0 to 5 can constitute the whole block address enough according to Figure 1. In the meantime, the counters for two adjacent blocks would both increase by one.

Some particulars must be mentioned here. When this com- position involves two adjacent blocks, the block address of the latter is larger than that of the former by one. This change would affect the join connection and the address composition slightly. The join connection between the number 7 mini- block and the number 0 mini-block needs new a rule. This new rule allows the nine bits of the former mini-block to increase by one because of carry and then equals those aligned bits of the latter mini-block. In the same manner, it would happen in the address composition involving two adjacent blocks.

C. Hash Collision One main challenge for de-duplication storage systems is  to ensure no hash collision. It means that the probability of two objects having the same hash value is lower than the probability of hardware bit errors [3]. Our DDSN uses two-stage verification: the coalition of eight sequential mini- blocks? checksums producing the identifier for a block as the first stage, and the verification of SHA-1 value of each block the second.

Assuming r-bit random ?rolling? checksums with a uni- form distribution and a set of n different data blocks, the probability p that there will be one or more collisions is computed by the Formula 2. According to the duplicate detection method of DDSN, we get the set of the mini- block addresses according to the first checksum firstly. Then we choose the next set and execute a join connection which requires the sequential number and nine identical bits. Finally this match step would repeat seven times and contribute to the whole address.

p ? (n? 1 2r  )? ((n? 1 2r  )? ( 1  ? 1  ))7 (2)  We consider the 32-bit ?rolling? checksum for the 1- TByte hard disk, which could contain 230 1-KByte mini- blocks. The hash collision probability for the composition of the block address is no more than 1/2100, so we deem that this method produces the identifier for each block.

In addition, the second stage is developed to solve the mistakes in the first step. These mistakes come from two ways mainly: hash collisions and the mismatching of check- sums in the memory and data on the hard disks. Mindless modification, hardware bit errors and many reasons could lead to mismatching. When DDSN composes a block ad- dress, a 160-bit block signature, using SHA-1 algorithm,     is calculated and sent to clients. If it equals to the one calculated at clients, we can declare that this block is found successfully. Otherwise, the block content would be transmitted without any duplicate detection method.

D. Optimization of Index  To make a further reduction on the index size, we have to decrease the density of the index. Now we index the mini-blocks with an interval of 256 bytes instead of every one byte. As a result, the index size shrinks to 1/256 of the former and we can put the index into memory with success. Because this interval is smaller than the size of mini-block, these checksums are also overlapping. In this way, the number of matched blocks decreases sharply. In order to get the best performance of de-duplication storage, we have to sacrifice some bandwidth. The first 256 bytes and the checksum of every 1KB mini-block would be transmitted from clients to the server end. We call these 256 bytes the floating bytes, which are illustrated in Figure 2. Therefore, we can know the ?rolling? checksums of the fixed-size mini- blocks which slide from A0 to A1, A4 to A5 and A8 to A9. Assuming that X1, X2, X3 and X4 are indexes for the duplicate data at the server end, since they are built with an interval of the number of floating bytes, the duplicate mini- block of the new file must be detected by one of these four indexes, as X3 in Figure 2. If eight sequential mini-blocks are detected and joined, the whole block is founded out with success. Otherwise, the floating bytes would be stored as a part of new file later. We do not need to transmit them again.

In brief, DDSN has the same performance of de-duplication storage with the sliding blocking method and saves 75% of duplicate data over the network in this case.

Figure 2: Floating Bytes and Index  The number of floating bytes can be varied. If it changes to 512 bytes, the index size shrinks to 1/512 of the former index and no more than 50% of duplicate data over the network would be saved. In this situation, 1 GB of data almost requires indexes of size 4 MB. For the content chunking algorithm, one chunk, whose expected size is set to 8 KB, needs one chunk address, one 160-bit hash value and the chunk size. It also needs to maintain about indexes of size almost 4MB for each 1 GB of data.



V. EXPERIMENTAL EVALUATION  We implement a prototype of DDSN and deploy it on a Dell R210 computer. In the meantime, we also implement  the basic functions of fixed size blocking, content chunking, and sliding blocking strategies. To evaluate the performance of these methods, we explored two different collections of real-word data sets to test the different aspects of the de- duplication network file system:  ? Users? personal files. The 7.5 GB of information is collected from 35 home directories of different users.

? Software distributions. We get the successive Linux kernel distributions from the public website and keep them in three different formats. The size of this data set is approximately 5.3 GB.

Firstly we should pre-process data sets. The block size or chunk size has a great influence on the performance of the de-duplication method. Consequently, these methods should apply the same block size or chunk size. But it is difficult for the content chunking method. Figure 3 shows the distribution of chunk sizes under users? personal files when the expected size is set to 8 KB. In particular, we obtained an average chunk size of 8.4 KB.

0 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64  N um  be r o  f C hu  nk s  Chunk Size(KB)  Figure 3: Distribution of Chunk Sizes  As a de-duplication network file system, saving more space is the most important aspect that we concentrate on.

Figure 4 shows the percent of detected duplicate data in storage. For both data sets, the sliding blocking method and DDSN perform best. Due to their homology, their performance is very approximate. The fixed size blocking algorithm always owns the worst capability, since it is sensitive to the modification position.

Users' personal files  Software distributions  % o  f D up  lic at  e Da  ta   Fixed Size Blocking  Content Chunking  Sliding Blocking  DDSN  Figure 4: Duplicate Data Detected in Storage     The bandwidth of the network file system is precious as well as the storage space. Figure 5 illustrates the percent of transmitted data over network compared with original data.

The sliding blocking method can not save any bandwidth, since it needs the whole new file to find the duplicate data at the server end. For both data sets, the content chunking method can save the most transmitted data over the network.

It almost does not transmit redundant data. Although the fixed size blocking algorithm has the similar principle, it can not perform well as the content chunking method, because the amount of its detected duplicate data can not reach the others. DDSN also performs better than the fixed size blocking algorithm in spite of transmitting the floating bytes.

Users' personal files  Software distributions  % o  f T ra  ns m  itt ed  D at  a  Fixed Size Blocking  Content Chunking  Sliding Blocking  DDSN  Figure 5: Data Transmitted over Network

VI. RELATED WORK  A number of algorithms or techniques have been proposed to detect data duplication. SIS [1] implements de-duplication function at the granularity of files. To obtain block-level de-duplication, the fixed size blocking method is proposed.

Venti [2] reports a reduction of around 30% in the storage space usage with this method. DDE [9] applies this tech- nique onto on-line IBM SAN File System. LBFS [3] divides the files into variable-sized chunks using Rabin fingerprint [8] instead of the fixed-position boundary. This adjustment, called content chunking, makes the storage system no longer sensitive to the modification position. Pasta [4] and Pastiche [5] also enjoys this advantage by identifying and storing replica only once. The original remote file synchronization algorithm, rsync [7], is designed to minimize the bandwidth usage. The sliding blocking method [6] extends rsync to detect the redundancy of the whole storage system. Although this modification does not support remote propagating any more, sliding blocking could find more replicas. Some complex storage systems, such as REBL [10] and DEBAR [11], are proposed based on a mixture of the above basic duplicate detection algorithms.



VII. CONCLUSIONS AND FUTURE WORK  We have described our new duplicate detection method and the prototype called DDSN. Our new duplicate detection  method can achieve the same performance in terms of stor- age space consumption as the sliding blocking method, the best of existing de-duplication storage algorithms for storage consumption. In the meantime, our new method overcomes its drawback that the whole file needs to be transmitted over the network and saves massive bandwidth for the duplicate data. In the future, we would consider reducing the system overhead such as the memory consumption.



VIII. ACKNOWLEDGEMENTS This work was supported in part by National Natural  Science Foundation of China Grants (No. 61272092, No.

60903108), the Program for New Century Excellent Tal- ents in University (NCET-10-0532), the Natural Science Foundation of Shandong Province of China Grant (No.

ZR2012FZ004), the Independent Innovation Foundation of Shandong University (2012ZD012), the Taishan Scholars Program, and NSERC Discovery Grants.

