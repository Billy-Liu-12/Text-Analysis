PCAR?an Efficient Approach for Mining Association Rules

Abstract   A lot of existing algorithms used for mining association rules identify frequent itemsets by the method of bottom-up combination of smaller frequent itemsets or top-down decomposing of larger infrequent itemsets, these methods result the large volumes of candidate itemsets. Actually as the supersets of infrequent items are infrequent itemsets, this paper presents a new efficient method, namely Pruning-Classification Association Rule (PCAR). PCAR combines minimum frequency items with minimum frequency itemsets. It firstly deletes infrequent items from itemsets, then classifies itemsets based on frequency of itemsets, finally discovers frequent itemsets.

The number of candidate itemsets is greatly reduced and itemsets need not to be combined or decomposed, therefore, operation time and memory requirement could be decreased accordingly. This method has significant advantage in mining association rule at large volumes of items and small frequency of itemsets. It is proved by experiments that PCAR outperforms the well-known Apriori and CBAR algorithms.

1. Introduction   Along with the development of information technique, the data mining technologies are used to uncover unknown and regular relationships hidden in data warehousing, which are very important to decision - making. Therefore, the method for effectively achieving association rules has been recognized as an important area of database research. R. Agrawal et al. proposed the well-known Apriori algorithm [1] for mining frequent itemsets. This method mined frequent itemsets through a loop of searching. It examined numerous candidates of the entire data warehouse repeatedly while mined frequent itemsets stage after stage, therefore, wasted large memory and results in inefficiency. Focusing on these defects, some researchers provide improved methods.

Mainly these methods developed in two ways. One was to reduce the candidate itemsets amount and memory requirement. For example, Brin et al. presented the Dynamic Itemset Count (DIC) algorithm for identifying  frequent itemsets [2], which added candidate itemsets at the different scanning time. It chose the candidate itemset before scanning database, hence, by evaluating all subsets of an itemset frequent or not, decided to add or not a new candidate itemset. It needed to scan database twice.

Comparing to the classical algorithms, fewer data and candidate itemsets were required. Pork et al. proposed an effective Direct Hashing and Pruning (DHP) algorithm [3] for reducing memory of candidate itemsets and efficiently controlling the number of candidate 2-itemsets. Savasere et al. proposed the partition algorithm [4] to mining partial frequent itemsets used as candidate frequent itemsets of database. It improved efficiency through dividing database into N parts and reduces the number of database scans, but wasted significant time in scanning infrequent candidate itemsets. Toivonen described Sampling Algorithm [5]. This method randomly selected a subset, searched frequent itemsets in the subset in stead of entire database. It scanned database only one time, but still wasted significant time in candidate itemsets, appeared efficient but inaccurate. Yuh-Jiuan Tsay et al forwarded CBAR algorithm?Cluster-Based Association Rule?[6] to reduce memory by creating clusters and assorting transaction records to different clusters. Q. Zou et al presented a pattern decomposition algorithm [7] to find out frequent itemsets. The other way was to change data construct. For example, the Column-Wise Apriori algorithm [8] proposed by B.Dunkel and the Tree-Based Association Rule(TBAR) [9] proposed by F.Berzal, reduced data scanning time and improved efficiency through changing storage construct of data and compressing database. The paper [10] described a new data structure, named as BitTable, to compress database for generating candidate itemsets and the support quickly.

It transformed data record into binary system, obtained frequent itemsets after newly encoding. The paper [11] proposed a new method to save transaction records in database by impact data structure FP-tree, and thus, discovered frequent pattern without scanning database.

However, the defect of the two methods appeared more memory required.

A conclusion could be obtained from above analysis: 1) mining frequent itemsets is the key point in the mining algorithm of association rules; 2) the solutions include decreasing the number of candidate itemsets, reducing the   DOI 10.1109/FSKD.2008.408     times of databases scans and changing data structure et al; 3) the efficiency and accuracy of current ways are not very good, and need to be improved.

Firstly, this paper Briefly describes the features of bottom-up and top-down algorithms, subsequently, explains the proposed PCAR algorithm; finally, contrasts the new algorithm with existing Apriori and CBAR in stability with help of the experiment.

2. PCAR algorithm and background   This paper presents a novel and more efficient PCAR algorithm. It comes from the analyzing and considering of Apriori and CBAR algorithm.

In mining algorithms, I={I1?I2?I3??Ii} represents a set of item, Ii represents ith item. The set of items is called an itemset or transaction record. The number of items in an itemset indicates its length. The database is combined with different length itemsets. Ti represents ith itemset. ITi ? .|Ti| represents the length of ith itemset ?that is the number of contained items?. Each itemset has an identifier TID ? MaxLength represents the maximum length of itemsets in the database. if T itemset contains all items of X itemset? IX ? ??then itemset T supports itemset X?that is TX ? . The total number of contained itemsets X is called the support of X?that is Support?x?. If the support of an itemset is greater than minimum support of user defined, then the itemset is called the frequent itemset.

Apriori algorithm gets large frequent itemsets through the combination and pruning of small frequent itemsets.

The principle of the algorithm is: firstly calculates the support of all itemsets in candidate itemset Ck obtained by Lk-1, if the support of the itemset is greater than or equal to the minimum support, the candidate k-itemset is frequent k-itemset, that is Lk, then combines all frequent k-itemsets to a new candidate itemset Ck+1, level by level, until finds large frequent itemsets, as shown in Fig.1.

Contrasted with Apriori algorithm, CBAR starts from finding large frequent itemsets, decomposition and deleting infrequent itemsets, generates new candidate itemsets, level by level, until finds small frequent itemsets, as shown in Fig.2.

Figure 1.  Apriori algorithm The kind of bottom-up Apriori-like algorithms needs to  combine small frequent itemsets and generates numerous candidate itemsets. Every candidate itemset needs to contrast with the all itemsets of the database. Therefore, it is low efficiency and only suited to the database contained more small itemsets, as shown in Fig3 line2. However, the kind of top-down CBAR-like algorithms needs to   Figure 2. CBAR algorithm [6]  decompose large itemsets, also generates numerous candidate itemsets, and is suited to the database contained more large itemsets, as shown in Fig3 line3.When not only the items are in large quantity and the number of itemsets is less than the total number of items combination theoretically, but also the number of medium length itemsets is more and the others is less in itemsets, as shown in Fig3 line1, these algorithms need to perform numerous of combination and decomposition, the efficiency is low. As any supersets of the infrequent itemsets are infrequent itemsets, we provide a new efficient Pruning Classification Association Rule (PCAR).

This algorithm, firstly, calculates frequency of items, deletes items which frequency is lower than minimum support, then calculates frequency of itemsets, divides itemsets to frequent itemsets and infrequent itemsets according to minimum support. Minimum frequency of items in the infrequent itemsets is calculated level by level until the number of infrequent itemsets is less than the minimum support. Contrasted with current algorithms, PCAR doesn?t need to combine smaller frequent itemsets or decompose larger infrequent itemsets.

It calculates the frequency of itemsets and items only, and then contrasts with the minimum support, not the entire database. This algorithm uses to obtain frequent itemsets quickly, greatly reduces the number of candidate itemsets, and significantly cuts down the operation time.

Itemsets Feature Analysis   i+7 i+6  i+5 i+4 i+3 i+2 i+1  i  Itemsets Length  P r o p o r t i o n  o f  t h e  d i f f e r e n t  I t e m s e t s %  1:PCAR algorithm 2:Bottom-up algorithms  3:Top-down algorithms   Figure 3. Itemsets feature analysis  Scan entire database, calculate support  Candidates 1-itemset  Frequent 1-itemset Infrequent 1-itemset  Candidates 2-itemset  Frequent 2-itemset Infrequent 2-itemset  Candidates M-itemset  Frequent M-itemset Infrequent M-itemset Frequent itemse  L2  L1  Cluster_Table(1)  Cluster_Table(2)  Cluster_Table(M-1)  Cluster_Table(M)  Contrasts with partial cluster tables  Contrasts with partial cluster tables  Contrasts with partial cluster tables  Contrasts with partial cluster tables  Large 1-itemsets  Large 2-itemsets  Large 3-itemsets  Large M-itemsets  Infrequent itemsets        3. PCAR program and example   PCAR algorithm identifies frequent itemsets by pruning infrequent items. Its procedure is shown as Fig.4.

Figure 4. Procedure of the PCAR  Firstly, the algorithm calculates the frequency of all items, denoted ItemSup(i), obtains minimum frequency, denoted MinSupportItem. Then:  1) If MinSupportItem is less than MinSupport, the item which frequency is less than MinSupport is called pruning items, denoted ItemP(j)?and will be deleted from all candidate itemsets C(i). Then the algorithm calculates the frequency of the itemsets, denoted CSup(i). The candidate itemsets, which frequency are larger than or equal to MinSupport, are the frequent itemsets L(i)?others are infrequent itemsets NL(i). If k, the number of infrequent itemsets, is less than MinSupport ? the procedure terminates. Otherwise candidate itemsets C(k) are set as infrequent itemsets and the procedure starts next loop.

2) If MinSupportItem is larger than or equal to MinSupport and frequent itemsets is null?then it can directly calculate the frequency of all candidate itemsets CSup(i). The candidate itemsets, which frequency are larger than or equal to MinSupport, are frequent itemsets L(i)?others are infrequent itemsets NL(i). Candidate itemsets C(k) are set as infrequent itemsets. If k, the number of infrequent itemsets, is less than MinSupport? the procedure terminates, otherwise starts next loop.

3) If MinSupportItem is larger than or equal to MinSupport, and frequent itemsets exist, then items which have minimum frequency are set as pruning items, denoted ItemP(i)?pruning items are read. The candidate itemsets that contain pruning items are classified to temporary itemsets T(j), else are classified to infrequent itemsets NL(i). The frequency of all items in T(j) is calculated. If the support of ItemP(j) is less than MinSupport, the ItemP(i) can be deleted from T(j).  The frequency CSup(j) of all itemsets is calculated. If the frequency of an itemset CSup(j) is larger than or equal to  MinSupport, the itemset is frequent itemset L(i). Besides, ItemP(i) are deleted from all temporary itemsets T(j), if T(j) is frequent itemsets, then it is added to frequent itemsets L(i)?otherwise it is added to infrequent itemsets NL(i), and sets candidate itemsets C(k) as infrequent itemsets. If number k of infrequent itemsets is less than MinSupport, the procedure terminates, otherwise starts next loop.

The example is used to describe the process of PCAR algorithm. Data table contains seven itemsets, as shown in Fig.5. The smallest length of itemsets is 2. MinSupport is defined as 2.

1) The algorithm calculates the frequency of items, obtains minimum frequency and sets MinSupportItem=1.

Those items which frequency is less than MinSupport become pruning items, ItemP={I7,I8,I9}. The pruning items{I7,I8,I9} are deleted from all candidate itemsets.

Then the algorithm calculates the frequency of itemsets.

According to the MinSupport, the frequent itemsets L and infrequent itemsets NL are generated, L2={I1I5?I2I5}? L3={I4I5I6}. Since the number of NL is larger than MinSupport, the candidate itemsets C is set as infrequent itemsets?  2) The algorithm calculates the frequency of items, obtains minimum frequency, ItemP={I6}. {I6} is deleted from the candidate itemsets C, and the frequency of the candidate itemsets is calculated. As the frequency of the candidate itemsets is less than MinSupport,  the infrequent itemsets are obtained, NL={I2I4I5?I1I4I5?I1I2I5};  3) The algorithm calculates the frequency of items, obtains minimum frequency, MinSupportItem=2. As MinSupportItem is equal to MinSupport, the items that occupy MinSupportItem are the pruning items, ItemP={I1,I2,I4}. The pruning item {I1} is read. Some candidate itemsets that contained the pruning item {I1} are selected to combine temporary itemsets T={I1I4I5? I1I2I5}, others are combined as infrequent itemsets, NL={I2I4I5}. The frequencies of all items in the temporary itemsets are calculated. The pruning items ItemP={I4,I2} are deleted from itemsets. The frequent itemsets are obtained, 2}51{ LII ? . After {I1} is deleted from temporary itemsets T={I1I4I5,I1I2I5}, 23}I52}{54{ LLIII ?? ? therefore, the infrequent itemsets are obtained, NL={I2I4I5}. As the number of infrequent itemsets NL is equal to 1?it is less than MinSupport, then the program terminates.

At last, the frequent itemsets are generated, L2={I1I5? I2I5} and L3={I4I5I6}, they can be converted to association rules relatively.

4. The evaluation PCAR and typical algorithms   The algorithms of PCAR, Apriori and CBAR are implemented by using Microsoft Visual Studio 2005 on a  Database  Infrequent itemsets NL(2)  Infrequent itemsets NL(1)  Pruning items ItemP(2)  Frequent itemsets L(1)  Candidate itemsets C(1)  Frequent itemsets L(2)  Candidate itemsets C(2)  Pruning items ItemP(1)  Infrequent itemsets = null Frequent itemsets L(i)         Figure 5. Example of the PCAR        pentium 2.0 GHz PC with 512 MB physical memory. The tested database is a patent database that contains 47 items describing the function-featured parts of the power tool.

The largest length of itemsets is 7. The stability of PCAR is tested at different minimum support and various data total number. The efficiency of PCAR is contrasted with Apriori and CBAR in the time to obtain frequent itemsets at same total number of itemsets and different minimum support. The procedure is shown as follows.

When the database contains 47 items, the largest length is 7, (1) sample randomly 30, 60, 90, 120, 150 patent records, the operation time of PCAR will go up with the total number of records in the database. When the minimum support is 2, 3, 4, 5, 6, the time will change as shown in Fig.6. (2) Sample randomly 150 records, when the minimum support is 6, 5, 4, 3, 2, the time of PCAR, Apriori and CBAR are shown as Fig.7.

30 60 90 120 150  Record Size  Ti me  (m s)        Figure 6. The time of PCAR at the different support      6 5 4 3 2  Minimum Support  T im e (m s) Apriori  CBAR  PCAR   Figure 7. Contrast PCAR with Apriori, CBAR  The experiments indicate that the PCAR appears more efficient and stable than CBAR and Aprior in various minimum supports.

5. Conclusions   The identification of frequent itemsets is very important in data mining, especially in mining association rules. The theoretical analysis and experimental results all show: the PCAR algorithm is better than current used algorithms in the efficiency and stability, also has value of application.

6. Acknowledgements   This research was supported by National Natural Science Foundation of China?50675196?.

