Fast Collaborative Filtering with a k-Nearest  Neighbor Graph

Abstract? Traditional user-based/item-based Collaborative Filtering algorithms predict the preferences of all of the unseen items of a user. While this approach facilitates evaluations of the accuracy of various algorithms using the root mean square error, it consumes a considerable amount of time to recommend items for users. In this paper, we present a fast Collaborative Filtering algorithm using a k-nearest neighbor graph. Not only does this algorithm predict the preferences of only the k-nearest neighbor items, but it also shortens the execution time by calculating a k- nearest neighbor item graph in less time based on greedy filtering.

The experimental results show that our approach outperforms traditional user-based/item-based Collaborative Filtering algorithms in terms of both the preprocessing time and the query processing time without sacrificing the level of accuracy.

Keywords? Fast Collaborative Filtering, real-time recommendation, k-nearest neighbor graph, Greedy Filtering.



I.  INTRODUCTION User-based collaborative filtering algorithms are effective  ways to recommend useful contents to users by exploiting the intuition that a user will likely prefer the preferred items of similar users. More specifically, when a user requests a recommendation, the user-based collaborative filtering scheme [1] predicts the user?s preferences for all of the unseen items based on similar users? preferences for those items. In a similar way, the item-based collaborative filtering scheme [2] predicts the preferences of the user for all unseen items based on the preference levels of their similar items.

Earlier studies in these areas produced movie recommendations of a high quality compared to baseline algorithms, which could only recommend the most popular movies or highly rated movies [3]. However, predictions are necessary for all unrated movies as well. While such an approach facilitates evaluations of the accuracy of various algorithms using a root mean square error approach, doing so consumes a significant amount of query processing time.

Moreover, the pre-computation time is also long especially for the user-based collaborative filtering, as it has to calculate all of the similarity values between users.

In this paper, we present a fast Collaborative Filtering algorithm using a k-nearest neighbor graph. Not only does this  algorithm predict the preferences of only the k-nearest neighbor items, but it also shortens the execution time by calculating a k-nearest neighbor item graph in a short period of time by calculating a k-nearest neighbor item graph in less time based on greedy filtering. We demonstrate that our approach outperforms traditional user-based/item-based collaborative filtering algorithms in terms of both the preprocessing time and the query processing time without sacrificing the level of accuracy.



II. RELATED WORK User-based collaborative filtering algorithms predict the  preferences of all items unseen by the user based on similar users? preferences for those items. Assuming that we recommend movies to users using the MovieLens dataset, the predicted rating for active user u for item i is defined as follows [1]:  ? ? (       ?)     (   )  ? |   (   )|   Here, k denotes the number of neighbors, and   ? and   ?  are the average ratings of user u and neighbor v, respectively. In addition,    (   ) is the similarity between u and v. We use the Pearson correlation coefficient as the similarity measure for the user-based Collaborative Filtering algorithm:  (   ) ? (       ?)(       ?)  ?? (       ?)   ?? (       ?)       In this equation, m is the number of co-rated movies.

Item-based collaborative filtering algorithms predict the preferences of items unseen by the user based on the preference levels of their similar items. In a similar way, the predicted rating is defined as follows [2]:   ?         (   ) ? |   (   )|    Note we use the adjusted cosine similarity as the similarity measure for item-based collaborative filtering:     (   )  ? (       ?? ?)(       ?? ?)    ?? (       ?? ?)   ?? (       ?? ?)       Here, m denotes the number of co-raters in this case.

Note that the k-nearest neighbors of user u are not identical to the k neighbors which have the highest similarity values in these collaborative filtering algorithms. In the user-based collaborative filtering algorithm, they are defined as the k- nearest users that have rated item i, whereas in the item-based collaborative filtering algorithm, they are defined as the k- nearest items that have been rated by user u.

Earlier studies in this area indicated that these types of algorithms produced movie recommendation results of a high quality compared to baseline algorithms, which only recommended the most popular movies or highly rated movies.

However, these types of algorithms consume considerable amounts of both the preprocessing time and the query processing time; as a pre-processing step, user-based and item- based collaborative filtering algorithms prepare a user-by-user similarity matrix and an l-nearest item graph, respectively.

When a recommendation is requested, both algorithms have to predict all of the unrated movies. In Section III, we present novel ways to cope with these problems.



III. REAL-TIME RECOMMENDATION Our approach consists of two main steps: First, we  approximately calculate the k-nearest item graph based on greedy filtering [4] as a pre-processing step. Second, we recommend items to users by using our revised version of Non- Normalized Cosine Neighborhood.

A. k-NN Graph Construction The construction of the k-nearest neighbor graph is a task  which involves finding the k nodes most similar to each node.

In this paper, we assume that a node is a movie. One of the easiest ways to construct a k-NN graph is to calculate the similarities between all of the nodes and extract the most similar nodes for each node. In spite of its simplicity, this brute-force approach requires  (  ) time complexity, which is burdensome when used in conjunction with large amounts of data. An alternative way to cope with this problem is to use Inverted Index Join using the fact that user-by-item matrices are usually very sparse. However, this approach is also not sufficient for handling large amounts of high-dimensional data.

Our main idea is to construct an approximate k-nearest neighbor graph based on greedy filtering [4] in order to speed up this process. If there is no decline in the quality of recommendations when we use approximate graphs, we do not have to spend time for building an exact k-NN graph. To applying the greedy filtering algorithm, we assume that the nodes (items) are represented by vectors. We then filter out vector pairs whose large value dimensions do not match at all.

Figure 1 shows an example of greedy filtering: There are five vectors and the colored portion denotes the high-value dimensions, and their values. In this example, we do not calculate the similarity between the vectors    and   , as they  do not have a common dimension in the colored portion.

Empirically, the time complexity of greedy filtering is  ( ).

Note this algorithm performs much better when we apply the TF-IDF weighting scheme. Because it is known that the TF-IDF weighting scheme does not decrease the quality of recommendations significantly, we add the following weights for the user-by-item matrix:  (    )     (  | | ( ))  Here, I is the set of items, and freq(u) is the number of items rated by user u.

B. Fast Recommendation Algorithm Recall that one of the main drawbacks of user-based/item-  based collaborative filtering schemes is that they have to predict all of the unrated items. To tackle this problem, we present the novel algorithm shown below. We assume that user u requests a recommendation:  Figure 2.   Recall of the Collaborative Filtering algorithms      Figure 1.   Example of Greedy Filtering [4]       1) For all unrated items i, empty the list of k-nearest items of i.

2) For all rated items i, add i to all of lists of its k-nearest neighbors.

3) For all unrated items i, if the number of its k-nearest neighbors is larger than k, delete all except for the most similar k neighbors.

4) Then, predict the ratings for the unrated items that have only k-nearest neighbors, as follows:    {   ?  ?(       ?)     (   )      sim(i,j) is the cosine similarity measure defined as follows:   (   )   ?   ?? ??  ? ??    The intuition behind this algorithm is that if one of the k-  nearest neighbors of item i is item j, there would be a high probability that one of the k-nearest neighbors of j is i. Because user-by-item matrices are usually sparse, this approach can reduce the computational cost of predicting unrated items.



IV. EXPERIMENTS We use the MovieLens dataset for comparisons. There are  1,000,209 ratings, 3,952 movies, and 6,040 users in this database. We follow the testing methodology of the recommender systems introduced by Cremonesi et al. [3]. We divide the ratings into two groups. One group of data is used for our training set (98.6% of ratings), and the other group of data is used for the probe set (1.4% of ratings). The test set consists of five-star ratings of the unpopular items (99.65% of the items) in the probe set. Then for each rating of movie m rated by user u in the test set, we randomly select 1,000 movies unrated by u, and recommend the top-10 items; if we recommend m, we refer to this as a hit. Finally, we measure the degree of recall as follows:  |        |  We considered six types of algorithms for a comparison: User-based CF implements the work by Herlocker et al. [1]; Item-based CF implements the work by Sarwar et al. [2]; Our Approach implements only the fast recommendation algorithm presented in Section III.B, and Our Approach, TF-IDF applies the TF-IDF weighting scheme to the MovieLens datasets and implements the fast recommendation algorithm. Finally, Our Approach, TF-IDF, GF (64%) and Our Approach, TF-IDF, GF (33%) implement all of the approaches presented in this paper.

The percentages inside the parentheses indicate the accuracy of the generated k-nearest neighbor graph. We can change the parameter values that affect the accuracy level during the construction of the k-nearest neighbor graph. Of course, there is a tradeoff between time and quality.

Figure 2 shows that our approach outperforms user- based/item-based Collaborative Filtering in terms of recall.

That is to say, fewer rating predictions yields better results.

When we apply the TF-IDF weighting scheme to our approach, the recommendation quality is slightly improved. When we use the approximate k-NN graph with 33% accuracy instead of an exact k-NN graph, the recall is decreased significantly. On the other hand, the recall does not change much when we use the graph with 64% accuracy.

Figure 3 shows the comparison results for these algorithms in terms of the preprocessing time and the query processing (recommendation) time for 100 queries. Clearly, our approaches significantly outperform existing approaches in terms of the preprocessing time. Also, although the difference between the query execution times of our approaches and that of the item-based collaborative filtering algorithm is not great in this figure, these differences grows as the number of nodes scales up.



V. CONCLUSION In this paper, we present a fast Collaborative Filtering  algorithm using a k-nearest neighbor graph. Not only does this algorithm predict the preferences of only the k-nearest neighbor items, but it decreases the execution time by calculating a k-nearest neighbor item graph in less time based on greedy filtering.

The experimental results show that our recommendation algorithm is much faster than traditional user-based/item-based    Figure 3.   Execution time of the Collaborative Filtering algorithms  Figure 1.

Collaborative Filtering algorithms and that it also produces recommendation results of a better quality.

