Efficient Progressive Sampling for Association Rules *

Abstract  In  data mining, sampling has ojien been suggested as an effective tool to reduce the size of the dataset operated at some cost to accuracy Howevez this loss to accuracy is ojien difjicult to measure and characterize since the exact nature of the learning curve (accumcy vs. sample size) is parameter and data dependent, i.e., we do not know apriori what sample size is needed fo achieve a desired accuracy on a particular dataset for a particular set of parameters.

In  this article we propose the use ofprogressive sampling to determine the required sample size for association rule min- ing. We first show that a naive application of progressive sampling is not very effrcientfor association d e  mining.

We then present a refinement based on equivalence classes, that seems to work extremely well in practice and is able to converge to the desired sample size very quickly and very accurately. An additional noveliy of our approach is the definition of a suppon-sensitive, interactive measure of ac- curacy across progressive samples.

1 Introduction  viability of using sampling[6] to reduce the dataset size.

While such methods have shown quite a lot of promise it has been observed by several researchers[l4, 15, 201 that it i s often very difficult to quantify, apriori. the quality of the re- sults obtained for a given sample size. Recently, to addresg this problem some researchers have proposed and evaluated progressive sampling[l4] for select data mining tasks. Pro- gressive sampling starts with a small sample and uses pro- gressively larger ones until model accuracy no longer im- proves beyond a user specified threshold. In this paper we study progressive sampling methods as they apply to asso- ciation rule mining, a key data mining task. Realizing an efficient method to progressively sample a dataset for asso- ciation rule mining poses several challenges.

First, and foremost, one needs to define a notion of model accuracy. In other words, for a particular dataset how does one define how good the sample is? This goodness cri- terion should be sensitive to relevant interaction parameters (e.g. support, confidence, important items' to the user) as well as the inherent properties of the dataset in question. A naive approach could be to compare the set of associations generated by the sample with the set of associations gener- ated on theentire dataset. Obviously, this is self-defeating and does not take into account aspects of user interaction.

As our ability to collect, store, and distribute huge mounts of data increases with advancine technnloev. ~ ~ ~ ~ ~ ~ ~ - ~ -  -, . ~~~~~~~ ~~ ~~~ ~ ~ ~ ~~~ ~~ .~~ discovering the knowledge hidden in these ever-growing databases has become a pressing problem, mis problem re.

femed to as data-mining, an effort 10 derive interesting con.

clusions from large bodies of data, is an interactive process.

In fact. interactivitv is often the kev to facilitating effective  Second, while defining a notion of model accuracy is im- portant, one must also be able to compute it efficiently, Zaki et ai[201, have observed, that at low sample sizes, there is a tendency to detect a large number of false positives. This Property can limit the effectivenessof progressive sampling.

~~~~~  data understanding and knowledge discovery. In such an environment response time is crucial. However, extracting knowledge from these massive databases is a compute and VO intensive process which makes the task of guaranteeing quick response times difficult.

In order to to minimize the YO traffic involved in such data-intensive applications researchers have evaluated the  'Th is  work was panially supported by an Amentech Faculty Fellow- ShiR.

0-7695-1754-4/02 $17.00 0 2002 IEEE  Third, as noted by Provost and Kolluri 1151 "most discus- sions on sampling assume that producing random samples efficiently from large datasets is not difficult. This is simply not true.". In fact most implementations require O ( N )  time where N represents the size of dataset and not the sample size (S). Naive implementations often may be much worse.

Note, that one cannot afford to spend O ( N )  time to generate  IFor example a user may be interested in associations pertaining to a specific item (say diapers).

mailto:srini@cis.ohio-state.edu   each progressive sample.

We address these three problems in the context of pro-  gressively sampling for association rules. Specifically our contributions are:  A novel measure of model accuracy for progressively sampling association rules. The measure is designed in such a way to be sensitive to user parameters and interactions while not requiring execution on the entire dataset.

An efficient technique for identifying the optimal sam- ple size. This key result is based on the identification and tracking of a representative set of frequent itemsets (a small subset of the entire set of frequent itemsets).

Essentially the computational element (computing the associations for a given sample size) is reduced sig- nificantly enabling faster convergence to the optimal sample size. This technique also addresses the high false-positive problem for low support values.

An efficient technique based on asynchronous U0 op- erations and a novel application of a well known sam- pling methodology to improve the efficiency of gener- ating a random sample, as perceived by the processor.

The rest of this article is organized as follows. In Section 2 we provide some background on progressive sampling and association rules. In Section 3 we present our progressive sampling approach. We empirically evaluate the proposed approach on synthetic and real datasets in  Section 4. Finally we conclude with directions for future work in section 5.

2 Background Discovery of association rules is an important problem in  database mining. The prototypical application is the analy- sis of sales or basket data [31 although more recently it has been adopted in the domains of scientific computing, bioin- formatics and performance modeling. The problem can he formally stated as: Let Z = {il, i2,. . . , a m }  be a set of rn distinct attributes, also called items. Each transaction T in the database 2) of transactions, has a unique identifier, and contains a set of items, such that T Z. An association rule is an expression A + B, where A, B C Z, are sets of items called itemsets, and A n  B = 0. Each itemset is said to have a support S if S% of the transactions in D contain the itemset. The association rule is said to have confrdence G if C% of the transactions that contain A also contain B, i.e., C = S(A U B)/S(A) ,  i.e., the conditional probahil- ity that transactions contain the itemset B, given that they contain itemset A.

Data mining of association rules f m m  such darabases consists offrnding the set of all rules which meer rhe user- specifred minimum confidence and support values.

Database Layoul: There are two possible layouts of the database for association mining. The horizontal layout con- sists of a list of transactions, where each transaction has an identifier followed by a list of items. The vertical layout consists of a list of items where each item contains a list of transactions that transacted on that particular item. A p proaches based on the horizontal format include the popular Apriori algorithm[3] and its variants. The Apriori algorithm uses the downward closure property of itemset support to prune the itemset lattice - the property that all subsets of a frequent itemset must themselves be frequent. Thus the frequent k-itemsets are used to construct candidate (k+l)- itemsets. A pass over the data is made to identify which of the candidate (ktl)-itemsets are actually frequent. This process is repeated till there are no more frequent sets.

The vertical format has the advantage that the support for candidate k-itemset can be computed by simple tid-list intersections. The tid-lists cluster relevant transactions, and avoid scanning the whole databases to compute support, and the larger the itemset, the shorter the tid-lists, resulting in faster intersections[Zl]. An additionaloptimization of com- pressing the vertical lists results in additional gains due to lower memory and YO traffic[4, 171. Note that sampling in the vertical format will be inefficient. Essentially sampling in the vertical context will necessarily have to keep track of which transactions are in the sample and which are out and one would need to to scan through each lid-list and mark the transactions in the sample and chose out of it. One can of course sample in the horizontal format and then convert to the vertical format on the By.

Equivalence Class Partitioning: One way to improve the vertical approach is to use it in conjunction with equiv- alence class partitioning. Let the set of large two-itemsets, L2, he {AB, AC, AD, AE, BC, BD, BE, CD, DE}. Equiva- lence class partitioning partitions these itemsets by their (k- 1 length where k= 2 in the above example) prefixes result- ing in the following four partitions:Sa = [A] = {AB, AC, AD, AE}, SB = [B] = {BC, BD, BE}, Sc = [C] = {CD}, and SD = [D] = {DE}. The same partitioning scheme can be recursively repeated to find all the associations. For in- stance, in the above example, partition [B] yields candidates {BCD,BCE,BDE}. Assuming that all of the candidates are deemed frequent then the level 3 partitions for [B] are SBC = [BC] = {BCD,BCE}, and SBD = [BD] = {BDE}. The advantage of this approach is that the algorithm can process an entire equivalence class partition before proceeding to the next partition. This improves memory locality and min- imizes U0 traffic in the ECLAT algorithm[2l]. an approach based on the above equivalence class partitioning.

Note, that past work has not considered using the equiv- alence class idea within the context of sampling. For the present work it is important to define a notion of an equiv- alence superclass. Informally, an equivalence superclass     is defined as the set of all frequent itemsets that can re- cursively be enumerated from a given partition. From the above example, the equivalence superclass from partition SB. denoted as ESB = {BC,BD,BE,BCD,BCE.BDE}.

Sampling for Associations: While several authors have proposed various strategies on the use of sampling for KDD[8, 14, 111 and database tasks[l2], we limit the dis- cussion in this section to those relevant to association min- ing. Toivonen [I81 presents an association rule mining al- gorithm using sampling. The approach can be divided into two phases. During phase 1 a sample of the database is ob- tained and all associations in the sample are found. These results are then validated against the entire database. To maximize the effectiveness of the overall approach, the au- thor makes use of lowered minimum support on the sample.

Since the approach is probabilistic (i.e. dependent on the sample containing all the relevant associations) not all the rules may be found in this first pass. Those associations that were deemed not frequent in the sample but were actually frequent in the entire dataset are used to construct the com- plete set of associations in phase 2. A detailed theoretical analysis of sampling (using Chernoff bounds) for associa- tion d e s  was presented by 7aki er a1 [20]. Chernoff bounds provide information on how close is the actual occurrence of an itemset in the sample as compared to the expected count in the sample. Based on itemset frequencies, using Chernoff bounds one can obtain a sample size[20]. How- ever, the sample size is independent of the original dataset size and can be quite large, sometimes larger than the orig- inal dataset! Empirical evidence in the same paper also showed that Chernoff bounds may be too pessimistic for association mining. It was also shown that sampling can be effective for association mining if the sample size were known apriori for the corresponding dataset and input pa- rameters. However, determining the optimal sample size was left as an open problem. Note, that determining the optimal sample size efficiently can significantly improve on the overall performance of Toivenen?s approach[l81 since with a good estimate one could minimize the computational and YO aspects of the second pass.

Progressive Sampling: In order to quickly estimate the optimal sample size, researchers have recently turned to progressive sampling. Before we detail this procedure we first define the notion of a learning curve. A learning curve is a mapping between sample size and model accuracy. T y p ically a learning curve is depicted with the vertical axis rep- resenting the accuracy of the model and the horizontal axis representing the sample size. Most learning curves typi- cally have steeply sloping portion early in the curve, and a plateaulate in the curve[l4,5]. The cost-performancetrade- off is best at the knee of the curve. Such curves exhibit the property that the slope of the curve is monotonically non- increasing with n (excepting for small local variance). Most  learning curves exhibit the above behavior but some curves can misbehave especially at small sample sizes[lO].

The goal of progressive sampling is to start with small samples and progressively increase them as long as model accuracy improves sufficiently. Using such a technique one can identify the knee of the learning curve using basic slope characterization across recently evaluated samples. One problem in the association rule mining context is how does one quantify model accuracy? A simple metric would he to compare the set of associations found for a given sample size with the set of associations found for the entire dataset.

However, to obtain the latter we would have to run the al- gorithm on the entire dataset!

The efficiency of progressive sampling is governed by the average case execution time performance of the algo- rithm, and by the sampling schedule. In the case of associ- ation rule mining algorithms while the worst case complex- ity is exponential the average case behavior typically tends to be linear, or in some cases quadratic. The issue of de- termining an optimal sampling schedule was addressed by Provost et a1[14], where the authors show that a simple geo- metric sampling schedule is efficient in an asymptotic sense for most induction algorithms with a run time complexity of O(n) or worse as long as the maximum sample size is n 12.

However, the above result is not useful if the desired ac- curacy is met only at a a sample size of 70% (for a linear time algorithm). This reduction (from 100%) may still re- sult in significant performance benefits. Another problem with the above theoretical model is that sampling overheads are ignored. Efficiently obtaining a sample from a large dataset, is often ignored by most researchers as pointed out by Provost and Kolluri[lS]. If one were to account for this the benefits of progressive sampling would decrease. An- other overhead, induced within the context of association rules, is the avalanche effect of detecting false positives at very small sample sizes (see Zaki et al fordetails[20]). We address these issues in the next section.

3 Methodology In this section we present our approach for efficient pro-  gressive sampling of association rules. We first describe our measure of model accuracy which is based on the notion of self-similarity of associations across progressive samples.

A novelty of the proposed measure is that it is functionally dependent on user input parameters (support, constraints etc.). We then identify a representative subset of frequent itemsets such that the behavior of our measure of model accuracy on this representative subset mimics the behavior of our measure on the entire set of associations. The final problem we address relates to that of sampling overhead.

Model Accuracy: Absolute model accuracy, for a given sample is difficult to measure without running the algorithm on the entire dataset. Since this is not possible to do due to     efficiency constraints we define a measure of accuracy that is based on the following key intuitions:  For progressive samples dl and dz, where size(d2) > size(&), the self-similarity between the set of associations generated under the two samples is likely to be low during the growth phase and is likely to be much higher during the ?plateau? phase. Therefore identification of the knee of the curve can be done by measuring the self-similarity between progressive samples.

We now define the our notion of interactive self- similarity: Let A and B respectively be a the set of frequent itemsets for a database sample d l  and that for a database sample dz. For an element E A (respectively in B), let supd, (z) (respectively supd,(z)) be the frequency of z in dl (respectively in 4 ) .  Our metric is:  where a is a scaling parameter. The parameter a has a de- fault value of l and can be modified to reflect the signifi- cance the user attaches to variations in supports. For a = 0 the similarity measure is identical to w, i.e., support variance carries no significance. Sin1 values are bounded and lie in [O,l]. Sim also has the properly of relative or- dinality, i.e.. if Sim(X, Y) > Sim(X, Z ) ,  then X is more similar to Y than it is to Z. Note, that while the above for- mulation does not explicitly consider correlations between itemsets (e.g. two itemsets (ABEK, AEFK) that have many items in common are not treated differently), they are ac- counted for implicitly as all itemsets that can he formed hy the common items (A,E,K) are part of the summation.

An important point raised by Das, Manilla and Ronkainen[7], while evaluating the similarity between two attributes was that using a different set of external probes to measure similarity could potentially yield a different simi- larity measure. In our case the action of modifying the ex- ternal probe set corresponds to modifying the association sets evaluated over different samples of the input database.

This is achieved either by modifying the minimum sup- port or by restricting the search for associations to those that satisfy certain conditions (Boolean properties over at- tributes) [Z]. Note that the optimal sample size (the knee of the curve) can vary for a different set of parameter values.

Picking a Representative Set: The above metric sug- gests using the entire association set from consecutive sam- ples to measure the self-similarity between progressive samples. However, as observed earlier, computing complete association sets for each sample may eventually defeat the purpose of sampling since evaluating each sample has vari- ous overheads associated with it. To overcome this problem what we need is a representative class of itemsets which has a behavior similar to the self-similarity C U N ~  of the original set of associations. Moreover, this representative  class of itemsets should satisfy the criterion that it should be efficient to compute. Given the above requirement, we evaluated three possible options for a representative set: a random sample of the set of frequent itemsets; a level-wise (horizontal-i.e., all k-itemsets for a given k) split of the fre- quent itemset lattice: an equivalence-superclass (vertical) split of the frequent itemset lattice.

The first option for a representative class is infeasible. A truly random sample of the set of frequent itemsets while ideal from a statistical perspective, is impossible to gen- erate, without first generating the set of frequent itemsets and therefore self-defeating. The second option is imprac- tical for higher order splits (the set of all k-itemsets where k is large) as again one has to compute pretty much all the frequent itemsets before reaching the higher order split in question. We found empirically that using lower order splits tends to result in an unreliable over-estimation (i.e. pre- mature convergence) of the similarity between subsequent samples and was therefore not useful. Detailed analysis of these options was considered and the reader is refered to an extended version of this article for details[l3].

Our proposed approach is to use equivalence super- classes (vertical splits) generated from the most frequent item(s) as representative sets. We considered using ran- domly selected equivalence superclasses as the represen- tative class of itemsets but realized that these could suffer from the same problem as the horizontal split approaches.

The key intuition behind using the most frequent item is that such an equivalence super-class in all likelihood most completely captures the entire set of frequent itemsets and the distribution of associated supports across all levels of frequent itemsets [131. As we shall see from the empiri- cal results in the next section this representative class based approach yields a very good predictor of the self similarity measure. The other nice feature of this representative class is that it can be quickly computed, with some straightfor- ward modifications current-day vertical-set approaches[l31.

Note, no changes are required to the self-similarity mea- sure, we are just replacing the base association set with the representative set as determined by the first (few) equiva- lence class partition(s). However, the representative class selection is somewhat dependent on the similarity measure.

For instance if the similarity metric, as discussed earlier, were restricted to those itemsets including a particular item (say A), then the hest equivalence super-class to choose might be the equivalence super-class generated by that par- ticularitem (A). Also, while we have argued intuitively[l3], that equivalence super-classes based on the most frequent item(s) are likely to he good representative sets in general, other splits, based on the constraints imposed on the sim- ilarity measure, may be better and are being investigated.

Efficient Sampling Methodology: Provost and Kolluri[lS] point out that most sampling algorithms     require O(N) or greater time to execute where N is the size of the database and that researchers rarely account for this overhead. For generating samples of the database we use the Method A algorithm presented by Vitter(l91. A simple algorithm for sampling generates an independent uniform random variate for each record to determine whether that record should be chosen for the sample. If m records have been chosen from the first t records then the next record will be chosen with probability (n-m)/N-I. This algorithm generates N random variates. Method A significantly speeds up the sampling process by efficiently determining the number of records to be skipped before the next one is chosen. It generates exactly n random variates. With appropriate support for database indexing the method A scheme allows the sampling procedure to take O(n) time2.

Note that even with direct indexing creating a sample still creates some level of overhead. This overhead can be split into two components computational overhead (deter- mining which transactions are in the sample) and WO over- head (reading in said transactions). The YO component can be overcome with suitable systems support in the form of asynchronous VO, a technique which allows WO opera- tions to overlap with useful computation. Essentially one can overlap the WO required for the next progressive sam- ple with the computation required for processing the cur- rent sample. In other words while we compute the asso- ciation set for the current sample one can do the WO for the next sample size (in the sampling schedule). With ac- tive disk-like approaches[l, 161, one can move the compu- tational overhead associated with sampling off the critical path as well. This approach lends itself to accessing data over the network as well. since even that can be effectively overlapped with useful computation.

Algorithm Details: The basic steps to the progressive sampling approach are highlighted in Figure 1. We present two approaches, one based on estimating self-similarity be- tween the current sample and the subsequent sample in the schedule (RC-SS), and the other based on estimating self- similarity between the current sample and the entire dataset using the first k equivalence super-classes (RC-S).

Step 0 for the RC-S algorithm computes the representa- tive set on the entire dataset. Step 1 for the RC-SS algo- rithm computes the representative set on the lowest sam- ple size in the schedule. Step 2 is an iterative for loop.

Each iteration of this for loop first computes the next sam- ple (Step3) in the schedule. Then the representative set for this sample is constructed (Step4). After this the self- similarity measure is computed. For the RC-SS algorithm we compute the self similarity between this representative set and the previous one. For the RC-S algorithm we com- pute the self similarity between this representative set and  'Nole lhal if each transaction in the database is directly indexable prov- ing this bund for the worst-case is trivial.

Step 0 Compute representative set on entire dataset (RC-S only)  Step 1: Compute initial sample and representative set on sample. (RC-SS only)  Step 2 For each sample size in the schedule: Step 3: Compute sample.

Step 4 Compute representative set.

Step 5: Is convergence criteria met?

Step 6 If yes set effective sample  size (ESS) and break; Step 7: If no continue; Step 8 Compute the rest of the entire set  of associations for the ESS.

Figure 1. Proposed Approach  the one pre-computed in Step 0. If the similarity met- ric is above a user-specified threshold, then convergence is achieved (Step 5 ) .  For RC-SS we modified the conver- gence criteria slightly after viewing empirical results. We found that the self-similarity curve for RC-SS is not always well behaved (is non-monotonic). For this algorithm we declared convergence only if the similarity metric is above the user-specified threshold for two consecutive iterations.

This avoids premature convergence due to local variances and also protects against the possible misbehavior of self- similarity curves (found mainly at small sample sizes). If the convergence criteria is met we break out of the for loop (Step 6) else we continue (Step 7). In Step 8, we compute the overall association set for the determined sample size.

4 Experimental Methodology In this section we empirically evaluate the proposed  methodology on several synthetic and real datasets. We first describe the experimental setup (machine configura- tion, dataset properties etc.). We then evaluate the proposed measure of model accuracy: self similarity between consec- utive samples; and the impact on this measure when using the first equivalence superclass as a representative set. We then quantify the performance gains from using this repre- sentative class (as opposed to the entire set of associations).

At the end of this section we quantify the gains from our sampling methodology.

Experimental Setup: We used three synthetic datasets generated from the IBM dataset generator program[3].

These datasets mimic the transactions in a retailing environ- ment. The properties of the synthetic datasets are inherent in the names. The number following the T refers to the av- erage transaction length, the number following the I refers to the average maximal potentially frequent itemset size, the alpha-numeric-code following the D refers to the total num- ber of transactions (IM means 1 Million). We also used two real datasets for our experiments. The first real dataset is the Gazelle dataset which formed a part of the KDD Cup 1999 competition. The properties of these three datasets are de- scribed in Table 2. The second real dataset is the n o o k     dataset which is a dataset from a prominent online book- store retailer in South America.

Database I numT I Sim T1014D5M I 5000000 I 260MB T815D25M 25000000 1.25GB 1 Gazelle I 59602 I 1.35MB 1 Vbwk 136809 4.3MB  Figure 2. Database properties All experiments unless otherwise noted, were performed  on a dual pentium node machine, lGHz Pentium III, having 512 MB RAM and running Linux 2.4. For all the experi- ments we used a geometric sampling schedule up to 40% of the original dataset and an arithmetic progressive schedule from that point on, if required.

Impact of Representative Set on Self Similarity: The self similarity plots for all the datasets are described in Fig- ure 3. In each graph the Y-axis corresponds to the self sim- ilarity and the X-axis corresponds to the sample size. In each graph there are four plots, the RC-S and RC-SS plots have already been explained in the previous section. The A- S plots (in bold) is the learning curve that we are trying to estimate. They represent the naive (impractical) approach of comparing the entire set of associations generated by the sample with the set of associations generated by the entire dataset. The A-SS plots mirror the procedure described for R-SS, the only difference being that the self-similarity mea- sure is computed on the entire set of associations.

On viewing the plots for the real datasets (Figures 3a and 3b) one can observe that the plots for the represeuta- tive set (prefix RC) closely follow the plots for the entire set of associations (prefix A) for the corresponding support values considered. For the gazelle dataset, note that if we required a self similarity cut off of 0.9, for 0.1% support we would never obtain this value (even if we went the dis- tance), whereas at 0.25% support (not shown, see[l3] for details) this could be achieved at a 50% sample size. This highlights the fact that the user-specified parameters has an important role to play in determining whether sampling is useful or not and if so at what level. Overall both plots fol- low the expected pattern of low self similarity at smaller samples, and higher self similarity at larger samples. The phase transition from lower self-similarities to higher self- similarities coincide with the knee of the learning curve.

The results for the synthetic datasets are better (see Fig- ures 3c-d). The shape of the self similarity plots for the representative class almost exactly mirror the self similarity plots for the entire set of associations. Unlike the plots for the real datasets, the self similarities are quite high even at low sample sizes. This is mainly due to the fact that the syn- thetic datasets are much larger than the real datasets there- fore the absolute number of transactions for a given sample size (expressed as percentage of the original) is much larger.

Overall the RC-S plots most closely mirrors A-S plots.

Another interesting trend that is observed across both real and synthetic datasets is that there seems to be a relatively consistent overestimation or underestimation of the self- similarity across RC and A graphs. For example in Fig- ure 3a the RC curves consistently underestimate their A- curve counterparts. This would seem to indicate that the curve we want to estimate (A-S) can be best estimated by using the RC-S curve and a translation factor that can be de- rived from the translations computed from the RC-SS and A-SS curves. This strategy is currently being evaluated.

Impact of Representative Set on Performance: In this section we highlight the performance gains from using the representative class of itemsets to compute self similarities over using the entire set of associations. Figure 4 plots the cumulative execution time for different sample sizes for the datasets under consideration. The cumulative execution time is the execution time of the algorithm (RC-S or RC-SS) when the convergence criteria is satisfied at the correspond- ing sample size. The Baseline Execution lime corresponds to the execution time of the association mining algorithm on the entire dataset. Essentially the break even point for progressive sampling, when computing on the entire set of associations (represented by the prefix A) at 0.5% support, is when the cumulative execution time intersects with this baseline. For the VBook dataset the break even point when using the entire set of associations is reached at a sample size of around 10% for this dataset at this value of support.

The poor performance here can be traced to the fact that for this support value at low sample sizes the basic association mining algorithm detects a large number of false positives.

However, for the representative class approach, after deter- mining the effective sample size, the algorithm still has to be completely executed on that sample size (sans the rep- resentative class). For this dataset if the effective sample size determined was 50% (corresponding to a value of 0.9 in the RC-SS (RC-S) plot from Figure 3b) then the total ex- ecution time is equal to 1.1s (1.15s for RC-S) which is still below the baseline of 1.5s. This result is especially encour- aging and shows that even for a small dataset the approach presented can be quite effective.

As can be observed from Figures 4b-c, the results on the synthetic datasets are even better. For T1014DSM for a self similarity cut off of 0.95 or higher the optimal sample size is 50% (this can be seen from Figure 3c). The RC-SS ap- proach required a total of 448 (496 for RC-S) seconds to compute the results (see Figure 4b). The approach using the entire association set requires 1037 seconds (which is above the baseline of 950s) to compute the same result. This results in  a factor of improvment of 2.4 (2.1 over the base- line). For TSISD25M (Figure 4c) for a self similarity cut off of 0.95 or higher the optimal sample size is 10% (from Fig- ure 3d). The representative class approach requires a total     Comparielon of Seif-Similarity CUNOS: Gazallt)  0 10 20 30 40 50 BO 70 80 Sample size  (a) Gazelle Camperision Ot Sell-Similarity Curves: T1014D5M  s 0.8 .E 0.75 I 0.7  0.55  0.55  ,..- 0.1% RC-S -  0.6 O.l%A-S - O.l%A-SS - -  r~~  0.1% RC.SS  0 10 20 30 40 50 BO 70 80 Sample size  (b) VBwk Cornparision 01 Self-Similatity Curves: TBISDZSM  0.05% RC-S - 0.05% A-S -  0.05% A-SS ~ ~ D - -  ' 0.75 0.7  0.65  0.05% RC-SS ...~.

0 10 20 30 40 50 BO 70 80 Sample Sire  tc) T1014D5M  .- I 0.75 0.08% RC-S -  0.08% A-S - 0.08% A.SS ~0 ~~~  0.6 0.55 0.5  0.08% RC-SS ..... .

0 10 20 30 40 50 BO 70 Bo  Sample size  (d) TBI5D25M Figure 3. Evaluation of Proposed Method: Similarity plots  PerbmanceCawkkn RCn. A VT!d P&manceCanpamlon: UCn.  A:T815025M IC  f F s Y i f  E  0.1  ~, ,,C' 9 . '  0.1XA - ,k -_  o.l%Rcs - wm 0.lXRCSS ...... 0.IXRC-S -- I " " " " ' IO M B U) M MI 70 80 W I W  0 10 M 30 10 M MI 70 80 90100  saw I O  sanp1e sa0 Sam* sa* (a) VBook (b) T1014D5M (c )  T815D25M  Figure 4. Impact of Representative Set on Performance  Sampling MethOdolcgy Performance: T1014D5M:0.05% sup SBrnpling Melhodobgy Breakdown: T1014D5M:0.05%  0 10 20 30 40 50 80 70 80 90 1W 0 10 20 30 40 50 60 70 80 90 100 Sample Sue Sample Size  (a) Overall Performance (b) Breakdown Figure 5. Sampling Methodology on Performance Breakdown     of 73.17 (125 for RC-SS) seconds to compute the results.

The approach using the entire association set requires 140 seconds to compute the same result. When compared with the baseline execution time (1730s) the representative class approach?s improvement factor is around 25.

Impact of Sampling Methodology: In Figure 5 we con- sider the performance of the sampling methodology pre- sented in Section 3. Figure 5a compares the cumulative execution time performance of our the approach using the representative class to identify the ideal sample size while overlapping the sampling and U 0  operations of the next stage with the computation of the current stage. In Figure 5a we compare the performance of ideal situation (where there is no sampling overhead TC-ideal) with the actual perfor- mance with overlapping (TC-overlap) and without overlap- ping (TC-nooverlap). For the overlapping computation we used two scenarios, one where the processor handling the WO and sampling was a 300Mhz Pentium 113 (i.e. a more realistic scenario for active disks with a much slower pro- cessor than the compute processor) and the other where the U 0  processor was lGHz (ideal baseline).

On viewing the graphs it is clear that the TC-ideal graph  and the TC-overlap (1GHz) are almost identical, reflecting the fact that almost all of the sampling overhead is over- lapped with useful computation (computing the association set for the previous sample size). On relaxing the assump- tion and allowing for the fact that the processor doing the sampling and WO is often much slower (300 Mhz) we ob- serve a marginal drop in performance (slightly under 4%), so still most of the sampling overhead is overlapped with useful computation. Note that if we did not overlap the sampling overhead with computation then we perform 10- 12% worse than the ideal (comparing the TC-Ideal and TC- noverlap graphs). This experiment assumes the best pos- sible sampling algorithm (the Sample A algorithm). We further broke down the performance of the sampling over- heads in Figure 5b for the two configurations (1GHz and 300Mhz). The performance of the naive algorithm (see Sec- tion 3) is much worse than the Sample A algorithm.

5 Conclusions and Future Work We have presented an efficient method to progressively  sample for association rules. Our approach relies on a novel measure of model accuracy (self-similarity of associations across progressive samples), the identification of a represen- tative class of frequent itemsets that mimic (extremely accu- rately) the self-similarity values across the entire set of asso- ciations, and an efficient sampling methodology that hides  3We did not have a dual processor system where one processor was fast and the other was slow. We timed the sampling overheads for each of the different sample sizes for his dataset on the slower machine, and we used these times (by precomputing the samples and busy waiting for the necessary amount of time) while evaluating the performance on this configuration.

the overhead of obtaining progressive samples by overlap- ping it with useful computation. We evaluated the results on a set of real and synthetic datasets. We extensively benchmarked each aspect of our algorithm and obtained uniformly good performance (several factor-fold execution time improvements) across both real and synthetic datasets.

In the current work we have considered each sample to be independent of the other. We would like to see if the pro- posed method can be improved by using adaptive sampling techniques[S]. Other directions of future work have been outlined already in the text.

