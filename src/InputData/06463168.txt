

Abstract ----The view that sampling technology could improve the efficiency of data mining significantly has been widely accepted by the research community.  The key to sample in data mining is how to design a sampling strategy to get a favorable sample to execute the mining algorithm at minor cost of accuracy. In this article we propose a progressive sampling algorithm based on confusion matrix to determine the optimal sample size. The novelty of this algorithm is that it can find the appropriate sample very quickly and very accurately without executing the data mining.



I. INTRODUCTION HE volume of electronically accessible data in warehouses and on the internet is growing faster than the speedup in processing times predicted by Moore?s law  [1]. Scalability of mining algorithms is therefore a major concern. Classical mining algorithms that require one or more passes over the entire database can take hours or even days to execute and in the future this problem will still worsen. One approach to the scalability problem is to exploit the fact that approximate answers often suffice and execute mining algorithms over a ?synopsis? or ?sketch?. Using a sample of the data as the synopses is the popular technique that can scale well as the data grows. Besides having desirable scaling properties, sampling is also suited to interactive exploration of massive data sets. Recent work in the area of approximate aggregation [2] processing shows that the benefits of sampling are most fully realized when the sampling technique is tailored to the specific problem at hand.

Association rule mining is one of the most important technique which aims at extracting interesting correlations in the transaction databases or other data mining repositories.

Among the areas of data mining, the problem of deriving associations has received a great of attention. The problem was formulated by Agarwal et al [3] in 1993 and is often referred to as market-basket analysis. Association rule mining is to find out association rules that satisfied the predefined minimum support and confidence from a given database. The problem is usually decomposed into two sub problems. One is to find frequent or large item sets whose occurrences exceed a predefined threshold in the database. The second problem is   This work was supported by the National Natural Science Foundation of  China for the Youth under Grant No. 11001248.

X Xie is with the School of Statistics and Mathematics of Zhejiang  Gongshang  University , Hangzhou, Zhejiang, PRC (Phone 18857957789 e-mail: xxy2003@zjnu.cn).

Y Zhang   was with College of Mathematics, Physics and Information Engineering of Zhejiang Normal University, Jinhua, Zhejiang, PRC  (e-mail: znuzy@zjnu.cn).

Y Xu   was with College of Mathematics, Physics and Information Engineering of Zhejiang Normal University, Jinhua, Zhejiang, PRC  (e-mail: xyt@zjnu.cn).

to generate association rules from those large item sets with constraints of minimal confidence.



II. RELATED WORKS To fasten the speed of association mining when size of the  database is too large, sampling technology has been used in this area popularly.

Parthasarathy [4] proposed an efficient method to progressively sample for association rules. This approach relies on a novel measure called model accuracy (self-similarity of association across progressive samples),the identification of a representative class of frequent item sets that mimic the self-similarity values across the entire set of association and an efficient sampling methodology that hides the overhead of obtaining progressive samples by overlapping it with useful computation.

Chuang et al [5] presented another progressive algorithm called Sampling Error Estimation (SEE) which aims to identify an appropriate sample size for mining association rules. SEE has two advantages (1). SEE is highly efficient because an appropriate sampling size can be determined without the need of executing association rules. (2).The identified sample size of SEE is very accurate, the association rules can be highly efficiently executed on a sample of this size to obtain a sufficiently accurate result.

Bronnimann et al [6] explored another sampling algorithm called epsilon approximation: sample enabled (EASE). EASE uses Epsilon approximation methods to obtain the final sub sample by process of repeated halving. This algorithm can process transactions on the fly, i.e a transaction needs to be examined only once to determine whether it belongs to the final sub sample.

Basel et al [7] recently presented a parameterized sampling algorithm for association rule mining. This algorithm extracts sample datasets based on three parameters: transaction frequency, transaction length and transaction frequency length and it empirically shown that it achieves 98% accuracy which outperform two-phase algorithm [8].

Most of algorithms mentioned before have to execute the association rule mining during the course of sampling which could lose the benefit of sampling for the cost of the designed algorithms will outpace the saving cost by sampling. For this reason, in this article we design a novel association rule mining algorithm based on progressive sampling without executing the mining course.



III. OUR WORKS A.   Confusion Matrix Supervised Machine Learning (ML) has several ways of evaluating the performance of learning algorithms and the classifiers they produce. Measures of the quality of  Sampling learning based Association Rules Mining Algorithm Xiaoying Xie, Ying Zhang and Yingtao Xu  T  October 18-20, 2012 Nanjing, Jiangsu, China          classification are built from a confusion matrix which records correctly and incorrectly recognized examples for each class.

Table 1 presents a confusion matrix for binary classification, where TP is true positive, FP is false positive, FN is false negative, and TN is true negative counts.(Table 1)  True Positive Rate (TPR) (or Recall R) is the proportion of positive examples correctly classi?ed as belonging to the positive class, determined by:  TPR = R = TP / (TP + FN)                         (1) Precision (P) is the positive predictive value determined by:  P = TP/ (TP + FP)                                       (2)   True Negative Rate (TNR) is the proportion of negative examples correctly classi?ed as belonging to the negative class, determined by:  TNR = TN / (TPN+ FP)                             (3)   TABLE I.

CONFUSION MATRIX FOR BINARY CLASSIFICATION  Class/Recognized As Positive As Negative  Positive TP FN Negative FP TN  B. Learning Curve Progressive sampling starts with a small sample and uses  progressively larger ones until model accuracy no longer improves. A central component of progressive sampling is a sampling schedule  0 1 2{ , , ... }kS n n n n= where each in  is an integer that specifies the size of a sample to be provided to an induction algorithm. For i < j, n ni j< .  If the data set  contains N instances in total, ni < N for all i .

A learning curve (Figure 1) depicts the relationship  between sample size and model accuracy. The horizontal axis represents the number of instances in a given training set which can vary between zero and N, the total number of available instances is N. The vertical axis represents the accuracy of the model produced by an induction algorithm when given a training set of size n.

Learning curves typically have a steeply sloping portion early in the curve, a more gently sloping middle portion, and a plateau late in the curve. The middle portion can be extremely large in some curves and almost entirely missing in others.

The plateau occurs when adding additional data instances does not improve accuracy. The plateau, and even the entire middle portion, can be missing from curves when N is not sufficiently large.  Conversely, the plateau region can constitute the majority of curves when N is very large.

C. SAMPLING LEARNING BASED ASSOCIATION RULES MINING 1) Description of our algorithm  During the association rules mining, sampling error means to the phenomenon that the support of each item in the sample will deviate from its support in the entire data [10], will result in incorrectly identifying whether an item is a frequent item.

In   Fig. 1.   Learning curves and progressive sample  2) Description of our algorithm During the association rules mining, sampling error means  to the phenomenon that the support of each item in the sample will deviate from its support in the entire data [10], will result in incorrectly identifying whether an item is a frequent item.

In this way, there are four results when sampling is realized for any item: (1) the item is frequent in the sample where it is really frequent in the entire data; (2) the item is not frequent in the sample where it is really not frequent in the entire data; (3) the item is frequent in the sample where it is not frequent in the entire data; (4) the item is not frequent in the sample where it is  frequent in the entire data; As the theory of confusion matrix, we could define the above four result as TP, TN, FP, FN. When sampling error is zero, then TP(s)+TN(s)=N, FP(s)=FN(s)=0, where N means the size of entire data and s means a sample.

Therefore, if sampling errors can be significantly de-creased when the sample size is larger than a sample size si, si can be suggested as the appropriate sample size for association rules. Such a size can be determined by generating a curve of sampling accuracy or sampling error versus the sample size. In addition, the corresponding sample size at the convergence point of the curve will be suggested as the appropriate sample size. In the following, we present the method to generate the optimal sample size.

( ) ( ) ( )  ( ) ( ) ( ) ( ) TP S TN Si iF Si TP S TN S FP S FN Si i i i  + =  + + + (4)  The formula (4) reflects the proportion between the number of correctly determined frequency items and the number of all items in the same sample. When ( ) 1F Si = , it means that the frequency of all items in this sample is identical to the entire dataset, so this sample could be used to association rule mining instead of the overall. In practical, we stop sampling when ( )F Si is close to 1.

3) Steps of algorithm In this section, we will design the steps of algorithm to get  the optimal sample whose size is the inflection point of the learning curve which was produced based on the formula (4).

Step 1: Traversing the entire date and generating random sample sequence { }, 1, 2, 3S i ni = ? ? ? , whose size  is 200 *0S n ii = + ; Step 2: Calculating the support of all items in the sample  and F value of samples; Step 3: Defaulting support threshold of all sample and  entire dataset;          Step4: Stopping sampling until ( ) ( ) ( ) 11 2F S F S F Sm m m= = ?+ + , then oss= Sm  4) Empirical Analysis This algorithm is realized in Windows XP system, 1.23 G  memory, 1.70 GHz processor PC, the data is download from free UCI (data mining laboratory data),which is part of the population census data set containing 32560 records. As the result in figure2, the optimal sample size is 22000. In this figure, we would find there are two peak in learning curve, one is at the point of size 2000, the other is at the point of size 22000. The reason of this result is that: when the size of sample is too small, the value of fp and fn is very small for the random sampling could not get enough information about the entire dataset. So the sample size 2000 is not the optimal sample size. That means if we choose the sample whose size is 22000 produced by the progressive sampling algorithm, the result of association rule mining is almost the same between the sample and the entire dataset, to do that we could save the cost of time and space.

All association rule mining results in different samples and entire data are displayed in table 2, table 3, from which we could find the efficiency and accuracy of the progressive sampling algorithm in this paper.

In table 2, we list the frequent items of 1,2,3,4,5,6 for different sample size from which we could find that the number of frequent items at size 22000 is very close to the entire dataset whose size is 32560. Since all association rules are produced based on the frequent items, if most of the frequent items are identical in these two samples, the association rules mining results are almost the same. This point of view was verified in talbe3. In table 3, it?s obvious that the number of association rules of different sample size such as 22000 and 32560 is almost equal. In other words, if we replaced the entire dataset with the optimal sample, we get the nearly same number rules.

D. UNITS Use either SI (MKS) or CGS as primary units. (SI units    Fig. 2.   Learning curves of the designed algorithm

IV. Conclusion As we all know, sampling is a simple and effective  technique to improve the mining efficiency. In this article, we quote the confusion matrix to redefine the sample error of the frequency of items between the samples and entire data. We design a formula to draw the learning curve to get the optimal sample size to instead of the entire data. The experimental results show that this algorithm is effective and robust.

TABLE II.

THE NUMBER OF FREQUENT ITEMS WHEN F(SI)=1 Sample size        Number of Frequent Items  L(1) L(2) L(3) L(4) L(5) L(6) 1800  13 40 50 29 8 1 2000 13 40 54 31 9 1  22000 13 42 52 27 7 1 26000 13 42 52 26 7 1 32560 13 42 52 26 7 1   TABLE III.

