Knowledge Integration of Distributed Enterprises  using Cloud based Big Data Analytics

Abstract? an efficient and adaptive centralization of disparate knowledge sources is a key factor for the success of knowledge management systems (KMS) in distributed heterogeneous enterprises such as healthcare and production systems with large scale and disparate knowledge sources. Professional end users of such systems (e.g. physicians and production engineers) should have nearly real-time access to the records to provide relevant decisions without any difficulty of dealing with IT systems.

Analysis and reasoning of professional end users? feedback lead helpful knowledge bases which can be used for future decision making situations such as diagnostics and predictions. This paper represents an overview of the cloud computing based conceptual framework for the integration of distributed knowledge sources in large scale heterogeneous enterprises such as grouped hospitals, clinic chains, production systems, supply chains and etc. The NoSQL database is used for providing a scalable data analytics. This framework facilitates accessible, efficient and always available knowledge bases for collaborative systems and reduces redundancy and costs by sharing the knowledge between individuals and experts. Three different application scenarios will be used to test and evaluate the efficiency of the proposed framework. Distributed product design and development, healthcare knowledge integration and providing readability in drug leaflets are target application domains of this research.

Keywords?Knowledge Integration; Cloud Computing; Secure Knowledge Centralization; Healthcare Knowledge Management;

I.  INTRODUCTION AND REQUIREMENT ANANLYSIS Centralization of the knowledge and expertise in an  efficient way is a key issue in collaborative and heterogeneously distributed environments, especially for Large Scale Enterprises (LSE) with huge demands for processing and storage resources [1]. Such Enterprises should have a convenient and powerful platform for the dramatically growing volume of data and knowledge [2] with sufficient security considerations. Utilization of technologies such as Service Oriented Architecture (SOA), grid [3], cloud computing and big data analytics [30] can improve efficiency of these IT-based applications and have been used increasingly in the last few years. Knowledge Integration (KI) [4] applications are data intensive processing and they need higher volumes of computing and storage resources as well as scalable software solutions. In addition, LSEs are currently dealing with so much IT related complexity, such as software  development, updates, licenses, energy costs and hardware upgrades. Having customized, pre-configured, and ready to use services provided by IT companies could help these enterprises to focus on their core competencies and improve Quality of Service (QoS) in their professional activities.

On the other hand, in the nowadays competitive market world, enterprises are in an effort to reduce costs, time to market and increase customer satisfaction as well as to cut the physical and geographical distance between professional and individual end users to support their individual users more efficient in a real-time and cost-effective way [5]. Knowledge transfer and reuse is effectively contributed in achieving these goals. Markus [6] defined knowledge reuse as a creation and use of written and computer-based records for future access, retrieve and reuse of knowledge and expertise. She identified three roles in knowledge reuse process: Knowledge Producer, Intermediary and consumer. Knowledge intermediary consists of activities such as indexing, categorization, standardizing, publishing, mapping, merging and etc. This step has a severe impact on the success of knowledge reuse due to its diverse functions in order to the correct allocation of knowledge sources to its consumers in a right time, place and situation. A consideration of knowledge reuse needs a correct understanding of what knowledge is. In practice, the term data, information and knowledge are used interchangeably, but giving a correct definition of knowledge is possible by distinguishing between knowledge, information and data [7]. A commonly held view, including minor variants is that data is raw numbers and facts, information is processed data [7], and knowledge is the result of learning and reasoning.

Integrated Knowledge Bases (KBs) can help, for instance discovering new disease, smarter medical research activities and lead to improved medical decision-making in healthcare organizations and medical centers. However, knowledge retrieval and healthcare information sources such as Electronic Medical Records (EMRs) [8], Picture Archiving and Communication Systems (PACS) [9] are heterogeneously distributed and need higher storage capacity and should be kept for a long period of time which imposes more complexity and resource needs. Meanwhile, another important challenge in such large scale distributed enterprises is the overheads such as time, expenses, and interoperability in transferring the knowledge from one center to another due to the lack of inter- operation interface between grouped centers. As an example, in      medical scenario, as patients may not enter always the same hospital a problem arises about transferring knowledge about the patient from one hospital to another. This is normally done by postal services or the patient himself by carrying physical media (e.g. knowledge and information about former diagnostics and medications) from one location to another.

Data privacy is the main limitation that doesn?t allow hospitals to share the data between each other.  A serious problem can arise in case of incomplete knowledge if patients are to undergo further medication or treatment. In this case even the patient?s life can be put at risk if, for example, medical staff takes steps which cause complications due to former medication and treatment. These overheads are because of diversified work environments and internal bureaucracies, standards and distributed storage of records and lack of interoperation interfaces and centralized KBs in collaborative LSEs.

Additionally, ensuring the confidentiality of information (e.g.

Patient records, relevant diagnostics, lab results in medicine and states of machineries, processes in product development), whilst at the same time allowing authorized experts (e.g.

physicians and production engineers) to access it conveniently, is a crucial requirement. This includes all steps of creation of datasets to their deletion in a safe and unrecoverable way.

Additionally, data has to be protected against unauthorized manipulation [10].

LSEs such as grouped medical centers or production chains have millions of users (e.g. patients, medical staff, managers and physicians in medical enterprises and engineers, employees, manufacturers and managers in production enterprises) to access knowledge sources for updating, processing, deleting and/or analyzing activities. They are faced with a large amount of data, which is mainly derived from the instruments and the staff. Therefore, these enterprises need a persistent storage and analytics for such large amounts of data and knowledge; through cloning of the disk content, the data may be made locally for many instances.  Elasticity of platforms which allows them to allocate a lot or next to none resources to a specific task is a key for nowadays information processing. Cloud based big data analytics is being used by such LSEs for development of elastic services for integrating, storing and processing of large scale disparate data and knowledge sources. Cloud computing is a model for enabling convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction [11]. Cloud computing provides scalable, reliable, and highly cost effective services at multiple granularities for a specified level of quality (of service) [12].

Enterprises can create new services by dynamically provisioning compute and storage services and offer as their own isolated or composite cloud services to other users or Small and Medium Enterprises (SMEs) [13]. Depending on the use case, security, and financial issues arise, services can be deployed in public, private or hybrid clouds.

The main focus of this paper is the concept of on-going research in developing a service for centralization of distributed Knowledge as well as interoperation interface between linked and collaborative centers in large enterprises  using cloud based big data analytics. The rest of the paper is structured as follows: related works to the concept (Section II) are abstracted in the next section. These related works and projects are about knowledge integration in the fields of Medicine and production, which are in connection to the application domains of the concept. Section III describes the architectural details and security measures of the proposed framework. Healthcare knowledge integration, centralization of product design and development knowledge and providing readability in drug leaflets using the proposed framework in this paper are three application scenarios described in Section IV to test and evaluate the efficiency of the framework.

Different examples of application scenarios are employed through the paper for clarification of points such as problems, challenges and solutions.



II. RELATED WORK Xuan et al. [14] presented an Activity-Oriented Access  Control (AOAC) to ubiquitous hospital information and services. The proposed AOAC model in this research increases efficiency of medical professionals by protecting the confidentiality of health information and allowing physicians to browse medical data at the point-of-care. AOAC framework is implemented as a client-server based solution which runs inside of the hospital. Agarwal et al. [15] presented a framework for a cloud computing based patient medical information system.

Different Care Delivery Organizations (CDOs) can have an access to patient information from different locations. This project provides a service oriented architecture for interfacing medical messages. Higher availability and application scalability and low level security are some highlights of this project. Al-Busaidi et al. [16] studied personalization of web information for patients by integrating patient medical data in Information System for Clinical Organization (ISCO) with relevant health information on the web via a patient personal knowledge base.  Multilayer client/server architecture is used to integrate patient database with the web.  Microsoft Vault health solution [17] is a commercial related work to the proposed framework. This solution provides an integration platform for medical information and history of patients.

Consumer (e.g. patient) is supposed as an owner of all relevant medical data. As a result, he has to get access to his profile and upload all medical records and update his medical profile himself. Therefore, medical centers are not capable of submitting medical records such as MRI files and etc. directly to the patient?s profile. Fitzgerald et al. [18] studied clear benefits of the emerging role of cloud computing in Healthcare Information System (HIS). They have described, quick startup of hospital-based and regional Healthcare Information Exchanges at low cost, improved quality of care, higher availability of Electronic Health Records (EHRs), reduced medical expenses and faster adoption of new applications as advantages of cloud based HIS.

Jiang et al. [19] proposed an ontology-based framework of knowledge integration to support business processes in collaborative manufacturing. They first defined ontology of product knowledge, which is composed of product design, process planning and manufacturing knowledge. Chen et al.

[20] proposed a distributed product knowledge service model     based on product lifecycle and its supply chain. Their proposed model is distributed, modular, flexible and product- oriented. Business Logic layer of this model itself covers Product Knowledge Service Functional Module, System Management & Control Functional Management Module (SMCFM) and Collaborative Organization Formation Functional Module.  Kern et al. [21] introduced a framework for internet supported inter-organizational product development. They used the internet as a basis for collaboration between partners. Revilla and Curry [22] developed a framework to test the impact of customer and supplier knowledge integration in product development performance. Based on their research, trust and learning culture influence the capability for knowledge integration in product development. Toussaint et al. [23] proposed a methodology to capture manufacturing knowledge and its application towards the design verification and validation of new engineering designs.

Being a mass customized and configurable to be used in different application domains as well as consideration of scalability, availability and accessibility are some of the additional features of the proposed framework in comparison to the above studied related works. As a result, distributed large scale enterprises will be able to support peak time processing without any need to buy further hardware or software solutions.

They can provision (rent) resources via cloud and release them after finishing the usage in order to have the advantage of scalability and pay-as-you-go model. Since cloud services support different platforms through virtualization and other techniques, they could also benefit on the accessibility of knowledge integration (KI) services.



III. ARCHITECTURE OF THE PROPOSED FRAMEWORK The proposed framework uses a multi-layered architecture  which offers services at different levels of abstraction. In this section we will give an overview of the layers and the functionalities they provide. The main focus of the proposed framework is integrating knowledge about different cases such as disease, manufacturing instruments and production processes from different sources in collaborative environments.

In addition, supporting a wide variety of application domains is another goal of the proposed framework. This goal is achieved by providing appropriate services that can be used as building blocks for situation- and use case-specific applications.

Knowledge Integration (KI) and Knowledge Search (KS) are included in the service layer. Integrated KBs are stored in the physical layer of the proposed architecture (figure 2). Platform security management and access control components of security layer support owners to secure information against unauthorized accesses.

Presentation Layer (PL) is a web-based interface at the front end of the framework which is the most visible layer to end- users. KBs can be instantly viewed by any authorized user connected to the cloud through PL. This layer uses web services and Mashup tools for serving clients via http and is based on open standards to enable multichannel deployment across different devices from desktop computers to smart phones. Mashup techniques are used to combine information  from more than one source into a single integrated storage tool. PL includes an administration console and a user portal.

PL provides a seamless compatible interoperation interface for linking distributed centers with different technological standard inter and intra enterprises.

Figure 1.  An Architectural overview on the Cloud-based Knowledge  Integration Framework  Platform as a Service (PaaS) layer of the framework consists of few main components, including Knowledge Integration (KI), Intelligent Knowledge Search and Data Integration and visualization. KI consists of some minor components such as knowledge mapping, merging, codification, representation, assessment and transfer.

Knowledge Codification is about detaching the knowledge from its source such as experts, books, articles, robots, Medias, etc. to make it easily accessible and transferable. In fact, it is the process of converting internal knowledge into accessible and applicable formats [24].

Knowledge Representation is based on techniques such as Semantic Networks (SN), trees, lists, or rule-based representation. As an example, SNs describe concepts and their relations using graph theory. Knowledge Assessment is validating and weighing the knowledge. It is attaching credibility, value, significance and weight to the knowledge.

Knowledge mapping component uses the history and logs to find the most similar knowledge for querying knowledge source using the similarity calculation algorithm (Figure 2) for concepts and relationships. Knowledge maps are used for the meta-knowledge component in the multi-layered architecture.

Profiling and navigated knowledge search is supported by the meta-knowledge component. Intelligent Knowledge Search aims to determine the possible sources of knowledge that are relevant to the cases (e.g. disease or machinery failures) under consideration. These sources may be the obvious repositories of enterprise KBs or even seemingly irrelevant sources that may contain useful information, like the social media, emails, technical reports, etc. The results of this component can be used in decision making by professional end users.

Figure 2.  Detailed overview on the process of Intelligent Knowledge Search  and Recommender component in proposed Framework  Figure 2 describes the process of Recommender Component and intelligent learning method in the proposed framework.

Each case is considered to be stored in the centralized KB with all relevant information such as symptoms as inputs to detect cases and relevant decisions as corrective actions such as maintenance in production. Case based reasoning is being used for recommender system. A recommender component of a proposed framework uses the knowledge mapping component to deliver decision alternatives to end users based on available knowledge sources in the system.  Belief revision and decision evaluation using end user feedback analysis is being used to improve the efficiency of the framework.

Infrastructure as a Service (IaaS) layer is the physical layer of the framework and works mainly for clouding. This layer can be multiplexed by the virtual infrastructure level to make the physical virtual by using virtualization technologies to scale more and support multi-platform possibility. The workload distribution component in this layer should efficiently provide the resources for the knowledge integration and storage services. Thus, dynamic Cloud management techniques should be investigated to facilitate scalable and elastic but at the same time cost efficient utilization of the Cloud resources.

The security of the proposed architecture is considered in two dimensions, namely platform and communication. The term platform describes the hardware and software components that are used internally to implement the proposed architecture. It includes for example servers and virtualization software on the IaaS as well as databases and Mashup tools on the PaaS layer. At a minimum this stack must consist of four components: A Knowledge Base (KB), a Virtual Machine Monitor (VMM), an Inference Engine (IE) and a Mashup Tool (MT).



IV. TEST AND EVALUATION SCENARIOS  A. Healthcare Knowledge Integration (HKI) Before discussing the scenario a brief look on standards is  necessary. Healthcare Information System (HIS) standards can be grouped into 5 categories: Vocabulary, Structure and  content, messaging, Visual integration, and Security Standards [25]. Messaging standards which have been considered in this research facilitates data and information exchange between different medical centers. Messaging standards are for instance Health Language Seven (HL7), Electronic data Interchange (EDI) X12, Health Insurance Portability and Accountability (HIPAA), Digital Image Communication in Medicine (DICOM) and Integrating Healthcare Enterprise (IHE) [26].

Proposed framework provides an interoperability platform for distributed medical centers with different messaging standards to efficiently transfer medical data and knowledge about patients.

The proposed framework aims to increase knowledge availability between hospitals. The targeted audiences are collaborating hospitals that are already interconnected and thus have similar privacy and security measures. As a consequence, our framework is a collaboration cloud which is exclusively accessible by entities belonging to the same organization.

Therefore, achieving compliance with the Health Insurance Portability and Accountability Act (HIPAA) is more than possible. Patients who agree for their data to be transferred to a cloud infrastructure can make sure that authorized medical staff from all participating hospitals can access it on demand. In case that a patient changes hospital for whatever reason, the data will be automatically available at the new hospital. This eliminates transfer times caused by postal services and avoids real dangers such as physical media being lost during transport.

Another opportunity that arises from this collaborative scenario is real-time processing of patient data to identify dangerous situations due to medication and treatment. In general, the condition of a patient is the result of the medication and treatment he/she experiences during the hospital stay. Certain combinations can lead to serious health problems and even cause death. However, the proposed solution may detect such dangerous states in the forehand and therefore raise an alarm whenever medical personnel wants to take any action that might harm the patient. This processing is done by a knowledge-based component in the collaboration cloud. Given a sufficient integration of medical instruments with the collaboration cloud, dangerous situations can be detected on the fly. All medical data will be kept in the cloud only with the explicit permission of the patient. At any time he/she can demand a full or partial deletion of the data which will cause the cloud infrastructure not only to safely delete all data in the cloud, but also demand all integrated medical devices to clear any cached data.

B. Knowledge Integration of Product Design and Development To achieve improved design efficiency, designers have the  advantage of being in contact with suppliers, the manufacturing process, partners and other sections to get feedback data, material information and so on. That is important to apply changes in the design process rather than manufacturing.  As shown in figure 1, collaboration in the design process can be visualized-based, co-design, and or hierarchical collaboration.

Visualization-based collaborative systems have been used to support light-weighted visualization, annotation and inspection of design models to provide assistance to the collaborative design activities. Co-design systems support interactive co-     design functions as teamwork [27]. Collaborative design team members can use standard communication protocols such as HTTP to share and get knowledge via web browsers from cloud deployed KBs.

Figure 3.   Collaborative product design [27]  Conceptual design, detailed design, special aimed design, and design verification step are the most knowledge intensive activities in product design process. In the conceptual design process the designer is dealing with the whole product or whole assemblies and works from a blank sheet of paper, generating and evaluating several ideas. Knowledge acquired in the design process is about how to undertake some action, whereas declarative (or factual) knowledge is about something [28]. The knowledge required to define specific components, including technical drawings and specifying, manufacturing requirements can be acquired in the detailed design process.

Design test results, challenging results from a formal design are the knowledge of design verification step. Special aimed design is a type of design to improve a considered factor of a product. Customer feedback is very important in this step. For example, reducing the costs or failure ratio for a specific product based on customer feedback is a special aimed design of next generation product.

C. Providing Readability as a Service There are several opportunities to transfer knowledge; the  most important barrier of knowledge transfer in this case is the ?understandability? and ?detect-ability? of information in a text. Thus information must be formulated that an averagely educated user can understand its content and follow its instructions.

Particularly this is the main goal in preparing and reviewing package insert leaflets of drugs. The scientific institute of the health insurance company, AOK, showed package leaflets of the 100 most commonly prescribed medicines to 70 test subjects. The result pointed out that the overwhelming majority of consumers does read the patient information before taking the medicine, but the information contained is regarded by most patients as not very helpful. [29] According to the actual situation in revising and improving packaging insert leaflets for drugs, the aim of this scenario is to assist revisers to improve  the readability of new and of existing packages to insert leaflets. Based on linguistic sciences, a criteria catalogue has been evaluated in which different kind of criteria like ?Typographical Criteria? or ?Linguistic Criteria? has been consolidated [30].

As a part of this assistance system, a standalone readability web service was designed. In comparison to the full version of the assistance system, the web service is a simplified version for cloud purposes. Figure 4 shows the activity diagram of the web service which can be included into a web portal or an application. It shows that an author can check a text on demand via these different kinds of applications.

Figure 4.  ?Readability as a Service? architecture  The cloud based readability service is called up via the internet and is analyzing the text based on the readability criteria catalogue. Because of the different kind of criteria, there are several check methods. Exemplary for a sentence analysis, the identification of non comprehensible terms will be mentioned. Every word in a text will be rated to find all non comprehensible terms in a text by using a text corpus, which was built up with a modified TF-IDF method. For the readability word weighting, journals from many different areas were analyzed to create this text corpus. The aim was to sort words of the German language to its level of familiarity. The wide selection of magazines makes sure that a list of words was created, which are used by the majority of the population. On the basis of this list, each word within a leaflet was evaluated according to its popularity.

However, finally a report, coded in XML, is generated and sent back to web portals or desktop applications. This report includes not only the analysis result of the paragraphs and sentence, but also information to present an author, a graphical overview of a text and a recommendation of a sentence rearrangement. By using the proposed framework in this paper, developers can easily integrate the validated readability service.

As a result, authors can easily increase the quality of their text on the fly. In fact, this text improvement service will make texts much easier for readers to understand an informational text.



V. CONCLUSION The concept of a secure knowledge integration platform  between different units of distributed centers in large scale enterprises is presented in this paper. Cloud computing based infrastructure has been used for centralization of information and knowledge to facilitate accessibility of the system. As a result, experts can support individuals in real time and access to central knowledge and information anywhere. The proposed framework results an improved interoperability between different heterogeneous centers.  In addition the system is capable of detecting critical conditions and cases using stored historical data and rules and facilitates professional decision- making. Centralized DBs and KBs help medical centers to reduce medical redundancy and repetitive testing.

This research is divided into 7 work packages as follows: ? Research requirements analysis and evaluating  technologies for developing components ? Implementing prototypes ? Evaluation of prototypes and re-evaluating the  technologies ? Framework implementation ? Framework Integration and test ? Case studies on the system ? Standardization  At the moment we are considering and evaluating different software stacks according to the requirements described in the paper.

