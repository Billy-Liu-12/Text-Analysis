A NEW RULE-BASED VIDEO CLASSIFICATION APPROACH

Abstract: This paper addresses the problem of lower precision in  automatic video classification. A novel rule-based video classification approach is proposed. Firstly, in the video segmentation process, a set of video attributes is extracted to represent the content of video and a video attribute database are generated. Then the decision tree and class association rule mining techniques are performed on this video attribute database to extract a decision tree rule set and a class association rule set respectively. Lastly, a combination and pruning algorithm are applied to combine these two rule sets to generate a final classification rule set. The experimental result verifies the consistency of decision tree classification with class association classification. The result also shows the final combined rule set has higher classification precision than just one rule set.

Keywords:  Association Rule Automatic video classification; Decision Tree; Class  1 Introduction  Owing to the decreasing cost of storage devices, higher transmission rates, and improved compression techniques, digital video is becoming available at an ever- increasing rate. To help users find and retrieve relevant information effectively, and to facilitate new and better ways of entertainment and multimedia applications, advanced technologies need to be developed for analyzing, representation, indexing and semantic categorizing the vast amount of videos. Content-based video classification is an important technique, and its goal is to identify the video content and classify it to pre-defined classes.

Although digital video can be labeled at the production stage, there is still a need for automatic classification of videos [l]. First, lots of videos currently exist that the date has not been labeled. Second, and perhaps most importantly, content-based classification  approaches are the ultimate filter especially for broadcasting. Unlike labels and watermarks that are susceptible to human error and fraud, content-based approaches are dependent solely on the actual material. The only limit of content-based approaches is the accuracy of the system itself. Furthermore the video classification system can be considered as a final check, and complementary in providing useful additional meta-data to any static labeling system. Manual annotation or analysis of video is an expensive and arduous task that will not be able to keep up with this rapidly increasing volume of video data in the near future. So people need automatic classification technology to manage the videos and discover knowledge from videos. Efficient video classification can bridge the gap between the video low- level features and its high-level semantic features, and thus facilitate the indexing of video databases, video summarization and on-line video filtering based on the user's profile [2] .

This paper attempts to address the problem of lower precision and efficiency in automatic video classification and present a novel rule based video classification approach, which combine decision tree (DT) mining and class association rules (CARS) mining to do the content- based automatic video classification. The remainder of the paper is arranged as follows. In section 2 the previous works related to video classification are briefly introduced.

The video segmentation and video attributes extraction in our method is described in section 3. The main part of our method is described in detail in section 4. The experimental results are reported in section 5. The last section is the conclusions of the paper.

2 Relative works  Digital video can be regarded as being made up of genres or classes according to content of video. The genre of video is the broad class to which videos may belong e.g.

sport, news, movie etc. Genre can themselves in turn be  0-7803-7865-2/03/$17.00 02003 IEEE  mailto:hotmail.com    made up of genres. Genre classifications at the same levels are mutually exclusive. So the video genres can be regarded as a tree structure. The brief history of the topic of genre classification shows that the genres tackled include: cartoon (3, news (S), commercial (6), music (2) and sport (S), where the number in brackets shows the number of occurrences of each genre [3].

Automatic content-based video classification technology has become an active research area in last few years. There are many approaches ranging from low-level, limited environment, event detection through to high-level, broad environment genre classification. These approaches can be based on static features, dynamic features or a combination of the two [l].

If the environment is limited then the classification task can become more specific. In this case the success of video classification relies on the input video being constrained to pre-classified classes such as news, music video and sports etc. For example, Zhou et a1 [2] investigates the use of video content analysis, feature extraction and clustering techniques for further video semantic classifications and a supervised rule-based video classification system for basketball is proposed.

If the input environment is relatively unlimited the classification approaches are those techniques that separate the video into pre-defined categories. In this case there is a great deal of variation. First, there is variation in the categories of video, in number and type. Some popular categories are sports, news, cartoons and commercials, etc.

An approach presented by Truong et a1 [4] includes all these categories plus music videos. This approach propose set of computational features originating from authors? study of editing effects, motion, and color used in videos for the task of automatic video classification and use decision tree to verify these features. A knowledge based approach presented by Chen et a1 [5] use a forward- chaining rule-based production system approach for their knowledge-based system. The authors developed the rules in the knowledge base after reviewing many video sequences from the five classes.

These approaches are mainly, however, applied to off- line content-based retrieval of video databases only and cannot construct efficient rule based classification knowledge base efficiently. As for the other applications such as real time Internet video streaming and on-line video indexing and filtering, generic models for all these videos are hard to establish and generally require fast and efficient content analysis, efficient semantic classification. Besides, because of the varied nature of the data, a specific data model is not effective for generic purposes. As a result, a  generic, fast and efficient framework for video classification still needs to be developed [2].

3 Video Segmentation and attribute extraction  Video consists of a series of images. An image is called a frame. A shot is defined as an unbroken sequence of frames taken from one camera [6]. It is the building block and a physical entity of a video and is delimited by shot boundaries. Using computer to detect and identify the shot boundary in a video is called automatic video segmentation or shot boundary detection, which is the foundation of many applications such as video classification, management, browsing, and retrieval.

In general, automatic video segmentation techniques can be classified into two categories, i.e. uncompressed domain and compressed domain [6]. Based on the metrics used to detect the difference between successive frames, the algorithms in uncompressed domain can be divided broadly into three categories: pixel, block-based and hisiogram comparisons. So far, the histogram-based approach is the most popular approach in uncompressed domain. Several researchers claim that it achieves good trade-off between accuracy and speed [6][7]. In this paper, we use an approach similar to the one used by Zhang et a1 [7]. We modify the algorithm by using a sliding window and using automatic dynamic threshold and just need one pass to scan the frame sequence [SI.

Selecting the attributes correctly is the key to success of automatic video classification. The attributes selected from video content should be able to represent the features of the video class. The approach in this paper selects video length, the shot transition mode, average shot length and some other color statistics of video content as the test attributes. We select these static video attributes to represent video because they can reflect the features of video and extracting them just need lower cost.

Consider a set of video clipsV = (vi,v2, ..., vN,}, where v, is the identifier of ith video clip. The selected attributes of video v, include:  The length of video clip in frames; Number of shots; Average shot length in frames; The percentage of cut; The average of percentiles of key frame position in shots; The average of color histogram differences between beginning and end of every shots; The average of color histogram difference between two successive frames of the whole video; The average luminance of the video in RGB color space; The average luminance standard deviation of the video in RGB color space.

A large number of videos belonging to various genres are analyzed and their attributes are extracted in the video segmentation process. Suppose the number of video attributes is Nu, the number of video classes is N, and c = {c ,  ,C *,..., c } are the set of video class labels where c, is a label of a video class. Each video identifier, its Nu attributes and its class label together form a (Na+2) tuple to represent this video. We call this (Nu+2) tuple video attribute tuple. Let video v, belong to class c,, then the video attribute tuple of v, is:  (1) Where all is the value of j r h  attribute of video vz, v, can  be the identifier of this tuple. The set of all video attribute tuples form a relational table and is saved into video attribute database D.

Most video attributes are continuous value but the decision tree rules are Boolean association rules and use categorical attribute values. In the same time, the decision tree rules and class association rules should have same format in order to compare them. It should use Categorical attributes to mine these two kinds of rules. So the continuous . attributes should be discretized here. A categorical version of video attribute database D, is formed after the original video attribute database D is discretized.

All decision tree rules and class association rules are mined on this categorical database.

There are many methods for discretization of continuous attributes and this paper use the Entropy method [9]. Let the irh attribute a, be continuous and it can be discretized into Nu, value intervals after the process of  discretization. If the attribute a, is categorical originally but has very fine granularity of value and has too many various values, it should be clustered in order to reduce the number of various values. Attribute value U,' in expression (1) can be replaced by (attribute, value) pair, namely (a,, f , ,  ) , after  all attribute values are discretized or clustered, where f , ,  is  the j,, value interval of i rh attribute. We call each (attribute, value) pair a video attribute item. All various (attribute, value) pairs constitute elementary item set Z={ (a,,f, ) 11G5Nu, 15jS N ,  }. All the possible elementary  items are mapped to a set of consecutive positive integers to facilitate data mining.

N ,  (v, I a; 9 a; ,..., a", 9 c, )  i  4 The proposed video classification approach  The proposed automatic video classification based on data mining use decision tree and class association rule  mining to extract a set of decision tree rules and a set of class association rules respectively, then these two sets of rules are combined to generate a final classification rule set  4.1 Extracting the decision tree rules  A decision tree (DT) is a flow-chart tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and leaf nodes represent classes or class distributions. The topmost node in a tree is the root node. In order to classify an unknown sample, the attribute values of the sample are tested against the decision tree. A path is traced from the root to a leaf node that holds the class prediction for the sample.

Decision tree can easily be converted to classification rules.

The popular decision tree induction algorithm C4.5 [lo] is used in our approach. The categorical video attribute database D, is used as training set to construct the DT in order to make the decision tree rules (DTRs) consistent with CARS in terms of the format of rules. The DTRs are then extracted from the decision tree and a DTR set Y?d is formed. The format of DTR is the form of classification IF- THEN rules. The left-hand-side of the rule ("LF" part) consists of conjunction of (attribute, value) pair. The right- hand-side of the rule ("THEN" part) consists of the class prediction of the video. The DTRs have the format showed in expression (2):  ( 2 )  Where a, and f , ,  are is the irh attribute and its j t h  value  interval respectively; and (a, = f , , )  denote a test that the  value of a, equal f,, ; c,is the class prediction of the video with 1 I m I N ,  ; A is conjunction or "AND" relationship.

The rule means that if all attribute in the left-hand-side of the rule equal their corresponding value then the video belong to class c,. The precision p of the prediction of the rule can be measured by expression (3):  A (a, = A , >  +Cm [PI K 6 N ,  p = t/w (3) Where t is the number of videos that belong to class c,  and be predict correctly in the process of extracting DTRS; w is the number of videos that be classified into class c, in the process of extracting DTRs.

4.2 Extracting the class association rules  It's been a long time to use association rule for classification. Integrating the classification rule mining with association rule mining for classification is called associative classification [ 111. We refer to the association      rule whose right-hand-side is restricted to the classification class attribute as class association rules (CARs). Recent studies propose the extraction of a set of high quality association rules from the training data set that satisfy certain user-specified frequency and confidence thresholds.

Effective and efficient classifiers have been built by careful selection of rules. Liu et a1 [11] firstly proposed classification integrating classification and association rule.

It is an effective and efficient method and overcome some constraints introduced by a decision tree induction method that examines one variable at a time. Li et a1 [12] presented a new associative classification that improved previous methods by performing classification using multiple strong association rules.

Let TED,  be a video attribute tuple and X a  is the set of video attribute item. We say that T contains X i f  and only if X c T .  A video class association rule (CAR) is an implication of the form r :  X )c ,, where X d ,  C,E C is a class label; s and c are support and confidence respectively. The support is the ratio of the number of cases in D, containing X and having class label c, versus the total number of cases in D,, denoted as s=sup(r). The confidence c is the ratio of the number of cases in 0, containing X and having class label c, versus the total number of cases containing X, denoted as c=conflr). The goal of class association rules mining is to find all rules that satisfy both the user-specified minimum support threshold and minimum confidence threshold. The task of mining CARs in this paper can be decompose into two steps:  (1) Using the CBA-RG algorithm [ 111 to generate all CARS.

(2) Pruning redundant and ineffective rules and extracting effective rules. The number of the C A R s  can be huge, and there may be many redundant and ineffective rules in the set of CARs and some CARs are inconsistent with each other. So we must prune these ineffective rules from the set of CARs. We use a method similar to the pruning process of CMAR [12] to prune rules and put the effective rules after pruning into the CAR set %,.

The CAR used to classify video has the following format in our approach:  ?,  .

{(a,,f,,>l(a,,f,,)E 1 J I i 5 n I  s,c > c, (4) Where (a , ,  f,, ) is the pair of i r k  video attribute and its  j t k  value interval; c, is the class prediction of the video with 1 I m I N, ; s and c is support and confidence respectively.

Expression (4) means that if a video contains all video attribute items in th,e left-hand-side then this video belongs to class c, with possibility of c%. The CAR in (4) can also  be transformed into IF-THEN format with the same meaning.

4.3 Generating final classification rule set  Most DTRs may be consistent with most CARs in classifying video. But some DTRs may still conflict with some CARS in classifying video because they are extracted using decision tree and class association rule mining respectively with different principle. Most DTRs and CARs may, complement each other at a certain extent. So in this paper we propose an algorithm that combine these two rule sets in order to integrate the their consistency and eliminate their inconsistency and improve the precision of video classification.

We use R to denote the final rule set after combining Rd and R,. In order to combine Rd and R,, we should make the rules in the two set have same format. The formats of the two kinds of rules are shown in expression (2) and (4) respectively. The left-hand-side of the rule in expression (2) is the conjunction of some tests consisted of (attribute, value) pairs. If these conjunction of tests are transformed into the format similar to the one in expression (4), the two kinds of rules in expression (2) and (4) will entirely have same format. Besides of the left-hand-side of two kinds of rules, the precision metrics in the two kmds of rules are same in fact. The former use the precision defined in expression (3) and the latter use the confidence c. These two kinds of precision metrics have same meaning. The confidence c is also denoted as conflr). So the two kinds of rules are same completely. In addition, the frequency of DTRs is also computed and is denoted as sup(r), which have the same meaning as the support of CARs.

Before combining the two rule sets, a global order on the generated rules is defined according to the support and precision of the rules. Given two rules rl and r2, r1 is said having higher rank than rz, denoted as rpr2 ,  if and only if (1) conf(rl)>confr2) : 2) confTrI)=confTrZ) but sup(rl)>sup(r2) : or 3) conflrl)=conf(r2) and sup(rl)=sup(r2), but rI has fewer video attribute items in its left-hand-side than r2 dose. We design an algorithm C2S to combine both DTR set and CAR set. The algorithm needs firstly to rank all rules in the two rule sets according to this global order and we use a function sort(R) to do this job. In the algorithm, the return of function lef(r) is the video attribute items contained in the left-hand-side of rule r; the return of function cluss(r) is the video class label of rule r; the returns of function sup(r) and conflr) are the support and confidence of rule r respectively. The algorithm is showed as follow.

classify the unknown video.

5 Experimental results   Algorithm C2S: Combining sets o f  DTRs and CARs Input: DTR set Rd and CAR set R, Output: The final rule set R  (2) forteach rule rd E Rd) do (3)  for(each unmarked rule r, E R,) do (4) if(left(rd) = lef(r,)) then ( 5 )  if(cluss(rd) = cluss(r,)) then (6)  (7) (8) if(sup(rd)>sup(r,)) then (9) (10) (1 1) (12) (13) (14) (15) (16)  (17) (18) (19) endfor (20) endfor (21) for(each unmarked rule r,E R,) do (22) append r, to R ; (23) R = sort@) The goal of the algorithm is to compare the rules in  two rule sets and select the rules with higher confidence and prune the redundant rules. Firstly the two rule sets are ranked according to the global order (line 1). The algorithm compare all unmarked rules in R, with each rule rd in Rd. If there is a rule r, in R, that have same left-hand-side with rd and the two rules have same class label, then this both rules are same completely, we insert the rule with higher precision between this two rules into R and mark r, (line 4- 6); If two rules have same left-hand-side and precision but different class label, then the algorithm insert the rule with higher support into R and mark r, (line 7-10), otherwise the algorithm insert the rule with higher precision into R and mark r, (line 11-13). If the video attribute items in the left- hand-side of rd is covered by the left-hand-side of r,, we say rd is more general than r,, and if rd has higher precision than r,, then the r, is pruned by marking it and rd is insert into R; else if r, is more general than rd and has higher precision than r,, then the r, is inserted into R and marked (line 14-17). If all above tests is failed then the algorithm just insert rd into R (line 18). After the outmost loop is finished, all rules in Rd are processed but some rules in R, aren't processed yet. The algorithm then insert these unmarked rules in R, into R and sort all rules in R (line 21- 13). R is the final set of class rules that can be used to  (1) Rd = Sort(&); R, = sort(R,);  append rd to R with higher precision between two rules and mark r,;  else if(confird) = confir,)) then  append rd to R and mark r,; else append r, to R and mark r,;  else if(confTrd) 7 conf(r,))then append rd to R and mark r,;  else append r, to R and mark r,; else if(le$(rd) c lej?(r,) and confird) 2 confir,)) then  append rd to R and mark r,; else if(lef(r,) c lef(rd) and confir,) 2 conf(rd))  then append r, to R and mark r,;  else append rd to R ;  An experiment is performed to evaluate the accuracy and efficiency of our method. We collect 136 MPEG video clips from various sources. These video clips have their respective length, frame size and bit rate. These video clips belong to five genres or classes: love movie (25), action movie (14), commercials (34), music video (52) and soccer video (1 1). where the number in brackets shows the number of video clips of each genre. It is noticeable that the genre or class of a given video can be, and is often contested by reviewers. The determination of a genre is made by viewing the video content and often comes down to subjective views and semantic subtleties. However, when it comes to automate the process, researchers have deliberately chosen genre that are relatively well defined and commonly recognized. The video classes in our approach should be selected carefully because we just consider the static visual attributes of video. Any two predefined video classes should have distinct differences in static visual content. For example, the movie video can be defined into some sub-classes such as action, love, comedy, cartoon etc. The action movies is different from other kind of movies in average frame difference and shot length, but the comedy and love movies have little difference in visual content, so they should be defined into one video class.

Sport videos have many sub-classes such as football, basketball, and volleyball etc. These matches have complete difference in visual content, so they must be defined into different video classes. We get the video attribute database D,, such as that described in section 3, after the process of video segmentation. The 80% of video attribute tuples in D, is selected randomly as training data set, the other 20% of tuples form the test data set.

We first construct the decision tree [4] and perform the class association rule mining [ll] on the training data set, then generate DTR set Rd and CAR set R, to validate the consistency of this two kinds of rule. In order to compare the method proposed by Truong et al [4] with ours, the experiment just use the video attributes mentioned in section 3 rather than the attributes used by Truong et a1 [4].

26 DTRs and 29 CARs are extracted respectively after the two mining process. Then the test data set is used to test these two kinds of rule respectively. The accuracy of class prediction is used to measure the precision of classification of these two sets of rule. The accuracy of using Rd to classify the test data set is 76.47% and the number of cases that be classified wrongly is 4; the accuracy of using Ra to      Rule set Rd  classify the test data set is 82.35% and the number of cases that is classified wrongly is 3. These results are showed in Table 1. Among the cases that are classified wrongly by the set of DTR, there is just one case that appears in the cases that is classified wrongly by the set of CAR. This point shows that there is complementary relation between the set of DTR and the set of CAR and we can combine them to improve the precision of class prediction.

Lastly, we compare the precision of classification of R with the precision of R d  and R, respectively. The two ride sets R d  and R, are combined using the algorithm C2S in section 4.3, and a new rule set R with 51 rules is gotten.

The accuracy of using R to classify the test data set is 88.23%. The results are also showed in Table 1. We find the final rule set R has higher precision classification than R d  and R,. This indicate that it is efficient to substitute the combining rule set R for individual Rd and Ra. to classify video  Table 1. Experimental results Rule No Precision I Errors 26 76.47% [ 4  I ~ R, I 29 I 82.35% I 3 I R 151 1 88.23% 1 2  The experiment results show that our approach of combining two rule sets outperform using just one rule set to classify the video.

6 Conclusions  A novel content-based automatic video classification approach based on data mining is proposed in this paper.

Firstly in the video segmentation process, the method extracts a set of video attribute to represent the content of video and generates a video attribute database. Then the decision tree and class association rule mining techniques are used on this video attribute database to extract a decision tree rules and class association rules. Lastly a combining and pruning algorithm is proposed to combine these two rule sets to generate a final rule set. The experiment results verify the consistency and certain complementarities of decision tree classification with class association classification. The results also show the combined rule set improve the classification precision and have higher classification precision than just one rule set.

