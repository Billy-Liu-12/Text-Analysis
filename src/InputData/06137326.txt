Using frequent closed itemsets for data dimensionality reduction

Abstract?We address important issues of dimensionality reduction of transactional data sets where the input data consists of lists of transactions, each of them being a finite set of items. The reduction consists in finding a small set of new items, so-called factor-items, which is considerably smaller than the original set of items while comprising full or nearly full information about the original items. Using this type of reduction, the original data set can be represented by a smaller transactional data set using factor-items instead of the original items, thus reducing its dimensionality. The procedure utilized in this paper is based on approximate Boolean matrix decomposition. In this paper, we focus on the role of frequent closed itemsets that can be used to determine factor-items.

We present the factorization problem, its reduction to Boolean matrix decompositions, experiments with publicly available data sets, and an algorithm for computing decompositions.

Keywords-frequent closed itemsets; set covering; dimension- ality reduction; Boolean matrices

I. INTRODUCTION  Dimensionality reduction represents a broad area in data analysis and includes various methods ranging from statisti- cal factor analysis [3], nonnegative matrix decomposition [5] to purely relational and order-theoretic methods [1]. In this paper, we contribute to this area by showing how frequent closed itemsets can be used to reduce dimensionality by sub- stituting a set of original items that appear in transactional data by (typically) a much smaller set of so-called factor- items derived from frequent closed itemsets. It is desirable to find the factor-items so that (i) their number is as small as possible, (ii) they explain the data sufficiently well, and (iii) they can be computed in a reasonable time. Needless to say, it is often the case that (i)?(iii) are in mutual exclusion and cannot be optimized all at once. The proposed reduction serves two basic purposes. First, the relationship between factor-items and original items can help reveal interesting patterns in data since factor-items can be interpreted as ?more general (fundamental) items?. Second, the reduction can be used as a preprocessing technique to simplify data sets which can be then used as input for other data-mining techniques like association rule mining.

The principal idea behind factor-items is that instead of considering transactions consisting of items, we consider transactions consisting of groups of items called factor- items. The approach is motivated by the fact that it is  often the case that users need a more abstract view on transactions which naturally leads to grouping items together instead of considering single ones. For instance, in market basket analysis, a factor-item can represent a group of items (frequently) bought by customers for a particular purpose (e.g., preparing a specific food). Thus, it is not surprising that finding factor-items has common ground with finding frequent itemsets. Let us note, however, that the goal of factorization is different from that of mining frequent closed itemsets. The primary goal of factorization is to reduce the number of considered items so that we comprise all or nearly all information from the original transaction data.



II. PROBLEM SETTING AND BASIC NOTIONS  Let us assume that the input data consists of ? transac- tions ?1, . . . , ?? where each transaction ?? (? = 1, . . . ,?) is a subset of a fixed finite set ? of all possible items. To emphasize ? , we shall say that ?1, . . . , ?? are transactions over ? . Under this notation, we consider the following  factorization problem: find a set ? ? 2? of nonempty subsets of ? and transactions ?1, . . . , ?? over ? such that, for any ? = 1, . . . ,?, we have  ?? = ? ??, (1)  where ?  ?? denotes the union of all ? ? ??. Then, each ? ? ? is called a factor-item and ?1, . . . , ?? are called factorized transactions.

The meaning of factorization and factor-items follows directly from the formulation of the factorization problem and corresponds to the informal description in Section I.

Each factor-item ? ? ? is a nonempty set of original items, i.e., ? ? ? ? ? . In addition to that, each ?? is a transaction which consists of factor-items. Thus, ?? ? ? .

Factorized transaction ?? represents the original transaction ?? in terms of the factor-items instead of the original ones.

Our requirement (1) postulates that ?? represents exactly the same information as ??. Namely, ?? contains an item ? if and only if there is a factor-item ? ? ?? which contains ?.

Example 1. Let us have a set of items ? = {0, . . . , 9} and   DOI 10.1109/ICDM.2011.154     consider the following transactions:  ?1 = {1, 2, 4, 6, 7, 8}, ?4 = {0, 1, 2, 4, 7, 9}, ?2 = {3, 5, 9}, ?5 = {1, 2, 4, 6, 7, 8}, ?3 = {1, 2, 7}, ?6 = {0, 3, 4, 5, 9}.

Using a set ? = {?1, ?2, ?3, ?4} of factor-items ?1 = {0, 4, 9}, ?2 = {1, 2, 7}, ?3 = {3, 5, 9}, ?4 = {1, 4, 6, 8},  we can express ?1, . . . , ?6 by factorized transactions  ?1 = {?2, ?4}, ?4 = {?1, ?2}, ?2 = {?3}, ?5 = {?2, ?4}, ?3 = {?2}, ?6 = {?1, ?3}.

Therefore, ?1, . . . , ?6 represent ?1, . . . , ?6 in a reduced space of items: instead of ? with ??? = 10, we can use ? with ??? = 4. One can check that ??1 = ?2??4 = ?1 and analogously for the other transactions. Thus, it is indeed the case that the factorized transactions represent the same information as the original transactions. Each factor-item ?? ? ? can be seen as a more general item saying ?all items in ?? are present?.

In much the same way as before, we may consider the approximate factorization problem so that (1) is replaced by a weaker condition ?? ?  ? ??. Depending on ?, we may  consider various types of approximate factorization, e.g.

?? ? ? ??. (2)  Clearly, (2) is a condition describing a lower approximate factorization. If ?1, . . . , ?? is lost,  ? ?1, . . . ,  ? ?? can be  used as its simplified replacement with at most as many items as in ?1, . . . , ??. Moreover, considering (2) instead of (1), we may want to quantify a degree to which the factorized transactions ?1, . . . , ?? approximate the original transactions ?1, . . . , ??. Provided that (2) holds, we define the approximation degree of ?1, . . . , ?? by ?1, . . . , ?? by??  ?=1 ? ?  ????? ?=1 ????  . (3)  Obviously, the approximation degree is a rational number from the unit interval. For convenience, we express approx- imation degrees by percents instead of fractions. Thus, 95 % approximation degree means that  ? ?1, . . . ,  ? ?? contains  95 % of items contained in ?1, . . . , ??.

Example 2. Consider the transactions ?1, . . . , ?6 from Ex- ample 1. If we omit factor-item ?3 and consider just ? = {?1, ?2, ?4}, we can approximately express ?1, . . . , ?6 by factorized transactions  ?1 = {?2, ?4}, ?4 = {?1, ?2}, ?2 = {}, ?5 = {?2, ?4}, ?3 = {?2}, ?6 = {?1}.

Moreover, if we try to recover the original transactions from the approximately factorized transactions, we get  ? ?1 = {1, 2, 4, 6, 7, 8},  ? ?4 = {0, 1, 2, 4, 7, 9},?  ?2 = {} ? ?2, ?  ?5 = {1, 2, 4, 6, 7, 8},? ?3 = {1, 2, 7},  ? ?6 = {0, 4, 9} ? ?6.

Except for ?2 and ?6, all transactions can be recovered from the corresponding factorized transactions exactly. The degree of approximation given by (3) is 2429 ? 0.83. As a conse- quence ? = {?1, ?2, ?4} and ?1, . . . , ?6 over ? provide 83 % lower approximation of the original transactional data.

In order to find smallest ? possible, it is sufficient to consider factor-items as closed itemsets. This result is a consequence of a recent result [1] which provides a char- acterization of optimal factorizations of Boolean matrices.

See also [6], [7], [8] for related approaches and analyses.

Recall that an ? ? ? Boolean matrix A is any real-valued matrix (in the usual sense) such that A?? ? {0, 1} for all ? = 1, . . . ,? and ? = 1, . . . , ?. Given an ? ? ? Boolean matrix A and an ? ? ? Boolean matrix B, we define an ?? ? Boolean matrix A ?B by  (A ?B)?? = max??=1 ( A?? ?B??  ) (4)  for all ? = 1, . . . ,? and ? = 1, . . . , ?. The operation (4) is called a Boolean matrix multiplication. As a consequence, (A?B)?? = 1 iff there is ? such that A?? = 1 and B?? = 1.

An ?? ? Boolean matrix C can be seen as representing relationship between objects corresponding to rows and attributes/features corresponding to columns. The fact that C?? = 1 is interpreted so that object given by row ? has attribute/feature represented by column ?. In Boolean factor analysis [1], we look for decompositions of C in terms of Boolean matrix multiplication. In a more detail, given an ?? ? Boolean matrix C, we look for an ?? ? matrix A and an ?? ? matrix B such that  C = A ?B (5) with ? as small as possible. Note that in terminology of Boolean matrix theory [4], such ? is called a Schein rank of C. The purpose of the decomposition (5) is to express the object-attribute relationship represented by C by a relationship between object and factors (matrix A) and factors and attributes (matrix B). Thus, the factors correspond to columns of A and rows of B. If (5) holds then according to (4), C?? = 1 can be verbally described as follows: ?There is a factor ? which applies to object ? and attribute ? is a particular manifestation of ?,? cf. [1].

We can show that the factorization problem can be reduced to finding decompositions of Boolean matrices as follows. For brevity, we let ? = {1, . . . , ?}, i.e., the original items are denoted by numbers. Then, transactions ?1, . . . , ?? over ? can be encoded by an ? ? ? Boolean     matrix T defined by  T?? =  { 1, if ? ? ??, 0, otherwise,  (6)  for all ? = 1, . . . ,? and ? = 1, . . . , ?. Under this notation, we get the following correspondence:  Theorem 1. There is ? = {?1, . . . , ??} ? 2? and transac- tions ?1, . . . , ?? over ? satisfying (1) for all ? = 1, . . . ,? if and only if there is an ? ? ? matrix R and an ? ? ? matrix F such that T = R ? F.

Proof sketch: Only if part: Let ? = {?1, . . . , ??} and ?1, . . . , ?? over ? satisfy (1) for all ? = 1, . . . ,?.

Then, considering T given by (6), we can introduce Boolean matrices R and F as follows:  R?? =  { 1, if ?? ? ??, 0, otherwise,  F?? =  { 1, if ? ? ?? , 0, otherwise.

It suffices to check that T = R ? F which follows imme- diatelly. If part: Let T = R ? F for an ? ? ? Boolean matrix R and an ?? ? Boolean matrix F, respectively. Put ?? = {? ?F?? = 1} for all ? = 1, . . . , ?. Moreover, let ?? = {?? ?R?? = 1} for all ? = 1, . . . ,?. It suffices to check that (1) holds. But this is indeed true.

The proof of Theorem 1 is constructive. It gives a way to reduce factorizations of transactional data to decompositions of Boolean matrices and vice versa.

Example 3. Considering again Example 1, the matrix coun- terparts of transactions ?1, . . . , ?6 (matrix T), factorized transactions ?1, . . . , ?6 (matrix R), and factor-items ? = {?1, . . . , ?4} (matrix F) are the following:? ????  0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 0 0 0 1  ? ????=  ? ????  0 1 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0  ? ???? ?  ? ? 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0  ? ?,  i.e., T = R ? F.

According to [1], the problem of optimal decomposition  of a Boolean matrix T can be solved by discovering a minimal set of so-called rectangles that appear in T. More precisely, if A is an ?? 1 matrix and B is an 1? ? matrix then A ?B shall be called a rectangle. For example,?  ????   ? ???? ? (0 1 1 0 1 0 1 1 1 0)=  ? ????  0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0  ? ????  is a rectangle. Observe that according to (4) for any ?? ? rectangle A ? B we have (A ? B)?? = 1 if and only if A?1 = 1 and B1? = 1.

Rectangle D is called a rectangle in C whenever D and C are both ? ? ? Boolean matrices such that D?? ? C??  for all ? = 1, . . . ,? and ? = 1, . . . , ?. The fact that D is a rectangle in C shall be denoted by D ? C. Moreover, D is called a maximal rectangle in C if D ? C and for each E such that D ? E and E ? C, we have D = E. Hence, maximal rectangles in T represent particular submatrices of T which are full of 1s and which cannot be enlarged.

According to [1], the optimal decomposition can be ex- pressed by means of maximal rectangles. For that purpose, denote by A?? the ?th column of A and by B?? the ?th row of B. Note that A?? is in fact an ??1 Boolean matrix and, analogously, B?? is an 1? ? Boolean matrix, i.e. each A???B?? is a rectangle in A?B. We now have the following characterization:  Theorem 2. Let T = A?B where A and B are ??? and ??? Boolean matrices, respectively. Then there is a number ? ? ? and maximal rectangles D1, . . . ,D? in T such that T = R ?F holds for ?? ? and ?? ? Boolean matrices R and F where R?? ? F?? = D? for all ? = 1, . . . , ?.

Proof: See [1, Theorem 2, p. 6].

Let us comment on the consequences of Theorem 2. It shows that if T can be decomposed into two matrices with the inner dimension ?, then T can be decomposed into matrices R and F with inner dimension at most ? such that R and F are fully given by at most ? maximal rectangles in T. Thus, according to Theorem 2, the task of finding a factorization with ? as small as possible reduces to the problem of finding a minimal set of maximal rectangles in T satisfying the conditions of Theorem 2. Moreover,  T?? = max ? ?=1  ( R?? ? F??  ) = max??=1  ( R?? ? F??  ) ??  = max??=1 ( D?  ) ?? , (7)  i.e. T can be seen as max-superposition of maximal rect- angles [1]. As a practical consequence, T?? = 1 iff there is a maximal rectangle D? such that (D?)?? = 1. Thus, the problem of finding the factorization reduces to the problem of finding a minimal set of maximal rectangles in T that cover the whole T, i.e., for each T?? = 1 there is at least one maximal rectangle D? ? T satisfying (D?)?? = 1.

From the computational point of view, the latter observation means that the decomposition problem for Boolean matrices is reducible to the well-known set covering problem [1].

Combining Theorem 1 and Theorem 2, we get a method for finding solution to the (exact) factorization problem of transactional data: we reduce the factorization problem to the decomposition problem using Theorem 1 and then reduce the problem to an instance of the set covering problem as we have outlined above.

Example 4. Consider the set of items ? = {0, . . . , 9} and transactions ?1, . . . , ?6 over ? from Example 1. The corresponding 6?10 Boolean matrix T given by (6) can be found in Example 3. One can easily check that the following     maximal rectangles in T cover the whole matrix T:  D1 =  ? ????  0110 1 0 111 0 0 0 0 1 0 1 0 0 0 1 0110 0 0 010 0 1110 1 0 010 1 0110 1 0 111 0 1 0 0 1 1 1 0 0 0 1  ? ????, D2 =  ? ????  0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 11 1 010 0 1 01 0 1 1 0 1 0 1 1 1 0 10 0 111 0 0 01  ? ????,  D3 =  ? ????  0 1 1 0 1 0 1 1 1 0 0 0 01010 0 01 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 01110 0 01  ? ????, D4 =  ? ????  0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1  ? ????.

Recall that an itemset ? ? ? is called closed if for every item ? ?? ? there is transaction ?? such that ? ? ?? and ? ?? ??. It is easily seen that maximal rectangles are in a one-to-one correspondence with closed itemsets [9]. The corresponding closed itemsets in this example are:  ?1 = {1, 2, 7}, ?2 = {0, 4, 9}, ?3 = {3, 5, 9}, ?4 = {1, 2, 4, 6, 7, 8}.

Thus, Theorem 1 and Theorem 2 yield that the solution to the factorization problem consists of ? = {?1, ?2, ?3, ?4} and factorized transactions ?? where  ?? = {?? ??? ? ??}. (8) Note that although ?4 is diffrent from ?4 from Example 1, ?1, . . . , ?6 remain the same. This is due to the fact that factor-item ?4 in Example 1 is not a closed itemset.

Taking into account the previous observations, an impor- tant question now emerges:  Do frequent closed itemsets constitute good factor-items?

The question has several important facets. First, the number of frequent closed itemsets is (for reasonable values of minimal support) much smaller than the number of all closed itemsets and there exist specialized and efficient algorithms for computing frequent closed itemsets (e.g., [2], [9], [10], [11], [13]). Using frequent closed itemsets can thus improve the proces of finding solutions to the factorization problem if only frequent closed itemsets are used. Second, there is an important question of determining the value of support prior to computing the factorization because various values yield factorizations of various quality (i.e., with various numbers of factor-items). It is even possible that with very high values of minimal support, the exact factorization may not be possible. Third, there is a question whether frequent closed itemsets can be used to find solutions to approximate factorization problems. The issues will be addressed in the next section.



III. USING FREQUENT CLOSED ITEMSETS AS FACTORS  In order to find answers to the questions raised in the previous section, we have used experimental approach and             0 1 2 3 4 5 6 7 8 9 10  d eg re e o f a p p ro x im  a ti o n (%  )  support (%)  retail T10I4D100K  kosarak debian tags          0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30  d eg re e o f a p p ro x im  a ti o n (%  )  support (%)  T40I10D100K anon. msweb  mushroom tic-tac-toe  Figure 1. Approximation degree with respect to support (?-axis: value of minimal support; ?-axis: approximation degree given by (3) achieved by found factor-items with given support).

have considered exact and approximate factorization of various data sets. For this purpose, we have taken the basic algorithm for finding decompositions which is based on computing all closed itemsets (i.e., maximal rectangles) and then applying a greedy set covering procedure to find a set of factor-items. This basic approach to factorize Boolean matrices is described as Algorithm 1 in [1]. During the computation, we have made several observations on the role of frequent closed itemsets.

The experiments have shown that using frequent closed itemsets as factor-items is a viable approach, however, the ability of such factor-items to approximate input data is limited. The general observed tendency is that with the increasing minimal support, frequent closed itemsets tend to lose the ability to constitute good factors.

The phenomenon we have just discussed can be illustrated by diagrams depicting the influence of the value of the minimal support on the approximation degree we are able to achieve. For instance, Figure 1 depicts such dependen- cies in case of selected data sets from publicly available     repositories1. Obviously, if the minimal support is set to 0, then 100 % approximation degree can be achieved and the algorithm is able to return an exact factorization (this is a consequence of Theorem 2 and the fact that closed itemsets are just frequent closed itemsets when the support is disregarded). With increasing values of minimal support the ability to approximate data reduces.

An interesting observation from Figure 1 is that for most of the used data sets, setting minimal support to 1 % is already sufficient to obtain a relatively high approximation degrees. The exceptions are retail and kosarak data sets, where the approximation degree is about 30 %, but notice that these two data sets are very sparse compared to the others (the densities of retail and kosarak are only 0.06 % and 0.02 %). In these two cases, setting minimal support to 0.1 % already gives a good approximation.

In general, the data sets we have used share the following property: if the required approximation ratio is 95 % (or close to that value) then the number of factor-items is usually considerably smaller than the number of original items. This can be seen in Table I showing values of minimal support, total numbers of frequent closed itemsets considered, achieved approximation degrees, numbers of factors, and dimensionality reduction in terms of percents.

Smaller values in the last column are better. For instance, the record for retail says that using minimum support of 0.02 %, we compute 6,710 factor-items that achieve 94 % approximation degree while reducing the number of items to 40 %. Thus, instead of original 16,470 items, the data set is explained with 94 % approximation degree by only 6,710 factor-items. This can be seen as a significant simplification.

Let us note that the approximate factorization may be interesting not only from the point of view of data simplifica- tion but also from the point of view of abnormality detection.

If we have a data set for which a high approximation degree is achieved by a small number of factor-items, it may be interesting to inspect the behavior of the remaining (uncovered) data. From the point of view of approximate factorization, these are the items that belong to ?? but do not belong to  ? ??, see (2). Their inspection may help to reveal  nontypical behavior (which may be of interest) or possible flaws in data (if data is obtained, e.g., by a measurement and binary scaling).



IV. ALGORITHM FOR COMPUTING DECOMPOSITIONS  Our experiments with publicly available data sets have shown the ability to reduce the number of items while achieving high approximation degrees. In addition to that, it has been shown that frequent closed itemsets with carefully selected minimal support can be used to find the factoriza- tion. The obstacle that remains is the choice of the minimal  1http://archive.ics.uci.edu/ml (UCI ML Repository), http://fimi.ua.ac.be/data/ (Frequent Itemset Mining Repos.) http://kdd.ics.uci.edu (UCI KDD Archive)  1: procedure FACTORIZE(?;?1, . . . , ??) 2: ? ? FCITEMSETS(?1, . . . , ??;??? ???????) 3: ? ? ?{{?} ? ?? ? ? = 1, . . . ,?} 4: ?? ?? ? 5: while ? ?= ? and 1? ?? ?? < ? do 6: ? ? FINDBEST(?,?) 7: ? ? UPDATELIST(?) 8: ? ? REMOVERECTANGLE(?,?) 9: STORE(?)  10: end while 11: while 1? ?? ?? < ? do 12: ? ? BESTREMAINING(?) 13: ? ? REMOVERECTANGLE(?,?) 14: STORE(?) 15: end while 16: end procedure  Figure 2. Algorithm for computing factor-items based on frequent closed itemsets combined with on-demand computation of remaining factor-items.

support. Using the basic algorithm, the value of minimum support is an input information that directly influences the approximation degree but the degree itself is not essential to a user who is interested in getting a factorization, i.e., a simplification of the initial set of items. Insted, the user may want to specify an approximation degree that should be achieved.

This motivates us to make a modification of the basic algorithm which takes advantage of frequent closed itemsets but is able to continue to achieve a preset approximation degree even if the computed frequent closed itemsets are not sufficient to do so. The algorithm we propose combines two approaches. The basic one with frequent closed itemsets as candidates for factorization (first stage of the algorithm) and an on-demand computation of factor-items by an incremental joining of ?promissing items? (second stage of the algoritm).

In theory, the second stage alone can be used to compute factor-items (cf. [1, Algorithm 2]) but our experience has shown the results are worse than in case of the basic algorithm used so far.

The proposed algorithm is depicted in Figure 2. The algorithm first computes all frequent closed itemsets with given minimal support. For that purpose, we can use highly efficient algorithms like LCM3, see [11]. The frequent closed itemsets are stored in a list ? . In addition to that, ? is set to the universe that is covered: the entries of ? are pairs consisting of transaction id and item; in practice ? can be represented by a sparse two-dimensional array. The size of ? is stored to ?. This value is further used to compute achieved approximation degree. The first loop between lines 5 and 10 selects the best factor-item ? ? ? (see line 6), updates ? by removing superfluous itemsets that no longer cover any entry from remaining ? (see line 7), removes from ? the maximal rectangle corresponding to ? (see line 8), and stores ? as     Table I EXAMPLES OF DIMENSIONALITY REDUCTION VIA APPROXIMATE FACTORIZATION USING FREQUENT CLOSED ITEMSETS  data set support freq. itemsets approx. factors dim. reduction retail 0.02 % 65,301 94 % 6,710 40 % anon. msweb 0.3 % 1,056 95 % 107 46 % debian tags 0.3 % 2,293 93 % 234 49 % T10I4D100K 0.4 % 1,992 95 % 631 63 % T40I10D100K 0.2 % 1,056 92 % 611 61 % mushroom 6.9 % 8,275 94 % 78 65 %  newly found factor-item. The process is repeated until the approximation degree ? ? [0, 1] is reached or until no other frequent closed itemset that covers any part of ? remains.

After that, it is possible that ? has not been reached and thus the algorithm computes remaining factor-items from all possible closed itemsets which are computed ?on demand? (see line 12). After the procedure finishes, all factor-items are stored, and the factorized transactions ?1, . . . , ?? can be determined from the original transactions and ? by (8).

Let us comment more on the auxiliary procedures used in the algorithm. Procedure FINDBEST returns ? ? ? which maximizes the overlap with ? . Thus, for ? = {? ?? ? ??}, the size of the intersection ??(???) of ? and the maximal rectangle induced by ? must be maximal among all ? ? ? .

Procedure UPDATELIST returns a subset of ? which consists of frequent itemsets such that the corresponding maximal rectangles cover at least one item in ? ? (? ? ?). Finally, procedure REMOVERECTANGLE returns ? ?(???), i.e. ? with all entries from the maximal rectangle corresponding to ? removed. The second while-loop uses procedure BE- STREMAINING which, given ? , computes the best factor- item by iteratively joining items so that the overlap with ? is increasing. Thus, it starts with the item ? which has the most occurrences in the remaining entires of ? . Then, it tries to add another item so that the overlap of the corresponding maximal rectangle with ? is greater. The process continues until no item can be added.



V. CONCLUSION  We have introduced the exact and approximate factoriza- tion problems for transactional data sets and showed that the problem of finding an optimal number of factor-items can be reduced to the problem of finding optimal decompositions of Boolean matrices [1]. This observation enabled us to use results from Boolean matrix theory and identify factor- items using maximal rectangles which correspond to closed itemsets. We have examined the possibility to use frequent closed itemsets as factor-items. The experimental results have shown that with suitably small values of minimum sup- port, frequent closed itemsets used as factor-items achieve high approximation ratios. Following these observations, we have proposed an algorithm for computing factorizations which first utilizes frequent closed itemsets as candidates for factor-items and then computes the remaining factor-  items by incrementally joining items. The experimental observations further showed that the proposed approach is capable of reducing the dimensionality of data while keeping high approximation degrees. The experiments with publicly available data sets showed that usually around 95 % of the data can be explained by factor-items the number of which is around half the number of the original items. That represents a significant reduction. The results and observations made in the paper also showed that frequent closed itemsets that are traditionally used for mining non-redundant association rules [12], are important from a different point of view?the dimensionality reduction.

