Association Rules Mining of Traditional Chinese Medical Syndrome  Differentiation Oriented

Abstract The paper expounds the association rules mining  procedure on Traditional Chinese Medical Syndrome Differentiation (TCMSD), comes down to a method - Apriori algorithm which creates the frequent item sets.

In the process of creating the frequent item sets, the efficiency of execution becomes lower rapidly as dimensions increasing, so DFP-growth algorithm is provided on the FP-growth algorithm. DFP-growth has the same structure as FP-tree, and makes use of a top-down increment strategy to obtain the frequent item sets.

1. Introduction With the development of database technology and  the application of database management system, data explosion but knowledge poverty comes out. For this reason data mining come into being. It is the technology that can obtain the useful knowledge by refining and logically analyzing huge data. Meanwhile, it is also one of the most advanced research areas in the world now.

A Syndrome in Traditional Chinese Medicine (TCM) often embodies many diseases in modern medicine, and is one symptom of many diseases in most cases.

Moreover, syndrome differentiation criteria and curative effect evaluation on TCM are still not prevalently approbatory, which is also a broad research space.

We combine logical analysis method with data mining technology for TCM cases, construct and perfect the technical platform and the method system of TCM syndrome differentiation criteria and curative effect evaluation of children pneumonia. This paper emphasizes on studying association rules mining of TCMSD oriented.

2. Association Mining Association knowledge reflects the dependence and  association between an event and others, which was firstly explain by Agrawal [1]. By now, many effective  methods of the frequent item sets mining have been come up with.

There are two kinds of algorithms. One is generating backup sets during the association rule mining, and Apriori [2] is one of the most representative algorithms. Apriori and Apriori-like algorithms derived from it both use the transcendent knowledge specialties of the frequent item sets to reduce the backup sets. And many of frequent item set methods that were not depended on Agrawal?s emerged [3].

Fig. 1 Association rules mining overview  The other is a method belonging to the patter growth. In the past years, some algorithms on the mode increment have been provided in order to avoid the problems from the appearance of backup sets, e.g. FP- Growth [4], TreeProjection, H-mine. Pattern increment algorithm segments database and saves the projection and condition database to the memory. Different from the backup sets? generation , it unites the scrape pattern segments to a longer segment and superior to the similar methods in Apriori, ad hoc, its advantage is more obvious for dense-database.

In the process of generating TCMSD, association mining will solve not only the association relationship between the data sets? properties, but also how to acquire the rate of contribution of special disease, primary disease and secondary disease of syndrome from ?syndrome belong to disease?. The process of   DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72    DOI 10.1109/CIS.Workshops.2007.72     association rule mining TCMSD is illustrated in Fig.1.

Namely, the date mining is composed of two phases? tasks. First, reading original data with association algorithm, mining all the association rules meet for the minimum supportability threshold (MST), and calculating the supportability and the reliability of each frequent pattern. Second, analyzing of each frequent item set mined, comparing the supportability and reliability with the given criteria in order to judge the special disease, primary disease and secondary disease.

This paper concerns on the first one.

3. Apriori algorithm Consider an experiment of 5 hundreds in 70-dim.

cases data, we see that it is difficult to execute the algorithm, and even cause the system crash. The algorithm can be done only less than 20-dim., yet its efficiency was very low, for it took more than 20 minutes. According to our analysis, the Apriori algorithm has double of fatal capability bottlenecks.

One is that the system needs a large I/O load as transaction database is being scanned. For each k Cyc, each element in backup set Ck must be verified by scanning the database once,  to judge if it joined the Lk or not. And the other hand is the algorithm will bring an enormous backup set. The k-backup set Ck from Lk- 1 increases in exponential function, so it is a challenge for time and memory space.

In the experiment under study, the symptoms signed by 0 almost represent that the actor is health, e.g. the degree of decompensation equal to 0 shows the respiration smoothing, and the degree of lung auscultation denoted by 0 describes respiration clear, namely, all are normal. And these normal data are not significant for the syndrome differentiation. After consulting the TCM experts, these symptoms were deleted, then the backup frequent item sets slipped down, the execution was simplified and the algorithm efficiency was improved. Moreover the association rule mining?s unit is the ?syndrome?, everyday, the symptoms and their appearance reduce much more than before. Therefore, a real item may use the algorithm. In fact, the essential disadvantage of the algorithm has not been eliminated yet, so we shall come up with a FP-tree algorithm on tree pattern in details.

4.  DFP-growth algorithm The mining process of DFP-growth is based on FP-  tree structure. And it constructs the FP-tree through the parent-node pointer and the sibling-node pointer of the tree. Afterwards, it makes mining by introducing two index mechanisms: frequentPattern, recording the frequent item set of a frequent item on some path;  record, linking all the frequentPattern of the same frequent item with the same frequent items on all paths. Thus, DFP-growth builds every frequent item set resulted from its parent-node in the tree by increment iterating, and finishes association rule mining through traversal in FP-tree only once.

Fig.2 transaction database (left), item frequency  (middle), after preprocessing database (right)  4.1 Pretreatment Process First, make sure each item?s frequency (i.e.

supportability) along with the scanning database for the first time, and sort them descendingly. Second, delete items from item set whose supportability is lower than the threshold given by the user. Then, sort each item of transaction on the supportability in its item set descendingly.

We shall explain the pretreatment process using partial data in the children pneumonia experiment with TCMSD. The left in Fig.2 lists the database information. Each item?s supportability in the database is illustrated in the middle table descendingly. If the minimum threshold given by users is 3, then the If and Ig must be abandoned. Afterwards, the right side shows the result of decreasing sort for each transaction on frequent item supportabilities? order.

4.2 Construction of the Tree  The tree begins to be constructed, which is basically a pre-tree of the transaction set. Namely, each path represents the transaction with the same prefixion, and every node in the tree has an item. Moreover, all of nodes in a same item are linked by a sibling pointer together,  in order to find some item?s transaction easily through traversal in the table. Each table is visited by a head element ?ahead? that records the total supportability about the item and will be saved in the recordlist at the same time.

The process of constructTree() with the data in fig.3 is showed in Fig.4 in the case of min Sup =40%. E.g., the first transaction ?Id, Ia?, leads the first branch of the constructed tree: < (Id:1), (Ia:1)>, which includes two nodes, Id is the child node link of the root; Ia links     to Id. The second transaction ?T2:Id,Ic,Ia,Ie? also leads to another branch, where Id links to the root, Ic links to Id, Ia links to Ic, Ie links to Ia. However, T2 and the existed path of T1 should share the prefixion <Id> together, which adds 1 to the count of the Id?s nodes and creates three new nodes (Ia:1), ( Ia:1), ( Ia:1) as the child chain in turn. because T2 in Ia and T1 in Ia are the same item, after the node ( Ia:1) is created in T2 , it links to the sibling of the node ( Ia:1) inT1. In general, when considering to increase a branch for a transaction, the count of each node along with the same prefixion adds 1, build a node following the prefixion and link to it. The algorithm of construct tree is shown in Fig.3.

Fig. 3 The process of tree construction  Algorithm 4?DFP-tree Construction Input: (1) transaction database D  (2) the MST Output:   DFP-tree  Procedure: (1) Frequent item sets L and their supportability  counts through pretreatment process initialize the recordList and simultaneously the head element ?ahead?.

(2) Create the tree node, signed by ?NULL?.

(3) Call constructTree() method to complete the  DFP-tree construction.

Compressed tree is also named the property  combination, i.e. in the constructed DFP-tree, join the same or the similar supportability counts together described them as one item. Thus, we can use one item to consider such items as pattern combination. After that, reverting these united items. The core idea of this algorithm is combining property items whose appearance counts are close, and then the key is to find the property with the appearance number alike. In the algorithm, program control the similar scale with a certain percentage P,Namely, property A and b can be combine, if )1()P1( PABA +????? ?  4.3 Generation of Frequent Item Set The construction of DFP-tree is generated with the  algorithm above. {< Id:8>,<Ib:5>, < Ib:2> }, where the frequent item set of node < Id:8> is NULL. The frequent item set including <Ib:5> in the FP-tree has the frequent item Id with the highest supportability, and the Id has no prefixion, then the frequent item set mining begins from the second item, that is Ib should be considered above all. Ib is displayed in two branches in DFP-tree. It is easy to find its direct prefixion < Id> through the node chain of Ib. ? <NULL> can?t be served as a prefixion? . Thus a frequent item set<( Id: Ib):5> of Ib will be educed directly. As for Ic, its three prefixions < Id:2>,< Id Ic:1>,< Id:2> are correspondingly to its three branches{< Ic:2>,< Ic:1>,< Ic:2>}, and the frequent item set of node <Ib:2> is also NULL Moreover, combining them with the Ic to get the such frequent item set as {< Id Ic:2>, < Id IbIc:1, Id Ic:1, Ib Ic:1>, < Ib Ic:2>}. Now consider to build index record , index three chains(see in Fig.4), unite the supportability count on every chain, and delete the frequent pattern unfit for min Sup in order to acquire the frequent pattern{Id Ic:3, Ib Ic:3} for Ic. Analogously, pattern {Id Ia: 4} can be found. Finally, consider the frequent item Ie, frequent pattern fit for min Sup doesn?t appear after Ie mining with the algorithm above.

Table 1 partial result of association rules  Prior item Next item supportability reliability  A6_193_0  A6_5_0  0.700893 0.987421 A6_193_0 A6_224_0 A5_1L2_1 A5_9L1_0 A5_8L1_0 A5_5L21_0 A6_223_0  A6_5_0 0.660714 0.954839  A6_222_0 A6_13_0 A6_2_0 0.664557 0.807453 A5_2L1_0 A5_5L4_0 A6_25_0  A6_2_0 0.680357 0.808217 A5_1L2_1 A5_9L1_0 A5_8L1_0 A5_5L21_0 A6_193_0 A6_235_0 A6_239_0 A6_224_0 A5_2L2_0 A6_223_0 A6_11_0 A6_011_0 A6_1_0 A6_3_0  A5_6L1_0  A5_1L1_0 0.640625 0.826012  A5_8L1_0 A5_5L21_0 A6_193_0 A6_235_0 A6_239_0 A6_294_0 A6_293_0 A6_292_0 A6_291_0 A6_296_0  A6_295_0 A6_284_0 A6_283_0  A6_282_0 A6_281_0 A6_26_0 A6_238_0 A6_237_0 A6_236_0 A6_234_0  A6_1_0  0.6410042 0.815957    4.4 Experiment Analysis Table 1 illustrate the experiment result, where the  minimum supportability is0.6, the minimum reliability is0.8. We can?t show the generated association rules one by one for the volume of rules are enormous. The name in the prior or the next item is the property sign.

For the frequent item set of the whole data set, DFP- growth algorithm just makes transversal in DFP-tree only once, the relation between the time needed and the nodes of this tree is linear. However, FP-growth has to recur and mine condition FP-tree in the frequent item  set mining, and it doesn?t make full use of the results generated in the process, but wastes much of the time on recursion transversal.

Besides its own design rule, other two factors affect the algorithm efficiency greatly. One is the minSupport inputted by users, the less the threshold, the more the frequent items left, the larger the frequent item set generated , and the more the time run; on the contrary, the larger the minSupport , the less the time algorithm used.  The other factor is the form of the association rule being, i.e. the more complicated the DFP-tree, the more the time for the tree mining.

5.  Conclusion The paper describes Apriori algorithm at the  beginning. And the experimental result shows that a large I/O load will appear along with scanning the transaction database many times, so the system runs abnormally. However, from the data of disease cases, the symptoms signed by 0, in the most case, represent the normal. And we can delete such cases to avoid the bottleneck. In fact, the result isn?t perfect as we thought. Whereupon, based on the FP-growth, we supply a new association mining algorithm, DFP- growth that filters the generated tree and simplifies structure of the tree, especially, with the top-down transversal. Unlike the FP-growth which uses the method of condition FP-tree made by recurrence for association rule mining, the DFP-growth constantly generates association rules with a kind of increment method, thereby, it, to a certainty, can improve the algorithms efficiency.

