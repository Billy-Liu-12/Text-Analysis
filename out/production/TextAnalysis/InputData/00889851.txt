Parallel Mining of Association Rules with a Hopfield type  Neural Network

Abstract  Association rule mining (ARM) is one of the data mining problems receiving great attention recently in the database community. The main computation step in an ARM algorithm is frequent itemset discovery. In this paper, a frequent itemset discovery algorithm based on the Hopfield model is presented.

1. Introduction  Data mining has recently attracted tremendous amount of attention in database research because of its applicability in many areas including decision support, marketing strategy and financial forecast [CNFF96]. Data mining algorithms can be classified into three categories: Association rule mining (ARM), clustering and classification. In this paper, we will concentrate on ARM.

The ARM problem was introduced in [AIS931 and can be formalized as follows. Let I = h1, i2 ,  ..., in}be the set of n distinct items and D a set of sales transactions, where each transaction Tis  a subset of I .  A subset of I containing k items is called a k-itemset. An itemset X is said to have a support s if s% of the transactions in D contain X as a subset (i.e., the ratio of the number of transactions in D containing X to the total number of transactions in D). An itemset is afrequent itemset if it occurs in the database with a certain user-specified frequency, called minimum support. In addition, an itemset X is maximal or large if it is not a subset of any other itemset.

An association rule is an expression of the form X + Y , where X and Y are subsets of Z and X n Y  = 0. An association rule X + Y is said to have confidence c if c% of the transactions that contain X also contain Y, i.e.,  c=support (X U Y ) /  support( X ). An association rule X + Y has a support s if s% of the transactions in D contain both X and Y. The task of mining association rules is to find all the association rules which satisfy both the user-defined minimum support and minimum confidence.

The process of mining association rules can be decomposed into two steps. First, all the frequent itemsets are identified. Then, the association rules satisfying both the minimum support and the minimum confidence are identified based on the frequent itemsets computed in the first step. In this paper, we only deal with the computationally intensive first step. A lot of literature[Z99] have pointed out that, the process of generating frequent itemsets turns out to be the bottleneck in ARM. Therefore, researchers have focused on developing algorithms for the generation of frequent itemsets. Finding frequent itemsets is a nontrivial problem. First, the number of transactions can be very large. Second, the potential number of frequent itemsets is exponential in the number of different items, although the actual number of frequent itemsets can be much smaller.

In addition, the frequent itemset search problem can be reduced to the task of enumerating only the maximal frequent itemsets since all the other frequent itemsets are subsets of these maximal frequent itemsets. Once the maximal frequent itemset have been .identified, the support of all uncounted subsets can be computed by making an additional database pass. Having the support of all frequent itemsets, the association rules are then generated.

2. Sequential mining of association rules Given n items there are 2n subsets that might potentially be frequent. An extensive search over this exponential space is infeasible except for very small values of n.

1082-3409/00 $10.00 0 2000 EEEi  mailto:paher@ec-lille.fr   Many algorithms for finding? frequent itemsets have been proposed in the literature. The most popular is the Apriori algorithm[AS94] and its extensions the DHP[PCY95] and PARTITION algorithms[SON95]. Apriori is an iterative algorithm that counts itemsets of a specific length is a given database pass. In the first iteration, the size-1 large itemsets are computed by scanning the database once.

Subsequently, in the kth iteration, the candidates k- itemsets are generated from the set of frequent (k-1)- itemsets found in iteration k- 1. The support counts of the candidates k-itemsets are then computed by scanning the database once. The algorithm after the scan discards candidates with support lower than the user-specified minimum and retains only the frequent k-itemsets. The process is repeated until all frequent itemsets have been enumerated. Several refinements have been proposed in literature that focus on reducing the number of database scans, the number of candidate itemsets counted in each scan, or both. Zaki presented in [Z99] a survey on some of the work done in building sequential ARM algorithms with a summary of the major differences among them.

3. Parallel mining of association rules  Since mining frequent itemsets requires lots of computation power, memory and disk YO, parallel algorithms are needed. Many parallel ARM have been proposed in literature for frequent itemset discovery and most of them are based on their sequential counterparts[Z99]. On distributed system, all the algorithm assume that the database is evenly distributed on the disks attached to the processors. The different algorithms presented in the literature propose different techniques in order to optimize the synchronization and the communication costs between processors andlor to use the aggregate memory efficiently. On shared memory systems, the database is logically divided into partitions and the main issue of ARM algorithms is to avoidreduce false sharing and to achieve good data locality[Z99]. Zaki presented in [Z99] an up-to-date survey on some of the work done in building parallel and distributed ARM algorithms for distributed or shared memory platform.

Techniques inspired from human being such as neural networks and genetic algorithms has been applied to mine classification rules from databases. Classification, which involves finding rules that partition a given data set into disjoint groups, is one class of data mining problems. An algorithm that extract classification rules from a trained neural network is presented in [LSL96]. Another algorithm proposed in [YYC97] combines neural network, genetic algorithm and symbolic learning approach to mine classification rules.

In this paper, a frequent itemset discovery algorithm that uses a Hopfield network is presented. As far as we know,  Hopfield model has not been used to solve the ARM problem. The Hopfield network is often used in literature in modeling a neural network for combinatorial optimization problems[GTFOO]. The rest of the paper is organized as follows. In the next section, we present a brief review of the Hopfield type neural network. We present then a frequent itemset discovery algorithm that uses a Hopfield network.

4. Hopfield type neural network  A neural network (NN) is a computational structure inspired by the study of biological neural processing. A NN can be viewed as a directed graph wherein nodes are the artificial neurons. Neurons are processing elements seen as units that are similar to the neurons in a human brain. Edges of the directed graph represent the synapses, i.e., connections between the neurons. The neurons can be connected in arbitrary way. The Hopfield type neural network is a fully interconnected system of N neurons.

Each neuron is modeled as a nonlinear devise (operational amplifier) with a sigmoid monotonic increasing function relating the output Vi of the ith neuron to its input state ui.

The frequent choice of the sigmoid function is  Vi = gi (ui ) = -(1+ tanh(hiui )) .

Each neuron receives resistive connections, to model synaptic connections, from other neurons. These synaptic connections are described through a matrix T = [Tij].

This matrix is symmetric (Tv=qi). Each neuron receives also an external current, known also as bias, Zi, which could represent actual data provided by the user to the NN. The evolution of the neurons is defined by the differential equation    dui N - - - -- ui +- C T..V. + I i dt j=1 1J J  The energy function of the Hopfield analog neural network is defined by:  Energy function, also known as the Lyapunov function, in a neural network is a function of outputs and weights which determines the state of the system. The term ?energy function? stems from an analogy between the network behavior and that of certain physical systems. A network of neurons will always evolve towards a minimum of the energy function, like physical systems evolve toward an equilibrium state. A stable state of NN corresponds to a local minima of the energy function.

In terms of the energy function given by (2), the dynamics of the ith neuron are described by    (3)  To map a problem onto a neural network, we have to perform the following steps:  Choose a neuralarchitecture (i.e., the number of nodes) which depends on the coding chosen for the solution problem.

Choose an energy function whose minimum value corresponds to ?best? solutions to the problem to be mapped.

Derive connectivities Tu and input bias currents Z; from the energy function.

Set up initial values for the input voltages ui that completely determine the stable output voltages of the network neurons.

5. Frequent itemset discovery with a Hopfield network  Hopfield and Tank [HTSS] showed that if a combinatorial optimization problem can be expressed in terms of a quadratic energy function of the general form given by (2), a Hopfield network can be used to find locally optimal solutions of the energy function. These may translate to local minimum solutions of the optimization problem. The network energy function is made equivalent to the objective function to be minimized. The constraints of the optimization problem are included in the energy function as penalty terms.

Let us consider the ARM formulated as follows. Given a set of n transactions and p items. The main computation step in an ARM algorithm is to discover frequent items. It consists of enumerating the frequently occurring item sets that appear together in many transactions and satisfy the user-specified minimum support. Let bTi be a 0-1 variable to denote the fact that item i is present in transaction T.

bn has value 1 if the 7th transaction contains the item i and value 0 otherwise. This suggests that a p-tuple is associated with each transaction with some element equal to 1 and the rest elements equal to 0. In a neural network presentation, this requires p neurons for each transaction.

Since there are n transactions, the total of N = np neurons is needed where n is the number of transactions and p is the number of items. These neurons can be all arranged in a two-dimensional array of p neurons per row and n neurons per column.

Let Vn denotes the output of the ith neuron in the array of neurons corresponding to the 7th transaction. The  energy function that maps the maximal frequent itemsets discovery problem to the Hopfield network is :  A n P  P n P  2 T=li=lj=l,i#j E=--- C C C VTiVTj + B  C CvTi(1-bTi)  T=l i=l  P (min- bKibK.) +- C n P  c P vTivTj(min- P bkibkj)e k=? J  2 T=l i=lj=l ,i#j k=l  where A, B and C are positive constants whose values decide the relation importance of the constraints to be satisfied. Summations range from 1 to n or from 1 to p.

Some summands appear twice with only the factors interchanged in their order of occurrence in the summand such as V12V13 also as V13V12 like in the first term, since indices i and j have the same range from 1 to p. For this reason, a factor of ?/z is added.

The first term in the expression for E corresponds to the constraint of finding maximal frequent itemset. In other words, this term enforces the constraint that a valid solution contains a maximum number of items. Note that, the third summation in the first term is over the indexj, from 1 to p but excluding whatever the value i has. This prevents redundancies from same factors like Vl2VI2.

The second term in the expression for E prevents from errors by ensuring that if a neuron v, produces a value 1 then the item i is effectively present in the Tth transaction (i.e., bTi = 1). When combined together, the first and the second terms ensure that each row will produce a valid solution with maximal size itemset from the transaction represented by that row.

The last term aims to satisfy the criterion that a solution must be a frequent itemset i.e. its support is more than the user-specified minimum support min. In this term, the exponential function is used so as if a chosen itemset has its support less than min, it is?penalized heavily. In other  words, let I = (min - c bkibkj), the term will have a positive value and hence increase the energy if I > 0.

The term will have a minimum value when the itemset is frequent i.e., when I I O .  Note also that  P  k=l  limI-Ie I = +oo and limC-Iee = 0.

Now, rewriting (1) to take into account the representation of the neurons with double indices, we get [GTFOO]:     we rewrite also the expression given by (2) and (3), we obtain  Determination of weights and Biases  Let us consider (5a) and the dynamics of the neural model. To obtain the weights TAi,Bj and biases IAi , we substitute (4) in (5c) and we compare the obtained expression with (5a). The connection strengths (i.e.. the weights) and the biases are derived as follows:  n (min-E bKibK.)  TAi,Bj = ~ - c . ( m i n - f b ~ b ~ ~ ) e  K J  On setting values of the initial input voltages of the neurons, the time evolution of the network will result in a steady output of the neurons, i.e., solution to the ARM problem.

We implement the Hopfield network in C++ and the parameters used in our experiments were A=5,50, B=100, C=200, Z = 1 and = 3 (It should be noted that there is not very much guidance as to how to choose the parameters in general for the use of the Hopfield network).

6. Conclusion  In this paper, we have presented an algorithm for frequent itemset discovery using the Hopfield analog neural network. We have showed how to map the constraints of the problem into an energy function (i.e., an objective function) for solution via the neural network. Hopfield network is often used by researchers to solve various combinatorial optimization problems. Our future work in this area include the use of Hopfield type neural network to solve other data mining problems such as clustering and classification.

