Parallelized Frequent Item Set Mining Using a Tall and Skinny Matrix

Abstract?Big data applications consist of very large collec- tion of small records, for example data from a retail website, data from movie streaming services, sensor data applications and many other such applications. Frequent item set mining is one of the common tools used for all these applications to gener- ate recommendations to improve user experience of the website.

Frequent itemset mining is also used to find interesting patterns on scientific databases such as gene expression database. One interesting way to represent such big data applications is by transforming them into tall and skinny matrices. In this paper we explore the concept of tall and skinny matrices to generate frequent item sets. The proposed algorithm is implemented on a map-reduce based framework such as Apache Spark and experiments are performed to test the scalability of the algorithm on a cloud platform.



I. INTRODUCTION  Association rule mining is one of the most commonly used data mining techniques to extract relationships between entities in a data set. Association rules are used to analyse sales transactions, but today the mining technique is used to analyse data from many other applications such as data from web search engines, movie streaming websites and also scientific databases such as gene expression databases and weather monitoring database. Apriori and FP-growth are the most commonly used algorithms for generating frequent itemsets [2] [3] [4]. These two algorithms cannot be scaled for big data applications but there are multiple implementations which parallelize these algorithms for big data applications [6] [10] [11]. Most of these implementations use a shared memory or schedule workloads unevenly which could cause a bottleneck for very large data sets. Since transactional data can be represented in a matrix form the paper explores a matrix based algorithm [7] [8] to achieve scalability on a cloud based platform where resources are not a constraint.

Big data applications mostly consist of a very large col- lection of small records. These applications can be translated into tall and skinny matrices. Constantine Paul G et al.

implemented map-reduce based QR fractorization on tall and skinny matrices which evenly distributes workloads across available resources [9]. In this paper we use the structure of tall and skinny matrix to achieve a parallelized frequent item set mining algorithm which distributes workloads evenly across available resources. The proposed algorithm is imple-  mented on a map-reduce framework such as Apache Spark and scalability of the algorithm is tested.

A. Tall and Skinny Matrix  A tall and skinny matrix has more rows than columns. A matrix of size n ? m is tall and skinny when n>>m, for example n could be in billion and m could be in thousands.

Most real world big data applications have a large collection of small records which can be represented by tall and skinny matrices. Algorithms such as TSQR [9] and SVD in Spark Mlib [13] take advantage of the tall and skinny structure of the matrix to achieve even work load distribution across available resources. Real world applications such as retail websites process thousands of transactions every second.

These transactions can be represented in a boolean matrix where columns represent the items and rows represent the transactions as shown in table 1.

TID Itemsets T1 A,D T2 B,C,E T3 A,B,C,E T4 B T5 A,B,C???  1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 1 0 0  ??? Table I  REPRESENTING THE TRANSACTIONS IN A BOOLEAN MATRIX  Since the transactions have only a finite number of unique items in them and number of such transactions can be very large, the boolean matrix created will be tall and skinny.

Since the matrix is tall and skinny, the transactions can be divided across systems and can be processed locally.

Allocating almost equal number of rows and manageable number of columns to each partition. Therefore ensuring the work load to be distributed across all the available resources.

This paper explores a map-reduce algorithm using the tall and skinny matrix structure for generating frequent item set.

The paper adopts a relational calculus based algorithm [7] [8] which generates test vectors of various sizes to discover frequent item sets in the data set.

DOI 10.1109/ICDMW.2016.198

II. ITEM SET MINING FOR SCIENTIFIC APPLICATIONS  A large number of scientific applications deal with very large matrices. Scientific applications such as weather monitoring, sensor data and bio-medical applications convert data into matrixes and preform pattern mining.

[17] describes a method which converts gene expression database into a binary matrix and then converts the matrix to transactions to extract patterns. The proposed algorithm can used for scientific applications which convert the data into tall and skinny boolean matrices.



III. RELATED WORK  The most common algorithms used to generate frequent item sets are Apriori [2] [3] and FP-growth [4] [5] .

There are multiple implementations which parallelize these algorithms for larger databases [6][10][11]. Some of the parallelized FP-growth implementations have inter-dependent tree structure on shared memory which could cause network bottlenecks for a very large data set. Inter-dependency of the data could require frequent synchronization between parallel threads which could result in high communication overhead [14]. PFP is a parallelized map-reduce implementation which uses FP-growth to generate the frequent item sets [11]. The algorithm creates a header table in the first phase of map-reduce process and in the second round of map-reduce process sub-trees are created using the header table. The header table contains groups of one item sets. The transactions are sorted and distributed based on the header table which results in uneven distribution of transactions across the resources. Due to this uneven distribution of transactions, some nodes would have to process more data than other nodes resulting in issues of scalability. The maximum parallelization which could be obtained in this approach will be number one item sets, where each group contains one item. This method could result in a bottleneck where more number of transactions are given to some reducers compared to other reducers. The approach does not scale when more number of resources available and it is not suitable for cloud environment where resources are not a constraint.



IV. TSPM ALGORITHM OUTLINE  Given a large collection of transactional database D, TSPM (Tall and skinny pattern mining) algorithm uses two Map-Reduce phases to find the frequent item sets given a minimum support value denoted by minsup. The first phase of map-reduce tasks are used to find all the one itemsets across the entire database to generate 1 list. The 1 list is pruned based on minimum support to generate the frequent one itemsets. The second phase of map-reduce tasks divide the database to C equal-sized data block and use the frequent one itemsets(1 list) to create the matrix  and mine the local patterns present in the data block. For the second phase of map-reduce tasks, transactions in the database D are divided into C equal-sized data blocks that is denoted by D = {B1, B2 . . . BC}. These data blocks are given as the input to the second phase map function of the TSPM algorithm. The map function internally calls LISC (local item set count algorithm) to mine all the patterns present in the data block using test vectors. After the LISC discovers the local item sets and their respective counts they are emitted to the reducer in form of key value pair, where the item set is the key and their count is the value.

The final stage collects all the results from the reducers and filters them based on the minimum support to produce the final frequent item sets. Algorithm 1 gives an outline of different stages in TSPM algorithm and Algorithm 2 outlines the LISC algorithm.

Algorithm 1 Map-Reduce algorithm TSPM  1: Procedure one itemset(D) 2: 1 list? 1itemsets 3: return 1 list 4: Procedure MAP (B,1 list) 5: l itemSet? LISC(B, 1 List) 6: for itemSet in l itemSet do 7: emit itemSet, count 8: Procedure REDUCE (itemSet,count) 9: count + = count  10: return itemSet, count 11: Procedure COLLECT (itemSet,count) 12: remove items which have count < minsup  A. Test vector adaptation for frequent itemset mining  Test vectors are Boolean vectors of a particular length.

These test vectors are used in VLSI domain to discover manufacturing defects which occur during a chip manufac- turing. But in this case, we used the concept of test vectors to discover the presence of an item set in the data block. We use the relational calculus concepts given by Liu et. at [8] to discover frequent item sets. The length of the test vectors generated at any given iteration in the algorithm will be maximum size of the number of unique items present in the data block. For example, if the number of unique items in the data block is m, which also indicates that the number of columns of the boolean matrix is m. To discover if there is an item present in the kth index, the test vector will have a 1 in the kth index and 0 in the rest of the indices of the vector.

The lenght of test vector will be m and the norm will be 1.

The dot product of this test vector with the transactions will determine if the transaction has a particular item at the kth  index. Test vector helps determine if a particular item set is present in the dataset. It is possible to query for a particular     item set and determine its frequency. By using these test vectors, the need of discovering all the subsets iteratively can be eliminated.

1) Query Substructures: Apriori is based on a level wise approach, in which all the frequent one item sets are generated and then using the frequent one item sets the frequent two item sets are generated. The pattern growth algorithms such as FP-growth constructs a tree and traverses the tree to generate the item sets. Some applications which deal with molecular databases would need to query if a particular substructure is frequently occurring. These pattern growth algorithms can not directly query for the frequency of a particular substructure. TSPM could support such queries since it could directly generate the test vector for the substructure and process all the transactions in one scan to obtain the frequency of the substructure.

Algorithm 2 Local Item Set Count  1: Procedure LISC(B,1 list) 2: 1 itemSet = 1 list 3: map f itemSet = [ ] 4: generate one itemsets and put into l 1items 5: prune l 1items based on 1 itemSet 6: Construct Boolean matrix M based on 1 itemSet 7: k = 2 8: while k > 1 and M.size > 0 do 9: for each row of M do  10: if sum(row) < k then 11: remove row from M 12: end if 13: end for 14: for each column of M do 15: if sum(column) = 0 then 16: remove column from M 17: end if 18: end for 19: generate test vectors of length (1 1items) and  norm k 20: for each test vector Ti do 21: T count = 0 22: for each row of M do 23: if dot(row,Ti) = k then 24: T count += 1 25: end if 26: end for 27: Generate itemset for Ti 28: add f itemSet(itemset, T count) 29: end for 30: k = k + 1 31: end while 32: return f itemSet  B. LISC Algorithm Outline  LISC (local item set count) is called on every data block during the second map phase. The data block and 1 list which contains the list of the one itemsets, is given as the input for the function. The LISC algorithm processes the data block and returns the item sets and their counts corresponding to that particular data block. Outline of the LISC algorithm is explained below using transactions in table 1 and 1 list containing frequent one itemsets for minimum support of 2. The transactions of table 1 are put into one data block, number of partitions is set to one.

1) The LISC algorithm runs on each data block. Based on 1 list, the frequent one itemset list obtained from the data block is pruned. In case of table 1 the 1 list would contain A,B,C,E. Item D does not satisfy the minimum support therefore it is removed in the first phase of map-reduce tasks.

2) The boolean matrix is built during the second scan based on local one itemset of the data block. The boolean matrix of the data block is given in equation 1.

?????? 1 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0  ?????? (1)  3) The matrix can be further pruned in this step, thereby reducing the number of transactions. Below is an example of processing the matrix in equation 1 by pruning transactions which have less than 2 items in them. Transactions 1 and 4 has been removed since it has only one item in it. The matrix transformation after removing transactions 1 and 4 is given by equation 2. Removing transactions could also enable the pruning of the columns which have only zeros.

?? 0 1 1 11 1 1 1 1 1 1 0  ?? (2) 4) Test vectors of size k are generated and checked with  all the transactions remaining after step 3. Below is an example of all possible test vectors of size 4 and norm 2 are given. For example, the test vector in row 1 is used to test if item set A and B are present in the same transaction. If dot product of the test vector and the transaction vector is equal to k, the count of the item set is incremented. The item set and the count are stored in f itemSet. Value of k is incremented after all the test vectors are processed for the paticular k.

???????? 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1  ???????? (3)  5) Step 3 and 4 are repeated until there are no more transactions left. The final local item sets are stored in f itemSet with their respective counts. When there are no more transactions left to process the f itemSet is returned to the reducer.



V. EXPERIMENTS  The map-reduce based TSPM algorithm is implemented on Apache spark [15]. The experiments were performed on a local cloud setting with 10 systems having the configuration of 2GB RAM and 4 cores each (40 cores total) . The dataset used for results shown in table 2 and figure 1 is a retail transaction data set [12] which has 88,000 transactions and over 16,000 unique items. Partitions in Apache spark represent degree of parallelism, which means the number of jobs which could run parallel at any given point. The dataset used for results shown in table 3 and figure 2 are from a synthetic dataset of size one million transactions and thousand unique items.

Number 10 cores (m) 20 cores (m) 40 cores (m) of Partitions  5000 14 10 7.3 10000 4.5 3 3 15000 3.3 2.1 2 20000 3 2 1.7  Table II RUNNING TIME ON VARIOUS NUMBER OF CORES OVER DIIFFERENT  NUMBER OF PARTITIONS WITH MIN SUPPORT 0.1  Number 20 cores (m) 40 cores (m) of Partitions  20000 13 9.2 30000 11 8.4 40000 10 7.4 50000 9 6.7 60000 9 6  Table III RUNNING TIME ON VARIOUS NUMBER OF CORES OVER DIIFFERENT  NUMBER OF PARTITIONS WITH MIN SUPPORT 1  Table 2 and 3 show the running time for various partition sizes over different number of cores. When the number of partitions are increased, the running time decreases. Table 2 and 3 show that the data is distributed evenly across all the  partitions and does not allocate a large chunk of data for any one partition. This even distribution of data ensures that the all the available resources are used equally. Table 2 shows that increasing cores and partitions, decreases the running time significantly. But after certain threshold, increasing cores and partitions does not effect the running time. For example in table 2, setting the number of partitions to 5000 would give block size of 18 and running time of 7.3 minutes. Decreasing the block size to 8 shows a significant improvement in the running time. But after block size of 8, there is no significant improvement in the running time.

Figure 1. Speedup Vs number of partition  Figure 2. Speedup Vs number of partition  Figure 1 and 2 shows the speedup achieved when the number of partitions are increased. The TSPM algorithm     shows a speed up close to the ideal speed up. Whereas PFP does not distribute data evenly across available resources, and hence has the same running time of 50 seconds and 2 minutes for figure 1and 2 respectively. The running time remained the same regardless of the number of partitions and cores present. Parallelism in PFP is limited by the number of groups generated by one itemsets, and therefore increasing the partitions has no effect on the running time. The TSPM shows improvement in performance as the number of re- sources increases. Therefore scaling the algorithm for very large data sets can be done efficiently using the proposed TSPM algorithm in the cloud where resources are not a constraint.



VI. MAP HEAVY VS REDUCER HEAVY  The Tall and Skinny algorithm is characterised by map- side scalability with uniform distribution of datasets whereas PFP-growth is characterised by reduce-side scalability with uneven distribution of the growth-tree data and consequent skew on the reduce side computation. The bottleneck caused by the reducer-heavy jobs would result in scalability issues when dealing with scientific data which could result in large trees on some nodes. The degree of parallelism is high for map-heavy jobs and therefore provide better scalability when using the cloud. More number of instances can be added based on the task sizes to increase the parallelism and reduce the running time. The saturation point for adding resources for reduce-heavy jobs is low, whereas for the map-heavy jobs is very high.



VII. INCREMENTAL PATTERN MINING  A lot of real world applications receive huge amounts of transactions every second and need real time data processing such as stream processing to act on the result immediately.

Retail websites are required to process their transactions in real time to facilitate a better interaction with their customers. Frequent item set mining is a popular tool used in retail websites for providing real time suggestions to the user based on their interactions with the website. There are a number of variations of Apriori and FP-growth for per- forming incremental frequent item set mining on streaming data [16]. These algorithms work based on sliding window, where a window of fixed sized is filled with records and processed and then joined with the future results. We use the results of the batch processing to profile the data sets and use LISC algorithm on the data streams. Since there are no interdependent processes in the TSPM algorithm, the LISC subfunction can be called on a sliding window based stream processing algorithm for frequent item set mining.



VIII. CONCLUSION  In this paper we present a tall and skinny matrix based map-reduce algorithm TSPM for mining frequent item sets.

The paper also shows the possibility of extending TSPM  algorithm for stream processing and querying substructures.

The proposed algorithm is tested on Apache spark platform.

The scalability of the proposed algorithm is compared with respect to PFP. The experimental results show that as the degree of parallelism increases, the performance of the TSPM algorithm improves whereas PFP does not show similar improvement.



IX. ACKNOWLEDGEMENT  I am grateful to Prof. Vinita Vasudevan and Prof. Ra- makrishna M for their valuable feedback given on the paper.

I would like to also thank Prof. Sayan Ranu and Prof.

Kamakoti since the inspiration for the ideas presented in the paper came from their class lectures.

