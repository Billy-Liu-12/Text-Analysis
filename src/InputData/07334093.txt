An Efficient Vertical-Apriori Mapreduce Algorithm for Frequent Item-set Mining

Abstract?Algorithms such as OPUS and Apriori-based Mapreduce for enhancing the efficiency of mining frequent item- set for pattern recognition application from transactional dataset have been proposed in the literature. Most of these algorithms are, however, evaluated offline on relatively small data size. When confronting with larger data size, which is inevitable for todays organisation, most if not all algorithms performed not as efficient as required to meet the real time big data driven decision making needs. We therefore attempt to solve these efficiency problems by proposing a VAMR (Vertical-Apriori Map-reduce) algorithm.

VAMR is based on data attribute identifier which is exploited as capability metric for mining frequency item-set from large dataset in a single node (for example in a single site enterprise) that has no distributed and parallel computing system environment.

Our evaluations using synthetic datasets and data from public repository suggest that VAMR algorithm can offer superior efficiency in mining frequent item-sets from large transaction dataset.

Keywords?Frequent item-set mining, Mapreduce, Apriori, big data, attribute identifier.



I. INTRODUCTION  Recent advance in ICT and smart sensor technologies has enabled massive data being transmitted and received across commercial, industrial and health care transaction processes.

Such massive amount of transactional data has embedded strategic and operational information that can be exploited by organisations management to make right time competitive advantage and sustainable growth decisions. One of the key requirements for making right time decisions is the efficient mining of the big data analytics. Hadoop is the computing platform that enjoys a good reputation as big data solution in recent years, since its core technique MapReduce [1] which inspired by Google MapReduce [2].

Mapreduce framework provided a good way to accelerate the processing of large dataset via its simple map and reduce processes. Current Mapreduce is used by Hadoop with its distributed database. It divided a large file into several chunks which stored by different nodes. When Hadoop is called to execute a job, each node will run map and reduce process on each node individually, then a centralized node will aggregate the results collected from nodes. The distributed mechanism makes the big data to be handled within acceptable time. Not only the distributed system assists Hadoop to handle the big data, but also the map reduce mechanism contributes great pro- portion of efforts for handling big data with simple processes.

Some parallel frequent mining algorithms are proposed by researchers to handle the large datasets [3]?[10]. However, by introducing parallel mining, it would raise other problems as well, such as job assignment and aggregation, data distribution and storage, node communication and parameter exchanging and etc. Large amount of time is required to manage the job assignment, resource usage and synchronization between nodes, which could lead to extra time to mining task.

Apriori [11] is the first proposed frequent pattern mining algorithm for transactional datasets, it starts from frequent 1- itemset to n-itemset by joining itemsets gradually until no further is discovered from the frequent n-frequent itemset.

Many researchers proposed new algorithms to overcome the drawbacks of Apriori such as: FP-Growth tree [12], CHARM [13] and etc. These algorithms were usually tested by selected datasets from some standard dataset repositories, which contain limited number of records, and limited number of attributes.

The performance of these algorithms to handle large-volume dataset is yet to be evaluated.

Most frequent itemset algorithms traverse the entire dataset in multiple passes, especially, the Apriori-like algorithms, when new candidate n-itemset is generated, it traverses the entire dataset to compute its frequency for generating frequent n-itemset and candidate (n+1)-itemset. If the dataset contains large number of records, it takes long time to process the mining task. Vertical frequent mining is different from tradi- tional horizontal frequent pattern mining method. It computes the intersections of Transaction IDs (TID, which is identifier for each transaction) to achieve same result with Apriori-like algorithms and this method only traverses the entire dataset once, i.e one-pass. Hence lesser mining time is required.

Vertical frequent mining is different from traditional hori- zontal frequent pattern mining method. It computes the inter- sections of Transaction IDs (TID, which is identifier for each transaction) to achieve same result with Apriori-like algorithms and this method only traverses the entire dataset once, i.e one- pass. Hence lesser mining time is required.

By combining vertical frequent mining method Mapreduce mechanism and Apriori we propose a new algorithm known as VAMR (Vertical Apriori Mapreduce) that is capable of mining large transactional dataset in an efficient manner within a single node. VAMR algorithm will be tested using self-generated synthetic datasets as well as public dataset. We run experi- ments to support the efficient mining performance of VAMR     algorithm with comparison to OPUS Miner [14] and Apriori Mapreduce [15] separately. Another spin-off contribution of our research is that our data generator also provides a new way to test the frequent pattern mining algorithm, since it allows us to generate different sizes of transactional datasets with the same standard format of Frequent Itemset Mining Dataset Repository. [16]

II. PRELIMINARY  A. Apriori  The term Apriori [11] means that all nonempty subsets of a frequent itemset must also be frequent. More specifically, if an itemset I fails to satisfy the minimum support threshold, then I is considered infrequent. If an item E is added to the itemset I , then the new itemset I ? cannot occur more frequently than I . Hence, new itemset I is not frequent either, I ? minimum support. By applying this theory, the Apriori algorithm scans the entire dataset to find the frequent 1-itemsets, where the 1-itemsets are generated by counting the item occurrences.

Then the frequent 1-itemsets are used to form the candidate itemset in order to generate the frequent 2-itemsets, where the frequent 2-itemset is generated by joining the frequent (n-1)-itemset with frequent (n-1)-itemset (in this case, n-1 is 1-itemset). After the join step the prune step is started, and those itemset containing any subset that does not appear in frequent (k-1)-itemset will be removed . The entire procedure will continue to search recursively until no more frequent n- itemsets are produced.

TID Itemset 1 Milk,bread 2 Egg,milk,sugar 3 Bread,egg 4 Milk,egg,flour 5 Milk,flour,bread,egg 6 Egg,flour 7 Milk 8 Milk,bread  TABLE I. SAMPLE TRANSACTION  Let us use the above dataset in I as an example. After the first scan is finished, we have the following(in II) 1-itemset and 2-itemset with support count.

Itemset Count Itemset Count Milk 6 Milk, egg 3 Egg 5 Milk, bread 3  Bread 4 Egg,bread 2 Flour 3 Sugar 1  TABLE II. FREQUENT 1-ITEMSET AND 2-ITEMSET  Once the 1-itemsets are found, we test whether all the itemsets satisfy the minimum support. Assume the minimum support of 50%; consequently, Sugar, Flour will be removed because they are lower than minimum threshold. Then the remaining itemsets are considered candidate itemsets for gen- erating 2-itemsets. The next step is to generate the 2-itemset by joining the candidate itemsets by itself. We will get TableII itemsets and their support will be calculated as well, which demonstrated in the TableII third and fourth column.

Comparing the support of each 2-itemset with the minimum threshold, all 2-itemset fail to satisfy the condition, so there  is no 3-itemset candidate, the program is then terminated.

Consequently, the final result only contains frequent 1-itemset.

B. Vertical Frequent Mining  Vertical frequent itemset discovery is different from tra- ditional Apriori frequent pattern mining, it was first used in [13], [17]. It computes the Transaction Identifiers to discover frequent patterns.

We use transactions in TableI as example  Item TID Milk 1,2,4,5,7,8 Egg 2,3,4,5,6  Bread 1,3,5,8 Flour 4,5,6 Sugar 2  TABLE III. SAMPLE TID SET  As shown in III, if we want to find the frequency of itemset milk, bread, which can be computed by intersecting TID set of milk and TID set of bread, which is 1,5,8.

C. Mapreduce  Mapreduce is the core technique of Hadoop, it derived from Google Mapreduce. This framework is designated for distributed system. In Hadoop, HDFS(Hadoop distributed file system) is designed to work with mapreduce. Each file in HDFS [18] will be separated into couple of chunks storing in different nodes. There is a node named namenode(in Hadoop) which is responsible to manage how many copies of each files and maintain the information of which node holds a part of a file.

In a distributed system, there is master node designated for job scheduling, and result collection and aggregation. Other nodes are named slave node which are designated for job execution. Mapreduce works under this environment. When a job is initialized, each node doing map process, which outputs the key-pairs ?K, value-list?, then the master will assign the key-pairs with same key value to specific nodes for reduce process. Reduce operation is trying to aggregate the result collected from different nodes into one in format of key- pairs?Key,Value?, ultimately, all results are saved into a single file.



III. THE PROPOSED VAMR ALGORITHM  Vertical-Apriori Mapreudce takes advantages from Apriori, Vertical frequent mining, and Mapreduce to minimize their drawbacks in order to efficiently discover frequent item-sets from large datasets.

In Apriori, for each frequent k-itemset it will scan the entire dataset for computing k-itemset frequency and its support.

It would consume large amount of time for scanning. Al- though distributed system provide an efficient way to process large dataset in parallel, the master node consumes time on management and coordination jobs, involving job scheduling, parameter exchanging, result aggregation and etc.

Vertical-Apriori mapreduce employs the vertical frequent pattern mining to avoid the multiple scanning of entire dataset.

2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA) 109    Fig. 1. Process of VARM  It also applies Mapreduce program paradigm on a single node in order to process large dataset in an efficient way.

Apriori cooperates with vertical frequent pattern to maintain the frequent 1-itemset TID set for minimizing memory usage.

The above Figure 1 illustrated the process of Vertical-Apriori Mapreduce.

Data: DataSet D Map(Key,Value-list) foreach Record ri in D do  foreach Item I in ri do output(I,1)  end end  Algorithm 1: Initial Map  Data: Key-pairs KP(Key,Value-list) Reduce(Key,Value) foreach Key k in KP do  foreach value in ks value list do k.value = k?s size of value list if k.value ? minimum supportthen  output(k,k.value) end  end end  Algorithm 2: Initial Reduce  As shown in Figure 1, it scan the entire dataset at the initial stage for discovering frequent 1-itemsets and their TID sets.

In the following phases, it keep discovering frequent (k+1)- itemset till there is no candidate itesmet.

{Bread, Egg}, {Bread, Milk}, and {Egg, Milk}. Once, the (k+1)-itemset candidate is ready, Map process will compute  its TID set and then the Reduce process reports the frequent itemset and its frequency. By doing this process recursively to discover frequent itemsets until there is no candidate generated, the algorithms is terminating.

Algorithm 1 and 2 showed the fundamental of the Mapre- duce algorithm for initial scan.

Data: Candidate Itemsets C Map(Key,Value-list) foreach Candidate c in C do  foreach Item i in c do c.value-list += intersection(i.TIDSet)  end end  Algorithm 3: Main Map Process  Data: Key-pairs KP(Key,Value-list) Reduce(Key,Value) foreach Key k in KP do  foreach value in ks value list do k.value = k?s size of value list if k.value-list.size ? minimum supportthen  output(k,k.value) end  end end  Algorithm 4: Main Reduce Process  Data: Key-pairs KP(Key,Value-list) NextCandidate(new keys) foreach Key k in KP do  foreach k(i+ 1) in KP do new keys +=join(ki,k(i+ 1))  end end  Algorithm 5: NextCandidate Process  Data: Key-pairs KP(Key,Value-list) Main() Inital Scan Candidate = NextCandidate(Reduce(Map(Dataset D))) while !candidate.isEmpty do  Candidate = NextCandidate(Reduce(Map(Dataset D)))  end Output file  Algorithm 3 ,4 ,5 and 6 demonstrated the entire Vertical- Apriori Mapreduce algorithm. Use above TableI as an exam- ple, our minimum support is 50%, so after initial scan, the frequent 1-itemset and its TID would be {Milk:1,2,4,5,7,8}, {Egg:2,3,4,5,6}, and {Bread:1,3,5,8}. Then, the nextCandidate will take these keys as input to generate next candidate.



IV. EXPERIMENTS AND EVALUATIONS  In our experiments, this Vertical-Apriori Mapreduce (VAMR) algorithm is implemented in Java and all experiments are running on the Windows platform with core i5-2520M and 4GB RAM.

110 2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)    Datasets used for experiments including datasets from public repository (frequent itemset dataset repository) [16] and self-generated datasets with different number of records and different number of attributes.

A. Data Generator  This data generator has three phases: generation, adjust- ment and writing into file.

In the generation phase, it requires the users to input the number of attributes and the number of records they want.

The number of variables is the maximum number of attributes each record can have. For instance, we specify the number of variables as 25, so for each single record, it might have 1,2,3,4... till 25 as a record. Each attribute is determined by a random Boolean value; which means there is 50% probability that an attribute can be generated. Otherwise it skips this number n to n+1 which is the next attribute. For instance, assume that now n is 9. If the Boolean value is true, then the system will output the number 9. Otherwise this number is skipped to 10. Once the generation is finished, then next stage is adjustment phase.

In the adjustment stage, the dataset will be adjusted based on the predefined rules. We have following four cases:  Case 1: both antecedence(s) and consequence(s) are ex- isted. There is 75% probability that keeps the consequence(s), and 25% probability that removes the consequence(s). E.g.

record {1,3,4,5}, pre-defined rule 3 ? 4 Consequence 3 and antecedence 4 are both existed, 25% probability consequence 3 will be removed.

Case 2: Antecedence(s) not exist(s) and consequence exist(s). There is 75% probability that removes the con- sequence(s), and 25% probability that remains the conse- quence(s). E.g. record {1, 4}, predefined rule 4 ? 3, 75% probability that consequence 4 will be removed  Case 3: Antecedence(s) exist(s) and consequence not exit(s). There is 75% probability that adds the consequence(s) into record, 25% probability that remains the same. E.g. record {1,4}, predefined rule 3 ? 4, there is 75% probability that 3 will be added into record {1,4}  Case 4: Antecedence(s) and consequence(s) neither does not exist, there is 75% probability that remains the same, 25% probability that adds the consequence into record. E.g. record {1}, predefined rule 3 ? 4, there is 25% 3 will be added into record {1}.

B. Experiments  In experiment 1, we generated 28 datasets in terms of different number of attributes, including 10-attribtue dataset, 15-attribute dataset, 20-attribute dataset, 25-attribute dataset and 30-attribute dataset and different number of records, including 12500-record dataset, 50000-record, 100000-record dataset, 200000-record, 250000-record dataset, and 300000- record dataset. All datasets will run through Vertical-Apriori Mapreduce and OPUS Miner [19] for comparison in terms of the running time in seconds. OPUS [20] is a branch and space search algorithm especially for unsorted spaces. Self- sufficient itemset [14] uses OPUS Algorithm to search itemset  in a given dataset. In this experiment, we turn off the filter which will screen the self-sufficient itemset, as we only wish to compare the discovery speed. The result is demonstrated in the following Table 4 and Figures 2 and 3.

Fig. 2. Result of OPUS  Fig. 3. Result of VAMR  It is obvious to see that OPUS miner takes longer time to process those datasets. With the increase of number of records and number of attributes, the running time rised up as well.

Especially, with increase of the number of attributes, the com- putation time of both algorithms is shooting up exponentially.

In terms of number of records, with two times number of records, the running time is doubled approximately. Figures 4 and Figure 5 illustrated the running time of both algorithm separately.

In Experiment 2, we compared the Vertical-Apriori Mapreduce with Apriori Mapreduce [15] in a distributed system using dataset from Frequent Itemset dataset repository T10I1D100K [16]. The results are as shown in the Figures 4 and 5. Comparing with [15] results, our results are much more efficient. With the increased minimum support, the performance on this dataset is rather stable, ranging from 0.53 seconds to 0.56 seconds. Apriori Mapreduce takes longer time to process this dataset, its run time decreased as when minimum support increased.

C. Limitation and Future Work  As we can seen from Table 5, both OPUS Miner and Vertical-Apriori Mapreduce consumes significant memory to process datasets. From 25-attribute with 50,000 records, OPUS miner fails to complete the task, while Vertical-Apriori Mapre- duce can handle 30-Attributes dataset but failed to process more than 50,000 records. This Out of memory situation was caused by following factors: i) Although the platform equipped with 4GB RAM, real RAM usage is lower than 4GB due to  2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA) 111    10-Attribute 10-Attribute 10-Attribute 10-Attribute 10-Attribute VARM OPUS VARM OPUS VARM OPUS VARM OPUS VARM OPUS  12500 0.01 0.03 0.22 1.4 0.74 10.4 2.79 78.5 9.79 OM 50000 0.19 1 0.33 2.1 1.2 23.8 22.01 OM 76.23 OM 100000 0.27 1.1 1.05 5.9 4.86 94.2 44.7 OM OM OM 200000 0.48 1 2.05 11.1 31.29 179.1 OM OM OM OM 250000 0.64 1.6 2.43 13.3 36.89 221.3 OM OM OM OM 300000 0.74 2 2.55 15.5 51.6 263.9 OM OM OM OM  TABLE IV. RESULT OF OPUS AND VAMR(OM:OUT OF MEMORY)  Fig. 4. Result of VAMR T10I4100K  Fig. 5. Result of Apriori Mapreduce of T10I4100K  the operating system allocation; ii) Language used including the application of some policies to suppress the usage of memory, for instance, Java has mechanism to control the usage of memory.

Memory consumption is the biggest challenge for discov- ering frequent itemset on a single node, since it requires to maintain a tree or a table of frequent patterns for further com- putation. Even with simple and well-designed data structure, increase in volume of dataset leads to significant increase on memory usage. Our future work underway includes the design of technique to reduce memory usage for frequent pattern mining.



V. CONCLUSION  We have proposed Vertical-Apriori Mapreduce algorithm and evaluated its performance using public repository dataset and self-generated datasets. Our evaluation results demon- strated VAMR algoirhtm is capable to process large dataset in an efficient manner, compared with OPUS Miner [20] and Apriori Mapreduce [15] , our Vertical-Apriori Mapreduce is more efficient.

This proposed algorithm adopted the advantages from Apriori, Vertical frequent pattern mining, and Mapreduce to minimize the demerits enables a single node to process large dataset in an efficient manner. However, it still consumes significant memory to process large dataset.

This proposed algorithm adopted the advantages from  Apriori, Vertical frequent pattern mining, and Mapreduce to minimize the demerits enables a single node to process large dataset in an efficient manner. However, it still consumes significant memory to process large dataset.

