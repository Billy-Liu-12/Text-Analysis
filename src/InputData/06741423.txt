A Distributed Mechanism for Remote Rendering of  Image Data

Abstract?In this paper, we propose a distributed rendering mechanism for image data using multiple servers. Since the stylization of large images also requires high computation overhead to render an image, a low-end client divides it into several image pieces. Each piece is sent to a different server, which then performs rendering. A client merges the rendered pieces to one output image again. The proposed method enables large image data to be rendered by collaboration of multiple servers with reasonable processing and communication cost.

Keywords? Remote rendering, image stylization, distributed processing, multi-server communication

I. INTRODUCTION With the advance of computing devices and Internet  technologies, we can easily create and get various multimedia contents. Due to the proliferation of mobile devices which embed cameras, users have even more chance to manipulate 2D image data with a variety of applications. As recent high- end devices provides sufficient computing and rendering resources, users want to process, transform, and visualize higher quality of 2D image data.

However, normal desktop PCs and mobile devices still lack enough rendering power due to hardware limitation. To address this issue, various researches have been done using remote rendering especially for 3D contents [1]-[7]. The remote rendering method enables low-end devices to display only result images which are rendered by a remote server, but it has an intrinsic problem. Although single server has a role of surrogate renderer of a client, it can get overloaded in terms of rendering time if the server performs rendering of large contents. As the contents size increases, it takes longer time to process the contents. To overcome the limitation of single server, some existing approaches use parallel processing on a multi-core graphics card [8]. However, this approach requires expensive hardware and sufficient knowledge of parallel programming [9].

In this paper, we aim to distribute the rendering overhead of single server to multiple ones in order to reduce overall delay of image stylization. To this end, a client is in charge of dividing an input image into several small pieces, if the image size is greater than threshold value. Since the stylization of a  pixel is affected by its surrounding pixels in our stylization method, an input image splits such that boundary pixels of pieces are overlapped. Otherwise, the stylization result at boundary pixels of two image pieces cannot be merged seamlessly. Each image piece is then sent to a different server so that multiple servers can handle only the separated image piece instead of a whole input image. The multiple servers conduct stylization of the received input image piece in parallel and independently. They do not interact with each other, and just send their result of image piece back to a requesting client.

While receiving result image pieces from servers, a client assembles them into one stylized image, which is then displayed to a user. The proposed distributed stylization approach addresses the bottleneck problem of single server rendering with the modest change of client side, and makes it possible to stylize even a large input image within reasonable delay.

The remainder of this paper is organized as follows. Section II discusses the literature survey of existing remote rendering and image stylization schemes. In Section III, we describe the main problem to consider and fundamental idea design of our proposed approach. In Sections IV and V, we introduce previous local stylization technique and its extension to the distributed manner of stylization. Communication aspects of the proposed distribution model benefits from communication middleware (CM) which is described in Section VI. In Section VII, we analyze our method to measure the performance in terms of processing and communication overhead. Finally, we conclude our paper in Section VIII.



II. RELATED WORK In this section, we discuss existing researches on 2D image  stylization techniques and remote rendering approaches.

For 2D image stylization which is a target of this paper, there have been various mechanisms which require high rendering computation. For real-time processing, many image stylization methods have been designed to be highly parallel and been implemented on the GPU [10]-[13]. Pixel operations are performed on a neighborhood of a pixel and the operations for each pixel are independent of the other pixels. The locality and independency of the computations are suitable for our distributed rendering framework.

Shi et al. [1] proposed a real-time remote rendering system for mobile devices. In their approach, a server sends the selected depth images to the mobile client, which then runs 3D image warping with received depth images to synthesize an updated image at the current viewpoint. They reduce the interaction latency by separating rendering process between a client and a server. Doellner et al. [2] devised a server-based rendering scheme for large 3D scenes using G-buffer cube maps. Their approach also splits rendering process for a server and a client. A server is responsible for rendering of virtual panoramas which are represented by G-buffer cube maps, and a client uses these maps to reconstruct the 3D scene. Diepstraten et al. [3] developed a remote rendering method for 2D image generation task from a 3D model in mobile devices. A server extracts 2D line primitives from arbitrary 3D scenes, and a client receives and renders only 2D line primitives. There are also other similar remote 3D visualization approaches [4, 5], a multitude of real-time rendering [6], and image-based rendering scheme [7]. Yoo et al. [8] presented a parallel design of the proxy server in the remote rendering framework. Using Compute Unified Device Architecture (CUDA) [9], they developed parallel rendering on Graphics Processing Units (GPUs) in the proxy server.

In summary, most existing approaches are focus on remote rendering for 3D content in single server. Although some researches lighten the burden of a server, they require complicated parallel programming model on high-performance graphics hardware.



III. DESIGN CONSIDERATION Remote rendering is used to solve the limitation of a device  which does not have enough computing resources for processing input contents. Typically, the remote rendering method has been developed for displaying 3D contents on a low-end device that has no 3D graphics accelerating capability.

Even though a device has the 3D accelerator, it can delegate rendering task to a high-performance machine in order to reduce processing delay. The same case happens to 2D image stylization because it also requires lots of rendering computation on a GPU.

While the remote rendering method lessens the processing overhead, it causes another overhead; communication delay between a client and a server. The communication delay occurs when a client sends source contents to a server, and when a server sends transformation result back to a requester. The delay is affected by available network bandwidth and contents size. Several Mbps connection speeds surely causes longer delay than Gbps networks with the same size of contents, and it takes longer time to send larger contents. Therefore, the remote rendering method has performance benefit only if the communication and server process overhead is lower than overall overhead in the case of only local rendering.

In addition to communication delay, a server can suffer from high processing overhead, if the input contents size is greater than the acceptable size due to its limitation of graphics hardware. In this case, graphics engine needs multiple rendering iterations because a buffer cannot read the full bytes of contents at a time. Server performance can be improved by multi-core graphics card and parallel programming model.

However, it needs high-performance hardware with parallel GPU architecture and knowledge of parallel computing platform such as CUDA.

Alternative approach with less cost is distributed rendering in which parallel processing is conducted by multiple servers as shown in Fig. 1. A client divides an image into several image pieces, and sends each piece to a different server. A server takes the same role of that of remote rendering, but it lessens the burden of rendering delay as it handles only separate part of the whole image. Rendered image piece is sent back to a client, which then collects results and generates a large rendered image. This approach can be easily extended by adding more servers, as the image size increases.

In order to realize the distribute rendering, a client must be able to partition an image into independent pieces, and rendered pieces must be reassembled such that boundary pixels of pieces are seamlessly connected. In other words, the recreated image must have the same quality as if an input image is stylized by one server. Another aspect to consider is additional overhead incurred by the distributed rendering. The overall overhead of distributed rendering consists of the partitioning and assembling process as well as communication delay. Distributed rendering would be a reasonable scheme, if the additional processing overhead is marginal and does not give a significant effect to the overall performance.



IV. STYLIZATION OF IMAGE DATA To stylize 2D image data in our system, we use a GPU-  based line drawing method proposed by Lee et al. [12].

Although this line drawing method renders a 3D mesh as a line drawing, it can be applied to any scene representation such as point set, implicit surface, or image-based representation because the method extracts lines from a shaded rendering of a scene. Therefore, it also works quite well using a 2D image as input like Fig. 2. In Fig. 2(b), dark lines are drawn along thin dark areas of tone and along boundaries between dark and light regions, and highlight lines are drawn along thin bright areas of tone. In addition, the method can capture the tone variations in broad region by combining toon shading with lines.

Fig. 1. Distributed rendering.

(a) input image (b) line drawing and shading?results  Fig. 2. 2D Image stylization.

We can draw lines with various line widths as shown in Fig.

3, and the line width is controlled according to desired level of details.

Viewing the tone image as a height field, highlight lines and dark lines correspond to ridges and valleys, respectively, as shown in Fig. 4. To extract lines along thin areas, we apply a ridge detection method using polynomial fitting. At each pixel, we fit a degree-2 polynomial to the tone values near the pixel.

In practice, we use 9 sample points arranged in a 3?3 grid around the pixel location, with spacing set to half the desired line width. We can determine if a pixel is on a ridge or valley using extracted geometric properties of a fitted polynomial.

However, since we use a degree-2 polynomial, we cannot distinguish two cases: a pixel near a ridge or valley (Fig. 5(a)) and a pixel on an edge (Fig. 5(b)). To distinguish such cases, we use an iterative search method, which moves the point sampling toward the detected ridge or valley line, fits a polynomial with new samples, and measure curvature and the distance to the new ridge or ridge line. In our implementation, the iterations are repeated at most five times. In the case (b), the new computed curvature falls below a threshold or the new location moves outside the fitted region. Details of the approach are described in [12].

(a) detailed lines (b) abstracted lines  Fig. 3. Control of line width.

Fig. 4. Ridges and valleys in height field.

Fig. 5. Ridge searching.

The line extraction process consists of two passes: the first to generate a tone image, which is a shaded rendering of a scene, and the second to detect ridges and valleys in the tone image. In the case of a 2D image as input, the input image itself can be used as a tone image and only a blurring step is performed to reduce noise in the image or sampling artifacts in the second pass. After the line extraction process, we can augment lines with toon shading to stylize tone variations in broad regions. The whole process is performed on GPU using a fragment shader, which uses pixels only in a local region around each pixel in all passes.



V. REMOTE DISTRIBUTED RENDERING For our distributed contents stylization, we separate the  required processes between a client and a server. A client is responsible for dividing an input image, merging result sub- images into one output image, and visualizing an output image.

A server takes a role of main processes for stylizing an image piece received from a client.

At the client side for the distributed rendering, we need to consider how to partition an input image. In this step, we need to make the boundary regions between sub-images shared by each other in order to provide the same inputs to a pixel shader at pixels around the boundaries as the inputs in the local contents stylization.

For each pixel, a fragment shader uses 9 pixels sampled around the pixel location to fit a polynomial in the second pass of the stylization. The sampling center can move inside the initial sampling region in the iterative searching process and we sample 9 pixels around a new sampling center for each iteration. As shown in Fig. 6, if the spacing between samples is w and the initial sampling center is at (x, y), the maximum horizontal or vertical distance from (x, y) to the pixels used by a fragment shader in the second pass is 2w. As pixel values are blurred by a Gaussian kernel of size k in the first pass of the  centered at each pixel from an input image to determine if a pixel is rendered as a line or not.

Fig. 6. A local region required for the processing.

In the image division step, we add a region of width 2w+k or height 2w+k along the boundary of each sub-image. Fig. 7 shows an example where an input image is divided into two sub-images horizontally. The dashed regions are added to each sub-image in order to compute pixel values correctly around the boundary between the two sub-images. Each sub-image is then sent to a different server and processed independently.

The solid-colored regions of the sub-images (I0 and I1) from different servers are sent back to a client, and are combined into a final line drawing result of the input image.



VI. COMMUNICATION MIDDLEWARE The proposed distributed stylization system is developed by  separating visualization process and rendering process between a client and servers, and the communication between participating nodes are realized by our communication middleware (CM) [14]. In this section, we introduce the supporting functionalities of CM for rapid development of the distributed stylization system.

The CM aims to provide an easy and efficient way of developing distributed applications. It supports various functionalities with options for different requirements of developers. The middleware deals with multi-user issues which have to be implemented by developers if it has only fundamental networking supports. Our system has a role of bridge between an application and underlying network infrastructure. Among basics to do this is to deliver messages and contents between these two entities, by which communicating nodes can interact with each another. With APIs provided by the CM, application developers can create, send, receive, and process an event. In addition to dealing with events, it supports other operations which detect a specialized event and conduct a dedicated service according to the event type. To support them, we follow the layered approach to design the middleware. Fig. 8 shows an overall architecture of the CM. In the point of view of applications, the CM consists of four main modules in order: CM stub, collaboration manager, event manager, and communication manager. We describe details of each module in the following sub-sections. To support the proposed distributed processing of image data, the CM is extended to enable multi-server-based client-server architecture. Additional processing servers can be dynamically added or removed to/from current server network according to the requirement of developers.

Fig. 7. Image partitioning.

Fig. 8. CM architecture.

A. CM Stub The CM stub is a core module which interfaces an  application. A developer can access most of supporting functionalities of the middleware with this module. In general, it provides APIs to start and stop the CM, register and deregister an event to be used among CM nodes, send an event, and assign an event handling callback function which is called by the CM whenever it receives an event from a remote node.

In addition to these fundamental operations, depending on the application type (a client or a server) and the communication architecture (client/server, peer-to-peer, or hybrid), which are assigned in a configuration file by a developer, the CM stub provides appropriate useful functions with various options. For example, if the application is a client type in the client/server architecture, the CM stub offers a function to connect to a server. What a developer has to do is to set the server information like the IP address and port in the configuration file and call the simple connection function.

B. Collaboration Manager The collaboration manager manages a different level of  user interaction area according to the application requirement.

It defines the number and relationship of sessions and groups in a hierarchical structure. As illustrated in Fig. 8, the collaboration manager contains session managers, each of which handles a session and a session manager can have more than one group manager which manages a user group. The developer can organize the structure of sessions and groups using the configuration file or APIs provided by manager modules. In the CM architecture, we need at least one session and group to allow users to interact with each other. It means that the minimal unit of interaction is a group. A user always has to belong to a group. For the proposed distributed stylization system, we use the default session and group which is the minimum requirement, because there is only one client for visualizing stylized images.

An event passing through the CM stub module from the application layer stops by the collaboration manager which then checks the destination users in sessions or groups, and delivered to the event manager before being sent. All inbound events from the network are also delivered to the collaboration manager. It checks the event header and does internal process if it is required. If an event handler is a session manager or a group manager, the collaboration manager delivers it to an appropriate manager.

C. Event Manager The event manager is in charge of an event in the system.

An event is a high-level form of exchange method between application nodes. As it includes semantics to be exchanged so that it can be understandable at the application layer, high-level manager modules (the collaboration manager, session manager, and group manager) and an application uses an event as a way of information exchange. One of key role of the event manager is to change an event to a low-level message (sequence of byte array) which is delivered to the underlying communication manager, and vice versa. The event manager provides several sending functions which take an event as a parameter and transform it into a message so that it can be sent via the communication manager. To the contrary, an incoming message given by the communication manager is converted to an event in the event manager which then delivers it to the collaboration manager for the internal operation. The event manager also forwards the converted event to the CM stub module which eventually delivers it to an application for the event process at the application layer as well.

D. Communication Manager The communication manager controls messages and  provides APIs to manage communication channels. The main role is to send and receive messages to/from underlying network. To support this, the communication manager runs a separate thread for waiting any incoming message. Whenever it receives a message from the network, the communication manager forwards it to the event manager. It also uses its own send functions embedded in each channel when a message to be sent is delivered from the event manager. If an application creates a channel, it is maintained in the communication manager as a channel list. Every event channel in the list is  checked by the receiving thread and any received message is notified to the event manager. The communication manager provides different dedicated communication sockets which wrap native socket APIs and makes it possible an easy way to open and close a channel depending on demand. Supported socket types are a server socket, a stream socket, datagram socket, and a multicast socket. For the proposed distributed stylization system, we use the server and stream socket between a server and a client application, because our system requires reliable communication of input and output image pieces.



VII. PERFORMANCE EVALUATION In this section, we discuss the analysis of performance of  the proposed distributed rendering scheme. The performance of remote and distributed rendering methods consists of local processing and communication delay. In the proposed method, an input image content, C, is divided into n pieces, Ci (1?i?n).

Ri is the rendered image piece of Ci, and R is a merged image of all Ri. S is a set of m participating servers, each of which is represented as Si (1 ?i?m). The overall cost of distributed rendering for a content C, CostC, is defined as following:  CostC = Proccl + Comm(cl,S) + ProcS + Comm(S,cl) ,  where Proccl and ProcS are processing delay at a client, cl, and servers, S, respectively. Comm(cl,S) and Comm(S,cl) are communication delay of content from a client, cl, to servers, S and vice versa.

Comm(cl,S) is the delay summation of transmitting all Ci to Si, and Comm(S,cl) is the summation of transmission delay from all Si to cl as described in equation (1) and (2), respectively. If the existing single-server-based rendering approach uses a server S?, its communication delays, Comm(cl,S?) and Comm(S?,cl), are almost same as that of the proposed scheme, because all image pieces must be sent. If multiple transmissions are done in parallel with separate channels, it can save the delay.

?   ? ????  ?            ?   ? ????  The processing delay of a server set, ProcS, occurs when participating servers process and stylize the input image pieces.

Comparing to our approach, the processing overhead of the single server method causes much higher overhead, because only one server must be responsible for rendering the whole content.

While the proposed scheme reduces its server processing and communication overhead, it pay higher cost for client processing overhead. Proccl is defined as equation (3) where Dpart is local processing cost for partitioning an input image, Dmerge is the cost for merging result image pieces, and Ddisp is the cost for displaying an output image.

Proccl = Dpart + Dmerge + Ddisp (3)     For the proposed scheme, Dpart and Dmerge are additional processing cost comparing to the single server approach, since the existing scheme does not need to partition and merge tasks.

We conducted an experiment to quantitatively measure the overall cost of distributed processing as we changed the number of servers and the size of input images. For the experiment, we implemented a test client/server applications using Visual Studio 2010 on the Windows 7. Test machines are connected through 100 Mbps LAN. When the client splits the image into pieces, the number of pieces is automatically decided to the number of connected servers. If the client connects to one server, it does not split the image.

The overall cost is measured as the total elapsed time from the moment when the client splits an image to the moment when it completes to merge image pieces. Fig. 9 shows the experimental result. Comparing to the case of single server, the two-server-case reduces the overall cost. Furthermore, the bigger is the image size, the more is the amount of cost reduction.



VIII. CONCLUSION In this paper, we proposed a distributed rendering scheme.

In our approach, a client splits the image into pieces, and then sends them to different servers. Each server receives an image piece and run a processing routine for the stylization of the piece. When processed image pieces are sent back to the client, it merges them into one image. Using multiple servers which stylize only parts of a whole image, the proposed system overcomes the limitation of single rendering server, and can process high quality image with marginal processing cost.

Currently, our research is still in progress, and we are planning to conduct more quantitative performance evaluations according to different image size and number of servers in an extensive manner. We also have a plan to extend our approach to adaptive distributed rendering technique. In this approach, the number of participating servers is dynamically chosen according to the size of input image in order to make the system more scalable than current fixed environment.

Fig. 9. Overall cost of distributed processing.

