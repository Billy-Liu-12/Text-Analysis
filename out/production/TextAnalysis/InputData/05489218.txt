A Real Time Vision-Based Hand Gesture Interaction

Abstract?Moving beyond mouse and keyboard, the evolu- tion of human-computer interaction (HCI) has been an interest research in recent years which witnessed the development from text-based like using a keyboard to graphic user interface (GUI) based on a mouse, from cumbersome data gloves and tracking devices to visual-based computer application. One of the interest fields is by using hand gestures to interact with computer. However, the complexity of a hand set a lot of challenges to be tracked. In real-time, the application requires high accurate detection and recognition. In additional the real and clutter environments have a big impact on recognition process because it included with irrelevant information from the application point of view. In this paper, a real time vision based hand gesture interaction prototype was proposed.

Currently a prototype has built for controlling the desktop cursor and concerned the tasks involving in navigation the desktop cursor by using hand gesture input modality.

Keywords-human-computer interaction; computer vision; hand gesture recognition;

I. INTRODUCTION  Human-computer interaction (HCI) can be described as an interaction between user exchanges of information with computer system [1]. For years, human-computer interaction was mostly based on mouse and keyboard to control the computer. The human-computer interaction research and development was therefore becoming an interest by many researchers in this recent year. There are several techniques introduced for the new Era of human-computer interaction such as face detection, speech recognition and motion ges- ture. As an example in mixed reality (MR), it opens a new direction for human-computer interaction which combined with computer vision techniques and it is possible to build an advanced input devices.

The computer vision devices can be implemented and upgrade to the new input devices in the future. It gives the input command to the computer rather than just a function of taking photo or record video. We can do more implementation to transform the computer vision devices to become an input command device to reach the function as keyboard or mouse. One of the ways to give signal to computer vision devices is by using hand gesture. More specifically hand gesture is used as the signal or input modality to the computer. Certain signal can be recognized by computer as an input of what computer should do. These  will benefits the entire user without using a direct device and can do what they want as long as the computer vision device can sense it. These make computer user easier than using the keyboard or mouse. The future computer or laptop may eliminate the use of keyboard and mouse by substituting with a vision-based interpretation devices.

Human communication comes in many modalities, in- cluding speech, gestures, facial and body expressions [2].

One of the interest fields is by using hand gestures to interact with computer as a non-contact human computer input modality. In the human-computer interaction literature, gesture has been used to determine various types of human hand movements for controlling of computer processes. The hand gesture interaction with computer is an interesting part in the study of human-computer interaction. In particular, visual interpretation of hand gestures can help in achieving the ease and naturalness desired for human-computer inter- action. Nowadays, there are several obstacles for achieving robust and efficient hand gestures detection methods in the real time environment. The complexity of a hand set a lot of challenges to be tracked and interpreted. The fact that the different gestures complexity such as variability and flexibil- ity of articulated hand structure, shape of gestures, real-time performance, varying illumination conditions and complex background noise. In real-time, the application requires high accurate detection and recognition. In additional the real and clutter environments have a big impact on recognition process because it included with irrelevant information from the application point of view.

Besides this, hand gesture can help in convey the message to the other. For example sign language is a set of gestures which was one of the most natural ways of exchanging information for most deaf people. This research aim is to develop a prototype using vision-based hand gesture interaction with computer. In our research, we are proposing a more effective and friendly methods for intelligent human- computer interaction by using hand gestures. The hand ges- tures are used to simulate and control the movement of the desktop cursor by replacing the function of the mouse. The challenge for this research is the complexity of the hand and the difficulty in detection and recognition process. In real time, the prototype requires a high accuracy in detection and recognition. Noisy encountered in the environment may have   DOI 10.1109/AMS.2010.55     a big impact on the detection and recognition performance of the hand gesture. The prototype is utilized a low cost computer camera as input tool to capture the video frame followed by the process of video frame in the real time.



II. LITERATURE REVIEW  The early technology of hand gesture detector used me- chanical device to retrieve the information of the hand ges- ture [3]. One of the example included data glove devices [4, 5], which user send the information to the computer system through the movement of fingers. Data glove is quite popular at the several years ago; however, the development of the computer hardware improved a lot in this recent decade make the hardware cheaper and offers a better performance in computing. The vision-based hand gesture slowly replaces the role of data glove with non wearable devices because it is more naturalness without using any devices in the hand and user friendly which is important in human-computer interaction. Compared with data glove, it look cumbersome and limitation in the movement of hand. Because vision is one of the six physical media that computer must be instantiated perceptibly when communicated to humans [1].

Hence, vision-based approach is more than wearable devices in hand gesture recognition. Normally, most of the gesture recognition systems divided into three stages which are im- age pre-processing follow by tracking and finally recognition stage [6] as shown in Figure 1.

Figure 1. Three common stages of gesture recognition systems  In tracking, there are several researchers who have done the similar research like Viola-Jones based cascade classifier, commonly used for face tracking in rapidly image processing [7, 8]. Cascade classifiers are currently considered more robust pattern detection against the noises and lighting conditions as well [9]. These methods also proved by many researchers that not only used to tracking for faces but also in recognize hand or other part of human body [9, 10, 11, 12, 13]. Other existing application liked HandVu by Kolsch and Turk [10, 11] which is the improvement from the Viola and Jones research [7, 8] and can be used to recognize six  types of hand gesture. A snapshot of the HandVu application is shown in Figure 2, the figure shown the two difference type of gesture recogntion in HandVu.

(a) First type of HandVu  (b) Second type of HandVu  Figure 2. Snapshot of HandVu application  Apart from this, Marcel and his colleagues proposed a new type of hand gesture recognition method based on Input-Output Hidden Markov Models by tracking the skin- color blobs of the human body [14] to reach the goal of recognition of two classes of gesture. Chen and colleagues [15] applied hidden Markov model method in training the hand gesture to recognition the hand postures. However, the hidden Markov model is more complex in training the hand gesture compared with Cascade classifiers. Recent works by many researchers have shown that hand gesture system is not just technical and theoretical but also practical implement into several type of application systems and various environment. For example Ahn et al. designed a new interactive way for the slide show presentation system in the virtual environment [16] show on the screen. Jain proposed a vision-based hand gesture pose estimation for mobile devices with single pointing gestures [17]. The other example is the sign language tutoring tool studied by Aran et al. [18] which their research designed to teaching the fundamental of the sign language in interactive way.



III. METHODOLOGY  A. Hand Gesture Training  The proposed real-time vision-based hand gesture recog- nition is using a cascade of boosted classifiers based on the Haar-like feature [8, 9, 19]. This approach provides a robust and cheap computing in pattern recognition in nearly the real time. The originally of the approach is proposed by Viola and Jones [7, 8] to recognize face and the algorithm also suitable used for hand gesture recognition. Before training, first a set of suitable hand postures need to be chooses in case to make the computing smooth while training and help to reduce the error rate, and also reduce the confusion when detecting hand gestures of the prototype. In the multimodal interface, hand gestures can be combined with other input modality like speech in a variant environment [20]. This research will simulate the movement of desktop cursor, hence a fist or close hand has been selected to simulate and replace the movement of desktop cursor because it has the less confusion cause [9] and palm or open hand to simulate no movement for the desktop cursor as shown in Figure 3.

(a) Open hand/palm (b) Close hand/fist  Figure 3. Selected hand posture for training. (a) Open hand or palm posture is selected for one of the selected hand posture, (b) Close hand or fist is chosen as the second selected hand posture  During training, a collection of 200 positive samples images for each posture and 400 negative samples images used to train with the classifier. The training process is using Haar Training system of OpenCV library [21] .The outcome of the training adapted to implement into the algorithm of prototype to recognize the hand gestures after the test samples successfully. The flow for the process of Haar training is shown as Figure 4.

B. Hand Recognition  Unlike most of the researcher, they using hand contour [22] and skin color detector to do the segmentation and track the positioning for the coordinate of hand gesture [20, 23, 24, 25]. However, one of the major different in this research is tracking and recognition happens in simultaneously. This approach is quite similar to the Kolch and Turk which their work can recognize six hand posture [10]. Below is the short description for the process of recognition of hand gesture.

Figure 4. Process of Haar training  ? Step 1: Put the hand at inside the ranges of the camera angle view.

? Step 2: When the hand is in open palm like Figure 3 (a), which meaning the recognizer freeze, and when the hand is in fist like Figure 3 (b), the recognizer will recognize as meaningful input command to the prototype.

? Step 3: The prototype will integrate with the computer application and start simulate in the posture of the fist until the hand shape change.

C. Process Flow of Integration  The flow of the integration process is explained from start- ing the prototype until the end of the prototype execution:  ? Step 1: Start the prototype and initializes the computer camera to capture the video for the real-time.

? Step 2: Move the hand inside the ranges of the camera view, the recognizer will process by processing the image frame whether it is the selected hand gesture.

? Step 3: If it is the selected hand gesture, the recognizer will select the closest selected hand gesture.

? Step 4: The prototype will simulate the movement of the desktop cursor by using the correct hand gesture.

? Step 5: The simulation end up when the selected hand gesture not longer in the camera view.

In the condition when more than single selected hand gesture, the nearest detected hand gesture will be the input modality. The other hand gesture will be ignore by the prototype. This is to prevent the confusing of the prototype when processed the hand gesture. The flow of the process diagram is shown in Figure 5.



IV. FINDINGS  An experiment is undertaken to test the functionality of the prototype. The prototype is testing at indoor environment because of the lighting and illuminate condition in a room is optimized for the recognizer. Figure 6 (a) shows the experiment environment for testing the vision-based hand gesture recognition in the real time. Figure 6 (b) shows a selected hand gesture captured by the camera in the free condition or open hand posture. In this gesture, the recognizer will detect as no activity or event for the desktop     Figure 5. The Prototype Process Diagram  cursor. Figure 6 (c) shows a red square boundary was draw when the selected hand gesture in the fist posture is captures under the camera view in the display to show that the hand gesture is been detected as a selected hand gesture with activity condition.

The experiment is successful as it reaches the target of interactions with the desktop cursor by using computer vision method, computer camera with hand gesture as the input modality in a real-time. The movements of the desktop cursor move in the smooth condition. The hand gesture does not confuse the recognizer in the static backgrounds and in suitable environment.



V. CONCLUSION  In this paper, we proposed an alternative method to re- place the traditional way that human interact with computer.

The prototype basically meet the objective of this project that is design and develop a prototype that enable user to use vision-based hand gesture to open an application of the computer desktop. There are many researchers doing their research about vision-based hand gesture and recognition but it is quite messy because there are no such general approaches that can be a standard for vision-based hand gesture interaction. The hand gesture interaction can serves as improving the interaction between human and computer.

The evolution of computer technology have reached that human can interact with computer by using non-verbal language. Therefore, there is needed for a robust and effi- cient approach for the vision-based hand gesture recognition that can work well in a real-time environment. This paper implements this new knowledge which can be a guideline to other researchers by using computer vision technology.

This technology should popularize because it is believed that  (a) An experiment lab  (b) A palm or open hand  (c) A fist or closed hand  Figure 6. The open testing of vision-based hand gesture in a real-time environment. (a) The indoor experiment lab with less clutter background, (b) The hand with open postures with no activity, (c) The fist been detected as trigger an activity of the computer.

the future environment will be full of computer vision hand gesture in controlling the machine.



VI. FUTURE WORK  The current prototype is shown to work fairly well for hand gesture recognition in the real-time. However, there are a few efforts that can be undertaken to enhance the proto- type. Current prototype only supports single hand in gesture interaction. Hence, multiple hands gesture interaction can be proposed. In the mean time, multiple hands increase the computational costs and complexity of the system. A robust approach is necessary to reduce this problem and complexity of hand gesture recognition. In the future suggestion, an evaluation testing on the prototype will be carried on by     several groups of users. Their responses, feedback, and observations toward their experience when testing with the prototype will be recorded during the evaluation testing.

Beside this, we will be focusing more on the improvement of the vision-based hand gesture recognition. In order to increase the accuracy rate, user satisfactions, and reduce the error rate, more hand posture will be selected to support the activity that can be done on the desktop. For our future work, we will propose a multi modal human-computer interaction system by integrating speech recognition together with the vision-based hand gesture interaction. By enhancing speech recognition into the system, the obtained result is predicted to be better comparing to only a single input modality.

