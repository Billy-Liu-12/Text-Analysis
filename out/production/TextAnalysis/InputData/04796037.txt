Finding Frequent Item Sets from Sparse Matrix

Abstract?According to the features of sparse data source while mining association rules, the paper designs a special linked-list unit and two strategies to store data in matrix. A novel algorithm, called SMM(Sparse-Matrix Mining), is proposed to find large item sets from sparse matrix. SMM maps database into a binary sparse matrix and stores compressed data into a linked-list, from which to find large item sets. It uses less I/O and computational time in mining. Experiments show that SMM finds large item sets efficiently and is well scalable.

Keywords-frequent item sets; sparse matrix; Compress by Column; linked-list

I. INTRODUCTION Mining association rules[1] is one of the most important  research content in data mining. Researchers have presented many algorithms which mainly consist of two major categories: The first is the methods which produce candidate item sets, such as classical Apriori algorithm[2] and its improved ones[3][4][5].The second is Patten-Growth method, such as FP-tree[6],and its improved ones[7][8]. FP_tree method uses the quality of Apriori but does not produce candidate itemsets. Based on the above-mentioned algorithms, many researchers present various improved ones, yet the data most of those methods target are common database, the performance evaluation of these algorithms is only experimentally proved in a research sense. In practical application, data source may indicate certain special characteristics, and there are not many researching documents focusing on special database, such as supermarkets? sale database. A big supermarket has thousands of goods, and its daily transaction at least includes thousands of times. But every time the goods sold at least include one kind, at most dozens of kinds. So the useful information stored in each transaction data is very little. If the whole database?s data are treated indiscriminately, obviously large amount of futile calculation will be operated.

This thesis proposes an algorithm named SMM(Sparse- Matrix Mining) focusing on finding large itemsets on sparse data. It also adopts the same method to generate large itemsets like Apriori. It presents a new data structure based on storing strategies of sparse matrix. SMM deletes large quantities of invalid data, thus reducing greatly the database scanning times and calculation time.



II. THE BASIC CONCEPTS OF DATA MINING AND APRIORI ALGORITHM  The basic concepts of association rule are presented in [1]. Suppose item set I={i1,i2,?,im} consists of m various items, D is transaction database, among D every transaction T is one of I?s subset, i.e. T I, every item has a unique ID called TID. One set items is called itemset, so k-itemset is a set including k items. The support of itemset X refers to the times of the transaction data which includes the specific itemset. If an itemset?s support is bigger than the minimum support specified by the user, then it is a frequent one, and the frequent itemset with a length of K is called frequent k- itemset. Association rule is a formed like A=>B, among them A? I, B? I, and also A B= . Support in rule A=>B is defined as the percentage of items with A B in D, it?s the occurrence probability of itemset A B in D.

sup(A=>B)=|{T:A B T}|/|D|. The confidence of rule A=>B is defined as the ration between the number of items including A B in D and the number of items including A in D, it is the occurrence probability of itemset B when itemset A appears, con(A=>B)=sup(A B)/sup(A), the rule with confidence bigger than the minimum confidence specified by the user is confident. The task of mining association rules is to find from database D the rule A=>B, in which the support and confidence suit separately the minimum support and confidence specified by the user.

Apriori is a basic algorithm to find frequent itemset, that  is, to find all the itemsets whose support is equal to or bigger than the given minimum support. Its fundamental theorem is to use an iterative technique called search layer by layer, to use k-itemset to find (k+1)-itemset. In the first scanning of the database, the support of all items is counted, and frequent 1-itemsets L1 is identified. In the following Kth time of scanning(k>1),first we use (k-1)-itemsetsLk-1 to produce k candidate itemsets Ck, and scanning the database to identify the support of all candidate itemsets to produce Lk. The producing of Ck is based on the support?s quality of downward closure, i.e., if an itemsets is frequent, then all its subset are also frequent.

The biggest disadvantage of Apriori is that it needs  scanning the database for many times, the utilization of memory is low [9]. Therefore, this thesis presents an improved storing strategy of sparse matrix and a new linked- list unit to delete the futile data. This algorithm only needs   DOI 10.1109/ICECT.2009.69     one scanning of the database, thus greatly reducing the I/O and improving the speed of operation.



III. THE DESCRIPTION OF SPARSE MATRIX  A. The Design of Linked-list Unit There are various storage structures of sparse matrix[10],  in order to make the adopted data structure suit to produce lager itemsets and improve the calculation efficiency, this paper designs a kind of linked-list unit and two strategies of sparse matrix.

As shown by Fig. 1, the linked-list unit consists of two  parts: The head table(HB) and transactions table(TB). HB is used to store large itemsets and their corresponding address information. Every item has two parts: Content which stores the content of the itemset, and address which stores the first position in TB related to the specific itemsets. TB is a linear linked-list storing TIDs related to the specific itemsets. The position of the first TID supporting itemset ISn is stored in correspondent address part in the HB, the integer stored in S(ISn) refers to the number of transactions related to the specific itemsets after that. Tk1, Tk2, Tks is a sequence of TID, among them k1<k2<?<ks. The last value of the linked-list is the flag of the end.

Other things needed are a support table SB and length  table of transactions LB. the SB use the formulation of TB, and its address part stores the support of corresponding itemsets. In order to find rules conveniently, it is used to record the support of large itemsets.

LB is a linear array storing integer values, produced  when mapping database into binary matrix, and it records the number of non-zero data in each transaction and used as pruning conditions when candidate itemsets occurs.

B. Binary-storing of Database In the database, the number of a specific kind of goods in  one transaction is an integer which is equal to or bigger than zero. But since the goods? name is the only mark that can identify a kind of goods, and in the process of generating rules, we are not concerned about the number of a particular kind of specific goods in the transaction. So binary-storing is adopted, and the transaction database is mapped onto a binary matrix. If in one transaction, a kind of goods occurs, then the corresponding element of the matrix is 1; If not, it is zero.

According to the above analysis, to a given database D, f:  ???. Where R=f(D)=(rij)nxm.

C. Compress by Column Compress by column is a generally-used compressed  storing strategy of sparse matrix [11]. Three arrays are applied: AA, JA and IA. AA stores non-zero elements of the matrix from column 1 to column n, JA refers to the line number of the corresponding elements in AA, the elements in IA are the index numbers of the beginnings of each line.

Fig. 2 shows a binary sparse matrix formed by mapping a  transaction database.

The matrix can be stored in standard format of  compressed by columns as the following:  Figure 1. A linked-list unit  Figure 2. Binary matrix  AA: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 JA: T1 T4 T5 T7 T8 T9 T1 T2 T3 T4 T6 T8 T9 T3 T5  T6 T7 T8 T9 T2 T1 T2 T8 IA: 1 7 14 20 21 Since the matrix is binary, then the non-zero data are  identified as 1, and needs no deliberate record, so array AA can be overlooked. And in array JA, at the beginning of every line of data, the number of data in this line is recorded, thus array IA can be omitted.

In the real process of this algorithm, the above storing  results can be further condensed. According to the minimum support, the corresponding line information of the itemsets whose support is smaller than the minimum support can be deleted from the list. For instance, when the minimum support is 2, the above-mentioned matrix will finally be changed as the following: TM: 6 T1 T4 T5 T7 T8 T9 7 T1 T2 T3 T4 T6 T8 T9 6  T3 T5 T6 T7 T8 T9 3 T1 T2 T8 After the mapping and transforming of the transaction  database, the counting of the large itemsets can be processed in a linear linked-list.



IV. SMM ALGORITHM AND ITS FULFILLMENT  A. The Description of SMM 1. Binary sparse matrix is produced. Scan the database  once line by line, and map the database into a binary sparse matrix, meanwhile count the length of every transaction.

2. Frequent 1-itemsets is produced. Scan once the  produced binary sparse matrix column by column, to every column i (corresponding to a specific kind of goods). If the number of 1(support) in column i is equal to or larger than the given minimum support, then the related transaction number and sequence of TID will be written into TB in the format specified by TM, meanwhile, The contents of the itemset corresponding to that column will be written into HB, and store the S(ISi) in TB into the address part of the itemset.

Also support is written into SB.

3. Candidate k-itemsets is produced from (k-1)-itemsets.

According to Apriori method, the (k-1)-itemsets produced in the last step is connected, and according to the character of downward closure of frequent titemsets, a possible candidate k-itemsets is produced.

4. k-itemsets is identified. According to the produced  candidate k-itemsets, compare the transaction number corresponding to the two (k-1)-itemsets which produce the itemsets in TB. If the number of the same transaction number is equal to or bigger than the minimum support, then the candidate k-itemsets is frequent then record the related information. When recording the sequel of transactions into TB, check the length of every transaction(LB ) in the process.

And delete the transaction which cannot possibly include (k+1)-itemsets, meanwhile modify the transaction number S(ISi) 5. The next possible candidate k-itemsets is produced,  then step 4 is repeated until all the candidate k-itemsets are produced.

6. Step 3 is repeated until k+1 frequent itemsets cannot  be produced any longer.

B. Example Demonstration In the sparse binary matrix shown in Fig. 1, suppose the  minimum supports 1, the finding process of frequent itemsets is described as the following: 1. To produce 1-itemsets. Scan the matrix column by  column, the number of 1 in the first column is 6, 1 occurs in line 1,4,5,7,8 and 9, so I1?s support is 6, bigger than the minimum support 2. Write I1 and its corresponding support6 into SB. And according to the information in TL, the length of all the above six transaction is bigger than 1, so write 6 into the subscript positions of 0 in TB, then fill in the related 6 transaction numbers in turn. Fill I1 into the Content part in the first item of the HB, meanwhile set the address 0. Then continue to scan the following other columns, record all information. Since I4?s support is 1, so I4 is not frequent1- itemsets, not recording all related information. The results produced are showed as Fig. 3.

2. To generate 2-itemsets. Finding frequent2-itemsets  from the linked-list shown by Fig. 3. According to Apriori method, we execute connection operation on the itemsets in HB continuously. Itemset I1, I2 is found by connecting the first two items I1 and I2. According to the contents of its address indicator, find the same transaction number and its quantities in TB from the six transactions whose subscript number starts from 1 and from the 7 transactions whose subscript number starts from 8, then obtains four same transactions with TID 1, 4, 8 and 9, that is the support of itemsets I1, I2 is 4, record this information into SB. Then because the length of transaction 4 is no bigger than 2, it is impossible to be used to produce frequent 3-itemsets, so in TB, record transaction number 3 and its corresponding transaction code 1, 8 and 9. And the result of producing 2- itemsets is shown as the Fig. 4.

3. To produce 3-itemsets. Repeating the above-  mentioned process, from Fig. 4, we can find frequent 3- itemsets. The result is shown in Fig. 5.

I1 I2 I3 I5   HB  TB 6 279875 141 3 753698 664 198 3 2 8 -1  I1 I2 I3 I5   the linked-list supports  Figure 3. The large 1-itemsets produced  Figure 4. The large 2-itemsets produced  Figure 5. The large 3-itemsets produced  If |Lk-1|<k, then frequent k-itemsets cannot possibly produced[1]. As there are 2 3-itemsets, so it is impossible to produce 4-itemsets, so the operation ends.

This method produces two itemsets I1, I2, I3 and I1, I2,  I5, which is the same result as that gotten from using Apriori algorithm directly. Compared with Apriori, SMM has the following improvements: This method produces two itemsets I1, I2, I3 and I1, I2,  I5, which is the same result as that gotten from using Apriori algorithm directly. Compared with Apriori, SMM has the following improvements: 1. It only scans the database once, thus greatly reducing  the I/O cost.

2. It uses special data structure, and removes large  amounts of futile data, thus reduces the usage of memory and the counting time.

3. In the calculating process, pruning columns and rows  are conducted at any moment to delete the items and transactions which are not useful to the next calculating step.

4. The candidate sets produced are judged instantly, it  need not extra space to store candidate sets specially.



V. RELATED EXPERIMENT RESULTS The validating experiments are conducted on Windows  XP, CPU Pentium 1.4G, 384M DDR. The database used in the experiment is a medium-sized supermarket?s sale database of one month, all together 7,130 transactions, the goods sold are classified into 107 categories, the database size is 11.6M,the percentage of useful data is about 18%, the     experiment is used to find the relationship of these categories in the selling. Three aspects of research are conducted separately in the experiment. First, in order to prove the optimization effect that SMM has compared with Apriori, compare their separate processing time under the circumstance of different support; secondly, in order to observe SMM?s capability to the number of transactions. The support is confirmed(1%), the number of transactions is changing, Under this circumstance, compare the running time of SMM and Apriori; Thirdly, in order to observe SMM?s capability to sparse extend of database, we use randomly-produced binary matrix(2000 transactions, 50 items), when the supports defined(1%), and the data?s sparse degree is different, we compare SMM?s operating time. The experiment results will be separately shown in Fig. 6, Fig. 7 and Fig. 8.

From the experiment results, we can see that, under all  circumstances, the capability of SMM is obviously superior to Apriori. Especially as the set minimum support increases, the relative speed of SMM to Apriori has the tendency to increase.

0 10 20 30 40 50        TI M E (m s)  MIN_SUP(%)  SMM  0 10 20 30 40 50        Apriori  Figure 6. Running times of SMM and Apriori when the minimum support different  1000 2000 3000 4000 5000 6000 7000       TI M E (m s)  TRANSACTIONS  SMM  2000 4000 6000      100 Apriori  Figure 7. Running times of SMM and Apriori when the number of transactions different  0 20 40 60 80  TI M E( m s)  RATE OF DATA(%)  Figure 8. Running times of SMM on different sparse data  But when the processed transactions increases, its relative speed decreases. The reason is: After SMM produces frequent k-itemsets, all frequent k-itemsets and related transactions need to be kept in memory. When support is relatively small, frequent1-itemsetsand frequent 2-itemsets?s numbers are very large, accessing these data will cause many times of page defaults, thus the calculating speed decreases.

Similar cases will appear when the number of transactions increases. The above can be proved by the experiment result in Fig. 8. When non-zero data increase, the data needed to be kept in memory increase, if the expense dealing with page defaults reaches a certain rate, it will counteract the algorithm?s advantage in I/O, and thus the speed will decrease.



VI. CONCLUSIONS SMM is targeted onto special sparse data; it uses and  improves sparse matrix?s storage strategy, and presents a pertinent linked-list unit. SMM considers and fulfills its optimization approaches in the following aspects: reducing the times of scanning the database, efficiently deleting futile items and transactions, condensing the expression of transaction database, and so on. The preliminary experiment results show that the capability of SMM is obviously superior to Apriori algorithm and it possesses good scalable.

