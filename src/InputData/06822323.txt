Power-Laws and Structure in Functional Programs  Ron Coleman

Abstract?We downloaded from GitHub, Inc., a portfolio of recently updated open source Scala projects containing 16,500 source files with 2.3 million lines of code and 223,000 methods written by hundreds of programmers. A related paper found in this ?big data? of functional programs non-Gaussian distributions of lines of code and cyclomatic complexity and speculated that the distributions resembled those of phenomena known to be fractal in structure, following power laws. In this paper we show that power-laws are the best, most parsimonious explanation of the distributions according to R2 analysis and minimum description length principles. We show furthermore that the data are robust and the power-law explanations are persistent even when the largest project in the portfolio, the Scala compiler and standard library, is removed. To our knowledge these findings are the first such report in the literature of power- law distributions of software complexity in functional programs.

Keywords?Power-laws; minimum description length; functional programming; Scala

I. INTRODUCTION Computer programs have structure [1]. Software  complexity metrics have been developed, more than 100 by some estimates, to quantify this structure, which is manifest in the lexical organization or ?style? of source code [2, 3]. Kokol, Brest, and Zumer reported evidence of fractal structures as long-range correlations of characters, operators, and strings in a collection of 20 FORTRAN programs [4].  Kokol and Brest studied a similarly small sample of randomly generated Pascal programs and identified fractal structures therein, too [5]. Their goal had been to devise a new type of software complexity metric founded on fractal statistics [6, 7]. Indeed, Kokol and others working in this line developed fractal metrics [8]. The advantage of their method, they pointed out, is it is language independent. The disadvantage is the fine details in character, operator, and string patterns obscure and ignore semantics of higher-level structures, scope, declaration, selection, iteration, etc., that are indispensible in understanding and refactoring program complexity. Nevertheless, the authors allege [5] the fractal metric is correlated with measures like lines-of-code (LOC) and McCabe?s cyclomatic complexity (M) [9].

In contrast, in this paper we are not studying source per se as in Kokol et al [4,5,6,7,8] but distributions of complexity measures about source, in our case, Scala [10]. Scala is a modern Java Virtual Machine (JVM) [11] language that blends functional and object styles and in many ways exemplifies what appears to be a renaissance of interest in functional programming [12, 13, 14, 15, 16, 17, 18, 19, 20] for developing practical applications. Indeed, the paper [21] analyzed LOC  and M of ?trending? Scala functional programs downloaded from the repositories of GitHub, Inc. [22] to investigate assertions by functional programming proponents that elaboration of the lambda calculus is a better way to modularize the structure of programs and hence, reduce complexity [23]. That paper reported, among other things, that LOC and M for Scala codes were correlated just as Hatton had found for FORTRAN and C codes [24]. However, the frequency distributions of LOC and M for Scala source reported by [21] were distinctively non-Gaussian. In other words, the Scala programs appeared to be highly modular in structure with a preponderance of short (LOC?3), straight-line (M=1) methods which seemed to support Hughes? contentions regarding why functional programming matters [23]. Yet [21] only speculated that the non-Gaussian distributions of LOC and M might follow power-laws, possibly being fractal [25].

We explore the matter here and find empirical evidence that power-laws are the best explanation of the data according to R2 analysis [26]. We show furthermore that power-laws are also the most parsimonious explanation according to minimum description length (MDL) principles [27]. Namely, if L(H) is the length of a hypothesis, H, to model the frequency distribution, D, and L(D|H) = b / R2 where b is the basis or number of unique values of LOC or M in D given H, then L(H)+L(D|H)  is minimized when H is a power-law. Finally, we show that power-laws are the most parsimonious account of the data even if the largest project in the portfolio, the Scala project (i.e., the compiler and standard library being largely written in Scala) were removed from the portfolio. To our knowledge, these results are the first such report in the literature of power-law distributions of software complexity measures in functional programs and particularly, Scala.



II. MOTIVATIONS AND METHODS  A. Scala projects The paper [21] compiled a portfolio of Scala projects  hosted on GitHub, Inc. [28], which claims to be the world?s largest open source community [22]. During the period 15-30 June 2013, we selected and downloaded all the projects that GitHub had identified as being recently updated by the project developers. They included the Scala compiler/standard library [10]; the Twitter, Inc., server and libraries [29]; several large, commercially inspired projects like Lift [30], Akka [31], and Casbah [32]; and many smaller and lesser known projects for computational finance, computer graphics, networking, web services, cryptographic algorithms, and artificial intelligence, among others. One exception was the Twitter source. The portfolio initially included some but not all the Twitter projects   DOI 10.1109/CSCI.2014.112    DOI 10.1109/CSCI.2014.112    DOI 10.1109/CSCI.2014.112    DOI 10.1109/CSCI.2014.112     written in Scala. Yet in the interests of possible future research, [21] gathered all 41 Scala projects on the Twitter page of GitHub [29], even if they weren?t recently updated. The other exception was Casbah, a database project. It had been used in testing of the compiler kit (see below) and was included in the portfolio for the sake of completeness. In total, the portfolio contains 262 Scala projects with 16,500 source files, 2.3 million LOC (i.e., 1.5 million LOC without comments), and 223,000 methods.

B. Sclastic To analyze the complexity of Scala programs, [21]  developed an experimental compiler kit, called Sclastic. The kit is almost entirely written in Scala and completely hosted by GitHub [33] (Sclastic is not part of the portfolio of projects we study here.) As a thorough review of Scala and Sclastic are beyond the scope of this paper, we offer only a minimalist outline of them. Readers may wish to consult Odersky, Spoon, and Venners [10] and/or the Sclastic source on GitHub [33].

Suffice it to say that Scala implements functional programming constructs with an object-oriented approach.

That is, there is the class structure, which contains zero or more functions also known as methods. While functions objects may be named or anonymous, they are instantiated, invoked, etc.

within the scope of a class.

The Sclastic compiler reads Scala source directly from the GitHub zip download in three distinct phases. The de- commenting phase strips the source of comments and empty lines. The parser phase identifies lexical objects, scopes, methods, and decision points (see below). The final phase, the method compiler, calculates LOC and M per method.

C. Line counts A line in Scala is a sequence of characters terminated by a  newline character. There are, however, two line counting considerations in relation to methods. First, class definitions may have statements outside of a method, the lines of which Sclastic interprets as class constructors. Second, Scala permits inner definitions of method and classes, that is, methods within methods, classes within classes, etc. In this situation there is the ?nominal? line count, which is the count of lines within the lexical scope of a method or class definition, including all inner definitions. There is also the ?effective? line count. This is the count of lines of a method or class excluding inner definitions. We used only the effective line count when calculating LOC per method.

D. Estimating M Sclastic uses a combination of two approaches to estimate  M. The first approach is based on an analysis of program flow given by McCabe [9]:  M = E ? N + 2 P (1)  where E and N are the number of edges and nodes, respectively, in the program flow graph, and P is the number of exit points for a given method. However, McCabe also provided a functionally equivalent simplification for Equation 1, namely,  M = ? + 1 (2)  where ? is the number of Boolean selection statements or decision points in a procedure. The signatures of decision points for Scala are if, while, and case-match statements and && and || operators. The signatures of these decision points are ?hard? in the sense that they are part of the Scala language.

Sclastic stores the ?hard-coded? signatures in a database, that is, a ?book,? which Sclastic queries when compiling the source to calculate ?.

As for the second approach, counting hard decision points alone is inadequate for properly estimating M because Scala source may also make decisions in the context of high-order functions that take Boolean returning function objects. We call these higher-order functions ?predicate contexts.? Some examples of predicate contexts are methods like filter, count, and find, which are defined on collections in the Scala standard library. However, Scala language designers may some day refactor the standard collections. Programmers may also extend the standard library and add new collections with new predicate contexts or develop collections with predicate contexts that are independent of the standard library. For these reasons the signatures of predicate contexts cannot be known in advance. They must be learned in an adaptive or ?elastic? manner that depends on the input source code. Thus, Sclastic makes one extra pass over the entire portfolio to collect the soft signatures and store them in the book during the parsing phase.

To calculate ? Sclastic queries the book and counts the number the hard and soft decision points within the lexical scope of a method.

E. Model selection When [21] applied the above methods to the portfolio, it  obtained frequency distributions of LOC and M per method that were distinctively non-Gaussian, resembling those of physical and aesthetic phenomena known to follow power-laws [25]. A homogeneous power-law has the functional form:  f (x) = c x? (3)  where c and ? are constants.

We test six other, alternative hypotheses and use R2 analysis and MDL principles to rank them. Each hypothesis, H, corresponds to one of four functional forms in the table below.

Table 1 Hypotheses (H), functional forms, and transformations H f (x) $ (f (x)) power-law c x? c * x ^ ? exponential c e?x c * E ^ x ? logarithm c ln(x) + ? c L x + a polynomial, deg=d c  + ? c x  for i=1..d c  + c  x + c  x ^ 2  + ?  In the case of polynomial functions, we use d=1-4. We use Microsoft Excel to fit the frequency distributions to each H and compute the R2 statistic.

We posit that ?simpler? formulations of f(x) connote greater parsimony. To approximate this ideal, the function, $, converts f(x) to a string. $ removes white space and parentheses and reduces power, exponential, and log functions to single letters.

$ preserves variable names and precisions numerical constants generated by Microsoft E returns the number of characters in the s $(f(x)).len gives a ?na?ve? or statistically unin of  f (x).

For instance, for the model,  f(x) = 191838x-2.615 we have  $ (191838x-2.615) => 191838*x^-  Here $(191838x-2.615).len = 15.

F. Minimum description length principle We use R2 as the primary means to assess  However, using R2 alone ignores parsimon provides one means that takes both fitness an account. Specifically, we look to minimize L compute L(H) as shown in Table 1 above the of the resulting string. That is,  L(H) = $ (f (x)).len  We model L(D|H) as  L(D|H) = b / R2  where b is the basis, which is to say, the num unique values of LOC and M. In a convention basis is the number of unique bins excluding zero frequency counts.

There is a need to create a ?true? histogram M distributions with zero frequency bins. T compare like distributions from different Kolmogorov-Smirnov (K-S) test [34].



III. DATA The table below gives data for the ten  sorted by the number of methods. (The LOC comments and empty lines.)  Table 2 Rank Project LOC Method  1 Scala 247,134 57,00 2 ScalaTest 169,968 16,60 3 Delite 41,125 8,92 4 Lift 58,407 8,69 5 Akka 65,713 8,27 6 SBT-0.13 35,949 6,912 7 Spire-2.10.0 17,162 4,88 8 Scalaz-Seven 27,847 4,85 9 Finagle 41,081 4,672  10 BIDMat 14,141 3,72  The Scala project is by far the largest number of methods. For this reason, we test with and without the Scala project to rule ou this project may have on the aggregate statistic  and formats of Excel. $(f(x)).len string. In effect, nformative length  -2.615  s the fitness of H.

ny. MDL theory nd simplicity into  L(H)+L(D|H). We en take the length  (4)  (5)  mber of observed nal histogram, the g those bins with  m of the LOC and he purpose is to portfolios using  n largest projects C are stripped of  ds % total 9 26 5 7 9 4 5 4 9 4 2 3 5 2 5 2 2 2 7 2  in LOC and the the distributions  ut potential biases cs.

The scatter plot in the figure representation of the frequency distr with (+) and without (o) the Scala pr    Figure 1 LOC distributions with (+) a compiler/standard library project.

The scatter plot in the figure representation of the frequency dis with (+) and without (o) the Scala pr    Figure 2 M distributions with (+) and compiler/standard library project.



IV. RESU  The table below gives the mod distribution with the Scala project in where the basis b = 88.

Table 3 Results of fitting each model distribution with the Scala project in  f (x) R2  L 492094x-2.311 0.96 3147.6e-0.064x 0.81  ?  ??  ???  ?????  ??????  ???????  ?????????  ? ?  Fr eq  L     1,000  10,000  100,000  1,000,000  ?  Fr eq  M /  e below gives a graphical ribution of LOC per method roject using log-log scales.

and without (o) the Scala  e below gives a graphical stribution of M per method roject using log-log scales.

d without (o) the Scala  ULTS  dels for the LOC frequency n the portfolio, sorted by R2  to the LOC frequency the portfolio  (H) L(D|H) L(H) + L(D|H)  15 91.7 106.7 17 108.6 125.6  ?? ??? ?????  LOC / method  ?? ???  / method     0.039x4- 6.5834x3+372.13x2 -  8035.3x+52286 0.43 46 204.7 250.7 -0.083x3 + 19.237x2  - 1294.1x + 23854 0.33 36 266.7 302.7 -10602ln(x) + 35029 0.31 15 283.9 298.9  13.829x2 - 1200.8x + 21084 0.21 25 419.0 444.0  -105.53x + 7818.7 0.09 16 977.8 993.8  The table below gives the models for the LOC frequency distribution not including the Scala project in the portfolio where the basis b = 87.

Table 4 Results of fitting each model to the LOC frequency distribution not including the Scala project in the portfolio  f (x) R2   L(H) L(D|H) L(H)+  L(D|H) 350467x-2.247 0.96 15 90.6 105.6 2728.4e-0.064x 0.87 17 100.0 117.0  0.0016x4- 0.478x3+49.527x2 -  1977.7x + 24241 0.47 46 185.1 231.1 -5226ln(x) + 20539 0.41 14 212.2 226.2  -0.0591x3 + 13.793x2 - 934.87x + 17239 0.35 37 248.6 285.6  2.2758x2 - 344.08x + 10889 0.22 25 395.5 420.5  -78.583x + 5803.2 0.10 16 870.0 886.0  The table below gives the models for the M frequency distribution with the Scala project in the portfolio where the basis b = 50.

Table 5 Results of fitting each model to the M frequency distribution with the Scala project in the portfolio   f (x)   R2   L(H)   L(D|H)  L(H)+ L(D|H)  256689x-2.661 0.98 15 51.0 66.0 2358.3e-0.119x 0.81 17 61.7 78.7  0.0519x4- 8.698x3+490.48x2-  10633x+69893  0.40 45 125.0 170.0  -14049ln(x) + 46810 0.31 15 161.3 176.3 -1.0482x3 + 133.75x2  - 4866.5x + 47952 0.29 37 172.4 209.4  16.979x2 - 1520.4x +  0.17 25 294.1 319.1  -331.09x + 13714 0.07 15 714.3 729.3  The table below gives the models for the M frequency distribution not including the Scala project in the portfolio where the basis b = 46.

Table 6 Results of fitting each model to the M frequency distribution not including the Scala project in the portfolio   f (x)   R2   L(H)   L(D|H)  L(H) + L(D|H)  191838x-2.615 0.98 15 46.9 61.9  2159.1e-0.12x 0.81 16 56.8 72.8 0.039x4-  6.5834x3+372.13x2 - 8035.3x+52286  0.42 46 109.5 155.5  -10602ln(x) + 35029 0.32 15 143.8 158.8 -0.7495x3 +  96.509x2 - 3564x +  0.29 35 158.6 193.6  13.829x2 - 1200.8x + 21084  0.17 25 270.6 295.6  -259.14x + 10678 0.07 15 657.1 672.1

V. DISCUSSION  A. Scala project The K-S test indicates the two LOC distributions are  statistically identical (P>0.999) with and without the Scala project. The K-S test indicates similarly for the M distributions (P=0.996). Thus, the Scala project apparently does not bias the results. First, we interpret this to mean the data is robust.

Second, the large-scale structural pattern in the Scala source is independent of project and by implication, independent of programmer.

B. Straight-line effect While we did not apply trend lines to Figures 1 and 2, they  nevertheless exhibit a high degree of linearity, at least by visual inspection. This condition is symptomatic of power-laws when plotted on log-log scales. That is, if a power-law is the generating process, then log(c x?) = ? log (x) + log (c) where ? is the line slope and log (c), the intercept. We have ? < 0 in each case and log (c) ? 5 (see Tables 3 ? 6). These correspond to the observed slopes and intercepts in the figures. There is a small ?bow? in the LOC curve which may account for the slightly, consistently higher R2 for M compared to LOC.

C. Power-law vs. exponential models The Tables 3 ? 6 show consistently the closest competitor  of the power-law interpretation is the exponential interpretation in terms of R2 and MDL. This is perhaps not surprising.

Exponential distributions are similarly ubiquitous in natural and artificial phenomena. For example, consider radio and light emissions from galaxies, oil field reserve sizes, foreign exchange rate variations, country sizes, etc. [35]. Although we might investigate further for a better fit using some sort of exponential interpretation, we suspect such a model will not likely be as parsimonious.

D. Log models While the log models do not overall compare well with  power-law or exponential models, the fitness of the log models is consistently better than the polynomial models except for the highest degree polynomial model. Even here the log is hardly distinguishable from the fourth degree polynomial. We account     for this because the log models are very compact, that is, considering only its L(H). For this reason, we consider a relatively attractive alternative, however implausible.

E. Linear models The linear models appear to fail utterly compared to the  other. The linear models are extremely simple?note their L(H)?but not very plausible. In our view, this observation supports the elegance of power-law explanations. That is, the power-law models are as simple as the linear models by L(H) and at the same time plausible by R2 and MDL.



VI. CONCLUSIONS Why Scala programs might be structured with a  preponderance of short, straight-line methods that seem to follow power-laws, is an intriguing question. Consider the codes were created by hundreds of programmers, 262 or more of them, each one presumably with a different programming style. Consider also minimalist and ?permissive? Scala grammar, in which, at least anecdotally, styles vary widely. We will only suggest that the data regarding power-laws do not contradict arguments that mathematical expressiveness of functional languages lends itself to writing modular codes. On the contrary, the data seems to support these claims. However, we leave open questions of whether these data are unique to Scala specifically and whether they apply to functional programs generally.

Thus, we believe it might be worthwhile to investigate other languages, functional and imperative, and consider how they compare with these results for Scala.  The data are only suggestive that the distributions may also be fractal in structure. This is something to pursue for future research.

Finally, future work might also investigate more closely specific projects in the portfolio, e.g., we alluded to the Twitter project.

ACKNOWLEDGEMENTS We thank the reviewers for their feedback, which we have  tried to incorporate here.

