Rule Mining and Classification in Imperfect Databases

Abstract-A rule-based classifier learns rules from a set of training data instances with assigned class labels and then uses those rules to assign a class label for a new incoming data instance. To accommodate data imper- fections, a probabilistic relational model would represent the attributes by probabilistic functions. One extension to this model uses belief functions instead. Such an ap- proach can represent a wider range of data imperfections.

However, the task of extracting frequent patterns and rules from such a "belief theoretic" relational database has to overcome a potentially enormous computational burden. In this work, we present a data structure that is an alternate representation of a belief theoretic relational database. We then develop efficient algorithms to query for belief of itemsets, extract frequent itemsets and generate corresponding association rules from this representation.

This set of rules is then used as the basis on which an unknown data instance, whose attributes are represented via belief functions, is classified. These algorithms are tested on a data set collected from a testbed that mimics an airport threat detection and classification scenario where both data attributes and threat class labels may possess imperfections.

Index Terms- Data imperfections, data ambiguities, data mining, association rules, classification, Dempster- Shafer belief theory

I. INTRODUCTION  Unless they are appropriately modeled and accommo- dated, one is compelled to make various "assumptions" and "interpolations" to avoid the difficulties associated with data imperfections in a database. In most practi- cal applications however such a strategy can severely impair the integrity of the decision-making process and yield inferences that are not trustworthy. For exam- ple, in a battlefield object identification scenario, one typically has to fuse evidence presented by various  All authors are with the Department of Electrical and Computer Engineering, University of Miami, 1251 Memorial Drive, Coral Gables, FL 33146 USA. Email: k.hewawasamnumiami.edu, kamal@miami.edu, s.subasingha@umiami.edu and shyu@miami.edu. This material is based upon work supported by the National Science Foundation (NSF) under the ITR (Medium) Grant IIS-0325260.

heterogeneous sources to make an informed decision.

The reliability of sensors, their 'footprints,' differences in domain expert opinions, etc., are all sources of data imperfections that have to be properly accounted for.

For instance, data imperfections generated from subjec- tivity of evidence can be critical; ignoring or making assumptions/interpolations regarding such evidence may lead to costly mistakes. Lack of better techniques for accommodating data imperfections is in fact a signifi- cant hindrance to the application of various computer engineering methodologies in other domains [1].

The probabilistic relational database model allows  probabilistic information to be associated with attributes.

In our work, we use a Dempster-Shafer (DS) belief theoretic model where each attribute is modeled via its own belief function. Such a DS belief theoretic relational database (DS-DB) provides a very convenient framework for modeling several common types of data imperfections: missing data, incomplete data and am- biguities generated from lack of evidence to discern among a set of hypotheses. More importantly, a DS- DB can better withstand modeling errors. Probabilistic approaches require one to make initial assumptions re- garding the model (e.g., independence of events, equi- probabilities, etc.). When these are close to reality, they perform extremely well; otherwise, conclusions made can be quite misleading. On the other hand, DS models are significantly more robust to such modeling errors (see [2] and references therein). We believe that such a methodology is exactly what is needed in the present context where one may not be able to justify the typical assumptions required for a probabilistic approach.

However, discovering interesting patterns and rules from a DS-DB is an enormous challenge because of the potentially crippling computational burden. Inspired by the itemset tree concept in [3], the work in [4] proposes a data structure referred to as the belief itemset tree (BIT) to address this difficulty. It however accommodates only data ambiguities - a much narrower class of data imperfections (see [5] and Table III for the nomencla- ture). In this work, we extend the BIT so that it can     accommodate general belief theoretic data imperfections; this generalization gives rise to an efficient association rule mining (ARM) algorithm enabling one to discover interesting rules hidden in the DS-DB.

This paper is organized as follows: Section II provides a brief review of essential DS notions; Section III de- scribes how we model database imperfections within the DS framework and the types of data imperfections that we capture; Section IV describes the BIT; Section V de- scribes how ARM is performed from the BIT; Section VI describes the task of classification; Section VII presents experimental results; finally, Section VIII provides the concluding remarks.



II. DS THEORY: A PRIMER A. Basic Notions  Let e = {O(1),... ,ON)} be a finite set of mutually exclusive and exhaustive hypotheses about some problem domain. It signifies the corresponding 'scope of exper- tise' and is referred to as its frame of discernment (FoD) [6]. A proposition Oi - referred to as a singleton - represents the lowest level of discernible information in this FoD. Elements in 20, the power set of E, form all hypotheses of interest. A hypothesis that is not a singleton is referred to as a composite hypothesis, e.g., (0(1), 0(2)). From now on, the term "hypotheses" is used to denote both singletons and composite hypotheses.

We use IE)I to denote the cardinality of E3. The set  A \ B denotes all singletons in A C E that are not included in B C E, viz., A \ B {o(i) E E a(i) E A, 0(i) B}; A denotes e \ A.

In DS theory, the 'support' for any hypothesis A is provided via a basic probability assignment (BPA):  Definition I (BPA): The mapping m : 2E) o [0, 1] is a BPA for the FoD E iff: (i) m(0) = 0; and (ii) ZACEmM 1.=ii The BPA of a hypothesis is free to move into its  individual singletons. This is how DS theory allows one to model the notion of ignorance. The set of hypotheses ZT that possesses nonzero BPAs forms the focal elements of E; the triple {E, Y, m} is referred to as the corre- sponding body of evidence (BoE).

Definition 2 (Belief, Plausibility): Given a BoE {0E,Y,m} and A C e, (i) B : 2 -+ [0,1] where BI(A) = ZBCA m(B) is the belief of A; and (ii) P1: 2E e [0, 1] where P1(A) = 1 - Bl(A) is the plausibility of A. l  So, while m(A) measures the support assigned to hypothesis A only, the belief assigned to A takes into account the supports for all proper subsets of A as well;  Bl(A) represents the total support that can move into A without any ambiguity. P1(A) represents the extent to which one finds A plausible. When each focal set contains only one element, i.e., m(A) = 0, VIAI #& 1, the BPA, belief and plausibility all reduce to probability.

B. Evidence Combination The evidence provided by two 'independent' BoEs  could be 'pooled' to form a single BoE via Dempster's rule of combination (DRC):  Definition 3 (DRC): Suppose the two BoEs {e,YF T,mI}, i = 1, 2, span the same FoD E).

Then, if K - EC,D:cnD=O ml(C) m2(D) = 1, the DRC generates the BPA m(.): 2 -+ [0, 1] where  m(A) =EC,D:CnD=Aml (C)m2(D) VA C E This combination operation is denoted as m = ml Dm2.



III. REPRESENTATION OF IMPERFECT DATA  A. Database We denote the database by DB = {Ri}, i = 1, nDB,  where Ri indicates a data record and nDB its cardinality (i.e., 'size'). Each Ri is taken to be of the following form:  Ri = [AVi, CLi], with AVi = [Al,i,* , AnA,i] * (1) Here, AVi and CLi denote the i-th attribute vector and its corresponding class label. Each attribute vector is taken to consist of nA attributes; Aj,j, j = 1, nA, denotes the j-th such attribute. From now on, unless the discussion warrants otherwise, we will ignore the subscript that identifies one data record from another, i.e., R denotes Rj, AV denotes AVi, and so on.

B. Relevant FoDs  1) Attributes: The FoD of Aj, j = 1, nA, is taken to be finite and equal to EA,, viz.,  FoD[Aj] = EAj = {o() *.* 0AIo }' (2) where nTA, denotes the number of possible values Aj may assume. The FoD EAV of each attribute vector AV is then the cross-product of these attribute FoDs, viz.,  nA FoD[AV] = E)AV = f1 E)Aj;  j=l (3)  its cardinality is IEAV = Hlj=l 2eAj We use two BPAs to capture attribute imperfections.

Definition 4 (Intra-BPA, Inter-BPA): For data record  AV = [All . .. I AnAl]     (i) an intra-attribute BPA (intra-BPA) is a BPA mAj : 20 Aj [0,1] defined on the FoD 0Aj and {EAj, IFAj I mAj } is the intra-attribute BoE (intra-BoE) it generates; and  (ii) an inter-attribute BPA (inter-BPA) is a BPA MAV : 2EAV 9-+ [0,1] defined on the FoD EAV and {(9AV, YAV, MAV} is the inter-attribute BoE (inter- BoE) it generates. D From now on, we adopt the convention of an underline  to denote the assumed value of a variable. For example, < AV = AV > where AV = [A1,. . . ,AnA] indicates that the attribute vector has assumed the value AV. Here, Aj C (Aj, j = 1,nA. Note that, Ai = 0 denotes that the attribute Aj is "not applicable" for the attribute vector [7]. We assume that an attribute vector whose attributes are all "not applicable" - this corresponds to the "null set" of EAV - is non-existent. To simplify our notation even further, we will denote < AV = AV > by simply <AV > whenever no confusion can arise.

Example. Table I shows 04 attribute records and the  intra-BPA of each attribute. Each attribute record is of the type AV = [A1, A2, A3] where E)Ai = {1, 2, 3}, i = 1, 3. Inter-BPA for each attribute record is in Table II. D  TABLE I INTRA-BPA OF EACH ATTRIBUTE IN 04 ATTRIBUTE RECORDS  Record  <A1 > A1 mA1 m  1 1.0 2 0.6  (2.3) 0.4  <A2> A2 mA2 ' 2 1.0 3 1.0  <A3> A3 mA3A  1 1.0 2 1.0  3 (1,2) 0.7 (1,2) 0.6 1 1.0 (1,2,3) 0.3 3 0.4  4 1 1.0 2 0.7 1 1.0 3 0.3  TABLE II INTER-BPA FOR EACH ATTRIBUTE RECORD IN TABLE I  Record   (2,3  <A1 xA2xA3> x 2 x x 3 x x 3 x    MAV (.) 1.00 0.60 0.40  3 (1,2) x (1,2) x 1 0.42 (1,2) x 3 x 1 0.28  (1,2,3) x (1,2) x 1 0.18 (1,2,3) x 3 x 1 0.12  4 1 x 2 x 1 0.70 1 x 3 x 1 0.30  2) Class Labels: The FoD of CL is taken to be finite and equal to OCL, ViZ.,  FoD[CL] = OCL = {OCL(1), ... , OCL(nECL)}, (4)  where nE0CL denotes the number of different class labels that are to be discerned. As for the attributes, we allow each CL to be described via its own intra-BPA {ECL, YCL, mCL}-  C. Types of Imperfections An intra-BPA allows several types of common data  imperfections that may occur in a database to be conveniently modeled. For example, suppose EAi = {0(1), 0(2), 0(3) }. Then, the types of data imperfections that the intra-BPA mA, (.) enables one to model are shown in Table III [5].

TABLE III TYPES OF IMPERFECTIONS  Type of Data Intra-BPA Hard (perfect) mAj (9(2)) - 1 0 Probabilistic mAj (0(l)) = 0. 1  (focal elements mAj (0(2)) = 0-7 are singletons) mA. (0(3)) = 0.2 Possibilistic mA (0(91)) = 0 7  (consonant belief mA, (0(1' 0(3)) - 0.2 function) mA, (EA.) = 0.1 Ambiguous MA,((1U,9(2) = 1.0  (value is ambiguous) Missing/unknown mAj (EAj,) = 1.0  (complete ambiguity) Belief theoretic ZBCe mAj (B) = 1.0 (most general)  The work in [4] can only accommodate ambiguous data. One major contribution of this current work is to extend this to general intra-BPAs.

D. From Intra-BPA to Inter-BPA The intra-BPA captures the uncertainty among the  values each attribute may take; an Inter-BPA is extremely useful when one is interested in capturing the interrela- tionship among different attributes. Given the intra-BPA for each attribute, we may generate an inter-BPA via  Definition S (Cylinderical Extension): [8] Cylindri- cal extension of < Aj > to E)AV iS CyleAV (AJ) = [E)A1 *... E)Aj_1, Aj,, )Aj+l *... OA,lA] C OAV.

Consider the intra-BoE {E)A, , FA,, mA } of the j-th  attribute. Then, the following is obvious: Lemma 1: M :j) 2EAV + [0, 1] where  MAV(cy AV (Aj)) =mAj (Aj), VAj C?TAj is a valid inter-BPA. We refer to it as the inter-BPA generated from the intra-BPA mA,- D  Definition 6 (Data Record BPA, Database BPA): Consider the database DB.

. - I 11    (i) For a given attribute record AV, suppose MU) denotes the inter-BPA generated from the intra-BPA mAj (.) via Lemma 1. Then, the BPA [8]  nA  MAV(B) = M ) (B), VB C 0AV, j=l  is referred to as the data record BPA of the given data record; the corresponding BoE is the data record BoE.

(ii) Let freq (AV = AV) be the sum of the BPAs allocated to exactly the attribute vector AV in all the data record BPAs. Then, the BPA  MDB(AV = AV) freq (AV = AV)  nDB for all AV that may appear as a focal element of at least one data record BPA is referred to as the database BPA of the database DB; the corresponding BoE is the database BoE. D Example. Table II actually gives the data record BPA  of each data record in Table I. O

IV. BELIEF ITEMSET TREE (BIT) In this section, the belief itemset tree (BIT) first  introduced in [4] is extended to accommodate general belief theoretic data imperfections.

Definition 7 (Projection, Extension): For < AV >, (i) its k-projection AV;k is the k-attribute vector <  AV;k >, where AVNk = [Al,... Akl] for k = 1,nA and AV'0 = 0; and  (ii) its k-extension AVtk is the nA-attribute vector < AVtk >, where AVtk = [AV'k, eAA+1 **...* (An A]* F Note that AVto generates the 'completely ambiguous'  attribute vector, viz., A, = eA3, Vi = 1,nA.

Definition 8 (Ancestor, Parent, Child): Given  < AV >, let < AV1 >=< AVtk > and <AV2>=<AVtt> . Then we say the following:  (i) <AV1> is an ancestor of <A2 > if k < e.

(ii) < AV1> is a parent of < AV2 >, or equivalently,  < AV2 > is a child of < AV1 >, if < AV1 > is an ancestor of < AV2 > and k = e - 1; we denote this as <AVl1 ><<AV2> or <AV2?>< AV1 >. L  So, an attribute vector can have multiple child attribute vectors; it can have only one parent though.

A. Belief Itemset Tree (BIT) Definition 9 (BIT): The belief itemset tree (BIT) of  the database DB consists of a set of nodes arranged in a tree structure of nA + 1 hierarchical levels. The set of nodes in level k, referred to as the level-k nodes, is denoted by N(k), k = 0, nA; level-nA nodes are also  called the leaf nodes of the BIT. Level-k nodes N (k) correspond to the k-projections generated by only those attribute vectors in DB.

(i) For k = 1, nA, an individual level-k node n(, E N(k) is identified via the particular value of its k-th attribute and its parent node as  n(k) n (A(kn(k-1), ek C I(n(k1)),  where n(k) is the k-projection n(k) < AV > < AV;k >=< [AV;(k-l),Ak] > and n(k-i) n(k) Here, I(n4(k 1)) is an index set; it spans over all the distinct k-projections that can be generated from only those attribute records in DB that map via the parent node n(k-1) In the BIT, 'branches' indicate only these parent-child relationships; 'node' corresponding to n(k) indicates its k-th attribute value Ak and a parameter freq(n(k)) > 0 which denotes the sum of masses of attribute vectors that map to n(k)  (ii) Level-O consists of one and only one node N(?0 - n(?), e = {1}, which corresponds to the completely ambiguous attribute vector, and we use the convention n() = n(0) (0, 0) and freq (n(?)) =nDB. D An algorithm that enables one to construct the BIT  and, in general, update the current BIT with an incoming arbitrary attribute vector with its own data record BPA, appears in Table IV.

TABLE IV ALGORITHM FOR UPDATING THE LEVEL-k OF THE BIT WITH THE  ATTRIBUTE VECTOR <AV>     Insert (AttributeVector AV, MAV (AV),Level k) { generate the k-projection AVIk - [Al .. , Ak];  if 3 a 'branch' n( 1) (Ak-17,) i i n( (AkA n(k 1) for some ?k E I(n(k1)) then freq (n()) = freq (n(k)) + MAV(AV);  else { (n(k- 1)) =7(n(kr1)) U {L} where L is a new index;  create the level-k node n(k) (Ak, n(k1)) > n(k 1) (Ak ) with freq(n(k) MAV(AV);  if k < nA then InsertNewTree (n(4)) } Insert(AV,k + 1) };  InsertNewTree (Node N) { create a new branch leading from N; insert an empty node at its termination };  Example. The BIT of the data records and their data record BPAs in Table II is in Fig. 1. 0     A3 IIL L J2 LJL 2 LiL2 LJ  Fig. 1. BIT corresponding to the database in Table II.

B. BIT and Database BoE  The following result establishes the relationship be- tween the BIT and the database BoE. Its proof is quite similar to the one in [4].

Theorem 1: Consider the database DB. Then, (i) all attribute vectors with an identical k-projection  AV'k map to one and only one level-k node n(k (ii) freq(n()) denotes the sum of BPAs of all at-  tribute vectors with an identical k-projection AVWk; and (iii) the database BPA is MDB(AV = AV) =  freq (AV = AV)/nDB = freq (n(nA))/nDB. II This forms the basis which enables the BIT to be used  as a very convenient tool for computing the beliefs of itemsets based on the database BPA. See Table V.

TABLE V ALGORITHM FOR COMPUTING THE BELIEF OF THE ARBITRARY  PROPOSITION <G>  1 ComputeBelief(Belief B,Proposition G, Level k) { 2 generate the k-projection G0' = [ G1,.. ; 3 for each tk E (nl.1) { 4 go to node (k) = n(k) A n(k-1)); 5 if G AD-h then { 6 if G =eA, Vj = k + 1, nA then 7 B= B + freq (n(k)); 8 else { 9 go to child node ni(k+=) n(^+l) (Ak+1,n(k)); 10 B = B+ComputeBelief(B, G, k + 1); } } } 11 return B};  Example. Fig. 2 shows how the algorithm in Table V uses the BIT in Fig. 1 to respond to the query Ql = Bl ([(1,2), 3,OA3). Note that this requires 02 nodes to be visited. Fig. 3 shows how the same BIT is used to respond to the query Q2 = Bl ([E)A,, (1, 2), 1). Note that this requires 07 nodes to be visited. [1  A3 L LJLL2Li Li2 iLi  Fig. 2. Propagation of the query Ql in the BIT in Fig. 1  A2  A3  Fig. 3. Propagation of the query Ql in the BIT in Fig. 1

V. ASSOCIATION RULE MINING (ARM)  An association rule is an expression of the form Rant =# ~Rcon, where the antecedent Rant and consequence Rcon are sets of attributes. The task of association rule mining (ARM) is to generate all as- sociation rules with support and confidence measures above given thresholds [9], [10]. If attribute and class label imperfections are to be accommodated, we must have Rant C eAV and Rcon CeCCL. Available methods [9], [10] however require that each Rant and Rcon be a singleton, e.g., < AV1 = 3 > A < AV2 = 4 >  .- < CL = 1 >. They are therefore incapable of discovering rules that accommodate imperfections, e.g., <AV1=(2,3)> A <AV2=4>==:<CL=(1,2)>.

ARM in such situations creates several significant  challenges. For example, although neither of the attribute records [2,4, OA3] nor [3,4, OA3] may pass the minimum support threshold on its own, when considered together as [(2, 3), 4, OA3], they may prove successful thus po- tentially generating a useful rule. To perform ARM in the presence of such data imperfections, we need the following notions [4].

A. Frequent Itemsets  Definition 10 (Itemset, Support, Frequent Itemset): Consider <AV>.

(i) It is referred to as a k-itemset if exactly nA - k of its feature attributes are equal to their corresponding FoDs; Ik denotes the set of k-itemsets (by convention, Io is the completely ambiguous attribute vector).

(ii) BlDB(AV = AV) is referred to as the support of < AV > and is denoted by Sp [AV = AV].

(iii) < AV > Ik is said to be frequent if Sp [AV = AV] > minSp, where minSp denotes a user-defined minimum support threshold; the set of frequent k- itemsets is denoted by FIk. ?  Table VI provides an efficient apriori-like algorithm that is capable of generating frequent itemsets in the presence of general belief theoretic imperfections. The  TABLE VI ALGORITHM FOR GENERATING FREQUENT ITEMSETS. FIall  DENOTES THE SET OF ALL FREQUENT ITEMSETS   GenerateFrequentltemSets() { FIall = 0; FI1 = 0; Il = GenerateOneItemsets(DB); for each itemset I E Ii  if BIDB(I) > minSp then FI1 = FI1 U I; AmbiguousIl = GenerateAmbiguousltemsets(I, \Fl1); for each itemset I E AmbiguousIl  if BIDB (I) > minSp then FI1 = FI1 U I; RemoveRedundantltemsets(FIi); n= 1; while FI,# 0{  FIall = FIall U FIn; In+, = GenerateCandidateItemsets(FIn); FIn+l = 0; for each itemset I E In+l  if BIDB(I) > minSp then FIn+1 = FIn+1 U I; n = n +1}  return FIall };  salient steps in the algorithm are as follows: Line #3: The entire database DB is scanned to find  all its 1-itemsets. The frequent itemsets that would be eventually generated from these would only consist of those attribute vectors that are present in DB. Such a strategy may prevent us from discovering other po- tentially important rules. For example, suppose the first attribute of each vector is modeled as a singleton. Then, with line #3 alone, it would be impossible to create potentially interesting relationships with an antecedent having an imperfect first attribute!

Line #6: To discover such relationships, we form imperfect attributes by combining the attributes of non- frequent 1-itemsets obtained in line #3.

Line #8: If these newly formed 1-itemsets turn out to be frequent, they are included in FI1. For example, if AV1 = [1, OA2, * * *, E)AfA] V AIi and AV2 = [2, OA2, ...*, EAnA] V FIk, we would form the 1-itemset AV1 U AV2 = [(1,2), OA2, ..,E)AnA] (line #6). It is now quite possible that AV1 U AV2 C FI1.

Line #9: This strategy however may create 'redundant' frequent 1-itemsets. For example, suppose DB contains the records AV1 and AV1 U AV2. Then, AV1 C FI1 = AV1 UAV2 E FIk. Since Al,1 C (All 2), for discovering rules that have Al I in its antecedent, it is sufficient to retain only AV1 UAV2, i.e., AV1 E FI1 can be considered redundant. In other words, any frequent 1-itemset that is a strict subset of another frequent 1- itemset has to be pruned.

Line #13: The next step is to form 2-itemsets from the frequent 1-itemsets thus obtained. Not all 1- itemset pairs produce 2-itemsets. For example, AV1 n AV2 V I2 while AV1 n A3 C I2 where AV3 = [EA1 3,EA3, ...*, EAnA] c Ii.

Line #16: The frequent 2-itemsets are those that pass minSp value.

Line #17: This same procedure is followed to generate frequent k-itemsets in general.

This algorithm calls for the computation of B1DB(AV = AV) often (line #16) and this is where the BIT comes in extremely handy.

B. Rule Generation  We now need to utilize an appropriate belief theoretic measure to indicate the support we place on each rule in which both attribute value and class label may possess imperfections. DS conditional notions are ideal candi- dates for this [11] and we define the threshold for the confidence in a rule via  minCf = BlDB(RconlRant)X Rant C (AV, Rcon C 0CL, (5)  Although various DS conditional notions are available in the literature, for our work, we prefer the Fagin-Halpern (FH) conditionals [12] because they can be considered the more natural extensions of the Bayesian conditional [13], [14]. We select only those rules that meet this minCf threshold; denote this rule set via R1ARM.



VI. CLASSIFICATION  The classifier must be able to classify incoming data instances that would possess attribute intra-BPAs.

A. Rule BPA and Rule Discount Factor Partition 'ZARM such that the antecedent of each  partition is identical. Suppose the antecedent of partition R(k) is R(k) where k = 1,K.

Definition 11 (Rule BPA, Rule Discount Factor): For the partition R(k), define the following:  (i n(k) 2Et.CmLL(CL(i) mCL : 2eCL [0.1] s.t. ___CL,) = meDB (CL IAVk), is the rule BPA; and  (ii) d(k) = [1 + Ent(k)] - [1 + log [IAVkl]]1 where Ent(k) - ZC kCLCL(C) log [m L(C)], is the rule discount factor ni One may use the iterative technique in [14] to compute  the conditional BPA required for the rule BPA. It com- putes the conditional BPA for each singleton first; then, it computes the conditional BPA for all doubletons, etc.

This iteration can be terminated when the calculated BPA reaches a preset threshold close to unity; the remainder is assigned to the complete set E)CL. The quantity Ent(k) accounts for the 'uncertainty' in the rule about the class label while the term 1/(1 + log [IAVk I]) accounts for the ambiguity in the rule antecedent. Hence, d(k) can be considered a measure of the total uncertainty in the rule.

In essence, each rule in R-ARM can be identified via the triple {AVk, m(k) d(k)} When classifying an incoming attribute vector <  AV > with its own intra-BPAs, the classifier first needs to generate the corresponding data record BPA as de- tailed in Definition 6. Then it needs to find a set of rules RAV C iZARM that 'match' < AV >. Different criteria may be used for this. We used  RAV = {{AVi Im L,d()} : AV C AVi (6)  if RAV = 0, we used the classification algorithm in [15] with the distance measure 0.5 [AV\ AVil + IAVi \AV] . The rule BPAs of the rules in RAV are then combined using the DRC with the rule discount factor taken into account [6].

The class beliefs thus generated are weighted averaged using the BPAs of corresponding focal elements of the generated data record BPA.

B. Decision Criterion The class beliefs themselves can be considered the  decision, or one can make a decision on the class label as follows: If there exists a singleton class label whose belief is greater than the plausibility of any other singleton class label, use the maximum belief with non-overlapping interval strategy [16] to make a hard decision on the class label; if such a class label  does not exist, a soft decision is made in favor of the composite class label constituted of the singleton label that has the maximum belief and those singleton labels that have a higher plausibility value. If this strategy leads to decisions that are too ambiguous, one can restrict the maximum cardinality of the decision to a pre-determined number and use the maximum belief criterion.



VII. EXPERIMENTAL RESULTS  The proposed strategy was applied in a security threat assessment scenario. For this purpose, an experimental platform located at the Distributed Decision Environ- ments (DDE) Laboratory in University of Miami is used to mimic an airport terminal. The platform is partitioned into a grid of 3 x 3 = 9 areas. Passengers are assumed to pose different threat levels, ThreatLevel, depending on the properties of what they carry. We use 4 different types of emitters to mimic these properties: ultrasonic, sound, light and magnetic. The presence of various combinations of these properties, or PaxType, is sensed using sensors placed at specific locations within the terminal. The locations of passengers are tracked by a camera placed above the platform.

Sensor footprints introduce imperfections into the collected data. Data were gathered for two different scenarios: (SI) normal situation where there is no ap- parent threat; and (S2) abnormal situation where there can be a potential threat. A data record consists of 9 attributes, each representing the 9 grid areas. Each attribute is described via an intra-BPA defined on the FoD 0PaxType = {P1, P2, P3, P4}. The threat level for each training set instance is taken to be either Ti or T2, i.e., they are perfectly classified.

Table VII shows the confusion matrix obtained for the experiment when C4.5 and the proposed algorithm are applied. The data set had to be slightly modified so that  TABLE VII CONFUSION MATRICES FOR C4.5 AND PROPOSED ALGORITHM  ThreatLevel C4.5 Proposed Algorithm Tl T2 Ti T2 (T1, T2)  Tl 141 16 136 6 15 T2 11 118 3 120 6  it could be applied to C4.5 [17]. The 'perfect' attribute values that C4.5 requires were obtained by applying the pignistic probability [2] on the intra-BPA. Note how our proposed strategy allows "soft" decisions to be made.

Table VIII compares the time taken to construct the BIT and to run 1000 belief queries on it with the time taken to compute the same belief queries by scanning     TABLE VIII COMPUTATIONAL TIME (IN SEC)  Ambiguities  0% 5% 10% 15% 20% 25% 30% 35% 40%  BIT Construction  0.31 0.33 0.38 0.39 0.36 0.34 0.39 0.36 0.39  Querying 0.14 0.17 0.19 0.19 0.20 0.20 0.22 0.22 0.25  Database Scanning  1.97 1.95 1.92 1.95 1.95 1.88 1.88 1.89 1.92  the database. Comparison was made on the synthetic threat assessment database used in [4]. Computational times were measured for datasets having different levels of ambiguities. Note that time consumed in constructing the BIT is a one-time cost for a given database. Typically a rule mining algorithm involves computing beliefs of a large number of itemsets and it is evident from the results that the BIT speeds up the process by a significant margin.



VIII. CONCLUSION For modeling and accounting for data imperfections,  DS belief theoretic notions have attracted considerable attention. In this paper, we propose the BIT to mitigate the otherwise heavy computational burden that is typi- cally associated with such methods. It is in essence an alternate representation of a database that may contain attribute and class label imperfections that are modeled as intra-BPAs. It allows one to efficiently respond to "belief" queries on the attribute vectors. The proposed ARM algorithm uses the BIT to extract both perfect and imperfect rules; they form the basis on which classification of an incoming data instance is performed.

Preliminary results show that this algorithm provides high classification accuracy; when sufficient evidence is lacking, it classifies the input into a "soft" class. In a real threat assessment application, various other issues also need to be taken into account, e.g., under-estimating the threat level can be more harmful than over-estimating, there may be situations where human intervention and/or immediate action have to be taken etc. To see how the proposed strategy can be used/modified in such situations, further studies have to be performed.

