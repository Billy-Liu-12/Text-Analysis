A Novel Clustering Algorithm for Mining Speech Data using Baysian Network-based  Mutliple Model

Abstract?With the development of speech recognition, speech data mining becomes a hot topic in fields of data mining and natural language processing. In this paper, a novel clustering algorithm is presented to describe how to do semantic mining and how to understand the developing trend of event implied in speech sequence. At first, the speech sequences are extracted into a Baysian network presenting the relationship between different speech elements. Then, we utilize a 3-dimensional space and sequence cluster techniques to excavate implied information from speech. Considering speech data features, we improve traditional distance-based clustering algorithm to get semantic information and enhance performance. The experimental results show that our algorithm is correct and effective.

Keywords-Speech data mining; Sequence cluster; Frequent sequence; Baysian Network

I.  INTRODUCTION Speech (voice) is the most direct, effective and important  data in human beings? communication. Since speech data is a special kind of unstructured stream, it is difficult for computer to identify various semantic rules from huge vivid information. In recent years, with the dramatic improvement on speech recognition, the accuracy rate is greatly rising and the availability of speech data is rapidly raised.  Facing numerous speech data, it is hoped to establish an effective way to manage, access and analyze them. Thus, we can quickly access valuable information from them.  At the same time, we also hope that the computer might automatically deal with boring speech (such as monitoring recording) and find the abnormal to eliminate the human?s labor, we even wish that the computer could find the implied information and related rules from mass of speech data. Based on these requirements, data mining technology has been introduced into the speech processing arena.

In recent years, speech data mining becomes a hot topic in fields of data mining and natural language processing. It has gradually become useful tools in many applications, such as human-computer interaction, search engine, network information filtering and information security, automatic interlocution, business decision, machine translation and intelligent robots. The essential problem in speech data miming is characterizing the semantic information and seeking relationships among events from speech contents. In a simple form, one can extend text data mining technology to speech data mining by using text tools on the output of a speech  recognizer. But it is not optimal, in this paper we will propose a novel framework to do effective semantic data mining on speech collections and to find implied relationships or trends among speech events.

Our work is close in motivation to the WASABI (Watson Automatic Stream Analysis for Broadcast Information) and SAMSA (Speech Analysis, Mining and Summary Application) project [1-5]. They applied the text mining methods to the conference, radio and telephone data, extracted valuable knowledge related to these talks, and developed a speech mining system named MeetingMiner. Rather than transiting voice signals to texts, a speech data mining method based on sound analysis was proposed [6-7], it directly excavated sequence from voice phoneme.

The overall goal of speech data mining is to discover implied semantic information. DIMACS research group in Rutgers University focus on how to mark human beings by excavating cross-features [8-9]. Gilbert uses clustering and statistic characterizing to mine association rules from the short-term speech and to classify semantic information [10- 11]. Association rule mining was applied to extract semantic knowledge from speech data [12].

Compared with previous work, rather than presenting how to extract information and summarize information from speech data, our work focus on developing an effective speech data mining approach to excavate knowledge of event sequence implied in speech utterances. In this paper, we will show how and why we adopt the cluster techniques to achieve our goal.

A novel clustering algorithm is presented to describe how to do semantic mining and how to understand the developing trend of event implied in speech sequence. At first, the speech sequences are extracted into a Baysian network presenting the relationship between different speech elements. Then, we utilize a 3-dimensional space and sequence cluster techniques to excavate implied information from speech. Considering speech data features, we improve traditional distance-based clustering algorithm to get semantic information and enhance performance.

The organization of the paper is as follows. First, the framework and the task of speech data mining are introduced.

Next, feature extraction and data preparation are presented. In Section IV, clustering algorithm for speech data mining base on Baysian network-based multiple data model is outlined. In Section V, we describe the experimental result on proposed methodology. Finally, the insights gained from this study and  2009 Pacific-Asia Conference on Circuits,Communications and System  DOI 10.1109/PACCS.2009.72     suggestions for further enhancements are discussed in Section VI.



II. SYSTEMIC FRAMEWORK FOR SPEECH DATA MINING A basic premise for speech data mining is that speech  collections are sequence data, e.g. speech is sequence of phonemes, language is sequence of word and delimiters, handwriting is sequence of stokes. Thus, when semantic information is implied in speech data, because of the sheer volume of speech data, both in the amount of speech records and in the number of speech features, efficient and intelligent sequence analysis tools are required to discover the semantics.

We develop a systemic framework of applying sequence data mining techniques to excavate speech data. This framework consists of data pre-processing, sequence data mining, semantics and knowledge evaluation. The end products are concise and intuitive patterns that can forecast the trend of speech event, and semantic information that can be easily understood.

Since speech stream is continuous sequence, it is too huge to be dealt with data at one time. Thus, a certain user-defined data window size is used as a criterion to catch speech data stream. It can be landmark window or sliding window. For the time-series data, we use the latter in this paper. A sliding window is featured with a fixed length time-span of the data elements, the data items are implicitly deleted from the sliding window, when it moves out of the window scope. Each event has an arrival time, the timestamp corresponds the position of an active data element in the current window.



III. DATA PRE-PROCESSING During data caching, speech redundancy or repetition is  inherent. To improve mining efficiency, data pre-processing is necessary. Our approach of data pre-processing includes data structure and data clean, feature extraction, entity computing.

Data structure: Since speech data is unstructured, there exists an inherent semantic gap between the lower level of raw speech data and the higher level of event information for analysis. To remove this variation, data structure and normalization are used. To perform data structure, our approaches include data quantization and data mapping. For example, [-4, -2] interval value is approximately equivalent to -3, [-2, 0] interval is approximately equivalent to -1, and [0, 2] interval is nearly equal to 1. When each approximation is presented by a two-bit value, the vector proportion is 2 bits/victor. Data mapping is handled by value replacement using formal expression.

Data clean: The purpose of data clean is to ignore noise words in speech utterances. For example: ?Oh! I want? go back to home? and ?I want to go home? have the identical features vector.

Entity computing: Entity computing consists of entity named, entity explained and events specified. Entity named is implemented through things encoded, entity explained is implemented through attributes defined for named entity, and events specified is implemented through ?who/which? ?when? ?what? found.

Feature extraction: To perform mining processing or build prototype, the speech data are embeded in a fixed dimensional space. Each speech is converted to a feature vector, and the sequence data mining operates on these vectors. Some researchers use individual words as features, others use bi- grams or tri-grams as features. Based on these methods, the purity of speech data should make great effect on the results of semantic mining. Thus, in this paper we will adopt a novel feature extraction method named weighting features extraction base on the number of times a word appears. More formally, given topic Ti, a typical speech feature vectors can be described as x={x1, ?, xL}, where xi?X, X?Ti and 1?i?L, L is the length of sliding window. To simplify computation, we construct a 0-1 vector for feature extraction responding to the Baysian structure described below. That is, xi=1 if xi?Ti, otherwise xi=0.



IV. CLUSTERING ALGORITHM FOR MINING SPEECH SEQUENCE  In this section we describe our data mining algorithm, and illustrate how to apply this algorithm to generate patterns from speech data. Here speech data refers to pre-processed speech records, each with a number of features.

A. Baysian Network-based  Model for Speech Data Sequences can be divided into consecutive series, parallel  and hybrid sequences. Consecutive series are linear input, and have no strict relations among data. In parallel sequences there is no priority among various data, all input come at the same time. In the mixed sequence, series and parallel sequence are mixed together. In view of the different types of sequences, we use the Bayesian network to describe different types of sequences in the matrix M, which stores n nodes of the Bayesian network.

Mij is the matrix element:  ? ? ?  = others  jnodeofsontheisinode M ij 0    the line i is a collection of the fathers of i-node, presented as ?i.

The matrix M describes the relationship between different sequence elements. The feature vector x describes data attributes of sequence elements. Thus, combing the feature vector and Baysian network matrix, we present the multiple data model for speech sequence data shown in Fig.1. The following subsection will present the mining algorithm base on this 3-demensional space model.

Figure 1.  3-Demensional space model for speech sequence     B. Baysian Probability Speech sequence is an ordered set of elements s=a1, a2,?,  aL. Each element ai could be numerical, categorical (domain a finite set of symbols ?, |?|=m) or of multiple attributes. The length L of a sequence is not fixed, and sequence order is determined by time or position and could be regular or irregular. We also define a probability model to execute sequence classification. More formally, for a certain speech sequence s=a1, a2,?, aL, the probability model can be described as:  p(a1,a2,?,aL)=p(a1)p(a2|a1)p(a3|a2a1)?p(aL|aL-1?a1), or  ? ?= L  iiL aaapaaap  1121 )...|(),...,,(  The probability p(ai|ai-1?a1) depends on the relationship between  ai and ai-1. If ai is independent from ai-1, p(ai|ai-1?a1) is p(ai). If ai is dependent with ai-1, but independent from ai- 2?a1, then p(ai| ai-1?a1) is p(ai| ai-1).

We improve conventional clustering algorithm to adapt speech sequence. After feature extracted, speech sequence data are embedded in a fixed coordinate space. But it is difficult to define the most appropriate length for sequence clustering. Thus, we use variant sequence length according to frequent sequential pattern.

C. Generating Frequent Sequence  Speech event is essentially a sequence of sets of episodes, which conform to the given timing constraints. As an example, the sequence <a><cb><d>, encodes an interesting fact that event <d> occurs after an event set<cb>, which in turn occurs after event<a>. If the support of sequence is no less than a specified threshold, this sequence is called frequent episode.

Frequent sequence mining aims at displaying a trend curve to indicate the general direction of the appointed frequent episode of speech data.

Because speech event is badly semantic associated with the former event, hence we use a prefix-based method to excavate frequent speech sequence. For example, <a>, <aa>, <a(ab)>, and <a(abc)> are prefixes of sequence <a(abc)(ac)d(cf)>. For a given entity, the speaker should tell the story following it, and the implied semantic information is also embodied in the speech event. The goal of sequence pattern mining for speech data is to find frequent sequence patterns with prefix entity designated.

There are numerous reasons for our preference for prefix- based algorithm, and we here explore only a few of most important ones. One main reason is that this algorithm is extremely simple and fast which greatly reducing the computation complexity. Next, other mining algorithms using multiple scans are not appropriate because speech sequence mining is a one-time operation. Besides, bottlenecks of traditional candidate-based algorithms have great influence on efficiency. All in all, taking into account these metrics mentioned above, we choose this prefix-based algorithm.

There are three steps to excavate speech sequences.

At first, find length-1 sequence, and discover the interested  entity. From this stage, rather than compared with user-  defined support threshold, Baysian probability of sequence is used to distinguish frequent sequence. That is, if p(ai|ai-1?a1) is larger than the average objective probability of a1, a2,?, ai, this sequence is called a frequent sequence.

Then, divide search space. The complete set of sequence pattern can be partitioned into several subsets according to the interested entity, which is discerned by feature vector.

Finally, find all length-2 sequence patterns having prefixes the interested entity, and extend to length-n sequence patterns.

In this stage, the maximal frequent sequence is the most probable event to appear within a fixed entity.

D. Clustering Sequence After mining maximal frequent sequence, we get the most  probable event. The next step is to get the semantic class from these events using clustering. For sequence clustering, we use distance-based approach to create groups such that similar sequences are in the same group. Given a fixed topic, each speech utterance is a feature vector x={x1, ?, xL} around this topic. For every two feature vectors, we can compute the distance between them to decide whether they belong to a same group. Assume that x and y are two feature vectors, the  distance d(x, y) is given by |||| ||1),(  yx yxyxd ??= . The range of  distance d(x, y) is from 0 to 1.

But in our 3-dimensional space base on Baysian network,  the d(x, y) is always less than 1. Since for two feature vector,  zyx  zyx  bbb  aaa kji  bac =?= , and ?sin|||||| bac ?= .

For two same vector, the d(x, y) is 1 which is error. Thus, we modify this computation and propose a novel clustering algorithm according to our 3-dimensional space data model.

Assume that x and y are two feature vectors, the distance d(x, y)  is defined as |||| ||1),(  yx yxyxd  T ??= .

Thus, the clustering algorithm is described as following: Input:  sliding window size w Output: semantic information of speech sequence t=0;    // timestamp buffer=?;     // buffer for storing recent sequence class=?;    // class having the same semantic information while (speech sequence is not stopped) {       t = t +1;  St = stream events in sliding window [t-w, t]; buffer = buffer ?St ;  //expiring history data generate frequent sequence using prefix-based algorithm  from buffer; for (each two frequent sequences) {  extract features into vector x and y;  compute |||| ||1),(  yx yxyxd  T ??= ;  if  (d(x, y)<1) {  class = class?{x, y};  }     } output class;   }

V. EXPERIMENTAL RESULT We evaluate our algorithm on accuracy and execution time.

The experimental environment is set up on a PC, which is CPU/2.00GHz and Memory/512MB installing windows platform. As a very rough test, we use FreeTTS as our experimental tools [13]. To simplify the pre-processing, we adopt the 20 newsgroups data set as our testing data, which is a text collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. Table 1 is the comparison of clustering accuracy of different methodology. We choose kNN (k-nearest neighbors algorithm, k=30) and SVM (Support Vector Machine) as our comparison. With the advance of feature vector length, the accuracy of our algorithm increases. The experimental result shows that our algorithm is more accurate than these two approaches in speech data mining using Baysian network-based multiple data model when vector length is larger than 5.

TABLE I.  COMPARISON OF ACCURACY  Algorithm Accuracy on dataset politics religion kNN (k=30) 0.803 0.64  SVM 0.85 0.32  our algorithm  |vector|=1 0.14 0.129 |vector|=2 0.35 0.29 |vector|=3 0.5 0.5 |vector|=5 0.87 0.7

VI. CONCLUSION In this paper, we have described a novel clustering  algorithm for speech data mining base on Baysian network- based multiple data model. The key idea is to extract speech sequence data into a multidimensional space and utilize sequence clustering techniques to excavate implied information from speech, which is reformed from traditional distance-based algorithm. There is great flexibility in how these techniques can be carried out. The speech extracted  features, the sequence clustering technique and the evaluations are all easily applied.

ACKNOWLEDGEMENT This paper is supported by Postdoctoral Science Foundation  of China under grant 20070410282 and Postdoctoral Science Foundation of Huazhong University of Science and Technology under grant 2007-39.

