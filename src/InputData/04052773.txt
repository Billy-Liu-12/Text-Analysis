Classifier Building by Reduction of an Ensemble   of Decision Trees to a Set of Rules

Abstract   The paper presents a new approach for building  classifiers by transforming an ensemble of classifiers into a single rule set. The proposed method improves generalization abilities of an ensemble with additional reduction of its complexity. It is dedicated to committees of decision trees and bases on transformation of a set of trees into a set of rules with a new, well-suited, weighed voting algorithm. The paper also presents experiments showing the properties and effectiveness of proposed method and direction of further research.

1. Introduction   One of the most studied tasks in data mining is the classification task. Its aim is to run a learning algorithm on a set of training examples, to produce a classifier. The classifier is a data model, which, when given a new example, predicts the corresponding class label. Classifiers enable automated data labelling and can be applied to a variety of problems ranging from strictly scientific to practical.

The information needed to design a classifier is usually given in the form of labelled data D={t1,t2,..,tN}, where ti=<x,c> ? X ? C is a data vector x=<x1,x2,..,xm,> associated with a class label c from a discrete set of classes C, Xi is the domain of xi and X= X1?X2?..?Xm is the domain of the data vector.

Components of the vector x are usually continuous or discrete (nominal) values, such as height, weight, age, eye colour and so on. These components of the data vector are often referred to as the features or attributes of an example. A classifier is, for a given data set, a hypothesis about the approximated function F : X? C.

Examples of simple classifiers include: linear and quadratic discriminants, the k-nearest neighbour rule, Parzen density classifiers, decision trees and neural networks, but one of the major advances in inductive  learning in the past  15 years was the development of ensemble approaches [1]. Classifiers combining methods have become popular because of the fact, that although for many data sets it?s easy to find a hypothesis which exactly classifies the training set, it is uncommon to find a single expert achieving the best results for the overall problem domain. Experimental results showed that ensemble methods can significantly increase prediction abilities of the whole system. The need for a general theory that would underpin classifier combination has been acknowledged regularly, but such a theory does not yet exist [2]. Plenty of methods have been developed for the construction of ensembles but they have to respect the assumption that base classifiers in an ensemble should be different from each other ? otherwise there is no gain in combining them. The base classifiers can be of any type mentioned above; different data subsets and different feature subsets can be used to build base classifiers, and finally different methods can be used by the classifier ensemble to make its decisions.

Nevertheless, there are also some drawbacks. In general, ensemble classifiers have an internal representation that cannot be easily viewed or parsed.

This can discredit the ensemble in some application areas, where justification or presentation of the decisionmaking process is essential, such as medical diagnostic systems. Furthermore, complex classifiers demand more storage resources and require more time to make a decision.

2. Reduction of an ensemble to a set of rules   The proposed method aims to improve generalization abilities of an ensemble with additional reduction of its? complexity. It is designed for ensemble classifiers, which are made up of classification trees as base classifiers. Decision trees offer some advantages, because they can handle both continuous and nominal data, generate interpretable  Intelligent Agents,Web Technologies and Internet Commerce (CIMCA-IAWTIC'06) 0-7695-2731-0/06 $20.00  ? 2006    classification rules, are fast to train and are often as accurate as, or even slightly more accurate than many other base classifiers. In the proposed approach a simple tree is treated as a set of Horn rules, making it possible to transform a set of classification trees into a single rule set. During the transformation process duplicate rules are removed, which is equivalent to the elimination of covering parts of trees. In the case of an ensemble this can be profitable, because one of the main issues involved in designing an ensemble classifier is to increase the diversity of simple classifiers [3]. Reduction to a rule set enables modification of the decisionmaking method of an ensemble, by giving voting rights to those rules, for which more than a given percentage of conditions (pc) for a particular data vector is satisfied. The weight of a vote of individual rules depends on the percentage of satisfied conditions, but what is more, rules with less than 100% of satisfied conditions have their vote?s weights additionally reduced. This reduction depends on a parametrically given factor (wr). This operation is equivalent to decision trees pruning and what is more, it enables involving additional information in the classification process. It can be additionally noticed that during the transformation process two rules are considered identical when both premises and the conclusion are the same. In effect the rule set can contain conflicting rules (the same premises and different conclusions). It is a consequence of that the ensemble of trees can consists of trees that can yield different conclusions for a given data vector. For completeness? sake, a detailed description of the training and classification algorithm involved in the proposed method is provided in Figure 1.

To evaluate the proposed method a variety of experiments were performed. The first step was to build an ensemble of classification trees. As a learner of base classifiers, the C4.5 algorithm was used ? this is one of the best available algorithms for generating classification trees [4]. Additionally, bagging was used as a method for generating diverse ensembles. Breiman introduced the term bagging as an acronym for Bootstrap AGGregatING [5]. The idea of bagging is simple and appealing: the diversity necessary to make the ensemble work is created using different training sets - each classifier is trained on a set of N training examples, drawn randomly by replacement of the original training set of size N. Such a training set is called a bootstrap replicate of the original set. Bagging was compared by Banfield [6] against 7 most popular tree ensemble creation techniques, and none of the methods he considered appeared significantly more accurate than bagging from a statistical viewpoint.

3. Experimental results   Experiments were performed on 10 publicly available data sets form UCI Machine Learning Repository [7]. Firstly, experiments were conducted to establish individually for each data set an optimal number of classification trees in an ensemble.

Secondly, during ten independent 10-fold cross validations (divisions of data set for training and testing data sets in proportion 9:1), the following steps were sequentially 100 times performed:  1. Building the ensemble 2. Evaluating the classification error of ensemble on  training and test data set 3. Transforming the ensemble into a set of rules and  counting the percentage reduction in the number of rules  Input data: 1. C={ c1,..,cj } set of class labels 2. data set: D={t1,t2,..,tN}, where ti=<x,c> , c? C 3. E=? the ensemble of classification trees 4. RE=? the ensemble of classification trees  reduced to a rule set  Training algorithm: 1. Train the ensemble of trees E 2. For each tree T ? E do  Turn T into a rule set RT For each rule r ? RT  do  - If r ? RE  then add r to RE 3.    Return RE  Classification algorithm: 1.    Initialise the parameters  pc ?  percent of satisfied conditions wr ? vote weight reduction coefficient  (wr<1) For each class label ci set the number of  votes Vci  to 0 2.    For each rule r ? RE do  Run r on the input vector x and determine pr - percentage of satisfied conditions  If pr ? pc then - If  pr = 100%  then weight of the vote w:=1  else  w:=wr*pr - Increase Vci  by w for class label ci in  conclusion of rule r 3. The class c with the maximum value of Vc is  chosen as the label for x  Figure 1. The training and classification algorithm for the proposed method  Intelligent Agents,Web Technologies and Internet Commerce (CIMCA-IAWTIC'06) 0-7695-2731-0/06 $20.00  ? 2006    4. Evaluating the classification error of the set of rules, on training and testing datasets, for 6 different values of the wr parameter (from 50 to 100) and  20 different values of the pc parameter (from 0.05 to 1).

The classification error was counted as a percentage  of vectors from data set for which the classifier gives a wrong answer. The obtained results were averaged and are presented in Table 1. At first glance it can be  noticed that the ensemble of decision trees indeed outperforms the single tree generated with the C4.5 algorithm (and other simple classifiers such as ID3, MLP, see e.g. [8, 9]). However, subsequent transformation of the ensemble into a rule set for most of the considered data sets can additionally reduce the obtained classification error. For the best possible error reduction, table 1 includes rule set results for voting parameters with the lowest rule set error for test data.

Table 1. Testing set classification errors for C4.5 tree, ensemble of trees and proposed rule set method for voting parameters with lowest rule set error for testing data  testing set classification error [%] voting parameters problem name C4.5  tree ensemble of  trees rule set pc [%] wr  balance 34.34 28.27 16.04 50 0.85  bupa 36.34 27.96 25.42 70 0.45  heart 27.22 24.95 18.60 60 0.85  ionosphere 11.48 9.33 9.08 50 0.05  iris 6.23 5.29 5.76 70 0.85  monk3 1.18 1.08 1.08 50 0.05  sonar 28.68 20.12 19.92 70 0.55  soybean 15.39 10.37 10.22 90 0.95  vote 5.01 5.27 4.02 50 0.05  wine 8.98 4.51 3.38 50 0.05  Table 2.  Testing and training set classification errors for ensemble of trees and the proposed rule set method for voting parameters with lowest rule set error for adequate training and testing data  Training set classification error [%] Testing set classification error [%] problem  name ensemble of trees  rule set error reduction ensemble of trees  rule set error reduction  balance 16.9 12.4 4.5 28.27 16.40 11.87 bupa 8.4 6.28 2.12 27.96 25.42 2.54 heart 8.89 6.95 1.94 24.95 18.60 6.35  ionosphere 1.68 1.77 -0.09 9.33 9.08 0.25 iris 2.24 2.37 -0.13 5.29 5.76 -0.47  monk3 1.08 1.08 0 1.08 1.08 0 sonar 0.37 0.3 0.07 20.12 19.92 0.2  soybean 3.30 3.15 0.15 10.37 10.22 0.17 vote 3.53 2.17 1.36 5.27 4.02 1.25 wine 0.57 0.31 0.26 4.51 3.38 1.13  Intelligent Agents,Web Technologies and Internet Commerce (CIMCA-IAWTIC'06) 0-7695-2731-0/06 $20.00  ? 2006    The size of error reduction differs for individual data sets, which is quite expected. The largest value ? 12.23% (over 40% of ensemble error) can be observed for the balance data set. Only two of the considered data sets did not see any additional error reduction: monk3 and iris. It can be noticed that for these sets there was also almost no difference between the error obtained with a single decision tree generated by the C4.5 algorithm and an ensemble. Additionally, analyzing of performance of ensemble and rule set classifiers on both training and testing data, presented in table 2, it can be seen that if transformation of an ensemble into a rule set reduces classification error for training data, then the error is also reduced for testing data.

Results achieved by the constructed rule set depend on values of voting parameters pc and wr, but it should be noted that for pc=100% (when only rules with all conditions satisfied are considered during voting), vote weight for other rules (wr) has no influence on results.

It is important to find a method for choosing (for a given data set) proper values of voting parameters. One possible method for determining pc and wr can be the performance of transformation method for training data. As an example, Table 3 presents the rule set classification error for testing data with voting parameters values pc and wr, for which the training data error was minimal. It can be seen that using the proposed selection of voting parameter values is not optimal, due to an increase in the rule set classification  error for testing data in table 3, in relation to results for the best voting parameters values included in table 1.

Regardless, the method still ensures significant classification error reduction, both for training and testing data, for a majority of the considered test problems. The only exception is the sonar problem, for which the described wr and pc selection techniques cause a slight classification error increase in comparison to general error reduction (presented in Table 1). This shows that, while the proposed transformation of an ensemble of trees into a rule set allows for building classifiers with very good classification properties, there is a need for further search for even better methods of voting parameters selection.

Finally, one more aspect can be considered. An interesting issue is, how many rules are removed during transformation of an ensemble into a rule set and how this is correlated with error reduction. Table 4 presents averaged results obtained during the described experiments. The size of error reduction is expressed as a percentage change of classifier error following the transformation of the ensemble into a set of rules.  This is compared with the percentage of rules, which are found to be identical to other existing rules and eliminated. By analyzing the table, some simple correlations can be observed. As can be noticed, the worst results were obtained for data sets for which rule reduction was very large: 74,2% removed rules for iris and 86,08% for the monk3 data set. This means that, in  Table 3. Training and testing set classification errors for ensemble of trees and proposed rule set method for voting parameters with the lowest training rule set error  training set classification error [%]  voting parameters testing set classification error [%]   problem  name ensemble of trees  rule set pc [%] wr ensemble of trees  rule set  balance 16.9 12.4 50 0.10 28.27 20.45  bupa 8.4 6.28 100 0.05 7.96 27.39  heart 8.89 6.95 100 0.05 24.95 23.25  ionosphere 1.68 1.77 50 0.05 9.33 9.08  iris 2.24 2.37 90 0.05 5.29 6.04  monk3 1.08 1.08 50 0.05 1.08 1.08  sonar 0.37 0.3 80 0.05 20.12 20.16  soybean 3.30 3.15 90 0.05 10.37 10.31  vote 3.53 2.17 90 0.05 5.27 4.02  wine 0.57 0.31 50 0.05 4.51 0.31  Intelligent Agents,Web Technologies and Internet Commerce (CIMCA-IAWTIC'06) 0-7695-2731-0/06 $20.00  ? 2006    the case of these data sets, despite the application of the bagging algorithm to generate diverse base classifiers, trees generated with C4.5 are very similar to one another. This, in turn, results in lack of significant improvement of ensemble performance over a single decision tree for these data sets. However, the most significant error reduction can be observed for problems for which the percentage of rule number reduction varies between 34 and 66%. For those data sets for which only 25% (or less) rules were removed, error reduction is also noticeable but not quite as significant.

4. Conclusions   The presented results show that proposed method of building and using rule sets as classifiers allows improving classification rules even with respect to ensembles of trees. The new, proposed rule set voting paradigm, which makes use of information hidden in Horn rules only partially satisfied for given data pieces, is an extension of  M of N rules (the influence of partially satisfied rules on voting result is proportional to the percentage of their satisfied conditions).

Additionally, this method is specially suited for reduction of ensembles of decision trees. Tests performed on training data enable us to determine the values of parameters of the proposed voting algorithm,  as well as predict error reduction size for test data, although additional research in this field is still needed.

Further statistical analysis of the method for different data sets as well as analysis of obtained rules is also required, but the experiments conducted so far are promising and show the main benefit: a rule set classifier is achieving better classification results than ensembles of decision trees and is easier to process in order to interpret or understand (by human) knowledge hidden in the classifier.

5. Acknowledgements   This work has been partially financed by Polish Ministry of Science and Higher Education with research funds for the years 2005-2007 as a scientific research project (the 3 T11C 05729 research grant).

6. References  [1] Dietterich T.G.,Ensemble methods in machine learning, Multiple Classifier Systems. First International Workshop, MCS 2000, Cagliari, 2000  [2] Kuncheva L.I., Combining Pattern Classifiers. Methods and Algorithms, Wiley, 2004  [3] Valentini G., Masulli F., Ensembles of learning machines, in Neural Nets WIRN Vietri-02, Series Lecture Notes in Computer Sciences, M. Marinaro and R. Tagliaferri, Eds.: Springer-Verlag, Heidelberg, 2002.

[4] Quinlan J.R., C4.5 Programs for Machine Learning.

Morgan Kauffman, 1993  [5] Breiman L., Bagging predictors. Technical Report 421, Department of Statistics, University of California, Berkeley,  [6] Banfield R.E., Hall L.O., Bowyer K.W. , Bhadoria D., Kegelmeyer W.P., Eschrich S., A Comparison of Ensemble Multiple Classifiers Systems, Cagliari, 2004  [7] Newman D.J., Hettich S., Blake C.L., Merz C.J., UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html], University of California, 1998  [8] Zarndt F., A Comprehensive Case Study: An Examination of Machine Learning and Connectionist Algorithms, Master of Science Thesis, Brigham Young University, 1995  [9] Hall M.A., Holmes G., Benchmarking Attribute Selection Techniques for Discrete Class Data Mining, IEEE Transactions on Knowledge and Data Engineering, vol. 15, no. 6, pp. 1437-1447, 2003  Table 4. Percentage reduction of classification error obtained on testing sets, by transforming an ensemble into a rule set, and percentage rule number reduction during the transformation.

