Hardware Architectures for Frequent Itemset Mining Based on Equivalence Classes Partitioning

Abstract?Frequent itemset mining algorithms have proved their effectiveness to extract all the frequent itemsets in datasets, however in some cases they do not produce the expected results in an acceptable time according to the application requirements.

For this reason, FPGA-based hardware architectures for frequent itemset mining have been proposed in the literature to accelerate this task. Most of the reported architectures are limited by the number of distinct items that could be processed and the available resources in the employed FPGA device. This study proposes a compact hardware architecture for frequent itemset mining capable of minimg all the frequent itemsets regardless of the number of distinct items and transactions in the dataset. The proposed architectural design implements a partition strategy based on equivalence classes. The partition on equivalence classes allows to divide the search space into disjoint sets that can be processed in parallel. Accordingly, a parallel architecture is proposed to exploit the benefits of the proposed search strategy.

Index Terms?Frequen Itemset Mining, Hardware Architec- ture, FPGA.



I. INTRODUCTION  Nowadays, information technology is present in every ac- tivity that we perform: in smartphones, personal computers, and even household appliances connected to the Internet that interchange and generate big amounts of data [11]. This amount of data and the diversity of the information exceeds the human capacity to process it and obtain rules that describe the relationship among the data [12].

Data mining has emerged to solve this problem using auto- matic or semi-automatic processes to analyze datasets to find patterns and then perform classification or prediction tasks [18]. One of the most spread technique in Data Mining is the Association Rule Mining technique, which computes rules in form of implications among a set of items [2]. A crucial step in the association rules generation is to count the frequency of items and itemsets to know their relevance; this process is known as frequent itemset mining [2].

Nevertheless, looking for frequent itemsets may become an expensive task due to large amount of data, sparse datasets, and a low minimum support value (henceforth ????). For these reasons, sometimes the implementations of these algorithms cannot return a solution in an acceptable time. One way to deal with this problem is to improve existing algorithms in  order to reducing execution time and proposing new heuristics to explore the search space or using different data representa- tions. In recent years, there is a trend to develop specialized hardware architectures to reduce the execution time of algo- rithms. In literature, there are several hardware architectures based on FPGA (Field Programmable Gate Arrays) and GPUs (Graphic Processor Units) that perform a full implementation of frequent itemset mining algorithms.

This paper describes a FPGA-based hardware architecture for frequent itemsets mining that takes advantage of inner parallelism in the algorithms. The main goal is to produce a compact hardware architecture in area resources that is able to find all the frequent itemsets regardless of the number of items and transactions in the datasets. The architecture also must be able to achieve a better speed up compared to optimized soft- ware implementations of frequent itemset mining algorithms.

Most of the reported work in literature has been designed for a fixed problem size, in others words, they have a limit on the number of different items that are processed, restricted by the resources of the device employed or by memory constraints.

Our proposed architectures address the previous limitations by partitioning the problem, then generating partial solutions for each partition, and finally, combining all the partial solutions to construct a global solution. The partition scheme is mainly based on equivalence classes. This approach divides the entire search space into disjoint sets of the original search space, in consequence, all the equivalence classes may be processed in an independent way. Accordingly, a parallel architecture also is proposed to exploit the benefits of the independent partitions into equivalence classes.

This paper is organized as follows; in section 2, frequent itemset mining and the most important theoretical concepts are exposed. In section 3 related work and previously pro- posed hardware architectures for frequent itemset mining are disclosed. Section 4 introduces the proposed search strategy based on equivalence classes, continuing in section 5 with the implementation of the proposed architectures. section 6 describes the experimental setup and the execution time comparison. Finally, in section 7 conclusions and possible future research lines are drawn.

2016 IEEE International Parallel and Distributed Processing Symposium Workshops  DOI 10.1109/IPDPSW.2016.98   2016 IEEE International Parallel and Distributed Processing Symposium Workshops  DOI 10.1109/IPDPSW.2016.98     null  a b c d  ab ac bc bd cdad  abc abd bcdacd  abcd  ID Items 1 a, d 2 b, c, d 3 a, c 4 a, c, d 5 a, b 6 a, c, d 7 b, c 8 a, c, d 9 b, c 10 a, d  Fig. 1. Transaction dataset and search space

II. FREQUENT ITEMSET MINING  Frequent itemset mining is a method for market basket analysis, and was introduced in [1] by Agrawal. Finding frequent itemsets and associations rules is essential for mar- keting applications, improving the arrangement of products on shelves and suggesting other products.

The first algorithm for frequent itemset mining was formal- ized by Agrawal in the 90?s [1, 2], and it is used to find patterns in datasets. These datasets are represented by transactions; each transaction is labeled with a unique identifier. Frequent itemset mining can be defined as follows:  Formally, let ? = {?1, . . . , ??} be a set of items. Let ? be a set of transactions, where each transaction ? is a set of items such as ? ? ? . And let ? be an itemset such as ? ? ?; without loss of generality, we will assume that all items in each transaction are sorted in lexicographic order. The support value of the itemset ? is the number of transactions over ? containing ? . An itemset is called frequent if its support is greater than or equal to a given threshold value(????).

A brute force approach that traverses all the possible itemsets calculating their support and removing infrequent itemsets is inefficient, in the worst case, a total of 2? itemsets could be generated for ? distinct items. For example in figure 1, we have four items; the search space contains 16 itemsets but only 9 are frequent (the gray ones) for ???? = 3. The number of itemsets and operations grows exponentially according to the number of different items, transactions, and the ???? value. Several algorithms have been proposed to efficiently find all frequent itemsets. There are those based on candidate generation like Apriori [1, 2] that explores the search space using breath- first search, and other ones based on pattern growth like FP- Growth [8] that creates a tree structure called FP-tree, and Eclat [19] that uses a depth-first search. These algorithms find all the frequent itemsets but in some cases, they do not return a response in an acceptable time according to the application requirements.



III. RELATED WORK  In recent years hardware architectures have been explored to offer a solution to the acceleration of frequent itemset mining algorithms. The leading technologies are GPU and FPGA and the most employed algorithms are Apriori [2], FP-Growth [8],  and recently Eclat [19].

In [3, 4, 16, 17] the proposed architectures are an Apriori im- plementation that use systolic arrays. The reason to implement a systolic array is to decrease the number of the connections among processors elements and to ease the control. The disadvantage of these approaches is that they are limited by the resources of the FPGA device employed and the number of processor elements implemented on the chip.

In [14, 15] a systolic tree structure is proposed to implement a hardware version of FP-Growth algorithm. A systolic tree is an array of pipelined processing elements in a multi-dimensional tree pattern. In [10], another hardware architecture to execute FP-Growth algorithm is proposed. Authors proposed an equiv- alence class segmentation based on Eclat algorithm, but once the segmentation is done, FP-growth algorithm takes the con- trol to generate frequent itemsets. The results reported show that the proposed architecture achieves better performance than the hardware implementation reported in [14], specifically for the Chess dataset, the architecture achieves a speedup of one order of magnitude.

Recently hardware architectures based on the Eclat algorithm have been developed [13, 20]. In [20], the architecture per- forms a depth first search strategy. Authors proposed a binary representation for datasets and itemsets. The architecture is formed by an external memory, FIFOs, an intersection module, and a support counting module. They obtain good results for sparse datasets, and a maximum acceleration of 48x is achieved.

All the previous works in literature have proved their efficiency to accelerate frequent itemset mining algorithms. But all of them are limited by the hardware resources available according to the FPGA device employed. For this reason, our goal is to implement a hardware architecture that can obtain all the frequent itemset regardless of the number of different items, transactions, and hardware resources.



IV. DATA REPRESENTATION AND SEARCH STRATEGY PROPOSED  Our proposal consists in a variant of the search strategy proposed for Eclat algorithm, and its primary objective is to perform independient partitions of the search space. The data representation employed is the vertical binary vector because the intersection and support counting operation can be implemented as a combinatorial system. The transactions are coded in 32 bits integers using the compressed array representation reported in [9]. The word size is 32 bits because the external memory is 32 bits wide. For example in figure 2, a binary vector dataset of five items is shown. For items ? and ?, their support values are calculated counting the set bits in the correspondent vectors being ?? = 8 and ?? = 5. The intersection operation is performed using boolean ??? operations. For example to get the itemset ??, an ??? operation between the binary vector of item ? and ? is performed. The result is the binary vector ?? shown in figure 2, and ??? = 3.

Our search strategy is a combination of breadth and depth     a b c d e  1 1 0 0 1 1  2 0 1 1 1 0  3 1 0 1 0 1  4 1 0 1 1 1  5 1 0 0 0 1  6 1 0 1 1 0  7 0 1 1 0 0  8 1 0 1 1 1  9 0 1 1 0 1  10 1 0 0 1 1  a b c d e  1 1 1 0 1 1  2 0 1 1 1 0  3 1 0 1 0 1  4 1 0 1 1 1  5 1 0 0 0 1  6 1 0 1 1 0  7 1 1 1 0 0  8 1 0 1 1 1  9 0 1 1 0 1  10 1 1 0 1 1  (a) Dataset us- ing vertical bi- nary vectors.

a  1 1  2 0  3 1  4 1  5 1  6 1  7 1  8 1  9 0  10 1  b  1 1  2 1  3 0  4 0  5 0  6 0  7 1  8 0  9 1  10 1  ab  1 1  2 0  3 0  4 0  5 0  6 0  7 1  8 0  9 1  10 0  (b) Intersection and support counting op- erations in binary vectors.

Fig. 2. Data Representation and operations used by our proposal.

1 2 3  4 5 6  Fig. 3. Search strategy proposed for four items.

first search. This strategy has the advantage that the search space can be partitioned, and each partition of the search space can be processed in parallel. For example, figure 3 describes the behaviour of the proposed strategy. The first step consists in taking item ? and generate all the 2-itemsets being ??, ?? and ?? frequent itemsets. The next step is to generate all the 3-itemsets. ??? is generated intersecting ?? and ??.

??? is generated intersecting ?? and ??. ??? is generated intersecting ?? and ?? and so on, until no more itemsets could be generated.

In this way, this search strategy can generate the equivalence classes in an independent way.



V. HARDWARE ARCHITECTURES PROPOSED  In order to exploit the benefits of the proposed search strategy; two hardware architectures have been proposed. The first one consists in a compact architecture that mines each equivalence class in a sequential manner. The second one is a parallel implementation that distributes the workload between two processor elements.

A. Compact Hardware Architecture  Our first proposal is the implementation of a full hardware implementation of the proposed search strategy, The behaviour of this architecture is divided into two parts; the first one consists in the generation of the frequent items and the second one consists in the frequent itemset mining using the proposed search strategy.

Standalone OS  Interrupt Handler  Software Program  Hardware Accelerator  Programmable Logic Module Memory  Subsystem  MMU  System Bus  Memory UART  Arbiter  Processing System  Fig. 4. Hardware architecture that performs the proposed search strategy.

Block RAM  Block RAM  Prefix  Suffix  Coun?ng Set Bits  Adder(+) Support Register  S_min Register  Comparator new_itemset  Load Prefix  Load Suffix  AXI MASTER  AXI MASTER  Address  Address  Fig. 5. Low level design of the proposed architectural design.

Figure 4 shows a high-level diagram of the proposed archi- tecture. This architecture is composed of a general purpose processor, an UART module, an off-chip memory, a memory subsystem and the hardware accelerator.

Figure 5 describes a block diagram of the hardware accel- erator. It consists of two dual block RAM memories called ?????? and ??????. The BRAMs have a storage capacity of 122 KiB, in consequence they can store one million of transactions but the architecture is not only limited to process one million of transactions because the Load Suffix and Load Prefix modules can iterate to cover more than one million of transactions. The outputs of each memory are connected to ??? gates that perform the intersection using 32-bits words.

The counting support module receives as inputs two 32-bit words that are the result of the AND gates. The output of the counting support module is accumulated in the support register until all the transactions have been covered. And finally, a comparator verifies the support register value with the ???? register value. If the current itemset is a frequent itemset, the prefix label is concatenated with the suffix label and then the concatenated label is stored in the off-chip memory with its corresponding binary vector. Figure 6 shows two finite states machines that describe the behavior of the proposed architecture. The first state machine corresponds to the frequent items generation task. In state ?0, the architecture receives the initial direction where the binary vectors are stored, the number of transactions, the number of items, the label of the actual item, the direction where the frequent item labels will be stored and the ???? value. In state ?1, the architecture reads the binary vector of the current item and stores it in the load prefix BRAM, and then the counting set bits module computes the support value. In state ?2, if the item is frequent, its label is stored in the off-chip memory as a frequent itemset. State ?3 verifies that all the items have     S0  S1  S2  S3  S4  S0: Receive Ini?al Parameters S1: Read Item S2: Compare Support Value and  write in FI memory sec?on S3: Stop condi?on S4: Indicate comple?on  (a) Items mining state ma- chine.

S0: Generate 2-itemsets S1: Load Prefix S2: Load Suffix S3: Compare support value and write in FI memory sec?on S4: Stop condi?on S5: Indicate comple?on  S1  S2  S3  S4  S5  S0  (b) Itemset mining state ma- chine.

Fig. 6. Finite state machines of the proposed search strategy.

Fig. 7. Search space for item a.

been processed.

The second state machine describes the behaviour of the  frequent itemset mining stage. In state ?0, the 2-itemsets are mined. Table I describes the operations involved in state ?0.

The first step consists in receiving a set of initial items, for this example the initial items are ? = {?, ?, ?, ?} being the ? ? ???????? itemsets the equivalence class to process. The first item in the initial items list determines the equivalence class to process. The second step consists in performing the intersection and support counting of the 2-itemsets; Prefix and Suffix BRAMs are used in this task. The item ? is stored in ?????? BRAM, and the next items will be stored in ?????? BRAM to perform the intersection and support counting operation. All the frequent 2-itemsets will be stored in the off-chip memory.

Once that the 2? ???????? have been calculated, the next step corresponds to state ?1 and it consists in the ?????????? mining. Table II describes the operations employed in this stage using the search space of figure 7. The 2? ???????? = {??, ??, ??}, so in state ?1 the binary vector of ?? is stored in Prefix BRAM, and in state ?2, then itemset ?? is stored in Suffix BRAM. In state ?3, the intersection operation and the support counting operation indicate that ??? is a frequent itemset and in consequence, ??? is written in the off-chip memory. The itemset ?? is not flushed from the Prefix BRAM because there is an itemset that shares the same prefix. So, ?? and ?? are intersected to generate a new itemset. In this case, ?? is flushed from memory because the next itemset in memory is ??? and it is a 3-itemset and they do not share the same prefix. The intersection of two itemsets can only be performed, if both of them have the same cardinality and share the same prefix. For example, the prefix of itemset ??? is ??  TABLE I 2-ITEMSETS GENERATION.

Frequent itemsets in memory  Prefix BRAM  Suffix BRAM  Frequent  { } a b Yes {ab} a c Yes {ab, ac} a d Yes {ab, ac, ad} a - Yes  TABLE II OPERATIONS PERFORMED BY THE ARCHITECTURE  Frequent itemsets in memory  Prefix BRAM  Suffix BRAM  Result in Suffix BRAM  Frequent Output Flush prefix BRAM  {ab, ac, ad} ab ac abc Yes Yes No {ab, ac, ad, abc} ab ad abd Yes Yes Yes {ab, ac, ad, abc, abd} ac ad acd No Yes Yes {ab, ac, ad, abc, abd} ad - - No No Yes {ab, ac, ad, abc, abd} abc abd abcd No No Yes  and the prefix of itemset ??? is ??, although they have the same cardinality they do not share the same prefix, and they cannot be intercepted to generate a new itemset. In contrast for itemsets ??? and ???, they share the prefix ?? and the same cardinality, in consequence they can generate the itemset ????.

The previous steps are executed until no more itemsets can be generated.

B. Dual Core Hardware Architecture  With the intention to get a speed up, a dual-core architecture is proposed. Figure 8 shows a high-level representation. In the logical programmable area, two hardware accelerators are implemented with the intention of distributing the workload between the two of them.

Previously it has mentioned that the proposed search strat- egy has the advantage of splitting the search space into disjoint sets or classes, in consequence, each core can process an equivalence class independently. Figure 8 describes the partition of the search space for four items. The first processor element receives the set of items ? = {?, ?, ?, ?} and it processes the equivalence class ?. Meanwhile, the second processor element receives the sets of items ? = {?, ?, ?} and it process the equivalence classes ?, ? and, ?. The dual core architecture obtains a parallelism to process independent equivalence classes, and this impacts directly on the perfor- mance of the proposed search strategy.



VI. EXPERIMENTAL RESULTS AND PERFORMANCE EVALUATION  A. Experimental setup  The hardware architectures have been evaluated using area and execution time metrics. The area evaluation is performed using the hardware report usage that provides Vivado HLS synthesizer. The execution of the architecture has been com- pared to the execution time of Apriori, Eclat and FP-Growth software implementations [5, 6]. These implementations can be found on the personal website of Christian Borgelt [7].

Hardware Accelerator  Programmable Logic Module Standalone OS  Interrupt Handler  Software Program  Memory Subsystem  MMU  System Bus  Memory UART  Arbiter  Hardware Accelerator  Fig. 8. Partition of the search space using 2 processor elements.

The software implementations have been tested on a PC with an Intel i3-3217U processor at 1.8 GHz and 8 GB DDR2 RAM memory with Windows 7 ultimate. The execution time considers the input and output operations and the CPU time for all the algorithms and the hardware architectures. The FPGA device employed is a Zynq 7020 of Xilinx.

B. Validation datasets  In the literature diverse datasets have been used to test the functionality of the software algorithms and hardware archi- tectures. In [2], an algorithm is proposed to generate synthetic datasets that imitates the characteristics of transactions in the retailing environment.

TABLE III DATASETS USED TO VALIDATE THE HARDWARE ARCHITECTURE.

Dataset Size (MB)  Average Length Transaction  Number of Transactions  Number of Items  Chess 0.013 37 3196 75 T40I3N500k 11.9 40 500k 299 T40I3N1000k 24.1 40 1000k 300 T60I5N500k 18.9 60 500k 500  The characteristics employed to generate the datasets are: number of transactions ???, average size of transactions ?? ?, average size of the maximal potentially large itemsets ???, number of potentially large itemsets ??? and, number of items ?? ?. All these characteristics are used to generate synthetic datasets. For this work, three values for ?? ?: 40, 60 and 90 have been chosen. The values for ??? are 3, 5 and 10. Table III summarizes the dataset parameter settings, and also an estimated of the size in MB of the datasets.

C. Performance evaluation  In this section the performance results for the compact architecture and the dual core architecture are presented. From figure 9 to 12, the compact hardware architecture and the dual core hardware architecture are compared to Apriori, FP- Growth, and Eclat; the ? axis represents the execution time of each experiment and the ? axis represents the minimum support value. Figure 9 shows the performance of the three software implementations and the hardware architectures for the chess dataset. For chess dataset, FP-growth obtained the  0.20.30.40.50.60.70.80.91  ?2  ?1      Support  R un  tim e  (s ec  on ds  )      Apriori Eclat Fp?Growth FIM Hardware Dual Core FIM Hardware  Fig. 9. Execution time comparison (Chess).

0.0450.050.0550.060.0650.070.0750.080.0850.090.095    Support  R un  tim e  (s ec  on ds  )      Apriori Eclat Fp?Growth FIM Hardware Dual Core FIM Hardware  Fig. 10. Execution time comparison (T40I3N500k).

best results among the four software implementations. The maximum speedup obtained by the compact architecture is 2.9x and 5.8x for the dual core architecture compared to the Fp-growth algorithm.

The better performance reported for the hardware architec- tures is when they have to deal with sparse datasets (Figures 10, 11, 12). The experiments show that the proposed archi- tectures have good performance and it obtains a speedup of 4x for the compact architecture and 12.7x for the dual core architecture compare with the best software implementations (it depends on the dataset and the support value). Table IV shows the area reports for both architectures. The operation frequency reported for both is 114 Mhz. The elements re- ported are DSP48E, Flip Flops and LUTs. For the compact architecture, the usage of Flip-Flops (3 %) and LUTs (9 %) is minimum because the architecture only needs a few registers to store ???? value and the control signals, this is the most compact design obtained.

For the dual core architecture, although there has been an increment in the resources employed, the architecture is still a compact one. Intuitively, an advantage of the compact design is that the number of cores that can be attached to the architecture can grow, and the workload can be divided among other processor elements to speed up the execution time.

0.040.050.060.070.080.090.1     Support  R un  tim e  (s ec  on ds  )      Apriori Eclat Fp?Growth FIM Hardware Dual Core FIM Hardware  Fig. 11. Execution time comparison (T40I3N1000k).

0.050.0550.060.0650.070.0750.080.0850.090.095         Apriori Eclat Fp?Growth FIM Hardwar e Dual Core FIM Hardwar e  Support  Ru nt  im e  (s ec  on ds  Fig. 12. Execution time comparison (T60I5N500k).

TABLE IV HARDWARE RESOURCES USED BY PROPOSED HARDWARE ARCHITECTURE.

Compact Architecture Dual Core Architecture Name DSP48E FF LUT DSP48E FF LUT Expression - 0 2618 - 0 3806 Multiplexer - - 1943 - - 2891 Registers - 3475 - - 5234 - Shift Memory - 0 164 - 0 279 Total 20 3475 4725 32 5234 6976 Utilization (%) 9 3 9 14 4 13  Compactness is the main advantage of the proposed hard- ware architecture. Although, it is a compact design, both versions accelerate the frequent itemset mining problem and speedup can be achieved.



VII. CONCLUSIONS  In this paper, a search strategy for frequent itemset mining that finds all the frequent itemsets regardless of the number of different items. The proposed search strategy fits well for hardware implementations because it splits the search space into separate equivalence classes making disjoint sets of the original dataset. In consequence, the amount of itemsets stored in the memory is reduced, this is an advantage for memory constrained scenarios like in the hardware architecture development. Another advantage of the partition into separate  equivalence classes is that the equivalence classes can be distributed among a set of processor elements to parallelize and distribute the workload. The most remarkable feature of this architecture is that gets a 4x to 12.7x speedup despite its compactness.

Based on the results obtained in this research, it is possible to implement an array of processor elements, in other words, scale up the proposed dual-core architecture from 2 to ? processor elements to get a better speedup.

