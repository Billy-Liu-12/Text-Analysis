Topic Detection based on Group Average Hierarchical Clustering

Abstract?Via analyzing characters of vast disaster news on the internet, a new topic detection algorithm based on Group Average Hierarchical Clustering (GAHC), which is suitable for the processing of big data on the network, is proposed in this paper. The core idea of GAHC is to divide big data into smaller groups, and then cluster groups hierarchically to generate final topics. During the process of clustering, vector space modal is used to represent news documents, and a similarity calculation model based on weights of time and place is proposed. The new algorithm can automatically organize similar disaster news materials, generate news topics, furthermore provide personalized service for users and form the topic detection system for disaster news. Experimental results demonstrate that the performance of the algorithm is good.

Keywords-topic detection; clustering; big data; GAHC

I.  INTRODUCTION With the development of information technology, more  and more data and information is available on the network.

According to CNNIC?s (China Internet Network Information Center) statistical result, the network users acquire about 86 percent of information from the news on the internet. With the rapid development of internet technology, people get more and more data and information from Web, and big data has become almost ubiquitous. How to quickly and accurately access hot topic from the vast data on the internet is the key problem to be resolved.

News topic provides the details about breaking news or widely concerned events. Nowadays most websites have provided news special column, but investigations show the organization of news special column is done manually. So how to automatically and accurately detect, track and report a topic of a new event is the critical problem.

Topic Detection and Tracking (TDT [1]) technology is presented under these circumstances, by which scattered information about a special event can be effectively collected and organized, so that all details about special event and relations between different events can be acquired more easily.

The two traditional categories of hierarchical clustering algorithm are agglomerative and divisive. Both of them have been applied to document clustering. When big data sets arrive, some incremental algorithms can efficiently process data. Such as ROCK [2] and CURE [3]. On the other hand, several static hierarchical algorithms have been proposed, including HFTC [4] and Malik?s method [5]. This method uses a routine based on a greedy algorithm, but it is not able to deal with dynamic data.

To acquire knowledge on the internet, a dynamic and agglomerative hierarchical clustering algorithm, more suitable for big data processing, is proposed for document clustering in this paper. GAHC based on Retrospective Event Detection (RED [6]) can divide big data sets into smaller groups to improve the computing speed and resource consumption. In order to improve the efficiency of algorithm, vector space modal is used to represent news document, and a similarity calculation model based on weights of time and place is also presented.

There are three advantages of GAHC.

(1) The system cost of clustering is reduced. Big data is  divided into smaller groups, so the computation complexity of large-scale data is greatly reduced.

(2) By employing the divide-and-conquer strategy, two similar documents are merged into the same group, and their similarity is computed, while irrelevant documents are separa-ted to avoid the aggregation of documents.

(3) GAHC can update the hierarchy when documents arrive (or are removed). It is an incremental and dynamic algorithm.

This paper is organized as follows: Section 2 describes knowledge learning process of topic diction. Section 3 presents document preprocessing and similarity calculation.

Section 4 shows the principle, preconditions, the basic flow, the threshold judgment and the complexity of GAHC.

Experimental results are shown in Section 5. Finally, Conclusion is presented in Section 6.



II. KNOWLEDGE LEARNING In our network-information-based topic detection system,  breaking event and hot news from Web with relevant risk information serves numerous network users in the form of topic, the knowledge learning process of topic detection is shown in Fig. 1.

In our system, firstly, related documents of news are collected and preprocessed. Secondly, the documents are represented by vector space model. Thirdly, the similarity of documents based on weights of time and place is calculated.

Finally, the topic structure is obtained by GAHC.

Figure 1.the knowledge learning process of topic detection   DOI 10.1109/CBD.2013.38     Definition 1: News event is defined as a special thing happened sometime and somewhere. An event can be described with many properties, including title, time, human name and place name.

Definition 2: Document vector space is composed of words and their corresponding weights, and document vector doc  are defined as follows:  1 1 2 2={( , ),( , ),...,( , )}i iword weight word weight word weightdoc The size of doc  is denoted as doc .

Definition 3: Event time is defined as the time interval  [ ]a btime time, Where atime is the first time to report a special news event and btime  is the latest time.



III. DOCUMENT PREPROCESSING AND SIMILARITY CALCULATION  A. Document Vectors Representation The elements of news are acquired by employing  information extraction technology combined with natural language understanding. From journalists? point of view, news stories are described as a topic, which usually includes following information: when, who, where and how does it happen. These properties are very important descriptive information and give the summary of news stories. In our topic detection system, each news is assigned with a semantic group, and each news is represented as four- dimensional feature vector  ={ , , , }Human Name Place Name Time ContentTags . Each document is represented as a corresponding vector after preprocessing.

B. The Feature Weight Calculation of Document Each group of Tags , for example Place Name , is a  weight vector, which is represented as i i i i  1 2 nW =(w ,w ,...,w )  (1 i 4)? ? and the weight iW  represent the weight of ? , where ? ? Tags  and iW  is also denoted as W? .

The support of word it ?doc  denoted as it ,docweight  can be computed using Term Frequency (TF) and Inversed Document Frequency (IDF)  [7].

i,i tt ,doc = f wweight  ?  ? ?  ?  ?? Where  ti, f  ? is the frequency of it  occurring in doc , it  is  marked by ?  and the weight of ?  is W? . Formula (1) can be normalized according to Formula (2).

ti,doc ti,doc  i,doct= max{ }  weight weight  weight  ?  C. Similarity Calculation Model Based on Weights of Time and Place The automatic document clustering is based on  similarity calculation and hierarchical clustering analysis.

The cosine function computing [8] is widely used in  documents? similarity calculation method. The similarity of document vector doc  and event template vector news  are defined using the cosine function computing, as is showed in formula (3).

t ti,doc i,news i  t ti,doc i,news  t  2 2 1 1  weight weight ( , )=  weight weight i i  Sim ?  = =  ??  ? ? doc news  doc news doc news ?  Where it ?doc news? indicates that it is the common element of doc and news , and  i,doct weight  represents the support of the word it to doc , and i,newstweight  represents the support of the word it to news . doc  is the size of doc , and news  is the size of news .

The time-distance of doc  and news  is defined in formula (4).

{ }( , )=min - , -doc news,a doc news,bDT time time time timedoc news  Where news,atime is the first time to report template news event and news,btime is the latest time.

The place-distance of doc  and news  is defined in formula (5), following the definition of the Boolean.

i i1   if t t  or no place name mentioned in both  and ( , )=  0  others  DP  ? ?? ?? ? ? ??  doc news doc newsdoc news  ?  For example, ?China? is represented as 5C.02, and ?The People's Republic of China? is represented as 5C. They are similar because they have the common prefixes. If the same city name exists in two documents, their place vectors of doc  and news  are similar. However, if no place name is mentioned in both doc  and news , their place vectors are also considered as the same.

After acquiring the time distance and place distance, the similarity of doc  and news  can be calculated according to formula (6).

( , = ( , )- ( , ) - ( , )-0.4 Score Sim DT  DP ? ? ?  docnews doc news doc news doc news  ? ? ?  Where ? is the influence factor of content similarity, and the range of ? is from 1 to 2; ? is the influence factor of time similarity, and the range of ?  is from 0 to 0.1; ? is the influence factor of place similarity, and the range of ?  from 0 to 0.1; 0.4 is a specified constant according to Inquery parameter.

When the same topic is reported by two documents, the vector based on time and place is used as a filter in the whole similarity calculation process. The model could reduce the complexity of clustering algorithm.



IV. GAHC  A. The Principle of GAHC The traditional hierarchical clustering algorithm aims to  build the hierarchy of documents. However, documents in each layer can not achieve the level of abstraction in practice. When the data scale is very large, the time complexity of hierarchical clustering algorithm is 2O( )n and a regular PC can?t meet the computing requirement of data processing.

GAHC is a better algorithm based on RED than ordinal hierarchical clustering. The average similarity between news reports in different topic clusters is maximized by employing the bottom-up greedy algorithm and the divide-and-conquer strategy.

B. Precondition of GAHC The application of GAHC should meet three  preconditions.

(1) The document must be represented as the vector with  factors of time and place.

(2) The input variable of GAHC must be news  documents sorted according to time.

(3) Its output is a hierarchical topic cluster structure.

C. The Basic Flow of GAHC Fig. 2 shows the basic flow of the GAHC.

(1) Each document is considered as a separate topic  cluster in document collections.

(2) The topic clusters of document collections are divided  into M  buckets in a time-continuous way, and each document is divided into only one bucket.

(3) Cluster analysis is respectively used in each bucket.

Two most similar topic clusters at a low level in buckets are repeatedly merged to form topic clusters at a higher level.

Each document that occurs in a leaf cluster occurs in all its ancestors. This procedure stops until the number of topic clusters in buckets reaches a predefined value P or the similarity between any two clusters is smaller than a predefined threshold S .

(4) The topic clusters of all buckets are collected on the condition of keeping time sequence, and the partition result of document collections is the current cluster collections.

The previous procedure is repeatedly done until the number of topic clusters at top level achieves a predefined value N .

(5)Re-cluster news documents according to the previous four steps in all the topic clusters at top level when new documents arrive or are removed.

D. The Threshold Judgment of GAHC A predefined threshold S  changes dynamically in the  third step of GAHC. The threshold model [8] based on time and place is used in our topic detection system. Two principle of threshold judgment is described as follows:  (1) Judge the threshold S . A news report away from a topic on time is difficult added to the topic. If the similarity of doc  and news  is smaller than S ( ( , ) )Score S?doc news the report is a new event. The  1 2 3c c c 4 5 6c c c ...i jc c ...k lc c   Figure 2. the basic flow of the GAHC  choice of S  is critical. The way that S  is directly set as a constant is unscientific, because S  is related to the specific document and the event. ( , )Sim doc news reflects the degree of correlation of doc  and news , and S  is set as  = ( , )S Sim doc news If ( , )Score doc news  is bigger than S doc is  corresponding to news  having the biggest similarity value.

If S  is bigger than 0.4, doc  is a new event, otherwise, doc should be belongs to news .

E. The Complexity of GAHC The time complexity of traditional hierarchical clustering  algorithm is 2O( )n while the time complexity of GAHC is O( )mn . Where n  is the number of news documents, and m is the size of the buckets, and m n? . GAHC can not only improve efficiency, but also increases the quality of topic clusters by considering the time sequence of news documents.



V. EXPERIMENTAL RESULTS  A. Test corpus Web information are mainly acquired by the web crawler  (robot) technique in the domestic and international research, such as Golaxy system [9]. Using the web crawler technique, these corpuses extracted from 405640 Chinese texts in the specified portal are trained in our topic detection system.

There are three representative events in 2013, such as, the earthquake in Sichuan Ya'an, the drought calamity in the southwest five provinces and the flooding in the northeast.

These test corpus are shown in Table 1.

B. Evaluation Standard There are several measures to evaluate the quality of  hierarchical clustering. The performance of topic detection is evaluated by Misses Rate (MR) and False Alarm Rate (FAR) in TDT. MR and FAR can be evaluated as follows:  Misses C A+C  False Alarm B B+D       TABLE I.  STATISTICS OF DOCUMENT COLLECTIONS   A widely F1-measure [10] (Larsen and Aone, 1999) is adopted, which compares the system-generated clusters at all levels of the hierarchy and combines the recall and precision factors. The F1-measure is calculated as follows:  Recall AR= A+C  Precision AP= A+B  2PRF1-measure= P+R  Where A is the number of related detected documents in the system, and B is the number of disrelated detected documents, and C is the number of related undetected documents, and D is the number of disrelated undetected documents. The higher the F1-measure, the better the clusters is, due to the higher accuracy of the clusters mapping to the topics.

C. Test results In order to evaluate the sensitivity of GAHC to  parameters ?  and ? , the F1-measure is depicted in Fig.3.

The clustering accuracy can be improved by adjusting parameters of GAHC. The smaller ?  value, the greater clusters are obtained. As we can observe, the high accuracy of GAHC is fairly consistent while ?  and ?  is set as a value between 0.01 and 0.05.

Three experiments are compared and analyzed about the original data, and their results are showed in Fig. 4 and Fig.

5. When the similarity of documents is calculated, parameters ? , ?  and ?  are adjusted to improve the performance of GAHC. In the first experiment, the weight of time and place is not considered ( =1.6, =0 and =0)? ? ?  .

In the second one, only the weight of time is considered ( =1.7, =0.02 and =0)? ? ?  . In the third one, the weight of time and place is considered ( =1.8, =0.05 and =0.04)? ? ? .

Fig. 4 shows MR and FAR value are reduced by employing the similarity calculation model based on time and place. On condition that FAR is 15%, MR is 19% when factors of time and place are considered, and MR is 26% when factor of time are considered. On the contrary, MR is 29% when factors of time and place are not considered.

Fig.5 shows the effectiveness of GAHC is influenced by factors of time and place. The experiment results show that the effectiveness of GAHC based on similarity calculation  Figure 3. GAHC sensitivity to ?  and ?  Figure 4. Efficiency of MR and FAR  Figure 5. GAHC effectiveness     Figure 6.  Time performance  model considering weights of time and place is better than based on traditional similarity calculation model considering content. For example, corresponding to the event of ?2013 the earthquake in Sichuan Ya'an?, F1-measure is 0.74 with factors of time and place considered, and the F1-measure is 0.69 with factors of time considered. On the contrary, F1- measure is 0.59 without factors of time and place considered.

The higher the F1-measure, the better the effectiveness of GAHC is. Again, GAHC based on similarity calculation model of time and place performs the best. When the similarity of document and news is calculated with factors of time and place considered, our experimental results prove that satisfied efficiency can be obtained.

Fig.6 shows the time consumed by CURE, ROCK and GAHC.  Each curve represents the time consumption to the document collections. As we can observe, ROCK method clearly overcomes CURE. Notice that the efficiency of CURE is comparable with ROCK, but its runtime continues to grow rapidly while there are 8000 or more documents.

ROCK runs faster than CURE.  CURE is not scalable since its high space complexity ( 2O( )n ). To sum up, GAHC based on similarity calculation model of time and place are more efficient than ROCK and CURE.



VI. CONCLUSION According to the requirement of the topic detection  system, the knowledge learning process of topic detection is  given, and a more suitable big data processing algorithm GAHC is proposed. Similarity calculation model based on weights of time and place can reduce the complexity of documents clustering algorithm. Our experimental results show that GAHC improves the performance of topic detection. In the future, our work will focus on the research of new clustering strategy to improve the efficiency of documents clustering algorithm by analyzing document content.

