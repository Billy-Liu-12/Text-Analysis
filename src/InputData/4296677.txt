Fuzzy Data Mining in Higher Dimensions for Data Analysis

Abstract  To extract fuzzy rules from databases or data files, we  first select a set of  attributes to associate and restrict all  records (rows) to these. We next embed these feature vectors  of small  dimension into a high dimensional feature space by  a Gaussian kernel mapping that yields a symmetric fuzzy  membership matrix. The entry value at row i and column j is  a fuzzy truth value that feature vectors i and j are in the  same cluster. We look for clusters where features A and B  associate in some way, e.g., (A is HIGH) and (B is LOW), so  if the support and confidence are high enough, we accept  that rule. The cluster centers become centers of fuzzy set  membership functions to use in fuzzy modus ponens with the  fuzzy rules . We apply our novel algorithm to analyze two  difficult well known datasets.

1. Introduction  The concepts that led to data mining appear in [7], 1987  and are documented in [8], 2001. The classical market basket  approach was developed in [1], 1993 for binary valued  attributes (present or absent). The three steps in data mining  [4, 5] are: 1) data preparation and preprocessing (cleaning  of noise and missing data, selection of attributes); 2) data  mining (clustering, classification, and association); and 3)  pattern evaluation and presentation (identification and  evaluation of results, and presentation in suitable form).

According to [14], 2006, the literature applications of  association rule mining are on relatively small datasets,  although association rule mining works best on large  datasets. Their results show that non-binary quantitative rule  extraction is as good as fuzzy rule extraction. But methods  used for associative rule extraction are subjective, and the  boundary problem (where does MEDIUM end and HIGH  begin?) is easily handled by fuzzy set membership functions  (FSMFs) where a feature may belong partially to multiple  fuzzy sets. Fuzzy rules also handle noise well.

Methods of clustering for the second step of data mining  include the k-means, k-nearest neighbors, fuzzy clustering,  and density clustering. While the fuzzy c-means [3], 1981, is  most often used for the clustering step of data mining, a more  flexible and robust (semi-density) method is the weighted  fuzzy expected value (WFEV) method [12]. Fuzzy data  mining has been a popular research topic [6], 2001, and [11],  1998, and research is now being done to find good fuzzy set  membership functions, whose shapes may be adjusted by  genetic algorithmic methods [9], 2006, [10], 1993.

A transaction is an instance of an N-tuple of values for N  ordered attributes (also known as  features). Let U  designate  the universe of transactions for a project of data mining. Let  A and B denote two features, where ||A|| and ||B|| represent  the cardinalities (counts) of their occurrences over all  transactions (feature vectors) in U . This is the binary situa-  tion where either A does or does not occur in a transaction).

In the fuzzy situation, A may partially occur with a fuzzy  Atruth f , i.e., A may take a value in a fuzzy set membership  function on the range of feature A.

To establish a rule (A => B) we must establish that  whenever the antecedent A occurs it is usual that the  consequent B occurs.The support of the rule A => B and the  confidence of the rule, respectively, are  supp(A => B) = ||A1B|| / ||U|| (1)  conf(A => B) = ||A1B|| / ||A|| (2)  where ||A1B|| is the cardinality of the class A1B of all transactions in U  where both A and B occur.

A rule (A => B) is of interest to a user if its confidence  (the frequency probability of B occuring whenever A occurs)  is above a certain threshold selected by the user, provided  supp(A => B) is  high enough to represent a sufficiently  large sample. But if the sample is relatively small, then we  would hesitate to use the rule even if it has a high  confidence. If the rule (A1B => C) is valid then (A => C) and (B => C) are also valid rules, but not vice versa. Also  rules may have the form (A <==> B) (A if and only if B).

The clustering of Step 2 in the first paragraph falls into  one of the following categories [8]: a) partitioning clustering;  b) hierarchical clustering; c) density clustering; d) grid-based     clustering; or e) model-based clustering. We use a new fuzzy  clustering method that employs a user adjustable threshold  for refinement, but it is not sensitive to the threshold value  and yields good results for a range of reasonable values. It  outperforms fuzzy c-means and other methods it has been  tested against (see [13]).

Our actual fuzzy rules use the linguistic variables of  Zadeh [15] that take values such as LOW, MEDIUM, or  HIGH with a fuzzy truth to form a fuzzy rule such as  (A is HIGH) =>[f] (B is LOW)  Section 2 lays out our method of mining rules, while the  Sections 3 and 4 describe the two respective datasets used  (the iris and the Wisconsin breast cancer data), as well as the  results of running our algorithms on this data. Section 5  provides analysis and conclusions for this approach.

2. The Proposed Method  A table of records contains Q rows (transactions) and  each row contains an instance of N values for the  N features,  called features (or fields in database parlance). We first  select two features at a time to determine if they are  associated strongly enough to form a useful rule. Figure 1  shows the situation for Q  rows of feature values in the fields.

Figure 1. A table of transactions for associating rules.

In the first step (data preparation and pre-processing), we  j kselect features A  and A  to be tested for association, and here  we let j = 1 and k = 2 for specificity. We select from the  1 2records all transactions where both A  and A  have valid  feature values (no missing values). We assemble these into  1 2a set of Q  feature vectors with only the features A  and A .

{x : q = 1,...,Q}  (3) (q)  1 2x  = (a , a )  (4) (q) (q) (q)  Then we fuzzify (standardize) each feature to obtain the  fuzzified 2-dimensional vectors of fuzzy components.

We use the fuzzified values of each selected feature  instead of the actual values. The fuzzification is done first by  means of a single Takagi FSMF for each feature. The single  FSMF is called by the linguistic variable STRONG . For each  feature it linearly maps its minimum feature value to 0 and its  maximum feature value to 1. Given a value of a feature, its  membership in STRONG is given by the fuzzy truth it is  mapped to by this FSMF.

Figure 2 shows a Takagi FSMF for feature A, given by  f = (a - minA) / (maxA - minA)  (5a)  where f is the fuzzy truth of (A is STRONG), and minA,  maxA are the respective minimum and maximum of the  range of A. The inverse mapping that defuzzifies the fuzzy  truths of the consequents is  a = (maxA - minA)f + minA  (5b)  Figure 2. A Takagi fuzzy set membership function.

Figure 3. Three FSM Fs for a feature.

jIt is also useful to employ FSMFs for each feature A   as  shown in Figure 3 so that rule can be a fuzzy rule antecedent.

1 2Our method is different in that we cluster the vectors (a , a )  into clusters. Instead of defning LOW, MEDIUM and HIGH  apriori as would be done if, e.g., Figure 3 were used, we look  1 2 at the cluster centers (c , c ) as centers of the FSMFs and use  1 1 2 2 1 2the format (A  is c  ) => (A  is c  ) where (c ,  c  ) is found  during the clustering process.

Our clustering method used in the second step (data  mining) must cluster the 2-dimensional vectors with fuzzy  components into clusters so we can check each to see how  1 2the values of A  and A  correspond. To do this, we compute  the fuzzy membership matrix M via the Gaussian kernel  (weighting) function  q,r[ m  ] = [ exp{-||x  - x || /(2F )} ]  (6) (q) (r) 2 2  1 2where x  = (a , a ). See [13] for an explanation of (q) (q)(q)  kernel partitioning with higher dimensional matrices.

q,rThis correlates x  and x  by the fuzzy truth value m (q) (r)  q,r(0 < m  # 1) and represents the fuzzy truth that each pair x  and x  belong to the same fuzzy class. If the fuzzy value  (q) (r)  q,rm  is relatively high, then they belong in the same cluster.

The diagonal of the fuzzy membership matrix consists only  of the maximal fuzzy truth value of 1.0.

If we now select a rather central threshold T, e.g, T = 0.5,  and put (although this is not necessary)  q,r q,rm  = 0,   if  m  < T  (7)  q,rthen the resulting thresholded fuzzy association matrix [w ]  naturally decomposes the fuzzified feature vectors into crisp  disjoint groups by an equivalence relation. If this is not done,  then we have a fuzzy clustering, but otherwise we can peruse  the results and either increase or decrease T. A good first  value is T = 0.5.

After we have the fuzzy clusters, we start with the largest  cluster first first and find its center vector denoted here as c  1 2= (c , c ). We process this cluster as described below and  then take the next largest cluster for processing, and continue  this until all clusters have been considered for rules.

1The processing consists of taking c  as the center of the  1 2 2FSMF for feature A  and c  as the center of a FSMF for A .

1 1 2 2We call these FSMFs (A  is c  ) and (A  is c  ), respectively,  1 1 2 2and use the cardinality ||(A  is c  ) 1 (A  is c  )|| as the cardinality of the cluster being processed. We also find the  1 1 2 2cardinalities  ||(A  is c  )|| and ||(A  is c  )|| by clustering on  each single feature separately and using the single cluster  k 1 1 1 1whose values match c , so that (A  is c  ) means: A  = a  and  1 1 1a  belongs to the FSMF with center at c  with fuzzy truth f  1 1= |(A  = a  )|. The rule will have the form  1 1 2 2 (A  is c  ) => (A  is c  )  if the antecedent has greater cardinality than the consequent.

1 1 2 2The fuzzy confidence fconf of (A  is c  ) => (A  is c  ) can  1 1now be found, where the instance A  = a  is counted only if  1 1a  is close to c .

12 1 1 2 2n  = ||(A  is c  ) 1 (A  is c  )||  (8)  1 1 1n  = ||(A  is c  )|| (9)  1 1 2 2 12 1fconf( (A  is c  ) => (A  is c  ) ) = n  / n   (10)  12 1 1 2 2The fuzzy truth f  = fconf( (A  is c  ) => (A  is c  ) ) is  used  when the rule is fired by the fuzzy truth of the antecedent  (see fuzzy modus ponens below).

Figure 4 (top part) shows the the case where a novel input  1 1 1 1value A  = a   is put through the FSMF for (A  is c  ) to  1 1 12obtain the fuzzified value f  of a . The fuzzy truth f  of the  1 1 2 2 rule (A  is c  ) => (A  is c ) is given by Equation (10). The  2 2fuzzy truth of the consequent (A  is c ) is determined from  12 1 1 2 2 both the fuzzy truth f  of the rule  (A  is c  ) => (A  is c )  1 1 1and the fuzzy truth f  = |(A  = a  )| in the fuzzy set we call  1 1(A  is c ).  The fuzzy modus ponens is  1 1 12 2 2 (A  is c  ) =>[f ] (A  is c )  1 1 1(A  is a  )[f ]  2 2 2        (A  is c )[f ]  2 2 2 12 1f  = |(A  is c  )| = min{ f , f  } (11)  Figure 4. Firing a fuzzy truth for consequent.

1 2The values c , c , etc. can be interpreted as being LOW,  MEDIUM LOW, MEDIUM, MEDIUM HIGH, or HIGH,  etc., which can be used to frame rules of the forms  1 2(A  is HIGH) => (A  is HIGH)  1 2(A  is HIGH) => (A  is MEDIUM HIGH)  :           :  1 2(A  is LOW)   => (A  is LOW)  The difference here is that we do not prejudge where the  center point of a FSMF should be as is done in Figure 3,  which may not fit the real situation as well. The output fuzzy  2truth, e.g., a   in Figure 4, can be defuzzified by putting it  2through the inverse Takagi FSMF to obtain a value for A  in  the original range of that feature.

The algorithm is given below at a high level.

Step 0: read in all feature vectors of N dimensions and map  each feature by its Takagi FSMF for fuzzification     Step 1: select two features A, B to check for association rules  Step 2: build Q  2-dimensional vectors (a,b) for association  qrStep 3: compute the fuzzy association matrix [ m  ]  and  zero out all entries below user given threshold T  to obtain the crisp fuzzy membership matrix  qr[ w  ]  that provides disjoint classes (T can be  slightly adjusted from the initial value 0.5)  qrStep 4: select largest cluster from [ w  ]   not yet processed,  1 2 1 2find its center c = (c , c  ) for (A is c ) => (B is c )  Step 5: compute cardinalities, fuzzy confidence and fuzzy  support for this rule  Step 6: if fuzzy confidence of rule is acceptable and sample  size is adequate, then add the new rule and its  fuzzy confidence to rule list  Step 7: if any clusters remain, goto Step 4  else if user selects more processing, go to Step 1  else stop  The Takagi FSMF at the beginning standardizes the data,  which is well known to prevent numerical instability and  round-off error in computations. If we are interested only in  LOW and HIGH FSMFs then we can use the logistic  function (sigmoid) in place of the Takagi FSMF.

The fuzzy membership (kernel) matrix is a new fuzzy  clustering method that is very powerful because it separates  clusters in a higher dimensional space with simple (linear)  hyperplanes whose inverse mapping back to the original  space would be a complex separating hypersurface that could  not be determined [13].

Clustering, including fuzzy clusering, need not find the  actual classes. We obtain rules from clusters, while classes  may consist of a single or multiple clusters. If two or more  1 1clusters provide the same rule format, such as, (A  = c  ) =>  2 2 1 1 2 2 k(A  = c  ) and (A  = d  ) => (A  = d  ) where d  is  kapproximately to c , k = 1,2, then we can combine their  counts in the rule support and confidence and use the  k kaverages of c  and d , k = 1,2. The averages should be  weighted by the cardinalities of the clusters.

3. Computer Runs on the Iris Data  The first dataset is the well known iris data that can be  obtained from the University of California, Irvine, Machine  Learning Repository [16] at  http://www.cs.uci.edu/~mlearn/MLRepository.html  There are 150 feature vectors of 5 features each, which  respectively give the petal length, petal width, sepal length,  sepal width, and the manually assigned class label (variety of  the iris species). Anderson [2] originally assigned these  labels manually (as the fifth feature) for 3 classes. However,  many researchers now think that there are only two classes  [12]. It seems that Classes 2 and 3 are the same class with  one having slightly smaller specimens (the length to width  ratios as features provide evidence for this). Feature 2 is very  problematic and confuses the classification.

In our first experiment, we want to check the associations  of each of Features 1, 2, 3, and 4 with the label of feature 5.

To save space, we present here the results of association of  15Features 1 and 5, but we report on other runs. We get n  =  50 from our kernel clustering using Equation (8). These are  all the same vectors labeled by Anderson as Class 1, while  the other clusters can be split in various ways into two sub-  clusters (but no clustering we know of yields cardinalities of  50 each for the remaining 100 vectors).

Table 1. Results of Run 1 on the Iris Data  Features  Cluster Size    c1 ,c2 Fuzzy  Confidence  1 => 5          50            0.196, 1  50/59 = 0.85  T 8            0.681, 2 X  32            0.546, 3 X 16            0.524, 2 X 17            0.832, 3 X 23            0.362, 2 X 1            0.167, 3 X 8            0.185, 2 X  X indicates value is too low to be of interest  Table 1 shows that Feature 1 implies the label (used here  as Feature 5) for Class 1 with an 85%  confidence for the  1fuzzy rule. We clustered Feature 1 singly to get n  = 59  1values that had a weighted average of c  = 0.196. The FSMF  1 1 1for (A  = c  ), where c  = 0.196 is the right-half of a  Gaussian where the entire Gaussian would be centered on  0.196, and with unit value from 0 to 0.196. Figure 5 shows  this on the left side.

1 1Figure 5. Half-Gaussian FSM F for (A  = c ).

4 4 5Next we consider the association for (A  = c  ) => (A  =  5c  ). To further elucidate, we show the actual clusters by  4 5vector number in Table 2 for Run 2, where (A  => A ). In  our file of iris data the vectors to cycle through labels 1, 2, 3  repetitively.

4 5Thus (A  = 0.0608) => (A  = 1) accounts for all 50  vectors labeled by Anderson as being in the first class. A  closer examination shows Clusters 2 and 5 together yield a  class of 50 (43 + 7) vectors for Class 2, which coincides with     Anderson?s labeling. The weighted average for the second  class is (43/50)0.533 + (7/50)0.375 = 0.510. Similarly, we  see that Clusters 3 and 4 yield Anderson?s third class of 50  vectors with weighted average (14/50)0.944 + (36/50)0.748  = 0.803 (see the last paragraph of Section 2).

Table 2. Results of Run 2 on the iris data.

Cluster 1, 50 vectors numbered:  1  4  7  10  13  16  19  22  25  28  31  34  37  40  43 46  49  52  55  58  61  64  67 70  73  76  79  82  85  88  91 94  97  100  103  106  109  112  115 118  121  124  127  130  133  136  139 142  145  148  Cluster 2, 43 vectors numbered: 2  5  8  11  14  17  20 26  29  35  41  44  47  50  56 59  62  65  68  71  74  77  80 83  86  92  98  101  104  107  110 113  116  119  122  125128  134  137 140  143  146  149  Cluster 3, 14 vectors numbered: 3  30  45  48  57  63  108  111  123  126  132  135  138  147  Cluster 4, 36 vectors numbered: 6  9  12  15  18  21  24 27  33  36  39  42  51  54  60 66  69  72  75  78  81  84  87 90  93  96  99  102  105  114  117 120  129  141  144  150  Cluster 5, 7 vectors numbered:  23  32  38  53  89  95  131  Cluster Center(1):  ( 0.060833, 0.0 )  Cluster Center(2):  ( 0.532946, 0.5 )  Cluster Center(3):  ( 0.943452, 1.0 )  Cluster Center(4):  ( 0.747685, 1.0 )  Cluster Center(5):  ( 0.375000, 0.5 )  Now we have the fuzzified/standardized centers of 0.061  (rounded), 0.510, and 0.803. Upon placing FSMFs like those  of Figure 5 at these values, we can use the 3 rules  4 5(A  = 0.061) => (A  = 0.0)  4 5(A  = 0.510) => (A  = 0.5)  4 5(A  = 0.803) => (A  = 1.0)  5where Feature A  is the class (1, 2, or 3) as labeled by  Anderson, but standardized here.

In the same way, Feature 3 is also a correct classifier  according to Anderson?s labeling, but it is much better than  Feature 1 and also better than Feature 4. In fact, Features 3  and 4 are sufficient to classify the iris data according to the  labels given by Anderson. However, we need to mention that  the labels themselves were used in the classification, and may  bias it. This is like the problem of training a neural network  or a support vector machine on data: they learn the labels,  which may be incorrect or noisy.

To prevent the labels from influencing the ability of  Feature 3 to cluster correctly according to Anderson?s labels,  we clustered the iris dataset on only Feature 3.These results,  in Table 3, coincided perfectly with Anderson?s classes.

Table 3. Results of Run 3 on the iris data.

Cluster 1, 50 vectors numbered: 1  4  7  10  13  16  19 22  25  28  31  34  37  40  43 46  49  52  55  58  61  64  67 70  73  76  79  82  85  88  91 94  97  100  103  106  109 112 115  118  121  124  127  130  133  136  139 142  145  148  Cluster 2, 39 vectors numbered: 2  5  8  14  17  20  21 26  35  41  42  47  50  56  60 62  66  68  71  72  74  77  80 81  83  84  86  104  107  110  113 117  122  125  134  137 140  141   Cluster 3, 23 vectors numbered: 3 9 12 15 24 27 30 39 51  63  75  78  87  90  93 99  105  108  111  114  123  132  135  Cluster 4, 14 vectors numbered:  6  33  36  45  48  101  102  120  126  129  138  144  147  150  Cluster 5, 13 vectors numbered: 11  29  38  53  59  65  92 95  98  116  119  128  149  Cluster 6, 5 vectors numbered: 18  54  57  69  96  Cluster 7, 6 vectors numbered: 23  32  44  89  131  146  From Table 3 we see that Cluster 1 is Class 1 as labeled  by Anderson, but we have not used labels in this run.

Clusters 2, 6, 7 provide the 50 feature vectors labeled as  Class 2, while Clusters 3, 4 and 5 comprise Anderson?s Class  3 53. The confidence here is 100% that (A  => A ). We also  2 5made runs for the rule (A  => A  ), but with poor results.

4. Runs on the Wisconsin Breast Cancer Data  This data can also be downloaded from the University of  California, Irvine, repository [16]. Our version of the file is  named wisc9-699.dta because there are 699 vectors. Each  vector contains in the first field an identification number that  we don?t use. After that there are 9 feature values followed  by the label feature of 2 for benign or 4 for for malignant.

The feature values were entered by humans during biopsy,  where all feature values are from 0 to 10 and describe some  feature of the cell (such as size, symmetry, etc.). There  appears to be a lot of noise in the feature values upon  examination because for a label of 4, the values of a given  feature vary from low to the high value of 10.

There are 241 records labeled as 4, and the remaining 458  are labeled as 2. A worthy goal is to find one or two features  that correlate highly with the labels. To this end we associate  Features 2 and 10 and check the clusters produced to see  how they match up with the labels.Table 4 shows the results.

2 10Table 4. Results of A  => A  on the wisc9-699 data.

2 2 10 10   A   = c      A  = c  Cluster Center(1):  ( 0.009859,  0.000000 )  Cluster Center(2):  ( 0.250000,  0.000000 )  Cluster Center(3):  ( 0.777778,  0.000000 )  Cluster Center(4):  ( 0.992284,  1.000000 )  Cluster Center(5):  ( 0.307256,  1.000000 )  Cluster Center(6):  ( 0.671362,  1.000000 )  Cluster Center(7):  ( 0.555556,  0.000000 )     2 10Table 4 shows that upon associating A  with A  we get  Clusters 4, 5, and 6 that have the output label of 4  (standardized to 1). Because we used the labels in clustering,  the results were perfect: we checked each vector of Clusters  4, 5, and 6 and they were the exact 241 clusters labeled 4.

2However, the A  values are inconsistent, except for Cluster  4, whose center 0.992 is out of the range of the Class 1  (Clusters 1, 2, 3, and 7 with conflicting centers of 0.00986,  0.2500, 0.7778, and 0.5556). Only Cluster 4 gives a  2consistent result, so we could form the rule (A  = 0.992) =>  10 2 10(A  = 1) (or (A  is VERY-HIGH) => (A  = MALIGNANT)).

This study has shown the problem with this data. Other  combinations also yield some erratic results.

5. Analysis and Conclusions  Our method uses a QxQ fuzzy membership matrix,  formed by a kernel mapping of the Q feature vectors of  dimension N, where N < Q, to correlate vectors of features  qrto be correlated. An entry m  provides the fuzzy association  of feature vectors x  and x , so that a value close to unity (q) (r)  implies they belong to the same cluster (with the fuzzy truth  qrm  ). By means of adjusting a quite nonsensitive threshold  we can obtain a partitioning by the matrix of the vectors into  disjoint clusters, or we can not use a threshold and obtain  fuzzy memberships from which to pick the associations.

After we obtain cluster centers we then position FSMFs on  such centers to use for fuzzy rules.

We analyzed the iris data via our methodology to show  that Features 3 and 4 can be used together, or separately, to  cluster the iris data into the classes designated by Anderson,  matches Anderson?s labeling completely. We also used our  algorithm to show the inconsistencies of the Wisconsin  breast cancer data. But we also showed that a very high value  of Feature alone 2 is effective in detecting the malignancy of  a cell.

The performance was excellent for the high confidence  rules for indicating the iris class or for implying malignancy,  a judged by the original labeling (of which one should  always allow for error). Future work will both refine the  method and the C++ programs we wrote to implement the  algorithm. It will also test the use of multiple antecedents.

6. References  [1] R. Agrawal, T. Imielinski and A. Swami, ?Mining association rules between sets of items in large databases,? Proc. ACM SIGMOD Int. Conf. Management Data, 1993, 207 - 216.

[2] E. Anderson, ?The irises of the Gaspe peninsula,? Bull.

American Iris Society 59, 1935, 2 - 5.

[3] J. C. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms, Plenum Press, New York, 1981.

[4] U. Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthurusamy, Advances in Knowledge Discovery and Data Mining, MIT Press, Cambridge, MA, 1996.

[5] Hichem Frigui, ?MembershipMap: data transformation based on granulation and fuzzy membership aggregaton,? IEEE Trans Fuzzy Systems 14(6), 2006, 885 - 896.

[6] A. Gyenesei, ?A fuzzy approach for mining quantitative association rules,? Acta. Cybern. 15(2), 2001, 305 - 320.

[7] P. Hajek and T. Havranek, Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory, Springer-Verlag, Berlin, 1987.

[8] J. Han and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, San Mateo, CA, 2001.

[9] Y.-C. Hu, ?Determining membership functions and minimum fuzzy support in finding fuzzy association rules for classification problems,? Knowledge-Based Systems 19, 2006, 57 - 66.

[10] C. L. Karr and E. J. Gentry, ?Fuzzy control of pH using genetic algorithms,? IEEE Trans. Fuzzy Systems 1, 1993, 46 - 53.

[11] C. M. Kuok, A. W.-C. Fu and M. H. Wong, ?Mining fuzzy association rules in databases,? ACM SIGMOD Rec. 27(1), 1998, 41 - 46.

[12] C. G. Looney, ?Interactive clustering and merging with a new fuzzy expected value,? Pattern Recognition 35, 2002, 2413 - 2423.

[13] C. G. Looney, ?Fuzzy connectivity clustering with radial basis kernel functions,? (preprint), to appear in Pattern Recognition).

[14] H. Verlinde, M. DeCock and R. Boute, ?Fuzzy versus quantitative associations rules: a fair data-driven comparison,? IEEE Trans. SMC-B 36(3), 2006, 679 - 684.

[15] L. A. Zadeh, ?Computing with words,? IEEE Trans. Fuzzy Systems 4, 1996, 103 - 111.

