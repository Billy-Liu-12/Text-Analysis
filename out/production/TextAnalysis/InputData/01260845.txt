Generalized Closed Itemsets for Association Rule Mining

Abstract  The output of boolean association rule mining algo- rithms is often too large for manual examination. For dense datasets, it is often impractical to even generate all frequent itemsets. The closed itemset approach handles this infor- mation overload by pruning ?uninteresting? rules following the observation that most rules can be derived from other rules. In this paper, we propose a new framework, namely, the generalized closed (or g-closed) itemset framework. By allowing for a small tolerance in the accuracy of itemset supports, we show that the number of such redundant rules is far more than what was previously estimated. Our scheme can be integrated into both levelwise algorithms (Apriori) and two-pass algorithms (ARMOR). We evaluate its perfor- mance by measuring the reduction in output size as well as in response time. Our experiments show that incorporating g- closed itemsets provides significant performance improve- ments on a variety of databases.

1. Introduction  The output of boolean association rule mining algo- rithms is often too large for manual examination. For dense datasets, it is often impractical to even generate all frequent itemsets. Among recent approaches to manage this gigan- tic output, the closed itemset approach [4, 5] is attractive in that both the identities and supports of all frequent itemsets can be derived completely from the frequent closed item- sets. However, the usefulness of this approach critically de- pends on the presence of frequent itemsets that have super- sets with exactly the same support. This means that even minor changes in the database can result in a significant in- crease in the number of frequent closed itemsets. For ex- ample, adding a select 5% transactions to the mushroom dataset (from the UC Irvine Repository) caused the number of closed frequent itemsets at a support threshold of 20% to increase from 1,390 to 15,541 ? a factor of 11 times!

In order to overcome this limitation, we propose in this  ?Database Systems Lab, SERC, Indian Institute of Science, Bangalore 560012, INDIA. Email: fvikram, haritsag@dsl.serc.iisc.ernet.in  paper the generalized closed (or g-closed) itemset frame- work, which is more robust to the database contents. In our scheme, although we do not output the exact supports of fre- quent itemsets, we estimate the supports of frequent itemsets within a deterministic, user-specified ?tolerance? factor. A side-effect of allowing for a tolerance in itemset supports is that the supports of some ?borderline? infrequent itemsets may be over-estimated causing them to be incorrectly identi- fied as frequent. Since our typical tolerance factors are much less than the minimum support threshold, this is not a major issue. Further, an extra (quick) database pass can always be made to check these borderline cases.

We provide theoretical arguments to show why the g- closed itemset scheme works and substantiate these obser- vations with experimental evidence. Our experiments were run on a variety of databases, both real and synthetic, as well as sparse and dense. Our experimental results show that even for very small tolerances, we produce exponentially fewer rules for most datasets and support specifications than the closed itemsets, which are themselves much fewer than the total number of frequent itemsets.

Our scheme can be used in one of two ways: (1) As a post-processing step of the mining process, or (2) as an in- tegrated solution. Further, our scheme can be integrated into both levelwise algorithms as well as the more recent two-pass mining algorithms. We note that integration into two-pass mining algorithms is a novel and important con- tribution because two-pass algorithms have several advan- tages over Apriori-like levelwise algorithms. These include: (1) significantly less I/O cost, (2) significantly better overall performance, and (3) the ability to provide approximate sup- ports of frequent itemsets (with a deterministic bound in er- ror) at the end of the first pass itself. This ability is essential for mining data streams as it is infeasible to perform more than one pass over the complete stream.

2. Generalized Closed Itemsets  In addition to the standard boolean association rule min- ing inputs (I, the set of database columns, D, the set of database rows and minsup, the minimum support thresh- old), the frequent g-closed itemset mining problem also takes as input ?, the user-specified tolerance factor. It pro-      duces as output a set of itemsets (that we refer to as the fre- quent g-closed itemsets) along with corresponding supports.

The frequentg-closed itemsets are required to satisfy the fol- lowing properties: (1) The supports of all frequent itemsets can be derived from the output within an error of ?. (2) If ? = 0, the output is precisely the frequent closed itemsets.

2.1. Generalized Openness Propagation  The key concept in the g-closed itemset framework lies in the generalized openness propagation property, which is stated in the following theorem1. Here, the supports of itemsets X and Y are said to be approximately equal or ?-equal (denoted as support(X) ? support(Y )) iff jsupport(X)? support(Y )j ? ?.

Theorem 2.1 If X and Y are itemsets such that Y ? X and support(X) ? support(Y ), then for every itemset Z : Z ? X , support(Z) ? support(Y [ Z).

Here Y can be considered redundant because its support can be estimated (within an error of ?) from that of X . This theorem implies that if Y can be considered redundant in such a fashion, then all supersets ofY can also be considered redundant. This result suggests a general pruning technique to incorporate into mining algorithms, which we refer to as ?-equal support pruning: If an itemset X has an immediate superset Y 2, with ?-equal support, then prune Y and avoid generating any candidates that are supersets of Y . The sup- port of any of these pruned itemsets, say W , will be ?-equal to one of its subsets, namely, (W ? Y ) [X .

2.2. Approximation Error Accumulation  A direct application of the ?-equal support pruning tech- nique outlined above will not produce correct results: the supports of all frequent itemsets will not be derivable even approximately from the output. This is because Theo- rem 2.1 considers for any itemset X , only one superset Y with ?-equal support. If X has more than one superset (say Y1; Y2; : : : ; Yn) with ?-equal support then a naive in- terpretation of the generalized openness propagation prop- erty would seem to indicate the following: Every itemset Z : Z ? X  V Z 6? Yk; k = 1: : :n, also has a proper super-  set Sn k=1  Yk[Z with ?-equal support. Although this is valid when ? = 0, in the general case, it is not necessarily true.

However, the following theorem reveals an upper bound on the difference between the supports of  Sn k=1  Yk [Z and Z.

Theorem 2.2 If Y1; Y2; : : : ; Yn; Z are supersets of item- set X , then support(Z) ? support(  Sn k=1  Yk [ Z) ?P n  k=1 (support(X)? support(Yk)).

1Proofs of theorems are available in the full version of this paper [2].

2An immediate superset ofX is a superset ofX with cardinality jX +  1j. Likewise, an immediate subset ofX is one with cardinality jX ? 1j.

Therefore, in our approach we solve the problem of ap- proximation error accumulation in the following manner: Whenever an itemset X having more than one immedi- ate superset Y1; Y2; : : : ; Yn, with ?-equal support is encoun- tered, we prune each superset Yk only as long as the sum of the differences between the supports of each pruned superset and X is within ?. While performing this procedure, at any stage, the sum of the differences between the support counts of each pruned superset and X is denoted by X:debt. Also, for each such superset Y , we include Y ?X in a set denoted by X:pruned, which along with X:debt needs to be propa- gated to all unpruned supersets of X due to Theorem 2.1.

For any itemsetX ,X[X:pruned is referred to as its cor- responding g-closed itemset and will have ?-equal support.

In the exact closed itemset case (? = 0), X [ X:pruned would be the closed itemset corresponding to X .

3. Rule Generation  We show that given the frequent g-closed itemsets and their associated supports, it is possible to generate associa- tion rules with approximate supports and confidences. This is stated in the following theorem:  Theorem 3.1 Given the g-closed itemsets and their associ- ated supports, let c? and s? be the estimated confidence and support of a rule X1 ?!X2, and c and s be its actual confidence and support. Then, c? ? ? ? c ? c?=? where ? = (1? ?=minsup); and s?? ? ? s ? s?.

Further, it has been shown earlier [4, 5] that it suffices to consider rules among adjacent frequent closed itemsets in the itemset lattice since other rules can be inferred by transi- tivity. This result carries over to frequent g-closed itemsets.

4. Incorporation in Mining Algorithms  As mentioned in the Introduction, our scheme can be used in one of two ways: (1) As a post-processing step of the mining process, or (2) as an integrated solution. Further, our scheme can be integrated into both levelwise algorithms as well as the more recent two-pass mining algorithms. We chose the classical Apriori and the recently-proposed AR- MOR [3] as representatives of these two classes of algo- rithms. Integration into Apriori yields a new algorithm, g- Apriori and into ARMOR, yields g-ARMOR.

Integrating the g-closed scheme into Apriori is fairly straightforward. However, g-Apriori utilizes a novel tech- nique for generating frequent g-closed itemsets from their generators [1] that avoids the costly additional pass required in the A-Close algorithm [1] for mining frequent closed itemsets. Integrating the g-closed scheme into ARMOR is      non-trivial because the complete supports of candidate item- sets are not available during algorithm execution. Details of integrations are in [2].

5. Performance Study    0 200 400 600 800 1000  % P  ru n  e d  Tolerance Count  Dataset: chess; minsup=80%  Figure 1. Output Size Reduction  0.1     0 200 400 600 800 1000  R e s p o n s e T  im e (  s e c o n d s )  Tolerance Count  Dataset: chess; minsup=80%  Apriori g-Apriori  g-ARMOR  Figure 2. Response Times  We have conducted a detailed study to assess the utility of the g-closed framework in reducing both the output size and the response time of mining operations. Our experiments cover a range of databases and mining workloads includ- ing the real datasets from the UC Irvine Machine Learning Database Repository, the synthetic datasets from the IBM Almaden generator, and the real dataset, BMS-WebView- 1 from Blue Martini Software.

Our experimental results for output size reduction, a sam- ple of which is shown in Figure 1, show that even by al- lowing for a very small tolerance, we produce exponentially fewer rules for most datasets and support specifications than the closed itemsets, which are themselves much fewer than the total number of frequent itemsets. For example, on the chess dataset (Figure 1) for a minimum support threshold of 80%, the percentage of pruned itemsets is only 38% at zero tolerance (closed itemset case). For the same example, at a tolerance count of 50 (corresponding to a maximum er- ror of 1.5% in itemset supports), the percentage of pruned itemsets increases to 90%! Further, the pruning achieved by our scheme is often significant even on sparse datasets.

Our experimental results for measuring response time performance, a sample of which is shown in Figure 2, show that g-Apriori performs significantly better than Apri- ori solely because the frequent g-closed itemsets are much fewer than the frequent itemsets. Finally, g-ARMOR is ob- served to perform over an order of magnitude better than Apriori over all databases and support specifications used in our experimental evaluation.

6. Conclusions  In this paper we proposed the g-closed itemset frame- work in order to manage the information overload produced as the output of frequent itemset mining algorithms. This framework provides an order of magnitude improvement over the earlier closed itemset concept. This is achieved by relaxing the requirement for exact equality between the sup- ports of itemsets and their supersets. Instead, our framework accepts the supports of two itemsets to be equal if their dif- ference is within a user-specified tolerance factor.

The complete details of the issues involved in the de- sign, implementation and evaluation of the g-closed itemset framework and of the g-Apriori and g-ARMOR algorithms are available in the full version of this paper [2].

