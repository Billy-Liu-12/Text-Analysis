On a Generalized Approach to Order-Independent

Abstract?Many extreme-scale scientific applications generate colossal amounts of data that require an increasing number of processors for parallel visualization. Among the three well-known parallel architectures, i.e. sort-first/middle/last, sort-last, which comprises of two stages, i.e. image rendering and composition, is often preferred due to its adaptability to load balancing.

We propose a generalized method, namely, Grouping More and Pairing Less (GMPL), for order-independent image composition in sort-last parallel rendering. GMPL is of two-fold novelty: i) it takes a prime factorization-based approach for processor grouping, which not only obviates the common restriction in existing methods on the total number of processors to fully utilize computing resources, but also breaks down processors to the lowest level with a minimum number of peers in each group to achieve high concurrency and save communication cost; ii) within each group, it employs an improved direct send method to narrow down each processor?s pairing scope to further reduce communication overhead and increase composition efficiency.

The performance superiority of GMPL over existing methods is evaluated through rigorous theoretical analysis and further verified by extensive experimental results on a high-performance visualization cluster.

Keywords?Image composition; parallel visualization; big data

I. INTRODUCTION  The scale of data generated by modern scientific applica- tions is rapidly increasing, ranging from terabyes at present to petabytes or exabytes down the road in the predictable future. Such data, now commonly termed as ?big data?, can be resulted from various sources including simulations, exper- iments, or observations, and must be processed and analyzed in a timely manner for scientific discoveries and inventions.

Among many existing methods for data analytics, visualization has been well recognized and widely used in broad science communities to make sense of the data. However, the sheer volume of today?s scientific data has posed a daunting chal- lenge on traditional visualization technologies, and parallel visualization has proven to be a promising solution to handle data of such scales. As a result, many parallel visualization architectures have emerged in the last decade [15], [16], [7], [13], [1], [19].

As put forth by Molnar et al., according to the time when primitives in the raw data are sorted on their corre- sponding processors, parallel visualization architectures can be classified into three categories, i.e. sort-first, sort-middle, and  sort-last [14]. Although the instances in each category may differ in their particular implementations, they generally share some common features. In sort-first, each primitive in the raw data is assigned to a certain processor, which is responsible for a predivided region of the display screen. This scheme could save network communications between the processors due to the predetermined locality of each primitive, but may result in unbalanced workload among the processors due to the static screen partitioning. In sort-last, primitives are not assigned as strictly. Instead, they are rendered into pixels by the processors they are initially assigned to, and these distributed pixels must be composited into a final image. This scheme may achieve a high level of workload balance at the cost of increased network communications. In sort-middle, the entire visualization process is naturally divided into two distinctive stages: geometry processing and rasterization. Such a partitioning of visualization groups together the operations with similar purposes and distinguishes from others. This scheme may consume more bandwidth and incur a longer processing time, and hence is investigated in more theoretical than practical settings.

Among the above three well-known parallel architectures, sort-last is often preferred in many applications due to its adaptability to load balancing. In general, the sort-last architec- ture comprises of two stages: rendering and composition [20].

However, one prominent issue associated with this scheme is the communication overhead caused by gathering and com- positing partial pixels distributed among processors after the rendering stage. As the data volume increases at an unprece- dented pace, more processors are needed to render larger image sizes in parallel, which aggravates this composition latency issue and inevitably degrades the performance of the entire visualization process. To address this performance limitation, we propose a generalized Grouping More and Pairing Less (GMPL) method for image composition in sort-last parallel rendering. GMPL is of two-fold novelty:  i) It divides the processors into groups based on the prime factors of the total number of processors avail- able in the system. In doing so, we are able to obvi- ate the common power-of-two restriction in existing methods to fully utilize computing resources, and break down the processors to the lowest level with a minimum number of peers in each group to achieve high concurrency and save communication cost.

ii) Within each group, it employs an improved direct send method to narrow down each processor?s pairing scope to further reduce communication overhead and increase composition efficiency.

The performance superiority of the proposed GMPL im- age composition method over existing methods is evaluated through rigorous theoretical analysis and further verified by extensive experimental results on a high-performance visual- ization cluster.



II. RELATED WORK  As one key step in a sort-last parallel rendering system, image composition has received a great deal of attention from many researchers. Existing efforts mainly focus on the following two aspects of research: i) design blending methods to determine how input pixels should be combined, and ii) organize composition workflows for higher composition effi- ciency with less resource consumption.

Blending methods can be divided into two categories: i) order-dependent methods and ii) order-independent methods.

Among the order-dependent methods, the most representative one is the Porter-Duff over operator [18]. In this operator, two input pixels are combined with a ratio that depends on the first input pixel?s ?-channel value, which makes the operator non-commutable and imposes a limitation on the design of potential algorithms. Among the order-independent methods, the most widely used one is the Z-buffer algorithm proposed by Catmull and others [2], [18]. The blending result of this method is only determined by the two input pixels? distances to the view point, i.e. the one closer to the view point is taken as the resultant pixel. It also means that the input order does not affect the blending result, hence facilitating the design of more flexible algorithms.

There also exist a wide range of research efforts in the as- pect of workflow organization for image composition. Among the most traditional ones, the Binary Swap method proposed by Ma et al. divides the entire procedure into several sub- stages according to the number of available processors [12].

At each sub-stage, each processor is paired with its counterpart according to its own processor label and the current sub-stage number. This method evenly distributes the composition work- load among all the participating processors, and minimizes the number of sub-stages. The main issue associated with this method is its constraint on the number of processors being a power of 2.

Lee et al. proposed the Parallel Pipeline method, where all the processors are arranged in a 2-D grid and the en- tire composition procedure is divided into two sub-stages accordingly [9]. At each sub-stage, each processor joins a group of processors sharing one common coordinator in the established 2-D grid coordinate system, and communicates with others in the current group to blend its own pixels.

Different from Binary Swap, this method removes the power- of-2 constraint on the number of processors. However, it introduces more communication cost than Binary Swap when the number of available processors happens to be a power of 2. Eilemann et al. proposed the classical Direct Send method, which can be viewed as a simplified version of the Parallel Pipeline method in 1-D [5].

Lin et al. proposed the Rotate Tiling (RT) method, which integrates the Binary Swap method with the Parallel Pipeline method [11]. By carefully dividing the rendered image of each processor into a certain number of blocks, Rotate Tiling reduces the communication overhead incurred in the entire procedure at the cost of unbalanced workload, which may negatively affect the overall visualization performance.

Yu et al. proposed the 2-3 swap composition method with the intention to generalize the original Binary Swap method [22]. In this generalized method, each processor is ini- tially assigned a different number of pixels for composition. As the process continues, the difference in the number of pixels on each processor may become smaller, and eventually, it reaches a finish point when the number of pixels each processor needs to handle is equal and the pixels on all the processors constitute a complete image. This method removes the constraint on the number of processors, but each participating processor may incur more communications with others and some processors may be left idle in the process. In addition, they also proposed and analyzed several intuitive methods such as Reduced Binary Swap to extend the application scope of Binary Swap.

More recently, Peterka et al. proposed the Radix-k method, in which the number of processors can be factorized ar- bitrarily [17]. Once a certain factorization is selected, the composition procedure is scheduled according to the num- ber of factors and the value of each factor involved in the factorization. This method does not impose any constraint on the number of processors, and has potential to achieve a good performance in terms of communication cost with a carefully selected factorization. However, since this method is intended for order-dependent problems, it raises an issue on how a proper factorization should be selected in the case of a dynamic image arriving order. In general, an exhaustive search of all possible factorizations might be needed for the best performance.

The proposed Grouping More and Pairing Less (GMPL) method provides a generalization framework that encompasses several existing algorithms. GMPL takes a prime factorization- based approach to strategically divide the processors into a set of groups at the finest possible grain and form a well- structured grid to minimize the communication overhead with evenly distributed workload. When the number of processors is a prime or a power of 2, GMPL reduces to an improved version of the Direct Send algorithm applied to all the processors or in each established group.

The rest of the paper is organized as follows. In Section III, we describe the problem of image composition and present the details of our proposed algorithm with a theoretical justifica- tion on its validity. In Sections IV and V, we analyze and compare our algorithm with existing ones through theoretical analysis and real-life experiments, respectively.



III. TECHNICAL APPROACH  We consider a general image composition problem in sort- last parallel rendering that involves N homogeneous proces- sors, denoted as W0,W1, . . . , WN?1, which are intercon- nected through a local switch. Each processor has a locally rendered image of an identical size P without any blank pixels, and needs to blend its own pixels with those corresponding    ones of the same position on all other processors using the Z-depth test (i.e. Z-buffer) method. In this image composition problem, we consider a multi-port communication model such that each processor is able to send and receive messages simultaneously with an equal amount of bandwidth.

A. Algorithm Design  Based on a thorough investigation into the existing methods for image composition, we provide a summary of the obser- vations and rationales in support of our algorithm design:  ? Given N processors, i) if they are arranged in the way of Direct Send, the latency would proportionally relate to the number of processors [22]; ii) if they are arranged in a 2-D grid of f i0?f  i 1, where N = f  i 0?f  i  and f i0, f i 1 ? 1, the latency can be reduced compared  with Direct Send [9]. The difference between these two arrangements lies in the decision on whether or not to factorize the number of nodes. Peterka et al.

further suggested that the N processors be arranged in a multidimensional grid based on the factors of N [17]. As factorization is conducive to the latency performance and can be conducted recursively, one may be able to achieve the highest performance gain by factorizing the number of nodes into prime num- bers, i.e. to the lowest degree.

? If the number N of processors happens to be prime, a grid-based grouping approach through factorization does not work, and therefore these N nodes may have to be arranged as in Direct Send. As Direct Send generally incurs a high latency, more efficient methods are needed to improve its latency performance.

By integrating the above two aspects into algorithm design, we propose a Grouping More and Pairing Less (GMPL) method for order-independent image composition, whose pseu- docode is provided in Algorithm 1, which calls PrimeFactorEn- coding() in Algorithm 2 andImprovedDS() in Algorithm 3. The PrimeFactorEncoding() function is used to calculate the code series of a given processor ID, which comprises of a sequence of coordinates in the hyperspace of the prime factors derived from the number of processors.

Note that the decision version of the prime or integer factorization problem is generally considered within the class of UP (Unambiguous Non-deterministic Polynomial-time) and outside the class of P [3]. However, there exist many special- purpose algorithms including trial division [6] and elliptic curves [8], and general-purpose algorithms including Dixon?s factorization method [4] and continued fraction factoriza- tion [10], which are highly efficient in factorizing very big numbers of dozens of digits. Considering the scale of today?s PC clusters is still quite limited with merely hundreds or thousands of processors on average, prime factorization can be performed online to support real-time operations. For larger numbers, prime factorization can be always done off-line before the visualization begins. The algorithm ImprovedDS is designed to coordinate the processors? sending/receiving and blending workflows to improve the latency performance of the original Direct Send method.

In Algorithm 1, each processor first obtains its code series from the PrimeFactorEncoding() function in line 2. Then, the  composition procedure is divided into k sub-stages, where k is the number of prime factors of the given number N of processors. At each sub-stage, we label N  ?  groups in line  6, where N ?  = N/ft and ft is the t-th factor of N . Here, each ?group? represents a collection of processors, which only communicate with others in the same group to blend their image tiles. In line 7, ?simultaneously" means that all the groups and their member processors are expected to start performing their tasks in parallel at the same time. In line 8, each denoted group ID also obtains its corresponding code series using the PrimeFactorEncoding() function with respect  to N ?. The N processors are then assigned to the N ?  groups as follows: a processor Wi is added to Gj if their corresponding code series (i.e. coordinate sequences) (di0, d  i 1, ? ? ?, d  i k?1) and  (dj0, d j 1, ? ? ?, d  j k?2) satisfy Eq. 1, as shown in line 10:  {  dir = d j r, if 0 ? r ? t? 2;  dir = d j r+1, if t ? r ? k ? 1.

(1)  When the group assignment is completed for all the processors, we call the ImprovedDS algorithm for each group in line 14 to coordinate the processors? communication and blending work- flows within the same group. After executing ImprovedDS, we remove all the group associations established in the current sub-stage in line 16 and move on to the next sub-stage for a regrouping of the processors. After completing these k sub- stages, each processor holds a composited image tile, which is then collected by processor W0 using a Binary-Tree scheme in line 18.

Algorithm 1 GMPL(W0,W1,W2 ? ??,WN?1) Input: N processors with partial images Output: A composited final image on W0  1: for i = 0 to N ? 1 do 2: (di0, d  i 1, d  i 2 ? ??, d  i k?1)=PrimeFactorEncoding(i,N );  3: end for 4: for t = 0 to k ? 1 do 5: Set N  ?  = N/ft; 6: Use G0, G1, ? ? ?, GN ??1 to denote N  ? groups;  7: for all j ? [0, N ?  ? 1] simultaneously do 8: (dj0, d  j 1, d  j 2 ? ??, d  j k?2) = PrimeFactorEncoding(j,N  ?  ); 9: for all i ? [0, N ? 1] do  10: if the coordinate sequences (di0, d i 1, ???, d  i k?1) of Wi  and (dj0, d j 1, ? ? ?, d  j k?2) of Gj satisfy Eq. 1 then  11: Add Wi to Group Gj ; 12: end if 13: end for 14: ImprovedDS(Group Gj , sub-stage number t); 15: end for 16: Remove all group associations; 17: end for 18: Processor W0 collects the composited tiles held by all  other processors Wi (1 ? i ? N ? 1) using a Binary- Tree scheme to compose the final image I;  19: return the final image I on processor W0;  For ImprovedDS in Algorithm 3, given a group Gj con- taining h processors and the sub-stage, i.e. t, where the algorithm is applied, all the h processors are first arranged in an increasing order according to their original processor IDs    Algorithm 2 PrimeFactorEncoding(i,N ) Input: a processor ID i ? [0, N ? 1], the number N of processors Output: A sequence of coordinates of the processor ID i in N ?s prime-factors? hyperspace  1: Prime factorize N such that N = f0 ? f1 ? f2 ? ? ? ? ? fk?1, where the factors are arranged in a descending order;  2: pid = i; 3: for r = k ? 1 to 0 do 4: dir = pid mod fr; 5: pid=pid div fr; 6: end for 7: return (di0, d  i 1, d  i 2, ? ? ?, d  i k?1);  in line 1. Similarly, ?simultaneously? in line 2 means that all the processors are expected to start performing their tasks in parallel at the same time. Each processor also needs to divide the sub-image it currently holds into h equal-sized tiles and label them by It,n(0 ? n ? h ? 1) in line 4 to facilitate the sub-image composition in the following h?1 steps, which are described from lines 5 to 9. Note that each processor executes lines 6, 7, and 8 in parallel. At each step s (0 ? s ? h ? 2) of sub-stage t, processor Wi?  n  sends out the image-tile It,j(s) to processor Wi?  l(s) , where  j(s) =  {  (n ? ?h2 ?+ s) mod h, if 0 ? s ? ? h 2 ?;  (n ? ?h2 ?+ s+ 1) mod h, if ? h 2 ? < s ? h? 2;  (2)  and  l(s) =  {  n+ 1 mod h, if 0 ? s ? ?h2 ? ? 1;  (2h? 2 ? j(s) + 1) mod h, if ?h2 ? ? s ? h? 2.

(3)  In the mean time, processor Wi? n  is also ready to receive  an image-tile labeled as It,b(s) from processor Wi? m(s)  , where  b(s) =  {  (?h2 ? ? T (n) + s) mod h, if 0 ? s ? ? h 2 ?;  ((n+ 1) ? ?h2 ?+ 1) mod h, if ? h 2 ? < s ? h? 2;  (4)  T (n) = (n? 1 + h) mod h, and  m(s) =  ?  ?  ?  n? 1 + h mod h, if 0 ? s < ?h2 ?;  (n? 1 + 2 ? s) mod h, if s = ?h2 ?;  (n+ 1 + 2 ? s) mod h, if ?h2 ? < s ? h? 2.

(5)  To better explain the proposed GMPL method, we present a step-by-step example of 10 processors as shown in Fig. 1.

In this example, the number of processors, i.e. 10, is prime factorized into 5 ? 2, and the composition procedure is thereby divided into 2 sub-stages with 2 and 5 groups, re- spectively. In the first sub-stage, i.e. sub-stage 0, processors W0,W2,W4,W6, and W8 constitute one group and act as W  i ?  ,W  i ?  ,W  i ?  ,W  i ?  , and W  i ?  , respectively, in the application  instance of the ImprovedDS method to their group; processors W1,W3,W5,W7, and W9 constitute another group and act as Wi?0  ,Wi?1 ,Wi?2  ,Wi?3 , and Wi?4  , respectively. Similarly, we can  derive the grouping and each processor?s activities in sub-stage 1 using the proposed algorithms.

Algorithm 3 ImprovedDS(Group Gj of h member processors, sub-stage number t) Input: group Gj of h member processors and their sub-images, number of the sub-stage where the method is applied.

Output: each member processor has its partial image compos- ited with other member processors  1: Denote the h processors in Group Gj as Wi?0 , ? ? ? ,  Wi? n  , ? ? ? ,Wi? h?1  , where i ?  0 ? ? ? ? ? i ?  n ? ? ? ? i ?  h?1;  2: for all h processors simultaneously do 3: Denote the partial image Wi?  n  currently holds as It; 4: Divide It into h equal tiles and label them as It,0, ? ? ?  It,n, ? ? ? It,h?1; 5: for step s = 0 to h? 2 simultaneously do 6: Wi?  n  sends its tile labeled as It,j(s) to Wi? l(s)  , where  j(s) and l(s) are specified by Eqs. 2 and 3, respec- tively;  7: Wi? n  receives a tile labeled as It,b(s) from Wi? m(s)  ,  where b(s) and m(s) are specified by Eqs. 4 and 5, respectively;  8: Wi? n  blends its own tile labeled as It,b(s?1) with the one received at the previous, i.e. s ? 1 step, when s > 0;  9: end for 10: end for  B. Algorithm analysis  We provide a thorough analysis of our proposed GMPL method in this subsection in reference to a composition pro- cedure comprised of k sub-stages.

1) Analysis of ImprovedDS: Considering a processor group Gj of h processors, at sub-stage t of ImprovedDS, we have the following properties:  ? Property 1: At each step s (0 ? s ? h? 2) of group Gj?s composition procedure, each processor Wi?  n  (0 ? n ? h?1) has an exclusive sending destination W  i ?  l(s)  and receiving source Wi? m(s)  within the group.

? Property 2: When the composition within group Gj is finished, processor Wi?  n  (0 ? n ? h ? 1) holds an exclusive tile labeled as It,r(n), into which all the h tiles with the same label in the group have been blended, where  r(n) = ((n+ 1) ? ? h  ?+ 1) mod h. (6)  Property 1 indicates that at any step of ImprovedDS, each processor?s sending or receiving partner has no dependency with any other one performing the same job in the same group, which, together with the multi-port communication model, leads to a high level of parallelism in the communication process of ImprovedDS. Property 2 is used to establish the correctness of its composition procedure.

a) Proof of Property 1:: Without loss of generality, we focus on the sending process, as the proof for the receiving process is similar.

Considering group Gj with h ? 1 steps, we divide these steps into two disjoint sets: [0, ?h  2 ?], and (? h 2 ?, h? 2].

Step 0  Step 1  Step 2  W  0,0 I  2,0 I  3,0 I  4,0 I  Step 3  1,0 I  0,0 I  2,0 I  3,0 I  4,0 I  1,0 I  0,0 I  2,0 I  3,0 I  4,0 I  1,0 I  0,0 I  2,0 I  3,0 I  4,0 I  1,0 I  W  W 6W 1W 3W 5W 7W8W 9W  Step 4  0,0 I  2,0 I  3,0 I  4,0 I  1,0 I  (a) Sub-stage 0  0,1 I  1,1 I  W  W  W 6W1W 3W 5W 7W 8W 9W  (b) Sub-stage 1  Figure 1: Illustration of the proposed GMPL method in the case of 10 processors.

In the first set [0, ?h2 ?], according to the upper part of Eq. 3, which determines the destination of the tile sent from each processor at a specific step s, each processor?s sending destination corresponds one-to-one to the processor?s unique ID in its current group. Therefore, there is no conflict of data sending between any two processors in the same group.

In the second set (?h2 ?, h?2], from the lower part of Eq. 3, we know that given a processor Wi?  n  and its current step s, Wi?  n  ?s sending destination at step s, i.e. W i ?  l(s) , is related to  the following three parameters: n (i.e. the processor?s unique ID in the current group, h (i.e. the number of processors in the group), and s (i.e. the step number in the entire procedure).

Considering that h and s are the common parameters for the entire group at a given step, each processor?s sending destination is only determined by its unique group ID. For clarity, we represent Wi?  n  ?s sending destination at a given step  s, i.e. W i ?  l(s) , as W (n). The conflict-free sending problem at  a given step s can be converted into the following problem: given i, j ? [0, h? 1], if i 6= j, then W (i) 6= W (j).

We use a proof-by-contradiction strategy to prove the above property. Given i, j ? [0, h ? 1], let us assume that i < j and W (i) = W (j). Based on the properties of the mod operation, which dominates the operations in function W (n), we can infer that W (j? i) = 0, i.e. j? i is a cyclic period of W (n). On the other hand, from W (n)?s definition, we know that W (n) naturally has a cyclic period h. For j ? i and h, since 0 ? i, j < h, j > i, it follows that 0 < j ? i < h.

According to a basic theorem in Algebra Theories [21], the following equality involving j ? i and h holds:  c = q ? (j ? i) + p ? h, (7)  where q and p are both integers, and c is the greatest common divisor between j ? i and h.

From Eq. 7, we know that c is also a cyclic period of W (n).

Since h is a prime, c must be 1. It means that W (0) = W (1) = ? ? ? = W (h ? 2) = W (h ? 1), which obviously is incorrect.

For example, W (0) = s ? 1, W (1) = (?h  2 ? + s ? 1) mod h, and W (0) 6= W (1). Therefore, it conflicts with our assumption that W (i) = W (j). Proof ends. ?  b) Proof of Property 2:: The proof of Property 2 is divided into two parts: i) processor Wi?  n  holds a unique tile  when composition is completed, and ii) the obtained tile It,r(n) has blended all the h tiles with the same label.

Combining Eq. 2 and 4, we know that for processor Wi? n  ,  after the h ? 1-step image-tile sending and receiving, all the tiles have been sent out except the ones labeled as It,b(h?2).

After blending them, we obtain the final tile held by Wi?  n  , i.e.

It,r(n), where r(n) = ((n+ 1) ? ? h 2 ?+ 1) mod h.

For the problem of complete composition, we consider the tile labeled as It,r,(0 ? r ? h ? 1), and the proof for the rest can be done in a similar way. According to the upper part of Eq. 4, we know that in steps [0, ?h2 ?], there are ?  h 2 ?  processors receiving tiles labeled as It,r , one tile for each receiving processor. These receiving processors would blend immediately once receiving the It,r-labeled tile. Thus, the number of It,r-labeled tiles in group Gj is reduced from h to h??h  2 ??1. In the following steps (? h 2 ?, h?2], It,r-labeled tiles  are sent to Wi? n  , one tile at each step. When the latter steps are  finished, the number of It,r-labeled tiles in group Gj is further reduced from h??h2 ??1 to h??  h 2 ??1? (h?2??  h 2 ?) = 1,  which is the final composited tile. Proof ends. ?  We would like to point out that the blending operations in ImprovedDS can also be performed in parallel with data communications with necessary synchronization. For example, upon the receival of a tile It,b(s), the processor starts blending, and in the meanwhile, it continues to receive another tile.

Hence, the sending/receiving parts of ImprovedDS overlap with the blending part. With some buffer in place for the purpose of data caching, such parallelization can be further exploited, especially on a homogeneous system.

In addition, compared with the original Direct Send algo- rithm, ImprovedDS results in a great improvement on latency.

For a group containing h processors, since each processor sends/receives with a fixed partner in the first ?h2 ? steps, the connections need to be established only once, instead of ?h2 ? times as in the Direct Send algorithm, hence leading to a significant performance gain, especially when h is large.

2) Analysis of GMPL: In the GMPL method, the activities of different groups are also performed in parallel. At each sub-stage t(0 ? t ? k ? 1), based on the unique code series of each processor and group, the way of group association in GMPL ensures that each processor is associated with only one group. Therefore, all the groups are independent of each other and work in parallel. Ideally, on a homogeneous system, each processor is expected to start and finish stage t at the same time.

The above analysis, together with that in Section III-B1, shows that the proposed GMPL method features completely balanced workload, a high level of resource utilization, and a high degree of parallelism, which, collectively, contribute to an improved latency performance. Furthermore, GMPL removes the power-of-2 constraint and works on an arbitrary number of processors. Particularly, when the number N of processors is a power of 2, GMPL reduces to the traditional Binary Swap method. When N is prime, the GMPL procedure has only one sub-stage where all the processors are in the same group executing the ImprovedDS algorithm directly.



IV. THEORETICAL ANALYSIS AND COMPARISON OF LATENCY PERFORMANCE  We conduct a theoretical analysis and comparison of la- tency performance between the proposed GMPL method and several existing methods including Binary Swap, Direct Send, and Parallel Pipeline. We consider the cost models for both data communication and image blending as follows:  ? Data communication: the time cost of send- ing/receiving a tile is calculated as: ? + n?, where ? is the link delay, ? is transmission time for each pixel, and n is the number of pixels in the tile.

? Image blending: the time cost of each image tile blending is calculated as n?, where ? is the blending time of each pixel, and n is the number of pixels in the tile.

Based on the above cost models, the entire composition time mainly consists of 3 components: connection establishing time Tl due to the link delay, image transfer time Tc, and image blending time Tb. Our performance analysis and comparison are focused on these 3 aspects as well as the total composition time Ta.

A. Binary Swap, Direct Send, and Parallel Pipeline  Binary Swap (BS) is one of the most traditional methods for image composition and has been thoroughly analyzed in the literature with the following three time cost components [12], [22]:  Tl(BS) = ? ? log2 N,  Tc(BS) = ??P ? (1?  N ),  Tb(BS) = ??2 ? P ? (1?  N ),  where N is the number of processors and P is the number of pixels in the image to be composited. The total time Ta(BS) BS takes is:  Ta(BS) = Tl(BS) + Tc(BS) + Tb(BS).

In Direct Send (DS), the three time cost components are [5], [22]:  Tl(DS) = ? ? (N ? 1),  Tc(DS) = ??P ? (1 ?  N ),  Tb(DS) = ??P,  respectively, and the total time cost is:  Ta(DS) = max{Tl + Tc, Tb}. (8)  Parallel Pipeline (PP) arranges the given N processors in a  f i ?  0 ?f i ?  1 mesh architecture, where f i ?  0 and f i ?  1 can be any two  over-1 integers that follow N = f i ?  0 ? f i ?  1 [9]. Given a specific  pair of f i ?  0 and f i ?  1 , the three time cost components of PP are as follows:  Tl(PP ) = ? ?  ?  s=0  (f i ?  s ? 1),  Tc(PP ) = ??P ? (1?  N ),  Tb(PP ) = ??P ? (1?  N ),  respectively, and its total time cost is dependent on the factor- ization result.

B. Grouping More and Pairing Less  In Grouping More and Pairing Less (GMPL), given N processors, if N = f0 ?f1 ?f2 ? ? ? fk?1, the composition process contains k sub-stages. From Algorithms 1 and 3, at each sub- stage t (0 ? t ? k ? 1), the number P (t) of pixels each processor holds when sub-stage t starts is  P (t) =  ?  ?  ?  ?  ?  P, if t = 0; P  f0?f1?f2???ft?1 , if 1 ? t ? k ? 1;  P N , if t = k,  (9)  the number gt of groups in stage t is N ft  , and the number wt of working processors contained in each group is equal to ft.

At sub-stage t, although the processors are assigned to different groups, each processor still works in a similar way.

Each processor needs to simultaneously send to and receive from other processors for wt ? 1 times. For the first ?  wt 2 ?  transfers, each processor sends/receives its tiles to/from the same processor, so the connection needs to be established only once when the first tile is transferred, and the rest ?wt2 ??1 transfers do not incur any extra connection overhead.

Therefore, the connection establishment latency for the first ?wt2 ? transfers is ?.

For the last wt?1?? wt 2 ? transfers, each processor changes  its sending/receiving partners at each step, and hence needs to establish a new connection each time. The connection establishment latency for the last wt ? 1 ? ?  wt 2 ? transfers is  (wt?1?? wt 2 ?)?. By summing up the connection establishment    cost for all wt transfers, we calculate the sending/receiving latency T tl in sub-stage t as:  T tl = ? ? (1 + wt ? 1? ? wt ?)  = ? ? (wt ? ? wt ?)  = ? ? (ft ? ? ft ?).

(10)  Furthermore, the total latency Tl of each processor for all k sub-stages is calculated as:  Tl =  k?1 ?  t=0  T tl  = ? ?  k?1 ?  t=0  (ft ? ? ft ?)  ?  k?1 ?  t=0  ( ft ) ? ?.

(11)  The image transfer time T tc of each processor in sub-stage t is straightforward, i.e.:  T tc = ? ? P (t)  wt ? ?  wt ?+ ? ? (wt ? 1? ?  wt ?) ?  P (t)  wt  = ? ? P (t)  wt ? (wt ? 1? ?  wt ?+ ?  wt ?)  = ? ? P (t)  wt ? (wt ? 1)  = ? ? (P (t) ? P (t)  wt )  = ? ? (P (t) ? P (t)  ft )  = ? ? (P (t)? P (t+ 1)).

(12)  Similarly, the total image transfer time Tc of each processor for all k sub-stages is:  Tc =  k?1 ?  t=0  T tc =  k?1 ?  t=0  ? ? (P (t)? P (t+ 1))  = ? ? (P (0)? P (k) = ? ? P ? (1?  N ).

(13)  The image blending time T tb of a processor at sub-stage t can be derived in a similar way to Eq. 12:  T tb = ? ? (P (t)? P (t+ 1)). (14)  It follows, for the total image composition time Tb of a processor during all the k sub-stages, that:  Tb = ? ? P ? (1?  N ), (15)  which is also derived in a similar way to Eq. 13.

Similar to Direct Send, since the processors in the same group are arranged in a pipeline to overlap the data send- ing/receiving and image blending tasks, the total time T ta a processor spends at sub-stage t is:  T ta = max{T t l + T  t t , T  t b}. (16)  Considering the homogeneity in the computing power, the amount of workload, and the composition workflow, the pro- cessors in the same group enter and leave the sub-stage t at the same time. As the same is true in all the groups, T ta is actually the time that each processor spends in sub-stage t. Similarly, the total time cost Ta for the entire composition procedure is:  Ta =  k?1 ?  t=0  T ta. (17)  As a summary, we provide the above theoretical latency performance analysis results of the image composition meth- ods in comparison in Table I. The comparison of the image transfer and blending time is straightforward, but not the comparison of the connection establishing time, which is in some form of summation over the factors of the processor number N . For an accurate comparison, we introduce a new variable F i. Given one arbitrary factorization of an arbitrary integer N , i.e. N = f i0 ? f  i 1 ? f  i 2 ? ? ? f  i ki?1  , we use F i to denote the following factors summation:  F i =  ki?1 ?  t=0  (f it ? 1).

The connection establishing time of PP can be represented by F i under the constraint that ki can be at most 2. The connection establishing time of DS can be represented by F i  under the constraint that ki must be 1, which also means that f i0 is N itself. The connection establishing time of GMPL can be represented by F i under the constraint that all the factors f ij(0 ? j ? ki ? 1) must be prime.

Based on F i, we are able to compare the connection establishing time of these four methods if we can determine how F i varies with different factorizations. We provide below one observation of such pattern:  Given an arbitrary integer N ?  , if there exists one possible  2-factor factorization N ?  = f i ?  0 ? f i ?  1 , where f i ?  0 ? 1, f i ?  1 ? 1,  then N ?  and F i ?  have the following relationship:  N ?  ? 1? F i ?  = N ?  ? 1?  ?  t=0  (f i ?  t ? 1)  = f i ?  0 ? f i ?  1 ? 1? (f i ?  0 + f i ?  1 ? 2)  = (f i ?  0 ? 1) ? (f i ?  1 ? 1) ? 0.

(18)  The equality in Eq. 18 is satisfied only if N is prime.

Eq. 18 can be further generalized as follows: for an arbi- trary N and its factorization f i, where N = f i0 ?f  i 1 ? ? ? f  i ki?1  , if  there exists f it (0 ? t ? ki?1), which is non-prime and greater  than 1, then f it can be further factorized as f i t = f  i ?  0 ? f i ?  1 . The  newly derived factors f i ?  0 and f i ?  1 , as well as the rest ki ? 1 factors in f i, constitute a new factorization f j of N and we denote its corresponding factor-summation as F j . Then based on Eq. 18, it follows that F i ? F j . If there still exists a non- prime factor in f j , the replacement of the non-prime factor can be recursively performed until all the factors are prime, at which point, the minimum factor-summation F l is achieved.

Methods Connection Establishing Time (Tl) Image Transfer Time (Tc) Image Blending Time (Tb)  BS ? ? log2 N ??P ? (1 ? N ) 2 ? ? ? P ? (1 ? 1  N )  DS ? ? (N ? 1) ??P ? (1 ? 1 N ) ??P ? (1? 1  N )  PP ? ?  ?  t=0  (f i ?  t ? 1) ??P ? (1 ? N ) ??P ? (1? 1  N )  GMPL ? ?  k?1 ?  t=0  (ft ? ? ft ?) ??P ? (1 ? 1  N ) ??P ? (1? 1  N )  Table I: Latency performance analysis of four methods in comparison.

The notation F i is also applicable to BS, where the processor number N is required to be a power of 2, i.e.

N =  log2 N?1 ?  t=0  2, (19)  and its corresponding connection establishing time is calcu- lated as:  ? ?  log2N?1 ?  t=0  (2 ? ?  ?) = ? ? log2 N. (20)  Obviously, Eq. 19 is the prime factorization required by GMPL. The performance superiority of BS over PP and DS confirms what we derive from the notation F i. Based on Eq. 18, we conclude that the connection establishing time of GMPL is less than, or at most equal to, that of PP, DS, and BS.



V. PERFORMANCE EVALUATION  To illustrate the actual composition result and evaluate the performance of GMPL, we implement and test it on a high- performance visualization cluster together with the other five algorithms in comparison including DS, PP, Reduced BS, and BS to support our theoretical analysis, as well as Radix-k to evaluate the effects of the order-dependence restriction. This cluster consists of 1 head node and 16 compute nodes, each of which is equipped with dual 6-core processors at the speed of 2.3 GHz for each core and 64GB RAM, and is connected through a 1GigE switch. The cluster is able to launch 17?12 = 204 MPI jobs in parallel.

We run GMPL and the other algorithms on different num- bers of cores ranging from 16 to 128 at an interval of 16 with different image sizes from 2048? 2048, 3072? 3072, 4096? 4096, to 5120 ? 5120 pixels. To facilitate comparison, the arbitrary factorization of the number N of cores in PP and Radix-k is specified as a factorization into 2 ? N2 and all its prime factors, respectively. Each input image is processed directly by all the composition methods without applying any optimization techniques such as blank pixel elimination and image re-coding.

For each problem instance, we repeat the experiment for 5 times, and plot in Fig. 2 the average of the total latency performance measurements of all the methods in comparison on four different image sizes. From these performance curves, we have the following observations.

1) For each image size, DS does not factorize the num- ber N of processors, PP factorizes N into two factors, Radix-k, GMPL, Reduced BS, and BS factorize N into prime factors (the prime factors are 2?s for Reduced BS and BS). The methods with more thor- ough factorization seem to outperform those with less factorization in terms of the total composition time, which is consistent with our derivation in Eq. 18.

2) When the image size is fixed, as the number of processors increases, the performances of PP and DS degrade quickly as their data transfer time cost is lin- early related to the number of processors. The perfor- mances of BS and GMPL are relatively more stable with the increase of processors as their workload is evenly distributed among the working processors and the data transfer cost logarithmically depends on the number of processors. The performance of Radix-k lies between PP and GMPL, since it takes relatively thorough factorization but retains the restriction of order-dependence. The performance of Reduced BS is unstable as it treats different numbers of processors in different ways. In most cases, Reduced BS takes longer than GMPL to complete the composition, even up to twice of GMPL on 112 processors with an image size of 5120 ? 5120. This observation may be related to the fact that the more pixels one image contains, the longer it takes those processors beyond 2n (n = log2?N?, N is the number of processors) and their partners to transfer and composite images when the number of processors is not a power of 2.

3) When the number of processors is fixed, as the image size increases, the processing time of each method increases correspondingly as expected.

Fig. 3 shows a set of composited images of size 20482  rendered from a brain CT dataset using the proposed GMPL method from different view angles. The composited images of other sizes are qualitatively similar.



VI. CONCLUSIONS AND FUTURE WORK  In this paper, we proposed a Grouping More and Pair- ing Less (GMPL) method for sort-last parallel rendering.

To reduce the image composition time, GMPL employs two approaches: prime factorization and Improved Direct Send.

Prime factorization is considered based on a common pattern that underlies the three most widely used methods: DS, PP and BS, i.e. the more thoroughly the number of processors    16 32 48 64 80 96 112 128      Number of processors  C o  m p  o s it io  n t  im e  ( s e  c )      GMPL Radix?k Reduced BS PP DS BS  (a) Image size of 20482 .

16 32 48 64 80 96 112 128       Number of processors  C o  m p  o s it io  n t  im e  ( s e  c )      GMPL Radix?k Reduced BS PP DS BS  (b) Image size of 30722.

16 32 48 64 80 96 112 128         Number of processors  C o  m p  o s it io  n t  im e  ( s e  c )      GMPL Radix?k Reduced BS PP DS BS  (c) Image size of 40962 .

16 32 48 64 80 96 112 128         Number of processors C  o m  p o  s it io  n t  im e  ( s e  c )      GMPL Radix?k Reduced BS PP DS BS  (d) Image size of 51202.

Figure 2: Latency performance of different methods on different image sizes and numbers of processors.

(a) Front View (b) Back View (c) Left View (d) Right View  Figure 3: Composited brain images of size 20482 using the GMPL method.

is factorized, the more latency performance gain may be achieved. ImprovedDS is considered in the case where the number of processors is already prime. In such a case, we designed a new sending/receiving scheme, which, ideally, is able to reduce the connection establishing time almost by half compared with the original DS method.

Based on rigorous theoretical analysis, we showed that GMPL possesses several salient features such as completely balanced workload, a high level of resource utilization, a high degree of parallelism, and no power-of-2 constraint.

Particularly, when the number N of processors is a power of 2, GMPL reduces to the traditional Binary Swap method, and when N is prime, GMPL executes the ImprovedDS algorithm.

It is of our future interest to explore and integrate var- ious optimization techniques to further improve the image composition performance. For example, removing blank pixels may reduce both transfer time and blending workload. For a comprehensive performance evaluation, we will conduct more experiments of GMPL and the other methods in comparison on large-scale clusters.

