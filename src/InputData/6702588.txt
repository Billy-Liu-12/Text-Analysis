A parallel IRAM algorithm to compute PageRank for modeling epidemic spread

Abstract?The eigenvalue equation intervenes in models of infectious disease propagation and could be used as an ally of vaccination campaigns in the actions carried out by health care organizations. The stochastic model based on PageRank allows to simulate the epidemic spread, where a PageRank-like infection vector is calculated to help establish efficient vaccina- tion strategy. In the context of epidemic spread, generally the damping factor is high. This is because the probability that an infected individual contaminates any other individual through some unusual contact is low. One consequence of this results is that the second largest eigenvalue of PageRank matrix could be very close to its dominant eigenvalue. Another difficulty arises from the growing size of real networks. Handling very big graph becomes a challenge for computing PageRank.

Furthermore, the high damping factor makes many existing algorithms less efficient.

In this paper, we explore the computation methods of PageRank to address these issues. Specifically, we study the implicitly restarted Arnoldi method (IRAM) and discuss some possible improvements over it. We also present a parallel implementation for IRAM, targeting big data and sparse matrices representing scale-free networks (also known as power law networks). The algorithm is tested on a nation wide cluster of clusters Grid5000. Experiments on very large networks such as twitter, yahoo (over 1 billion nodes) are conducted.

Keywords-Epidemic; PageRank; Scale free networks; Power law; IRAM; Big data

I. INTRODUCTION Dynamic complex systems appear in many areas such  as physics, biology, and computer networks etc. In the domain of health research, quick response and effective control of widely spreading health crises stay a big challenge for public health officials as well as scientists. In order to simulate the epidemic spreading, such as H1N1 outbreak in France, traditional models need hundreds of experiments and compute the expected outcome by averaging. In addition, these experiments should be adjusted on a daily basis during the initial outbreak. For example, Network Dynamics and Simulation Science Laboratory (NDSSL) has proposed a parallel simulation model Simdemics [1][2], designed to scale to the entire United states (300 Million people). This solver generates roughly 300 GB of data and is expected to increase as more details is added. One run takes 3000 cpu hours on a 1.5TF machine of 448 cores and one experiment takes 100 to 300 runs. As a result, They could only expect 1 to 4 experiments per year.

To answer urgent requests during the beginning phase of outbreak, an eigenvalue model is proposed in [3]. In this model, a PageRank-like Infection vector is calculated, which could help health officials to decide the relative importance of different agents or groups of agents in a population facing an epidemic. Concerning computational aspect, difficulty for computing PageRank arises from the size of network and from the big damping factor. Due to similar characteristics, this problem is also encountered for other real world applications. In this present paper, we study the computation of PageRank within this context.

PageRank citation ranking was initially introduced in [4] to bring order to the Web. A page has high rank if the sum of the ranks of its inlinks is high. In other words, rank is propagated through links. To use mathematical formalism, we look for a PageRank vector x, which is the dominant eigenvector of the Google matrix,  A = ?P + (1? ?)veT , 0 ? ? < 1 (1) where the matrix P is a column stochastic matrix, called transition matrix, representing the outlink structure of the Web, e is the vector (1, ..., 1)T , ? is called the damping factor, and the vector v is the teleportation vector, which ensures the uniqueness of the PageRank vector. A difficulty in PageRank model is caused by the existence of dangling nodes [5]. These nodes will result in one or more columns of zeros in transition matrix P . Several ideas have been proposed to deal with this problem [6][7]. Research by the initial PageRank paper [4] indicates that the PageRank could be calculated by removing the links to dangling pages from the web network. However, theoretically this process might generate new dangling pages and iteratively remove all pages from the network. We simply add an artificial loop with probability 1 to these nodes themselves. By this way, diagonal elements corresponding to dangling nodes in matrix P are filled with 1. This handling can be justified by similar arguments as showed in [7].

Many algorithms [8][9][10][11] have been proposed for computing PageRank, a survey can be found here [12].

A PageRank is the eigenvector associated to dominant eigenvalue of the Google matrix. However, in real world applications, the computation of PageRank has two chal- lenging aspects. First, the matrices involved are very large  2013 25th International Symposium on Computer Architecture and High Performance Computing  DOI 10.1109/SBAC-PAD.2013.2     and relies on a sparse matrix-vector product (MVP) kernel.

Suppose z is a vector of p-norm 1, Az can be written as ?Pz + (1 ? ?)v(eT z) where eT z is a scalar. So the MVP of A is expressed as MVP of a sparse matrix P plus a vector. Otherwise, any direct computation using A is bottlenecked by memory on large networks. In fact, the Google matrix A becomes a dense matrix due to the part (1 ? ?)veT . For above reason, algorithms based on MVP might be advantageous. Secondly, the damping factor ? is generally very high. For example, in the model of epidemic spread, the virus has the probability 1?? to jump randomly from an infected individual to any other individual through some unusual contact. Intuitively, this event happens rarely and for disease spread ? must be very close to 1. This is in fact an argument in favor of using Arnoldi-type methods, as opposed to the power method.

In this paper, we justify the choice of implicitly restarted Arnoldi method (IRAM) [13] in PageRank computation.

We discuss some improvements over it to address the two difficulties stated in the previous paragraph. The model of parallelization used is so general that it could be adapted for any modern (possibly future) parallel architecture. Our numerical results show that: (i) the strategies proposed could accelerate the convergence of IRAM for matrices derived from real applications; (ii) the PageRank-like infection vec- tor could be used as an ally of vaccination campaigns in the actions carried out by health care organizations.

The remainder of this paper is organized as follows. In section 2, we briefly discuss how to use PageRank in models of epidemic spread, which explains the motivation for the present work. In section 3, we justify the use of IRAM as computation method for PageRank. In section 4, we discuss some improvements for accelerating IRAM?s convergence.

In section 5, we present a parallel IRAM implementation, targeting very large and scale-free real networks. Section 6 is devoted to numerical experiments. Finally, future work along with the conclusion are discussed in section 7.



II. MODELING OF EPIDEMIC SPREAD  The common concept between PageRank model and the epidemic model is the random walk. The formulation of epidemic spread can also be expressed as (1). Here, the matrix P is derived from social networks. A social network might be a directed network in the context of epidemic spread. For example, blood disease could only happen from donators to acceptors. As a result, the matrix P might be a non-symmetric matrix. According to (1), a virus has a small probability (1 ? ?) to jump from any individual to any other individual in a social network. This would happen, for example, when an infected person (virus carrier) meets someone outside his normal contacts. Because of the scale-free nature of social networks, entries of v should be proportional to individuals? number of contacts. Individuals with higher rank in PageRank-like infection vector may have  bigger importance in the spread of epidemics. Therefore, vaccination strategies can be established accordingly. Further insight is given by numerical experiments in the section VI.



III. COMPUTATION METHOD  An efficient solution to a very large sparse eigenvalue problem strongly depends on the proper choice of iterative methods. Our first objective is to choose the best method to calculate the dominant eigenvector in this context.

A lot of research found that the damping factor ? strongly affects the convergence of iterative methods [9][14]. So another special attention has been paid to investigate how the convergence of the proposed algorithm is influenced by this degree of teleportation.

Krylov subspace method allows approximation of an eigenpair (?, x) of A by a Ritz-elements pair (? ? C, x(m) ? Km) where the subspace Km is defined by  Km = Span(x0, Ax0, ..., A m?1x0) (2)  This method approximates k eigenpairs of A by those of a matrix of order m, where k ? m ? n. This matrix, designated by Hm is obtained by orthogonal projection of A onto an m-dimensional subspace Km. Let Wm be the matrix whose column w1, w2, ..., wm constitute an orthogonal basis of Km. The problem is to find ? ? C and y ? Cm such that  (Hm ? ?I)y = 0 (3) where the matrix Hm of dimension m ?m, is defined by Hm = W  ? mAWm. Note that W  ? m is the transpose conjugate  of Wm, ? is considered as an approximated eigenvalue of A and Wmy as its associated approximated eigenvector.

Therefore, some eigenvalues of A can be approximated by the eigenvalues of the matrix Hm. Solving (3) is relatively easy thanks to the Hessenberg structure of matrix. The basic Arnoldi procedure with the orthogonalization refinement [15] is described in [16]. Special attention should be paid to conducting MVP for Google matrix A. Yet a disturbing aspect of Arnoldi method is that the number of iterations (the size of the subspace m) needed to compute the desired eigenpair is unknown, except with a very fortunate choice of starting vector w0. The basic algorithm increases m until the dominant eigenvalues of A are found. For storage, in addition to A, the method keeps m vectors of length n and an m?m Hessenberg matrix, which gives O(nm+m2/2). For computation complexity, matrix-vector product costs about (2m?nnz) operations, where nnz is the number of nonzero elements in A. The modified Gram-Schmidt procedure costs O(m2n) operations. Since the size n of a real network may attain millions or even billions of nodes, increasing m causes both storage and computational overhead.

One way to avoid this difficulty is by restarting tech- niques. The idea is to restart the iteration with a vector that has been preconditioned so that it is more nearly in a k- dimensional invariant subspace which contains the dominant     eigenvector. As stated before, the most consuming part in Arnoldi procedure is the MVP due to the very large size of the matrix. Implicitly restarted Arnoldi method (IRAM) [17][13] provides a means to reduce the number of MVP needed by Arnoldi procedure from m to r = m ? k, with (m? k) the number of shifts used in QR iterations. IRAM allows to compute a few eigenvalues (k ? m) such as those of the largest real part or the largest magnitude.



IV. IMPROVE THE CONVERGENCE OF IRAM FOR PAGERANK COMPUTATION  For PageRank computation in real world applications, IRAM should not be used naively. Due to the very large scale of problem, subspace size m must be small to maintain the orthogonal basis Wm in memory. It is known that the eigen- information of interest may not appear when m is too small [17]. In addition, high damping factor results in clustered eigenvalues around the dominant one [14], which will slow down the convergence of IRAM even further. By following, we explore two improvements over IRAM to address these difficulties.

A. By multiple subspace sizes  In IRAM, only the initial vector is used to improve the quality of the subspace during iterations. The authors of [20] propose to take into account the size of subspace as well. The idea is based upon the projection of the problem on several Krylov subspaces instead of a single one.

This approach, called implicitly restarted Arnoldi method with multiple subspaces (IRAMMS), makes use of Arnoldi method to compute the Ritz elements of a large matrix A in a set of l nested Krylov subspaces. If the accuracy of the desired Ritz elements calculated in none of these subspaces is satisfactory, IRAMMS selects the best of these subspaces. This subspace is one that contains the ?best? current Ritz elements. Then a QR shifted algorithm will be applied to the mbest ?mbest matrix which represents A in this mbest?size projection subspace. As these are the non desired eigenvalues which are chosen for shifts, the leading submatrix issued from QR algorithm concentrates the infor- mation corresponding to the desired eigenvalues. IRAMMS completes Arnoldi projections on nested Krylov subspaces starting with this submatrix whose size is the number of wanted eigenvalues. This method can be considered as an IRAM with the largest subspace size which makes use of eigen-information of some of its nested subspaces in order to update its restarting vector. The interest of the approach is that the additional information obtained by multiple subspaces allows to take advantage of the appearance of the eigen-information of interest due to the larger subspace- sizes as well as the orthogonality due to the smaller subspace sizes. Consequently, the convergence properties of IRAMMS has to be better than that of IRAM.

B. By varying the number of shifts used  This subsection deals with the problem of clustered eigen- values caused by high damping factor. An unfortunate choice of k has inadvertently led to an attempt to select the wanted eigenvalues from a tight cluster. This is something that the polynomial filters simply cannot do. As a result, we may encounter stagnation of IRAM in this situation. A simple strategy is to increase k when stagnation occurs. The idea is to compute these clustered eigenvalues as a whole instead of picking several up among them. However, this solution may not be useful when the cluster is big because k is limited by the size of subspace m (k ? m), which must be choosen small in real applications.



V. PARALLELIZATION  In this section, we begin by introducing a matrix parti- tioning scheme and a load balancing strategy to deal with the problem of big data size and scale-free network structure respectively. Then, we present the parallelization of IRAM as well as the programming model used.

A. Matrix Partitioning  Let us consider the domain decomposition of the problem.

Suppose nodes in a n-sized network are represented by the set V = {1, ..., n} and its Google matrix is A. Remember that the network is very large and of scale-free structure.

IRAM is composed mainly of sparse MVP. The vectors forming orthogonal basis in the computation should be stored in parallel among p processors, because each of them could be larger than any single processor could handle.

For example, take n = 1, 000, 000, 000 for a network, the corresponding vector wj contains 109 entries. Computing spectrum of Hm introduces complex arithmetics in compu- tation. As a result, complex vector wj could take as mush as 16?109 bytes ? 15 GB of memory in double precision. This issue of storage requirement is worsened when the subspace size m is relatively important. In our implementation, the column-wise block-striped decomposition is used.

For this decomposition strategy, each processor is re- sponsible for ?n/p? columns in A and elements of wj .

The initial multiplication complexity is O((n/p) ? nz col), where nz col is the maximum number of nonzero elements in columns. After computation, an all-to-all communication is needed for each process to send its block i of partial results to processor i. At this point, every processor i has p blocks of partial results corresponding to its portion of the vector Awj . The following addition complexity of this step is O((n/p) ? p) = O(n). Thus the total computa- tional complexity is O((n/p) ? nz col). During an all-to- all exchange, every process sends p ? 1 messages, and the total number of elements passed is less or equal to nz col. So the communication complexity of this algorithm is O(p + nz col). Furthermore, each processor needs only nz col extra storage for partial results.

B. Load Balancing  To balance the workload, we take a data-centric partition- ing approach to handle the power law degree distribution in scale-free networks. The objectives are twofold: first, each processor should have at most ?n/p? columns, where n is the number of columns and p is the number of processors; second, each processor should have roughly equal number of nonzero elements. This results in a NP-hard graph partition- ing problem. We propose to use a simple heuristic method to perform load balancing.

Suppose there is p processors. We begin by sorting the columns according to its number of nonzero elements. Then from dense to sparse we attribute the column i to processor i(i = 1, ..., p). After that the rest sorted columns should be attributed one by one to the processor with the least number of nonzero elements each time. Another constraint is when a processor has ?n/p? columns, it should not be considered for attribution any more. However, with this partitioning scheme the columns in each processor are usually not contiguous, which will generate complex communication pattern while doing sparse MVP. A simple solution is by renumbering nodes so as to make the group of columns in the same processor contiguous. The load balancing procedure above is equivalent to symmetrically permuting rows and columns of A. In other words, we construct a new matrix B = T?1AT , where T is the product of successive permutation matrices: T = (T1 ? T2 ? ...). Then  Bu = ?u? T?1ATu = ?u? A(Tu) = ?(Tu), (4) so that A and B have the same eigenvalues, and if u is an eigenvector of B, then x = Tu is an eigenvector of A.

C. Parallel IRAM  IRAM consists of four main tasks. First, the projection phase manipulates the n-sized data sets for sparse MVP. The second phase which includes implicitly shifted QR iterations acts on m-sized data sets. The third phase constructing the r additional steps of Arnoldi factorization manipulates on n-sized data sets for sparse MVP as well. At last the convergence test deals with n-sized data sets to calculate ?fm?. Because phase one and three constitute the most expensive part of the algorithm, we propose to distribute them between processors and to run phases two and four redundantly on all processors.

We use message passing as the global programming model. This model is assuming that the system has a number of processors that have local memories and communicate with each other by means of memory transfer from one processor to another [21].



VI. EXPERIMENTS  Considering the preferential attachment of scale-free net- works [22], we choose v in (1) to be proportional to nodes?  Table I HARDWARE DETAILS OF CLUSTERS  Name of Cluster CPU Network Memory Taurus Intel Xeon Gigabit Ethernet 32 GB Graphene Intel Xeon X3440 Gigabit Ethernet 16 GB  degree and normalizes it by ?1-norm? in all of our experi- ments. The numerical library Lapack is used for conducting implicitly shifted QR iterations. In subsection VI-D, experi- ments of multiple subspace sizes are implemented in MatLab because the objective is to show numerical improvement over the classic method and the results should be valid for sequential or parallel implementation.

Grid5000 platform.

We run our experiments on a nation wide cluster of  clusters Grid5000. Grid5000 is a scientific instrument for the study of large scale parallel and distributed systems.

It provides a highly reconfigurable, controllable and moni- torable experimental platform to its users. The infrastructure of Grid?5000 is geographically distributed on different sites hosting the instrument, initially 9 sites in France (10 since 2011). Porto Alegre, Brazil is now officially becoming the first site abroad. We conduct our experiments mainly on two clusters (some hardware details are given in Table I): - Cluster ?Taurus?: 16 nodes?2 cpus per node?6 cores per cpu=192 cores.

- Cluster ?Graphene?: 144 nodes?1 cpus per node?4 cores per cpu=576 cores.

Test data.

- ba: This real network is collected at the Oregon router views [23].

- stanford, com-Youtube, Wiki-Talk, soc-LiveJournal1: These networks are collected from Stanford Large Network Dataset Collection [24].

- twitter: This network is collected from 467 million Twitter posts from 20 million users covering a 7 month period from June 1 2009 to December 31 2009 [25]. This dataset is relatively more realistic to represent a social network.

- yahoo: This dataset contains URLs and hyperlinks for over 1.4 billion public web pages indexed by the Yahoo!

AltaVista search engine in 2002. The dataset encodes the network or map of links among web pages, where nodes in the network are URLs.

The statistics for the above datasets are presented in Table II. n is the number of nodes, nnz is the number of links.

The number of links in the table is bigger than that in initial datasets because we add links for dangling nodes.

A. Comparison with other methods In the first place, we compare the behaviour of residual  norm ? Ax? x ? of IRAM with that of power method. The idea of Power method is to write the initial vector x0 as a linear combination of  ?n j=1 ?jvj , where vj are eigenvectors     Table II STATISTICS FOR DATASETS  Name n nnz Storage ba 7010 13985 117 KB  stanford 281,903 2,321,669 30 MB com-Youtube 1,134,890 2,988,374 38.7MB  Wiki-Talk 2,394,385 5,21,410 66.5MB soc-LiveJournal1 4,847,571 68,993,773 1.1GB  twitter 41,652,230 1,469,914,131 25 GB yahoo 1,413,511,394 8,050,112,173 78 GB  1e-08  1e-07  1e-06  1e-05  0.0001  0.001  0.01  0.1      0  20  40  60  80  100  120  re si  du al  n or  m  iteration  Power IRAM: m=4 r=3  Figure 1. Convergence behavior for the 281, 903 ? 281, 903 stanford matrix, ? = 0.85  of A. Without loss of generality, suppose ?1 is the dominant eigenvalue, we have:  xk = Axk?1 = A2xk?2 = ... = Akx0  = Ak n?  j=1  ?jvj = n?  j=1  ?jA kvj =  n?  j=1  ?kj?jvj  = ?k1(?1v1 + n?  j=2  (?j/?1) k?jvj)  (5)  For j > 1, |?j/?1| < 1, so that (?j/?1)k ? 0, leaving only the pagerank eigenvector v1. We choose the damping factor to be 0.85 and the tolerance value to be 1E ? 7 for both methods. The initial vector is taken as the vector e = (1, 1, ..., 1)T . Within IRAM, the size of Krylov subspace is 4 and the number of shifts used is 3. The result is presented in Fig. 1. This figure shows that residual in IRAM decreases fast even in the first iteration while power method has big residual norms during the beginning iterations.

Noticed that a power iteration is extremely cheaper com- putationally than an IRAM iteration. To avoid confusion, in  Table III NUMBER OF MATRIX VECTOR PRODUCTS FOR THE 281, 903? 281, 903  stanford NETWORK  ? Power m=4 m=8 m=16 0.85 122 79 71 61 0.90 184 124 99 91 0.95 367 238 148 136 0.99 1775 394 358 316             4  6  8  10  12  14  16  18  20  E xe  cu tio  n tim  e in  s ec  on ds  Number of processors  alpha=0.85 alpha=0.90 alpha=0.95  Figure 2. Scalability experiment for soc-LiveJournal1 network, where m = 3 and r = 3.

Table III, we compare the number of matrix vector products used in both methods, where ? is the damping factor, m is the size of Krylov subspace. We choose the number of shifts to be m ? 1. The tolerance value used is 1E ? 8 and the initial vector is taken as the vector e. This table shows that the number of matrix vector products used in IRAM are less than that of power method.

From the two tests above, we could conclude that IRAM has a faster convergence than power method for the testing matrix. Also, some experiments using explicitly restarted Arnoldi method on this network have been given in [8].

B. Experiments on damping factor ? We study the influence of damping factor on convergence  rate. For stanford network, this dependency is quantified in Table III. It is found that with bigger damping factor ?, more iterations are needed to reach the accuracy for both methods. However, IRAM has a much better performance than Power method for bigger ?. As explained in [14], bigger ? engenders a closer-to-1 second largest eigenvalue.

This fact also favors Arnoldi-type methods, as opposed to the power method.

C. Experiments on load balancing and scalability We have performed scalability experiments on soc-  LiveJournal1 network with damping factor ? = 0.85, 0.90 and 0.95. We run our experiments on cluster ?Taurus? using up to 20 cores. It is known from the previous experiment that higher damping factor necessitates more iterations, thus more computation is needed. As shown in the Fig. 2, our parallel IRAM algorithm is scalable to handle Google matrix derived from scale free network soc-LiveJournal1. This shows that the proposed load balancing strategy is successful to handle matrices of this particular structure. For small damping factor, our parallel algorithm could obtain up to 4? acceleration with many cores.

D. Experiments on multiple subspace sizes  For our experiments here, we compare the convergence of IRAMMS with IRAM. Objective is to investigate whether     0 5 10 15 20 25 30  ?18  ?16  ?14  ?12  ?10  ?8  ?6  ?4  ?2  Implicitly Restarted Arnoldi Method  iteration  re si  du al  n or  m /n  or m  f     IRAM?MAX IRAMMS  Figure 3. IRAMMS(5, 8, 11, 14, 17, 20) versus IRAM-MAX(20) with com-Youtube matrix, where ? = 0.85, k = 4 and the tolerance ? = 10?14.

0 100 200 300 400 500  ?14  ?12  ?10  ?8  ?6  ?4  ?2  Implicitly Restarted Arnoldi Method  iteration  re si  du al  n or  m /n  or m  f     IRAM?MAX IRAMMS  Figure 4. IRAMMS(5, 8, 11, 14, 17, 20) versus IRAM-MAX(20) with Wiki-Talk matrix, where ? = 0.85, k = 4 and the tolerance ? = 10?12.

multiple nested subspaces could be used to accelerate the convergence of IRAM when m is small in real applications.

We show graphically in Fig. 3 and Fig. 4 the norm of residual as a function of iteration number to reach conver- gence using IRAMMS and IRAM. We see that there is no convergence for IRAM in Fig. 4 for Wiki-Talk network while the method converges with multiple subspaces. Moreover, the convergence of IRAMMS in Fig. 3 for com-Youtube network is better than IRAM as well.

Due to the limitation on subspace size for large scale applications, the implicitly restarted Arnoldi method may not be efficient for computing the dominant eigenvector for such large sparse non-Hermitian matrices. Making use of several nested Krylov subspaces could help to improve the convergence as shown in our experiments. Furthermore, the number of MVP in IRAMMS is decided by the largest subspace size because other subspaces are nested within this one. As a result, IRAMMS(m1, ? ? ? ,ml) accelerates the convergence of IRAM(ml) with the same number of matrix- vector products in each iteration.

1e-08  1e-07  1e-06  1e-05  0.0001  0.001  0.01  0.1   0  10  20  30  40  50  60  re si  du al  n or  m  iteration  m=4,r=1 m=4,r=2 m=4,r=3  1e-08  1e-07  1e-06  1e-05  0.0001  0.001  0.01  0.1   0  500  1000  1500  2000  2500  3000  3500  re si  du al  n or  m  time (seconds)  m=4,r=1 m=4,r=2 m=4,r=3  Figure 5. Convergence experiments for different number of shifts on twitter network, where ? = 0.85.

E. Experiments on varying number of shifts For our experiments here, we study the impact of number  of shifts r on convergence. The damping factor ? is fixed to 0.85.

In the test on twitter network, we set the m to be 4 and we change the value of r to 1, 2 and 3. We run our experiments on cluster ?Taurus? using 32 cores. The results are presented in Fig. 5. We notice from the figure that the value of r could influence the convergence rate for the dominant eigenpair. For twitter network specifically, parameters (m = 4, r = 2) allow the fastest convergence for the problem. Similar experiments are conducted for yahoo network on ?Graphene? cluster. The results are presented in Fig. 6. Parameter configurations (m = 8, r = 1) and (m = 8, r = 2) have almost the same convergence rate, while (m = 8, r = 3) setting converges much slower.

To conclude, even if the objective is to find the dominant eigenvector, the number of ?wanted? eigenvalues for IRAM could be taken more than 1 to accelerate dominant eigen- pair?s convergence. In other words, the number of shifts r could be different from m? 1.

F. Vaccination strategies based on PageRank  In this experiment, we use a small network ba to sim- ulate the real world epidemic spread with distribution of vaccination. We consider people who receive vaccination being permanently immunized against viruses. For larger network, parallelization will be needed due to the memory     1e-10  1e-09  1e-08  1e-07  1e-06  1e-05  0.0001  0.001  0.01  0.1   0  5  10  15  20  25  30  35  re si  du al  n or  m  iteration  m=8,r=1 m=8,r=2 m=8,r=3  1e-10  1e-09  1e-08  1e-07  1e-06  1e-05  0.0001  0.001  0.01  0.1   0  10000  20000  30000  40000  50000  60000  re si  du al  n or  m  time (seconds)  m=8,r=1 m=8,r=2 m=8,r=3  Figure 6. Convergence experiments for different number of shifts on yahoo network, where ? = 0.85.

and computation requirement but the implementation of such parallel simulator is not the objective of the test.

We assume a universal infection rate ?, a jumping rate 1 ? ? (damping factor) and a curing rate ? for every individual. Before each simulation, we randomly choose a set of infected individuals. Then the propagation of virus proceeds by time step. During each time step, an infected individual infects each of its neighbours with probability ?. And this infected individual also passes the disease to another random chosen non-neighbour by probability 1? ?. Additionally, every infected individual is cured with probability ?. The result is the average over 10 runs and it is presented in Fig. 7. Here, we compare three cases.

0  20  40  60  80  100  120  140  N um  be r  of in  fe ct  ed in  di vi  du al  s  Time  Simulation without vaccination Random distribution of vaccination  Distribution of vaccination using our model  Figure 7. Time series of infection in an 7010-node power-law social network ba, with ? = 0.85, ? = 0.2 and ? = 0.24  First of all, without distribution of vaccination, we try to give the worst case for time evolution of infection. Sec- ondly, with random distribution of vaccination, we begin the simulation by distributing vaccination to a random chosen group of individuals. Then, we simulate time evolution of infection. Thirdly, with distribution of vaccination using the PageRank-like vector, we calculate the infection vector for the underlying social network and then distribute vaccination to individuals with big ranking in the vector.

The figure verifies the absence of epidemic threshold in scale-free networks [26]. Without interventions, the epi- demic will always enter an endemic state. The second curve, in top-down order from the figure, shows that random distribution of vaccination could not prevent the virus from entering the endemic state. However, distributing vaccination to individuals with big ranking in the PageRank-like vector makes the epidemic die out quickly. This simple experiment confirms the important implication of infection vector for the control of epidemic spread.



VII. CONCLUSION  Modeling of epidemic spread benefits a lot from network research to understand infection evolution in a population.

PageRank-like model could give insight for understanding the impact of social network structure on propagation of virus and could possibly help identifying individuals most likely to spread the disease. Besides, parallelization makes the model computationally adavantageous over Monte Carlo simulation. Numerical results obtained are quite promising.

We demonstrate that PageRank can be computed using numerical methods based on sparse MVP and propose to use a parallel implicitly restarted Arnoldi method. The proposed parallel algorithm takes into account the scale-free structure of the underlying networks and is scalable to handle memory and computation issues arising from very large networks such as twitter and yahoo network. Additionally, it is found in Experiment VI-D and VI-E that the subspace size and the number of shifts used in IRAM could help to accelerate the convergence of method even under constraints caused by storage.

For future work, we intend to expand our epidemic model by including various indicators of epidemic spread, such as characteristics of individuals as well as that of viruses, spreading timestamps, etc. Moreover, we intend to investigate the behaviour of other parallel method based on IRAM [27] within the context of PageRank.

