Finding Robust Itemsets Under Subsampling

Abstract?Mining frequent patterns is plagued by the prob- lem of pattern explosion making pattern reduction techniques a key challenge in pattern mining. In this paper we propose a novel theoretical framework for pattern reduction. We do this by measuring the robustness of a property of an itemset such as closedness or non-derivability. The robustness of a property is the probability that this property holds on random subsets of the original data. We study four properties: closed, free, non- derivable and totally shattered itemsets, demonstrating how we can compute the robustness analytically without actually sampling the data. Our concept of robustness has many advantages: Unlike statistical approaches for reducing patterns, we do not assume a null hypothesis or any noise model and the patterns reported are simply a subset of all patterns with this property as opposed to approximate patterns for which the property does not really hold. If the underlying property is monotonic, then the measure is also monotonic, allowing us to efficiently mine robust itemsets. We further derive a parameter-free technique for ranking itemsets that can be used for top-k approaches. Our experiments demonstrate that we can successfully use the robustness measure to reduce the number of patterns and that ranking yields interesting itemsets.

Keywords-pattern reduction; robust itemsets; closed itemsets; free itemsets; non-derivable itemsets; totally shattered itemsets

I. INTRODUCTION  Frequent itemset mining was first introduced in the con- text of market basket analysis [1] and has been used since to address many data mining problems such as frequent pat- tern mining, association rule generation [2], clustering [3], classification [4], temporal data mining [5] and outlier detection [6]. The mining of itemsets is a core step in these methods that often dominates the overall complexity of the problem. The number of frequent itemsets can be extremely large even for moderately sized datasets complicating a manual analysis or further automated processing steps.

Researchers have proposed many solutions to reduce the number of patterns reported depending on the context in which the patterns are used or the process in which the data was generated, for example closed itemsets [7] to avoid redundant association rules, constrained itemsets [8] to incorporate prior knowledge, condensed representations [9] to answer frequency queries with limited memory, margin- closed itemsets [5] for exploratory analysis, and surprising  itemsets [10], [11] or top-k patterns [12] for itemset ranking.

Many of reduction techniques have a drawback of being  fragile. For example, given a non-closed itemset X , adding a single transaction to dataset containing only X will make X closed. In this paper we introduce a novel theoretical framework that uses this drawback to its advantage. Given a property of an itemset (closedness or non-derivability, for example) we can measure robustness of this property.

A property of X is robust if it holds for many datasets subsampled from the original data. We demonstrate that we can compute this measure analytically for several important classes of itemsets: closed [7], free [13], non-derivable [14], and totally shattered itemsets [15]. Computing robust itemsets under subsampling turns out to be practical for free, non-derivable, and totally shattered itemsets, for closed itemsets the test for robustness is prohibitively expensive.

A possible drawback of our approach is that it depends on a parameter ?, the probability of including a transaction in a subsample. In addition to providing reasonable guidelines to choose ? we introduce a technique making us independent of ?. We show that there is a neighborhood near 1 in which the ranking of itemsets does not depend on ?. We further demonstrate how we can compute this ranking without actually discovering the exact neighborhood or computing the measure for the itemsets. We give exact solutions for free and totally shattered itemsets and provide practical heuristics for closed and non-derivable itemsets.

In the remainder of this paper we describe related work and motivate our approach in Section II. Itemsets robust under subsampling and algorithms to find them are described in Section III. Section V demonstrates how the subsam- pling approach can reduce the number of reported itemsets significantly. The results are discussed in comparison with approximate itemsets in Sections VI.



II. RELATED WORK AND MOTIVATION  The design goal of condensed representations [9] of frequent itemsets is to be able to answer all possible fre- quency queries (approximately). For example, non-derivable itemsets [14] exclude any itemset whose support that can be derived from others in the condensed representation using logical rules exactly or approximately.

DOI 10.1109/ICDM.2011.69     This is useful to support further mining tasks such as generation of association rules where the frequencies of all subsets of a (closed) itemset are needed to determine the confidence of all possible rules. For other tasks knowing the frequency of all frequent itemsets is less useful because there is a large redundancy in the set of frequent itemsets. All frequent itemsets can be grouped into equivalence classes where all itemsets in a class are observed in the same set of transactions. The maximal element of each equivalence class is a closed itemset [7]. No more items can be added to this set without losing some transactions. The minimal elements of the equivalence class are free itemsets [13] or generators.

No items can be taken out without adding transactions.

However, even the number of closed and free itemsets can still be very large for low minimum support thresholds. It can be further reduced by clustering itemsets representing similar sets of transactions [16], enforcing itemsets to have a minimum margin of difference in support [5], or ranking itemsets by significance [10], [11], [17], [18].

The above approaches have in common that the complete dataset is considered and no assumption on potential noise are made. In fault tolerant approaches the strict definition of support, requiring all items of an itemset to be present in a transaction is relaxed, see [19]?[22], assuming that items can present or absent at random in the transactions.

These approaches can reveal important structures in noisy data that might otherwise get lost in a huge amount of fragmented patterns. One needs to be aware though that they report approximate support values and possibly list itemsets that are not observed as such in the collection at all or with much smaller support. Also the design goal is not to reduce the number of reported patterns, only [23] considers closedness in combination with fault tolerance.

Unlike the approaches based on significance [10], [11], [17], [18], we do not assume a statistical null hypothesis.

We also do not assume any noise model, such as flipping the values of a matrix independently. Instead our goal is to study robustness of a given property based on subsampling transactions.



III. ROBUST ITEMSETS A. Notation and definitions  In this section we review the preliminaries and introduce the notation used in the paper.

A binary dataset D is a multiset of transactions, binary vectors of length K. The ith element of a transaction represents by an item ai, a Bernoulli random variable. We denote the collection of all items by A = {a1, . . . , aK}.

An itemset X is a subset of A. Given a transaction t and an itemset X , we define tX to be the binary vector obtained by keeping only the items occurring in X .

Given an itemset X = (x1, . . . , xN ) and a binary vector v of length N , we define the support  sp(X = v;D) = |{t ? D | tX = v}|  to be the number of transactions in D, where items in X obtain values of v. We often omit D from notation, when it is clear from the context. In addition, if v contains only 1s, we simply write sp(X). Note that sp(X) coincides with the traditional definition of a support for X . Discovering frequent itemsets, that is, itemsets whose support exceeds some given threshold is a well-studied problem.

EXAMPLE 1 Throughout the paper we will use the following toy dataset  D =  ? ? 0 0 0 0 10 1 0 1 11 1 1 1 1  0 1 0 1 1 1 1 1 1 1 1 0 0 0 0  ? ?  containing 5 items, a, b, c, d, and e, and 6 transactions as a running example. As an example, for this dataset we have sp(ab) = 2, sp(ab = [1, 0]) = 1.

We say that a function f mapping an itemset X to a real number f(X) is monotonically decreasing if for each Y ? X we have f(Y ) ? f(X).

Closed Itemsets: An itemset X is closed, if there is no Y ? X such that sp(X) = sp(Y ), i.e., they are maximal among the itemsets having the same support. We define a predicate  ?c(X;D) =  { 1 if X is closed in D, 0 otherwise .

Free Itemsets: An itemset X said to be free if there is no Y ? X such that sp(X) = sp(Y ), i.e., free itemsets are minimal among the itemsets having the same support. We define a predicate  ?f (X;D) =  { 1 if X is free in D, 0 otherwise .

A vital property of free itemsets is that they constitute a downward closed collection allowing efficient mining with an Apriori-style algorithm (see Theorem 1 in [24]).

EXAMPLE 2 Closed itemsets in our running example are a, e, bde, and abcde. On the other hand, itemsets a, b, c, d, e, ab, ad, and ae are free.

Non-derivable Itemsets: An itemset X is said to be derivable, if we can derive its support from the supports of proper subsets of X , otherwise an itemset is called non- derivable. We define a predicate  ?n(X;D) =  { 1 if X is non-derivable in D, 0 otherwise .

PROPOSITION 3 An itemset X is derivable if and only if there are two vectors v and w with v having odd number of 0s and w having even number of 0s and sp(X = v) = sp(X = w) = 0.

Proof: To verify whether an itemset is derivable, we compute bounds for the frequency by using the inclusion- exclusion principle. An itemset is derivable if and only if the upper and lower bounds are equal. We can show that the upper bound is equal to u = sp(X) +minv sp(X = v), where v has odd number of 0s. Similarly, the lower bound is equal to l = sp(X) ? minw sp(X = w), where w has even number of 0s (see [14]). Itemset is derivable if and only 0 = u? l = minv sp(X = v) + minw sp(X = w).

Corollary 3.4 in [14] states that non-derivable itemsets are downward closed, hence we can mine them using an Apriori- style approach.

We say that an itemset X is totally shattered if sp(X = v) > 0 for all possible binary vectors v. In other words, every possible combination of values for X occur in D. Again, we define a predicate  ?s(X;D) =  { 1 if X is totally shattered in D, 0 otherwise .

Totally shattered itemsets are related to VC- dimension [15],and we can show that a totally shattered itemset is always free and non-derivable (but not vice versa).

EXAMPLE 4 Itemset ab in the running example is totally shattered. Itemset ac is non-derivable but not totally shat- tered because sp(ac = [0, 1]) = 0.

It is easy to see from the definition that totally shattered itemsets constitute a downward collection, hence they are easy to mine using an Apriori-style approach.

B. Measuring robustness  In this section we propose a measure of robustness for itemsets with a predicate ?. Intuitively we consider an itemset robust if the predicate is true for many subsets of the database.

In order to define the measure formally, we first define a probability for a subset of D.

DEFINITION 5 Given a binary dataset D, and a real number ?, 0 ? ? ? 1, we define a random dataset D? obtained from D by keeping each transaction with probability ?, or otherwise discarding it.

Let S be a subset of D. The probability of D? = S is equal to  p(D? = S) = ? |S|(1? ?)|D|?|S| . (1)  DEFINITION 6 Given a binary dataset D, a real number ?, and an itemset predicate ?, we define the robustness to be the probability that ?(X;D?) = 1, that is,  r(X;?,D, ?) = p(?(X;D?) = 1) = ?  ?(X;S)=1  p(D? = S) .

For notational clarity, we will omit D and ? when they are clear from the context.

EXAMPLE 7 Consider itemset ab in our running example.

Let ? = 1/3. Note that sp(ab = [0, 0]) = sp(ab = [1, 0]) = 1 and sp(ab = [0, 1]) = sp(ab = [1, 1]) = 2. In order for ab to still be totally shattered on a subset each of these supports needs to stay greater than zero. The probability of this event is equal to  1/3? 1/3? (1? 2/3? 2/3)? (1? 2/3? 2/3) = 25/729, because for the first two cases we need to sample the single transaction upholding the property and for the other two cases we need to make sure we do not skip both of the two transactions we need to uphold the property.

Our main goal is to mine itemsets for which the robustness measure exceed some given threshold, that is, find all itemsets for which r(X;?,D, ?) ? ?.

Let us first consider the effect of ?. If we set ? = 1, then r(X;?,D, ?) = ?(X;D). Naturally, we expect that when we lower ? then the robustness would decrease. This holds for predicates that satisfy a specific property.

DEFINITION 8 We say that a predicate ? is monotonic w.r.t.

deletion if for each itemset X , each dataset D, and each transaction t ? D it holds that if ?(X;D) = 0, then ?(X;D ? t) = 0.

PROPOSITION 9 Let ? be a predicate monotonic w.r.t. dele- tion. Then r(X;?,D, ?) ? r(X;?,D, ?), for ? ? ?.

Proof: We will prove the proposition by induction over |D|. Proposition holds trivially for |D| = 0. Assume that theorem holds |D| = N and let D be a dataset with |D| = N + 1.

Fix t ? D and define a new predicate ?t(X;S) = ?(X;S ? {t}), where S is a dataset. ?t is monotonic w.r.t deletion. Otherwise, if there is a dataset S, a transaction u ? S an itemset Y violating the monotonicity, then S?{t}, the same transaction u and the itemset Y will violate the monotonicity for ?.

Moreover, since ? is monotonic w.r.t deletion, it holds that ?(X;S) ? ?t(X;S). This in turns implies that  r(X;?, S, ?) ? r(X;?t, S, ?) . (2) Let us write D? = D ? {t}. Then we have, r(X;?,D, ?) = (1? ?)r(X;?,D?, ?) + ?r(X;?t, D?, ?)  ? (1? ?)r(X;?,D?, ?) + ?r(X;?t, D?, ?) ? (1? ?)r(X;?,D?, ?) + ?r(X;?t, D?, ?) = r(X;?,D, ?) ,  where the first inequality holds because of Equation 2 and the second inequality holds because of induction assumption.

This proves the proposition.

It turns out that all the predicates we considered in Section III-A are monotonic w.r.t. deletion.

PROPOSITION 10 Predicates ?c , ?f , ?n , and ?s are mono- tonic w.r.t. deletion.

Proof: An itemset is not totally shattered if there is a binary vector v such that sp(X = v;D) = 0. This immediately implies that sp(X = v;D ? {t}) = 0. Thus ?s is monotonic w.r.t. deletion. Similarly, Proposition 3 implies that ?n is monotonic w.r.t. deletion.

An itemset X is not free, if there is x ? X such that there is no transaction u ? D for which ux = 0 and uy = 1 for all y ? X ? {x}. If this holds in D, then it holds for D?{t}. This makes ?f monotonic w.r.t. deletion. Similarly, an itemset X is not closed, if there is x /? X such that there is no transaction u ? D for which ux = 0 and uy = 1 for all y ? X . If this holds in D, then it holds for D ? {t}.

This makes ?c monotonic w.r.t. deletion.

EXAMPLE 11 The itemset bd is not closed because its superset bde is always observed when bd is observed. No matter which transaction we delete (one with or without bde) this will not change. Note, however, that bde can become non-closed if transactions 2 and 4 are deleted because then abcde will have the same support of 2.

In order to mine all significant patterns we need to show that the robustness measure is monotonically decreasing.

This is indeed the case if the underlying predicate is mono- tonically decreasing.

PROPOSITION 12 Let ? be a monotonically decreasing predicate. Then r(X;?,D, ?) is also monotonically de- creasing.

Proof: Let Y and X be itemsets such that Y ? X .

Then r(X;?,D, ?) is? ?(X;S)=1  p(D? = S) ? ?  ?(Y ;S)=1  p(D? = S) = r(Y ;?,D, ?) ,  which proves the proposition.

C. Computing the measure  In this section we demonstrate how to compute the ro- bustness measure for the predicates. Computing the measure directly from the definition is impractical since there are 2|D|  number of subsets of D. It turns out that computing free, non-derivable, and totally shattered itemsets has practical formulas while the robustness measure for closed itemsets has no practical formulation (see Table I).

To facilitate the analysis we introduce the following function: Given an itemset X and a set of binary vectors V ? {0, 1}|X| we define  o(X,V, ?) = ? v?V  1? (1? ?)sp(X=v) .

Table I COMPUTATIONAL COMPLEXITY OF ROBUSTNESS AND ORDERS.

COMPUTING MEASURES IS EXPLAINED IN SECTION III-C. COMPUTING ORDERS IS EXPLAINED IN SECTION IV. K IS THE NUMBER OF ITEMS,  |C| IS THE NUMBER OF FREQUENT CLOSED ITEMSETS.

predicate measure order order estimate  free O(|X|) O(|X|) ? totally shattered O(2|X|) O(2|X|) ? closed O(2K?|X|) O(2K?|X|) O(  ? ?C2??)  non-derivable O(2|X|) O(22 |X|  ) O(4|X|)  PROPOSITION 13 Given an itemset X , let V be the set of |X| vectors having |X| ? 1 ones and one 0. The robustness of a free itemset is r(X;?f , ?) = o(X,V, ?).

Proof: Given an item x ? X , define a random variable Tx = sp(X ? {x} ;D?) > sp(X;D?). X is still free in D? if Tx is true for all x ? X . Tx is true if and only if D? contains a transaction t with tx = 0 and ty = 1 for y ? X ? {x}. There are sp(X = v;D) such transactions, where v ? V is the vector for which vx = 0. p(Tx) is the probability of not removing all these transactions, thus  p(Tx) = 1? (1? ?)sp(X=v;D) .

Since each of these transaction is missing only one x ? X , there is no common transactions between different events Tx, making them independent. Thus, we can conclude r(X;?f , ?) =  ? x?X p(Tx) = o(X,V, ?).

PROPOSITION 14 Given an itemset X , let V be the set of all binary vectors of length |X|. The robustness of a totally shattered itemset is r(X;?s , ?) = o(X,V, ?).

Proof: Given a binary vector v ? V , define a random variable Tv = sp(X = v;D?) > 0. X is still totally shattered in D? if Tv is true for all v ? V . p(Tv) is the probability of not removing all these transactions, thus p(Tv) = 1 ? (1 ? ?)sp(X=v;D). Again, since no transac- tion can contribute to different Tv being true, the random variables are independent and we obtain r(X;?s , ?) =?  v?V p(Tv) = o(X,V, ?).

Note that this formula in this proposition corresponds  directly to Example 7.

We will now consider closed itemsets. Unlike with  free/totally shattered itemsets, there is an exponential num- ber of terms. The key problem is that closure depends on the items outside the itemset whereas other predicates consider only the items inside the itemset.

PROPOSITION 15 The robustness of a closed itemset is  r(X;?c , ?) = ? Y?X  (?1)|Y |?|X|(1? ?)sp(X)?sp(Y ) .

Proof: Given an item y /? X , define a random variable Ty = sp(X ? {y} ;D?) = sp(X;D?). X is still closed in     D? if all Ty are false, thus r(X;?c , ?) is equal to  1? p( ? y/?X  Ty ) =  ? Y ?Z=?  (?1)|Z|p( ? y?Z  Ty ) ,  where the equality follows from the inclusion-exclusion principle. Through this transformation we now need to determine the probability of all Ty being true. For this all sp(X) ? sp(Y ?X) transactions containing X but not Z must have been excluded from D?, hence  p ( ? y?Z  Ty ) = (1? ?)sp(X)?sp(Z?X) .

Substituting this above and writing Y = X ?Z leads to the proposition.

EXAMPLE 16 In our running example, we have sp(bde) = 4. This itemset has 3 superitemsets having the supports sp(abde) = sp(bcde) = sp(abcde) = 2. Hence, the measure r(bde;?c , ?) is equal to  1? (1??)4?2 ? (1??)4?2 + (1??)4?2 = 1? (1??)2, where itemsets bde, abde, bcde, and abcde correspond to the terms on the left side in the given order.

PROPOSITION 17 Given an itemset X , write V to be the set of binary vectors of length |X| having odd number of ones. Similarly let W be the set of binary vectors of length |X| having even number of ones. The robustness of a non- derivable itemset is  r(X;?n , ?) = 1? (1? o(X,?, V ))(1? o(X,?,W )) .

Proof: Let us define an event TV to be the lack of  v ? V such that sp(X = v) = 0. Similarly, let TW be an event corresponding to the lack of w ? W such that sp(X = w) = 0. According to Proposition 3, an itemset X is derivable if TV and TW are both false.

Using the same argument as with Proposition 14, we see that p(TV ) = o(X,?, V ). Similarly, p(TW ) = o(X,?,W ).

Since V ?W = ?, events A and B are independent. Hence, r(X;?n , ?) is equal to  1? p(?TV ? ?TW ) = 1? (1? p(TV )(1? p(TW )) .

This completes the proof.



IV. ORDERING PATTERNS  The robustness measure depends on the parameter ?. In this section we propose a parameter-free approach. The idea is to study how measure is behaving when ? is close to 1.

We can show that there is a (small) neighborhood close to 1, where the ranking of itemsets does not depend on ?. We can compute a ranking that can use to select top-k itemsets by robustness without actually computing the measure or determining the neighborhood.

We will first introduce the general idea and then demon- strate how can we compute the ranking for free and totally  shattered itemsets and how can we estimate the ranking for closed and non-derivable itemsets. For computational complexity see Table I.

A. Measuring robustness when ? approaches 1  When ? = 1 then D? = D with probability 1 and the measure is equivalent to the underlying predicate, providing only a crude ranking: itemsets that satisfy the predicate vs. itemsets that do not. If we make ? slightly smaller the measure will decrease a little bit for each itemset. The amount of this change will vary from one itemset to another based on how likely removing only very few transactions will break the predicate for this itemset. We can use the magnitude of this change to obtain a more fine-grained ranking by robustness. The key result for this is that there is a small neighborhood below 1 in which the ranking of itemsets based on the measure does not depend on ?.

PROPOSITION 18 Given a predicate ? and a dataset D, there exists a number ? < 1 such that  r(X;?,D, ?) ? r(Y ;?,D, ?) if and only if r(X;?,D, ??) ? r(Y ;?,D, ??) ,  for any itemset X and Y and ? ? ? ? 1, ? ? ?? ? 1.

Proof: Fix X and Y and consider  f(?) = r(X;?,D, ?)? r(Y ;?,D, ?) .

Since the measure is a finite sum of probabilities that are, according to Eq. 1, polynomials of ?, the function f is a polynomial. This implies that f can have only finite number of 0s. Consequently there is a neighborhood N = [?, 1] such that either f(?) ? 0 for any ? ? N , or f(?) ? 0 for ? ? N . Since there are only finite number of itemsets, we can take the maximum of all ?s to prove the theorem.

Proposition 18 allows us to define an order for itemsets based on the measure for ? ? 1.

DEFINITION 19 Given a predicate ?, and a dataset D, we say that X ?? Y , where X and Y are itemsets, if there is ? < 1 such that r(X;?,D, ?) ? r(Y ;?,D, ?) for any ? such that ? ? ? ? 1. Moreover, if r(X;?,D, ?) < r(Y ;?,D, ?) for some ? ? ?, then we write X ?? Y .

Note that Proposition 18 implies that for any X and Y , either X ?? Y or Y ?? X . That is, we can use this relation to order itemsets.

B. Free and totally shattered itemsets  In this section we will demonstrate that we can compute the order for free and totally shattered itemsets without finding an appropriate ?. We will do this by analyzing the coefficients of the measure viewed as a polynomial of ?.

The key step is the following lemma that can be proven by elementary real analysis.

LEMMA 20 Let f(x) = ?N  i=0 aix i be a non-zero polyno-  mial. Let k be the first index such that ak ?= 0 If ak > 0, then there is a ? > 0 such that 0 ? x ? ? implies f(x) ? 0.

Similarly, if ak < 0, then there is a ? > 0 such that 0 ? x ? ? implies f(x) ? 0.

We cannot use Lemma 20 directly with Proposition 13 and Proposition 14 because both polynomials contain an exponential number of terms. However, the polynomials are regular enough so that we can compute the order without expanding the polynomials. In order to that we need the following definition for ordering sequences.

DEFINITION 21 Given two non-decreasing sequences s = s1, . . . , sK and t = t1, . . . , tN , we write s ? t if either there is sn < tn and si = ti for all i < n or s is a proper prefix sequence of t, that is, si = ti for i ? K < N . We write s ? t, if s = t or s ? t.

The following proposition will allow us to order itemsets without expanding the polynomials in Propositions 13?14.

PROPOSITION 22 Assume two polynomials  f(?) = K? i=1  (1? (1? ?)si) and g(?) = N? i=1  (1? (1? ?)ti),  where s = s1, . . . , sK and t = t1, . . . , tN are non- decreasing sequences of integers, si, ti ? 0. If t ? s, then there is a ? < 1 such that ? ? ? ? 1 implies f(?) ? g(?).

Proof: The case s = t is trivial. Hence we assume that s ?= t. If s1 = 0 or t1 = 0, then f(?) = 0 or g(?) = 0, and the result follows, hence we will assume that si, ti > 0.

Let {ai} and {bi} be coefficients such that f(1? x) =  ? i  aix i and g(1? x) =  ? i  bix i .

Let In be the collection of all subsequences of s that sum to n. Similarly, let Jn be the collection of all subsequences of t that sum to n. Then, it follows that  an = ? I?In  (?1)|I| and bn = ? J?Jn  (?1)|J| .

Assume that t ? s. If s is a prefix sequence of t, then  g(?) = f(?)  N? i=K+1  (1? (1? ?)ti) ? f(?),  which proves the proposition. Let n be as given in Defi- nition 21. For every i < sn, the subsequences in Ii and Ji contain entries from s and t with indices smaller than n. Since s and t are identical up to n, then it follows that Ii = Ji and consequently ai = bi. Let I ? Isn . Assume that |I| > 1. Since, we assume that si > 0, I is a subsequence of s1, . . . , sn?1. This means that we will find the same subsequence in Jn. Let A be the number of singleton sets in Isn and let B be the number of singleton sets in Jsn .

These singleton sets correspond to the entries in s and t having the same value as sn. By definition, B > A. We have now an ? bn = B ? A > 0. Lemma 20 now implies that f(1 ? x) ? g(1 ? x), when x is close to 0. Write ? = 1? x to complete the proof.

The polynomials in Propositions 13?14 have the form used in Proposition 22. Consequently, we can use the proposition to order itemsets. In order to do that we need the following definitions.

DEFINITION 23 Given a dataset D and an itemset X , we define a free margin vector mv(X;D,?f ) to be the sequence of |X| integers sp(X = v;D), where v is a binary vector having |X| ? 1 ones, ordered in the increasing order.

Similarly, we define a totally shattered margin vec- tor mv(X;D,?s) to be a sequence of 2|X| integers sp(X = v;D) ordered in the increasing order.

COROLLARY 24 Given itemsets X and Y and a dataset D, X ??f Y if and only if mv(X;D,?f ) ? mv(Y ;D,?f ).

COROLLARY 25 Given itemsets X and Y and a dataset D, X ??s Y if and only if mv(X;D,?f ) ? mv(Y ;D,?s).

EXAMPLE 26 In our running example, sp(ab = [1, 0]) = 1 and sp(ab = [0, 1]) = 2, hence the free margin vec- tor is equal to mv(ab;?f ) = [1, 2]. Similarly, we have sp(ae = [1, 0]) = 1 and sp(ae = [0, 1]) = 3, hence the free margin vector is equal to mv(ae;?f ) = [1, 3]. Hence, we conclude that ab ??f ae.

C. Closed itemsets  In this section we will introduce a technique for estimating the ranking for closed itemsets. As the measure for closed itemsets has a different form than for free or totally shattered itemsets we are forced to seek for alternative approaches.

Let us consider Proposition 15. Let ak be the coefficient for the kth term of the polynomial for r(X;?c , ?) given in Proposition 15. If we can compute these numbers efficiently, we can use Lemma 20 to find the ranking.

We will do this by first expressing ak using closed itemsets. In order to do that let cl(X) be the closure of an itemset X . Let us define  e(Y,X) = ? Z?X,  cl(Z)=Y  (?1)|Z|+|X|  to be the alternating sum over all itemsets containing X and having Y as their closure. Since all the itemsets having the same closure will have the same support we can write the coefficients ak using e(Y,X),  ak = ? Y?X,  sp(X)?sp(Y )=k  (?1)|Y |+|X| = ?  Y?X,Y =cl(Y ) sp(X)?sp(Y )=k  e(Y,X) . (3)     To compute e(Y,X), first note that e(X,X) = 1. If Y ?= X , then using the following identity?  Y?Y ??X Y ?=cl(Y ?)  e(Y ?, X) = ? Z?X  (?1)|Z|+|X| = 0  we arrive to  e(Y,X) = ? ?  Y ?Y ??X Y ?=cl(Y ?)  e(Y ?, X) . (4)  Thus, we can compute e(Y,X) from e(Y ?, X), where Y ?  is a closed subset of Y . This is convenient, because when computing e(Y,X), say for ak, we have already computed all the subsets of Y for previous coefficients.

EXAMPLE 27 Consider itemset e in our running example.

There are two closed supersets of e, namely bde and abcde, having the supports 4 and 2, respectively. Using the update equations, we see that e(e, e) = 1, e(bde, e) = ?1, and e(abcde, e) = 0. As sp(e) = 5, we see that the non-zero coefficients ai are a0 = 1 and a1 = ?1.

The problem with this approach is that we can still have an exponential number of closed itemsets. Hence, we chose to estimate the ranking by only using frequent closed itemsets and estimate the remaining itemsets to have a support of 0.

This estimation is achieved by removing all closed non- frequent itemsets from the sums of Eqs. 3 and 4 and adding an itemset containing all the items and having the support 0. The code for this estimation is given in Algorithm 1.

Algorithm 1: Algorithm for estimating coefficients of the polynomial given in Proposition 15.

input : X an itemset, C, frequent closed itemsets output: {ak}, coefficients of the polynomial  1 if A /? C then add A to C with sp(A) = 0; 2 C ? {Y ? C | X ? Y } ; 3 L ? sets in C ordered by the subset relation; 4 e(X,X)? 1; 5 for Y ? L do 6 e(Y,X)? ??Z?C,Z?Y e(Z,X); 7 k ? sp(X)? sp(Y ); 8 ak ? ak + e(Y,X);  Algorithm 1 takes O(|C|2) time. In practice, this is much faster because an average itemset does not have that many supersets.

Now that we have a way of estimating ak from frequent closed itemsets, we can, given two itemsets X and Y , search the smallest k for which the coefficients differ in order to apply Lemma 20. Note that if the index of the differing coefficient, say k, is such that sp(X)?k is larger or equal to the support threshold, then ak is correctly computed by our estimation, and our approximation yields a correct ranking.

D. Non-derivable itemsets  In this section we will discuss how to estimate the ranking non-derivable itemsets. The ranking for non-derivable is particularly difficult because we cannot use Proposition 22 to avoid expanding the polynomial given in Proposition 17.

We cannot expand the polynomial since it has O(22  |X| )  terms. Moreover, we cannot use the estimation trick done with closed itemsets because the problem is the exponential number of combinations of subsets of |X|. Hence, we resort to a simple heuristic.

First note that we can rewrite the measure as  o(X,?, V ) + o(X,?,W )? o(X,?, V )o(X,?,W ), where V and W are as defined in Proposition 17. This for- mulation implies that any term of form (1? ?)  ? v sp(X=v),  where v sums either over a subset of V or a subset of W , is canceled out. On the other hand, the terms having the form (1??)sp(X=v)+sp(X=w), where v ? V and w ?W , will be among the smallest ones. Hence, we propose the following margin vector to use as a heuristic.

DEFINITION 28 Given a dataset D and an itemset X , we define a non-derivable margin vector mv(X;D,?n) to be a sequence of 4|X|?1 integers sp(X = v;D)+sp(X = w;D), where v is a binary vector having odd number of ones and w is a binary vector having even number of ones, ordered in the increasing order.

We will rank itemsets by comparing their margin vectors.

We should stress that this is heuristic since, unlike with free and totally shattered itemsets, we have no guarantee that terms containing more than two supports will cancel out and not disrupt the ranking. Nevertheless, this ranking makes sense in the light of Proposition 3: an itemset is derivable if and only if one the entries in the margin vector is 0. If the entries in the margin vector are large, then the itemset is ?far away? of being derivable.

EXAMPLE 29 Consider itemset ac in our running example.

We have sp(ac = [0, 0]) = 3, sp(ac) = 2, sp(ac = [1, 0]) = 1, and sp(ac = [0, 1]) = 0. Thus the margin vector is equal to [0 + 2, 0 + 3, 1 + 2, 1 + 3] = [2, 3, 3, 4].



V. EXPERIMENTS  In this section we present our experiments.1  A. Datasets  We used datasets from three repositories. The 10 FIMI [25] datasets include large transaction datasets derived from traffic data, census data, and retail data. Two datasets are synthetically generated to simulate market basket data.

The datasets from the UCI Machine Learning Reposi- tory [26] represent classification problems from a wide  1The implementation of our algorithms is given at http://adrem.ua.ac.be/ implementations/     variety of domains. We used the itemset representations of 29 datasets from the LUCS repository [27]. Finally we used 18 text datasets shipped with the Cluto clustering toolkit [28] but converted to itemsets using a binary representation of words in documents discarding the term frequencies.

B. Reducing the number of patterns The goal of the first experiment is to show that this new  constraint for itemsets can significantly reduce the number of itemsets reported in the results by removing itemsets that are spurious in the sense that they are unlikely to be observed on many subsamples. Throughout this section we will use ? to indicate the threshold for the support.

A first question is how the parameters should be chosen.

The smaller we set ?, the stricter the filtering will be.

? should not be very close 1, because otherwise brittle itemsets that could lose their predicate by removing only a few transactions still have a high likelihood of being found. This implies that robustness values are packed close to 1 when ? is large, and this might lead to problems due to floating point arithmetics. So a small ? is important to emphasize the quantitative difference between itemsets of various robustness, however, too small ? will skew the distribution towards 0 too much, which can lead to computational issues. The larger the minimum robustness, the stricter the filtering will be. The robustness threshold is more application dependent but it should not be close to zero, otherwise no reduction will be observed.

We did a parameter study for the itemset version of the Zoo dataset that describes 101 animals with 42 boolean attributes. The number of itemsets reported is shown in Figure 1. One can see how smaller ? and larger robustness thresholds reduce the numbers of free itemsets by almost 2 orders or magnitude. The transition is very smooth, show- ing that the parameters can be chosen without unexpected effects. The results for non-derivable and totally shattered itemsets and other datasets were very similar.

Min. Probability  A lp  ha      0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90  0.10  0.20  0.30  0.40  0.50  0.60  0.70  0.80  0.90  1.00 2  2.2  2.4  2.6  2.8   3.2  3.4  3.6  3.8  Figure 1. Log of the number of free itemsets on Zoo (? = 0.01) dataset using thresholds for the subsample size (?) and the minimum robustness.

Based on this study we chose ? = 0.5 and a minimum robustness of 0.1 to drill deeper into the robustness of the  reported itemsets. Excluding singleton itemsets we plotted histograms with the empirical distribution of robustness values associated with the reported itemsets. Figure 2(a) shows that for the Zoo dataset there are many free itemsets with very different robustness showing a rich structure that can be exploited to rank and reduce the number of itemsets. Similar results were observed for many of the UCI datasets. Figure 2(a) shows a representative example for the text datasets. While the distribution is much more skewed, a large robustness threshold would also reduce the number of itemsets by about 50%. Finally, Figure 2(c) shows an example for a large transactional dataset with 88k transactions. Using ? = 0.5 generated a distribution where all values were close to one so we needed to set ? = 0.01 to better show the quantitative difference of the itemsets.

This shows that the more transactions a dataset con- tains, the more skewed the distribution for a fixed ? will be. For experiments with all datasets we set ? = max(0.1,min(0.5, 1000/|D|)), that is we use samples of 1000 transactions but for small datasets we use 50% and for very large datasets we use 10%. Using this parameter we computed all robust itemsets with a robustness ? 0.1 and computed the median robustness of the reported itemsets to summarize the distributions. Figure 3 plots the median robustness against the order of magnitude that the itemsets can be reduced when using a robustness threshold of 0.9. For many datasets a significant reduction is observed. For some datasets with a median close to or equal to 1 the reduction is small, indicating that most itemsets found are quite robust in this data.

0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.5   1.5   2.5  Median probability  O rd  er o  f r ed  uc tio  n      free ndi ts  Figure 3. Median of the itemset robustness vs. order of magnitude in numerosity reduction (log10 scale) using robustness threshold 0.9.

C. Ranking without ?  Our next experiment was to compare parameter-free rank- ings described in Section IV against the rankings based on robustness. We expect that rankings are similar for high ? values and increase when we lower ?. For comparison we used Kendall?s ? distance, that is the number of discordant pairs, normalized such that the distance ranges between     0 0.2 0.4 0.6 0.8 1        Probability  Ite m  se ts  (a) Zoo (? = 0.5, ? = 0.01)  0 0.2 0.4 0.6 0.8 1          Probability  Ite m  se ts  (b) LA12 (? = 0.5, ? = 0.05)  0 0.2 0.4 0.6 0.8 1        Probability  Ite m  se ts  (c) Retail (? = 0.01, ? = 0.0080)  Figure 2. Distribution of robustness for free itemsets and minimum robustness 0.1.

0 and 1. Values close to 0 means that rankings are in agreement. Since the values are typically very small we represent the values on a log-scale. A typical example is given in Table II for Mushroom and Zoo datasets.

In general, the distance values are small, suggesting that the rankings are similar. The values increase as we lower ? which is expected since the parameter-free approach is based on large ? values. Rankings for non-derivable itemsets tend to be higher, which is also expected, since the ranking for non-derivable itemsets is a heuristic.

Table II log10(KENDALL?S TAU DISTANCE) FOR Mushroom AND Zoo DATASETS.

Mushroom (? = 0.05) Zoo (? = 0.01)  ? free ts ndi free ts ndi  0.1 -2.55 -2.13 -2.21 -0.81 -1.14 -1.07 0.2 -3.17 -2.65 -2.54 -0.99 -1.06 -1.22 0.3 -3.91 -3.08 -2.8 -1.22 -1.27 -1.33 0.4 -4.98 -3.46 -2.92 -1.54 -1.56 -1.36 0.5 -6.72 -3.79 -3.07 -1.95 -1.88 -1.44 0.6 -7.58 -4.06 -3.11 -2.49 -2.24 -1.5 0.7 ?? -4.98 -3.63 -3.76 -2.7 -1.55 0.8 ?? -4.99 -4.11 ?? ?? -1.6 0.9 ?? ?? -5.07 ?? ?? -1.64  D. Top-k closed itemsets Closed itemsets are often used for tasks requiring in-  terpretation of the itemsets, because a maximum elements of an equivalence class they offer the most detailed de- scription. We studied the highest ranked closed itemsets for text datasets that are easily understood without domain knowledge. As an illustrative example, we used the re0 news dataset from which we mine 2493 closed itemsets with minimum support ? = 0.05. We ordered these itemsets using the estimation technique given in Section IV-C and list the top 45 itemsets in Table III. The ranking is different from one using support, less frequent (but more robust) itemsets are commonly ranked higher that frequent itemset.

For example, ?bank pct rate? occurs before much more frequent itemset ?bank pct?.

Table III TOP-45 CLOSED ITEMSETS FROM re0 (? = 0.05) DATASET.

1. pct 792 16. week 310 31. canada 117 2. bank 702 17. pct earlier 127 32. pct month 261 3. trade 485 18. japan 318 33. econom 295 4. billion 552 19. trade current 126 34. billion dlr mln 116 5. market 554 20. dlr 472 35. told bank 116 6. billion dlr 346 21. bank pct rate 287 36. told nation 116 7. offici 342 22. dollar 336 37. pct japan 115 8. mln 420 23. statem 122 38. pct adjust 115 9. nation 323 24. committe 121 39. billion current 115  10. rate 566 25. nation month 121 40. european 114 11. bank market 369 26. ministri 120 41. month japan 114 12. foreign 331 27. pct rise 269 42. bank ad market 114 13. pct figur 132 28. bank pct 407 43. action 114 14. pct rate 418 29. pct rate feb 119 44. trade world 114 15. month 391 30. lead 118 45. nation japan 114

VI. DISCUSSION  The experiments have shown that the number of itemsets can be largely reduced on many datasets when requiring a certain robustness. The fact that the results vary by dataset are another indication of the well known fact that itemset data with different structures (dense vs. sparse, many items vs. many transactions) behave very differently in mining tasks.

We believe that robust itemsets can be beneficial for post- processing techniques such as [29] or [30] that use itemsets as their input and remove redundancy in the pattern set.

Robust itemsets can be used as an alternative input reducing their runtime without sacrificing performance. Also, robust itemsets could be used instead of closed-itemsets as seeds to the AC-Close algorithm for approximate itemset mining [23] improving its efficiency that was criticized in [19].

The ranking of itemsets by robustness presents a new interestingness measure that can be used to choose the top-k itemsets for interpretation or other data mining tasks. The intuition of robustness should be easy to understand for analysts but which ranking is better for specific data mining tasks remains to be studied.



VII. SUMMARY We have shown how robustness under subsampling for  common classes of itemsets can be computed efficiently without actually sampling the data. The experimental results show that the number of reported itemsets can be largely reduced on many datasets, in other words spurious itemsets that would not have been found in many subsets of the data are removed. The approach can further be used to rank itemsets for top-k mining by robustness. Future work will investigate the effect of using robust itemsets on data mining tasks such as clustering, classification, and rule generation using itemsets.

