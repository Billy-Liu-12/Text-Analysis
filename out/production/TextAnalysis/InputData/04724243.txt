A New Parallel Algorithm for the Frequent Itemset Mining Problem

Abstract  A new parallel algorithm for finding the frequent item- sets in databases is presented. It differs fundamentally of well known Apriori algorithm, where at the beginning of every step, the dimension of the new frequent itemsets in- creases by 1 . In our algorithm the frequent itemsets are de- termined by progressively enlarging the interval which the individual items appertain, i.e. if at the k-th step the new candidates are from [i, i + k] intervals, i = 1, 2, . . . , n? k, at the next step, k + 1, the new candidates will belong to [i, i+k +1] intervals, i = 1, 2, . . . , n?k?1. The frequent individual items are identified by their index. The basic idea is that the new frequent itemsets with individual items from the interval [i, j], simultaneously contain the items i and j. The frequent itemsets are built by sharing the work be- tween n processors. Hereby, the processor Pi computes, step by step, the sets Fi,j of the frequent itemsets with indi- vidual items from the intervals [i, j], j = i, . . . , n. In order to compute the set Fi,j , the processing unit Pi uses Fi,j?1 obtained in the previous step and Fi+1,j received from the processor Pi+1. The main advantage of our parallel algo- rithm is that it uses a communication pattern known before algorithm start, which allows mapping communication to hardware. Another major advantage is that the set of the transactions can be distributed to processors prior to begin- ning. This is possible because a processor Pi has to com- pute Fi,j , j = i, . . . , n and therefore only the transactions containing the frequent item i are needed.

1 Introduction  Association rule discovery (ARD) refers the following problem: Given a data set, one must determine all relations between elements in such way that the presence of a given element implies the presence of another element. Such rela- tions may be associations between elements that belong to the same set or between different data sets.

Association rule discovery implies an essential stage: frequent itemset mining (FIM) in target databases.

Let I be a set of items and D a database of transactions.

Every transaction is a set of distinct items ( itemset) from I .

An itemset with k items is referred to as a k-itemset. The support of an itemset X , denoted as ?(X), is the total num- ber of transactions in which that itemset occurs as a subset.

A second formal definition for the support of an itemset X is given by Agrawal [1]: an itemset X has a support of s if s% of transactions in D contains X as a subset. This second formal definition is somewhat more rigorous, as it emphasizes that the maximum support of an itemset cannot exceed the total number of transactions in D. An itemset is called frequent if its support is grater than a user-defined minimum support value. A frequent k-itemset X is maximal if no other k?-itemset (where k < k?) contains X as a subset [9].

An association rule is an expression X ? Y , where X and Y are disjoint itemsets [1]. An important note is that an association rule must not be considered as an implication, but rather as a coexistence of the two itemsets. The support of an association rule is given by the support of the X ? Y itemset. The confidence of an association rule is the condi- tional probability that a transaction contains Y , given that it contains X [9]. The confidence is computed using the formula c(X ? Y ) = ?(X?Y )?(X) . Minimum confidence of a rule is a user defined value. An association rule is strong if it has a support greater than minimum support value and confidence greater than the minimum confidence value.

Let n be the total number of individual items and m the total number of transactions in target database D.

The first stage of an ARD algorithm is finding all fre- quent itemsts. The search space for this step is exponential in 2: the total number of candidate sets is 2n.

The second stage of an ARD algorithm implies search- ing, step by step, all frequent itemsets previously found, in order to discover all possible association rules that meet the minimum-support and minimum-confidence conditions. A general way to generate the association rules is X\Y ? Y ,  2008 International Symposium on Parallel and Distributed Computing  DOI 10.1109/ISPDC.2008.45     where X is a frequent itemset and Y is a subset of X . Mo- hammed proves that this stage is O(r ? 2l), where r is the number of frequent itemsets and l is the longest frequent itemset (the itemset containing the most items)[9].

2 Related work  The Apriori algorithm represents the foundation of as- sociation rule mining algorithms. Apriori algorithm (Algo- rithm 1) was proposed by Agrawal and Srikant [1].

Algorithm 1 Apriori Notations: Lk = set of k-itemsets having minimum support (frequent itemsets) Ck= set of candidate k-itemsets (items to be counted) Initial conditions: D = set of transactions L1 = {frequent 1? itemsets} Algorithm: for (k = 2;Lk?1 ?= ?;k++) do  Ck ? Apriori gen (Lk?1) // new candidates for all transactions t ? D do  Ct ? Subset (Ck, t) // candidates contained in t for all candidates c ? Ct do  c.count? c.count + 1 end for  end for Lk ? {c ? Ck|c.count ? minsupp }  end for Answer =  ?  k  Lk  At the first step, the Apriori algorithm determines all fre- quent 1-itemsets, by scanning the transaction database D.

At the subsequent k-th step, first, the candidate k-itemsets will be generated, based on frequent (k-1)-itemsets found in the corresponding k-1 step. Then, the support for each can- didate k-itemset will be computed and the candidates that meet the minimum support condition will be retained.

An optimal generation of candidate k-itemsets is based on the Apriori Principle [8]: If an itemset is frequent, then all of its subsets must also be frequent. This theorem can easily be proved by analyzing the definition of support and considering the following property of the itemsets:  ?X,Y : X ? Y ? s(X) ? s(Y ) (1) where X , Y are itemsets. Also, Agrawal states that the re- sults of Apriori are not influenced by imposing a particular order for the individual items prior to determining frequent itemsets [1].

Among the optimized versions of the algorithm can be enumerated Apriori-TID and Apriori-Hybrid [1], DHP [5],  SON [6]. They improve the performances of the base algo- rithm through a series of modifications: reduce the number of database scans, reduce the size of the analyzed dataset at each scan, use of various pruning techniques.

The HPA (Hash Partitioned Apriori) algorithm is one of the most efficient parallel implementations of Apriori algorithm. This algorithm partitions both candidates and database transactions between processing nodes, using a hash function [7].

The steps of the HPA algorithm are as follows: Step 1. Generate candidate k-itemsets. All processors  have all the large (k ? 1)-itemsets in their memory when the k-th pass starts. Each processor generates candidate k- itemsets using large (k ? 1)-itemsets, applies a hash func- tion, and determines a destination processor identifier. If the identifier is the processor?s own, the itemset is inserted into the hash table, otherwise it is discarded.

Step 2. Scan the transaction database and count the sup- port value. Each processor reads its transaction database and computes the support for each k-itemset. It generates k- itemsets from those transactions and applies the same hash function used in the first step. Then, the processor deter- mines the destination processor and sends the k-itemsets to it. When a processor receives these itemsets, it searches the hash table for a match and increments the match count.

Step 3. Determine large k-itemsets. Each processor checks all the itemsets it has and determines large itemsets locally, then broadcasts them to the other processors.

Step 4. Check the terminal condition. If the num- ber of the large k-itemsets is 0, the algorithm terminates.

Otherwise, an all-to-all communication broadcasts large k- itemsets to all the processors and the algorithm enters the next iteration.

The speedup of the HPA algorithm is depending on the hash function. HPA avoids the memory overflow caused of large number of candidate itemsets by partitioning those itemsets among nodes using hash function as in the hash join [7].

Apriori algorithm has two major drawbacks with respect to storage space, memory use and running time: multiple scans of the initial database and the complex phase of effi- cient candidate generation. In order to avoid the above men- tioned issues, a series of alternative algorithms have been proposed. One of them is the FP-Growth (Frequent Pattern Growth) algorithm which uses a tree structure [4]. The al- gorithm generates all frequent itemsets with only two scans of the target database, without employing a candidate gen- erating step. The algorithm has two basic steps: building the FP-Tree (Frequent Pattern Tree) and generation of fre- quent itemsets. Although it surpasses the performances of the Apriori algorithm, the FP-Growth algorithm is difficult to use within interactive systems within which users fre- quently modify support and confidence factors and within     which database size is not constant. Another tree based algorithm for mining association rules is RARM (Rapid Association Rule Mining) [3]; this algorithm employs tree structures in representing the input data set.

3 New parallel algorithm for frequent item- set mining problem  The basic idea of our new parallel algorithm for the FIM problem is to determine, step by step, the frequent itemsets, by enlarging the interval to which the individual items be- long to, i.e. if at the k-th step the new candidates are from [i, i + k] intervals, i = 1, 2, . . . , n ? k, at the next step, k + 1, the new candidates will be from [i, i + k + 1] inter- vals, i = 1, 2, . . . , n? k ? 1 [2].

Let us consider n individual frequent items and m trans- actions. In the first step, the new candidates will be sub- sets of the sets {1, 2}, {2, 3}, , {n ? 1, n}. In the sec- ond step the new candidates will be subsets of the sets {1, 2, 3}, {2, 3, 4}, . . . , {n ? 2, n ? 1, n}. In the last step the new candidates will be subsets of the set {1, 2, . . . , n}.

This approach simplifies the candidate generation function and has bigger parallelism potential than Apriori algorithm.

Will will use the following notations: Fi,j = the set of all frequent itemsets from the interval [i, j] (set of items having minimum support); Ci,j = the set of candidate itemsets from the interval [i, j].

Lemma 1. A frequent itemset from Fi,j which does not simultaneously contain the items i and j belongs to Fi,j?1 or Fi+1,j .

Proof. Let us suppose an itemset {k, ..., l}, k ? i, l ? j?1, is in Fi,j but not in Fi,j?1. This means that Fi,j?1 does not contains all frequent itemsets from the interval [i, j ? 1].

Similarly, for an itemset {k, ..., l}, k ? i + 1, l ? j.

Lemma 2. The new frequent itemsets in Fi,j simultane- ously contain the items i and j Proof. Lemma 2 is an immediate consequence of Lemma 1.

From Lemma 2, it results that the candidates from the interval [i, j] are obtained in accordance with the relation (2)  Ci,j = {X?Y |(X ? Fi,j?1?i ? X)?(Y ? Fi+1,j?j ? Y ) (2)  Figures 1 and 2 present an example with 10 transactions and 9 frequent individual items. The transactions are de- noted by T1, T2, , T10 and the individual items i1, i2, . . . , i9 are identified with their indices.

T1 ? {1, 2} T2 ? {1, 2, 3, 5, 9} T3 ? {1, 4} T4 ? {3, 4, 5, 6, 7, 8} T5 ? {7, 8, 9} T6 ? {2, 4, 6, 8} T7 ? {1, 3, 5, 7, 9} T8 ? {1, 4, 5, 6, 7} T9 ? {2, 4, 5, 6, 9} T10 ? {3, 4, 5, 6, 7} minimum-suport = 2 Initial Frequents items: 1,2,3,4,5,6,7,8,9.

F1,1={1}; F2,2={2}; F3,3={3}; F4,4={4}; F5,5={5}; F6,6={6}; F7,7={7}; F8,8={8}; F9,9={9}.

Step 1 Initial frequent itemsets in step 1: IF1,2={1,2}; IF2,3={2,3}; IF3,4={3.4}; IF4,5={4,5}; IF5,6={5,6}; IF6,7={6,7}; IF7,8={7,8}; IF8,9={8,9}.

Candidates: C1,2={{1,2}}; C2,3={{2,3}}; C3,4={{3.4}}; C4,5={{4,5}}; C5,6={{5,6}}; C6,7={{6,7}}; C7,8={{7,8}}; C8,9={{8,9}}.

New frequent itemsets: NF1,2={{1,2}; NF2,3 = ?; NF3,4={{3,4}}; NF4,5={{4,5}}; NF5,6={{5,6}}; NF6,7={{6,7}}; NF7,8={{7,8}}; NF8,9 = ?.

Frequent itemsets at the end of the step 1: F1,2={1,2,{1,2}}; F2,3={2,3}; F3,4={3,4,{3,4}}; F4,5={4,5,{4,5}}; F5,6={5,6,{5,6}}; F6,7={6,7,{6,7}}; F7,8={7,8,{7,8}}; F8,9={8,9}.

Step 2 Initial frequent itemsets in step 2: IF1,3={1,2,3,{1,2}}}; IF2,4={2,3,4,{3,4}}; IF3,5={3,4,5,{3,4},{4,5}}; IF4,6={4,5,6,{4,5},{5,6}}; IF5,7={5,6,7,{5,6},{6,7}}; IF6,8={6,7,8,{6,7},{7,8}}; IF7,9={7,8,9,{7,8}}.

Candidates: C1,3={{1,3},{1,2,3}}; C2,4={{2,4},{2,3,4}}; C3,5={{3,5},{3,4,5}}; C4,6={{4,6},{4,5,6}}; C5,7={{5,7},{5,6,7}}; C6,8={{6,8},{6,7,8}}; C7,9={{7,9},{7,8,9}}.

New frequent itemsets: NF1,3={{1,3}}; NF2,4={{2,4}}; NF3,5={{3,5},{3,4,5}}; NF4,6={{4,6},{4,5,6}}; NF5,7={{5,7},{5,6,7}}; NF6,8={{6,8}}; NF7,9={{7,9}}.

Frequent itemsets at the end of the step 2: F1,3={1,2,3,{1,2},{1,3}}; F2,4={2,3,4,{3,4},{2,4}}; F3,5={3,4,5,{3,4},{4,5},{3,5},{3,4,5}}; F4,6={4,5,6,{4,5},{5,6},{4,6},{4,5,6}}; F5,7={5,6,7,{5,6},{6,7},{5,7},{5,6,7}}; F6,8={6,7,8,{6,7},{7,8},{6,8}}; F7,9={7,8,9,{7,8}, {7,9}}  Figure 1. New parallel algorithm for the FIM problem - Example, Part I     Step 3 Initial frequent itemsets in step 3: IF1,4={1,2,3,4,{1,2},{1,3},{3,4},{2,4}}; IF2,5={2,3,4,5,{3,4},{2,4},{3,4},{4,5},{3,5},{3,4,5}}; IF3,6={3,4,5,6,{3,4},{4,5},{3,5},{3,4,5},{4,5},{5,6},{4,6}, {4,5,6}}; IF4,7={4,5,6,7,{4,5},{5,6},{4,6},{4,5,6},{5,6},{6,7},{5,7}, {5,6,7}}; IF5,8={5,6,7,8,{5,6},{6,7},{5,7},{5,6,7},{6,7},{7,8},{6,8}}; IF6,9={6,7,8,9,{6,7},{7,8},{6,8},{7,8},{7,9}}.

Candidates: C1,4={{1,4},{1,2,4},{1,3,4},{1,2,3,4}}; C2,5={{2,5},{2,3,5},{2,4,5},{2,3,4,5}}; C3,6={{3,6},{3,4,6},{3,5,6},{3,4,5,6}}; C4,7={{4,7},{4,5,7},{4,6,7},{4,5,6,7}}; C5,8={{5,8},{5,6,8},{5,7,8},{5,6,7,8}}; C6,9={{6,9},{6,7,9},{6,8,9},{6,7,8,9}}.

New frequent itemsets: NF1,4={{1,4}}; NF2,5={{2,5}}; NF3,6={{3,6},{3,4,6},{3,5,6},{3,4,5,6}}; NF4,7={{4,7},{4,5,7},{4,6,7},{4,5,6,7}}; NF5,8=?, NF6,9 = ?.

Frequent itemsets at the end of the step 3: F1,4={1,2,3,4,{1,2},{1,3},{3,4},{2,4},{1,4}}; F2,5={2,3,4,5,{3,4},{2,4},{3,4},{4,5},{3,5},{3,4,5},{2,5}}; F3,6={3,4,5,6,{3,4},{4,5},{3,5},{3,4,5},{4,5},{5,6},{4,6}, {4,5,6},{3,6},{3,4,6},{3,5,6},{3,4,5,6}}; F4,7={4,5,6,7,{4,5},{5,6},{4,6},{4,5,6},{5,6},{6,7},{5,7}, {5,6,7},{4,7},{4,5,7},{4,6,7},{4,5,6,7}}; F5,8={5,6,7,8,{5,6},{6,7},{5,7},{5,6,7},{6,7},{7,8},{6,8}}; F6,9={6,7,8,9,{6,7},{7,8},{6,8},{7,8},{7,9}} . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

F1,8 F2,9  F1,7  F1,6  F1,5  F1,4  F1,3  F1,2  F1,1  F2,8  F2,7  F2,6  F2,5  F2,4  F2,3  F2,2  F3,9  F3,8  F3,7  F3,6  F3,5  F3,4  F3,3  F5,9  F5,8  F5,7  F5,6  F5,5  F7,9  F7,8  F7,7  F4,9  F4,8  F4,7  F4,6  F4,5  F4,4  F6,9  F6,8  F6,7  F6,6  F8,9  F8,8 F9,9  F1,9  Initial  Step 1  Step 2  Step 3  Step 4  Step 5  Step 6  Step 7  Step 8  Figure 2. New parallel algorithm for the FIM problem - Example, Part II  The new parallel algorithm for the FIM problem (Algo- rithm 2) is based on the fact that the frequent itemsets are built by sharing the work between n processors. The sets Fi,j , j = i, . . . , n, are computed, in successive steps, by the processor Pi. In order to compute the set Fi,j , the processor Pi uses Fi,j?1 from the previous step and Fi+1,j received from the processor Pi+1 (Figure 3). The main advantage of our parallel algorithm is that the set of the transactions can be distributed to processors prior to the beginning of the analysis. This is possible because a processor Pi needs to compute Fi,j , j = i, . . . , n only Di, the set of the trans- actions which contain the individual frequent item i. There- fore, when the algorithm starts, Pi needs to have access to Di and Fi,i = {frequent item i}.

F1,8 F2,9  F1,7  F1,6  F1,5  F1,4  F1,3  F1,2  F1,1  F2,8  F2,7  F2,6  F2,5  F2,4  F2,3  F2,2  F3,9  F3,8  F3,7  F3,6  F3,5  F3,4  F3,3  F5,9  F5,8  F5,7  F5,6  F5,5  F7,9  F7,8  F7,7  F4,9  F4,8  F4,7  F4,6  F4,5  F4,4  F6,9  F6,8  F6,7  F6,6  F8,9  F8,8  F1,9  ? ? ? ? ? ? ? ?  ? ? ? ? ? ? ?  ? ? ? ? ? ?  ? ? ? ? ?  ? ? ? ?  ? ? ?  ? ?  ?  Initial  Step 1  Step 2  Step 3  Step 4  Step 5  Step 6  Step 7  Step 8  F9,9  P1 P2 P3 P4 P5 P6 P7 P8 P9  Figure 3. Processors communication at new parallel algorithm for the FIM problem  When the processors number (m) is less than the fre- quent items number (n), the parallel algorithm can be mod- ified as follows: In the first stage, the frequent itemsets from the intervals [(i, i + n ?m], i = 1, . . . , m are sequentially computed. Then, at the second stage, the last m ? 1 steps of the parallel algorithm are applied.

The natural communication topology is the chain (Figure 4).

P1 P2 P3 P4 P5 P6 P7 P8 P9  Figure 4. Chain communication topology     Algorithm 2 New parallel algorithm for the FIM problem for k = 1 to n? 1 do  for all i : i ? {1, . . . , n? k} par do j ? i + k Fi,j ? Fi,j?1 ? Fi+1,j Ci,j ? {X ? Y | (X ? Fi,j?1 ? i ? X) ?  (Y ? Fi+1,j ? j ? Y ) for all transactions t ? Di do  Ct ? subset(Ci,j , t) // candidates contained in t for all candidates c ? Ct do  c.count? c.count + 1 end for  end for Fi,j = Fi,j ? {c ? Ci,j | c.count ? minsupp }  end for end for  Algorithm 3 Scalable parallel algorithm for k = 1 to n?m do  for all i : i ? {1, . . . , m} do j ? i + k Fi,j ? Fi,j?1 ? Fi+1,j Ci,j ? {X ? Y | (X ? Fi,j?1 ? i ? X) ?  (Y ? Fi+1,j ? j ? Y ) for all transactions t ? Di do  Ct ? subset(Ci,j , t) // candidates contained in t for all candidates c ? Ct do  c.count? c.count + 1 end for  end for Fi,j = Fi,j ? {c ? Ci,j | c.count ? minsupp }  end for end for for all i : i ? {1, . . . , m} do  send Fi,j to Pi end for for k = n?m + 1 to n? 1 do  for all i : i ? {1, . . . , n? k} par do j ? i + k Fi,j ? Fi,j?1 ? Fi+1,j Ci,j ? {X ? Y | (X ? Fi,j?1 ? i ? X) ?  (Y ? Fi+1,j ? j ? Y ) for all transactions t ? Di do  Ct ? subset(Ci,j , t) // candidates contained in t for all candidates c ? Ct do  c.count? c.count + 1 end for  end for Fi,j = Fi,j ? {c ? Ci,j | c.count ? minsupp }  end for end for  4 Performance Analysis  Our algorithm uses a fixed communication pattern. This is the greatest advantage of our proposal.

In the worst case, D1 is D. This is the case when ev- ery transaction from D contains 1, the firs frequent item.

Anyway, only in the first step, D1 is entirely scanned. In the k-th step, the transactions from D1, which have all the items less than k + 1, are skipped.

The processor Pi searches for new frequent itemsets only in the transactions which contain the item i. Also, in the k- th step, Pi skips in Di all the transactions for which the maximum item is less than k + i.

If the transactions are previously ?cleaned? of not fre- quent items, Di, i = 1, . . . n, can be easily extracted from D. Having Di for every processor Pi, a sorting algorithm can be applied now in order to obtain the reverse lexicogra- phy order of transactions from Di.

Example: After sorting in reverse lexicography order D1 resulted from the example presented in Figures 1 and 2, the transactions order will be as follows:  T1 = {1, 2} T3 = {1, 4} T8 = {1, 4, 5, 6, 7} T2 = {1, 2, 3, 5, 9} T7 = {1, 3, 5, 7, 9} Now, at the k-th step, Pi can easily skip in Di all the  transactions which have the maximum item less than k + i.

The problem remains load balancing. The processor Pi  works a step more than the processor Pi?1. In order to improve the processors? usage, a two rows mesh commu- nication topology can be used, so that when ?n2 ? proces- sors are free and the others ?n2 ? processors have to compute Fi,j , i = 1, . . . , ?n2 ?, j = i + k and k > ?n2 ?, every pro- cessor Pi can send a half of its work to P2?n2 ?+1?i [Figure 5]. This means that Pi will combine the the first half of set Cii,j?1 = {X |X ? Fi,j?1? i ? X} with the set Cji+1,j = {Y | Y ? Fi+1,j? j ? Y } and P2?n2 ?+1?i will combine the the second half of set Cii,j?1 = {X |X ? Fi,j?1? i ? X} with the set Cji+1,j = {Y | Y ? Fi+1,j ? j ? Y }. Then, they will search in Di to compute the support of the ob- tained candidates. They will skip in Di all the transactions which have only items less than j. This kind of work can start in the step ?n2 ? and can continue to the end.

The idea presented above (splitting the task between pro- cessors) can be applied recursively. As result, the most ad- equate architecture for load balancing is a mixture of linear array (chain) and hypercube (Figure 6). Let us suppose that, at one moment, p = ? n  2k ? processors are busy and n?? n  2k ?  processors are free. At this point, every busy processor Pi (i = 1, . . . , p) will send a half of its work to Pi+p. Then, the procesor Pi will send half of its work to the processor Pi+2p and he procesor Pi+p will send half of its work to the     F1,10 F2,11 F3,12 F4,13  P1 P2 P3 P4  P16 P15 P14 P13  F5,14 F6,15 F7,16 F8,17  P5 P6 P7 P8  P12 P11 P10 P9  Figure 5. Two rows mesh processors commu- nication topology  P1 P2 P3 P5 P6 P7 P8 P9 P11 P13P14P15P16P10P4 P12  Figure 6. Mixture of chain and hypercube  processor Pi+p+2p and so on. Thus, in order to transfer a part of its work, the procesor Pi needs a direct link-up to the processors Pi+2kp, k ? 0. The architecture from Fig- ure 6 provides a chanel for every comunication implied by the work spliting step.

5 Conclusion and Future Work  The idea of our new FIM parallel algorithm differs fun- damentally of the well known Apriori algorithm. Our algo- rithm determines, step by step, the frequent itemsets, by en- larging the interval to which the individual items belong to, unlike the Apriori algorithm, at the beginning of every step, increases the dimension of the new frequent itemsets by 1.

Our algorithm uses a communication pattern known before algorithm start. This is a very important issue because it allows hardware implementation for the processors? com- munication pattern. Another major advantage is that the set of the transactions can be distributed to processors prior to the beginning of the analysis.

In order to compare our algorithm with other FIM algo- rithms, we will use a MPI framework, which is in the final stage of implementation. At the end, a grid service for as- sociation rule discovery problems will be carried out.

The processors? workload allocation can still be im- proved. This is one of the future objectives. The distribu- tion of the transactions between the processors will be also in focus.

6 ACKNOWLEDGEMENT  The Excellence Research Program, through grant 74 CEEX-II03/31.07.2006 ? ?Academic Grid for Complex Applications (GRAI)? has supported the research for this paper.

