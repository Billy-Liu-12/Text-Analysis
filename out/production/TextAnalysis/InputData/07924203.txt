

Nonlinear Discrete Cross- Modal Hashing for Visual- Textual Data  Dekui Ma Dalian University of Technology  Jian Liang and Ran He Chinese Academy of Sciences Institute of Automation  Xiangwei Kong Dalian University of Technology  Discrete cross-modal  hashing is a  supervised method  that exploits  classification tasks to  learn heterogeneous  binary codes. DCMH  also updates the  binary codes for each  modality and learns  discrete hashing  codes bit by bit,  making it promising  for large-scale  datasets.

H ashing is an effective technique  for approximate nearest-neigh-  bor search. Because hashing  methods have low storage costs,  they?ve drawn considerable attention in the big  data era, with numerous methods being pro-  posed in the past few years.1,2 Traditional hash-  ing methods focus on homogenous data forms.

However, the ever-increasing amount of multi-  media data on social websites and mobile appli-  cations are naturally surrounded by textual  information, including descriptions, tags, and  user comments. To capture these heterogene-  ous image and text modalities, researchers  have proposed numerous cross-modal retrieval  methods.3?5 Furthermore, the binary codes  for cross-modal retrieval?that is, cross-modal  hashing?have been exploited to meet the needs  of storage usage and training time.6?8  Most cross-modal hashing methods focus on  how to design hashing functions to preserve  data similarities in the Hamming space (see the  ?Related Work in Cross-Modal Hashing? side-  bar for more information). However, these  approaches typically relax the binary con-  straints to simplify the optimization process,  thereby degrading retrieval performance.

Inspired by unimodal hashing methods,2 we  developed a discrete hashing method for cross-  modal retrieval called discrete cross-modal hash-  ing. DCMH employs an iterative optimization  method to learn hashing functions without  relaxing the discrete constraints. We formulate  the objective function by reconstructing the  semantic intersimilarity matrix and regard the  learned binary codes as ideal features for intra-  modal classification. To simplify the optimiza-  tion process, DCMH uses linear regression to  form both hashing functions and the classifica-  tion matrix. To address the NP-hard binary opti-  mization problem, we apply the discrete cyclic  coordinate descent method.2 The overall objec-  tive function consists primarily of two intramo-  dal hashing functions and one intersimilarity  reconstruction term; the intramodal hashing  function primarily relies on binary features clas-  sification-error criterion.

Here, to show the effectiveness of our hash-  ing model and optimization methods, we  describe the traditional relax-and-threshold sol-  ution (dubbed DCMH rat) and compare it with  DCMH (see the sidebar for more on the relax-  and-threshold solution).

This article expands on our previous confer-  ence paper9 as follows. First, we provide a relaxa-  tion solution with our objective function and  compare it with the formerly proposed discrete  solution to further verify the advantages of the  proposed objective function and the benefits  brought by discrete optimization. Second, to fur-  ther show the effectiveness of our proposed  methods, we add two novel large-scale datasets,  including a multilabel dataset, to the experi-  ments. Finally, we evaluate the intramodal  retrieval performances?that is, image-to-image  and text-to-text?to prove our cross-modal mod-  el?s generalization abilities.

Discrete Cross-Modal Hashing In this section, we explain the proposed  method and describe the associated optimiza-  tion algorithm.

Problem Definition  For simplicity, we assume here that there are  only two modalities, but DCMH can be easily  1070-986X/17/$33.00?c 2017 IEEE Published by the IEEE Computer Society  Multimedia Capturing, Mining, and Streaming     extended to more. Assume X ? fxigni?1; xi ? fx1i ; x2i g represents n data points of two different modalities, where x1i 2 Rm is an m-dimensional image feature, and x2i 2 Rd is a d-dimensional text feature vector. Given the code length k, our  goal is to learn hashing functions fq(?) that map the original continuous features x  q i to binary  codes h q i 2 f?1;1g  k; q ? f1;2g. Here, for each modality, we adopt the simple linear hashing  function fq??? ? sgn?WTq x?, where matrices Wq  are the projection matrices that we need to  learn. Y ? {0,1}c?n denotes the label matrix and  yi ? R c denotes the ith label vector, where c is  the number of semantic categories in the  dataset.

Intermodality Similarity Preservation  Unlike previous unified binary-code-based  methods,10,11 we used two binary matrices, H1 and H2, each of which represents a separate  Related Work in Cross-Modal Hashing Existing cross-modal hashing methods can be categorized  as unsupervised and supervised methods. One classical  unsupervised method extended spectral hashing to the mul-  timodal setting by minimizing the weighted distance.1 Gui-  guang Ding and his colleagues used collective matrix  factorization for different modalities to obtain the hashing  functions with latent a factor model.2  Supervised methods usually achieve much better per-  formance because they use semantic labels or pairwise rela-  tionships to learn the discriminative hashing functions via  label-similarity preserving criterion.

Jingkuan Song and his colleagues considered the differen-  ces between each modality by exploring single modality cor-  relations and keeping the different modalities? codes  consistent.3 Other researchers proposed maximizing the  semantic correlation and further optimizing the objective  function in a greedy way for large-scale datasets.4 In addi-  tion, Yueting Zhuang and his colleagues used neural network  models for cross-media hashing,5 while Xiaobo Shen and his  colleagues exploited matrix factorization for multiview  data.6 Recently, Dekui Ma and his colleagues proposed a sim-  ple two-step approach and obtained impressive retrieval per-  formances on various benchmark datasets, where the binary  codes obtained via unimodal hashing methods were consid-  ered as unified codes for both modalities.7  In addition to data similarity preservation, quantization  qualities are also crucial for hashing-based retrieval methods, as  proven in the classical unimodal hashing papers.8 Similar to  the unimodal hashing methods, cross-modal hashing  approaches have inevitable binary constraints, which make the  objective function challenging to optimize. To make the opti-  mization problem feasible, most hashing approaches adopt a  two-step relax-and-threshold strategy: first, they learn real hash-  ing functions to relax the constraints, and then they threshold  them to obtain the discrete codes. However, this trick brings  nonnegligible quantization errors, and is thus suboptimal.

Recently, many research efforts?including the classic iter-  ative quantization (ITQ) method?have aimed to minimize  quantization.9 By introducing a rotation matrix, ITQ mini-  mized quantization errors and thus obtained better hashing  projection matrices. By introducing an auxiliary variable for  discrete codes, supervised discrete hashing (SDH)10 reformu-  lated the objective function and obtained an efficient dis-  crete solution via cyclic coordinate descent. Work by Go Irie  and her colleagues was pioneering in its focus on quantiza-  tion errors for cross-modal hashing.11 Their efforts sought  binary quantizers for each modality by simultaneously mini-  mizing the binary quantization problem and subspace  learning.

