An Algorithm for Classifying Articles and Patent Documents Using Link Structure

Abstract  Studying link structure of the World Wide Web (WWW) is an area which has attracted a lot of interest in recent times.

Several papers have been published on structural analysis of hyperlinked environments such as the WWW. The WWW can be modeled as a graph and valuable information can be derived by analyzing links between the web-pages primar- ily for the purpose of building better search engines. Many novel methods have been presented to discover communities from the WWW and discover authoritative web-pages. Ci- tation analysis is a branch of information science on which plenty of research has been done. Citation analysis per- tains to analysis of articles and research paper citations in a scholarly field and deriving useful information from it. It has primarily been used as a useful tool to quantify and judge the impact of a paper or a journal. The work pre- sented in this paper lies at the intersection of the two fields: structural analysis of WWW and citation analysis. In this paper, we present a method for classifying documents (such as articles and patents containing references) to a class or topic based on their link structure, references and citations.

The method consists of analyzing the link structure of a corpus to first identify authoritative papers and assigning a class label to them. The class labels are assigned man- ually by a domain expert by going through the respective documents. The next step consists of identifying related pa- pers to the authoritative papers using citation analysis. The authoritative papers, their class labels and their related pa- pers constitute a model. Papers for which class label needs to be determined are classified based on the created model.

1 Introduction  Organizations like universities, government research labs and corporate research labs produce intellectual prop-  erty (IP) assets in the form of research papers and patents.

Often, a collection of all or a subset of the published pa- pers and patents (applied or granted) by an organizations needs to archived and analyzed for a variety of reasons. For example, a government research lab would like to analyze its patent portfolio to gain insights about the areas where it has filed a large number of patents versus the ones where not much patent activity has happened within the organi- zation. Another example can be a case where a corporate research lab would like to prepare a comparison table be- tween itself and its close competitor in terms of the num- ber of paper publications across variety of research areas.

Organizations perform research paper and patent portfolio analysis for gaining useful insights on its intellectual prop- erty. Such insights acts as one of the inputs in coming up with a plan for driving its research or preparing a research agenda. One of the fundamental tasks, while analyzing a paper or a patent portfolio is to first assign topics or subject areas to each of the document (paper of patent). A bar chart or table can be generated to plot the number of documents belonging to a specific area once topics and sub-topics are assigned to each document. However, assigning topics and defining a taxonomy is subjective and can vary depending on the context or perspective of the user. For example, a research paper on association rules mining can be assigned a topic as ?association rule mining? or ?data mining? where association rule mining is a sub-topic within data mining.

Authors of papers and patents can define a meta-data and as- sign class labels to each document. A predictive model can be built using pre-classified documents (training data) and new documents can be classified by applying the predictive model. The process of building a predictive model using pre-classified documents falls under the category of super- vised learning. The limitation of this model is that the class labels are the ones assigned by the author of the documents.

Quite often meta-data of documents is not available and if it is available then it can be at different granularity levels. We propose an algorithm that determines classes from a docu- ment corpus and build a model that can be used to classify   DOI 10.1109/WAIM.2008.31    DOI 10.1109/WAIM.2008.31     new documents to one of the identified classes. Such an algorithm can be useful in scenarios where there is a col- lection of documents without any class labels assigned to them by the authors and there is a need to determine major topics within the document collection as well as assign new documents to one of the discovered topics in the corpus.

An unsupervised classification of papers and patents with respect to document features is necessary for quick and accurate retrievals. Papers can be classified using vari- ous techniques which utilize different fields in a paper such as title, abstract or content or combinations of these. The problem with approaches which depend solely on content of the papers is that sometimes two papers may use several similar terms but used but the meaning or semantics can be quite different [1]. This is called as semantic gap. In order to address this, we can make use of citations or references present in a paper and a patent document. A research paper usually involves references to related papers of the same or related field of research and in turn these referred papers also refer to some other papers and so on, forming a linked graph structure very much similar like WWW. It is likely that the papers which cite same set of papers or cited by same set of papers are related to similar filed of research.

The link based structure of citations and references can be utilized to classify a paper. Analyzing the link based struc- ture by using some of the well-known algorithms for graph environment[2, 3, 4, 5]; we can build a predefined classifi- cation model. We have used Google?s PageRank Algorithm [6] for finding out some of the authoritative papers (papers which have received highest number of citations). Rest of the papers are then compared with these authoritative pa- pers and classified. The authoritative paper along with re- lated papers forms a community which can be later used as a model for classification of new papers.

Such a system will be very helpful to scientists and re- searchers in organizing, searching (reducing search time by restricting the search to specific topics of interest with improved accuracy), trend analysis of papers and topics.

This solution applies to classification of scientific docu- ments containing references and citations or patents. This does not apply to general documents which are not linked to other documents though citations or references.

2 Proposed Algorithm  The proposed algorithm for predicting or assigning class label to an unseen document is a two phase process. Phase I consists of building a model using a corpus (training data), which is a collection of academic publications or patents which have references. Model building is a multi-step pro- cess that makes use of techniques such as PageRank algo- rithm for determining authoritative pages, using co-citation analysis and bibliographic coupling for finding papers in the  Figure 1. Input and output tables for the PageRank computation module  Figure 2. Input and output tables for the Au- thoritative Page Computation module  training data that are similar to the authoritative papers. The trained model is then used to predict community member- ships for new documents in the second phase (called as scor- ing). Model scoring also makes use of co-citation analysis and bibliographic coupling as similarity measures between the paper that needs to be classified and the papers in the trained model. The following subsections describe the al- gorithm in detail for model building and scoring.

2.1 Phase 1: Model Building  Model building is a four step process. Step 1, as il- lustrated in Figure 1, consists of identifying authoritative papers using the popular Google?s PageRank algorithm[6].

PageRank algorithm is a method to assess the importance of a web page on WWW based on the pages linking to it     as well as their importance. The fundamental idea behind PageRank can also be applied to the references and cita- tion table (set of documents having links within the set and outside the set) obtained from a corpus of papers. One pa- per citing another paper can be regarded as one web page linking to another web page. Similar to the WWW envi- ronment, citing a paper affirms the importance of the paper referenced. As shown in Step 1 of Figure 1, the result of ap- plying PageRank algorithm to the table containing papers and its references is a table consisting of a column assign- ing a numeric value to each paper. The assigned value is the measure of importance of the paper. The popular way of implementing PageRank is as an iterative procedure which requires iterating through collection of documents for com- puting the PageRank values[7]. The formula used in our im- plementation is Equation 1 from Sergey Brin and Lawrence Page[6]. In Equation 1, PR(A) is defined as PageRank of Page A, and C(A) is defined as the number of links going out of page A. Equation 1 for PageRank has a parameter called as the Damping Factor(d)[8]. For experimental pur- poses, results are shown with various Damping Factor and Iterations combinations.

PR(A) = (1 ? d) + d (PR(Ti)  C(Ti) + ... +  PR(Tn) C(Tn)  ) (1)  We consider the top-n PageRanked papers across vari- ous combinations of Damping Factors and Iterations as au- thoritative papers. Step 2, as illustrated in Figure 1 uses the PageRank values calculated in Step 1 and extracts the top-n as authoritative papers. Some of these Authoritative Papers are merged with others if they show some good cor- relation as explained in later part of the paper. These au- thoritative papers are listed a good number of times as cita- tions of many papers (i.e. these papers have many inbound links) and can be used to represent a ?Domain? or a ?Class?.

These Classes (predefined categories) are used for the pur- pose of document classification or categorization. The class labels to the authoritative papers are manually assigned by a domain expert. In our approach, the model building pro- cess is unsupervised in the sense that the number of classes and labels are not predefined. The various topics to which authoritative papers belong become the class labels.

Step 3, as illustrated in Figure 2 consists of computing the similarity of each paper with the identified authorita- tive papers. To find similarity between two documents, we use Bibliographic Coupling and Co-citation. Bibliographic coupling uses bibliographic citations for the task of text classification. A paper or patent will refer to another pa- per if they have some commonality in topic or issue they represent. The more common the bibliographic citations be- tween two documents, better confidence we have in saying those two documents represent the same issue (closeness in  domain) [2]. Similarly, Co-citation uses citations received by two documents as a measure of similarity [3]. If two or more documents are cited by same set of documents, then it is more likely that these documents deal with similar issue or domain.

Similarity measure of a document with an Authorita- tive document is a weighted summation of its Co-citation and Bibliographic coupling scores with respect to Authority document. Illustrations of such a measure is shown in Table 1. In Table 1, the similarity measure of Query documents (Shown as Columns) with Authority documents (Each row represents one Authority Document) is computed.

Next step in model building as shown in Step 4 of Fig- ure 2 is finding similar documents with the Authority nodes representing Classes or Domain and their respective degree of similarity. The degree of similarity is used from Author- itative similarity table shown in Step 3 of Figure 2. Pa- pers above a threshold value of similarity are papers re- lated to respective classes. These classes along with their related papers and degree of relatedness form a model. The model will consists of different classes and these classes will in turn contain an Authoritative document, the docu- ments which are members of the cluster or class. The re- sulting model can be used to classify new documents with little human intervention and expert knowledge.

2.2 Phase 2: Model Scoring  After the document classification model is built, we can classify a new unknown paper to be of a particular commu- nity or domain or class using this model. To classify a paper using the model, the list of references and citations for the new paper is extracted. Then, using Bibliographic Coupling and Co-citation, the new papers similarity (score) with each class i.e . Authoritative paper and its cluster members is computed. These scores can then be combined using a av- erage weighted sum function to calculate the final score for that community. After the similarity scores for each of the class has been calculated, we can assign the document to that particular community which showed maximum simi- larity. Such a system enables scoring (prediction) without analyzing the actual content of the paper. The algorithm works better with patents and paper citations, as the graph noise is less compared to WWW (i.e. the noise references are very less if any).

3 Experimental Results  The dataset used for our experiments was from KDD- 2003 cup (Knowledge Discovery in Databases Contest, 2003) [9]. The dataset has publications in ?Theoretical high energy physics? (hep-th). It is a two column dataset. The first column represents the Document ID of a publication,     Figure 4. Analysis to decide on Number of iterations need for PageRank to converge  0.3  0.6  0.85  0.9 9204099(1)  9410167(35)  9906064(19)  9610043(36)  9802150(32)  9802109(28)  9711200(53)  9402002(28)  9510017(60)  9402044(42)  9207016(24)  9205068(25)  9503124(60)  9407087(60)  9908142(15)  9201015(22)  9408099 (60)  D a  m p  in g  F a  ct o  rs o  f S  co ri n  g F  u n  ct io  n s  o rd  e re  d b  y It  e ra  tio n  s  Paper Ids from the union  Distribution of Top-10 Ranked points.

The data points  Figure 5. Simulation results  while the second column represents the Document ID cited by the document in first column. Each row forms a citation instance. A directed graph is constructed from this dataset, with edges from Document ID in the first column towards Document IDs in the second column. Each of the document is represented as a node in the graph and is referred by its Document ID. An edge in such a graph refers to a citation.

For example, as shown in Figure 3, a directed arc from node ?A? to ?B? represents citation received by paper ?B? from ?A?.

In Figure 3, the number of citations received by paper ?B? is 2.

The total number of publications available in KDD-2003 dataset are 27,770 published during 1992 ? 2003. Details on the characteristics of the data is available at [10].

We used PageRank algorithm to determine authoritative papers. The first step towards model building consists of finding the PageRank of each paper so that authoritative pages can be determined. The amount of time taken by     Figure 3. Sample citation graph  PageRank algorithm to converge to a stable value is a func- tion of the number of edges in the citation graph [11]. As the size of the citation graph (number of nodes and edges) increases, PageRank algorithm needs more time per itera- tion to converge to final values. To reduce the time of con- vergence, we pruned the graph to remove the nodes with indegree 0 along with their corresponding edges. Nodes with indegree 0 are documents which are not cited by any of the documents. e.g. Node E in Figure 3. This lead to removal of 4,586 nodes, resulting in 23,184 nodes. How- ever, the nodes with indegree 0 may cite other documents.

Removal of such nodes will have an effect on the cited pa- pers in terms of their PageRank value. The PageRank of the cited paper will decrease as a result of removal of a paper that cites it. In order to offset or compensate the loss in the PageRank value, the graph needs to be updated such that the weights contributed by an indegree 0 node are added to the corresponding cited node. The formula we used is as follows:  w?i = wi + 1 ? d N  (2)  wi = weight of node i cited by node with indegree 0 w?i = updated weight of wi d = damping factor of PageRank algorithm N = no. of documents cited by node with indegree 0  In our experiment, we found that for the given dataset, the change in final Page Rank order with and without com- pensation of weights is negligible. We noticed that even though the PageRank values may differ a little but the or- der or rank remained the same. In our model, the purpose of applying PageRank algorithm to the citation graph is to determine authoritative pages and we do not use the actual PageRank values during subsequent steps. Hence in our ex- periments, we removed the nodes with indegree 0 without compensating the loss of weight caused by the removal and saved time during model building.

As given in Equation 1, the PageRank algorithm is a function of damping factor. To determine the number of iterations needed for PageRank algorithm to converge, we experimented with various damping factors (d). For prede-  fined number of iterations, PageRank of all the documents is calculated and sorted in descending order. We call the top k documents of this sorted list as snapshot for itera- tion i, denoted as Si. Let S denote the set of all such Si (S = {S1, S2, . . .}). The measure which we used to test the convergence is the variance shown in top ? k ranked documents in terms of their frequency of occurrence over various iterations. If document dj is found ndj times in S, then variance for a particular iteration i is as follows:  vi = ?|S|  i=1 ndj k|S| , (3)  |S| = number of elements in S i.e., no of iteration snapshots and 1 ? ndj ? |S|.

Equation 3, can be expressed in matrix form as Equation 4. In Equation 4, D is occurrence matrix(Ixn) and F is frequency matrix(nx1). Let Fi denote elements of matrix F i.e., ith row of F . The variance matrix is defined as  V = 1?n 1 Fi  (D.F ) (4)  In Equation 4, element Di.j = 1 if and only if document dj is present in Si. Element Fi = p if document di occurs p number of times in set S.

Figure 3 is a consolidation of four 3D plots of damp- ing factor on X-axis, number of iterations for PageRank on Y-axis and variance in % on Z-axis. Each plot consider dif- ferent number of top ? k documents. We used values for k = 10, 20, 40 and 50. We used damping factors 0.3, 0.6, 0.85 and 0.9. The number of iterations vary from 1 to 200. The Z-axis is the measure of the variance calculated using Equa- tions 3 or 4 as a percentage. The graphs in Figure 3 clearly highlight that the variance stabilizes after 10?th iteration for all damping factors. We concluded that the optimal number of iterations for the PageRank algorithm to converge on our experimental dataset is 10 for various damping factors over various k values.

Authority nodes are then determined as top ? k nodes with high PageRank values after sufficient iterations. For all our experiments, we have taken k value as 10. We con- sidered the top 10 ranked documents for all values of damp- ing factors and snapshots. The resulting set consists of 17 unique documents. These 17 documents are treated as Au- thority documents.

Figure 5 shows the distribution of these 17 authoritative documents. Various damping factors are represented on Y- Axis. Range between each of the damping factors (on Y- Axis) is in turn divided into 15 sub divisions representing iterations 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 40, 60, 80, 100, 200.

For example, the presence of a ?+? mark just above damp- ing factor say d, indicates the presence of the document with ID as on X-axis among the top 10 page-rank documents     retrieved from snapshot taken after 2 iterations for damp- ing factor d. So, the Document with ID 9711200, which is found 53 times over all the iterations and damping factors among total combinations of 60 which are spread over 15 iterations and 4 damping factors, is occurring in all the 15 iterations with damping factor 0.3 and 0.6. While the paper with IDs 9402002, 9207016 and 9201015 are seen gaining good page ranks only after first few iterations, some papers like those with Ids 9205068 is seen losing its place in top 10 with iterations.

As it is seen that the PageRank converges in as many as 10 iterations in hep-th dataset, 10 iterations are enough to get a near approximate of PageRank over the hep-th dataset.

The scoring function with damping factor 0.6 seemed work better on the experimental dataset. It shows on average, less error on PageRank over iterations.

Once we obtain authoritative papers, the next step to- wards model building is to determine papers from the dataset that are similar to the authoritative papers. We used combination of two popular similarity measures: Biblio- graphic coupling [2] and Co-citation analysis [3].

We have used following similarity measures:  ? For Bibliographic Coupling, if dp is bit vector which has its i?th bit set to 1 if it refers document i, and if dp.dq represents the scalar product between vectors dp and dq ,  ?bc(dp, dq) = dp.dq dp.dp  + dp.dq dq.dq  (5)  ? For Co-citation, if d?p is bit vector which has its i?th bit set to 1 if document i refers document p.

?cc(dp, dq) = d?p.d ? q+??log  ( d?p.d?q d?p.d?p + d?q.d?q ? d?p.d?q|  )  (6) where, ? is constant integer used to scale the penalty factor. We have added penalty function,  log (  d?p.d ? q  d?p.d?p+d?q.d?q?d?p.d?q  )  The document similarity scoring function as in Equa- tion 7 uses weighted normalized sum of these two similar- ity measures. In our experiments, we have assigned equal weights to Co-citation and Bibliography Coupling. Simi- larity of each of the authoritative papers is computed with all the papers in the experimental dataset. We then find the documents over a threshold ? (according to similarity score) which are most similar to authority nodes. For our experi- ments, as the number of documents are less in number and very close in domains, we have chosen top 5 documents in- stead of choosing documents over a ?. The authoritative papers along with these 5 papers form a model. The follow- ing are the formula for scoring functions:  f(dp, dq) = ?cc(dp, dq)  Max(?cc) +  ?bc(dp, dq)  Max(?bc) (7)  When a new document needs to be classified (scored), its similarity score with current authority nodes and its top- 5 similar nodes is computed. Table 1 shows the obtained similarity scores of a particular document compared with various Authoritative documents. Table 3 gives the list of 5 most similar documents with each of the Authority doc- uments. These scores are then used to determine the do- main to which the query document belongs. The similarity score is a combination of Co-citation[3] and Bibliography Coupling[2] scores as explained in Step 3 of Section 2.1.

The scoring functions with which we tested are simple av- erage scoring function (F1) and weighted average scoring function (F2) of similarity scores with each of the member of the cluster/domain.

