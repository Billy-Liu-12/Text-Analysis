A Hybrid Framework for Building a Web-Page  Recommender System

Abstract? Recommender systems aim to facilitate World Wide Web users against information and product overloading. They are usually intermediate programs that try to predict users' preferences and items of their interest. In this paper, we present a hybrid framework that uses open source information such as web logs in combination with social network analysis and data mining, to extract useful information about users browsing patterns and construct a recommendation engine. A case study based on real data from an organization of 250 employees is presented and a system prototype is constructed based on the results.

Keywords-component; recommender system; social network; data mining; association rules; system prototype; hybrid framework.



I.  INTRODUCTION It is widely accepted that World Wide Web has become the  main source of information for practically anything someone would like to know. This abundance of information, together with the advertisements and in combination with the various hyperlinks inherent to the webpages, has increased the information that is irrelevant to a user's interest on a topic.

Thus, finding information has changed into a time consuming and sometimes disturbing task for both novice and experienced users. To facilitate users in this quest, recommendation systems are implemented as an intermediate service between the user and the provider of the information, aiming to predict the user?s preferences and recommend items of his/her interest.

Recommendation systems usually draw data from the existing users of a website or service. These data are usually recorded into the log files of web and proxy servers, resulting into data sources that contain record entries that store the time and the URL of the web pages accessed by each user of the network. From these data, valuable information can be extracted about each user?s preferences, increasing the accuracy and quality of the recommendations presented.

This paper proposes a hybrid framework that uses web logs as data source and combines social network analysis with data mining, to increase the accuracy and the quality of web page recommender systems. The work presented is divided into the following four main phases, a) data collection, b) social network analysis, c) data mining, and d) implementation of the recommender system. Data is collected from various sources which may be open or proprietary and can provide information  about the time and the URL of the web pages a user has requested. These web logs are pre-processed to remove unnecessary or private data and integrate them into one data set that contains only the necessary information in the appropriate format for the social network analysis. The data set is then represented as social networks of users and web pages, and various measurements are performed and analyzed to conclude on the networks stability and the importance of certain actors.

The data set is then expanded and the same process is repeated until a sufficient amount of data has been analyzed. The framework continues with the data mining process that again pre-processes the web logs to transform them to the appropriate format for the application of data mining algorithms. The frequent item-sets mining results into frequent patterns of web pages access and are used as input for the association rules mining; this consequently results in a set of associations between web pages, based on which recommendations will be made by the recommender system. The construction of the recommender system starts with the preparation of the grouping of rules and the selection of the more accurate ones.

Finally, the recommender system is implemented focusing on the recommendation engine which is a system prototype that runs completely on the client's browser, differentiating from the usual approach that places the recommendation engine at a web or proxy server.

The remainder of the paper is organized as follows. Section 2 provides a brief overview of related work on web log preprocess approaches of mining web page navigational patterns and the types of recommender systems that are common in literature. Section 3 describes in detail the proposed framework, the problems and methods of the data preprocess, the measurements and algorithms that are applied, as well as the process of constructing the recommendation engine.

Following, in Section 4 the framework is applied on real usage data from an organization of 250 employees and results in a system prototype that is implemented as an extension of the Google Chrome web browser. The paper concludes in Section 5 with the future work that could improve the performance and accuracy of the recommender system.



II. RELATED WORK The most important problem in collecting reliable usage  data is caching, either from the users? browsers or proxy servers. This process is necessary when the objective is to  2011 European Intelligence and Security Informatics Conference  DOI 10.1109/EISIC.2011.40     minimize the traffic over the network and increase performance. As a result, web server logs do not include requests that were satisfied by locally cached web pages, or in the case of proxy server intermediation, all requests have the same identifier even though they correspond to various users.

Cooley, Mobasher and Srivastava [1] confronted this problem with   preprocessing of the logs, user identification and session identification.

Preprocessing of the logged data is necessary to remove records that are not actually relevant to the user browsing behavior. These records are HTTP requests that are implicitly made from the browser in order to complete the display of the web page. As the HTTP protocol requires each request to be a separate session, a record log is created for each one. Common solution to this problem which was also employed in the proposed framework is to remove requests based on suffixes, i.e. jpg, jpeg, gif, cgi, etc. Depending on the information of interest, the list of suffixes to be removed can vary.

User identification is another important task since requests of different users are logged in the proxy server as being made from the same IP address. This task is more complicated and the proposed methods in the literature rely on the cooperation of the user or on heuristics [1]. The user's cooperation is usually achieved by requiring login to a web site first which tracks the usage. Accepting cookies from a server is another form of user cooperation, as the user's browser will send the cookie with each new request, and thus by identifying the cookie the web server actually can identify the user. There are however serious drawbacks in this approach since the user may delete stored cookies or be negative to registrations, as privacy is in most cases of primary concern. Heuristics are mostly based on the assumption that different operating systems or web browsers at the same IP address indicate different users, but  two users with the same IP address that use the same browser on the same operating system can be easily be regarded as a single user. Another heuristic method is to combine the web log with the site topology. A web page that is not reachable through the links of the web pages already accessed by the user can be assumed that was requested by another user having the same IP.

Session identification is usually applied in usage logs that cover long periods of time, since a user may visit the same web page more than once during this period. Each time the user accesses the web site it is considered a new session and the aim of the method, is to divide the web pages the user has accessed to separate sessions. A common approach is to define a timeout, which in literature varies from ten minutes to two hours, after which it is assumed that the user starts a new session.

Coming to the construction of collaborative recommender systems, the major approaches in literature are the use of memory-based and model-based algorithms [2]. Memory- based algorithms use the entire data set that corresponds to the items each user accessed and is represented as a user-item matrix. In order to generate recommendations the k-nearest- neighborhood or association rules algorithms can be applied.

In k-nearest-neighborhood, the similarity between users in  item rating or accessed web pages is calculated in the user- item matrix and similar users form a proximity-based neighborhood. It is assumed that the items accessed by the user's neighbors will probably interest him/her and thus they are recommended.

Association rules are usually applied to ?market basket? data, meaning that each user transaction has an ID and the items accessed. Usually they are mined using the Apriori or FP-growth algorithms. These algorithms initially generate frequent item-sets, which are patterns that appear often in the transactions and then associations between these sets of items are derived [3]. Association rules are interpreted as if a user accessed items A, then he will probably access items B (A=>B). The strength of these rules is evaluated by their support and confidence. Association rules are used in the proposed framework to create recommendation, so they are presented in detail to the following sections.

The model-based collaborative filtering approach, aims to derive a model from the rating data that will be used in continuance to generate recommendations. This is achieved applying machine learning algorithms, such as neural networks, Bayesian networks, clustering and latent semantic analysis [4]. Each of these is covered in detail in data mining literature and they are not discussed in this work since they are not related to the proposed framework.

In recent literature, social network analysis algorithms are also combined with data mining, representing the collaborative relationships as social networks. These networks are analyzed in order to understand the relationships, between users and items of their interest, the collaboration among users, how they change in time and their reflections on their preferences [4]. Based on social network analysis, new friends or new professional contacts can be recommended. Merging social network analysis and data mining has proved to increase the quality of the data and as a result the efficiency of the recommender systems.

The proposed framework benefits from the available web log analysis methods to prepare the data for social network analysis and data mining. Social network analysis facilitates the identification of important web pages and users, representing the user usage as a network of interaction between the users and the web pages. For the generation of recommendations the memory-based approach of association rules mining is employed, as it can be applied efficiently on large data sets and the quality of the recommendations can be easily evaluated. The combination of different methods or approaches increases the complexity of the construction process, but significantly benefits from the advantages of each one which is the main objective of the proposed framework.

Social network analysis is usually performed on networks where people are the actors and by finding friends, communities or similarities among them, enhances the quality of the recommendations. The proposed framework differentiates from this approach, choosing web pages as actors of the social network. The users are also part of the approach as through their usage behavior the web pages are implicitly connected forming social networks. The users influence web pages networks adding the dynamic and evolutionary features     to these networks. The information extracted from the analysis of both users and web pages social networks leads to useful conclusions that are used to increase the quality of the recommendation engine.



III. PROPOSED FRAMEWORK  A. Data Collection and Preprocess The required data to work with the proposed framework  are the web pages that have been accessed by each user. These can be collected using the SNMP protocol, applications that monitor the traffic on a network such as tcpdump, argus, mrtg, ethereal and other packages or logged information from web and proxy servers. Initially, a data set is collected covering a small period of time, e.g., a few hours, and consequently this data set is extended to several days of network traffic.

Data preprocess is a necessary process to improve the quality of the data and as a consequence the results of link analysis and data mining [3]. Its input is the previously mentioned data and the output is data sets containing only the necessary data for the link and data analysis processes. The forms of data preprocess can be summarized to data cleaning, data integration, data transformation and data reduction. Data cleaning attempts to correct incomplete, noisy and inconsistent data. Data integration merges data from various sources, since network traffic can be recorded into flat files and database tables, while transformation brings them to the appropriate format for analysis and mining to be performed by the software tools. The data is then reduced, replacing each web page with a numeric value, making it is easier to handle and less demanding in storage.

The main problem with web logs is that both proxy servers and browsers cache web pages and that web browsers automatically request only new content in order to complete the display of a web page. This results in difficulties to identify the user and its behavior, in addition to the HTTP protocol that requires a separate connection for every requested file. There are processes available to overcome these problems at [1] aiming to user and session identification and including user path completion and formatting. The preprocessing to be performed depends on the actual data that are considered adequate in order to identify the user's behavior. If, for example, a user is accessing paintings on a web site, the jpg or gif requests should not be removed but in all other cases they would be removed as automatic requests of the browser to complete the display of the web page.

B. Social Network Construction and Analysis  The social networks are composed of nodes and links.

These nodes relate with other nodes through their links. The links can have a direction; link from node A to B is different from node B to A. A 2-mode network is represented with an incidence matrix, where a value indicates the presence of a link, a number if weighted or 1 if binary. This 2-mode network can then be folded to create two 1-mode networks, one for each dimension. To fold a network, it is first transposed to the  desired dimension and then multiplied with the initial incidence matrix, resulting to the adjacency matrix.

In our framework, the construction of the 2-mode network begins with a small data set, a few hours of network traffic.

These relational data are used to create the incidence matrix, which is a |V| by |E| array. The |V| array is the host IP addresses and the |E| array the web pages requested, when a host i requests a web page j the weight of the link is added to cell ij.

The two-dimension incidence matrices will be folded into both dimensions to form two 1-mode networks, with the respective adjacency matrices. The result of the |V| x |E| folding will be the |V| x |V| and |E| x |E| arrays, where each cell contains the weight between vi and vj , ei and ej , respectively.

The analysis of the two 1-mode networks aims to the identification of important nodes in each network, meaning important users and web sites. To measure importance the degree, closeness, eigenvector and betweenness centrality will be measured.

The degree centrality is the number of links that a node has and is distinguished into in an out degree, when the links are directed to or from the node, respectively. In our case, the constructed network is undirected, so there is no need to distinguish the in from the out degree; the total degree centrality of the nodes will be measured. Let G=(V,E) be the graph representation of a square network and a node v. The Total Degree Centrality of node v = deg / 2?(|V|-1), where deg = card{u?V | (v,u)?E ? (u,v)?E} [12]. A node with high degree centrality is well connected node and can potentially directly influence many other nodes [5].

Closeness centrality is the average geodesic distance of a node from all other nodes in the network, where geodesic distance is the length of the shortest path between two nodes.

Let G=(V,E) be the graph representation of a square network, then the closeness centrality of a node v?V is  v = (|V|-1)/dist, where dist = ? dG (v,i) ,  i?V, if every node is reachable from v and v=|V| if some node is not reachable from v ([6] as cited in [7]). The closest a node is to others, the fastest its access to information and influence to others [8].

Another measurement that is used to identify important nodes is betweenness centrality, which is defined for a node v, as the percentage of shortest paths, between node pairs, that pass through v. Let G=(V,E) be the graph representation of a symmetric network. Let n=|V| and a node v?V. For (u,w) ?VxV, let nG (u,w)  be the number of geodesics in G from u to w. If (u,w)?E, then set nG (u,w) =1. Now, let S= {(u,w) VxV |dG (u,w)=d (u,v)+ dG (v,w)} and let between = ?( nG( u,v )?nG (v ,w )) / nG (u ,w ), (u ,w?S), then the betweenness centrality of node v = between / ((n-1)(n-2)/2) ([6], as cited in [7]). A node with high betweenness is important because it connects many nodes and a possible removal would affect the network.

The last node level measurement that is applied is the eigenvector centrality. It is a measure of the node's connections with other highly connected nodes. It calculates the eigenvector of the largest positive eigenvalue of the adjacency matrix representation of the square network. To     compute the eigenvalues and vectors a Jacobi method is used ([9], as cited in [7]). The nodes with high eigenvector centrality can mobilize other important nodes [8].

Apart from the node level measurements it is important to analyze the networks, from a network level perspective. The measurements that are applied in the proposed framework are density, fragmentation, component count, isolate count. These measurements are very useful as they describe the network as a whole [7], which in combination with the node level measurements provide us comprehensive information about the networks' cohesion.

Fragmentation measures the proportion of the network's nodes that are disconnected. Let an undirected network, G=(V,E) with n = |V| and sk be the number of nodes in the kth component of G, 1 ? k ? n , then Fragmentation = 1 ? (?sk ( sk ?1)) /n(n ? 1)  ([5], as cited in [7]).

Density is the ration of the number of links, existing on a network, versus the maximum possible ones. For a network with adjacency matrix M and dimensions m x n, the density is calculated by the form Density = sum(M)/(m?(m-1)), if the network is unimodal and Density = sum(M)/(m?n), if the network is bimodal Density = sum(M)/(m?n) ([12], as cited in [7]).

Following the identification of important nodes in the networks, the proposed framework continues with the removal of the top valued ones. The nodes are sorted with descending order and we start removing the top nodes one-by-one, repeating the measurements after each removal. This process is repeated for each measurement and it aims to observe how the network is affected from the removal of each node. The removal of a node is an exogenous impact, a shock for the network, whose results to the network's dynamics and cohesion are observed. A network may remain stable, the links between the nodes do not change, or mutate, initiate an evolutionary process [10], or its cohesion may change increasing the network's fragmentation and components [11].

The next phase in the proposed framework is to extend the data collection to the period of one day. This data set is divided to characteristic time periods and is further analyzed, repeating the measurements previously presented.

The data set is divided to twenty-four subsets, one for each hour of the day. The node and network level measurements are calculated for each network and the results are compared in the dimension of time to identify its dynamics, whether it is stable or not, whether it remains coherent or not. Drastic changes are obvious in social networks, whilst small are difficult to detect. This makes CUSUM charts suitable for social networks, as they perform well in detecting small changes over time and also provide detection of the point the change occurred [10]. The CUSUM control chart compares sequentially the statistic Ct against a decision interval until Ct > A'. Since one is not interested in concluding that the network process is unchanged, the cumulative statistic is Ct+=max{0,Zt - k + Ct-1+}. EWMA control chart and moving window analysis are also applicable, but in the proposed framework only CUSUM control charts are generated.

The analysis of a network on the time domain, may lead to erroneous conclusions because of periodicity. For example, a company meeting scheduled to take place at a specific day and time, every week, could mislead the analysis to identify a shock at that network. In the proposed framework the process to identify and handle periodicity analyzed in [10], is used, applying spectral analysis to the network's data on the dimension of time.

In continuance, the same data set is divided into two subsets, working hours and not-working hours, and the same analysis is applied. Finally, the same process is followed after dividing the data into four subsets, the working hours split into two and not-working hours split into two subsets. To conclude, the data collected are extended to six days and the same procedure is repeated. The output of this analysis helps us determine the cohesion and stability of the data. Patterns or specific time periods might need to be taken into account while mining for the association rules.

C. Data Mining  In this section data mining techniques are applied to the dataset. Depending on the results of the social network analysis, on the network's stability or patterns of usage, the logs can be divided by day or by specific time periods and the data mining process is repeated to each one of these subsets.

The process starts with the preprocessing of the data, continues with frequent item sets and association rules mining algorithms.

The transactional data, as previously prepared and used so far are in a multi-instance format as shown in Table I (a). In order to apply the data mining algorithms they need to be transformed to the single-instance format of Table I (b).

TABLE I: Multi - Instance (a) vs Single - Instance (b) Transactional Data   IP Webpages  IP Webpages  IP B 40  IP B 40  IP C 138  IP C 138, 139, 140  IP C 139  IP D 138, 139, 140  IP C 140  IP E 1  IP  D 138  (b)  IP D 139  IP D 140  IP E 1  (a)  An itemset in our case is a set of web pages. An itemset containing k items is a k-itemset, which has a support count that equals to the number of its occurrences in the transactions data set. When an itemset satisfies a minimum support count threshold, it is a frequent itemset, denoted by Lk. The mining of the frequent itemsets can be performed applying Apriori or FP-Growth algorithms. Apriori is a seminal algorithm that scans the data set to find frequent 1-itemsets and then joins them to generate candidate 2-itemsets. These candidate     itemesets are evaluated scanning again the data set and the iterations continue finding (k+1)-itemsets from previously known k-itemsets. Its drawbacks are that it may generate a huge amount of candidate sets and the repeated scan of the database, which is a costly transaction [3], [4].

In the proposed framework, the FP-Growth algorithm is preferred, as it is faster than Apriori and is suitable for large data sets. The algorithm applies a divide-and-conquer approach and consists of two steps, the FP-tree construction and the mining of the frequent itemset. To construct the FP- tree, a ?null? root node is created and then the data set is scanned to obtain a list of frequent items. These are ordered in descending order based on their support. Using this order, the items in each transaction of the data set are reordered, while each node n in the FP-tree represents a unique itemset X. All the nodes have a counter indicating the transactions that share the node, except for the root node. The algorithm scans the items in each transaction, it searches the already existing nodes in FP-tree and if a representative node exists the counter of the node is incremented by 1, else, a new node is created.

The support of each item is stored in a header table, while the same table is used for each item to point to its occurrences in the tree. This way the problem of mining large data sets for frequent patterns, has transformed to the mining of the FP- tree. The FP-tree mining starts from each frequent pattern of length-1 (initial suffix), constructing its conditional pattern base (the set of prefix paths in the FP-tree co-occurring with the suffix pattern), then constructs its conditional FP-tree and mines recursively this tree. The pattern growth is achieved concatenating the suffix pattern with the frequent patterns generated from the conditional FP-tree [3].

The above algorithms provide us with a set of frequent itemsets, which will be used as input for the association rules mining algorithm. An association rule is an expression A?B, where A and B are itemsets, of web pages in our case, and it is translated in if a host requested the web pages of A, then he will also request the web pages of B itemset. The support of rule is support(A?B)=support(A?B) and  it is the percentage of the transactions that this association rule appears, while the confidence of the association rule, confidence (A?B) = support(A?B)/support(A), is again a percentage that indicates the conditional probability that a transaction containing A will also contain B. The higher the support count and confidence is, the stronger the rule is. Both have to satisfy the thresholds that are set previously from the analyst [4].

The association rules are then subject to correlation analysis, since rules with high support and confidence may sometimes be misleading. In the proposed framework the lift correlation measure is measured for each of the resulted association rules. Let an association rule A?B, the lift corresponds to lift(A?B) = P(B|A)/P(B), or lift(A?B) =confidence(A?B)/sup(B). The numerator is the likelihood of a host requesting both, while the denominator is what the likelihood would have been if the two visits were completely independent. Values greater than one indicate positive correlation between the two itemsets, values less than one indicate negative correlation and values equal to one indicate  independence of A and B [3]. Correlation analysis will output A?B [support, confidence. Lift] strong association rules to be used for the recommendation engine.

D. Recommender System Construction  In this section the association rules which were produced from the data mining process, are used for the construction of the recommender system. The association rules whose Lift is negative or 1, are discarded as they indicate negative correlation or independence of the itemsets, respectively.

From those with positive Lift value, some will be selected based on their support or confidence. High support indicates that the rule appears often in the transactions data set, thus high confidence indicates increased probability for the consequence of the rule to appear together with the antecedent. All association rules satisfy the support and confidence thresholds that were set during the mining process, so they can all be used for the recommendation system, but depending on their values and their total number, the rules may be further filtered to reduce their multitude and keep the stronger ones. For example, if all rules have high confidence, then they can be sorted based on their support, to choose the highest valued ones and discard the others. In this framework we selected the rules with high confidence since the aim is to provide accurate recommendations to the user.

The set of association rules that will be finally used for the recommendation engine is then grouped based on the number of items in their antecedent. The result is a group of rules with one item in the antecedent, a group with two items and so on. The recommendation system captures the user's request, searches the one-item antecedent for a match and, if found, recommends the items in the consequence. When two web pages accessed by the user are known, it searches the one-item group for the second web page and in continuance searches the two-items group for both consequent requests.

When no match is found the system erases the user's browsing history and starts tracking it again when a match is found.

Following, the recommender system can be implemented either to run on a web or proxy server, or on the client. In the first approach, a database server is also needed to store the user IP and the respective web pages accessed as well as the association rules. In the second approach which was chosen for the system prototype, there is no need to track the different users, but the recommendation engine and the association rules need to be installed on each host.



IV. SYSTEM PROTOTYPE The proposed framework was applied to real data collected  through the log of a Microsoft ISA server used for Internet access from an organization of 250 employees.

The recommender system was implemented as an extension for the Google Chrome web browser, using JavaScript and the Chrome API. The extension's architecture is depicted in Figure 1 and has four main components:      Figure 1: Extension?s Architecture  1) manifest.json: contains the necessary information for the installation of the extension.

2) contentscript.js: gets loaded when a new browser window or tab opens and gets executed each time a request is performed.

3) backgroundpage.html: runs in the background and implements the recommendation engine.

4) popup.html: pops up when the user clicks on the extension's icon and displays the recommendations.

The above components can be installed in a single directory and then a browser action icon is displayed. When a user opens the web browser the contentscript.js is loaded and executed each time a URL is requested. It sends a message to the background.html page calling a Chrome API function. The background.html page has a listener that waits for messages from the content script. When the message is received the listener calls a JavaScript function that implements the recommendation algorithm. It then captures the requested URL and searches in antecedent of the rule, and if the URL is found then it recommends the consequent of the association rule. This process runs in the background and when the user wants to present the recommendations, he/she just clicks on the browser action icon causing the popup.html to pop up and present the recommended URLs, Figure 2. Choosing one of the recommended URLs opens it in a new tab.

Figure 2: Snapshot of the Pop-up Page  The application runs only in the user's browser and monitors only the user?s usage data, so there is no need to distinguish between different users as in most recommender systems. The association rules are stored in a JavaScript file, rules.js, which can be easily replaced with an updated one. In this file, two JavaScript arrays are used to store the antecedence, (left[]) and consequence (right[]) of the association rules. The recommendation engine searches the left[] array for a match and if found, recommends the  corresponding value of the right[] array. The whole application is of small size, consumes the minimum resources and can be easily distributed and managed.



V. CONCLUSION A hybrid method for the construction of a web page  recommender system was presented that combines social network analysis and data mining to open source web usage data and results in the construction of a system prototype. The process started with the collection of the data which were then preprocessed and represented as social networks linking users and web pages. The analysis of the networks led to the identification of the critical users and web pages. This was achieved by separating the data to specific time periods and by analyzing and comparing various combinations of these data sets in the dimension of time. Data mining algorithms were then applied in continuance to mine association rules that were used for the recommendation engine; additionally, correlation analysis was performed to verify the strength of the rules.

Future work may include the extension of the framework to include content-based filtering, explicit ratings from the users and classification of the users according to their usage behavior and preferences. The Chrome extension could also be distributed and tested from a larger audience of users in order for the system to be further evaluated. The social networks constructed by the users and by the web sites could also be further analyzed to identify how they influence and affect each other. A step further at the association rules preparation could be taken, clustering the rules and ranking them based on the confidence of the recommendation, correlating the groups of association rules. The rules? clustering increases the search performance, which could be further examined in combination with the systems scalability.

