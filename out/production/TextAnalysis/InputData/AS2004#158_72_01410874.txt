<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">PEWeb:  Product Extraction from the Web Based on Entropy Estimation

Abstract Mining product descriptions (PDs) from e-commercial web sites is an important task in information extraction from the Web. In this paper, we propose an efficient technique for this task. The technique first discovers the set of PDs based on the measure of entropy at each internal node in the HTML tag tree. Afterwards, a set of association rules based on heuristic features is employed to filter the output and therefore enhance the precision. The experimental results of PEWeb system show that the proposed method outperforms existing automatic techniques remarkably.

1. Introduction With the explosion of the Web, a large amount of infor- mation has become available online. A majority part of this huge data repository is formatted in regularly structured ob- jects like records or PDs. A PD usually contains common fields such as name, manufacturer, price, etc. Automatic extracting PDs from the Web is useful for information in- tegration and therefore help users can easily locate desired products from a huge number of e-commercial web sites.

This task has several challenging problems, that are how to efficiently recognize product regions with in a web docu- ment, how to exactly split a product region into separated records, and how to get rid of noisy objects.

Several approaches such as wrapper-based, NLP-based, ontology-based, and HTML-based techniques have been employed to extract information from the Web. Our method is also a HTML-based technique that analyze the hierar- chical structure of the Web and automatically recognize PDs. Several typical HTML-based systems for mining data records are MDR [2], OMINI [3], and IEPAD [4]. OMINI uses a set of extraction algorithms to locate the smallest sub- tree that contains all objects of interest. Then, it employs a suite of object extraction algorithms to find the correct ob- ject separator tags that can separate objects. IEPAD pro- poses a method to find patterns from the HTML tag strings, and then uses the patterns to extract objects. However, both OMINI and IEPAD return many noisy objects because the diversity in separator tags and drawback of PAT tree in lo- cating match of patterns, respectively. MDR employs the edit distance string matching algorithm to recognize data regions containing a set of generalized nodes. Afterwards, MDR determines data records in each generalized node by using coarse-grained heuristic observations. The first draw- back of MDR is the computational complexity of the edit  distance algorithm. The second drawback of MDR is that its course-grained heuristic observations such as dollar sign are not enough for identifying true data records and, there- fore, results in many noisy objects.

In this paper, we propose a new, simple but efficient technique to automatically extract PDs from the Web. The    technique to automatically extract PDs from the Web. The method is based on the observation that PDs usually have similar display format and they are contained in similar HTML subtrees. To extract these PDs, the input HTML page must first be parsed to form a DOM-like HTML tag tree. Then, Shannon?s entropy estimation is performed at each internal node of the tree to measure the similarity among subtrees. Any node with high entropy value, i.e.

high similarity among its subtrees, should contain potential PDs in its descendant nodes. Finally, a set of association rules based on heuristic features is used to eliminate noisy objects. The experimental results of PEWeb system show that our system outperforms MDR dramatically in terms of precision. In addition, the simple representative value (RV) mapping allows fast extraction comparing to MDR system that uses edit distance string matching.

The remained part of the paper is organized as follows.

Section 2 presents three steps of the proposed technique.

Section 3 demonstrates experimental results and some dis- cussions. Finally, section 4 concludes the paper and states the future work.

0-7695-2100-2/04 $ 20.00 IEEE 2. The Proposed Approach HTML Tag Tree: HTML documents naturally have hi- erarchical and nesting structure of tags. Each input web page is parsed to build a HTML tree that is somewhat simi- lar to a DOM tree. Additionally, each tree node (i.e. HTML tag) contains other types of information such as tag weight (tw), representative value (rv), entropy ratio (er), etc.

Figure 1. Product descriptions &amp; tag tree Entropy Measurement: This section mainly describes entropy estimation to identify product regions containing potential PDs. This idea originates from the observation that similar PDs are usually contained in similar subtrees.

The term similar, in this sense, means that these subtrees have analogical structures in both subtree skeleton and tag location. Figure 1 shows that four PDs reside in very similar subtrees D, E, F , and G surrounded by four ovals. The essential problems is to measure the similarity among these subtrees. To do so, subtrees must first be mapped in to RVs that should satisfy two properties: (1) two subtrees have close RVs if they are similar in structure, otherwise they have different RVs and (2) the computational complexity for this mapping is as small as possible. We propose a simple mapping as follows. The RV of a subtree T (T is also the root node), denoted as T.rv, is calculated by the following formula, T.rv = T.tw + ?

Ni?N (Ni.tl ?Ni.co?Ni.tw) (1) where N is the set of all descendant nodes of T . Ni.tl is the tag level, i.e. the distance from Ni to the root node T . Ni.tw is the tag weight of the tree node Ni. The main usage of Ni.tw value is to help distinguish among different HTML tags. Ni.co is the child order (from left to right) of the node Ni among its siblings. This map- ping is simple (O(n)) and accurate enough to efficiently deal with the huge volume of web data. After mapping  RVs for all subtrees in the whole HTML tree, Shannon?s entropy is estimated at all internal nodes. Suppose a node T has a set of subtrees S = {T1, T2, . . . , Tn} with the set of RVs R = {T1.rv, T2.rv, . . . , Tn.rv}. Let P = {Ti|Ti ?

S and Ti.dp ? DTh}. The Shannon?s entropy of T w.r.t their children?s RVs, denoted as E(T ), is calculated as, E(T ) = ?

?

Ti?P Ti.rv?

Ti?P Ti.rv    Ti?P Ti.rv ln Ti.rv?

Ti?P Ti.rv (2) Our observation shows that PDs usually reside in sub- trees with the depth greater than or equal to a minimum depth threshold DTh. Thus, we use this threshold to re- move noisy objects. The E(T ) value in (2) is not normal- ized. This means that E(T ) tends to be high when the car- dinality of P is growing. Thus, we need to normalize this formula to get convenience for later comparison. In (2), Shannon?s entropy get its maximum value ln |P | when all RVs are equal. Therefore, we get the normalized value of E(T ) between 0 and 1, denoted as T.er (entropy ratio) as, T.er = E(T ) ln |P | (3) The algorithm for entropy ratio calculation is really sim- ple. It is omitted due to space limitations. Sometimes, one PD may span two or three adjacent subtrees. We easily rec- ognize this case because entropy ratio calculated on the set of every two or three subtrees tends to be higher than that calculated on the set of every single subtrees.

Product Description Extraction: We extract all sub- trees of any node that has high entropy ratio. These ex- tracted subtrees have high probability of containing regu- larly formatted objects. However, a large portion of them are noisy objects (e.g, advertisements, navigation links) and need to be removed. A scoring technique based on a set of heuristic rules is employed to filter the output. Each heuris- tic rule is an association rule [1] in which the antecedent is a conjunction of several features (i.e. items) generated from the subtree such as the number of dollar sign, the number of hyperlinks, the number of images, the number of dig- its, etc., and the consequent is a binary feature indicating whether the subtree is a PD or not. The scoring technique is as follows. For each extracted subtree, we apply all as- sociation rules for it. Each time a rule holds, we increase the score of the subtree by x (= support ? confident) points, otherwise we decrease x points. Extracted subtrees having score greater than or equal to the minimum score threshold (STh) will be retained.

3. Evaluation 3.1. Computational Complexity The computational complexity of MDR [2] is O(nK) without considering the complexity of edit distance algo- rithm, where n is the average number of children of internal nodes and K is the maximal number of nodes that a gener- alized node contains. LetN be the number of internal nodes in the whole HTML tree, the overall complexity of MDR is 0-7695-2100-2/04 $ 20.00 IEEE O(NnK|s|2), where |s| is the average length of tag strings associated with HTML subtrees. The overall complexity of  PEWeb system is the sum of those of RV mapping algorithm O(N), entroy ratio calculation algorithmO(Nn), and prod- uct extraction from the Web algorithm O(N). Thus, the overall complexity of PEWeb system is O(Nn). This com- plexity is much smaller than that of MDR. Actually, when K and s are large MDR takes long time to complete.

3.2. Experimental Comparative Evaluation In this section, we evaluate the experimental results of our system (PEWeb). The first version is now available at www.jaist.ac.jp/?hieuxuan/softwares/peweb/. We also compare PEWeb with MDR, the state-of-the-art system for mining data records from the Web. MDR can be downloaded from www.cs.uic.edu/?liub/MDR/MDR-download.html.

Testing Data: Experimental web pages are listed in Ta- ble 1. Several pages in the list are from the experiments of MDR [2]. These pages belong to different domains such    MDR [2]. These pages belong to different domains such as book, hardware and software, food, art, toys, medicine, automobile, cosmetic, etc. These pages are also diverse in representation style.

Point of View of Product Description: MDR [2], OMINI [3], and IEPAD [4] consider data records as regu- larly formatted objects. This means that the outputs of these systems contain both PDs and noisy objects (e.g. advertise- ments, navigation links, form components). That is why the recall and precision of these systems are so high in the cor- responding papers. We consider data records as PDs and our system tries to eliminate noisy objects as many as pos- sible. Our system will follow the later point of view, i.e.

only PDs are considered data records.

Parameter Setting: Both PEWeb and MDR have sev- eral options. MDR has the similarity threshold and the dol- lar sign option. The default value of the similarity thresh- old is 60% and this is also the recommended value. The second option of MDR is the dollar sign. If users check the dollar sign option, the outputs only have data records containing dollar sign. The default value of this option is false (i.e. not checked). PEWeb?s options include minimum depth threshold (DTh), minimum entropy ration threshold (ERTh), and minimum score threshold (STh). The default values of these options are 3, 0.90, and 15 respectively. In the experiments, we use the default option values for both PEWeb and MDR except that the dollar sign option of MDR will be tested for both of its values.

Experimental Results: The experimental results are shown in Table 1. In this table, the second column is the URLs of tested pages, the third column is the number of PDs available in the input page, the next two columns are the number of found PDs and the number of correctly found PDs of PEWeb, the next two columns are the number of found PDs and the number of correctly found PDs of MDR (the value in parentheses is experimental value with dollar sign option checked). The last line are the standard mea- sures recall and precision computed for 35 web pages.

Discussion: In Table 1 we see that both recall and preci- sion of PEWeb (97%, 96%) are higher than those of MDR (84%, 60%). That the precision of PEWeb is lager than that of MDR is reasonable because the outputs of MDR con- tain noisy records. The recall of MDR should be greater than that of PEWeb because measuring similarity using edit distance algorithm should be a little bit more accurate than measuring similarity using entropy estimation. However, the experimental results in Table 1 is opposite. This situ- ation can be explained that MDR can not fix all potential errors of HTML code in input pages, so there are some PDs residing in web pages are can not be reached. When MDR  is run with option dollar sign checked, the recall (74%) of MDR decreases and the precision (88%) of MDR increase.

The decline of recall is because there are several PDs that do not contain the dollar sign ($). They contain no currency signs or other kinds of currency (e.g. ?) rather than dollar sign. Besides, the increase of precision can be explained that many noisy objects containing no dollar sign are elim- inated. However, this precision can not attain the precision of PEWeb because there are some objects containing dollar sign but they are not the true PDs.

4. Conclusion and Future Work We proposed a simple and efficient method to automat- ically extract PDs from the Web. Our technique uses en- tropy estimation and association rule-based filtering. Mea- suring similarity among subtrees is very efficient because it is easy to implement and has a small computational com- plexity. Classifying outputs based on association rules of important heuristic features helps PEWeb system effectively eliminates noisy objects. The PD generated by PEWeb is a chunk of HTML tags and text. In the future, we will label data fields in PDs such as name, price, manufacturer, cat-    data fields in PDs such as name, price, manufacturer, cat- egory, etc. Some models suitable models for this task are Hidden Markov Models, Conditional Random Fields.

