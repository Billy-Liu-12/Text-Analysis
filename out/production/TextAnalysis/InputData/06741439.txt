A New Sampling Approach for Classification of  Imbalanced Data sets with High Density

Abstract?Class imbalance of datasets is a common problem in the field of machine learning. In recent years, because the traditional classifier algorithms are designed only for balanced cases, these classifiers always achieved poor performance in imbalanced data classification issues, especially for the imbalanced data with a really high density. This paper introduces the importance of imbalanced data classification in various fields first; then, contends existing methods of solving the imbalanced data classification problem; finally, proposes two new sampling methods, which are based on borderline- SMOTE, for the imbalanced data with high density, especially for big data with this kind of distribution feature. These two new algorithms are not only over-sampling the minority samples near the borderline, but also creating appropriate synthetic samples in the majority class samples side and under- sampling some particular majority class samples. Experiments show that these two algorithms could achieve a better performance than random over sampling, SMOTE (Synthetic minority over-sampling technique) and Borderline-SMOTE in AUC (Area under Receiver Operating Characteristics Curve) metric evaluate method, when the sampling rate makes the majority class and minority class samples approximate equilibrium.

Keywords-imbalanced data; classification; high density; big data; sampling method

I.  INTRODUCTION Imbalanced data sets problem occurs when the number of  majority (negative) class instances outnumbers the amount of minority (positive) class instances. In recent years, with rapid development of computer science and technology, classification of data sets attracts more and more attentions.

Imbalanced data sets exist in different areas of a wide range of issues, such as face recognition [1], the analysis of satellite imagery detection of oil spills [2], financial risk analysis [3], medical diagnostic decision making [4], and so on. In many areas, comparing with samples in the majority class, minority samples tend to contain more valuable information. E.g., in diagnosis of cancer, the minority samples are positive, and the majority samples are negative.

As a rule, it is more severe that a potential cancer patient (positive) is misdiagnosed as a health person (negative) than  the opposite situation, because this patient might have missed the best time for treatment. Ambiguous boundaries of classes and unsatisfactory performance of classification algorithms are caused by reasons as follow. First, traditional machine learning classification algorithms are designed for data sets with balanced distribution; second, previous studies have shown that data sets with imbalanced distribution features, often have three factors that had negative affect on performance of classification work [20], which are small minority samples size, overlapping and small disjunct. And since many imbalanced data sets have high density distribution feature [5], so-called high density distribution feature is that all or most of samples are distribution in a relatively narrow samples space. Comparing with the data sets with medium density or low density data distribution, since most minority samples are distributed very close with each other, classes boundaries of data sets with high density are more difficult to be distinguished, minority samples are more often regarded as noise, or classified into a fault class; Combining with the general problems in all the imbalanced data sets which have described above, they cause that the performance of classification of imbalanced data sets, which have the high density distribution feature, was further reduced. Especially, for the big data, which have more instances distribute in a relative smaller space, make the data sets with higher density and lead work performance worse.

Facing to the imbalanced data sets with high density, this paper proposes two new sampling methods accordingly.

They are based on the ideas that more distinct border lines could make more contributions to the classification work, and minority samples tend to contain more valuable information than majority samples. And these two sampling methods will be validated through experiments. This paper only studies binary class problem.

The structure of this paper is organized as follow. Section 2 gives a brief introduction to the recent developments in the domains of imbalanced data sets. Section 3 describes two new sampling methods for resolving the problem of classification of imbalanced data sets with high density.

Section 4 presents the experiments and compares the new sampling approaches with other traditional over-sampling methods, and analyze the experiments? results. Section 5 draws the conclusion.

This study was supported by the Key Foundation Research Project of Shenzhen (NO. JC201104210033A and NO. JC201005260148A) and the Technology Innovation Project of Shenzhen (No.

CXZZ20120618155717337).



II. CURRENT APPROACHES The studies show that there are many solutions that solve  the problem of imbalanced learning. These approaches could be mainly divided into two categories. The first category of methods solve the problem at data level, that is, methods make imbalanced data sets into a balanced distribution through appropriate sampling; the second type of approaches considers classification of imbalanced data sets at algorithm level, which improves traditional and existing machine learning classifier algorithm, then getting a new classifier with a better performance on imbalanced classification.

A. Approaches at Data Level Solving the problem of imbalanced learning through  taking sampling methods, that is, considering this problem from the perspective of changing the data distribution characteristics. In other words, reconstructing the original imbalanced data sets, and making the new training sets have the balanced data distribution characteristics. Researches have shown that balanced data sets always have a better classification performance than imbalanced data sets [6]? [7], under the same existing classifier algorithm. Sampling methods are divided into two approaches; one is over- sampling, another is under-sampling.

Random under-sampling method is based on the idea that through removing the samples in the majority class randomly, changing the distribution of class into a balanced state.

Random over-sampling algorithm is which trying to balance the distribution of class by randomly adding instances in the minority class. Both over-sampling and under-sampling have significant drawbacks, such as under-sampling may lose some potential information, and over-sampling may lead the overfitting [8].

There are many potential problems when using random under-sampling method or random over-sampling method.

However, there are still a lot of sampling algorithms making improvements on them. E.g., Kubat and Matwin [9] proposed a method to do the under-sampling work in the majority class, which is called one-sided selection; Mani and Zhang [10] used k nearest neighbors classifier to select samples which should be under-sampling; Chawla et al.

proposed SMOTE algorithm which creates synthetic samples in the minority class. And experiments show that it outperformance traditional sampling methods; Han et al.

described that instances, which are distribute near class border lines, as having more influence on performance of classification work, and proposed an improved SMOTE algorithm, which is called Borderline-SMOTE. This new algorithm only creates synthetic samples by using specific original samples, which distribute near the border line. And the new algorithm can achieve a better prediction for the minority class [12].

B. Approaches at Algorithm Level As what have been mentioned above, the sampling  methods change the data distribution characteristics, and then solve the problem that standard classifier algorithms could not adapt to imbalanced data sets. But there are still many researches trying to solve imbalanced learning problem at the  algorithm level. E.g., a reasonable cost matrix could help cost sensitive learning find the minimum classification cost, and then get the best prediction result [13].

Since the minority samples are always classified into a wrong class and regarded as noise samples in the real world.

In the cost sensitive learning, minority instances are always given a relative higher misclassification cost than majority instances. This strategy makes classifier algorithms pay more attention to predict these difficult samples. In addition, in recent years, as a kind of effective classification method, Ensemble Learning is more widely applied in imbalanced classification than before. By sorting and combining weak classifiers, ensemble learning achieves a strong classifier, which has a better work performance. There are a large amount of improvements that base on ensemble learning method, such as Wang et al. described Threshold SMOTE with Attribute Bagging [21], Utasi investigated weighted conditional mutual information based boosting [22] and Chawla et al. proposed SMOTEBoost [14], which have a good performance on predicting the minority class, through combining sampling methods and Ensemble Learning.



III. A NEW SAMPLING APPROACH FOR IMBALANCED DATA SETS WITH HIGH DENSITY: DISTINCT-BORDERLINE  ALGORITHM Previous study [12] indicated that, due to the ambiguous  class boundaries, samples, which distribute near the border lines, are easier to be misclassified than others. In other words, these boundary samples often have a more significant influence on the classification performance. It is clear that a lot of boundary samples, which in big data distribution space, make the classification performance worse. Then that study also proposed an improved SMOTE algorithm, which only uses boundary samples to create artificial samples and adding them into minority class.

To begin with, from what have been mentioned above, comparing with data sets with low density and medium density, it is clear that the distances between majority samples and minority samples are smaller in a high density data set; and the class border lines are more ambiguous. So, more minority samples will be regarded as noise samples or misclassified into a wrong class in the high density imbalanced data set. Moreover, although under-sampling may make some valuable information to be ignored; after considering the situation that researchers always pay more attention to minority samples, if a small amount of specific majority boundary instances are removed, it is a useful way to make the class border lines more clearly and also could improve the classification performance. In other words, more minority samples will be classified correctly and boundary samples in the majority side were removed, thus making majority and minority have a more striking distance.

This paper proposes two new sampling methods that base on the definition of border line subset in Boderline-SMOTE algorithm and the approach of creating synthetic samples in SMOTE algorithm. Suppose the whole training set is S, the majority samples set is Smaj, the minority samples set is Smin, the boundary subset of majority class is SBmaj, the boundary     subset of minority class is SBmin. The main steps of the DB (Distinct-Borderline) algorithm are as follow:  Step 1. For each sample in the minority class Smin, calculating it?s the number of K nearest neighbors from the whole training data set S, by using KNN classifier algorithm.

Especially, K should be suitable for the following condition: 0 ? K ? ? S ?.

Step 2. According to the results of the first step, this step calculates the boundary subset of minority class SBmin. The main idea is as follows: for K nearest neighbors of each sample in Smin, if the size of its neighbors from Smaj outnumbers the size of its neighbors from Smin, then this sample would be regarded as the boundary sample, and added to the SBmin. In contrast, if the size of its neighbors from Smaj outnumbers the size of its neighbors from Smin, this sample would be regarded as the noise or inner class instances, and anything should not be done. It should be noted that ??SBmin???????Smin ?.

Step 3. According to the results of the second step, for each sample in SBmin, this step calculates its nearest sample from Smaj, by using the nearest neighbor algorithm, and adding this nearest majority sample into SBmaj. When this step is finished, each sample in SBmin could be mapped to only one sample from SBmaj, and these two boundary subsets make up the class border lines together. Especially, ??SBmin???????SBmaj ?.

Step 4. In this step, according to the sampling rate, specific numbers of synthetic samples are to be added into the original training set S. First of all, some synthetic samples, which are generated through SMOTE sampling method, are added into the minority class. It should be noted that DB1 (Distinct-Borderline1) sampling algorithm only generates artificial samples from SBmin, that is, both Xi and Yi, which are used to create synthetic sample Xnew, come from SBmin. While, for DB2 (Distinct-Borderline2) ampling algorithm, Xi is selected from SBmin; Yi, however, could be selected from either SBmin or Smin. Second, synthetic majority samples start to be generated in majority class and added into S, and the number of synthetic samples should equal to???SBmaj??. The algorithm uses SMOTE [11] method to generate synthetic instances. The brief introduction is as follows: taking generate synthetic in the minority class as an example. First of all, Xi ? Smin, and Yi is a neighbor sample which is selected from Xi?s K nearest neighbors randomly.

Second, a different factor Difi is generated randomly, which should fit the condition that 0 ? Difi ? 1. Third, Xi, Yi and Difi are put into the (1), which is proposed below. Finally, the synthetic sample Xnew is generated. The approach of generating synthetic samples in the majority class is in the similar way.

Xnew ? Xi ????Yi ? Xi ? ? Difi      i = 1,2,3?,? SBmin  ?       (1)  Step 5. Since minority samples always contain more valuable information than majority side; the boundaries samples, which appear both in SBmaj and Smaj, are removed after adding synthetic samples both in majority class and minority class make the class border lines more clearly. In other words, the algorithm selects specific samples, which distribute in the specific area, and then using under-  sampling method in them. As a result more minority samples could be classified correctly.

After all the steps are finished, a new training set Snew is generated by using DB algorithm.



IV. EXPERIMENTS AND ANALYSIS In this paper, all of data sets in experiments come from  UCI repository [16]. The number of boundary samples is taken as a criterion, which shows density of data sets (the number of boundary samples is to be calculated during the experiment). This paper only study binary class problem. If there is a data set contains more than two class labels, the class with the minimum size is taken as minority class, and remaining classes are regard as majority class. The data sets are described in Table I. The ratio of sampling make imbalanced data set change into balanced data distribution, and that is determined like (2). Satimage data set sources from a satellite. User data set describes the students' knowledge status about a subject. The instances in Segment data set are drawn randomly from a database of outdoor images. Transfusion data set demonstrates a marketing model, and uses a binary variable to represent whether he/she donated blood. Breast data set is used to diagnose the breast cancer. Australsian data set is used to determine that whether credit card applications should be approved or not.

Ratio  ????Smaj  ????100?????Smin??                           (2) The experiments use C4.5 classifier in Weka [17], [18] to  compare the work performance of original training set with the new training set, which is generated by DB algorithm.

First of all, C4.5 is a widely used algorithm; and at the same time, it is also a classifier which has common standards.

Second, C4.5 algorithm is sensitive to the data distribution.

The more imbalanced data distributes, the worse classification performance C4.5 algorithm achieves [19]; this could make the improvement more significantly. Finally, C4.5 algorithm has the transparent classification progress, which is says that researchers could understand the whole classification progress, and this may provide help to the future study.

Considering the distribution characteristics of imbalanced data sets, the size of majority class samples outnumbers size of minority class samples; it leads to a phenomenon that a relatively high accuracy could be achieved, when all of the minority instances are ignored. So, the present atomic evaluation metrics of classification cannot adapt to the imbalanced classification. At the same time, some composite evaluation metrics also have problems. E.g., when the classification accuracy of majority samples is very high, G- Mean often achieves a high value by ignoring some minority samples. In this paper, AUC is used to evaluate work performance. Larger AUC-value means that the boundaries between majority class and minority class are distinguished more clearly [15].

The first experiment is used to verify the hypothesis that DB1 and BD2 are more effective to solve the imbalanced learning problems with high density data distribution characteristic.

TABLE I.  DATA SETS DESCRIPTIONS  Data Sets  Number of  Instances  Number of  Attributes  Class Labels Minority : Majority  Percentage of  Minority class  Ratio of Sampling to  Balanced Distribution  Satimage 6435 36 4:remains 9.7% 928% User 403 5 very_low:  remains 12.4% 706%  Segment 2310 19 7:remains 14.3% 600% Transfusion 748 5 1:0 23.8% 320%  Breast 699 10 4:2 34.5% 190% Australian 690 14 1:0 44.5% 24.8%   These two new methods are compared with the typical  existing sampling methods, which include random over- sampling, SMOTE and Borderline-SMOTE. The value of K, which in KNN classifier algorithm, is set 5, as indicated in important references [11], [12]. Fig. 1 illustrates AUC values when proposing different sampling approaches with different ratio of sampling that are applied to different data sets respectively with C4.5.

Table II describes that, comparing with best performance of existing sampling methods, DB1 and DB2 could improve the AUC value in the areas around the vertical line. What can be seen that DB1 and DB2 algorithm give an increment of AUC in Satimage, Breast and Australian. Especially, there is the most significant increment of AUC in Satimage, the ratio of increment is 5.019%; in other words, after sampling by the methods which proposed in this paper, the classifier finds 14 more minority samples. Table II also shows that the increments of AUC are related to the number of boundary samples. If there are more boundary samples in a data set, there are more significant increments. As what have been mentioned above, the number of boundary samples tends to reflect the data distribution density; the more boundary samples a data set has, the higher density the data set has. If an imbalanced data set has really high density, the problem of overlapping and small disjunct would be more serious, and the border lines would be harder to be distinguished. As a result, existing methods usually achieve poor classification performance when facing this kind of data sets. Since DB1 and DB2 generate synthetic samples in both sides; at the same time, considering minority instances always contain more valuable information than majority instances, they do under-sampling work in majority class side, this idea make the border lines more clearly than other methods. So, we make hypothesis that DB1 and DB2 are especially fit to solve the problems about imbalance data sets with high density.

The second experiment is used to verify the assumption which is got after analyzing results of the first experiment.

The experiment steps are as follow: firstly, half of samples in Satimage data set are removed randomly, but the percentage of minority samples is not changed (error should no more than 0.5%). Comparing with original Satimage data set, the half of Satimage data set has a relatively low density.

Secondly, sampling these two training data by using DB1 and DB2 algorithm, will achieve four training data sets.

Finally, each one of these four training data sets is to be used in C4.5, and then tested at the same testing set.

TABLE II.  INCREMENT OF AUC  Data Set Comparing with traditional methods  Ratio of Increment  Number of Boundary Samples  Satimage Great 5.019% 143 User Very Little 0.041% 12  Segment Equal 0% 3 Transfusion Very Little 0.044% 35  Breast Little 1.05% 59 Australian Little 1.03% 81   Fig. 2 shows the experiment result. It indicates that, after  sampling by DB1 and DB2, data sets with high density outperformance data sets with relatively lower density.

Comparing with the data set with relatively lower density, the best improvements of AUC for DB1 and DB2 are 7.89% and 8.5% respectively. So, a conclusion could be safely achieved, it is clear that DB1 and DB2 are especially fit to solve the problems about classification of imbalance data sets with high density.

(c)    (b)     (a)           Fig. 1.   (a), (b), (c), (d), (e) and (f) illustrate that AUC values when proposed different sampling approaches with different ratio of sampling are applied on Satimage, User, Segment, Transfusion, Breast and Australian respectively with C4.5. Especially, areas around the vertical line make majority class and minority class have the same samples size approximately.

Fig. 2.  Comparison of the improvement of AUC in data sets with different density.



V.  CONCLUSIONS In recent years, with rapid development of computer  science and technology, classification of imbalanced data sets attracts more and more attentions. Since traditional classifier algorithms not take imbalanced distribution into consideration, they always achieve poor classification performance. At the same time, high density distribution characteristic makes the distances between two classes smaller, the border lines more difficult to be distinguished and the problem of overlapping and small disjunct more seriously et al. . In addition, those data sets with big samples size always have higher density. These reasons make existing sampling methods achieve worse performance. For solving the problem what has been mentioned above, this paper proposed two new sampling methods, which are DB1 and DB2; and then adopting C4.5 and AUC evaluation metric to verify their performance in the experiments part.

First of all, considering that boundary samples have more influence on classification, the algorithms generate synthetic samples in both majority and minority boundaries areas.

Second, considering that minority samples always contain more valuable potential information than others, and in order to make the border lines more clearly, the algorithms remove specific majority samples at the same time. The experiments indicate that the algorithms, which suggested in this paper, make performance of classification of imbalanced data sets with high density improve greatly.

