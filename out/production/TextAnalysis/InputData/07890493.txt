An Incremental Change Detection Test Based on Density Difference Estimation

Abstract?We propose incremental least squares density difference (LSDD) change detection method, an incremental test to detect changes in stationarity based on the difference between the unknown prechange and the post-change probability den- sity functions (pdfs). The method is computationally light and, hence, adequate to process continuous datastreams, as those emerging from the Internet of Things and the big data frame- work. The incremental change detection test operates on two nonoverlapping data windows to estimate the LSDD between the two pdfs. We construct a theoretical framework that shows how the distribution of LSDD values follows a linear combi- nation of ?2 distributions and provides thresholds to control false positive rates. The proposed test can operate online, with needed estimates and thresholds computed incrementally as fresh samples come. Comprehensive experiments validate the effec- tiveness of the test both in detecting abrupt and drift types of changes.

Index Terms?Change detection, incremental computing, incremental least squares density difference change detection method (LSDD-Inc), probability density function (pdf)-free.



I. INTRODUCTION  T IME invariance is a strong hypothesis to make whendealing with datastreams, no matter whether refer- ring to learning problems [1]?[4] or control ones [5], [6].

In fact, in the long acquisition run, we cannot guaran- tee anymore that the interaction between sensors and the environment/system or the environment/system itself will not change [7], [8].

In order to cope with this very relevant issue, researchers have developed methods for an online detection of changes in stationarity, and methodologies to learn in such evolving environments. Most of existing research aims at detecting changes in stationarity in the datastreams by extracting fea- tures and inspecting associated statistics. Not rarely, features are extracted from two nonoverlapping data windows referring  Manuscript received June 27, 2016; accepted March 6, 2017. This work was supported by the National Natural Science Foundation of China under Grant 61573353, Grant 61533017, and Grant 61603382. This paper was recommended by Associate Editor F. Sun.

L. Bu and D. Zhao are with the State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences and University of Chinese Academy of Sciences, Beijing 100190, China (e-mail: bulipolly@gmail.com; dongbin.zhao@ia.ac.cn).

C. Alippi is with the Politecnico di Milano, 20133 Milano, Italy, and also with the Universit? della Svizzera italiana, 6904 Lugano, Switzerland (e-mail: cesare.alippi@polimi.it).

Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.

to prechange and possibly postchange conditions, respectively.

The prechange window is composed of stationary samples x extracted according to some unknown, but fixed, probability density function (pdf) p(x) to constitute the nominal reference set Zp. The latter sliding window collects instead fresh sam- ples as they come, extracted according to unknown pdf q(x) to populate the test set Zq. A change occurs in the sliding win- dow when q(x) ?= p(x). Following this comment, and given the fact we have only information about the prechange and post-change samples, a change occurs when ?data in Zq do not follow the distribution that generated Zp according to a defined confidence level.? The opposite holds.

Some change detection methods operate by comparing features extracted from the two windows, e.g., the sample mean or rank-based statistics, to detect changes in sta- tionarity. In this direction, a change-point formulation is proposed in [9] to inspect changes affecting mean or vari- ance in normally distributed samples. The method is then extended in [10] and [11] to deal with univariate non-Gaussian sequences. Changes are detected when designed statistics based on Mann?Whitney [12] and Lepage [13] tests exceed thresholds associated with predefined false positive (FP) rates.

In order to deal with multivariate cases, [14] proposes a KNN-based test measuring the proportion of samples among k nearest neighbors that belong to a given window. It is shown that the derived statistics asymptotically satisfy a normal dis- tribution, from which a threshold can be derived to meet a tolerated FP rate.

Only few papers attempt at directly comparing the known pdfs, e.g., with the KL-divergence or the Hellinger distance.

The main restriction here is that reality is mostly pdf-free, in the sense that the distribution families are unknown. In order to handle this issue, researchers have found ways to estimate the pdfs directly from collected samples, with all associated limits, commonly by relying on histograms or kernel density estimation methods [15]. Most of the methods work incre- mentally to reduce the computational load associated with the integration of new samples in the change detection test. For instance, [16] suggests to extract frequency histograms from Zp and Zq and compare them according to the KL-divergence; a partition incremental discretization algorithm is applied to guarantee incremental computation. A different approach is proposed in [17], where frequency histograms are estimated with a kdp-tree computing the relative entropy: the tree is updated incrementally by adapting the corresponding nodes with new instances. A Gaussian mixture model (GMM) is con- sidered in [18] to approximate the pdfs of the neighborhood  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

mailto:bulipolly@gmail.com mailto:dongbin.zhao@ia.ac.cn mailto:cesare.alippi@polimi.it http://ieeexplore.ieee.org http://www.ieee.org/publications_standards/publications/rights/index.html   This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

of each pixel in SAR images: the KL-divergence is then used to detect changes. However, the work in [19] shows how the Hellinger distance seems to be more attractive than the KL-divergence for change detection given its symmetry and boundedness properties. In [20], pdf p(x) is approxi- mated with GMM, while Lepage and one-sided t-tests are applied to monitor the changes in the log-likelihood of instances.

Last methods are based on a two-step procedure requir- ing at first to estimate the two pdfs and then evaluate their distance; variability associated with the finiteness of the data window and presence of noise in input data reduce the effec- tiveness of the methods. In order to mitigate this problem, some results present in the literature aim at directly mea- suring the density-ratio of the two distributions [21] or their density-difference [22] directly from available data windows.

In our previous work [23], we investigated the performance and extended the least squares density difference (LSDD) method. A family of ensemble LSDD-based methods was also introduced in [24]. Even though effective, these last meth- ods are computational intensive and lighter solutions must be proposed when computation is an issue.

In this paper, we investigate the LSDD method for change detection, shed light on some properties associated with the method and propose an incremental change detection test to reduce the computational request. The novel contributions reside as follows.

1) A theorem stating that the estimated LSDD values D?2? are distributed as a linear combination of noncentral chi- square distributions.

2) A theorem linking window size with FP and negative rates. As a consequence, the change detection test can adaptively enlarge the window size to improve detection performance without requesting any retraining phase.

3) Computationally light incremental algorithm for D?2?.

The structure of this paper is as follows. Section II briefly  recalls the LSDD method. Section III provides the main theoretical results and introduces the adaptive threshold mech- anism. The detailed description of the incremental LSDD change detection method (LSDD-Inc) is given in Section IV.

Finally, experiments showing the validity of the proposed change detection method are presented and commented in Section V.



II. LSDD METHOD  The LSDD is defined as the scalar  D2(p, q) = ?  (p(x) ? q(x))2dx (1) where x ? Rd is a real vector, and p(x), q(x) are two unknown pdfs. Instead of estimating p(x) and q(x), we directly estimate the difference p(x) ? q(x) with the Gaussian kernel model  g(x,?) = k?  i=1 ?i exp  ( ??x ? ci?   2? 2  ) (2)  where k is the number of kernel functions, ci the ith kernel center, ? = [?1, ?2, . . . , ?k] a parameter vector, and ? is a scale parameter.

The optimal parameter ? is the one minimizing the loss  J(?) = ?  (g(x,?) ? (p(x) ? q(x)))2dx + ??T?. (3)  ? > 0 is an L2-regularizer controlling overfitting.

After some calculus, we obtain that  J(?) = ?  g(x,?)2dx ? 2 ?  g(x,?)(p(x) ? q(x))dx  + ?  (p(x) ? q(x))2dx + ??T?  = ?TH? ? 2hT? + ?  (p(x) ? q(x))2dx + ??T? (4)  where H is a k ? k matrix, and h a k ? 1 vector  Hi,j = ?  exp  ( ?||x ? ci||   2? 2  ) exp  ( ?||x ? cj||   2? 2  ) dx  = ( ?? 2  )d/2 exp  ( ?||ci ? cj||   4? 2  ) (5)  hi = ?  exp  ( ?||x ? ci ||   2? 2  ) p(x)dx  ? ?  exp  ( ?||x ? ci ||   2? 2  ) q(x)dx (6)  i, j = 1, . . . , k. Defined Zp = {xp,1, . . . , xp,n} as the data set drawn according to p(x) and Zq = {xq,1, . . . , xq,m} that from q(x), Monte Carlo sampling provides estimates  h?i = 1 n  n? l=1  exp  ( ?||xp,l ? ci ||   2? 2  )  ? 1 m  m? l=1  exp  ( ?||xq,l ? ci ||   2? 2  ) . (7)  Finally, ?? is  ?? = arg min ?  ( ?TH? ? 2h?T? + ??T?  )  = (H + ?I)?1h?. (8) By replacing p(x) ? q(x) with g(x, ??), two equivalent  expressions of the D2-distance can be obtained  D?21(p, q) = ?  g (  x, ?? ) (p(x) ? q(x))dx = h?T?? (9)  D?22(p, q) = ?  (g(x, ??))2dx = ??TH??. (10)  In order to reduce the bias introduced by ?, we can write (details in Appendix A)  D?2?(p, q) = 2h?T?? ? ??TH?? = h?TH?1? h? (11)  where H?1? = 2(H + ?I)?1 ? (H + ?I)?TH(H + ?I)?1.

We comment that the higher D?2? the larger the discrepancy  between p(x) and q(x).

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

BU et al.: INCREMENTAL CHANGE DETECTION TEST BASED ON DENSITY DIFFERENCE ESTIMATION 3

III. SOME THEORETICAL RESULTS ABOUT LSDD  Estimate D?2? is a random variable depending on the particu- lar realization of sets Zp and Zq as well as their cardinalities n and m, respectively. As such, in order to introduce a confidence level for FP, we need to determine the generating distribution and, then, the influence of the windows sizes n and m on the proposed method. Subsequent theorems address these aspects.

A. Distribution of D?2? Theorem 1: In stationary conditions, the distribution ? of  D?2? is a linear combination of k(k + 1) noncentral chi-square distributions provided that kernel centers are given.

The proof derives from the central limit theorem and is given in Appendix B. The theorem also implicitly states that once n and m are fixed, so it is the resulting distribution of D?2?.

Since the underlying distribution is now available, we can derive the threshold T? permitting to detect changes in sta- tionarity at confidence level 1 ?? (the FP rate is hence set to ?). If the number of available samples is not enough to con- figure the parameters of the distributions, we can derive the required threshold, as suggested [17], as the 1 ? ? percentile of the estimates so that  Pr (  D?2? > T? )  = ?. (12)  As a consequence, the hypothesis test behind the change detection test can be written as  H0 : p(x) = q(x) H1 : p(x) ?= q(x).

Whenever pdf q(x) differs from p(x), values D?2? exceed T? with confidence level 1 ? ?, i.e., H0 is rejected, and a change is detected. It has to be noted that the detected change occurs in the current sliding window Zq, and we do not know the exact change location within the window. Other methods can be used to improve the location estimate, e.g., as proposed in [25].

In those cases where the training set is small and we cannot generate enough estimates for D?2? (say [Nt/(n + m)] < 100), we propose to use a bootstrap procedure [17], [23] to gen- erate enough D?2? values to configure H0. This procedure is appropriate since it is proved that bootstrap approximates the source distribution provided the pooling set is sufficiently informative [26].

In this paper, bootstrap operates as follows. At first, win- dows Zp of size n, Zp,i, i = 1, . . . , M and Zq of size m, Zq,i, i = 1, . . . , M are drawn from the stationary training set with replacement. The first M subsets are assumed to be gener- ated from p(x) and the second ones from q(x). For the generic ith window couple {Zp,i, Zq,i}, the ith estimate D?2? is computed according to (11). The M couples are then representative of the situation in the stationary condition and used to configure test H0. The needed threshold is then computed according to the predefined FP rate as shown above once a tolerated FP rate has been given.

Fig. 1. Distribution of D?2? with different n. We centered the distributions and shifted threshold T? (to T ??) so that the new expectations of D?2? under H0 are zero.

It should be emphasized that the proposed method only assumes the training set to be stationary to design the change detection test associated with H0. This hypothesis is reason- able since time variance generally develops late in time, e.g., think of sensor aging.

B. Influence of the Window Size  The influence of the window size on the change detection test is presented as follows.

Theorem 2: The relationship between the window size and the LSDD statistics  1) The expectation of D?2? shows an inverse dependence in n and m  E (  D?2?  ) = f  (  n ,   m  ) . (13)  2) The difference of expectations EH1(D? ?) ? EH0(D?2?)  inversely depends on m only  EH1  ( D?2?(p, q)  ) ? EH0  ( D?2?(p, q)  ) = f0  (  m  ) . (14)  3) The probability of an ? > 0 deviation bound diminishes when n and m increase  Pr (???D?2?(p, q) ? E  ( D?2?(p, q)  )??? ? ? )  ? f1 (   n ,   m  ) . (15)  The proof and functions f , f0, and f1 are given in Appendix C.

Theorem 2 indicates that the ? deviation bound decreases  with n, whereas the difference between expectations does not, so that the overlap between the distributions ?H0 and ?H1 of D?2? diminishes.

The schematic of Fig. 1 shows how distributions change when the window size moves from n to n? > n. Define T?? to be the threshold associated with the larger size n?, in turn associated with FP rate ??. Since point 2) of Theorem 2 states that the difference of the expectations does not depend on n, we keep the distance between EH0n?(D?  ?) and T?? constant,  that is  EH0n? (  D?2?  ) ? T?? = EH0n  ( D?2?  ) ? T?. (16)    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Fig. 2. Description of the enlarged reference set. The green points represent continuously generated samples and the red ones are samples included in subsets Zp and Zq, respectively. A change occurs in Zq and the location is shown with a black arrow.

This comment leads to a major outcome since it permits us to both express the FP and false negative (FN) rates in terms of n?  FP rate : Pr (  D?2?H0n > T? )  = ? > Pr (  D?2?H0n? > T?? )  = ??  FN rate : Pr (  D?2?H1n < T? )  > Pr (  D?2?H0n? < T?? ) . (17)  In particular, (17) shows that a larger n? permits to achieve a better change detection performance by both lowering FP and FN rates. In addition, since the probability associated with the ? deviation bound also decreases with the increase of m (15), m also controls the FP rates so that a larger m reduces FPs.

However, its influence on the FN rate is unknown at this stage of research.

C. Self-Adaptive Thresholds  During the operational phase, a single window of size n extracted from the training set represents a set of realizations following p(x). At the same time m samples compose the slid- ing window associated with q(x). A reservoir sampling method was proposed in [27] to mitigate the fact that a single window is considered, which updates Zp to achieve lower FP rates [23].

Other approaches consider ensemble methods to include sev- eral reference windows to better represent p(x) [24]. However, despite the fact we might consider those approaches, a natural question arises: ?can we adapt the reference set Zp to host more than n instances during the operational phase of the change detection test?? The answer to the question is clearly ?yes,? but it would a priori request a computationally expensive train- ing phase. We then search for methods that can host more data with size n? (n? > n), and compute the new threshold T?? directly from the available estimate D?2?H0n.

We propose in the sequel an adaptive mechanism for gen- erating online the new thresholds as data come from the datastream. Initially, at training time, the change detection test undergoes a configuration phase where the limited size of the training set forces the designer to consider small values for n and m. Given n and m, threshold T? is derived accordingly.

Then, during the operational phase, more stationary instances with size n? so that n? > n are added to Zp. We follow (16) to determine the new associated threshold T?? , which permits to control the FP rates. An intuitive description of the enlarged reference set is shown in Fig. 2, where more stationary samples are included in Zp.

Appendix C shows that in a stationary situation (H0 holds), the expectation of D?2? with sizes n and m can be expressed as  EH0n (  D?2?  ) = (   n + 1  m  ) C1 (18)  where C1 is a constant depending on p(x) and q(x) = p(x). It can be proved that having n? and m, the new threshold becomes  T?? = EH0n? (  D?2?  ) ? (  EH0n (  D?2?  ) ? T?  )  = (  n? + 1m n + 1m  ? 1 )  EH0n (  D?2?  ) + T? (19)  where both the required expectation EH0n(D? ?) and threshold  T? are available.

Since Theorem 2 tells us that a larger m lowers the FP rate,  it might be worth to enlarge the Zq with size m?(m? > m) and compute the associated threshold T??? with ??? < ?? < ?. We can write that  T??? = (  n? + 1m? n + 1m  ? 1 )  EH0n (  D?2?  ) + T?. (20)  This mechanism permits the method to operate online and, once a potential change is detected, to host more samples to decide whether confirm or reject the change in stationary hypothesis with better confidence by operating on n and m directly.



IV. LSDD-INC: INCREMENTAL LSDD-BASED CHANGE DETECTION TEST  An incremental computational approach is always appre- ciated when dealing with datastreams both to speed up the computation and relieve the storage needs. This section moves in this direction by proposing an incremental change detection test.

We comment that when the k kernel centers are given, both matrix H and H?1? are fixed for a given ?. As a result, the online computation of (11) only requires h? to be estimated, as Zq updates with new instances. Value h? at the (i+m)th sample can be expressed as  h?j(i) = C2 ? 1 m  i+m? l=i+1  exp  ( ?||xq,l ? cj ||   2? 2  )  = C2 ? f2 (21) where C2 = (1/n?)?n?l=1 exp(?[(||xp,l ? cj ||22)/2? 2]) repre- sents a constant value associated with Zp. Since we assume that the training set is stationary, we add the whole set into Zp with n? = Nt in this paper. It should be noted that Zp can be enlarged further with n? > Nt provided that new incoming are granted to be stationary.

When the test window Zq slides and collects the (i+m+1)th instance, we have that  h?j(i+1) = C2 ? 1 m  i+m+1? l=i+2  exp  ( ?||xq,l ? cj ||   2? 2  )  = C2 ? f ?2    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

BU et al.: INCREMENTAL CHANGE DETECTION TEST BASED ON DENSITY DIFFERENCE ESTIMATION 5  Algorithm 1 LSDD-Inc 1: Input: Training set Nt, window sizes n and m, FP rate ?, number  of resampled subsets 2M; Output: Change location.

2: Bootstrap 2M subsets from the training set, the first M subsets coming from distribution p(x) with size n, and the remaining M ones from q(x) = p(x) with size m;  3: Provide the M LSDD estimates according to (11); 4: Derive thresholds T? according to (12); 5: Take advantage of the whole training set so that n? = Nt; derive  the new threshold T?? according to (19); 6: Prepare Zq with m recently collected samples; i = 1; 7: Estimate C2 and f2 according to (21); 8: while (1) do 9: Incrementally estimate f  ? 2 and update h? according to (22);  10: Estimate the LSDD value D?2? with (11); 11: if D?2? > T?? then 12: Change in stationarity is detected at location i, with confi-  dence 1 ? ?; 13: Break; 14: else 15: Update sliding window Zq; 16: i = i + 1; 17: end if 18: end while  f ?2 = f2 +  m  ( exp  ( ?||xq,i+m+1 ? cj||   2? 2  )  ? exp (  ?||xq,i+1 ? cj||  2? 2  )) . (22)  Within this incremental approach, at each time step, only two instances xq,i+m+1 and xq,i+1 need to be recomputed, and at most (m + 1) samples need to be stored.

The final incremental change detection method is given in Algorithm 1. Since we deal with datasteams (i.e., data are generated continuously), the loop terminates (step 13) only when a change is detected. Reactions to the change, e.g., to update the application or retrain the detection method, can be considered.



V. EXPERIMENTS  A. Datasets  To contrast the performances of the proposed incremen- tal method LSDD-Inc with other change detection methods, seven applications are considered, including unidimensional and multidimensional ones. Since the exact change location in real applications is hardly available, most of the applica- tions (D1?D6) are simulated. However, one real-world dataset (D7) is considered to test the effectiveness of the method in a real application.

1) Samples of application D1 follow a gaussian distribu- tion N(0, 0.5). The change induces a slow drift in the distribution toward distribution N(0.5, 0.5).

2) Application D2 is inspired by a 10-D problem [28], whose instances satisfy a multivariate gaussian distri- bution. Means are fixed at u1,i = u2,i = 0; the covariance shifts from ?1,ij(i=j) = 0.5, ?1,ij(i?=j) = 0 to ?2,ij(i=j) = 0.5, ?2,ij(i?=j) = 0.4, i, j = 1, . . . , 10.

TABLE I DETAILS OF THE DATASETS  3) Application D3 refers to a two-class rotating mixture of Gaussians application [29] with class centers shift- ing from u1 = [1/  ? 2, 1/  ? 2], u2 = [?1/  ? 2,?1/?2]  to u1 = [1/ ?  2,?1/?2], u2 = [?1/ ?  2, 1/ ?  2].

Covariance matrices are fixed at 	1 = 	2 = [0.5, 0; 0, 0.5].

4) Application D4 refers to problem [30] with samples satisfying the restriction: (x1 ? a)2 + (x2 ? b)2 ? r2.

Changes occur with the radius r slowly drifting from 0.2 to 0.3. a = b = 0.5; variables x1 and x2 are uniformly distributed in interval [0, 1].

5) Application D5 refers to a moving hyperplane prob- lem [30] with y ? ?a0 + a1x1 + a2x2. a1 = a2 = 0.1, a0 shifts from ?1 to ?3.2; xi(i=1,2), y are uniformly distributed in intervals [0, 1] and [0, 5], respectively.

6) Application D6 is the STAGGER problem [31] with categorical features. We transform this classification problem into a detection one by taking only one class of samples. Changes occur with concept1 shifting to concept2.

7) Application D7 is a real application with samples col- lected from a combined cycle power plant [32], [33], where hourly averaged temperature, ambient pressure, relative humidity and exhaust vacuum measurements are used to predict the net hourly electrical energy output.

We normalize the dataset to interval [?1, 1], and add a change by shifting the normalized temperature from x1 to ?x1.

We summarize these datasets in Table I to show their dif- ferent properties, where Syn is short for synthetic, Dim for dimension, ChgType for change type, and AttrType for attribute type.

B. Other Methods  Four methods are introduced for comparison, includ- ing our previous (LSDD-CDT test) [23], a statistical test (LogKStest) [20], an incrementally distance- based method (HDDDM) [19] and a hierarchical method (H-ICI) [34]. We point out that the last three methods are well established change detection methods, working either on pdfs or in 1-D applications.

We also propose LSDD-Inc2, an evolution of LSDD-Inc, that takes advantage of the fact FP rates reduce with larger m? as claimed by Theorem 2 (here we consider m? = 2 m).

Needed thresholds follow (20).

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

In more details.

1) LogKStest: The test proposed in [20] detects changes by  monitoring the log-likelihood of the pdf p(x) of scalar x  L(x) = log(p(x)).

By estimating p(x) with a mixture of w Gaussians  L?(x) = ?w  ( log ( (2?)d  ) det ( p,i?  )  + (x ? up,i?)?	?1p,i? ( x ? up,i?  ))  where i? is the Gaussian of the mixture maximizing likelihood.

A Kolmogorov?Smirnov test [20] is then used to test whether L?(x) evaluated over the two windows follows the same pdf or not.

2) HDDDM: This method approximates p(x) and q(x) with histograms, and detects possible changes by inspecting their Hellinger distance [19]. When no changes are detected, the histogram of p(x) is updated by adding samples of Zq into Zp incrementally, while the estimate of q(x) is updated with the new acquired samples. The t-statistic is used to derive thresh- olds as suggested in [19]. Since the method is proposed for sequential batch learning, we take samples in one test win- dow (nonoverlapped) as a batch, and use the same detection procedure as recommended in [19].

3) H-ICI: The H-ICI test [34] is a two-layered hierarchi- cal CDT whose first level detects possible changes based on the intersection of confidence intervals (ICIs) rule [35], and the second one confirms changes with the Hotellings T-square statistic. The method can detect changes accurately with low FP rates, particularly in 1-D applications.

C. Experimental Setup  Given that only a finite training set is available and that the window sizes influence the distribution of D?2?, n and m should be fixed when generating the bootstrap-based distribu- tion approximating the real one. In addition, a smaller window size is always associated with a shorter execution time, which can be relevant in some applications. In this paper, we con- sider two configurations for training phase: n = m = 100 and n = m = 200.

Other needed parameters are chosen as follows. The size of training set Nt is 2000, changes in applications D1?D7 occur at sample 6001 and last to the end, and the number of taken bootstraps M is 2000. k = n + m, the kernel centers are ran- domly sampled from the training set and then fixed before training. The FP rate ? for most of the methods is set to 1%; ?s , ?w , and ?c as requested by LSDD-CDT and correspond- ing to the three thresholds Ts, Tw, and Tc, respectively, are set to 10%, 2%, and 1%. Each experiment on each application is repeated 500 times.

The choice of the scaling parameter ? and the regularization parameter ? influences the accuracy of the density differ- ence estimation method and the change detection performance.

In this paper, ? is chosen as the median distance between instances in the training set ? = median(||xi ? xj||2, 0 < i < j ? Nt) [36], which is commonly used with a radial basis function kernel. With reference to Appendixes A?C,  ? should be small to reduce the bias and is selected by control- ling the relative difference (RD) between D?21 and D?  2; RD is  set to 0.2.

Since most of the applications follow Gaussians or uni-  form distributions, the maximum number W of Gaussians for LogKStest is set to m/10 based on experimental evidence. In this paper, W = 10 and 20 correspond to m = 100 and 200.

LogKStest, HDDDM, and H-ICI keep the settings suggested in their relative manuscripts.

At last, we consider five indexes to evaluate the detection performance of the proposed LSDD-Inc.

1) FP Rate [FP (%)]: It represents the percentage that a test erroneously detected a change when no changes are present.

2) FN Rate [FN (%)]: It represents the percentage that an existing change is not detected.

3) Accuracy [Acc (%)]: It represents the percentage that changes are accurately detected when they occur; Acc = 1-FP-FN.

4) Delay (Del in Samples): It measures the promptness in change detection. A delay is recorded only when the change is accurately detected; both the mean and the standard deviation (in parentheses) are also provided.

5) Computational Time [CT(s)]: It measures the execution time needed to execute the test (reference platform: Intel Xeon X5650 at 2.66 GHz, 48 GB RAM, MATLAB R2011b). Results are averaged over 500 runs.

D. Abrupt Versus Drift Changes  The first experiment refers to an unidimensional gaussian distribution. The pdf in stationary conditions is N(0, 0.5), and changes start at sample 6001 with the pdf shifting to N(0.5, 0.5). The window sizes are n = m = 100. During the training phase, the first 2000 instances are used to derive the threshold T? associated with the predefined FP rate ? = 1%.

During the test phase, the whole training set is used so that n? = Nt; the new threshold is T?? as given in (19).

Fig. 3 shows how the detection method operates in the case of an abrupt type of change [Fig. 3(a) and (c)] and a drift one [Fig. 3(b) and (d)].

The blue solid lines and the red dotted ones in Fig. 3(a) and (b) show the change location and the detected location, respectively. Changes can be detected immediately once the differences between Zp and Zq are significant, which explains why significant abrupt changes are detected earlier, whereas slow drifts introduce a larger detection latency.

E. FP and FN Rates  Here, we design two experiments applied to synthetic appli- cations D1?D6 to verify how the real FP rates are aligned with the predefined, expected, ones; we then investigate FN rates.

The experiments follow the same training procedure described in Section IV. Then, the first experiment referring to FP rates continues to work on a stationary dataset, i.e., p(x) = q(x). 2Mt subsets are randomly generated to provide Mt esti- mates, Mt of which populate Zp (size n) and Mt populate the test set Zq (size m). The second experiment about the FN rates    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

BU et al.: INCREMENTAL CHANGE DETECTION TEST BASED ON DENSITY DIFFERENCE ESTIMATION 7  (a) (b)  (c) (d)  Fig. 3. Examples of detection performances with different change types: (a) abrupt and (b) drift changes. (c) and (d) Detection results, the blue line refers to the estimated LSDD values; and the red dotted line is the threshold.

A change is detected once an LSDD value is above the threshold.

(a) (b) (c)  (d) (e) (f)  Fig. 4. Real FP rates versus the expected FP rates over different experiments.

Lines represent the results with different (n, m) sizes. In the abscissas we have the expected FP value, on the ordinates the real computed FP rates.

(a)?(f) D1?D6.

works on a nonstationary dataset where p(x) ?= q(x). Finally, the real FP rate is computed on the first test, as the ratio of FPs on Mt tests, while the FN rate is experimentally assessed on the second experiment as the ratio of FNs.

In order to show the effectiveness of self-adaptive thresh- olds and the influence on FN rates, various combinations of sizes n?, m? ? 100 are considered during the test phase. The new threshold T??? is derived according to (20). In this exper- iment, Mt is set to 2000, the predefined FP rates belong to set {10%, 2%, 1%, 0.2%, 0.1%}, and sizes n?, m? to {100, 200}.

Experiments are repeated 200 times to compute averaged FP and FN rates.

Results are shown in Figs. 4 and 5, respectively. Each subfigure in both figures shows the results on each applica- tion: (a)?(f) D1?D6 in Fig. 4; (a) D2, (b) D3, (c) D5, and (d) D6 in Fig. 5. In the two figures, the abscissas refer to the predefined FP rates, while ordinates refer to averaged FP  (a) (b)  (c) (d)  Fig. 5. Real FN rates under different predefined FP rates. Lines represent the results with different (n, m) sizes. In the abscissas we have the predefined FP rates, on the ordinates the real computed FN rates. (a) D2. (b) D3. (c) D5.

(d) D6.

and FN rates, respectively. Lines represent results with differ- ent (n, m) sizes. Since drift changes in applications D1 and D4 are with different change rates, we do not record their FN rates.

As shown in Fig. 4, the real FP rates with n, m = 100 are close to the predefined ones, situation which indicates that the proposed method is effective in controlling the FP rates. Moreover, the FP rates with sizes n(m) > 100 are much smaller than the predefined values with n = 100, which yields to the expected conclusion that a larger window size helps to achieve lower FP rates. Results with cases (100, 200) and (200, 100) are similar and the corresponding lines overlap.

In Fig. 5(a) and (b), lower FP rates correspond to higher FN rates when the two distributions ?Ho and ?H1 overlap, which can be verified in Fig. 1. When changes are significant, the FN rates stay at zero as shown in Fig. 5(c) and (d).

F. Change Detection Performance  In this section, we compare the LSDD-Inc detection method with existing methods. Two different window sizes with n = m = 100 and n = m = 200 are applied to all methods, and dur- ing the test phases with LSDD-Inc and LogKStest, we update the reference Zp with the whole training set, i.e., n? = Nt.

For LSDD-Inc2, n = m = 100 during the training phase and n = Nt, m? = 200 in the test phase. The detection performance is shown in Table II; ND represents not detected.

In order to show performances of different methods, we apply statistical tests on the detection accuracy [Acc(%)].

For instance, the Friedman test verifies the significance of differences [37]. The test ranks the jth method for application Di r ji performance (1 is the best method). Ranking results are given in Table III, where average ranks are assigned in case of ties. The ranks for each method are averaged Rj = (1/7)? r ji to compute the statistics  FF = 11.51 where FF satisfies an F-distribution with 9 and 54 degrees of freedom. However, the critical value of such a distribu- tion F(9, 54) with confidence level ? = 0.05 is 2.0585,    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

TABLE II CHANGE DETECTION PERFORMANCE ON DIFFERENT APPLICATIONS  which is much smaller than FF . Therefore, the null hypoth- esis is rejected, which means these detection accuracies are significantly different.

Then, we considered the Nemenyi test. The test investi- gates the differences between averaged ranks and the critical difference (CD)  CD = 5.1205.

However, most of the differences between Rj(j = 1, . . . , 10)  are smaller than CD, which rejects the hypothesis about significant difference. Here, we can claim that LSDD-Inc2 sig- nificantly outperforms LogKStest, whereas no such conclusion can be confirmed for tests LSDD-Inc2 and LSDD-CDT.

We further conducted a pairwise comparison with the Wilcoxon Signed-Ranks Test. The method ranks the  differences in accuracy of two methods for each dataset, and computes a statistic z, where the null hypothesis is rejected if z is smaller than ?1.96 at confidence 0.95 [37]. It should be noted that for fair comparison, only different methods with the same window sizes or the same methods with different sizes are tested in pairs. Those above average rank 5.5, are removed because of their poor performance.

Results are shown in Table IV, where 1 indicates the rejec- tion of the null hypothesis so that the compared methods are significantly different, 0 means nonrejection and * says that no comparison can be carried out. We can conclude the following.

1) H-ICI shows no significant detection performance dif- ferences when compared with other methods.

2) LSDD-Inc2 is significantly better than other methods except H-ICI.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

BU et al.: INCREMENTAL CHANGE DETECTION TEST BASED ON DENSITY DIFFERENCE ESTIMATION 9  TABLE III COMPARISON ON DETECTION ACCURACY: FRIEDMAN TEST  TABLE IV PAIRWISE COMPARISON: WILCOXON SIGNED-RANKS TEST  3) Methods with larger window sizes, i.e., n = m = 200, outperform those with smaller ones, as expected.

4) Given the same window sizes, LSDD-Inc works signif- icantly better than LSDD-CDT and LogKStest, but only slightly better than HDDDM.

It can be concluded that LSDD-Inc can detect changes more accurately than H-ICI and other methods when apply- ing the same window sizes, and enlarging the test set Zq will improve the detection performance with smaller FP rates, like LSDD-Inc2.

LogKStest shows the worst performance with the highest FP rates, although it reports a contained latency. Moreover, it can not deal with applications with categorical features, i.e., discrete values, because the estimation of pdfs with GMM fails to converge. In other cases with continuous pdfs, increasing sizes n, m will reduce the FP rates but cause larger detection delays.

HDDDM demonstrates similar performance as LogKStest especially with small window sizes, whereas it does not limit the attribute types. With the increase of n and m, the FP rates decrease, while the FN rates and detection delays unexpectedly increase. This mainly results from the inappropriate updating of the reference set. The data set has to be large enough to perceive the difference. In this case, if changes are not detected timely, nonstationary instances in Zq will be removed into Zp which may change the underlying pdf of the reference set.

H-ICI has the highest accuracy in some applications, but it fails to detect changes in multidimensional applications (D2-3) and application with categorical features (D6).

The LSDD-CDT presents acceptable results. However, since the reference set Zp is almost fixed with only n samples, the FP rate appears to be high. Furthermore, the execution time  is high because of the exhaustive computation of deriving ?? and h? with (n + m) samples each time.

LSDD-Inc shows excellent performance with accurate detec- tion in most applications. The detection delay is smaller than other methods when dealing with abrupt changes, and com- parable when detecting drift ones. It also shows that with the increase of window sizes, the delay decreases in the drift cases, whereas it increases in the abrupt ones. This happens since in the drift case, more nonstationary samples in Zq help to reveal the differences between Zp and Zq earlier. However, in the abrupt case, a larger window includes more stationary samples which lowers the differences. LSDD-Inc fails in application D7 where a small fluctuation occurs before the artificial change.

The problem can be solved by considering a larger window size. In addition, thanks to the incremental computation, the execution time reduces compared to LSDD-CDT.

LSDD-Inc2 provides the highest accuracy. Thresholds T??? with n? = Nt, m? = 2m contribute to reduce the FP rates as ana- lyzed in Section III. FN rates are 0 in all the applications, since distributions ?H0 and ?H1 of estimates D?  ? weakly overlap.

Even if H-ICI works perfectly in 1-D applications with con- tinuous data, LSDD-Inc and LSDD-Inc2 tests outperform other methods.



VI. CONCLUSION  In this paper, we propose an incremental change detection algorithm based on the LSDD method (LSDD-Inc). We prove that in stationary conditions, the estimate D?2? with fixed ker- nel centers is distributed as a linear combination of k(k + 1) nonindependent noncentral Chi-square distributions. We pro- vide a theoretical bound between the window size and FP rate, which permits the test to adapt the window size to improve detection performance without the need to retrain. During the training phase, a bootstrap-based distribution is considered to approximate the real one, and thresholds are derived accord- ing to the desired FP rates. When the window sizes increase, new thresholds can be determined directly from already avail- able estimates. For online detection, we also estimate D?2? incrementally.

Comprehensive experiments show that the proposed method LSDD-Inc provides good performances in terms of promptness and accuracy.

APPENDIX A  CHOICE OF THE REGULARIZATION PARAMETER ?  Properties of Matrix H  As shown in (5), H is a real symmetric matrix. As such: 1) H can be decomposed as H = V	VT , where V is an  orthogonal matrix and 	 is a diagonal one; 2) all the elements on the diagonal Hj,j(j = 1, . . . , k) are  equal to (?? 2) d/2  . Furthermore, Hj,j = max(H) > min(H) > 0, where max(H) and min(H) describe the maximum and minimum values of elements in H, respectively.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Given any nonzero vector Z = {zj, j = 1, . . . , k}  ZTHZ = k?  i=1  k? j=1  zizjHi,j  > min(H) k?  i=1  k? j=1  zizj  = min(H) (  k? i=1  zi  )2 > 0  and matrix H is positive definite.

Write the diagonal matrix 	 as  =  ? ???  r1 r2  ? ? ? rk  ? ???  where without loss of generality r1 > r2 > ? ? ? > rk > 0. We have that r1r2 ? ? ? rk = |H| < (?? 2)kd/2, and ?ki=1 ri = k.

Two Equivalent Expressions  From (9) and (10), the two equivalent expressions can be expressed as  D?21(p, q) = h?T?? = h?T(H + ?I)?1h? D?22(p, q) = ??TH?? = h?T(H + ?I)?TH(H + ?I)?1h?.

Since (H +?I) could be decomposed as V	?VT with 	? = + ?I, we transform the two expressions as  D?21(p, q) = h?TV	?1? VTh? D?22(p, q) = h?TV	?1? 		?1? VTh?  where  ?1? =  ? ????  r1+?  r2+? ? ? ?  rk+?  ? ????  ?1? ?1 ? =  ? ????  r1 (r1+?)2 r2  (r2+?)2 ? ? ? rk  (rk+?)2  ? ????.

When ? > 0, (1/ri) > [1/(ri + ?)] > [ri/((ri + ?)2)] > 0, i = 1, . . . , k, where (1/ri) is the ith eigenvalue of H?1. We can therefore conclude that D?21(p, q) > D?  2(p, q) > 0.

Choice of Parameter ?  D?21(p, q) and D? 2(p, q) can be weighted with parameter a to  reduce the bias introduced by ? as  D?2?(p, q) = ah?T?? + (1 ? a)??TH??.

We have that  D?2?(p, q) = h?TV  ? ????  r?1  r?2 ? ? ?  r?k  ? ????VTh? (23)  where (1/r?i) = [(ri + a?)/((ri + ?)2)] and its derivative with respect to ? is  d (  r?i  )  d? = (a ? 2)ri ? a?  (ri + ?)3 . (24)  In addition, the ratio between the original eigenvalue (1/ri) and the new one is  ri r?i  = 1 ri  ? (ri + ?)  ri + a? = 1 + (2 ? a)ri? + ?2  ri(ri + a?) . (25)  From (24), when 0 ? a ? 2, the derivative is smaller than 0, which indicates the decreasing property of the new eigenvalues. The right-hand side of (25) is greater than 1 so that (1/ri) > (1/r?i). Thus, with the decrease of ?, the value of (1/r?i) increases and approaches (1/ri), i.e., the smaller ? the smaller the bias.

By expanding (1/r?i) with Taylor   r?i = 1  ri + (a ? 2)?  ri2 ? (2a ? 3)?   ri3  + ? ? ? + (?1)n+1 (na ? (n + 1))? n  rin+1 + Rn(?) (26)  when 0 < ? < 1, the setting of a = 2 could eliminate the influence brought by the low-order terms of ?. Finally, we have  D?2?(p, q) = 2h?T?? ? ??TH?? and (1/r?i) = [(ri + 2?)/((ri + ?)2)] where (1/r?1) < (1/r?2) < ? ? ? < (1/r?k) < (2/?).

On the other hand, ? is required to control overfitting which is essential to avoid the singularity of H. A method to control the RD between D?21 and D?  2 has been proposed in [23]  RD = h? T?? ? ??TH??  h?T?? = 1 ? ??  TH??  h?T?? that the largest ? (? < 1) with corresponding RD smaller than a given constant is selected.

APPENDIX B  PROOF OF THEOREM 1  D?2?(p, q) can be represented as D? ?(p, q) = h?TH?1? h? =?k  i=1 ?k  j=1 H ?1 ?(i,j)h?ih?j with H  ?1 ? = V(2	?1? ?	?1? 		?1? )VT .

h?ih?j = (1/4)(h?i + h?j)2 ? (1/4)(h?i ? h?j)2 with  h?i + h?j = 1 n  n? l=1  ( ? ( xp,l, ci  )+ ?(xp,l, cj))  ? 1 m  m? l=1  ( ? ( xq,l, ci  )+ ?(xq,l, cj))  h?i ? h?j = 1 n  n? l=1  ( ? ( xp,l, ci  )? ?(xq,l, cj))  ? 1 m  m? l=1  ( ? ( xp,l, ci  )? ?(xq,l, cj))  where ?(xl, ci) = exp(?[(||xl ? ci||22)/2? 2]). Assume ?(xp,l, ci) + ?(xp,l, cj), ?(xq,l, ci) + ?(xq,l, cj), ?(xp,l, ci) ?    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

BU et al.: INCREMENTAL CHANGE DETECTION TEST BASED ON DENSITY DIFFERENCE ESTIMATION 11  ?(xq,l, cj), and ?(xp,l, ci) ? ?(xq,l, cj) follow distributions with the means and variances as (upij+, ?2pij+), (uqij+, ?2qij+), (upij?, ?2pij?), and (uqij?, ?2qij?), respectively. The central limit theorem guarantees that, when n and m are sufficiently large, the following terms converge to Gaussian distributions:   n  n? l=1  ( ? ( xp,l, ci  )+ ?(xp,l, cj)) d?? N (  ?pij+, ?2pij+  n  )   m  m? l=1  ( ? ( xq,l, ci  )+ ?(xq,l, cj)) d?? N (  ?qij+, ?2qij+  m  )   n  n? l=1  ( ? ( xp,l, ci  )? ?(xq,l, cj)) d?? N (  ?pij?, ?2pij?  n  )   m  m? l=1  ( ? ( yp,l, ci  )? ?(yq,l, cj)) d?? N (  ?qij?, ?2qij?  m  ) .

Considering the case with fixed kernel centers {ci, i = 1, . . . , k}, matrix H as well as H?1? are fixed. In this case, since variables xp and xq are independent and the exponen- tial functions are measurable, h?i + h?j and h?i ? h?j also follow gaussian distributions:  h?i + h?j d?? N (  ?pij+ ? ?qij+, ?2pij+  n + ?  qij+ m  )  h?i ? h?j d?? N (  ?pij? ? ?qij?, ?2pij?  n + ?  qij? m  ) .

Therefore, (h?i + h?j)2/([(?2pij+)/n] + [(?2qij+)/m]) and (h?i ? h?j)2/([(?2pij+)/n] + [(?2qij+)/m]) are noncentral Chi-square dis- tributed with 1 degree of freedom. Since we have established the preliminary results, the proof of Theorem 1 is as follows.

Proof (Theorem 1): h?ih?j = (1/4)(h?i+h?j)2?(1/4)(h?i?h?j)2 is distributed as a combination of two nonindependent noncentral Chi-square distributions.

Following the symmetry of matrix H, we have (H?1? )T = 2(H + ?I)?T ? (H + ?I)?1HT(H + ?I)?T = H?1? so that H?1? is symmetric.

As a consequence, D?2?(p, q) = ?k  i=1 ?k  j=1 H ?1 ?(i,j)h?ih?j is dis-  tributed as a linear combination of k(k + 1) nonindependent noncentral Chi-square distributions.

APPENDIX C  PROOF OF THEOREM 2  Expectation With Finite Samples  Based on the analysis in Appendix B, we compute the expectations  E  (( h?i + h?j  )2) = D(h?i + h?j )  + E2 (  h?i + h?j )  = ? pij+ n  + ? qij+ m  + (?pij+ ? ?qij+)2  E  (( h?i ? h?j  )2) = D(h?i + h?j )  + E2 (  h?i + h?j )  = ? pij? n  + ? qij? m  + (?pij? ? ?qij?)2  E (  h?ih?j )  = 1  E  (( h?i + h?j  )2)? 1  E  (( h?i ? h?j  )2)  = 1  (( ?2pij+  n + ?  qij+ m  ) + (?pij+ ? ?qij+)2  ? (  ?2pij? n  + ? qij? m  ) ? (?pij? ? ?qij?)2  ) .

Proof [Theorem 2 1)]: When xp and xq are generated from the same distribution, i.e., under H0 with p(x) = q(x), we have ?2pij+ = ?2qij+, ?2pij? = ?2qij?, ?pij+ = ?qij+, and ?pij? = ?qij?.

Therefore  EH0  ( D?2?(p, q)  ) =  k? i=1  k? j=1  H?1?(i,j)E (  h?ih?j )  = 1  (  n + 1  m  ) k? i=1  k? j=1  H?1?(i,j) ( ?2pij+ ? ?2pij?  )  which shows that by increasing sizes n and m, the expectation values of estimated LSDD values under H0 decrease. When H1 holds, i.e., p(x) ?= q(x)  EH1  ( D?2?(p, q)  )  = 1  k? i=1  k? j=1  H?1?(i,j)  (( ?2pij+  n + ?  qij+ m  ) + (?pij+ ? ?qij+)2  ? (  ?2pij? n  + ? qij? m  ) ? (?pij? ? ?qij?)2  )  which indicates in the nonstationary conditions, the expecta- tion values of D?2? also decrease with the increase of n and m.

Proof [Theorem 2 2)]: The difference  EH1  ( D?2?(p, q)  ) ? EH0  ( D?2?(p, q)  )  = 1  k? i=1  k? j=1  H?1?(i,j) (   m  ( ?2qij+ ? ?2pij+ ? ?2qij? + ?2pxj?  )  + (?pij+ ? ?qij+)2 ? (?pij? ? ?qij?)2 )  (27)  which is independent of n, but on m only. In other words, EH1(D?  ?(p, q)) decreases with the increase of n, whereas the  difference between the expectations under H0 and H1 is fixed no matter how n varies.

Deviation Bounds for Tests With Finite Samples  In order to obtain the upper bound of the difference between D?2?(p, q) and its expectation E(D?  ?(p, q)) under both hypothe-  ses H0 and H1, we apply the McDiarmid bound [38], [39] on the estimate D?2?(p, q).

Proof [Theorem 2 3)]: Let n + m independent vari- ables xp,1, . . . , xp,n, xq,1, . . . , xq,m sampled from some set A, and assume that D?2?(p, q) = f (xp,1, . . . , xp,n, xq,1, . . . , xq,m): Am+n ? R. Changing either xp,l or xq,l in f results in changes    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

of h? of at most (1/n) or (1/m). We first consider the case with h??i = h?i + (1/n), and ?1 ? h?i, h??i ? 1  sup xp,1,...,xq,m,x?p,l  ???D?2?(p, q) ? D??2? (p, q) ???  = ???h?TH?1? h? ? h??TH?1? h??  ???  = ??????  k? i=1  k? j=1  H?1?(i,j)h?ih?j  ? k?  i=1  k? j=1  H?1?(i,j) (  h?i + 1 n  )( h?j + 1  n  )??????  = ??????  k? i=1  k? j=1  (  n h?iH  ?1 ?(i,j) +   n2 H?1?(i,j)  )??????  ? 2 n  k? i=1  k? j=1  ???h?i ??? ???H?1?(i,j)  ???+ 1 n2  ?????? k?  i=1  k? j=1  H?1?(i,j)  ??????.

H?1? can be expressed as  H?1? = V  ? ????  r?1  r?2 ? ? ?  r?k  ? ????VT (28)  and H?1?(i,j) = ?k  l=1 (1/r?l)Vi,lVj,l. Since V is an orthogo- nal matrix, it is obvious that (1/r?1) < H  ?1 ?(i,i) < (1/r?k),  i = 1, . . . , k. We then estimate the bound of H?1?(i,j) as  H?1?(i,i) + H?1?(j,j) + 2H?1?(i,j) = k?  l=1   r?l  ( Vi,l + Vj,l  )2  <  r?k  k? l=1  ( Vi,l + Vj,l  )2 = 2 r?k  H?1?(i,i) + H?1?(j,j) + 2H?1?(i,j) >  r?1  k? l=1  ( Vi,l + Vj,l  )2 = 2 r?1  .

Therefore, (2/r?1) < H ?1 ?(i,i)+H?1?(j,j)+2H?1?(i,j) < (2/r?k), and we  can derive that |H?1?(i,j)| < (1/r?k) ? (1/r?1) < (1/r?k) < (2/?).

In addition  ?????? k?  i=1  k? j=1  H?1?(i,j)  ?????? = ???????  k? i=1   r?i  ? ? k?  j=1 Vji  ? ?  ???????  <  ???????  r?k  k? i=1  ? ? k?  j=1 Vji  ? ?  ???????  = k r?k  < 2k  ? .

The upper extreme satisfies  sup xp,1,...,xq,m,x?p,l  ???D?2?(p, q) ? D??2? (p, q) ???  < 2k2  n   ? + 1  n2 2k  ? = 2k(2nk + 1)  n2? .

The same procedure can be applied when considering h??i = h?i + (1/m), and we obtain  sup xp,1,...,xq,m,x?q,l  |D?2?(p, q) ? D? ?2 ? (p, q)| <  2k(2mk + 1) m2?  .

Consequently, according to McDiarmid?s Inequality [38], [39], for any ? > 0, we have  Pr (???D?2?(p, q) ? E  ( D?2?(p, q)  )??? ? ? )  ? exp ? ?? ?2?2  4k2(2nk+1)2 n3  + 4k2(2mk+1)2 m3  ? ?  where Pr denotes the probability over n samples with pdf p(x) and m with q(x).

The inequality reveals some properties: 1) with the increase of sizes n and m, the deviation bounds  decrease; 2) a larger ? or a smaller k (less centers) will bring a larger  deviation bound; 3) without adding other restrictions, the above inequality  applies to both cases under H0 and H1.

