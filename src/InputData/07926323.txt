See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

ABSTRACT | In this paper, we discuss and review how combined  multiview imagery from satellite to street level can benefit scene  analysis. Numerous works exist that merge information from  remote sensing and images acquired from the ground for tasks  such as object detection, robots guidance, or scene understanding.

What makes the combination of overhead and street-level images  challenging are the strongly varying viewpoints, the different  scales of the images, their illuminations and sensor modality,  and time of acquisition. Direct (dense) matching of images on a  per-pixel basis is thus often impossible, and one has to resort to  alternative strategies that will be discussed in this paper. For such  purpose, we review recent works that attempt to combine images  taken from the ground and overhead views for purposes like scene  registration, reconstruction, or classification. After the theoretical  review, we present three recent methods to showcase the interest  and potential impact of such fusion on real applications (change  detection, image orientation, and tree cataloging), whose logic  can then be reused to extend the use of ground-based images in  remote sensing and vice versa. Through this review, we advocate that cross fertilization between remote sensing, computer vision, and machine learning is very valuable to make the best of geographic data available from Earth observation sensors and ground imagery. Despite its challenges, we believe that integrating these complementary data sources will lead to major breakthroughs in Big GeoData. It will open new perspectives for this exciting and emerging field.

KEYWORDS | computer vision, remote sensing, ground based  imagery, localization, classification, object detection, data fusion   I .  IN TRODUCTION  Traditionally, interpretation of satellite and aerial imagery has been approached separately from scene analysis based on imagery collected at street level. The reasons are par- tially historical: the Earth observation and remote sensing community has worked on overhead images whereas the computer vision community has largely focused on map- ping from data acquired at ground level. In this paper, we advocate to view these efforts as complementary. By taking into account both points of view in a common modeling framework, we aim at achieving complete, yet accurate modeling, as we observe multiple facets of envi- ronmental systems simultaneously: the differences in spatial and radiometric resolutions, not to speak of the drastic changes in perspective, allow studying processes under different viewpoints, thus potentially leading to one, holistic, complete reconstruction of the scene.

A. Earth Observation  Earth observation has experienced a significant evo- lution during the last decades, due to advances in both hardware and software. Manual interpretation of analog, aerial surveys that were limited to campaigns at city scale, was the dominant technology for most of the 20th cen- tury [1]. The launch of satellite sensors for Earth obser- vation (EO) changed this situation and allowed a signifi- cant improvement in terms of spatial scale and temporal coverage. Since Landsat 1 (1972) and its 80-m spatial resolution and 18-day revisit period, the ground sampling distance, image quality, and availability of EO data have been growing rapidly. Today, state-of-the-art satellites offer decimetric spatial resolution (e.g., WorldView-2 and 3, Pleiades); their constellations offer weekly or even daily revisit cycles (e.g., RapidEye, Copernicus Sentinels).

New swarm-like satellite schemes, which rely on a mul- titude of small, low-cost sensors, are currently changing the field and will soon have an impact on how we work in  Manuscript received December 14, 2016; revised March 13, 2017; accepted March 14, 2017.

S. Lefevre and A.S. Nassar are with the IRISA institute, Universit? Bretagne Sud, Vannes, France (e-mail: sebastien.lefevre@irisa.fr; ahmed-samy-mohamed.

nassar@irisa.fr).

D. Tuia is with the MultiModal Remote Sensing group, University of Zurich, Switzerland and with the Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, the Netherlands. (e-mail: devis.tuia@ geo.uzh.ch).

J.D. Wegner is with the Photogrammetry and Remote Sensing group, ETH Zurich, Switzerland (e-mail: jan.wegner@geod.baug.ethz.ch).

T. Produit is with G2C institute, University of Applied Sciences, Western Switzerland (e-mail: timothee.produit@heig-vd.ch).

Toward Seamless Multiview Scene Analysis From Satellite to Street Level By S?BaStien Lef?vre, DeviS tuia, Jan Dirk Wegner, timoth?e ProDuit, anD ahmeD Samy naSSar    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  2 Proceedings of the IEEE  EO (e.g., PlanetLabs or Skycube). This ever-increasing data flood can only be analyzed if we rely on automated methods that allow close to on-the-fly image interpretation for such big data. Current scientific efforts in computer vision, pat- tern recognition, machine learning, and remote sensing are making considerable progress, especially since the come- back of deep learning.

B. Street-Level Images and Social Media  The massive amounts of geotagged images on social media (e.g., Flickr, Facebook, Instagram) provide another possibility for big data solutions [2]. Flickr gathers 122 millions of users, sharing daily one million of photos, for a total of ten billion shared images. Beyond generic photo- sharing platforms such as Flickr, some geolocation-oriented solutions have been introduced in the last years. 360cities provides a collection of high-resolution georeferenced pano- ramic photos. Panoramio was a geolocation-oriented photo- sharing platform, offering 94 millions of photos (with 75 billions of views). It is now part of Google Maps, illustrating the convergence between ground-level imagery and remote sensing technologies. Mapillary aims to share geotagged photos in a crowdsourced initiative to offer an immersive street-level view of the world. Its 100 millions of photos are exploited with computer vision techniques in order to detect and recognize objects, perform 3-D modeling, or text/sign understanding.

In addition to crowdsourced geotagged images, Google Street View [3], Bing Maps Streetside, and Apple Maps are offering ground-level views that are acquired with planned mobile mapping campaigns at worldwide scale. Dedicated imaging campaigns guarantee much denser and complete data coverage (i.e., all street scenes of a city) as opposed to most crowdsourcing efforts that usually do not have much success beyond tourist sights. Imagery acquired with large campaigns usually facilitates processing because data are provided in a homogeneous format and exhibit less illumina- tion change. In addition to worldwide imaging campaigns, there are many similar initiatives at the national or regional scale, e.g., Tencent Maps in China, Daum Maps in South Korea, Mappy in France, GlobalVision VideoStreetView in Switzerland, CycloMedia in various European countries and United States cities, etc.

Geotagged ground-based images shared via social media (captured by pedestrians, cars, etc.) thus appear as a rele- vant complementary data source for EO applications. These data provide a high-resolution viewpoint from the ground that offers much potential to add missing information to overhead imagery to map and monitor natural and human activities on Earth. Furthermore, it allows circumnavigating typical restrictions of remotely sensed data: it is not affected by cloud coverage nor limited to nadir or oblique views, and it is widely available via the pervasive use of mobile phones, which allows for immediate feedback. Nonetheless, this  imaging modality comes with its own specific challenges, including a large variability in terms of acquisition condi- tions (leading to various perspective effects, illumination changes), frequent object occlusions, or motions. Successful coupling of ground-level and remote sensing imagery requires addressing their differences in viewpoint, sensor modality, spatial and temporal resolutions, etc.

C. Contributions  We present a thorough and detailed review of works that combine overhead and street-level images. Three pro- jects are described in detail to complete this survey; each of these presents a different method for a geodata task, and emphasizes the impact of cross-fertilizing computer vision and remote sensing. This paper is meant to survey existing methods for aerial-to-ground image combination, to high- light current challenges but also to point in promising direc- tions for future work, by showcasing three recent applica- tions from the authors? respective groups [4]?[6]. We hope it will serve as a motivation for colleagues working in EO, remote sensing, machine learning, and computer vision to more closely cooperate, to share ideas, and to show that there are many common interests.

D. Paper Organization  The paper is organized as follows. We provide in Section II a survey of the literature in ground-aerial/spatial match- ing, and we review existing works considering three specific problems that are geolocalization, matching and reconstruc- tion, and multimodal data fusion. These various problems are illustrated in Section III through an in-depth presenta- tion of some selected examples that correspond to some of the authors? previous works. The three presented systems aim at coupling ground imagery and remote sensing to per- form change detection, image geolocalization and orienta- tion, and tree cataloging, respectively. Finally, we conclude this paper with Section IV.

II .  LITER AT U R E SU RV EY  Ground-to-aerial matching of images or point clouds has received much attention in research, but is still largely unsolved. In the following, we roughly subdivide literature into three main application scenarios that call for different approaches to the problem: geolocalization, object detec- tion and reconstruction, and multimodal data fusion.

A. Geolocalization  A great number of pictures found on photo-sharing plat- forms do not provide information about their spatial loca- tion. If available, it is either derived from the GPS of the device or an approximate location is entered manually by the photographer at the time of upload.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  Proceedings of the IEEE 3  However, a large number of images without geolocali- zation is available on social media and could be used, for example, to improve indexing, facilitate search, and offer augmented reality experiences. For these reasons, locating pictures has become a fast moving field in computer vision, which has attracted attention in recent research.

Early works focused on the use of geolocated images to extract semantic geographic information and enrich global maps. Proximate sensing has been used by Leung and Newsam [7] to build a land cover map of a 100  ?  100 km region of England (with a 1-km spatial resolution). To do so, input pictures are extracted from the Geograph database based on their grid location. Such a system remains tribu- tary of having all pictures georeferenced. It is also possible to match photos that do not have geotag with those in a data- base made of geolocated ones. The IM2GPS system from Hays and Efros [8] retrieves the pictures, which are the most similar to those in the database and is used to study whether these images are also geographically close in space or if they represent more spatially diffuse (and, therefore, difficult to precisely locate) types of land cover. Existing land cover and light pollution global maps (and their scores at the geolo- cated pictures? location) are also used to estimate land cover and light pollution in all the pictures.

However, the applicability of these approaches is reduced to popular locations (often tourist sights), where most web-shared images are taken and for which data avail- ability (and even redundancy) can be guaranteed. One pos- sibility to adapt geolocalization models to wider areas is matching ground-level photos to overhead images. Aerial or satellite images at very high resolution are available almost everywhere on the planet and provide a much more com- plete and homogeneous image database. A true challenge that received much attention in recent research is finding correspondences between overhead and ground-level views.

Dense, pixel-wise image matching is literally impossible due to the great changes in viewpoint. Overhead images and photo collections on the ground also differ greatly in resolution, illumination, etc., which calls for very robust, truly multimodal approaches to search for correspondences across aerial and ground views.

Recent works aim at developing feature extraction methods that define common representation spaces, in which a ground photo is projected close to the correspond- ing aerial image patch. Kernel canonical correlation analysis (KCCA [9]) has been used by Lin et al. [10] to learn the relationships between pictures and aerial images or land cover maps available online. The KCCA score is used to map geographically the similarity of a picture with appearance observed from above. This way, one can evaluate the confi- dence of the localization in the geographical space and, for example, discriminate pictures of vague environments (e.g., a beach) from pictures with strong location uniqueness (e.g., the Eiffel tower). Deep neural networks play a key role in works aiming at image localization using overhead imagery  (e.g., [11]), with Siamese Networks being the most popular variant in current systems [12]?[14]. We describe a method that relies on Siamese CNNs as a workhorse to perform change detection in Section III-A.

In robotics, localization of a rover has been tackled by the joint use of overhead imagery and ground sensors embedded onto the vehicle. Among existing systems, we can cite the use of a ground LiDAR to detect tree stems in forest environment [15], where the detected candidates are then matched with tree centers extracted from RGB aerial image analysis to locate the rover. Videos on the rover are com- pared to very-high-resolution grayscale images in [16], and location is retrieved using a particle filter. Finally, Google Maps images can be matched with panoramic images from multiview cameras installed on the rover [17], by creating a synthetic overhead view using the images from the ground cameras and performing keypoint matching.

B. Matching and Reconstruction  While geolocalization often requires only sparse cor- respondences across views, object detection and particu- larly reconstruction calls for more precise and often dense matching between data of different viewpoints. Per-pixel matching from aerial to ground is an extreme wide-baseline matching problem. Direct (dense) matching of aerial nadir images and street-level panoramas is very hard, because it requires compensating for 90? viewpoint changes, and usu- ally also scale differences. Street-level images often have much higher resolution and show more details than aerial or satellite images. Much of the scene can only be observed from either street-level or overhead perspective, for exam- ple, building roofs can hardly be captured from street level, but dominate building appearance in overhead images (the inverse counts for building facades). As a consequence, most approaches try to avoid per-pixel matching and propose late fusion of detection or reconstruction outcomes. Objects are detected or reconstructed separately per viewpoint, and results are combined.

Nevertheless, many works aim at fine-grained 3-D reconstruction of cities, where missing parts of the scene, invisible from the air, are added from street-level imagery.

The ultimate goal is a complete, detailed, watertight 3-D model (in mesh or voxel representation) at city scale that allows seamless navigation in 3-D.

To the best of our knowledge, the first attempt to this aim is the work of Fr?h and Zakhor [18], [19]. They densely model building facades using a mobile mapping system equipped with laser scanners and a camera. Airborne laser scans are used to fill in missing data, like roofs and terrain. A digital surface model (DSM) is created from airborne scans, triangulated and textured with aerial images. Building facades modeled from street-level data are globally regis- tered to the DSM derived from airborne scans using Monte Carlo localization (a particle filter variant), which refines    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  4 Proceedings of the IEEE  initial poses of street-level scans with respect to an edge map derived from the DSM.

An alternative method is proposed by Fiocco et al. [20], who combine terrestrial and airborne laser scans for the purpose of 3-D city reconstruction. After manual coarse alignment of terrestrial and aerial scans, roof edges are detected in both data sets, projected to the horizontal plane and matched to refine rotation and translation parameters.

The combined point cloud is processed to a global triangle mesh, which is further filtered via a volumetric octree. The final mesh is extracted by contouring the iso-surface with a feature-preserving dual contouring algorithm.

In contrast to the previous two approaches, Kaminsky et al. [21] do not combine multiple laser scans in 3-D, but aim at aligning 3-D terrestrial point clouds computed from images via structure-from-motion to a 2-D overhead image.

They estimate the optimal alignment with an objective func- tion that matches 3-D points to object edges in the overhead image. B?dis-Szomor? et al. [22] propose an approach that efficiently combines a detailed multiview stereo point cloud with a coarser, but more complete point cloud acquired with an airborne platform to reconstruct one, joint surface mesh.

At its core, the method does point cloud blending, by prefer- ring points stemming from street-level data over airborne points, and volumetric fusion based on ray casting across a 3-D tetrahedralization of the model. Bansal and Daniilidis [23] match street view pictures to a 3-D model of the sky- line rendered by modeling the buildings using LiDAR scans, whereas Hammoud et al. [24] use a set of modality-specific scores to match human annotations on the pictures with quantities extracted from a series of additional modalities: presence of points of interest from Open Street Maps, build- ing cubes generated using LiDAR scans, land cover maps, and estimation of the 3-D skyline.

Other works rely only on optical images available world- wide (typically oblique and nadir images) for pixel  matching.

Shan et al. [25] propose a fully automated georegistration pipeline that can cope with ground to aerial viewpoint variation via object-specific, viewpoint-adaptive matching of key points. The approach mainly relies on planar, verti- cal building facades that can be observed in oblique aerial images, and street-level photos. Once ground images have been warped to oblique geometry, standard scale-invariant feature transform (SIFT [26]) is applied to match across views. Similar approaches are found in [27], where building facades are detected in aerial oblique images and populate a database that is used to match facades detected in pictures; and in [28], where Bing?s aerial views are matched with pictures from several sources (including Panoramio, Flickr, and Google Street View) using a self-similarity index, where facades detected in the pictures are assigned to the closest cluster of facades extracted from the aerial view. Finally, Cham et al. [29] locate Google Street View images using a vector layer of buildings footprints by finding corners and building outlines in both sources.

In mountain environments, the skyline has been used as a strong feature to locate pictures in virtual 3-D envi- ronments rendered from digital elevation models (DEMs) [30], [31]. The pictures, once located in the virtual 3-D environment, can be used for digital tourism or can be aug- mented with peaks? names and vector information. Such augmented panoramas can then be used for studying attrac- tiveness for photography of the landscape, e.g., Chippendale et al. [32] built a synthetic panorama from the global NASA digital terrain model around the area imaged in the picture and then align it to the pictures by matching salient points.

Once the pictures have been located, an attractivness index is computed for every voxel in the synthetic image. Such index accounts for socially derived aesthetics metrics, for example, the number of views and comments a picture has received on the social media platform it was hosted on. An example of picture-to-landscape model matching in the Swiss Alps is presented in Section III-B.

C. Multimodal Data Fusion  The previous two sections reviewed localization and matching by joint use of overhead and ground imagery. This section reviews recent research under the common name of multimodal data fusion, a term often used in remote sensing for strategies using multiple different overhead sensors [33] to map a certain scene. Here, this definition is extended to ground imagery. It includes approaches using ground pic- tures to enrich land cover classification or to improve object detection.

Land cover classification is probably the most studied area of remote sensing. Pictures from photo-sharing plat- forms might be used to validate the results of a land cover classification algorithm trained on overhead images [34]: as an example, Foody and Boyd [35] compare pictures centered at prediction locations to assess how accurate the GlobCover land cover layer is. Other efforts were also reported for the classification of pictures retrieved from photo-sharing platforms such as Flickr or databases such as Geograph to describe land cover: proximate sensing [7] was used to pro- duce a map of the fraction of land developed in a 100 ? 100-km region of England. The prediction is performed at a 1-km resolution, by averaging binary classification scores (developed versus undeveloped) obtained for ground images that fall into a common grid cell of the Geograph database.

The classifier used is a support vector machine (SVM) trained using edge information as descriptor. In a fol- lowup work [36], the authors compared results obtained on the Flickr and Geograph pictures: they conclude that the Geograph results are more accurate, probably since the database is explicitly built to be geographically representa- tive of land types in England. Works in [37] and [38] clas- sify Flickr pictures to predict the presence or absence of snow and compare the pattern retrieved to those obtained by satellite-based products. The patterns look similar, but    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  Proceedings of the IEEE 5  the poor spatial coverage of Flickr leads to a very sparse map compared to the one that can be obtained by satellite images.

Estima et al. [39], [40] study the effects of such poor spatial coverage at the country level and conclude that these pic- tures are not distributed evenly enough to be used alone in a land cover classification effort, thus calling for multimodal mapping systems.

Building up to that observation, some studies make suc- cessful use of both modalities (ground pictures and overhead images) to solve geospatial classification and object detection problem: Wegner et al. [6] apply a strategy for detecting indi- vidual trees at city scale from street-level and aerial images.

Tree detection is performed separately for all available street- level and overhead images, and all detections are projected to geographic coordinates followed by soft, probabilistic fusion (more details in Section III-C). Matthyus et al. [41] propose a joint system to detect roads observed in both street view and aerial images: using a structured SVM [42] they learn a set of potentials accounting for 1) road smoothness; 2) lanes size; 3) the fact that roads are often parallel to each other; 4) they enforce consistency between detections in both modalities; and 5) with Open Street Maps centerlines. Their results on the KITTI data set [43] augmented with aerial images pro- vide impressive results against state-of-the-art detectors.

Going beyond the classical tasks of land cover classifi- cation and object detection, an increasing interest is being observed on tasks related to the prediction of city attributes [44], [45]. Pictures acquired on the ground can be used to predict activities (recreational, sports, green spaces), feel- ings of security, crime rates, and so on. Such attributes are generally learned from the pictures themselves via visual attributes [44] and existing image databases, which provide tags related to the activities [45]. The attributes are then mapped in space, but few works make direct use of over- head images to provide complete maps carrying informa- tion where no pictures are available (we exclude the idea of direct spatial interpolation used in [46]): in [47], pretrained CNN models learned on picture databases (ImageNet and Places) are then applied on satellite image patches in order to see if similar geographical regions of the images were being activated by the same filters; in [48], Lee et al. predict a set of city attributes (digital elevation, land use, but also population density, GDP and proportion of infant mortal- ity) by training classifiers on Flickr pictures labeled by the scores found at their geolocation on GIS or remote sensing maps. In [49], Produit et al. use a one-class SVM [50] to predict the suitability of a spatial location for taking beauti- ful pictures: the model relates a set of geographical features (extracted from a DEM or open GIS layers) to the density of pictures found on Panoramio at that location. In [51], Luo et al. classify activities happening in the pictures using two models based on handcrafted features and fusing the clas- sification scores at the end.

A final set of works aims at transferring attributes from the pictures domain to the overhead images domain, in a  domain adaptation [52] setting. Attributes from the pictures domain would allow obtaining labels for activities unseen (and unrecoverable) in the images, as, for example, the type of activity being pursued on a grassland. Studies [53], [54] search for common embedding spaces (for instance, using subspace alignment) where images are mapped close to pic- tures depicting the same visual features. Once the mapping is done, the attributes of the nearest neighbors can then be transferred.

III .  SELECTED E X A MPLES  We describe three methods in detail to show how possible solutions for combining ground and aerial imagery may look like for specific tasks. Three systems that respectively aim to perform change detection, image geolocalization and ori- entation, and tree cataloging are explained and remaining challenges are highlighted.

A. Multimodal Change Detection Between Ground- Level Panoramas and Aerial Imagery  Despite the proliferation of EO programs, the endeavor of updating aerial or satellite high-resolution imagery is found to be quite costly and time consuming for geographi- cal landscapes around the world, particularly when they are constantly and rapidly changing. Consequently, this acts as a limit toward maintaining an updated data source. We provide an efficient and low-cost solution by relying on geo- referenced panoramic photos from digital cameras, smart- phones, or web repositories as they offer high spatial infor- mation of the location queried.

With volunteered geographic information (VGI), humans are able to be extremely beneficial to geographical information systems by acting as intelligent sensors with a smartphone equipped with a GPS and a camera. Many web services support georeferenced information: blogs, wikis, social network portals (e.g., Facebook or Twitter), but also community contributed photo collections discussed in the introduction of this paper. Such collections also benefit from the recent rise of affordable and consumer-oriented virtual reality (VR) headsets, in addition to 360? camera mounts and rigs that aim to generate panoramic spheres. As such, these new data sources will lead to new availability of an extensive collection of georeferenced panoramic photos which could open new perspectives for virtual experiences or social media  In this section, we describe a recent multimodal approach [4] to change detection, which makes use of both ground-level panoramic photos and aerial imagery.

Furthermore, we introduce here several improvements to the existing method that exploit deep neural networks to improve the overall results.

1) Method: The proposed method is summarized in Fig. 1. We first examine how to transform the georefer- enced ground-level 360? panoramic photos (panoramic    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  6 Proceedings of the IEEE  spheres) to top-down view images to resemble those being obtained from a bird?s eye view. The transformed or warped photo acquired is then compared with its counter- part aerial image from the same geolocation. This compari- son indicates whether a change occurred at that location.

This prevents routinely updating large landscapes and allows requesting such an update of the aerial image only when necessary.

a) Top-down view construction: The panoramic pho- tos being used here, obtained from Google Street View or from other sources, go through a warping procedure to get a bird?s eye view image as proposed in [55]. The first step is constructing a spherical 3-D model of the panoramic image.

World coordinates are then computed using an inverse per- spective mapping technique. Finally, bilinear interpolation is used on the panorama pixels to be able to retrieve the color for the ground location.

An example of a top-down view image reconstructed from a panoramic photo is given in Fig. 2. While this process seems to be straightforward, its quality strongly depends on the kind of landscape considered. Indeed, top-down view construction for urban areas is quite problematic compared to suburban or rural areas due to how complicated the scene is. This causes many objects such as signs, or cars to blem- ish the output image, as well as objects toward the far end of the image to get distorted heavily. Some illustrations are provided in Fig. 4 (columns 1 and 2).

b) Registration: The top-down view image built in the previous step represents only a small portion of an  aerial image to be compared with (e.g., Bing Map images are  150  ?  150 m      2  ). Hence, to compare correctly the images and detect changes, it is necessary to localize the top-down view image into the aerial image, then crop the area that contains the objects in the field of view of the Google Street View image. Similarly, as already done in document image analy- sis [56], we have explored performance comparison of SIFT, SURF, FREAK, and PHOW for matching ground images to satellite images. The comparison has proven that SIFT is a superior method for the matching process, even when the satellite image contains many elements or is complicated.

At first, SIFT [26] key points are extracted from both aer- ial and top-down images, and relevant descriptors are gener- ated. Pairs of similar descriptors are found using Euclidean distance. The best match is then selected from the group of matches through a  k -nearest neighbor ( k -NN) classifier.

We rely here on the FLANN library known for its good per- formance with  k -NN search [57]. Furthermore, to obtain geometric transformation between the matched key points, we use a homography matrix [58]. To achieve this, we elimi- nate the outliers with the RANSAC algorithm [59]. Finally, if the required amount of inliers is achieved (a minimum of 4 is considered), the homography matrix can be computed.

An example of registration is provided in Fig. 3.

c) Classification: In our previous experiments [4], we have observed that the correlation coefficient called  z -factor (see the next step and [4]) was highly sensitive to the kind of landscape to be processed (i.e., urban and rural/suburban environments), making it difficult to choose an appropriate change detection threshold.

Therefore, we introduce here an additional preclassifica- tion step to determine if the scene is rather urban or rural/ suburban. To achieve this image classification task, we rely on a standard fine-tuned AlexNet deep architecture [60] trained with Caffe [61] using a data set of aerial images with an output of two classes (urban and rural). There was no need to classify the top-down view images as well, since the pair of images in the data set are supposed to be of the same location, therefore of the same type.

d) Comparison: Detecting the change between multi- ple images of the same location has been explored in previ- ous works [62], [63]. Our initial work [4] was relying on a correlation index computed between the top-down view image and the related localized portion of the aerial image.

Fig. 1. Flowchart of the proposed method.

Fig. 2. Top-down view image (right) constructed from panoramic image (left).

Fig. 3. Aerial localized image (middle) using top-down view image (right) on original aerial image (left).

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  Proceedings of the IEEE 7  A low correlation value indicates a change. To eliminate the cases in which some areas resulted in a very low cor- relation value, the  z -factor was calculated as proposed by [64].

Seeking to attain a better accuracy, we explore here the use of Siamese Networks [65]. Such networks are most commonly used for face verification applications and have shown to be more accurate than hand-crafted features [66], with the goal of minimizing the feature vec- tors distance between matching images, and maximizing the distance for unmatching images. The Siamese Network architecture is made of two identical convolutional neural networks (CNNs), each network being an AlexNet [60] trained using Caffe [61]. During training, pairs of images (aerial or top-down view) are used to feed the two CNNs.

The contrastive loss function [67] is used to pull together matching pairs, and push far away unmatching pairs. The network architecture does not share weights or parame- ters considering each image is domain specific [66]. Each CNN generates a low-dimensional feature representation  or vector for the aerial and top-down view images, and Euclidean distance is used to determine whether the views are close to each other. Changes are detected by setting a margin or a threshold.

2) Data Acquisition: Due to the requirement of a large number of panoramic spheres in order to train a neural net- work, our initial data set [4] was not sufficient. We recall we had to build a labeled data set since we were not aware of any public data set addressing the aforementioned image types. Depending on randomly uploaded panoramic spheres on web repositories or by human effort was not a viable option to build our data set. Accordingly, we used Google Street View as it is the most reliable and vast georeferenced source available for panoramic spheres. However, a pro- cedure had to be devised to be able to acquire the photos efficiently. Having the geocoordinates of the Google Street View panoramas, we were able to amass aerial imagery asso- ciated with the same coordinates from Bing Maps (which was the aerial imagery data source).

Fig. 4. Each collection above shows a panoramic image and its construction into a top-down view image, followed by an aerial image, and the localized part in the aerial image. While the top (rural) and middle (urban) collections are correctly addressed by the proposed approach, the bottom collection illustrates a challenging situation occurring when urban scenes contain many visible objects in the panoramic image.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  8 Proceedings of the IEEE  The data acquisition process thus relies on three succes- sive steps. 1) Using Open Street Maps (OSM), a region of interest is selected and downloaded through OSM export- ing tool. The OSM file is then parsed for geocoordinates of the routes, since Google Street View panoramas are most likely to be located on routes to save processing time look- ing elsewhere. 2) Through Google Maps API and Bing Maps API, requests were sent to receive the IDs of the images from both services. Due to proximity and availabil- ity issues, many IDs were duplicated so map and reduce operations were performed, to eliminate redundancy and ensure having a pair for every coordinate. 3) Finally, each pair of images was downloaded spanning four different cit- ies (Vannes, Rennes, Lorient, Nantes) of Brittany, France.

The dataset contains both rural and urban areas. After that, we go through a necessary visual verification to ensure that each image pertains to the other (i.e., correspond to the same location), thus guaranteeing a fair evaluation of our method. Also, each pair is labeled visually into two separate tasks: presence/absence of changes and land type (urban/ rural). In order for the data set to contain a substantial amount of unmatching pairs, a subset of the data set was purposefully matched incorrectly by selecting pairs from dif- ferent places, and by selecting old panoramic images from Google Street View that are correctly localized with their corresponding aerial image. This collection led to a total of 27 000 images.

3) Results and Discussion: The testing data set was com- posed of 8000 image pairs of aerial images acquired from Bing Maps and corresponding top-down views built from Google Street View panoramic images. The aerial image classification step into urban/rural using CNN achieved an accuracy of 91%. But let us recall that this classifica- tion is only an intermediate step to ease the overall change detection process. The latter was able to identify changes with an overall accuracy of 73%. The preclassification step has proven to be effective since, if bypassed, the accuracy dropped to 60%. One could expect these results to improve with the increase of the size of the dataset used for training or by feeding more subtle changing information to the net- work. Compared to our original paper not relying on deep learning [4] and evaluated on a smaller data set, the change detection accuracy grows from 54% to 73%, leading to an improvement of about 20%. We refer the reader to our pre- vious paper [4] for more details on our former results.

Interesting extensions to this work would be exploring having several orientations of each top-down view image and ranking them by closest distance. One could pick the image ranked the highest to avoid detecting a false change due to dif- ferent orientations. Road signs, lamp posts, and cars are a great cause of false positives for change detection. Detecting these objects and eliminating them from the scene would definitely affect the outcome. Finally, photometric corrections should be applied in the top-down view construction step to minimize the  distortions that occur to the objects far off in the scene. Indeed such distortions cause narrow places (especially in cities) to lead to difficult comparison between top-down view and aerial images, as illustrated in the last line of Fig. 4.

B. Terrestrial Image Orientation in Landscape Areas  Terrestrial pictures are a rich source of information for the study of landscape variations. Specifically, in studies looking at temporal evolution [68], they provide unmatched spatial and temporal resolution. However, unlike standard remote sensing data, sufficiently accurate orientation is often missing, which is required to relate pixels in images with world coordinates. This limited orientation accuracy hinders their use for climate change or territorial develop- ment studies in practice.

In a database of web-shared pictures, the localization of pictures is of different provenance and quality. Moving from the most to the least accurately localized source, images roughly fall into four categories.

?  Photos taken with a GPS compass-enabled camera, having an accurate location and orientation stored in their metadata. The accuracy of the GPS position varies according to the number of satellites available and the perturbation of the signal generated by sur- rounding objects (such as buildings or other metal- lic objects). GPS-located pictures started to be mas- sively available with the spread of smartphones and new generation reflex cameras.

?  The location of the picture is entered manually by the user, through some web-sharing platforms that provide a web interface. The accuracy of the image location depends on the ability of the user to indi- cate the proper location, the resolution of the man- ual mapping interface, etc.

?  The user has added some tags to the picture. Those tags may contain place names that help to roughly locate the picture.

?  No geographic coordinates or any hint that points at a specific location are provided at all. Only the picture content can be used to compute the location.

In what follows, we present a pose and orientation frame- work that can be deployed with pictures issued from photo-sharing web platforms. The original framework was proposed in [5]. The process has two distinct steps: in the first, we find the orientation for a set of precisely geolocated pictures in an automatic way. This is achieved by match- ing skylines extracted both in the picture and in a syn- thetic landscape rendered from a high-resolution DEM. In the second step, we find both location and orientation of a second database of poorly geolocated pictures by finding appearance correspondences between the pictures and the    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  Proceedings of the IEEE 9  high-resolution DEM, this time augmented with the pre- cisely oriented pictures issued from the first step.

Our goal is to meet the requirements of augmented real- ity applications, which do not need photogrammetric accuracy, but rather a visually good alignment of the image with a 3-D model. To this end, we exploit both the geolocalisation infor- mation recorded by the camera (or provided by the user) and the 3-D models stored in a GIS database. Our proposed system helps addressing two problems of current natural landscape augmentation systems: on the one hand, the orientation pro- vided by general public sensors (e.g., smartphones) does not reach the required accuracy; on the other hand, the match- ing of an image with a 3-D synthetic model without geospatial constraints is difficult because state-of-the-art keypoint match- ing algorithms cannot be applied. This is due to the facts that: 1) the texture of real and synthetic images is too dissimilar (especially in the steep slopes areas); and 2) the geographical search space is too large. Hence, the solution we describe below uses this two-step logic to benefit from the prior geolocalisation to geospatially constrain the matching with the 3-D model.

1) Method: In the following, we present the proposed pipeline, which is summarized in Fig. 5. The input pic- tures considered here are representative of a web-shared collection: they have various orientation information (GPS, user- provided locations or place name) and are gen- erally clustered around easily accessible locations from which a point of interest is visible. These pictures are not yet oriented and we call them query pictures. In the con- text of landscape images, a reference 3-D model can be generated from a digital elevation model textured with an orthoimage. Examples of the pictures and of the DEM are shown in Fig. 6.

a) Orient GPS-located pictures: The first step (Fig. 5, top half) aims at orienting pictures that already have a GPS position. In the context of our accuracy requirements (align- ment of the image with the landscape model), we assume that the location measured by the GPS is exact. If the field of view (FOV) is estimated from the focal length recorded in the image metadata, only the image angles (heading, tilt, and roll) must be recovered.

Fig. 5. Flowchart of the proposed pipeline: t is the vector containing the X-, Y-, and Z-coordinates describing the camera position; r is the vector containing the camera angles: heading, tilt, and roll. The top schema describes the orientation of the GPS pictures for which t is known. The bottom schema describes the orientation of the other pictures, for which prior knowledge of an approximate orientation t0 and r0 is assumed. (NNDR stands for nearest neighbor distance ratio).

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  10 Proceedings of the IEEE  We compared two methods to estimate these angles. The first estimates the orientation from the horizon silhouette.

Indeed, a 360? synthetic panorama can be generated for a given location and the 3-D silhouette can then be extracted and matched with the 2-D silhouette detected in the pic- ture to orient the camera. We developed our own method for skyline alignment, but a large range of studies exist in this field [30], [31]. In our method, we use dynamic time warping (DTW [69]) to extract correspondences between the picture-extracted skyline and the reference skyline.

The second method correlates patches of the synthetic panorama image with patches extracted from the query  picture. Both the query picture and the reference panorama patches are described with a HOG descriptor at several scales and matched by a correlation-based distance.

Once the GPS-located query images have been oriented, they are augmented with three bands recording the  X -,  Y -, and  Z -coordinates of the pixels. These pictures can then be used to texture the DEM and thus improve the landscape model and serve as reference images for the next step.

b) Estimate location and orientation of poorly referenced pictures: The second step (Fig. 5, bottom half) matches the remaining query images, which have a less accurate geolo- calisation (e.g., the one indicated by the user), to the textured  Fig. 6. Query pictures. The pictures inside the frame are examples of GPS-localized images. Bottom: snapshot of the reference landscape model (DEM textured with an orthoimage).

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Lefevre et al . : Toward Seamless Multiview Scene Analysis From Satellite to Street Level  Proceedings of the IEEE 11  landscape model obtained at Step 1. We developed an algo- rithm for the pose estimation with pose and landscape model priors (PEP-ALP) [5]. This algorithm is based on a Kalman filter able to fuse the a priori orientation parameters (pose prior) and the 2-D/3-D correspondences provided by the matching of the query pictures with the reference landscape model (landscape model prior). PEP-ALP is based on the following steps: first, we use the collinearity equations [70] and variance propagation to draw in the query pictures a series of confidence ellipses. These ellipses are centered on 3-D keypoints extracted from both the reference images and the landscape model. They correspond to regions in the 2-D query image, where the 3-D keypoint is expected to be located. Then, keypoints from the query pic- ture are matched to those in the landscape model. Using this geometric constraint (i.e., the confidence ellipses) decreases the number of potential correspondences and discards false positives. This is because keypoints not falling within an ellipse are removed. As the Kalman filter is an iterative process, the detected 2-D/3-D correspondences update the orientation esti- mation and the orientation covariance. At each step, the size of the ellipses is reduced together with the matching threshold, which results in an improved estimate of the orientation.

Note that if the query picture only has an approximate location (such as one indicated by the user at small zoom or derived from a geotag), a prior orientation must be com- puted before applying PEP-ALP. To this end, the query pic- ture can be matched with 3-D keypoints extracted from the reference pictures located in the spatial neighborhood. If a small overlap exists with the reference images, a state-of- the-art RANSAC [71] algorithm can extract a set of corre- spondences, providing the initial orientation. This orienta- tion is then used as prior for PEP-ALP.

2) Results and Discussion: In the following, we present two applications of the proposed workflow to picture collec- tions of the Swiss Alps.

a) Orientation of GPS images with horizon and tex- tured model matching: The orientation of the GPS images was tested on a set of images in the Swiss Alps acquired by the authors. In a set of 100 images, both the horizon match- ing and the textured model matching were able to recover 80% of the azimuth within 5?. The failures can be explained by several reasons: on the one hand, the horizon match- ing cannot be applied if the weather is not clear or when the mountain peaks are hidden (typically by a foreground object closer to the camera), but this morphologic feature is relatively unaffected by lightning and seasonal variations.

On the other hand, the matching with the synthetic pano- rama is less dependent on the horizon visibility, but is more impacted by seasonal variations that may change the land cover and harm matching based on keypoints.

b) Orientation of an online collection of web-shared images: We tested the approach with a set of pictures downloaded from Panoramio in a famous Swiss alpine landscape (Zermatt). In total, the set of images in the area of interest is composed of  198 images, among which 10 have a GPS location and 118 have the focal stored in the metadata. Some examples of the pictures considered, as well as the landscape model, are shown in Fig. 6.

The initial reference landscape model consists of a DEM at 25-m resolution textured with an orthoimage at 0.5-m resolu- tion. The orientation quality was assessed in two ways. First, we measured the distances between remarkable points of the ortho- image and query pictures in the geographical space. Second, the oriented pictures were projected on the DEM to render a virtual model. This model allowed us to visually assess the alignment of the pictures with the DEM. An illustrative video can be found on https://youtu.be/87dHVDdlPSs; see Fig. 7.

