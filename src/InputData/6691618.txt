Efficient Large Graph Pattern Mining for Big Data in the Cloud

Abstract?Mining big graph data is an important problem in the graph mining research area. Although cloud computing is effective at solving traditional algorithm problems, mining frequent patterns of a massive graph with cloud computing still faces the three challenges: 1) the graph partition problem, 2) asymmetry of information, and 3) pattern-preservation merging. Therefore, this paper presents a new approach, the cloud-based SpiderMine (c-SpiderMine), which exploits cloud computing to process the mining of large patterns on big graph data. The proposed method addresses the above issues for implementing a big graph data mining algorithm in the cloud. We conduct the experiments with three real data sets, and the experimental results demonstrate that c-SpiderMine can significantly reduce execution time with high scalability in dealing with big data in the cloud.

Keywords-Big data; Cloud computing; Graph pattern min- ing;

I. INTRODUCTION Graph mining has been studied to explore the patterns  from networks or databases. In recent years, many appli- cations have applied this method, such as [1], to discover infrastructure patterns in management database. And also in [2], they provided an advanced graph mining in evaluating community of social networks. In biomedical data, [3] proposed a subgraph isomorphism algorithm, which showed a significant reduction in the running time. However, those algorithms are difficult to support big graph data due to the complexity of the problem and numerous combinations required to be carefully examined. Some experiments have shown in [4] that the number of nodes that can be sup- ported is 1000 in SEuS and MoSS. Although SpiderMine and SUBDUE have demonstrated their scalability in these experiments, the size of the experiment was still small and shed doubt on whether the scalability of big data can be guaranteed. Therefore, this paper constructs a framework to mine the top-K largest patterns in a big graph data to provide an efficient way in dealing with big graph data.

According findings of [4], SpiderMine exhibited the best performance in running time compared to either SUBDUE, SEuS, or MoSS. This paper thus selects SpiderMine as the baseline for comparison.

Recently, three merits of cloud computing have been  manifested: loading balance, scalability, and efficiency. The main concept of cloud computing is to partition the tasks to mappers, data computing, and then merge the tasks to reducers in a parallel manner. However, graph mining with  cloud computing is challenging due to the following reasons.

1) Graph partition problem. The traditional algorithm is difficult to generate small and equal diameter patterns and also entail pattern costs when using inappropriate pattern cut.

These costs consist of pattern recovery, pattern re-searching, and pattern merging. For this reason, how to define and effectively use the appropriate pattern cutting is one of the major tasks of this paper. 2) Asymmetry information. The lack of communication between each machine may cause information loss and increase unnecessary computations. 3) Pattern-preservation merging. When doing the merging step, it is crucial to ensure that the patterns located in different partition can still be discovered and effective summarized with other patterns.

More specifically,this paper presents a cloud implemen-  tation, named c-SpiderMine, for SpiderMine [4], which has been demonstrated as an efficient and effective al- gorithm for mining labled graphs. Note that SpiderMine is an approximation-based graph pattern mining approach, which uses the random walk to discover patterns but cannot guarantee that all the patterns can be found. As such, this paper aims to find patterns by using the complete patterns searching, which walks from all nodes to avoid the pattern loss when using the random walk [5]. c-SpiderMine consists of three phases: partition, mining, and merging. The partition phase partitions one big graph data into several subgraphs by minimum cut algorithm to minimize partition/merge costs.

The second phase, mining phase, exploits SpiderMine to mine the patterns, which reducer can effectively reduce the cost of graph isomorphism tests and generating large patterns with much lower combinational complexity. More importantly, this paper constructs a global table to avoid asymmetry information in this phase. The last phase focuses on merging the patterns. We present a pattern key (PK) function to preserve the patterns, which guarantees that all patterns can be successfully recovered and merged. Our results indicate that compared to SpiderMine, c-SpiderMine has a superior ability in execution time with different mini- mum supports and graph sizes.

The rest of this paper is organized as follows. Section II  surveys the related works, graph pattern mining. Section III formulates the challenges of this paper. Then, we elaborate the details of the c-SpiderMine in Section IV. The perfor- mance of the c-SpiderMine is described in Section V. Finally, Section VI shows some conclusions and future works.



II. RELATED WORKS  Frequent pattern mining is one of the most crucial topics in graph mining. However, mining a single large graph have drawn less attention, due to its high complexity. SUBDUE [6] can be considered as the most famous graph pattern mining for a single large graph based on the principle of minimum description length (MDL). SUBDUE has the ability to discover substructures, compress the database, and represent the concept of structure. However, SUBDUE is computation intensive. Another algorithm, called SEuS, addresses the efficiency issue by summarizing the structure of the data and pruning the un-frequent search space [7].

However, although the pruning method is useful for graphs with a small size, it is still difficult to handle larger- sized graphs and mine larger frequent patterns [4]. In [5], authors proposed an approach to mine frequent patterns in a single graph, but it cannot cope with large graphs.

Another efficient algorithm, SpiderMine [4], was proposed to overcome the bottleneck by merging the small frequent patterns into larger patterns. It adopts a probabilistic mining framework to find the top-K largest pattern by (i) identifying an affordable growth path, (ii) generating large patterns with lower complexity, and (iii) reducing the costs of graph isomorphism testing. SpiderMine successfully discovers the largest patterns and has the lowest running time compared to the other three algorithms of SUBDUE, SEuS, and MoSS.

There have also been a few graph mining approaches  developed on the cloud in the last two years. In [8], the authors proposed a library to find the radius and connected components in tera- and peta-scale graphs, which demon- strated good scalability in the MapReduce model. They also proposed GIM-V to demonstrate scalability and linear running time [9]. A graph mining system was proposed [10], OPAvion, with three modules: the summarization, anomaly detection, and interactive visualization module. The OPAvion leads users incrementally to explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes? vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph. As in [11], authors introduced Mizan, which provided efficient fine-grained vertex migra- tion to balance computation and communication. The results indicated that Mizan provides up to 84 percent improvement over techniques leveraging static graph pre-partitioning. An efficient analysis platform for large graphs was presented, which is called Gbase [12]. Gbase provides parallel indexing mechanism for graph operations that both saves storage space and query responses. Previous works can be generally classified into single large graph [4][5] subgraph mining [10][12], graph mining tools [8] [9], and load balancing enhancement [11]. Nonetheless, the above approaches are not designed for mining the frequent patterns of a massive graph in the cloud.



III. PROBLEM DESCRIPTION  In this section, we first describe the problems of partition in Section III-A. Section III-B discusses the problem of  how some frequent patterns will be missed without in-time information sharing. The last section outlines the pattern- preservation merging problem.

A. Graph partition Even though cloud computing demonstrates good scal-  ability and efficiency, it still faces the challenge of how to partition data while minimizing the patterns cost. It is important to preserve the patterns when doing the partitions, given that if too many patterns are destroyed in this phase, it may cost more time to search for these patterns in the remaining phase. Moreover, the destruction of patterns also causes more costs in merging phase since too many edges will be partitioned. Hence, this paper applies the minimum cut edge maximum flow algorithm to solve this problem.

To specify our problem in terms of partitioning, we denote  that the input data graph as G and the partition dataset as S. We define a general graph partition definition as follow: Definition 1. Given a graph G = (V,E), a cut edge set C(Ec) where Ec divides G into partitions {S1, S2, . . . , Sn} such that  ? i Si = V , and Si  ? Sj = ? for any i ?= j. The edge cut  set Ec is the set of edges whose vertices belong to different partitions.

In Definition 1, we can discover that any union of Si  and Sj is null set. That is, the cut edge between Si and Sj will not be retained in any partition.Therefore, one pattern owning cut edges in the original graph G will be partitioned into different data partitions and will lose its original structure. The concepts of the SpiderMine algorithm is that Pattern 1, P1 can be extended with another pattern P2, where there is one vertex u belonging to the vertex set of P1, V [P1] and P2, V [P2]. That is, P1 can be extended to a larger pattern with P2 if ?u ? V [P1] and u ? V [P2]. However, by Definition 1, Si  ? Sj = ?, and thus in Si and Sj , no pattern  belonging to Pi and Pj can be merged and grow through SpiderMine after each mapper generates frequent patterns in MapReduce Model.

B. Asymmetrical information After partition phase, each mapper starts its own task until  the task is complete. However, each mapper only concen- trates on its own condition, which may cause an asymmetry information. For example, if there is an infrequent pattern in machines 1 and 2, but total number of this pattern is frequent, then according to this example, the system will prune this pattern because it is infrequent. In the classical MapReduce Model, in the partition phase, we partition graph G into several subgraphs, S1,S2,,Sn. In the mining phase, we need to mine the initial less frequent graph patterns, called spiders, which were defined in the Definition 2. We will face the problem of how to count support of a frequent pattern if one pattern has many similar patterns in different data partitions. In this paper, we use the BSP Model to run a parallel pattern growth algorithm in different cores and to maintain a global table to record the global support count.

Definition 2. An r-spider is a frequent pattern whose radius is constrained in r. Each spider is denoted by the head     Single Large Graph  Merging Patterns  Preprocess Partitioning Graph  Mining Pattern  Mining Pattern  Mining Pattern  Extending Pattern  Extending Pattern  Extending Pattern  Pruning Isomorphism  Patterns  Pruning Isomorphism  Patterns  Pruning Isomorphism  Patterns  subgraph  < Cut Edge Sets/ Hashcode, Sup Count>  <Pattern Size, Pattern>  <Pattern Hashcode, Pattern>  ...

...

...

Partition Phase  Mining Phase  Merging Phase  Graph Patterns  Figure 1. The framework of c-SpiderMine.

in the graph. The radius of a spider is the minimum eccentricity among the nodes of it. Therefore, radius(spider) = min{e(v) : v ? V (spider)}.

C. Pattern merging  In the merging phase, we will generate the global frequent patterns from the spiders, which are generated in the mining phase. The easy way to solve the problem is to send the spiders and merge them. However, if we merge all the graphs on one machine, two problems emerge. Firstly, the storage spaces of the reducer cannot read all the frequent subgraphs from all the mappers, because of the data size of the frequent pattern sets would be larger than the original input graph sizes. The other problem is that it is hard to define the suitable merging keys. A general selection of the key replicates the cut nodes. However, choosing these nodes as keys leads to the problem that some larger patterns cannot be merged.



IV. C-SPIDERMINE  In light of problems posed in Section III, this section presents the details of the proposed method, c-SpiderMine to solve those problems. Section IV-A describes how a big graph data is partitioned into several subgraphs with the minimum edge cut algorithm. Then, Section IV-B, addresses the details of how each machine can communicate with each other through global table. The last section outlines that how PK function recovers and merges spiders, which can prevent the problem of missing patterns. Fig. 1 shows the main framework of this paper.

A. Partition phase  According to the content of Section III-A, it is difficult to partition a graph in an appropriate manner. Hence, this paper applies the minimum cut edge algorithm to solve these two problems. The advantage of this cut is that the smaller the number of edges that are cut the fewer the patterns that will be destroyed. In other words, we can preserve more existeing patterns, which can reduce the time for searching the same patterns in the remaining phases. The definition of the minimum cut edge set is presented in Definition 3.

Definition 3. Given a graph G(V,E) where V is the vertex set and E is the edge set, a minimum cut edge set Ec(S, T ) of G(V,E) is a partition of V into S and T = V ? S.

such that s ? S and t ? T and the capacity of Ec(S, T ) = ?u?S,v?TEc(u, v) is the minimum.

There is, furthermore, another issue in this phase. As the  problem outlined in Section III-C indicates, it is hard to merge patterns through the heuristic method, which may cause missing patterns. As such, this paper provides a PK function. In this phase, we will record the cut keys between two subgraphs as a PK.

To divide the graph G(V,E) into k balanced subgraphs  with each of them being able to preserve their structures, we first partition a graph into several subgraphs by minimum cut edge sets Ec. Then we replicate all the node pairs (u, v) on the minimum cut edge set Ec in both of the subgraphs that u and v belong to respectively. The algorithm of this phase is formulated in Algorithm 1.

B. Mining phase  In this section, we introduce our own top-k largest pat- terns mining algorithm, which provides another insight to combine the SpiderMine and MapReduce model. In the first step of this phase, we grow spiders by the pattern- growth algorithm proposed by MoSS [5] to mine all the frequent graph patterns within the radius constraint. The reason that this paper chooses MoSS is that it is the state-of- the-art pattern-growth algorithm that can mine all complete frequent graph patterns in a single large graph. Though  Require:  Ensure:  Algorithm 1: PartitionPhase  Gsub  k-Partition(G, k) for each gi ,gj Gsub do  Ec {(vi,vj)| vi gi(V), vj gj(V) } // add the cut edge set Ec among gi and gj gi(E) gi(E) EC // add the connected edge among the cut node  set Vc of gi gi(E)  gi(E) { (vi,vj) | vi EC |  vj EC i  j } output all subgraph Gsub  1: 2: 3:  4:  5:  6:  graph G=(V,E) k, number of graph partition Gsub ={g1,?,gk}, the partitioned subgraphs of G     Figure 2. An example of global table in BSP model.

MoSS cannot perform well on larger graphs, we can add the constraint of the graph pattern radius r to make MoSS more efficient in mining all frequent patterns. Then, we can get all initial spiders through this way with a single processor. A pattern-growth algorithm will first choose a node as an initial pattern. Then the algorithm generates new candidates by extending the pattern with those edges, which connect to the pattern. This algorithm also collects the pattern embedding. If the count of the embeddings is lower than the support threshold, the algorithm prunes the candidate.To grow spiders in parallel, this paper uses the BSP model to grow spiders in different subgraphs within the same depth, which means that it generates all frequent spider candidates with the same number of edges and nodes in the same superstep.

In the second step of mining phase, the support counts  of each spider candidate are maintained by constructing a global table. The key is the hashcode of each frequent graph pattern candidate and the value is the support count sum of the embedding number in each subgraph of the candidate.

During the growing of the frequent graph pattern candidate sets, we encode candidate patterns by canonical form [13] and emit the local support of each candidate pattern to the global table. Then we can prune infrequent candidates after finishing a superstep and make sure all processors grow the possible embeddings of one candidate. In this way, we can guarantee that there were no pattern be pruned because of the asymmetry information. Fig. 2 shows the example of the global table in BSP model. The whole procedure of the mining phase is shown in Algorithm 2.

C. Merging phase  The merging phase is composed of two MapReduce jobs.

The first job extends spiders in different subgraphs to larger patterns. To solve the merging problem, this paper proposed a PK function, which provides an overlap-based keys. PK function aims to preserve original relations and provide linkage between two subgraphs. The PK function is defined in Definition 4.

Definition 4. Given a subgraph, g(V,E),where V is the node set and E is the edge set,the replicate node set is Vc.

We defined the overlap cut node set of a cut node vc ? Vc is {Ovc} = {u|?(vc, u) ? E}.

The second job, called the pruning pattern job, prunes the  Gid  k // key is subgraph id Gsub  v // value is subgraph data sync and sort all node in Gsub by label frequency for all gi Gsub do  prune infrequent label and re-label the nodes of gi output <Gid, Gsub>  Algorithm 2: MiningPhase Require:  Ensure:  Gsub, partitioned subgraph r, graph radius , minimum support threshold  <Ec(Gid), S?>, cut edge set and frequent graph pattern sets in Gsub  1: 2: 3: 4: 5: 6:  1: 2: 3: 4: 5: 6: 7: 8:  S S1; if S do  for each s S do if supglobal(s) < and Radius(s) r then  S? S-{s} else  S? GrowPattern(s) // generate candidate graph patterns and update supglobal  sync(s, supglobal)  // BSP model synchronization output <Ec(Gid), S?>  Reduce(Key k, Values v[])  9: 10: 11: 12:  13: 14:  Map (Key k, Value v)  Gid  k ; // key is subgraph id Gsub  v ; // value is subgraph data S1 all local frequent 1-edge graphs in Gsub for each s S1 do  supglobal(s) CalcuateSupport(s)  duplicate patterns when two patterns are isomorphic. After the pruning pattern job, we can count the support of each pattern. Finally, we send all the patterns to the merge pattern job. Since we have pruned the infrequent patterns and have done the isomorphism test in the previous job, we can merge the patterns by checking whether the two patterns own the same PK. If two patterns own the same PK, we will merge them via this same spider. We iterate this step until the diameter of the new generated patterns exceeds the bounded diameter. An example of our framework is shown in Fig. 5.



V. EXPERIMENTS  In this section, we present the performance evaluation of c-SpiderMine with real data sets. The setting of the environment will be introduced in Section V-A. To the best of our knowledge, there is no related work on mining the frequent patterns of a big graph data in the cloud, and we thereby compare the proposed framework with SpiderMine in Section V-B. Finally, we will show the executive time and scalability of c-SpiderMine in Section V-C.

A. Experiment environment  This paper implements c-SpiderMine on HAMA 0.5 and Hadoop 1.0.3 in a cloud computing environment consisting of 33 virtual machines. One node serves as the master node     Algorithm 3: MergingPhase Require:  Ensure:  while v do for each pi, pj v do  if pi = pj then prune pj from v  else if pi and pj can be merged F MergePattern(pi , pj)  else F pi ; F pj  if F  then output F  <Ec, Cand_Patterns>, a set of graph cut edge and candidate pattern pairs F, frequent graph patterns  1: 2: 3: 4: 5: 6:  Reduce(Key k, Values v[])  Cand_Patterns v for each vi Cand_Patterns do  for each vj Cand_Patterns-{vi} i  j do Q = SpiderExtend(vi, vj)  if Q  then output <|Q|, (Ec,Q)> // pair of pattern size and merged patterns  1: 2: 3: 4: 5: 6: 7: 8: 9:  10:  Map (Key k, Value v)  Figure 3. An example of c-SpiderMine.

and the others are solely slave nodes. All the experiments are performed on Intel Xeon servers with 256GB main memory and 1GB Ethernet.

B. Comparison with SpiderMine  To demonstrate the effectiveness of c-SpiderMine, this paper selects the baseline algorithm, SpiderMine in order to compare the execution time for a different number of nodes, the execution time with different minimum support, and the memory usage. Two big data sets have been selected  from the website [14]. The first big data set is using com- DBLP, which contains 317,080 nodes and 1,049,866 edges.

The second one selects the Amazone0302, which consists of 262,111 nodes and 1,234,877 edges. Both of these two data sets have been widely used with ground-truth communities.

As shown in Fig. 4(a), a higher execution time occurs when the node size becomes larger. In this figure, we can see that SpiderMine is difficult to support the graphs with the data size larger than 20,000. In contrast, the c- SpiderMine shows a good ability when the data size becomes bigger. c-SpiderMine clocks a longer execution time than SpiderMine when the data size is below 20,000, because it requires an initial setting time, which may create a lead of 180 seconds longer than the SpiderMine. Fig. 4(b) reveals that c-SpiderMine?s better performance in execution time compared to SpiderMine, even if the minimum support is low. Further, we can find that c-SpiderMine outperforms SpiderMine when the minimum support is less than 0.82 percent. The minimum support controls the numbers of fre- quent patterns, when the minimal support becomes smaller the more frequent patterns will be generated, thus causing a greater execution time. If the minimum supports grow, the execution time decreases. In summary, the proposed method, c-SpiderMine, displays a good ability in execution time for handling big graph data and reducing memory usage, and in a more efficient manner than SpiderMine. It should be noted that this paper also compares c-SpiderMine with MoSS.

However, MoSS crashed and cannot be presented in these three experiments when the node size larger than 1,000, which echoes the findings in [4].

C. Scalability Although the proposed method exhibited a better perfor-  mance than the baselines, the capability of the c-SpiderMine to support a big graph data still needs to be verified. This section will provide some experiments in different minimum support settings, number of machines, and average degrees in real data set.

1) Effect of the number of minimum support settings: We report the execution time of com-DBLP in Fig. 7(a) and Amazone0302 in 7(b). Both experiments process different minimum support settings from 0.01 percent to 0.035 per- cent in the three different nodes sizes (N) of 40,000, 70,000, and 100,000. The results show that when the minimum support settings increase, the execution time decreases. This  (a) (b)  Figure 4. The comparisons between c-SpiderMine and SpiderMine: (a) com-DBLP. (b) Amazone0302.

(a) (b)  Figure 5. The execution time with different numbers of nodes and minimum supports: (a) com-DBLP. (b) Amazone0302.

(a) (b)  Figure 6. The execution time with a different number of machines and minimum supports: (a) com-DBLP. (b) Amazone0302.

means that when the minimum support settings become larger, fewer patterns are generated and execution time is reduced. Moreover, when N grows, the execution time also increases, which clearly indicates that more nodes generate more patterns and cost more time. The experiments demonstrate that the c-SpiderMine displays scalability in execution time when the node size and minimum support become larger.

2) Effect of the number of machines: In this section, we conduct the performance of various numbers of machines.

This paper uses 4, 8, 16, and 32 machines to verify c- SpiderMine on com-DBLP with minimum support settings 0.25, 0.35, and 0.4 percent, and 2, 4, 8, 16, and 32 machines on Amazone0302 with minimum support settings 0.2, 0.28, and 0.35 percent. In Fig. 8(a) and 8(b), the execution time exponentially decreases as the number of machines increase. The result indicates that more machines enhance performances efficiency, which further demonstrates that cloud computing is a direct way to achieve high scalability in big graph data mining.



VI. CONCLUSION In this paper, we presented a new approach called c-  SpiderMine, which effectively combines the BSP model, SpiderMine, and cloud computing in handling big graph data. The results show that the c-SpiderMine performs well in memory usage and execution time with different data sizes and minimum supports than the SpiderMine, which demonstrates that c-SpiderMine efficiently mines top-k large patterns in the cloud. We also verified the scalability of c-SpiderMine in different minimum support with differ- ent node sizes, machines, and average degrees. The c- SpiderMine has demonstrated a superior ability in dealing with a big graph data in the cloud. In future work, more real  big data sets can be examined for this approach. Moreover, more other data mining algorithms can be implemented in the cloud to efficiency handle big data.

