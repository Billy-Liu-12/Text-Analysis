Building Classifiers with Association Rules based on Small Key Itemsets

Abstract  We present a simple method for building classifiers based on class-association rules. The method uses a prefix tree structure for mining the frequent itemsets and class- association rules extracted from a training dataset. The rules of a classifier are selected from those built on key item- sets with small sizes, having maximal confidences and max- imal supports, and correctly classifying each object of the training dataset. The comparisons with some existing meth- ods in classification, via the experimental results on large datasets, show that on average the present method is better in terms of accuracy and computational efficiency.

1. Introduction  The classification problem [8, 12] can be explained as follows. Given a set of objects and a set of class labels, find an attribution of class labels to each object. Solutions to this problem consist of methods for building classifiers based on training datasets, such as naive-Bayesian classi- fications [8], decision tree [17], rule based methods [4, 6].

This paper presents an approach to build classifiers based on class-association rules [10, 13, 11], using a prefix tree struc- ture for extracting frequent itemsets and class-association rules. The efficiency of the approach is shown through the experimental results on large datasets [5].

A dataset is a set of objects (or transactions). Each object is represented by an identifier and a list of valued attributes; each valued attribute is called an item. An itemset is a set of items. An itemset of size k, called a k-itemset, is an itemset that has exactly k items. The support of an itemset X , denoted by sup(X), with respect to a dataset D, is the number of the objects in D that have all the items of X . It is well known that if X ? Y then sup(X) ? sup(Y ).

A class association rule (CAR) is an expression of the form X ? c, where X is an itemset and c a class label. In the supervised classification problems, one usually disposes  of training datasets and test datasets; the class label of each object of these datasets is known. A training dataset is used to extract a classifier that maps a dataset into a set of class labels.

Let r be a CARX ? c. An objectO is covered by r ifO has all the items of X (we say O satisfies r). An object O is correctly classified by r if O satisfies r and c is actually the class label of O. The support of r with respect to a dataset D, denoted by sup(r), is the number of the objects inD that are correctly classified by r. The confidence of r, denoted by conf(r), is defined by sup(r)/sup(X).

Example 1 Consider the dataset D, given in Table 1. With respect to this dataset, a? C is a CAR with support 2 and confidence 0.67, as sup(aC) = 2 and sup(a) = 3. Other examples of CAR are ab? C, ab? C ?, b? C, b? C ?.

Table 1. A training dataset  Oid Itemset Class label 1 acd C 2 abe C 3 abd C? 4 bce C?  A partial order, called the precedence order and denoted by ?, is defined over CARs as follows [13]. Given two CARs r and r?, r ? r? (read r precedes r?) if conf(r?) < conf(r), or conf(r) = conf(r?) and sup(r?) < sup(r), or conf(r) = conf(r?) and sup(r) = sup(r?) and the  number of items in LHS(r) (the left-hand side of r) is less than the number of items in LHS(r?).

2. Related work  2.1. Computing Frequent Itemsets  Several algorithms for computing frequent itemsets exist [1, 10, 3, 9]. These algorithms can be divided into two cat- egories: one generates frequent itemset candidates and the     other does not. Apriori [1] and its variants are in the first cat- egory. They compute frequent itemsets by level-wise: can- didate itemsets of size i > 1 are generated on frequent item- sets of size i? 1. The second category consists of methods developed on the FP-growth algorithm [9]. FP-growth uses the prefix tree structure for compactly stocking datasets in the main memory, and recursively mines frequent itemsets following a divide-and-conquer strategy, using this struc- ture.

Even with the large support constraint, for a dense dataset the number of frequent itemsets can be still tremen- dous. The concepts of closed and key itemsets [14, 23] are interesting solutions to representative itemsets. An item- set X is called closed, if there is no itemset Y such that X ? Y and sup(X) = sup(Y ). An itemset X is called a key, if there is no itemset Y such that Y ? X and sup(X) = sup(Y ). The methods for computing frequent itemsets are adapted for computing frequent closed and key itemsets [14, 23, 2, 15, 16, 20].

2.2. Classification by Association Rules  Classical algorithms for building classifiers as [18, 6, 22] compute one rule at a time, using heuristics based on sta- tistical analysis (e.g., information gain). In contrast, the algorithms based on class-association rules, as CBA [13], CMAR [11], search for the set of high confidence class- association rules, by extracting the frequent itemsets from the entire training dataset.

Using an Apriori-like algorithm, CBA extracts class- association rules (CARs) and sorts them in a precedence ordered list. In this order, CARs are selected for building the classifier. A CAR is selected if it correctly classifies at least a training object. Once a CAR is selected, all the objects that are covered by the rule are discarded from the selection process.

CMAR [11] is developed on the ideas of CBA, but it uses FP-growth for computing CARs. The important con- tribution of CMAR is that only positively correlated CARs, after the ?2 test, are selected for the classifier. In contrast to CBA, CMAR allows each training object to be covered by several CARs.

HARMONY [21] uses the same strategy as FP-growth for computing CARs, and sorts items in the correlation co- efficient ascending order, by default. An important differ- ence from CBA and CMAR is that during the CAR min- ing process, HARMONY maintains for each training ob- ject a top-k list of the highest confidence rules mined so far (k ? 1) that correctly classify the object. At the end of the process, those rules are regrouped in lists, by their class la- bels, to form the classifier. In these lists, rules are sorted in descending order of confidence, and the rules with the same confidence, are sorted in descending order of support. For  classifying a test object ti, HARMONY computes the sums of the confidences of the top-k highest confidence rules that cover ti, by class label. The class label with the largest sum is selected for predicting ti.

2.3. Contribution of the proposed approach  The present approach to build classifiers has the follow- ing characteristics:  ? It uses the set-enumeration technique [19] to search for frequent itemsets and stores them in a prefix tree. In contrast to FP-Growth, it does not use the prefix tree to store the original or conditional datasets, but to enumerate the itemsets and to represent the class-association rules.

? It can compute all frequent itemsets with only one dataset reading (with a pass through the prefix tree for pruning infrequent itemsets) if the main memory is large enough. Otherwise, it can be slightly modified to perform a similar level-wise computing as Apriori. In contrast to Apriori, it combines the two phases, generating frequent itemset candidates and computing their supports into one phase. Moreover, it can start with frequent i-itemsets, and pass from k-itemsets to (k+j)-itemsets, for i, j ? 1, k ? i.

? For classification, it uses the same idea as HARMONY: building the classifier with the highest confidence CARs as- sociated with training objects. In contrast to HARMONY, the search for those rules is differed at the end of the process of mining frequent itemsets. Moreover, we do not search for frequent itemsets of all sizes, but we limit with key itemsets of small sizes (? 5). With this limit, we can consider the CARs with small supports.

The experimental results on large categorical datasets show that on average the approach is efficient, in compari- son with some existing important approaches, such as HAR- MONY [21], FOIL [18], CPAR [22], and SVM [7].

3. Prefix tree of itemsets  3.1. Prefix Tree Structure  The prefix tree structure offers a compact representation and efficient access to itemsets, by regrouping their pre- fixes. The last prefix tree in Figure 1 represents the item- sets: a, ac, ad, acd, c, cd, d. In this figure a dark arrow from a node N to a node M means M is a child of N . A light arrow represents the sibling relationship. All siblings of M are children of N . For a node N ,  ? ival(N), sup(N), and lc(N) respectively denote the item value, the support value and the list of pairs (x, kx) associated withN , where x is a class label and kx is the occurrence count of x, at N .

? chd(N) and sib(N) denote respectively the child and the sibling of N that are directly pointed by N .

Let l be a nonempty list of items. Then hd(l) denotes the first item of l, and tl(l) denotes the list of remaining items after removing hd(l)).

3.2. Basic Algorithms  The Build(D, p) algorithm constructs the prefix tree p of itemsets extracted from a dataset D. Initially p is empty. Build successively reads the objects of D, and calls the Update function for updating p. For each object (l : c), where l is its list of items, and c is its class label, Update(p, l, c) updates p with c and all itemsets obtained from the items of l. It calls the AddCls(c, lc) function for adding a class c into a list lc of pairs (x, kx), by increasing the count kc.

Example 2 Figure 1 progressively shows how a tree (ini- tially empty) is built with the object (acd : C), using the re- cursive Update function. In the last prefix tree, each node N is associated with a pair (sup(N), lc(N)); an element of lc(N) is denoted by x : kx.

Function Update(p, l, c) { if l is not empty then {  if p is empty then { created p with item value hd(l), support 1, and (c, 1) as the first value of the list lc(p); Update(chd(p), tl(l), c); Update(sib(p), tl(l), c); }  else if ival(p) < hd(l) then Update(sib(p), l, c); else {  if ival(p) = hd(l) then { sup(p) = sup(p) + 1; AddCls(c, lc(p)); } else {  created a node q with item value hd(l), support 1, and (c, 1) as the first value of the list lc(q); set sib(q) = p; set p = q; }  Update(chd(p), tl(l), c); Update(sib(p), tl(l), c); }  } }  a a  c  d  a  c  d  c  d  a  c  a  c  d  c  dd  a  c  d  c  dd  d (1, C:1)  (1, C:1)  (1, C:1)  (1, C:1) (1, C:1)  (1, C:1) (1, C:1)  a  c  d  d  Figure 1. Building the prefix tree of itemsets of object (acd : C).

3.3. Scalability and Optimizations  For a dense dataset, though the prefix tree structure of- fers a compact representation of itemsets, the main memory can be overloaded. Usual solutions to this problem limit the size of itemsets or consider only frequent itemsets. We in- troduce parameters into Build and Update to control the sizes of itemsets: for each object, instead of computing all subparts of its itemset, only subparts with the limited sizes are considered for building the prefix tree. We use function PruneInfrq to prune infrequent itemsets.

Build can construct the prefix tree by level-wise as Apri-  ori: beginning with the prefix tree of i-itemsets, i ? 1, it successively develops the prefix tree with k + j-itemsets (k ? i, j ? 1), based on the prefix tree of the itemsets of size k built in the previous step. When all training ob- jects are considered, we know the supports of all h-itemsets, k < h ? k + j. This method generalizes Apriori because i and j are not necessary equal to 1. The LevelBuild func- tion is a specific example of the level-wise construction of the present approach, where i begins with 1 and j = 1.

Function LevelBuild(D, p, max) { for (i = 1; i ? max; i++) {  Build(D, p, i); PruneInfrq(p, minsup);  } }  The main difference with Apriori is thatBuild combines the two phases, generating candidates and computing their    supports, into one phase. Moreover, let N be a node corre- sponding to a k-itemset, at which the current step develops children. An item is developed as a child of N only if it ap- pears both in the siblings of N (corresponding to frequent itemsets) and in the considered training object. This op- timization is different from Apriori, because the candidate generation of Apriori considers only the frequent itemsets of the previous step.

4. Mining class-association rules  4.1. Reduction of the Prefix Tree  In class-association rule based classification, for the same confidence and the same support, CARs with a few number of items (called general rules) are usually preferred to ones with larger numbers of items (called specific rules), because the former rules can be applied to a larger number of objects. These CARs can be characterized by Proposition 1.

Proposition 1 Let X1 and X2 be itemsets such that X1 ? X2 and sup(X1) = sup(X2). Then for any class label C, sup(X1 ? C) = sup(X2 ? C) and conf(X1 ? C) = conf(X2 ? C).

The most general rules are those built on key itemsets.

By Proposition 1, we can use only key itemsets for build- ing classifiers. We search for the most general rules in two steps. The first step is applied to the prefix tree: if a node N has the same support as its predecessor, then we mark N to avoid performing a CAR with it. The second step concerns the reduction when building the classifier.

4.2. Building Classifiers  After the prefix tree is built, the dataset is read again. For each training object, the Match function searches among non-marked nodes of the prefix tree for nodes correspond- ing to CARs that correctly classify the object. Those nodes are collected in a temporary list named lnd (initially set to empty), in descending order of confidence and support of the corresponding rules. When a training object is com- pletely passed through the prefix tree, all nodes correspond- ing to rules with maximal confidence and maximal support are performed to add to the classifier (Function AddRule); those nodes are at the beginning of lnd. In the classifier (denoted by lrc) rules are grouped by class label: each class label corresponds to a list of rules, sorted in the precedence order. We remark that not all performed rules are accepted in the classifier. In particular, let r be the rule currently con- sidered for inserting into lrc, and rc be the rule in lrcwhich is currently compared with r. If r and rc have the same class  and LHS(rc) ? LHS(r) and conf(r) ? conf(rc) then r is rejected. This rejection includes the non-key pruning indicated in the previous sub-section. In contrast to HAR- MONY, we postpone the search for the highest confidence rules to the end of the mining CAR process.

As another contrast to HARMONY, we impose the sup- port constraint, using minsup, only on i-itemsets, with i ? 2. This constraint is not applied to itemsets with size > 2, excepting those with support 1. However, notice that only rules with maximal confidences and maximal supports (with respect to each training object) are added to the clas- sifier.

Function BuildClassifier(D, p) { lrc = ?; For each object (l: c) of the training dataset f do  lnd = empty; Match(p, l, c, lnd); For each node N of lnd do {  build a CAR r(N) with class label c; lrc = AddRule(r(N), lrc); }  return lrc; }  The method for classifying a test object is similar to one of the methods of HARMONY: for each class label c, we search lrc for all rules that cover the object and compute the sum of confidences of these rules. The object is classified in the class corresponding to the maximal sum.

5. Experimental Results  The present approach, let us call it SIM (for SIMple method), is implemented in C, and experimented on a lap- top with a Pentium 4, 1.7 GHz mobile processor, and 768 MB memory, running Linux version 9. As we can see SIM has several common and different points with HARMONY, it is interesting to compare it with HARMONY. Another reason is that HARMONY is on average better than many important approaches, as reported in [21]. For comparison, we shall evaluate both approaches on the same 10 large UCI categorical datasets obtained from the author of [5], using the 10-fold cross validation. The runnable program of HARMONY is kindly provided by the authors of [21].

The parameter setting for HARMONY is done as described in [21]: minsup = 50 and items are sorted in the cor- relation coefficient ascending order (the order with which HARMONY gets the best results in general). In particular, for connect which is a very dense dataset, only the items with supports less than 20, 000 are considered. The same consideration is applied for SIM.

Table 2. The 10 UCI datasets.

Database #objects #items #class adult 48482 131 2 chess 28056 66 18  connect 67557 66 3 led7 3200 24 10  letRecog 20000 106 26 mushroom 8124 127 2  nursery 12960 32 5 pageBlocks 5473 55 5 penDigits 10992 90 10 waveform 5000 100 3  Table 3. Accuracy comparisons of HARMONY reported in [21].

Database FOI CPA SVM HARMONY adult 82.5 76.7 84.2 81.9 chess 42.6 32.8 29.8 44.9  connect 65.7 54.3 72.5 68.0 led7 62.3 71.2 73.8 74.6  letRecog 57.5 59.9 67.8 76.8 mushroom 99.5 98.8 99.7 99.9  nursery 91.3 78.5 91.3 92.8 pageBlocks 91.6 76.2 91.2 91.6 penDigits 88.0 83.0 93.2 96.2 waveform 75.6 75.4 83.2 80.5 Average 75.7 70.7 78.7 80.7  The common points of parameter setting for SIM are: (i) The itemset mining starts with 2-itemsets; for all k-  itemsets, k ? 5.

(ii) The infrequent itemset pruning is performed with  minsup = 50, and only for the first step. Further steps are developed on frequent 2-itemsets, but the pruning is no more performed, excepting for itemsets of support 1. The experiments conducted in the present work show that the non-key itemset pruning is sufficient to manage key item- sets with low supports.

For each dataset, we report the result corresponding to the last value of k (k ? 2) before the mean of the test classi- fication accuracy decreases. Table 2 recalls the characteris- tics of the 10 UCI datasets. Table 3 represents the accuracy comparison (reported in [21]) of HARMONY with FOIL [18], CPAR [22], and SVM [7], using the 10-fold cross validation. Tables 4 and 5 represent respectively the ex- perimental results of HARMONY and SIM realized by this work, with the following notations.

? C-sec: the total runtime for building classifiers, from  the beginning of the execution of a program until the classi- fier is completely built.

? t-sec: the total runtime for classifying test datasets.

? Acc.: the mean accuracy of prediction.

? Max: the maximal length of itemsets considered for  building the prefix tree.

The terms ?the total runtime? mean ?the total runtime in  seconds of the ten executions of each 10-fold cross valida- tion?, and not ?the average runtime of each 10-fold cross validation?.

Table 4. Experimental results of HARMONY.

HARMONY Data C-sec t-sec Acc.

adult 793 8 83.4 chess 10 4 44.9  connect 142 39 77.3 led7 2 0 74.4  letRecog 2093 7 70.8 mushroom 9 0 100  nursery 8 0 92.9 pageBlocks 8 0 91.2 penDigits 243 1 96.0 waveform 7047 1 77.9 Total/Avg. 10354 62 80.9  Table 5. Experimental results of SIM.

SIM Data C-sec t-sec Acc. Max adult 169 14 84.2 3 chess 42 28 60.6 5  connect 581 529 77.7 4 led7 3 0 74.4 5  letRecog 826 26 71.3 4 mushroom 65 1 99.9 3  nursery 27 2 98.3 5 pageBlocks 8 0 90.9 4 penDigits 292 7 97.0 4 waveform 53 2 79.8 3 Total/Avg. 2067 610 83.4  6. Discussions and conclusion  The comparisons of HARMONY and SIM show that: ? At the runtime level: for the datasets with short ob-  jects, SIM is in general slower than HARMONY. In par- ticular, for connect (with the previous consideration) the    total runtime of SIM is about five times longer than HAR- MONY. This is because SIM always reads data from disk and the number of rules in its classifier is much larger than that of HARMONY. However, for datasets with long ob- jects, the runtimes of HARMONY is in general much longer than SIM, sometimes with an important difference. For in- stance, the runtime of HARMONY on waveform is more than 130 times longer than SIM. The reason is that HAR- MONY can consider the itemsets of all sizes (for waveform, most of LHS of CARs are in sizes from 5 to 9), while SIM considers the key itemsets with small sizes (for waveform, the maximal size is 3).

? At the classification accuracy level: both methods are comparable, excepting for datasets chess, nursery and waveform. For chess, nursery and waveform, SIM is more accuracy. The good accuracy of SIM can be explained by the selection of CARs having the maximal confidences and maximal supports (with respect to each training object) among those with small supports. These results also vali- date the idea of using CARs built on key itemsets, in par- ticular those of small sizes, and the possibility of selecting CARs with the maximal confidences and maximal supports for the classifiers, among those with low supports. The low supports allow to get high confidence rules, and the rules with small size key itemsets avoid overfitting the training dataset, but improve the runtime. Notice that though we aim to get rules with the highest confidence (with respect to each training object), through the experiments conducted we observe rarely rules with confidence 100%, because of small size itemsets and multiple class labels of the datasets.

