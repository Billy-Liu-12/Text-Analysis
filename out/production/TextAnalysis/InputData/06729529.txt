Mining Probabilistic Frequent Spatio-Temporal Sequential Patterns with Gap Constraints from Uncertain Databases

Abstract?Uncertainty is common in real-world applications, for example, in sensor networks and moving object tracking, resulting in much interest in itemset mining for uncertain transaction databases. In this paper, we focus on pattern mining for uncertain sequences and introduce probabilistic frequent spatial-temporal sequential patterns with gap constraints. Such patterns are important for the discovery of knowledge given uncertain trajectory data. We propose a dynamic programming approach for computing the frequentness probability of these patterns, which has linear time complexity, and we explore its embedding into pattern enumeration algorithms using both breadth-first search and depth-first search strategies. Our extensive empirical study shows the efficiency and effectiveness of our methods for synthetic and real-world datasets.

Keywords-Uncertain databases, Uncertain pattern mining, Sequential patterns, Spatial-temporal data

I. INTRODUCTION  Sequential pattern mining is an important task in data mining and has been extensively studied [1], [2], [3], [4], [5]. Spatio-temporal sequential pattern mining in trajectory databases is one of its major variations that has also drawn a considerable amount of attention [6], [7], [8], [9], [10], [11].

A trajectory of a moving object consists of time-stamped location data across a sequence of ordered timestamps. A spatio-temporal sequential pattern in a trajectory database is a set of objects (also known as an object cluster) that move ?together? for a subsequence of timestamps. This type of pattern has proved attractive in a number of real-world ap- plications. For example, identifying common routes among convoys may lead to more effective traffic control [10], and the discovery of common movements of animals may serve as a basis for knowledge discovery in ecology [11].

The main computational challenge in mining spatio-  temporal sequential patterns is the extraction of objects co-occurring at a minimum number (mint ) of consecutive timestamps. This can be seen as a generalization of the frequent itemset mining problem [12], where the order of transactions is additionally being taken into account. To provide more flexibility, the consecutiveness requirement may be relaxed and a maximum gap constraint imposed instead. For example, vehicles (objects) in a convoy may  mostly be traveling together for consecutive timestamps, but may temporarily become separated at traffic lights (causing a gap in being together), then being together at a later time.

In the frequent itemset mining problem, items are con-  sidered as occurring ?together? if they occur in the same transaction. In comparison, objects are considered as being together at a timestamp if their locations are in close proxim- ity. A common way to determine the spatial proximity of ob- jects is through a clustering algorithm (e.g., DBSCAN [13]).

Alternatively, objects are considered as close to each other at a given timestamp, if they occur at the same point of interest (POI). Spatial proximity of objects is usually application- dependent and commonly considered as a data preprocessing step. In this paper, we consider objects as being together if they are in the same cluster according to some closeness measure. The number of timestamps where the same objects are together is analogous to the pattern?s ?support?. A spatio- temporal sequential pattern is considered as frequent if there are at least mino objects together for at least mint timestamps and the timestamps satisfy a maximum gap constraint g.

Our work tackles the problem of mining spatio-temporal  sequential patterns where the input data is uncertain.

A. Uncertainty in Pattern Mining  Recent research [14], [15] has shown that the modeling of uncertainty can be important in a wide range of real-world applications. Various factors contribute to data uncertainty, including incompleteness of data sources, the addition of artificial noise in privacy-sensitive applications and, most importantly, uncertainty arising from imprecision in mea- surements and observations. In the case of ubiquitous com- puting applications, object trajectory data is often acquired by position-aware devices such as GPS and WiFi systems.

However, such devices have limitations in their measurement accuracy and so the location data they record is sometimes represented by a probability-density function (pdf) [16] to represent this uncertainty.

Two types of frequentness (or support) measurements  for an uncertain pattern have been proposed: expected sup- port [14], [17], [18] and probabilistic frequentness [19], [20],   DOI 10.1109/ICDM.2013.150     [21]. In the former, the support of an itemset is estimated by the expected value of its support. However, as pointed out in [20], [21], one major drawback of such a measure is that it does not indicate the confidence level of the estimation, which may lead to the loss of important patterns. In [19], [20], [21], a probabilistic threshold ? based on possible world semantics is introduced. A pattern is considered as frequent if its probabilistic measurement exceeds ? . We will use possible world semantics to assess the frequentness of an uncertain spatio-temporal sequential pattern.

As an example scenario, Figure 1 shows moving objects in  an uncertain trajectory database. In this example, the location of an object is defined by a one-dimensional pdf due to its positional uncertainty, which is represented as a line in the figure. The circles at each timestamp can be seen as a POI or a cluster boundary depending on the application. To compute the spatial proximity of objects with uncertain locations, one could apply first an uncertain clustering algorithm (e.g.

UK-means [16]) to obtain precise (hard) clusters and then a classical pattern mining algorithm. However, similar to the use of expected support, such a method would not provide any explicit confidence measure in the result. Alternatively, fuzzy clustering [22] would assign each object a ?degree of belongingness? (belongingness probability) for each cluster, i.e., one object can belong to more than one cluster. The be- longingness probability is not necessarily based on positional uncertainty. Various fuzzy clustering algorithms [22], [23] handling both certain and uncertain data have been proposed to produce soft clusters.

Our proposed methods do not rely on a particular pdf or  technique for deciding the spatial closeness of objects, which are instead modeled as preprocessing steps. We only require a set of (fuzzy) clusters as input for each timestamp, where each object is associated with a belongingness probability that specifies the confidence the object is in a cluster at a given timestamp. An example of such an input is shown in the right hand side of Figure 1. To measure the frequentness of an uncertain pattern, we need to calculate the confidence that the set of objects O is in the same cluster for at least mint timestamps that fulfil the gap constraint g, where |O| ?mino.

If the confidence exceeds a user-given probabilistic threshold ? , then this pattern is considered as probabilistic frequent.

Given an uncertain database, our problem is to find all probabilistic frequent spatio-temporal sequential patterns.

B. Related Work  The problem of classical sequential pattern mining has been an area of extensive research in the context of de- terministic databases [1], [2], [3], [4], [5]. Several varia- tions have been proposed. Sequential patterns in trajectory databases [6], [7], [8], [9], [10], [11] and sequential patterns in event databases [24], [25] are two popular approaches.

In the context of uncertain databases, the problem of  uncertain frequent itemset mining in probabilistic databases  o1 o2  o3  o4 o5  o6  o1  o2  o3 o4  o5 o6  o1 o2  o3 o5  o4  o6  t1 t2 ? timet6  o6 ?o4 5  t Clusters 1 {o1:0.6,o2:1.0,o3:0.8};  {o4:0.7,o5:1.0,o6:0.8}; {o1:0.4}; {o3:0.2}; {o4:0.3}; {o6:0.2}  2 {o1:0.8,o2:1.0}; {o3:1.0,o4:0.5}; {o5:0.7,o6:0.5} {o1:0.2}; {o4:0.5}; {o5:0.3};{o6:0.5};  3 {o1:0.4,o2:1.0}; {o3:0.8,o4:0.5,o5:0.6} {o1:0.6}; {o3:0.2}; {o4:0.5}; {o5:0.4}  4 {o1:0.6,o2:1.0}; {o1:0.4} 5 {o1:0.8,o2:1.0}; {o3:0.9,o4:0.6,o5:1.0}  {o1:0.2}; {o3:0.1}; {o4:0.4} 6 {o1:0.4,o2:1.0};  {o3:1.0,o4:0.9,o5:0.9,o6:0.8} {o1:0.6}; {o4:0.1}; {o5:0.1}; {o6:0.2}  Figure 1. An example of an uncertain database. The location of an object is represented as an one-dimensional pdf due to its positional uncertainty.

Each object is associated with a belongingness probability specifying the confidence that it is in the cluster.

was earlier studied under the expected support measure in [17], [14], [18], [26]. However, later [19], [20], [21] found that the use of expected support may lead to the loss of important patterns. Thus, the use of a probabilistic frequentness measure has been more popular recently. A recent survey for comparing these two measures and analyz- ing their relationships is given in [27]. For the problem of uncertain sequential patterns, to the best of our knowledge, the approaches in [28], [29] are the only studies in the literature. In [28], the authors measure the frequentness of a pattern in uncertain event databases based on its expected support, but this may sometimes lead to the loss of interest- ing patterns [20], [21]. In comparison, our methods proposed in this paper are based on probabilistic frequentness support.

In a more recent work, Zhao et al. [29] addressed uncertain sequence mining under two different models of uncertainty, sequence level uncertainty and element level uncertainty.

Their study is complementary, yet distinct from our work in this paper, since they focus on the case where unlimited gaps are permitted when matching a query subsequence against a longer sequence. In contrast, in our study gaps are allowed in a pattern according to a maximum gap constraint g.

C. Challenges and Contributions  Mining spatio-temporal patterns from uncertain trajectory data is a challenging problem. In particular: ? As the length of a trajectory (sequence) increases, the number of possible worlds grows exponentially. This makes naive methods for checking pattern characteris- tics (such as support) infeasible.

? Enabling maximum gap constraints increases flexibility for spatio-temporal patterns. However, checking pattern characteristics for uncertain data in the presence of gaps is considerably more complex than without gaps.

? Mining collections of spatio-temporal patterns for un- certain trajectory data requires a pattern enumeration framework (e.g., breadth first or depth first). The merits of different frameworks are currently an open question.

In this paper, we address these challenges. In particular, we make the following contributions: ? We define a probabilistic model for uncertain spatio- temporal sequential patterns under the possible world semantics and incorporate a maximum gap constraint.

? We introduce a linear time approach to calculate the frequentness probability of an uncertain spatio-temporal sequential pattern. This is the first such linear time result we are aware of for uncertain sequences. Previous linear time results [20] have been developed in the context of itemsets rather than sequences.

? We explore integration of our probabilistic frequentness checking algorithm within apriori-based approaches us- ing both breadth-first and depth-first search. We analyze the relative merits of each of these approaches.



II. PROBLEM DEFINITION  Let TS = {t1, t2, ..., tn} be a linearly ordered list of n timestamps (called the time space). Let OS = {o1,o2, ...,om} be a collection of m objects that appear in TS (called the object space). Objects O ? OS are observed at timestamps T ? TS in various locations. We refer to T as a timestamp sequence and its length is |T |. A trajectory database D contains location data of moving objects OS across TS. We first provide definitions of our patterns for certain databases.

Definition 1: A set of moving objects O (called the ob-  jectset) that are in the same cluster for a timestamp sequence T is denoted as S = (O : T ) and called a spatio-temporal sequential pattern, where O? OS and T ? TS.

Definition 2: Given a sequence of timestamps T repre-  sented by {tk1 , tk2 , . . . , tkp} where tki ? T , the maximum gap in T is calculated by max(kq+1?kq?1), ?q ? [1, p), and is denoted as ?T .

Example 1: For T = {t1, t2, t4, t7, t8} the maximum gap is  ?T = 2.

Definition 3: A (maximum) gap constraint is a user-  specified integer g, g? 0. We say S= (O : T ) fulfils the gap constraint g iff the occurrence of O in T satisfies ?T ? g.

Definition 4: Given user specified values for mino, mint  and g, S = (O : T ) is called a frequent spatio-temporal sequential pattern iff |O| ? mino, |T | ? mint and ?T ? g.

Example 2: Given mino = 2, mint = 3, g = 1, we say  that S = (o1,o2 : t1, t2, t3, t4, t6) is a frequent spatio-temporal sequential pattern.

A. Uncertain Trajectory Database Model  We now describe our model for representing uncertainty for object trajectory data. To generalize our methods, we treat the computation of the spatial proximity of objects with uncertain locations as a preprocessing step. This enables the use of different pdfs for the representation of uncertainty and different clustering algorithms depending on the application.

Specifically, we assume that at each timestamp t, the objects have been partitioned into a set Ct of (fuzzy) clusters.

t Clusters Ct  1 c11={o1:0.6, o2:1.0} c12={o1:0.4}  2 c21={o1:0.3, o2:0.8} c22={o1:0.7} c23={o2:0.2}  3 c31={o1:1.0, o2:1.0}      i World P(Wi) i World P(Wi)  1 t1:c11={o1,o2} t2:c21={o1,o2} t3:c31={o1,o2}  0.144 5 t1:c11={o2}, c12={o1} t2:c21={o1,o2} t3:c31={o1,o2}  0.096  2 t1:c11={o1,o2} t2:c21={o1}, c23={o2} t3:c31={o1,o2}  0.036 6 t1:c11={o2}, c12={o1} t2:c21={o1}, c23={o2} t3:c31={o1,o2}  0.024  3 t1:c11={o1,o2} t2:c21={o2}, c22={o1} t3:c31={o1,o2}  0.336 7 t1:c11={o2}, c12={o1} t2:c21={o2}, c22={o1} t3:c31={o1,o2}  0.224  4 t1:c11={o1,o2} t2:c22={o1}, c23={o2} t3:c31={o1,o2}  0.084 8 t1:c11={o2}, c12={o1} t2:c22={o1}, c23={o2} t3:c31={o1,o2}  0.056  Figure 2. An example of a spatio-temporal database of uncertain clusters and its possible worlds.

An object o ? OS with an uncertain location observed at timestamp t ? TS has a belongingness probability of P(o ? c) being in cluster c ? Ct , where P(o ? c) ? [0,1] and ?c?Ct P(o ? c) = P(o@t) = 1, where P(o@t) is the probability that o is observed at t. Conversely, if o is not observed at t, then o is not in any cluster and P(o@t) = 0.

We assume that different objects and different timestamps are mutually independent, i.e., the belongingness probability of an object has no effect on those of other objects. Similar probabilistic independence assumptions have been made in previous related work [14], [20]. Therefore, for objectset O, we have P(O ? c) = ?o?OP(o ? c). The probability that objects O are in the same cluster at t is P(O@t) = ?c?Ct P(O ? c). We also say that O@t means O occurs at timestamp t in the same cluster.

Example 3: Given t1:c11= {o1:0.3, o2:0.5, o3:1.0}, c12=  {o1:0.6, o2:0.5, o5:1.0}), c13 = {o1:0.1} and O = {o1, o2}, then P(o1 ? c11) = 0.3, P(O ? c11) = 0.15 and P(O@t1) = P(O ? c11)+P(O ? c12) = 0.45.

We define a spatio-temporal database of uncertain  clusters D = {Ct1 ,Ct2 , ...,Ctn} as a collection of objects with uncertain locations within the clusters at timestamps {t1, t2, ..., tn}. Figure 2 shows an example of D with three timestamps and two objects.

An uncertain database D can be instantiated into a possi-  ble world w that contains a collection of certain clusters at each timestamp. Suppose for an object o at each timestamp, thatM is the number of clusters for which P(o? c)> 0. Then there are M possible memberships for o at each timestamp.

The number of possible worlds of D increases exponentially with both |OS| and |TS| (i.e. M|OS|?|TS|). The right hand side of Figure 2 shows all possible worlds derived from D. The probability of whether a possible world w exists is denoted as P(w) and ?P(w) = 1. Assuming independence, the probability that w exists is computed as:  P(w) =? t?TS ? o?t P(o ? ct(o,w)) (1)  where ct(o,w) is the cluster that o is in at timestamp t in possible world w. For example, P(o1 ? ct1(o1,W6)) = P(o1 ? c12) = 0.4.

Example 4: In Figure 2, P(w = W6) = P(o1 ? ct1(o1,w))?P(o2 ? ct1(o2,w))?P(o1 ? ct2(o1,w))?P(o2 ? ct2(o2,w))?P(o1 ? ct3(o1,w))?P(o2 ? ct3(o2,w)) = 0.024.

Note that the assumption of independence between objects  is not always true, since in some applications, the locations of objects can be dependent. For example, two classmates might go to the lecture theatre at the same time and we can infer one location from the other?s. Furthermore, the locations of an object at two consecutive timestamps are normally constrained by a speed limit, i.e., the current location of an object depends on its location at a previous timestamp. More sophisticated models using conditional probabilities (e.g. a Markov model) might be used in these scenarios. If such knowledge is available, it can be integrated into our techniques, by modifying Equation 1.

B. Probabilistic Frequent Spatio-Temporal Sequential Patterns  Recall that given thresholds mino, mint and g, we say S = (O : T ) is frequent if |O| ? mino, |T | ? mint and ?T ? g. In the uncertain scenario, co-occurrence of the objects at the timestamps T is no longer certain. Instead co- occurrence is described by a discrete probability-distribution function (d-pdf). For example, in Figure 2, the d-pdf of T for objectset O = {o1,o2} is: P(T = {t1, t2, t3}) = P(W1) = 0.144, P(T = {t1, t3}) = P(W2) + P(W3) + P(W4) = 0.456, P(T = {t2, t3}) = P(W5) = 0.096, P(T = {t3}) = P(W6) + P(W7)+P(W8) = 0.304.

Let T be the timestamps that objects of O are in the  same cluster, the probability that |T | ? mint and ?T ? g is called the frequentness probability and it is denoted as Pg?mint (O). The frequentness probability is interpreted as the probability that objects of O are in the same cluster for at least mint timestamps that satisfy gap constraint g. An uncertain spatio-temporal sequential pattern is denoted as S?=O (co-occurrence of the objects at T is uncertain). Using a confidence threshold ? , we formally define a probabilistic frequent spatio-temporal sequential pattern as follows: Definition 5: S? = O is called a probabilistic frequent  spatio-temporal sequential pattern iff |O| ? mino and Pg?mint (O)? ? .

Definition 6 (Problem definition): Find for a spatio-  temporal database D of uncertain clusters the complete set of probabilistic frequent spatio-temporal sequential patterns.

The main computational challenge of our mining task is  the calculation of frequentness probabilities for candidate patterns. This is detailed in the next section.



III. CALCULATING FREQUENTNESS PROBABILITIES  A simple way to compute the frequentness probability is to enumerate all possible worlds of D and sum up the probabilities of possible worlds with |T | ?mint and ?T ? g:  Pg?mint (O) = ? w?W :(|T |?mint ,?T?g)  P(w) (2)  Table I SUMMARY OF THE USE OF NOTATIONS.

?T The maximum gap produced by T .

?T, j The tail gap produced by T at the first j timestamps.

Tj First j timestamps of TS, and Tj = {t1, t2, ..., t j}.

P(O@t) Probability that objects in O are in the same cluster at t, i.e. ?O  occurs at t?.

P(O@t) Probability that objects in O are NOT in the same cluster at t.

Pg?mint Probability that O occurs at least mint timestamps of TS with gap constraint g.

Px?i, j Probability that O occurs at least i timestamps of Tj with gap constraint x.

P?y,x?i, j Probability that O occurs at least i timestamps of Tj with tail gap constraint y and gap constraint x.

Example 5: In Figure 2, if we set mint = 2 and g= 1, for pattern S = (o1,o2 : P1?2(O)), we have P  ?2(O) = P(W1) +  P(W2)+P(W3)+P(W4)+P(W5) = 0.696.

As mentioned before, as the size of D increases, the  number of possible worlds increases exponentially, up to |W |=M|TS|?|OS| whereM is the number of clusters for which P(o ? c) > 0. With the assumption that timestamps in TS are mutually independent [14], [20], we can simplify the calculation as follows:  Pg?mint (O)= ? T?TS:(|T |?mint ,?T?g)  (? t?T P(O@t)? ?  t?TS?T P(O@t))  (3) where P(O@t) = 1 ? P(O@t). The computation of Pg?mint (O) via Equation 3 needs to enumerate ?  |TS| i=mint  (TS i  ) combinations of T where ?T ? g, which is still inefficient and the computation cost still increases exponentially with respect to |TS| (the total length of the trajectory).

Example 6: In Figure 2, to calculate P1?2(O) for pat-  tern S = (o1,o2 : P1?2(O)), the combinations of T fulfilling |T | ? 2,?T ? 1 are {t1, t2}, {t2, t3}, {t1, t3} and {t1, t2, t3}.

P(O@t1) = 0.6, P(O@t2) = 0.24, P(O@t3) = 1.0. Thus, P1?2(O) = P(O@t1) ? P(O@t2) ? P(O@t3) + P(O@t1) ? P(O@t2)? P(O@t3) + P(O@t1)? P(O@t2)? P(O@t3) + P(O@t1)?P(O@t2)?P(O@t3) = 0.696.

A. A Dynamic Programming Approach  To avoid the exhaustive enumeration of possible worlds (or different combinations of mutually independent times- tamps), we propose a dynamic programming approach to efficiently calculate the frequentness probability. We first introduce some notations, which are summarized in Table I.

Let Tj = {t1, t2, ..., t j} be the first j timestamps of TS,  where Tj ? TS. The probability that O occurs at least i     P  6,3?  P 2,2  5,2  ?  ?  P  5,3? P  4,3? P  3,3?  P 2,2  4,2  ?  ?  P 2,1  4,2  ?  ? P 2,1  3,2  ?  ?  P 2,0  3,2  ?  ? P 2,0  2,2  ?  ?  P 2,2  4,1  ?  ? P 2,2  3,1  ?  ?  P 2,1  3,1  ?  ?  P 2,0  2,1  ?  ? P 2,0  1,1  ?  ?  P 2,1  2,1  ?  ?  P 3,0 P 2,0 P 1,0 P 0,0  ? ?????? ?? ? ?????????? ?? ???  ?  ?  ? ???  ? ???  ???????????? 	?????? ??  ?? ??????? ??  ??????????????  ??????????  ?????????  Figure 3. Details of how to calculate P2?3,6(O) using our dynamic programming approach (4 layers in total).

timestamps of Tj with gap constraint x is:  Px?i, j(O) = ? T?Tj :(|T |?i,?T?x)  (? t?T P(O@t)? ?  t?TS?T P(O@t))  (4) Thus, the frequentness probability is given by Pg?mint ,|TS|(O).

1) Tail gap: The treatment of gap constraints is a central  feature of our approach. To develop a dynamic programming method that can handle the gap constraint, we introduce the notion of a tail gap.

Definition 7: Given a sequence of timestamps T repre-  sented by {k1,k2, ...,kp}, where tki ? T , and given Tj = {t1, t2, ..., t j} where T ? Tj ? TS. The tail gap produced by T at the first j timestamps, denoted as ?T, j, is defined as ?T, j = j? kp.

Example 7: For T = {t1, t2} and T6 = {t1, ..., t6}, ?T,6 =  6?2= 4.

Definition 8: Given a positive integer y and Tj =  {t1, t2, ..., t j} where Tj ? TS, we say T ? Tj fulfills the tail gap constraint iff T satisfies ?T, j ? y.

Definition 9: P?y,x?i, j (O) is defined as the probability that  O occurs at least i timestamps of Tj with tail gap constraint ?y and gap constraint x.

Lemma 1 shows one important property of P?y,x?i, j (O) which will be used in our dynamic programming approach.

Lemma 1: 1 P?y,x?i, j (O) = P  ?( j?i),x ?i, j (O), ?y> j? i.

Example 8: P?2,2?3,4(O) = P ?1,2 ?3,4(O).

2) A dynamic programming scheme: Let T be the timestamps that O occurs at the first j timestamps Tj = {t1, t2, ..., t j}, then Pg?i, j(O) is equal to the probability that T fulfills |T | ? i and ?T ? g. Our dynamic programming approach is to split the problem of computing Pg?i, j(O) at the first j timestamps into subproblems of computing the frequentness probabilities at the first j?1 timestamps. Let T ? be the timestamps that O occurs at the first j?1 timestamps 1The proofs of the lemmas in this paper can be found on  http://www.csse.unimelb.edu.au/?jbailey/stsp.pdf.

Tj?1. The information we require is what conditions must be met by T ? to ensure |T | ? i and ?T ? g.

The conditions T ? needs to meet to make |T | ? i as  follows. If O@t j, then O must occur at least i?1 timestamps of Tj?1 (|T ?| ? i? 1). Otherwise, O must occur at least i timestamps of Tj?1 (|T ?| ? i). This technique has also been used in previous work on probabilistic top-k queries [30].

However, we take the order into account and the maximum gap is allowed in a sequence. Thus, techniques for handing the gap constraint are required, which leads to Lemma 2.

Lemma 2 shows the conditions T ? needs to meet to  make ?T ? g as follows. If O@t j, T ? needs to satisfy both gap constraint ?T ? ? g and tail gap constraint ?T ?, j?1 ? g.

Otherwise, T ? needs to satisfy gap constraint ?T ? ? g.

Lemma 2: Let T and T ? be the timestamps that O occurs  at Tj and Tj?1 respectively. If O@t j, ?T ? g? (?T ? ? g and ?T ?, j?1 ? g). Otherwise, ?T ? g??T ? ? g.

With the above discussions, we can split the problem of  computing Pg?i, j(O) as follows. If O@t j, then P g ?i, j(O) is  equal to the probability that T ? fulfills thresholds |T ?| ? i?1, ?T ? ? g and ?T ?, j?1 ? g. Otherwise, Pg?i, j(O) is equal to the probability that T ? fulfills thresholds |T ?| ? i and ?T ? ? g. Lemma 3 shows how to use the dynamic programming scheme to compute Pg?mint ,|TS|(O) .

Lemma 3: Entry: x= g, i= mint , j = |TS|  Px?i, j(O) = P ?x,x ?i?1, j?1(O)?P(O@t j)+Px?i, j?1(O)?P(O@t j)  (5) where if 1? i< mint : P?y,x?i, j (O)=P  ?x,x ?i?1, j?1(O)?P(O@t j)+P?y?1,x?i, j?1 (O)?P(O@t j)  (6) or if i= 0 : P?y,x?i, j (O) = P0, j(O)+P  ?y,x ?1, j(O) (7)  recursion termination conditions:  P?y,x?0,0(O) = P0,0(O) = 1; (8)  P?y,x?i, j (O) = P x ?i, j(O) = 0,?i> j or ?x< 0,y< 0; (9)  Lemma 3 is explained as follows. Equation 5 is the entry of our dynamic programming approach with x= g, i=mint and j = |TS|. Then P?x,x?i?1, j?1(O) of Equation 5 is calculated by Equation 6 if 1? i<mint or by Equation 7 if i= 0. P0, j(O) of Equation 7 is the probability that objects in O are not in the same cluster at the first j timestamps. It is calculated by P0, j(O) = ?1?k? j P(O@tk) = P0, j?1(O)? P(O@t j). These equations are calculated recursively. The recursion termina- tion conditions are shown in Equation 8 and Equation 9.

Figure 3 shows an example of how to use our dynamic  programming approach to calculate P2?3,6(O). Theoretically, the equations in Lemma 3 can be used as a top-down approach. However, this approach leads to repeated cal- culations of internal results. For example, in Figure 3, P?1,2?2,3(O) is used by both P  ?3,4(O) and P  ?2,2 ?2,4(O). Instead,  as a dynamic programming method, we use a bottom-up     approach. As shown in Figure 3, we start from the bottom layer, and store internal results for further calculations until we reach the leftmost node of the top layer. In each layer, j ? [i, |TS| ?mint + i], thus the width of each layer is |TS| ?mint + 1. For mint > 1, there are mint + 1 layers in total. For mint = 1, we still need one internal layer, i.e., three layers in total.

Figure 4 (a) provides a zoom in of the internal layer i= 1.

In each column j, only those P?y,x?i, j (O),y ? [?ymin,?ymax] need to be calculated and stored. The rightmost node of each row gets the ?ymax for its column, where ?ymax = min( j? i,g) (Lemma 2). For example, P?2,2?1,3(O) gets ?ymax = 2 for column j = 3. Whilst the leftmost node of each row gets ?ymin for its column, which is calculated as follows. j of the upper-left node is |TS|?mint+ i where ymin get the maximum value of ?ymin = g. The difference in j between the upper- left node and other leftmost nodes is (|TS| ?mint + i)? j.

Then ymin of each leftmost node is equal to g minus that difference. Also, ?y, j ? 0. Thus, ?ymin = max(g? (|TS| ? mint + i? j),0). For example, P?1,2?1,3(O) gets ?ymin = 2? (4?3) = 1 for column j = 3.

As shown in Figure 4 (a), each internal layer can be  seen as a parallelogram with width = |TS| ?mint + 1? g and height = g+ 1. Thus, the number of nodes in each layer is given by (|TS| ?mint + 1? g)? (g+ 1) = ?g2 + (|TS|?mint)?g+ |TS|?mint +1. It is a quadratic function that reaches its peak for g = (|TS| ?mint)/2. Figure 4 (b) shows the number of nodes per internal layer for calculating Pg?5,20(O), g ? [0,15]. #node is minimal when g = 0 or g= |TS|?min, and maximal when g= (|TS|?mint)/2.

Intuitively, g = 0 means all the timestamps of T are  consecutive without a gap. While g= |TS|?min means there is no gap constraint with T . In this case, timestamps are treated as a set and can be seen as transactions in frequent itemset mining.

We now state a key theorem for the computational com-  plexity of our method to compute probabilistic frequentness.

Theorem 1: 2 Both the time complexity and space com-  plexity for calculating Pg?mint ,|TS|(O) are O(|TS|).

Proof: To calculate Pg?mint ,|TS|(O), we need one bottom  layer and one top layer, and mint?1 internal layers (or one internal layer for mint = 1). Thus the total number of nodes needed is 2 ? (|TS|?mint+1)+(mint?1) ? (|TS|?mint+1? g) ?(g+1) for mint > 1, or 2 ?(|TS|?mint+1)+(|TS|?mint+ 1?g) ?(g+1) for mint = 1. Thus, it requires O(mint ?g ? |TS|? min2t ?g?mint ?g2) = O(|TS|) time and O(|TS|) space.

Our dynamic programming approach can check proba-  bilistic support in linear time with respect to the length of the trajectory TS. This is a substantial improvement over the naive approach of enumerating all possible worlds.

Example 10 in Appendix shows how to use our approach  to calculate P2?3,6(O) from Figure 1, where O= {o1,o2}.

2Assuming parameters mint and g are constants, as was done in [20].

????? 	?????? ??    ???????????? 	?????? ??? 	??   ?? ???  ??  ????? ????	? ???????? 	?????? ?? 	????????  ????	 ??????? 	????? ?  ? ? ? ?  P 2,2  4,1  ?  ? P 2,2  3,1  ?  ?  P 2,1  3,1  ?  ?  P 2,0  2,1  ?  ? P 2,0  1,1  ?  ?  P 2,1  2,1  ?  ?  ? ???  ?? 0 5 10 15      g  #N od  es  ?  ?  ?  ?  ?  ? ? ? ? ?  ?  ?  ?  ?  ?  ?  peak  (a) Zoom in of i= 1 (b) #Nodes with respect to g  Figure 4. (a) Zoom in of the internal layer of i= 1 in Figure 3. The shape can be considered as a parallelogram. (b) Number of nodes in each internal layer of Pg?5,20(O) with respect to maximum gap constraint g.

B. Probabilistic Monotonicity Discussion  In this subsection, we explore and discuss probabilistic monotonicity criteria related to our formulation. We also show that how these monotonicity criteria can be used as a probabilistic pruning technique to further reduce the computation cost of our dynamic programming approach.

Lemma 4: P?y+1,x?i, j (O)? P?y,x?i, j (O).

Lemma 4 indicates that increasing the tail gap allowed in a sequence increases the chance a pattern fulfills the tail gap constraint. In contrast, Lemma 5 shows that increasing the minimum number of timestamps required in a sequence decreases the chance a pattern fulfills mint threshold.

Lemma 5: P?y,x?i, j (O)? P?y,x?i+1, j(O).

Compared to the support monotonicity criterion of uncertain frequent itemset mining [20], Lemma 5 extends the criterion by applying gap constraints to two sides of the equation.

Based on Lemma 4 and Lemma 5, we have: Lemma 6: P?x,x?i, j (O)? P?x,x?i+1, j+1(O).

Lemma 6 suggests that the frequentness probability of the upper-left node of the ith internal layer is no less than that of the upper-left node of the (i+1)th internal layer.

Lemma 7: P?x,x?i, j (O)? Px?i+1, j+1(O), if x= j? i.

Lemma 7 indicates that, under the setting of x = j? i, the frequentness probability of the upper-left node of the (mint? 1)th internal layer is no less than that of the leftmost node of the top layer. Recall that only patterns with Pg?mint ,|TS|(O)? ? are of interest in our task. Together with Lemma 6, we now have the following probabilistic pruning rule.

Pruning Rule 1: During the bottom-up computation of  Pg?mint ,|TS|(O), where g = |TS| ? mint , if the frequentness probability of the upper-left node of an internal layer P?g,g?mint?k,|TS|?k(O), where 1 ? k < mint , is less than ? , then this pattern can be pruned.



IV. MINING PROBABILISTIC FREQUENT SPATIO-TEMPORAL SEQUENTIAL PATTERNS  We introduced a linear time solution to calculate the fre- quentness probability. We now need to efficiently enumerate different combinations of objectsets with |O| ? mino in OS to find all patterns which are probabilistic frequent in an     o1 o2 o3 o4 o5 o6  o3o4o5  ?  o1o2 o1o3 o1o4 o1o5 o2o3  o2o4 o2o5 o3o4 o3o5 o4o5  ??????? ?? ??  ?????? ?? ??  ?  ? ?  ??!??" ????????  ? ???#???????  (a) Breadth-first implementation  o4 o5o3 o4 o5  o1 o2 o3 o4 o5  o2 o3 o4 o5  o5  o5  o3 o4 o5  ??!??" ????????  ? ???#???????  ? ? ? ?  ?  o6  (b) Depth-first implementation  Figure 5. Two implementations of Apriori-based algorithms running on the database in Figure 1.

uncertain database. We discuss both breadth-first and depth- first implementations of Apriori-based algorithms.

Breadth-first implementation: It has been shown that the  Apriori algorithm [12], which uses a breadth-first strate- gy, is promising for identifying frequent itemsets in both certain and uncertain transaction databases [20]. The anti- monotonicity of support, also called the Apriori property, can reduce the search space and thus speed up the mining process. In order to apply an Apriori-based algorithm, we first need to show that the anti-monotonicity property still holds for the frequentness probability with gap constraint g.

Lemma 8: Pg?mint (O)? P  g ?mint (O  ?), ?O? O?.

Lemma 8 provides a pruning technique for bottom-up pat- tern searching. That is, if Pg?mint (O) does not fulfill the probabilistic threshold, then Pg?mint (O  ?) does not fulfill the probabilistic threshold if O ? O?. We call this the Apriori pruning rule. Similar to the classical Apriori algorithm, our algorithm using breadth-first implementation also involves two main steps: a join step and a prune step. The join step is responsible for generating new candidates. In the prune step we calculate the frequentness probability for each candidate and extract probabilistic frequent patterns where |O| ? mint . We start with an objectset containing a single object, then iteratively generate new candidates by a join operation. In each iteration, we scan the database to calculate the frequentness probability for each candidate. Next, we output those patterns satisfying |O| ?mint and probabilistic support. Finally, we use the Apriori pruning rule to eliminate those candidates with Pg?mint (O)< ? .

Example 9: Patterns discovered from the database in  Figure 1 with the setting of mino = 2, mint = 3, g = 2, ? = 0.1: {o1,o2 : 0.82},{o3,o4 : 0.43},{o3,o5 : 0.39},{o4,o5 : 0.24},{o3,o4,o5 : 0.10}.The process is shown in Figure 5 (a).

Depth-first implementation: One bottleneck of the Apriori  algorithm is that the join step becomes time-consuming for a large number of candidates. In our depth-first implementa-  Algorithm 1 Depth-first implementation 1: Calculate the frequentness probability p for each 1- objectset and insert into OFeq if p? ?  2: m? maximum object in OFeq 3: Call Extend({o}, m, OFeq) for each o ? OFeq 4: Function Extend(O, m, OFeq) 5: n? maximum object in O 6: for i ? OFeq and n< i? n do 7: O? O?{i} 8: p? CalculateFrequentnessProbability(O, D, mint , g) 9: if p? ? then {//Apriori property} 10: OUTPUT (O : p), if |O| ? mino 11: Extend(O, m, OFeq)  tion, we simplify the candidate generation by simply adding one object to the k-objectset to form the (k+ 1)-objectset, which does not require a join operation. Such a search order also carries over from a smaller objectset to a larger objectset. It means the depth-first implementation also can take the advantage of the Apriori property in the generation of candidate (k+1)-objectset from k-objectset.

Algorithm 1 shows the depth-first implementation. We  first select frequent 1-objectsets (line 1), and then recursively generate candidate (k+ 1)-objectset from k-objectset (line 5-12). At each iteration, only the frequent k-objectsets are extended (Apriori property, line 10). We use a bucket tree structure to store candidate patterns and Figure 5 (b) shows an example of using the depth-first implementation to find patterns from the database in Figure 1. The tree grows from left to right. Each bucket store a pattern represented by the path from root. For example, the bottom-left bucket represents the objectset {o1,o2,o3}. The tree stops growing once it encounters the infrequent pattern.

Compared with the former implementation, the depth-first  strategy does not fully use the downward closure of the probabilistic support. This is due to the fact that the depth- first implementation does not know all frequent k-objectsets before considering the (k+ 1)-objectset. This may lead to a bigger search space. For example, Figure 5 (b) needs to calculate the frequentness probability of 20 candidates compared to 17 candidates of Figure 5 (a). The frequentness probabilities of three candidates {o1,o2,o3}, {o1,o2,o4} and {o1,o2,o5} are not calculated by the former implementation since they contain infrequent subset {o2,o3}, {o2,o4} and {o2,o5} respectively.



V. EXPERIMENTS  We first consider large synthetic datasets to test the effi- ciency of our dynamic programming approach for computing the frequentness probability. Then, we use both synthetic datasets and real-world vehicle tracking datasets to evaluate the effectiveness of our two Apriori-based algorithms. All     0 20 40 60 80      #Timestamps  E la  ps ed  ti m  e (S  ec .)  ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  ? DP Basic  0 20 40 60 80      #Timestamps * 10000  E la  ps ed  ti m  e (S  ec .)  ?? ? ? ?  ? ? ? ? ? ?  ? ? ?  ?  ?  ?  ?  ?  ?  ? ? DP  0 4000 8000     Maximum Gap Constraint  E la  ps ed  ti m  e (S  ec .)  ?  ?  ? ? ? ?  ? ? ? ?  ?  ?  ? ? ? ?  ? ?  ?  ? ?  ? DP  (a) DP method vs. Basic method  (b) Scalability of DP method  (c) Elapsed time with respect to g  Figure 6. Synthetic Dataset: (a) Dynamic programming approach vs. Basic method. (b) Scalability of DP method. (c) Effect of maximum gap g for DP method.

0 4000 8000     #Timestamps  E la  ps ed  ti m  e (S  ec .)  ? ? ? ? ? ? ? ? ?  ? ? ?  ? ? ? ?  ? ?  ?  ? DP DP+Pruning  0 4000 8000     #Timestamps  E la  ps ed  ti m  e (S  ec .)  ? ? ? ? ? ? ?  ? ? ?  ? ? ? ?  ?  ?  ? ?  ?  ? DP DP+Pruning  0 4000 8000     #Timestamps  E la  ps ed  ti m  e (S  ec .)  ? ? ? ? ? ? ? ?  ? ? ? ? ?  ?  ?  ?  ? ?  ?  ? DP DP+Pruning  (a) mint = |TS|?25% (b) mint = |TS|?50% (c) mint = |TS|?75% Figure 7. Synthetic Dataset: Effect of probabilistic pruning rule on elapsed time with respect to mint .

experiments were performed on an Intel Core i5 2.3GHz machine with 8GB main memory.

A. Evaluation on Calculating the Frequentness Probability  We first use synthetic datasets with varying parameter set- tings to test the performance of our dynamic programming approach. The size of the datasets varies from 10 timestamps to 106 timestamps. The probability that an objectset O occurs at t is randomly chosen as P(O@t) ? (0,1). All results are the average of ten runs. The probability threshold ? is 0.9.

Both mint and gap constraint g were set to 10 unless stated otherwise. Abbreviations used in our figures: (1) Basic: basic approach of probability calculation (c.f. Equation 3); (2) DP: our dynamic approach; (3) DP+Pruning: our dynamic approach with pruning (c.f. Section III-B); (4) BFS: the breadth-first implementation of our algorithm; (5) DFS: the depth-first implementation of our algorithm.

Figure 6 (a) compares the performance of our dynam-  ic programming approach against the basic method. The elapsed time of the basic method increases exponentially when |TS| increases. The elapsed time of the basic method is out of the scale from the figure if |TS| is less than 50.

In fact, the total number of combinations of timestamps in the basic method has already reached  (50  ) > 1010. Thus  the basic method is not practical. The scalability of our proposed dynamic approach is illustrated in Figure 6 (b).

The performance of proposed approach is very promising: as TS increases, the running time grows in a linear trend as in line with our time complexity theorem (c.f. Theorem 1).

2 3 4 5 6 |Objectset|  #P at  te rn  s          0 1 0  Bus Truck  0.2 0.4 0.6 0.8 1.0    Probabilistic threshold  E la  ps ed  ti m  e (S  ec .)  ?  ? ?  ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ?  ? Bus?BFS Truck?BFS Bus?DFS Truck?DFS  0.2 0.4 0.6 0.8 1.0      Probabilistic threshold  #P at  te rn  s  ? ? ? ?  ? ? ?  ? ? ? ? ?  ? ? ?  ? ? ?  ? Bus Truck  (a) #Patterns with respect to |O|  (b) Elapsed time with respect to ?  (c) #Patterns with respect to ?  Figure 8. The number of patterns discovered from two real-world datasets and the effect of ? .

0 200 600 1000     #Objects  E la  ps ed  ti m  e (S  ec .)  ?  ?  ? ?  ?  ? ?  ? ?  ?  ? BFS DFS  0 200 600 1000     #Objects  E la  ps ed  ti m  e (S  ec .)  ? ? ?  ?  ? ?  ? ?  ?  ?  ? BFS DFS  0 200 600 1000      #Objects  #P at  te rn  s  ?  ? ?  ?  ?  ?  ?  ?  ?  ? ? Uniform distribution  Normal distribution  (a) Elapsed time with respect to #objects using uniform distribution  (b) Elapsed time with respect to #objects using normal distribution  (c) #Patterns with respect to #objects  Figure 9. Experiments on the datasets generated by the Brinkhoff simulator.

Effect of the Gap Constraint g: To isolate the effect of the maximum gap constraint g on the running time, we turn off the minimum timestamp threshold by setting mint = 1.

This minimizes the effect of mint on the running time. The running time for calculating Pg?1,104(O) is shown in Figure 6 (c), where g varies from 0 to 9999. The running time reaches a peak at g= 5000 (the mid point) which is consistent with our analysis from Figure 4 (b).

Effect of Probabilistic Punning Rule: In Section III-B,  we showed that the probabilistic pruning rule can be applied if g= |TS|?mint . Using this setting for g, we test the running time with three different values of mint as shown in Figure 7.

Setting a strict mint = |TS|?75% threshold, the probabilistic pruning rule shows a significant speed up. In contrast, if we apply a relatively loose mint , then the two lines in Figure 7 (a) overlap, since patterns in such a setting more likely fulfill the probabilistic threshold and cannot be pruned.

B. Evaluation on Mining Probabilistic Frequent Spatio- temporal Sequential Patterns  Two real-world vehicle traffic datasets3 and synthetic datasets generated by the Brinkhoff generator4 were used to evaluate our two Apriori-based algorithms for mining probabilistic spatio-temporal frequent sequential patterns.

1) Traffic Datasets: The settings for our two real-world  dataset are as follows: (1) a bus dataset recording 2 school  3http://www.rtreeportal.org 4http://www.fh-oow.de/institute/iapg/personen/brinkhoff/     buses collecting and delivering students around Athens for 108 days and consisting of 145 trajectories; (2) a truck dataset recording 50 trucks delivering concrete to construc- tion sites around Athens over 33 days and consisting of 276 trajectories. To increase the size of moving objects, we considered each distinct trajectory as the ID of an object, yielding 145 buses with 1713 timestamps and 276 trucks with 2449 timestamps. This is a common pre-processing method [10]. The timestamp update frequency was set to every 30 seconds. Any second of a timestamp falling into the range [0?, 30?) was normalized to 15? and to 45? otherwise.

For example, the timestamp 23:22:22 gets normalized to 23:22:15 and 23:22:58 to 23:22:45. The soft clusters at each timestamp are obtained by the fuzzy c-means clustering algorithm [22] with m= 2 and EPS= 0.01, where m is the weighting exponent and EPS is the termination criterion.

Each object is assigned a belongingness probability by the fuzzy clustering algorithm. The number of clusters at each timestamp is drawn from [2, 5].

We first ran our algorithms on the datasets with mino = 2,  mint = g = 10 and ? = 0.5. The number of found patterns is illustrated in Figure 8 (a). The largest object cluster we discovered from the bus dataset has 6 objects, compared to 4 for the truck dataset. In addition, 2-objectsets and 3- objectsets are a dominant proportion of the patterns found in the truck dataset, whilst the size of objectsets in the bus dataset is more uniform. We further test the effect of the probabilistic threshold ? on the running time and number of patterns found, see Figure 8 (b) and (c) respectively. As ? in- creases, the Apriori rule has more effect and thus the running time of both implementations decreases. The effect is more obvious for the truck dataset. With a smaller search space, the breadth-first implementation outperforms the depth-first implementation for both datasets. In Figure 8 (c), the truck dataset returns more patterns than the bus dataset does for lower ? . However, as ? increases, the number of patterns found in both datasets decreases significantly especially for the truck dataset. There are only 25 patterns found in the truck dataset with ? = 0.95.

2) Brinkhoff Generator: Synthetic Datasets: We use the  map of Oldenburg as input map data. To control the exact size of the objectset we test, we vary the number of objects from 100 to 1000 and set the number of newly generated objects at each timestamp to zero. The maximum number of timestamps is set to 10000. To make moving objects last longer (thus the data has more timestamps), we set the speed divided by 250 which is the default value for slow. Other parameters were set as default. In order to test the effect of the probabilistic distribution of belongingness probabilities on our algorithms, we assign the belongingness probabilities to objects manually, rather than by applying a fuzzy clustering algorithm. We first apply DBSCAN [13] with MinPoints= 2 and ? = 0.05 to obtain (certain) clusters at each timestamp. MinPoints denotes the minimum number  of objects in a cluster with a radius of ? . Then, we assign the belongingness probability to each object. The belongingness probabilities of these datasets were assigned according to two different distributions: (1) Each object was assigned a probability according to a uniform distribution in the range of (0.5, 1.0]. (2) Each object was assigned a probability according to a normal distribution with a mean of 0.5 and a standard deviation of 0.2 in the range of [0, 1.0] (if the prob- ability is outside the range, we assign the boundary value of 0 or 1). The parameters of our algorithms are set as mino= 2, mint = g= 10 and ? = 0.5. The results are shown in Figure 9.

Again, the algorithm using the breadth-first implementation generally outperforms the depth-first implementation. The elapsed time for each of the two different probabilistic distributions is not significantly different. However, there are many more patterns found in the datasets that use a uniform distribution than the datasets with a normal distribution as shown in Figure 9 (c).



VI. CONCLUSIONS  In this paper, we have formulated and studied the problem of mining probabilistic frequent spatio-temporal sequential patterns in uncertain databases. We proposed a dynamic programming approach for computing the frequentness prob- ability with linear time complexity. This is a somewhat surprising result. Linear time support checking has been shown to be possible for uncertain itemsets. However, se- quences have a more complex structure than itemsets and gap constraints add even further complexity.

We further introduced and evaluated two Apriori-based al-  gorithms using breadth-first and depth-first implementations for efficient enumeration of all probabilistic frequent spatio- temporal sequential patterns from uncertain databases.

In our further study, we aim to extend our current ap-  proach to be able to handle the trajectory data where the identity of objects is uncertain.

