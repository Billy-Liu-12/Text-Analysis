Rule Discovery with PVMA Algorithm

Abstract Data mining is becoming increasingly important since the sue of database grows even larger and the need to explore hidden rules from the database becomes widely recognized. This paper presents PVMA algorithm, a parallel Apriori algorithm that uses Parallel Virtual Machine (PVM) to discovery rules in database. This algorithm exploits the data parallelism, by distributing the data being mined across all available processors.

Keywords: data mining, candidate, itemsets, parallel

I. Introduction Data mining or knowledge discovery in database ( D D )  has in the recent years attracted a lot of interest in the database community. The interest is motivated by the large amount of computerized data that many organizations now have about their business. For instance, supermarkets store electronic copies of millions of receipts, and banks and credit card companies maintain extensive transaction histones. The goal in database mining is to analymthese frequent data sets and to discover patterns or regularities that are useful for decision support.

- - - D i s ~ ~ ~ , ~ r 3 . - - c f - - ~ ~ ~ ~ ~ . ~ t ~ o ~ . ~ . r ~ l e s  is one of the most important data mining problem, for example, given a database of sales transactions, it would be intended to discover all associations among items such that the presence of some items in a transaction will imply the presence of other items in the same transaction. The problem of mining Association rules in the context of database was first explored in (Agrawal et a1.,1993). The mathematical model of association rules is: let I=[il, i2 ...

i,) be a set of literals, call items, let D be a set of transaction where each Ransaction T is a set of items such that Td, the quantities of items bought in a hensaction are not considered, this mean that each item is a binary variable representing if an item was bought, each transaction is associated with an identifier, called TID .Let X be a set of items. A transaction is said to contain X if and only if XGT. An association rule is an implication of the form X=>Y, where X c  I, Y d  and X n Y= 4~ .The rule X=>Y holds in the transaction set D with confidence c if c% of transaction in D that contains X also contains Y.

The rule X=>Y has support s in the transaction set D ifs% of transactions in D contain XU Y. Confidence c denotes the .strength of implication and support indicates the frequencies of the occumng patterns in the rule. It is often desirable to pay attention to only those rules which may have reasonably large support, are referred to as strong rules (Agrawal et al., 1993) The task of mining association  0-7803-7840-7/031$17.00 0 2 0 0 3  EEE  rules is essentially to discover strong association rules in large database. In general, the problem of mining association rules is decomposed into the following two steps:  I )  Discovery the frequent item sets. i.e., the sets of item sets that have transaction support above a pre-determined minimum supports.

2) Use the frequent item sets to generahle the  It is noted that the overall performance of mining association rules is determined by the first step. After the frequent item sets are identified, the corresponding association rules can be derived in a straightforward manner. Efficient count of frequent item sets is that the focus of most prior work.

association rules for the database.



II. Apriori Algorithm Fig 1 gives the Apriori algorithm. The first pass of the  algorithm simply counts item occurrences to determine the large 1-items. A subsequent pass, say pass k, consists of two phases. First, the large itemsets L., found in the (k-1)th pass are used to generate the candidate itemsets Ck, using the Apriorisen function. Next, the database is scanned and the support of candidate in Ck is counted.

I*.Apriori Algorithm */ /*CL: set of candidate K-itemsets. Each member of this set has two fields:  /*Lk: set of frequent k-itemsets. Each member of this set has two fields:  i) itemset and ii) support count *I  i) itemset and ii) support count */ LI=( frequent, 1-itemsets); For (k=2;L k-l#O;k++) do begin  Cl;apriori_gen(Lk- l);/*new candidate*/ For all transaction t eD do begin Increment the count of all candidates in Ck that are  end Lk=( ceCklc.count>minsup) end Answer=ukLk  insert into Ck select piteml, p.item2, ..., p.itemk-I, q.itemk-1 from Lk-lp, Lk-lq where p.iteml=q.iteml, ..., p.itemk-l=q.itemk-l I  pitemk-l<q.itemk-l; /*prune step*/  contained in t  I*Apriori_gen function*/  -844-    for all itemsets ccCk do for all (k-1) subsets s of c do if(seLk-I) then  delete c kom Ck Figure 1.Apriori Algorithm  F o r e x a m p h l e t 4 b e  {{1,2,31, [1,2,41, [1,3,41, [l,3,51, [2,3,4]]. After the join step, C, will he ~[1,2,3,4),(1,3,4,5]1. The prune step will delete the itemset [ 1,3,4,5) because the itemset [ 1,4,5] is not in 4.

We will then be left with only (1,2,3,4] in C,.



III. Apriori Algorithm On PVM(PVMA) This algorithm is the parallel based on Apriori algorithm  The algorithm assumes a shared-nothing architecture, where each N processor has a private memory and a private disk. The processors are connected by a communication network and can communicate only by message. The communication primitive used by the algorithm are part of the PVM (Parallel Vntual Machine) communication library. The algorithm through dishibute database achieve the parallelism. If there are N processors, each processor attain UNth data about the whole database, then complete the Apriori algorithm in each database subsets. The algorithm works as follows: k=l: 1. Each processor P generates its local candidate itemset C,?, using its local dataset D?.

2. Processor F exchanges its C,? with all other processors.

3. Each processors union the local C< to determine global C,.

b l :  1. Each processor Pi generates the complete Ck, using the complete frequent itemset Lc, created at the end of pass k -1. Observe that since each processor has the identical b.,, they will be generating identical Ck.

2. Processor P makes a pass over its data partition D? and develops local support counts for candidates in Ck.

3. Processor Pi exchanges local Ckcounts with all other processors to develop global Ckcounts.

Processors are forced to synchronize in this step.

4. Each processor Pi now computes Lk from Ck.

5. Each processor P? independently makes the decision to terminate or continue to the next pass.

The decision will be identical as the processors all have identical 4.

From the above algorithm, we can conclude: Steps 1-2 and 45 are similar to that of the serial algorithm. The non-obvious step is how processors exchange local counts to arrive at global Ckcounts. Since each processor has the exact same ck, each processor puts its counts values in a common order into a count array. All that is needed now is the perform a parallel vector sum of the arrays. This only requires communication count values and can be done in O(log(n)) communication steps.

Number of prhcessor

IV. Results  Sequential Recessing Panllel fiocessing S p e d time time  0.561s  0.191s 2.973    [6] E.H. Han, G. Karypis, and V. Kumar, ?Scalable Parallel Data Mining for Association Rules;? Proc. ACM Conf Management of Dnta, ACM Press, New York, 1997, pp. 277-288.

