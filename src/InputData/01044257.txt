

2002 FIRST INTERNATIONAL IEEE SYMPOSIUM "INTELLIGENT SYSTEMS". SEPTEMBER 2002  Finding the Base of Global Implications for the Association Rule Problem  Luminita Dumitriu, Teodor Dumitriu, and Cornelia Tudorie  AbsbackUsing concept lattices as a theoretical background lor finding association rules 111 has led to designing algorithms like Charm 121, Close 131 or Closet 141. While they are considered as extremely appropriate when finding concepts lor association rules, they do not cover a certain area 01 significant results, namely the pseudo-intents that form the base lor global implications. We propose an approach that, besides linding all proper partial implications, also finds the pseudo-intents. The way our algorithm is devised, i t  allows certain important operations on eonccpt lattices, like adding or extracting items, meaning we can reuse previously lound results.

Inder Terns-Data models, data processing.

1. INTRODUCTION  LATELY, corporations have made serious investments into using information technology to increase their management quality. Also, large amounts of important business data are now stored in database systems and their quantum is expected to grow. An important goal of data mining is to extract valuable implicit patterns from large quantities of data.

Association rules, introduced by [ 5 ] ,  provide useful means to discover associations in data. Let I = { i , ,  i 2 ,  ... , i,) he a set of  m literals called ifems. Let the database D = { r,, b, . . ., t,#} be a set of n transactions, each transaction consisting of a set of i temslofIand associated with a unique transaction identifier called / id .  I is called a k-ifernsel if k is the size of I.

A transaction t e D  is said to contain an itemset I if IC t .  The support of an itemsetlis the percentage of transactions in D containing I: support (0 = I{t ED 1 I ct)l / I{/eD)l.  An association rule is a conditional implication among itemsets, I * 1', where itemsets I, I ' d  and I nP =O. The confidence of an association rule I': I* P i s  the conditional probability that a transaction contains r, given it  contains L conidence($ = supporr ( I  U 1') / support (0. The support of r is defined as support(r) =supporr (lu P ) .

L.Dumitriu i s  with the Computer Science and Engineering Department, University "Dunarea de Jas", Galati> 6200 Romania (telephone: ++4093 161 3 14, e-mail: LuminiIa.Dumitriu@ugaI.ra).

T. Dumitriu, is with (he Elechical Engineering Department, University ''Dmiaroa de SOS", Galati, 6200 Romania (e-mad: Teodor.Dumi hi a@ugal.ro).

C. Tudorie IS  with the Computer Science and Engineering Department, University "Dunarea de SOS", Galati, 6200 Romania (c- mail: Comelia.Tudone@ugal.ro).

The problem of mining association rules in a database is defined as finding all the association rules that hold with more that a user-given support threshold, minsiip, and a user-given confidence threshold, minconJ According to [SI this problem is solved in two steps: 1. Finding all frequenl itemsets in the database, meaning  itemsets with support higher than or equal to rninsup.

2. For each frequent itemset I, generating all association  rules I' I \ I', where P c I, with confidence greater than or equal tominconl:  The second problem can he solved in a straightforward manner aAer the first step is completed. Hence, the problem of mining association rules is reduced to the problem of finding all frequent itemsets. This is not a trivial problefn, since the numbcr of possible frequent itemset is equal 1 0  the size of the power set of I ,  d4. There are many algorithms proposed in the literature, most of them based on the Apriori mining method [6], that relies on a basic property of frequent itemsets: all subsets of a.frequent ifemsel arefiequent. This property can also be said as all supersets of an infTequenr itenisel are infrequent. This approach works well on weakly correlated data such as market basket data. Over correlated data, such as census data, there are other approach, as Close [3], CHARM [2] and Closet [4], which are more appropriate.

These approaches search for closed itemsets, structured in lattices that are closely related with the concept lattice in formal concept analysis [7]. The main advantage of a closed itemset approach is the smaller size of the resulting concept lattice versus the numher of frequent itemsets, ie. a search space reduction.

In this paper, we propose extending our approach [SI with finding the global implication base. It is also a closed itemset approach, but is more user-oriented; it takes into account the fact that an association rule mining process leads to a large amount of results, that is, most of the time, difficult to understand by the user. To prevent this to happen, we add a user-selection to the process: the interesting frequent items.

This way we build a partial, easier to understand, model of the data. We still have to provide means to model all the data. 'To achieve this goal, our approach allows the user to pick a previously found partial data model (expressed as a concept lattice and a data context) and some new frequent items to he added to it, thus obtaining an extended or even a full data model. The results of an extension operation consist of the supplementary results, the &model. The reverse operation to the extension of a model is reducing a model with some frequent items; we have considered it  to be used whenever a large data model is incomprehensible to the user.

The main advantages of our approach are:  0-7803-7134-81021510.00 0 2002 IEEE 21s    - a smaller size of results leads to a higher comprehensibility of a data model or a &model;  - the data models can be reused, when extending or reducing them with sets of items, thus spearing the time spent building them.

We are enlarging the data model with the base for generating rules with confidence = 1, the pseudo-intenis, and  .? . '., show how it works with the extension and reduction .. '.. operations.

. I  The rest of the paper is organised as Follows. Section 2 reviews related work and compare it with the contribution o f the paper. In section 3 we define our notion of data model and we present the main operations with data models. In section 4 we give some experimental results. The paper is concluded in section 5.

11. RELATED WORK AND CONTRIBUTIONS  In this section we first present the Apriori-type approach.

Then, we describe the use of closed itemset lattice as theoretical framework for the closed itemset-type approach.

In the end, we briefly describe o w  method.

TABLE1 CO- OFEXAMPLE DATABASE:  I  Transaction Item List 1 A B C D 2 A C E 3. A B C D 4 A B C E 5 A B C D E  - 6  A D E A .  Apriori-type approach  Let's consider the example database in Table I .  The frequent itemsets, listed in Table 2, form a lattice with partial order induced by the inclusion operator as in Figure 1.

TABLE I1 FREpmIsnnsm FORMINWP=M%  Generating all possibly frequent itemsets and testing their support is clearly an impracticable solution when m, the cardinal of I, is large. The Apriori methodology is described as follows.

First, the items in I are sorted in lexicographic order. The frequent items are computed in one pass over the database.

They are stored in the set L, of frequent I-itemsets. For each iteration i, a candidate set, C,, is computed joining L,, with itself. In a database scan the support for each candidate in Ci is computed. The frequent itemsets found in C, are stored in  \-. C. AC a?/..

a?%; B. D. E. AB, AI), AE, EC, AB13  BD. CE. C- M?4  Fig. 1. The frequent itemset lattice, where the partial order relationship is the set inclusion.

L. the set OF frequent i-itemsets, and C, is discarded. The process continues until all candidates are infrequent.

The join operation in iteration i selects pairs of i-1 -itemsets having i-2 items in common, reuniting their elements into a candidate of size i .  Afterwards, the candidates that do not have all their subsets of i -1  items in L,., are pruned.

The number of effective iterations is equal to the height of the frequent itemset lattice.

B. The closed itemset approach In this section we define a context, the Galois connection  of a context, a concept of the context, a lattice ofconcepts.

For a more details on lattice theory see [7].

A context is a triple (T,  I, D) where T and I are sets and D dW. The elements of T are called objects and the elements of I are called attributes. For any f E T and i E I, we note tDi when t is related to i, i.e. ( t ,  i )  E D.

Let (T, I,  0) he a context. Then the mappings s: @(T)+ @(I),s(X)={ie I I ( V t e X ) t D i } t: @(I)+ p(T),s(Y)={ Is T l ( V i E Y ) t D i }  define a Galois connecrion between @(T) and @(I), the power sets of T and I, respectively.

A concept in the context (T ,  I, D) is a pair ( X, U), where X c  T, Y c  I, s(X)=Y and t(Y)=X. Xi s  called the exrenf and Y the intent of the concept (X,Y). This leads to s "YY) = s (X) =Y and t O s(X) = t(Y)=X being closure operators.

A concept (X,  Y') is asubconcept of (X,Y), denoted (X',Y) 5 (X,Y),iffX'rX(iffYaY).

Let C he the set of concepts derived from D using the Galois connection. The pair L=(C, 5) is a complete lattice called a lattice of concepts.

The closed itemset approach uses from the concepts only their intent. Some of the frequent itemsets found in the previous section are not closed itemsets. For example, the itemset ABD, where t(ABD) = { 1, 3, 5 ) .  hut s({l, 3, S))=ABCD. The frequent closed itemsets for the example database in Figure 1 are C = {A, AC, AD, AE, ABC, ACE, ABCD). The lattice L is represented in Figure 2.

As we can observe, there are 7 frequent closed itemsets and 19 frequent itemsets. Ignoring the itemsets that are not closed is a significant pruning criterion For the search space.

A closed itemset in the context of mining association rules is    the maximal itemset of a collection of itemsets that share the same transaction set.

We will present the CHARM method for finding closed  5 e& F D I /i  i r i  Fig. 2. The lattice of frequent concepts for the example database  itemsets. First, the frequent items are found, along with their transaction sets. In the order defined on I ,  the first frequent item i, is picked. The algorithm attempts extending this item with in. The support of the new itemset is computed while intersecting the transaction sets of the two items. There are 4 cases to take into account: - if the new itemset is infrequent, il is extended with i3,  continuing in a depth-first manner; i f  the transaction set of the new itemset is equal to the one of il ,  then i l  is replaced with the new itemset, and we continue extending the new itemset with the next frequent item; if the transaction set of the new itemset is equal to the one of it, then i2 is discarded and the new itemset is added as a child of i f ,  and we continue extending the new itemset with the next frequent item; otherwise, the new itemset is added as a child of if, and we continue extending it with the next frequent item;  When there are no more items to extend the current itemset with, the algorithm picks the next itemset in a depth first manner.

The algorithm ends, finding the following closed itemsets:  (ABC; 1,3,4,5), (ACE; 2,4,5), (ABCD; 1,3,5), as in Figure 2.

'She CHARM algorithm does not determine the association rules.

-  -  -  (A; 1,2,3.4,5,6), (AC; 1,2,3,4,5) (AD; US, 61, (AE; 2, 4,5, 6),  C. Our approach The common part of our approach with CHARM, for  example, is the extension idea. The difference is that we extend a lattice built on a subset of I with some new frequent items. The basic observation for our approach relies on a property of a non closed itemset. An itemset i is not a closed itemset because there is a closed itemset c, where ic c and t( i) = t(c). If we select from t(c) the subset of transactions containing some itemset i', where i ' is  disjoint with c, we find that the resulting transaction set is the one for both the i U i' and c U i'itemsets. Thus, i ui 'will  never he a closed itemset, any i' disjoint with c. We will call this property th.: rion- closure up-propagation. This means that no closu' i:einset can originate in a non-closed itemset through e x f w h n  with  a new item. Thus, extending all closed itemsets in the lattice built on a subset of I with some new frequent items, we will obtain a cover set for all the closed itemsets involving the new frequent items.

We consider the basic operation of extending the closed itemset lattice with one frequent item at a time.

t ' 4  Fig. 3. Extending L, with D,  Lets say we already have the lattice for {A, B, C} and we want to cxtcnd it with {D, E}. As wc can sec in Figurc 4, we practically make a copy of the initial lattice with new, extended itemsets. We eliminate itemsets D and ACD, since they are not closed, and we have the lattice for the { A, B, C, D) set of items. We extend it with E. There are only a few fiequent itemsets from the extended ones and the itemset E is not closcd bccausc of AE. After eliminating E, we obtain the same frequent closed itemset lattice as CHARM does.

Until now, the main difference between our approach and CHARM is that we can start from a previously found lattice.

Fig. 4. Extending L4 with E  Now, we can find all rules with confidence smaller than 1.

The association rules with confidence equal to 1 can he expressed through a base from which every rule can he generated. As shown in [ 9 ] :  The set (X+ c(X) i X X is a pseudo-intent) is a base f o r all global implications, where c is the closure operator,     and X is a pseudo-inrent f X  # c(X). and for all pseudo- intenrs Q c X ,  c(Q) -d  The fact that X t c(X), tells us that X is not a closed itemset. For the initial lattice built on {A, B, C]  we have 3 itemsets that are frequent, but not closed: {B, C, BC}. Since c(R) = ABC, c(C)=AC and they do not include any other itemset, we can say that B and C are pseudo-intents, while BC is not, since Bc BC, but ABCQBC. Thus, the base for global implications consist2of B-AC and C-tA rules.

When we extend the initial lattice with D, as in Figure 3, we find some new potential pseudo-intents {D, ACD] and after extending with E, as in Figure 4, we add E to this set, too. We have c(D)=AD, c(ACD)=ABCD and c(E)=AE. All of them are pseudo-intents in the context of the new lattice and the old ones remain pseudo-intents as well.

In fact, the situation is not always this simple. Let's consider another initial lattice, namely the one built on {B, C, D, E], and let's extend it with the A item. As we can see in Figure 5,  the initial lattice contains the frequent closed sets {C, D, E, BC, CE, BCD]. The frequent itemsets that are not closed are B and CD. For this lattice we have c(B)=BC and c(CD)=BCD, both of them being pseudo-intents.

When we extend the lattice with A, the itemsets C, D and E become not closed, due to AC, AD and AE, respectively, and this relationship propagates over their subconcepts when extending them with A. Thus, all the previously closed itemsets become not closed. We can prove that storing only  newly constructed itemset was a closed itemset, we could not calculate a pseudo-intent starting from that itemset.

An important observation refers situations as the one in Figure 5. If we find a rule with confidence equal to I ,  for all the supersets of the itemset that becomes not closed, we don't need to calculate the support of their extended itemsets, since they are equal to one another. Thus, con$dence(C+A) = 1 leads to confidence(BC4A) = 1, conJ?dence(CE-tA) = 1, conJ?dence(DCD+A) = 1, so supporl(C) = supporl(AC), support(BC) = support(ABC), supporr(CE) = supporl(ACE). We consider this situation, whenever it occurs, as a significant pruning criterion to calculating the support of an extended itemset.

Another operation we have considered is reducing a lattice of closed itemsets with some items. The only way the initial lattice is affected during the extension operation is as in Figure 5 ,  when the original closed itemset becomes not closed and disappears. Keeping this observation in mind, we conclude that, for a closed itemset involving an item to be eliminated, we have to check the existence of the reduced itemset and, if i t  does not exist, we create it with the same support. I f  we keep track of the base of global implications we deal with two steps: - we eliminate the current item from the left-side or from  right-side of the rule, if it exists; if either the left-side or the right-side of the rule becomes empty, the rule is discarded.

-  Fig. 5 .  Extending the concept lattice built on (B, C, D, E )  with A )  the rules C+A, D+A and E t A ,  called generuling rules, along with the old pseudo-intents, we can calculate the pseudo-intents for the final lattice. The list of corresponding closed itemsets is:  c(B)=ABC, c(CD)=ABCD, c(C)=AC, c(D)=AD, c(E)=AE. The 1-frequent itemsets in the list are pseudo-intents since there are no other pseudo-intents to cancel their status. CD is not a pseudo-intent since there are C and D to contradict it. If we add to CD the missing items from c(C), namely c(C)\C = A, we get ACD;that is still an itemset that i s  not closed, belongs to the same closed itemset as CD, and respects the definition of a pseudo-intent. If the  111. DEFINING AND OPERATING WITH DATA MODELS  For our approach, a data model for the association rule problem consists of three components: - the mining context; - the corresponding results; -  There is no question on the necessity of the results in the data model. The only issue is whether we keep track of the pseudo-intents or not. We represent the results as the set of closed itemsets and their associated support information  the observations added by user.

and, when needed, the set of pseudo-intents and their associated closed itemsets.

The mining context stores information on the set of frequent items and the transactions involved in building the model. We also have to store the minsup value; otherwise the results have no relevance.

The user added information is optional and it is meant to record his remarks on the contents of the data model, as a remainder.

When choosing a data model to be extended, we have to check the mining context in order to refer the same transactions, with at least minsup for the support threshold.

while extending the model with new frequent items. If we look for pseudo-intents wc can only choose a previously found model containing associated pseudo-intents. The minconfparameter does not affect choosing the model, since we store the set of frequent closed itemsets, not the association rules.

The correctness of the data model extension can he proved as follows: PI. We extend the lattice 4, with &+,, obtaining L'k+,. We say that L, U L',,, 3 L+,.

Pz. We eliminatc non-closed itemsets from 4, U L'k+l. We say the result is k+,. If we keep track of pseudo-intents, we add to their set all generating rules found during the elimination phase.

P,. Processing the set of potential pseudo-intents, we can calculate all pseudo-intents associated to the lattice of closed itemsets.

The correctness of the data model reduction can he proved as follows: P'l. We eliminate from 4, all closed itemsets involving it; when the original closed itemset does not exist, we create it with the same support as the extendrd one. We say that the result is 4,.

Pz. If we keep track of pseudo-intents, we eliminate ik from the left-side or from the right-side of the global implication; whenever a rule has an empty antecedent or consequent we discard it. We say this new set is the base for global implications associated with b.,.

We have mentioned in the first section the &model. This kind of model is used for result displaying reasons only.

Whenever we enlarge a previously found model we display to the user only the differences between the initial model and the final one, in order to reduce the amount of presented results to the new ones. We have not yet dealt with presenting to the user the closed itemsets that become not closed during the extension.

The way we assume the data model processing will be useful is as follows: - first, the user can mine a few items, getting a fast  response and a small amount of results; if the user is satisfied by a certain data model he can store it  and enlarge it, afterwards, with some other items; if the user is not satisfied with the model he can reduce it by eliminating some items, until he can understand the results.

More, we can use the extension operation on an empty  -  -  data model, building data models from scratch.

~   W. EXPERIMENTAL RESULTS  We have performed several experiments on different data collections from the public domain. We have used a computer Pentium 11, 350MHz, 128 MB RAM. The first experiment that is revealing for this paper is about generating non-concepts. We have used the data from http://lih.stat.cmu,edu/ datasetslboston- corrected data, as it is more revealing for our experiment. We have run an exhaustivc Apriori-type algorithm, wc will call it AI, that finds all Crequent itemsets, closed itemsets as well as non- closed itemsets, and the proposed algorithm, A2, for miusup= 1, 1.5,2,3,4 %. The results as shown in Table 3  We have limited the experiment in what concerns the minimum support, due to the long response time of AI.  As we can see from Table 3, our algorithm builds some non- closed itemsets, but significantly less than an exhaustive algorithm. More, it does not store all non-closed itemsets, but pseudo-intents.

Nrm-closed Non-dostd Psouda- ilamstls iternsets mlcnts  4% 1113 1 27 26 WO I 627 171 A$ -Yo f;.490 36 1 m I .5% 11047 Se6 147 1 Yo IeSsP 8% 21s  In what concerns the time response of the algorithm when mining from scratch or from previous results, we will show a relevant one, run on the same data collection with minsup= I%. Over a selection of 4 items, we have added another 122 items, in several steps, recording the extension time in Textend. Afterwards, we have run the algorithm with the corresponding number of items from scratch, recording the time in T. We have afterwards computed At between successive queries, in AT. The results are in Table 4. We can see that sometimes the computed time is bigger than the algorithm time, and sometimes is smaller. We know that, theoretically, adding an item depends on the initial number of closed itemsets. The basic algorithm considers attributes in a specific order, namely in the increasing order of support (apparently and with no theoretical justification, it is the best choice). Adding items, means considering them last. This is the reason why Textend is not equal to the AT. Anyway, even i f  different, Textend is considerably smaller than mining from scratch.



V. CONCLUSION  In this paper, we have introduced the idea of data model, expressed as frequent closed itemset lattice, with a base for global implications. There are no other approaches that determine the pseudo-intents. The definition in [7] suggest having all non-concept itemsets and validating each of them as a pseudo-intent, staring from the small one; behind this idea is the necessity of generating all non-concepts. We have shown that with a subset of them, significantly smaller, we can still calculate the global implication base. We have  http://lih.stat.cmu,edu   also described two important operations applicable on data models: extension and reduction with several items. The main advantages of our approach are: - Constructing small models of data, makes them more  understandable for the user; the time of response is small; Extending data models with a set of new items returns to the user only the supplementary results, hence a smaller amount of results; the response time is considerably smaller than building the model from scratch; Whenever data models are incomprehensible, some of the items can he removed, thus obtaining an easier to  - Extending or reducing a model spears the time spent building it, thus reusing knowledge.

We are considering applying this approach to the more complex mining process of finding quantitative association rules, in order to detect the best mappings for quantitative attributes in the context of a data model.

