

Abstract?Data Mining means extracting unique information from pre-existing large data sets. Data Mining techniques have been applied to almost all areas such as Education, health care, Stock market, etc. Since the data size is getting increased exponentially, maintaining high performance plays a major challenge in the field of data mining. The data may be in structured format or unstructured format and hence the research on extracting the frequent pattern which is repeated in an iterative fashion seems to be really difficult. The paper aims at evaluating the performance of a Hadoop single node cluster with respect to Big Data. Apriori based Association rule mining algorithm was used to find the frequent patterns and then their rules. Finally the time taken to perform the entire data mining task was found and the performance is evaluated.

Keywords?Big Data, Hadoop, Data Mining, Frequent Item Sets, Cluster

I. INTRODUCTION  In our day to day life the need of data and the process of applying are relatively huge. Due to this enormous development in the recent times the data may be in structural format or semi structural format and the volume of data gets increased simultaneously. There may be various mining process, but in parallel they have their own limitations such as time frequent, data accessibility.

When it comes to Big Data [7] it?s not an easy task to find their associations with the normal mining process [16], hence to overcome that the performance is done in Hadoop. With the concept of ?MapReduce? we can easily associate the relative terms and generate the frequent item set in less time sequence. Searching in Big Data and extracting the  relevant information is not an easy task to be achieved.



II. REVIEW OF EXISTING WORK  Real-time Analytical Processing (RTAP): was used in stream computing to improve business responses (i.e., data in motion). It can be implemented in Big Data but the need of this analytical processing and where it is being applied gets differed from it [2]. It is applied in data ingestion and processed in real time streaming, data queried in online fashion and RAM based processing  A. Map Reduce Framework  Is one of the Google?s assets provided to the world. Due to this framework the distributed programming has become an easy way for execution and retrieving the information from the Big Data. It became more popular due to these advantages and it is freely accessible. The wide range of usage and the availability needed in the industry has to provide Map Reduce with relative efficient and easy to use data mining methods.

Sandy Moens et al [1] proposed a model relate to Frequent Item set Mining, it implements two mining methods for large datasets. They are Dist- Eclat and Big FIM. It deals with the speed and the optimization through which scalability is defined on large data sets. In this frequent item sets are mapped and mined using these methods with K-fIs algorithm.

Xindong Wu et al [7] present a model related to data mining in Big Data using HACE theorem. They have explained about the data driven models, Big Data revolution, Big Data processing       models through which security, information analytics and their challenges are defined.

Pre-loaded local  Input data        Exchange of data  Reduced output  Data          Fig. 1 Overview of MapReduce Framework    Chanchal Yadav, et al [8] presents a model about their approaches to handle large data. It explains that while extracting the data should have some pattern from that mining, which is applied and analyzed. It may relate to any field or any traditional architecture, but we have to define the structure and methods to implement using various algorithms and tools to extract it.

The concept involved in MapReduce [6] is mapping and splitting various nodes in Hadoop Distributed File System. The reduced job is like it combines those data tuples from the data node. That is, through which it generates a new name node for each process it gives the direct access to the data node. Once the mapped process gets initiated later the reduced job will be performed. By this process the MapReduce improves the performance and the real time access (Figure 1).



III. PROPOSED SYSTEM  The implementation of MapReduce can be done in the Apache Hadoop framework. Which is an open source tool. It is a data set cluster in which data can be stored and processed [11]. Heterogeneous Big Data is the main source to be executed in this framework. During the implementation the retail data is converted/considered as the Big Data source and the storage medium in Hadoop Distributed File System [13]. Once this data is processed in the framework, then the MapReduce is implemented by considering all the data which is split into multiple nodes termed as name node and the data node. Both have the details of mapped data through which data is easily accessed. The analyzing factor plays a major role in mining Big Data. Retail data set [10] has various factors such as item number, amount, and purchase data. The retail data set has been extracted to Hadoop Distributed File System. Apache Hadoop map and reduce the retail data set.

Shared memory Parallelization [3] is used to find performance of the mining schemes. Once the association [12] is extracted from this MapReduce frameset, then the data will not be in a single node and it will be split in multiple nodes to generate same associations simultaneously from all the nodes. To this shared memory parallelization is being applied.

The Architecture (Figure.2) of this model implements Data achieve, Transaction system, Operational Data Store (ODS), Big Data Engine[14] and Data Analytics. They are interrelated and both are used to provide the efficient mining process to find the associations existing in Big Data.

Initially, all the datasets are extracted from the Transaction system, which is the information processing system that involves the collection, modification and retrieval of all the data. It also includes reliability, consistency and supports the transaction process. Various processes are involved, such as Bach processing, real time processing, Time sharing and Transaction processing using this process the data can be easily used in further processing methods.

Node1    Node2    Node3    Mapping Process  Node1     Reduce  Node2     Reduce  Node3     Reduce    Output                                     Fig. 2 Architecture of Proposed Mining Framework  Once the extraction is processed, then the left out data is sent to the data archive. It is also interconnected with Big Data engine so that left out data from that engine is also sent to this data archive  Operational Data Store (ODS) is a type of database that serves as an intermediate area of a Data warehouse, through which data can be accessed quickly and efficiently. It contains a small amount of information that is extracted from the transaction system. Using data all the mining [5] and mapping process is executed to find the relative parallel associations between them. Only the relevant information will be present in this module.

Big Data Engine is the backbone of this process model. Through this all the associations and the item sets are extracted and sorted out. The modules inside this Engine are Hadoop, HDFS [15] and MapReduce. Hadoop Distributed File System is the file system for Hadoop through which data is stored and accessed. MapReduce is the concept which is implemented inside the Hadoop, through which the relative or frequent sets [4] are mapped and retrieved (Figure 3). Performance evaluation of data retrieval and map reducing has been carried out.



IV. PROCESS FLOW  B.File System Counters  It deals about files in HDFS. File system counters contains the information about the number of files read, written, read operation, write operation and large read operation. Initially, these processes take place in two layers, one on file system and the other in HDFS.

C. Job Counters  Using the extraction process, the performance is found.  Performance is evaluated by launching the data, mapping it and reducing it further. The time take for the process was in milliseconds. The process remains the same, even if the data size varies.

D. Map-Reduce Framework  ? Mapping input & output records ? Reduced input & output records ? Time elapsed ? CPU time spend ? Physical & virtual memory    Data  Archive  Transaction  Systems  Big Data process flow  Hadoop  HDFS  Map Reduce  DW/ANALYTICS/DATA MINING   DW OLAP        Fig.3 Module of execution    E.Errors  It deals with, ? Bad id ? Connection ? I/O ? Wrong length & map

V. PERFORMANCE EVALUATION  The data set is stored in the local host and moved into the Hadoop Distributed File System.

Framework [9] used to be a Single Node Hadoop Framework on a processor. Performance and Speed are the two major factors to be considered for Big Data.  When Big Data is implemented on this platform, there was a small difference in performance and the speed.

Initially we moved our retail data set from local host to HDFS. The data set can be of any format and any size. From HDFS the data set in a MapReduce framework were extracted. The execution time for the whole process is about 15sec.

The total time spent by all mappings in occupied slots is in milliseconds and the total time spent by all reducing in occupied slots is also in milliseconds.

When the process was repeated, the time taken for  mapping and reducing was decreased. From this, it can be inferred that Big Data on Hadoop environment is more efficient.  The performance evaluation graph will describe the real time difference of mapping and reducing (Figure 4).

Mapped reduced  1st process  2nd process  3rd process   Fig. 4 Performance Graph of Mapping and Reducing

VI. CONCLUSION  It's based upon performance evaluation in the Hadoop single node cluster. The resultant output is obtained by the same performance. Based on the processor and the size of the data set the performance and the execution may differ in the multiple node clusters. But in this execution, the performance level is high and the execution time is very less and while repeating the same process the execution time keeps on reducing, that results in the performance in Hadoop with Big Data.

