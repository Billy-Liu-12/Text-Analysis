TEXT CATEGORIZATION BASED ON FREQUENT PATTERNS WITH TERM  FREQUENCY

Abstract: The association categorization technology based on frequent  patterns is recently presented, which build the elasdRcation rules by frequent patterns in various categories and classii the new text employing these des. But the current association classification methods exist shortage in two aspects when it is applied to classii text da&: one is the method iguored the information about word's frequency in a text ; the other is the method need pruning d e s  when the mass rules are generated, but that lead to the veracity of classifying dropped. Therefore, this paper present a texi categorization algorithm based on frequent patter0 with term frequency, and obtains higher performance than other association categorization methods and some current text classification methods. Our study provides evidence that association rule mining can he used for the construction of fast and effective classifiers for automatic text Categorization.

Keyword: Association Rnles;Text categorization; Frequent Pattern  1. Introduction  Automatic text categorization has always been an important application and research topic since the inception of digital documents. Today, text categorization is a necessity due to the very large amount of text documents that we have to deal with daily. Most of the existing approaches for text categorization are based on Statistics and Machine Learning. Among these approaches, the famous are Niiive Bayes[l], k-Nearest Neighbor (KNN)[Z], Linear Least Squares Fit (LLSF)[3], Neural Network[4], Support Vector Machine (SVM)[6] and so on.

Recently,. some new algorithms of classification are introduced, which apply association rule [7] to classifying data, for example CBA [IO], CMAR [9] and ARC [Ill.

These classification algorithms discover association rules in the training samples and use these rules to build classifier.

But when these algorithms are used in text datasets, there are some shortages: (1) term frequency information of text  0-7803-8403-2/04/520.00 WOO4 IEEE  is neglected in all text classification methods based on association rules. The term frequency is important feahm of text datasets, so it can't be ignored, especially to association rules defined based on probability of words occurrence; (2) among most of the existing classification methods based on association rules, rule pruning strategies are applied when the mass rules are generated. Although that can speed up the categorization process, the classifying accuracy is reduced at the same time.

In this paper, we present a new text association categorization method - text categorization based on association rules with term frequency. We mainly make the following contributions: ( I )  during the training process, we introduce term frequent into frequent pattem, and put forward to frequent pattem with term frequency mining algorithm, (2) we apply frequent pattems with term frequency to build text classifier, and store the rules in classification rules tree (CR-tree). Our method can achieve better efficiency even without pruning rules, and avoids the problem of classification accuracy drop due to rule pruning in other method [9, 1 I].

2. The Feature Vector Expression for Documents  Text categorization based on association rule usually consists of the following steps: fustly, documents' feature vectors are generated. The unshuctured text data is transformed into structured text data that is expressed by the document feature vector. Secondly, association rules are found by Apriori-lie algorithm, once the entire set of rules has been generated, an important step is to prune rules in order to reduce the amount of rules. The last step is to classify new documents applying the association rules generated in the second step.

In order to convert the text data into the document feature vector, we apply N-gram language model and x'statistics technology to selecting features of training document set. If D is a training document set, C = [C,,   mailto:c-xiaoyun@Zlcn.com    C?, ..., Cm]  is a set of categories, a document d j  E D is assigned to category Ci , d j  =(r,(w,),t2(w2) .._., t.(w,)) is used to model the document d,,where wi is the term frequency of keyword ti ,it indicate the occurrence frequency of ti in the document d i  .

We discover the term frequency in a text dataset generally gather in the specifically range; only terms frequency of a few documents are higher. So we define maximum term frequency threshold M ( t , )  for term r k  to limit term frequency at lower level. The term frequency over the maximum threshold will he replaced with the threshold. Maximum threshold strategy can prohibit from generating excessive rules.

3. Generating Classification Rules  3.1. The Approach for Association Rules Discovery  Association rule mining is a data mining task that discovers relationships among items in a transaction database. Association rules have been extensively studied in the literature. Formally, association rules are defined as follows: Let I = ( i  l,....in) he a set of items and D be a set of transactions, where each transaction consists of a subset of items in I. An association rule is an implication of the fo rmX- tY,  w h e r e X c I , Y c I a n d X n Y = 0 .  The rule X -t Y holds in D with confidence c if c% of transactions in D that contain X also contain Y. The rule has a support s in the transaction set D if s% of the transactions in D contains X U Y 171.

3.2. Mining Frequent Pattern with Term Frequency  The general algorithms of association rules mining don't involve term frequency, so we can't directly applied them to mining frequent term sets in the document set with term frequency. We fmtly introduce some symbol.

Definition 1 Given terms tj(wj)andt,(wj),  (1) We say ti(wi) and t,(wj) are similar whether wj = wi  (2) If wc = wj , ti(wi) and tj(wj) are equal, written hold or not, written &,,ti) ;  t,(WJ = t,(W,) Given terms t j (wj)  andtj(wi),  (3) If I, 2 t j  and w, h v j ,  we say term t , ( w i )  including rj(wj) ,written t,(wi) & ti(wj); (4) If t, Qjandw, <wi, we say term rj(wt) belong to  ti(wj), written rt(wj) C, rj(wi);  Example 1 (IC91 & (11 (2)l. 111 (5),W)I 2 M5)I.

U1(5). M2)I (11 (3, Iz(1)l. 111 (.VI= 111 (91 .

Definition 2 Given a term t, the term frequency of t represents the time of t occurs in the document d or the term set X , written by R(t, d) or R(t, X)  respectively.

Definition 3 Given a document d that contains the  term-set X , the support count 9, of X in d is  Example 2 Given a document d= (11(10), 12(3), 13(6)), the support count of the term-set {11(5), 12(1)] for d  n  The support count 'p would be 1 when the frequency of all terms is equal 1.

Definition 4 Given a document set D , every document d, E D has the following form:  d,  = (t,(w,),tZ(wz). .... t.(w,)) the support of term set X in D is  (2) kX,d , )  ID1 (P(X, D)= E-  Definition 5 Given the minimum support threshold E, if q(x.0) holds, then term set X is frequent in D. The frequent term set X is said tofrequent pattern too.

Lemma 1 If a term set X with term frequency is frequent, then any superset of X must also he frequent.

The proof is easy so we skip over. But the superset must consider the term frequency according to definition 1. For example, term t , (3 )  is the superset of t , (2)  ort,(l).

Algorithm 1 Find frequent pattern with term frequency Input A subset 0, of the Document set D, where C, is the category attached to the document d, ; A minimum support threshold E ; Output the frequent pattern set F of category C, Method (1) For each term t E 0, do begin  (2) N t I =  M d R ( t ,  d,  )I z; ) ; (3) j=1; (4) repeat ( 5 )  t (j).support + dtW,Q); (6) (7) ]=j+l:  F,=( t (i) I t ( j ) . ~ ~ p p o r V ~ )  ;      (8) until(j=l)or(i<M[t]andr(j-l)?Fl) (9) End' (IO) For (i=Z Fj.l# 0.i ++) do begin  (12) (13) Foreach X inTj dobegin  (15) End (16) (17) End (18) F+U,(X E i$ I i  > 1)  (1 1) Tj=(Fi.i W FiJ Tj=Ti-[ X I (i-l)tem-setofXB Fi.I)  (14) x .support = d X . d Fi=[ X E &  I X . support>_& ]  Note, the i-term set indicate the term-set consist of i different keywords. such as ab(3) and cd( 1) are.2-term sets.

Algorithm 1-6 steps generate frequent I-termset Fl: the keyword t maybe have many frequent I-termset, for example t(l),t(2),t(3) and so on. if t(2) is not belong to F, then t (i) (i>2 1 can not belong to FI .

In step 7-12, all the fresuent i-term sets Fj are generated; the candidate i-term set Ti is generated by joining two different frequent i-1 term sets x . x ' ~ F i . ~ .  The joining condition of frequent i-1 term sets include: (1) two frequent ill term sets X and x 'have and only have one different keyword; (2) The term frequency is same if the keyword is same in X and x .

Example 3 Given the training documents of the category in C, as following, assume the minimum support count is 4.

di=(I1(3). I&). MI)) dz=(Iz(l). Id4). M2)) d d I i ( 4 ) .  Iz(5). b(3)) (b=(I1(2). Is(1). MI)) 4=(11(3) . I&). Id2)) 11 Iz 13 b 1s  d 1 3 2 0 0 1 dz 0 1 4  0 2 d 3 4 5 0 3 O a 2 0 1 0  1 4 3 4 2 0 0  the term frequency matrix is showed as follows:  The frequent pattern set for C, is F=~Il(I)J~(I),13(l),  If the document set D=U:,D. includes m classes, the category collectionc= [cl,c2,,..cm) ,whereD, consists of all documents belonging to C, . The frequent pattern set found from document subsets of various categories are different, but they are likely to intersect each other.

Definition 5 the classification rule for category C,(i = 12 .... m) is formal as X X, and X is frequent in D, . X is the condition part of the d e ,  C, is the consequence  I 5 ~ l ~ . ~ l ~ ~ ~ ~ ~ z ~ ~ ~ . ~ l ~ ~ ~ ~ z ~ ~ ~ ,  Il(l)IZ(Z). IZ(1) 1 0 ) .  Ii(2) IZ(2) I.

part of the rule.

Definition 6 The confidence of rule x x, is defined by  (3)  In accordance with definition 6, we know that the higher confidence of a rule is, the larger probability that the term set X occur in certain documents of the category c, is, contrarily, the smaller the probability that the term set x appear in other categories is.

The classification rules that are built by frequent term sets in example 1 as following: ldl)*C, Ir(l)*C, MI)*C, 4(I)*C, II(Z)*C, 1d2)*Cr  I~(l)Alr(l)*C, Idl)A4(2)*C, IAl)AIdZ)*C, I@)AL(Z)*C, We considered two approaches to building a text  classifier based on term association. The first one is to extract association rules from the entire training set [9,10], but it is difficult to handle categories that are small or overlapping. As a result we adopt the local approach to solve the problems, that is, classification rules are generated from each category respectively.

4.1 Building Classifier  Classifying the New Document Using the CR-tree  We expect the algorithm can handle the pmblem of the multi-class categorization, so following notions are introduced.

Define 7 Given a document d to classify, class confidence for category C is the sum of rules' confidence that match with document d and point to category C, that is  (4)  Where R, is the set of rules that match with d and its class label is C.

Define 8 If there are n rules matching with document d, which ml of them have class label CI, m2 of them have class label C2, ..., and mk of them have class label Cr. the c h s  diferencefuctor of category C, for document d is:  Ma+(Q(d ,C,d - Q(d,C,) (i. j = l,Z ..m) L61 (5)  Difference factor shows difference between all other classes and the highest confidence class. When solve the problem of multiple-class classification, those categories that difference factor is less than given threshold 6 are selected, contrarily, in the case of single-class classification, only the highest confidence category is selected.

The detailed classifying process is elabrated in the following:  6(d,CJ) = Mux(Q(d,C,))      Step 1: Find all matched rules with the new document d (the matched rules of d are denote those rules that condition part appear in d).

Step 2 Group all matched rules of d by category label and the p u p s  are sorted order by the class confidences P descending.

Step 3: Only those categories that exceed given threshold 8 areselected.

Obviously, step 1 is the step that spends the most time in whole process of classifying.

4.2 Search Matching Rules Based CR-tree  The existing association classification method speeds up the process of classify by pruning rules [8,9,11,12], hut that will lead to lower the accuracy of categorization.

Wherefore, we search the matched rules by classification rule tree and don?t prune any rules.

Theorem 1 (1) If a document d to classify doesn?t match with the rule R, :T X , then the document doesn?t match with the super rules of RI :T X . (2) If a document d doesn?t match with the pattem q ,  then the document don?t match with the super-pattem of TI.

The amount of rules that must be scanned during the classification phase can be reduced by theorem 1. For example if the rulea(3) A b ( l ) X ,  does not match with the document d, its supper rule 4 3 )  A b( l )  A d ( Z ) X ,  have no use for being review because it must not match with the document d.

We stored all classification rules in Classification Rules Tree (CR-tree), which is a prefix tree structure. The construction process of CR-tree is explained as follows: (1) A CR-tree has a mot node: (2) all nodes in the path from root node to no root node denote the term appearing at left hand side of rules, called Panern Node, if the path consist of all terms that occur at left hand side of a rule, then the lasted pattem node is Rule Node, which can denote a rule. (3) the node that denote super pattem is the child of the node that denote sub pattem. Since CR-trees are applied to store classification rules, rule nodes must store the category label and the confidence of the rule.

Example 4 Assume a classification rule set R found in training phase is show as tahle(a):  tahle(a) table (b)  I Rule U I 12 Rule U  1 1 ; b(l)a(Z)=>CZ 60% 1 ; a(Z)b( l )=S Firstly, all terms appearing at left hand side of rules are sorted according to frequency descending order. the result set is L:(b(l):5,a(2):4, c(l):2, d(l):l]. We reordered rules in R by L and obtain rules set R? is show in table (3). Then building CR-tree by R (as shown in Figure 1 ) .

a(Z)Wl)C(l)=X, b(l)a(2)c(l)=X3 60% Wl)d(l)=>CI 70% b(l)d(I)=>Ct 70% b(l)c(l)=>C1 60% b(l)c(l)=>Cx 60%  (5 c(l):C3,60 Figure1 Classification rules tree (CR-tree) for example 4  The sub-pattem and super-pattem share same prefix nodes in CR-tree leads to a lot of storage space he saved.

According to theorem 1,all children node pattems impossibly appear in d if the father node pattems don?t appear in document d. So, we needn?t consider the sub-tree of the rule node or pattem node that doesn?t match with the new document, and can review directly its right sibling node. If there isn?t the right sibling node, the algorithm backtracks to the father node of the current node and considers the right sibling node of father node.

5. Experimental Results and Algorithm Analysis  In order to test performance of our approach, we implement this algorithm. Data set is subset of news in People Daily from 1993 to 1997, which consist of 2000 training documents and 2315 testing documents, the data set is divided into ten categories. The measure used is micro-average of F, .

Table 1 shows a comparison between our TRARC classifier and other well known methods. The text features are obtained by N-Gram technology and x2  statistic method. the number of feature is 60.

\ TRARC (Ourapproach) ARC Training time(sec) 196 51  Classi&ing tiWsec) 19 46 The number of rules 5834 1641 MicmaveragoFdB) 88.4 84.86  We conclude as following: Our method performs well as compared to the other methods. It outperforms most of the conventional methods, such as KNN, Bayes and SVM. Specially, classification accuracy of TRARC algorithm is better than ARC due to considering the term frequency in TRARC.

Although the classification rules generated by TRARC are further more than ARC, TRARC based on CR-tree is faster than ARC.

Training time of TRARC is longer than ARC, since TRARC must review more candidate term se6 after using term frequency.

The performance of association classification method is higher than Bayes, KNN and SVM under lower feature numbers.

KNN SVM  0.04 0.06 18  60 92 14 - - -  80.57 75.4 80.3  CR-tree is obviously better than algorithms no applying CR-tree whether super rules are pruned or not.

Figure 2 shows the effect of using maximum term frequency. From figure 2(a) we can find, with increase of maximum term frequency. F1 value has upward trend.

Figure 2(b) illuminates augment of maximum term freqnency result in the training time increase. By figure 2(c), we found, although the number of rules and training times are increased, the classification time of TRARC maintain invariableness because of applying CR-Tree in TRARC, whereas the classification time of RARC without using CR-tree is obvious increased. Figure3 shows that training time and classification time descend evidently with the increase of support threshold, and the efficiency of TRARC is better than one of RARC.

Without C R - k  In table 2 shows the efficiency of algorithms using  With CR-Vee  \ The number of Rules Classification lime(Sec) Miemverage-Fl(%)  Table 2 The effect of using CR-tree or super rules pruning  Super N k S  Super N k  super NleF super d e s 434 5834 434 5834 40 52 5.5 19  80.4 88.9 80.4 889  -  88 ;815m I 615 c 60 - -  ~ 413  85 e 15 I  9 IW .$ 80 -  2 40 b 87 'g 215 2 20 - - 1  'j: 0 : : : : : : : : :  .Y 86 I 0 0 1 2 3 4 5 6 1 8 9 1 0  0 1 2 3  4 3 6 7 8  9 1 0  0 1 2 3 4  5 6  7 8 9 1 0  (a) @) (C)  Figure2 The effect of maximum term frequency on micro-average F1 value, training time (minsup=ZO%) and classification time      IQ 20 30 40 50 60 10 20 30 40 50 60 IQ 20 30 40 50 60  uinimun S Y I I W l t  h i m u n  s u o ~ r t t x )  uinimn supwrt(X)  I d T R A R C  +mc I  (a) (3) (C)  (maximum term frequency is 5) Figure 3 The effect of minimum support threshold on micro-average F1 value, training time and classification time  6. Conclusions  In this paper we have presented a method applying frequent patterns with term frequency to build the classifier TRARC, and classification rules are stored by CR-Tree.

The maximum term frequency threshold is introduced to limit the amount of rules discovered. Our experiment proves the method can greatly improve association classifier?s performance, and avoid the problem of classification accuracy declining that is brought by pruning rules.

