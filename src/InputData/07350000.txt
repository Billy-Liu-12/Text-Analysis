Relative Patterns Discovery toward Big Data Analytics

Abstract?Recently, enterprises and governments invested aggressively in big data analytics because it is truly representative of popular opinion based on millions of people.

Despite bringing new opportunities, big data encounters the challenges such as extremely large number of observations (e.g., millions of transactions), high dimensionality (e.g., thousands of items), and immediate response. Taking big data into consideration, the conventional association analysis is frustrated by the extraction of patterns information.

Specifically, the computational complexity of frequent itemsets mining increases exponentially by the number of items, which has been proven to be an NP-Complete problem. Although many studies used a pruning-patterns strategy to reduce the complexity, it probably distorts the shape of data and incurs inaccurate result. In this paper, we introduce relative patterns discovery (named RPD) that explores the same patterns between each two observations. To show that RPD is a pragmatic solution toward big data analytics, we design a scalable outlier detection method (named SOD) based on the concept of RPD. Particularly, SOD can score the anomaly without enumerate all the relative patterns. The empirical investigations, conducted with various real-world datasets, demonstrate that SOD performs well even in the environment of large number of observations and high dimensionality.

Keywords-Association analysis; big data; data mining; outlier detection

I.  INTRODUCTION With rapid progress in information and communication technology, several practical applications have been developed such as electronic commerce, electronic government, social networking sites, and so on, which significantly affect the daily lives of people. These various applications hold the massive amounts of data on people?s behaviors and sentiments. Such a kind of data is called big data simply [1]. In the past, enterprises and governments strove to better understand the behavioral patterns that hid behind data; recently, they invested aggressively in big data analytics [2] because big data is truly representative of popular opinion based on millions of people.

McAfee and Brynjolfsson [3] considered that big data is not just a fashion term about analytics; in fact, big data has  three substantial differences: volume, velocity, and variety.

The volume indicates that big data involves analyzing extremely large amounts of data. For example, in Walmart (the world?s second largest public corporation [4]), 245 million customers visited Walmart-related stores each week, and Walmart Supercenters offered about 142000 different items. In spite of the fact that the massive data looks like a bit of a mess, it reflects the views of customers, which can help enterprises to create many niche markets and applications. The velocity means that big data focuses on real-time information. For example, the MIT Media Lab measured the position information through mobile phones, which is used for predicting the number of customers who were visiting the department store during the Christmas shopping season. By real-time insight, executives can gain competitive advantage. The variety signifies that big data consists of many sources such as demographics, GPS signals of mobile phones, the media in social networking sites, and so on. Hay, George, Moyes, and Brownstein [5] discussed a big data application in health affairs, for example, combined online social media with epidemiologically relevant environmental information. This integrated approach can update the real-time spatial map of disease risk and then immediately supervising global infectious disease. Despite bringing new opportunities, big data encounters the challenges such as extremely large number of observations (e.g., millions of transactions), high dimensionality (e.g., thousands of items), and immediate response (e.g., analyzing the massive data and reporting the result of analysis within short time).

Taking big data into consideration, the conventional association analysis [6] is frustrated by the extraction of patterns information. In the conventional association analysis, a low value of minimum support usually leads to vast amounts of frequent itemsets and massive association rules, but most of them are useless. For example, suppose that one of observations contains 100 items and the lowest value of minimum support is given by a user (i.e., the command to find out all itemsets that their occurrence is equal to or larger than once). In this case, there is at least  1002  (approximately 1.2676506e+30) frequent itemsets and   DOI    DOI 10.1109/ICEBE.2015.74     100 100 13 2 1+? +  (approximately 5.1537752e+47) association rules. Using the higher value of minimum support can significantly reduce the number of frequent itemsets and association rules; however, a very large amount of patterns are pruned simultaneously. Such a pruning way probably distorts the shape of data and incurs inaccurate result [7].

In general, an algorithm with lower computational complexity [8] would be more efficient, especially if the size of input data is large. The previous methods of frequent itemsets mining perform better in the cases of higher minimum support [9]; however, in the worst case, they need to run in exponential time. Specifically, the computational complexity of frequent itemsets mining increases linearly by the number of observations (or called transactions), but it increases exponentially by the number of dimensionalities (or called items). Moreover, discovering the frequent itemsets has been proven to be an NP-Complete problem [10]. In spite of the fact that frequent itemsets mining achieved success in many applications, it is impractical to resolve the issue of big data owing to the inevitable challenges: extremely large number of observations, high dimensionality, and immediate response.

The challenges of big data stimulate us to provide novel perspectives on association analysis. In addition to the higher frequency of itemsets, it would be a potential application in the exploration of behavior and relationship between observations. In this paper, we introduce relative patterns discovery (named RPD [7]) that explore the same patterns between each two observations. It is sensible to examine the behavioral characteristic of an observation by comparison with that of other observations. For example, in anomaly detection (or called outlier detection) [11], an observation tends to be anomalous if this observation contains significantly fewer or more characteristics in comparison to other observations.

Based on the concept of relative patterns, we devise a scalable outlier-detection method (named SOD) to show that the concept of RPD is a pragmatic solution toward big data analytics. Recall that RPD needs to find out the relative patterns between each two observations. In the worst case, there are 2?  relative patterns, where ?  denotes the number of non-identical observations. In fact, our proposed SOD primarily aims for evaluating the resemblance between observations so that it would be better to save an unnecessary expense such as enumerating all relative patterns. Fortunately, we discover an algebraic property that can efficiently extract the count of characteristics without enumerating all relative patterns.

In SOD, we create a characteristics table (like a hash table) by counting the elements of every observation. Then, for every observation we compute its resemblance score by referring the characteristics table. Simply, SOD runs in linear time and its computation complexity increases linearly by the number of non-identical observations as well as dimensionalities. The more important is that SOD only needs to compute the resemblance score of observations by  using the updated characteristics table. The size of characteristics table, which is equal to the number of items, is in a constant state. For example, in the case of Walmart, even though the number of transactions (made by customers) increase significantly, there are still 142,000 items and the characteristics table just costs around 5 megabytes ( 5142000 2  bits? ). Thus, SOD is scalable even if both the number of observations and the number of dimensionalities are very large.

The remainder of this paper is organized in the following.

In Section 2, we introduce the concept of relative patterns discovery (RPD). In Section 3, we describe the scalable outlier detection method (SOD). In Section 4, we guide the design of empirical investigation that is used to evaluate the effectiveness and efficiency of SOD, and we discuss the results of analysis. Finally, we conclude this paper in Section 5.



II. RELATED WORK  A. Frequent Itemsets Mining We take a simple example to explain the task of frequent  itemsets mining. Example 1: given that a transaction database has three items apple, beer, and cookie that are denoted as a, b, and c separately. Then, the number of potential itemsets is seven, i.e. {a}, {b}, {c}, {a, b}, {a, c}, {b, c}, and {a, b, c}, and the number of potential association rules is twelve, i.e. {a}?{b}, {a}?{c}, {b}?{a}, {b}?{c}, {c}?{a}, {c}?{b}, {a}?{bc}, {b}?{ac}, {c}?{ab}, {ab}?{c}, {ac}?{b}, {bc}?{a}, where the rule {a}?{b} means that customers would like purchase the item b if they have already bought the item a. Formally, the task of association analysis [6] involves up to 2 1? ? candidates for frequent itemsets, and the number of the potential association rules is up to 13 2 1? ? +? + , where ?  is the number of items (products).

Suppose that we have five transactions (a, b, c), (b, c), (c), (a, b, c), and (b, c). Then, the number of occurrences of itemsets is presented as below. The 1-itemsets (an itemset contains one item): {a}=2, {b}=3, {c}=4; the 2-itemsets (an itemset contains two items): {a, b}=2, {b, c}=3, {a, c}=2; and the 3-itemsets (an itemset contains three items): {a, b, c }=2. Assume that executives are interested in the itemsets with high rate of purchase and they set the threshold (or called minimum support) ? as 3. Then, only the itemsets {b}, {c}, and {b, c} are satisfied with the value ? and therefore be defined as frequent itemsets.

B. Relative Patterns Discovery Pai et al. [7] discovered that a frequent-itemsets-based  outlier detection method is inaccurate when pruning a fair number of patterns (i.e., using the higher value of minimum support). From a novel perspective on association analysis, Pai et al. [7] devised relative patterns discovery (called RPD) to explore the relation between each two observations. The definition 1 formally describes what a relative pattern is.

Definition 1. Suppose the data D is composed of ?     observations and ? attributes. For each two observations, a relative pattern is a collection of attributes that occur in both of the two observations.

Without pruning patterns, RPD can represent a natural panorama of data, which is useful for analyzing new data.

Indeed, it is unattainable to determine which patterns are useless beforehand. To show the applicability of RPD, Pai et al. [7] designed an outlier detection method (called UA) by using the knowledge of relative patterns.

We provide an example of relative patterns discovery and the outlier detection method Pai et al. [7] proposed.

Example 2: Given a dataset with six observations, namely, O1=(a2, b1, c1), O2=(a1, b1, c1), O3=(a2, b2, c2), O4=(a1, b2, c2), O5=(a2, b2, c2), O6=(a1, b1, c1), every observation has 3 dimensionalities (i.e., a, b, and c), and each dimensionality has 2 attributes (e.g., b1 and b2 in dimensionality b).

O2'  O1'  {b1,c1} {   }  {a2}  ?  O3'  O4'  {b2,c2}  {a1}  {   }?   Fig. 1. An example of relative patterns discovery.

In the initial phase, we count the number of occurrences  of observations and we reduce the duplicate observations.

Now, we have four distinctive (non-identical) observations: O1?=((a2, b1, c1), 1), O2?=((a1, b1, c1), 2), O3?=((a2, b2, c2), 2), and O4?=((a1, b2, c2), 1). In the second phase, we use the operation of intersection to find out all the relative patterns as shown in Figure 1, i.e., O1? ? O2? ={b1, c1}, O1? ? O3? ={a2}, O1? ? O4? ={?}, O2? ? O3? ={?}, O2? ? O4? ={a1}, and O3? ? O4? ={b2, c2}. Note that a duplicate observation cannot produce any new relative pattern. For efficiency purpose, we can reduce a duplicate observation without distorting the number of relative patterns. In the third phase, we count the number of occurrences of relative patterns, i.e., ({b1, c1}, 3), ({a2}, 3), ({a1}, 3), and ({b2, c2}, 3). In the fourth phase, we evaluate the score of anomaly by using the equation (1), where variable ??  is the length of the observation ? , variable ??  is the length of relative pattern ? , and ?  is the number of relative patterns.

2 2   _ =( ( ) )+ ( ( ) )Score O count count ?  ? ? ? ? ? ? =  ? ??? ?       (1) The scoring result is presented as below.

2 2 2 1 2 1 1 2 1 1({ , , },{ },{ , }) (1 3 ) (3 1 ) (3 2 ) 24;O a b c a b c= = ? + ? + ? =  2 2 2 2 1 1 1 2 1 1({ , , },{ },{ , }) (2 3 ) (3 1 ) (3 2 ) 33;O a b c a b c= = ? + ? + ? = 3 33;O = 4 24;O = 5 33;O = 6 33;O =  By definition [12], an outlier is ?an observation that deviates so much from other observations as to arouse suspicion that it was generated by a different mechanism?.

The example 2 shows that the method UA can discern the anomalous observations O1 and O4. In the experiments [7] with eight real-world datasets [13], a frequent-itemsets-based  outlier detection method is inaccurate when the value of minimum support is high. On the contrary, for the best result, the method UA can enhance the accuracy from 0.5 to 0.99.

Recall that in the worst case, i.e., given the least value of minimum support (?=1), the state-of-the-art frequent- itemsets-mining methods need to run in exponential time for enumerating all itemsets. In contrast, even in the worst case, i.e., none of duplicate observations, RPD runs in quadratic time and UA runs in cubic time.

The NonDuplicate  Observations Keys Values  Array  c1  The Score of O1'  (1+2)?2,  The Charact- eristics  1O1'=(a2, b1, c1) 2O2'=(a1, b1, c1) 2O3'=(a2, b2, c2) 1O4'=(a1, b2, c2)  b1a2  Total: 12  1?3,  ( )count ch  ?  ? ? ? =  ?? 0.

count ch? ??  c1b1a1  c2b2a2  c2b2a1  c1  a2 b1 c1  a2 b1 (1+2)?1,  The NonDuplicate Observations Keys Values  Array  c1  The Score of O2'  (2+1)?2,  The Charact- eristics  1O1'=(a2, b1, c1) 2O2'=(a1, b1, c1) 2O3'=(a2, b2, c2) 1O4'=(a1, b2, c2)  b1a2  Total: 15  2?3,  ( )count ch  ?  ? ? ? =  ?? (2+1)?1.

count ch? ??  c1b1a1  c2b2a2  c2b2a1a1  c1 0, b1  b1 a1 c1  The NonDuplicate Observations Keys Values  Array  c1  The Score of O3'  (2+1)?1,  The Charact- eristics  1O1'=(a2, b1, c1) 2O2'=(a1, b1, c1) 2O3'=(a2, b2, c2) 1O4'=(a1, b2, c2)  b1a2  Total: 15  2?3,  ( )count ch  ?  ? ? ? =  ?? (2+1)?2.

count ch? ??  c1b1a1  c2b2a2  c2b2a1c2  a2 0,  c2b2a2  The NonDuplicate Observations Keys Values  Array  c1  The Score of O4'  (1+2)?1,  The Charact- eristics  1O1'=(a2, b1, c1) 2O2'=(a1, b1, c1) 2O3'=(a2, b2, c2) 1O4'=(a1, b2, c2)  b1a2  Total: 12  1?3,  ( )count ch  ?  ? ? ? =  ?? (1+2)?2.

count ch? ??  c1b1a1  c2b2a2  c2b2a1c2  a1 c2b2  a1  0,  b2  Fig. 2. An illustration of the integrated mechanism RS.



III. THE ENHANCED MECHANISMS The requirement of immediate response inspires us to  ponder whether we can greatly facilitate the process in UA.

To simplify the process, we design an integrated mechanism (named RS) that performs RPD and a scoring metric at the same time. Instead of using the square of the length of relative patterns, we compute the resemblance score of observations by using the characteristics of relative patterns.

Figure 2 illustrates the process of RS. The initial phase of RS is the same as that of UA. In the second phase, we perform the operation of intersection to discover the number of occurrences of characteristics in every relative pattern.

For example, in the operation of intersection O1? ? O2? ={b1, c1}, there are two characteristics b1 and c1, and the relative pattern {b1, c1} occurs three times . In the third phase, the anomalous score is computed by the equation (2). The variable ch?  means the number of characteristics in the     observation ? , and variable ch?  is the number of characteristics in the relative pattern ? .

_ =( )+ ( )Score O count ch count ch ?  ? ? ? ? ? ? =  ? ??               (2) After inspecting the processes of RS, we find out many  duplicate operations, i.e., redundantly extracting the characteristics of every relative pattern. Fortunately, we discover an algebraic property that can efficiently extract the count of characteristics of every relative pattern without enumerating all relative patterns. Taking advantage of the algebraic property, we design a scalable outlier detection method (named SOD) to significantly improve the process of RS. Figure 3 shows the process of SOD. In the initial phase, we read the characteristics of non-duplicated observations. For example, in the observation O1?, we read the characteristics ?a2?, ?b1?, and ?c1? in sequence. In the second phase, we build a characteristic table that is composed of the characteristics and their count. In the third phase, we compute the score of every observation by indexing the characteristics table. For example, in the observation O1?, the occurrence of O1? is one so that the initial value of O1? is 1 3 3? = . Then we add the values of the characteristics that occur in O1?. Since O1? has three characteristics, the score of O1? is 3 (3 3 3) 12+ + + = . We can observe that the result of SOD is equal to that of RS. In the following, we prove the assertion that the results of RS and SOD are identical.

Proof. Suppose we have ?  observations. We select an  arbitrary observation Oi that contains ?  characteristic(s).

We separately perform the intersection between Oi and other observations, i.e., O1 ? Oi, O2 ? Oi, ?, and Oi-1 ? Oi. The result of the intersection belongs to two states: existing characteristic(s) and empty. According to the theorem ?commutative property of sets? [14], a different sequence of intersections yield the same characteristic sets, e.g., the operation O1 ? O2 is equal to that of O2 ? O1. Thus, the state of existing characteristic(s) ensures that the intersections between other observations produce the same count of the characteristic. In example 2, either O1? or O2? has the relative pattern {b1, c1}, and the count of characteristic b1 is 3 regardless of in the round R1 (O1? ? O2?, O1? ? O3?, and O1? ? O4?) or round R2 (O2? ? O1?, O2? ? O3?, and O2? ? O4?). By definition, the intersection of any set with an empty set is an empty set. Hence, the state of empty ensures that an intersection between other observations (from O1 to Oi-1) cannot produce the characteristic of Oi if when the intersection between Oi and each of other observations yields an empty set. In example 2, the observation O2? does not have the characteristic a2. In other words, O2? cannot produce a2 by intersecting with other observations.  Thus, for the observations O3?, the intersection results between O2? and other observations cannot change the count of the  characteristic a2 in O3?            ? We use an example to explain the difference between RS  and SOD. Example 3: Suppose that there are four observations such as O1=(d1, e1, f1), O2=(d1, e2, f1), O3=(d1, e3, f2), O4=(d1, e2, f3). In the process of RS, for the characteristic d1, we need to compute the intersection six times, i.e., O1 ? O2, O1 ? O3, O1 ? O4, O2 ? O3, O2 ? O4, and O3 ? O4. In the process of SOD, for the characteristic d1, we just compute a count of four. In terms of time complexity, RS runs in quadratic time; by comparison, SOD just runs in linear time.

The NonDuplicate Observations Keys Valuesc1  The Scores Keys Values O1'  (1?3)+3+3+3=12  The Charact- eristics  1O1'=(a2, b1, c1) 2O2'=(a1, b1, c1) 2O3'=(a2, b2, c2) 1O4'=(a1, b2, c2)  a1 2+1=3 O2'  (2?3)+3+3+3=15 O3'  (2?3)+3+3+3=15  O4'  (1?3)+3+3+3=12  Array  b1a2  c1b1a1  c2b2a2  c2b2a1  1+2=3 2+1=3 1+2=3  1+2=3 2+1=3  a2 b1 b2 c1 c2  Fig. 3. An illustration of the mechanism SOD.

TABLE 1 Description of Datasets (before Data Pre-processing)  Dataset Observations Dimensionalities Area Missing Values BCWO 699 9 Life Yes  CE 1728 6 Business No CMC 1473 9 Life No LC 32 56 Life Yes NU 12960 8 Social No SH 267 22 Life No  Abbreviations: BCWO, Breast Cancer Wisconsin (Original); CE, Car Evaluation; CMC, Contraceptive Method Choice; LC, Lung Cancer; NU, Nursery; SH, SPECT Heart.



IV. EXPERIMENT  A. Dataset Description and Evaluation Procedure We conduct a series of experiments with the various real-  world datasets [13]. Table 1 shows the detailed description of datasets. For formalization, initially we go through the data pre-processing. We considered the missing values as a pattern instead of pruning the observations that have missing values. Our method SOD is unsupervised; in other words, we neither train a classifier nor score observations by using the class labels. Hence, we delete the dimensionality ?class label? from every dataset. In the dataset CMC: We select two classes ?no-use? and ?long-term?. After the data pre- processing, the number of observations is reduced from 1473 to 962 but the number of dimensionalities is the same as the original. In the dataset CE: We select two classes, i.e., ?very good? and ?unacceptable?. After the data pre-processing, the number of observations is reduced from 1728 to 1275 but the number of dimensionalities has not changed. In the dataset LC: We utilized two classes, i.e., ?type 2? and ?type 3? pathological lung cancers; the number of observations is      BCWO CE CMC   LC NU SH  Note that the line with red color denotes the ROC Curve of our method SOD, and the dotted line with blue color is the ROC Curve of the method UA.

Fig. 4. Comparison of Effectiveness (Accuracy).

TABLE 2 Comparison of Efficiency  Efficiency (in millisecond)  Dataset BCWO CE CMC LC NU SH  SOD  SOD- P1 2 4 4 1 12 2  SOD- P2 2 4 4 0 11 1  SOD- P3  1 1 1 0 5 0  Total  5 9 9 1 28 3  UA  UA-P1  25 7 15 7 14 7 UA-P2 602 1446 1193 6 23596 257 UA-P3 6263 10077 21999 69 162955 20775 UA-P4 3 2 2 1 3 1 Total 6873 11532 23209 83 186568 21040  Abbreviations: SOD-P1, Read Data into Memory and Build the Hash Table; SOD- P2, Compute the Score; SOD-P3, Save the Result in the File; UA-P1, Read Data into Array; UA-P2, Discover Related Patterns; UA-P3, Compute the Score; UA- P4, Save the Result in the File.

reduced from 32 to 23 but the number of dimensionalities has not changed. In the dataset NU: Two classes, including ?not recommend? and ?very recommend?, are adopted for testing. After the data pre-processing, the number of observations is reduced from 12960 to 4648 but the number of dimensionalities is the same as the original.

These experiments run on Windows 7 laptop equipped with a 1.6GHz Intel i7 Q720 and 4GB of RAM. We implement the methods UA and SOD by using C# programming language and Microsoft Visual Studio 2010.

We use the value of Area Under the Curve (AUC) to evaluate the accuracy. Note that the values of AUC is calculated by the function ?ROC Curve? in the software ?IBM SPSS Statistics 20? [15]. The score of every observation is input to the test variable, and the state variable is the class labels provided in each dataset. We specify all parameters by default. According to the study [16], the AUC value of 0.5 indicates no apparent accuracy, and the AUC value of 1 means perfect accuracy.

TABLE 3 The Value of AUC in SOD and UA  Accuracy Dataset  BCWO CE CMC LC NU SH SOD 0.982 1.000 0.623 0.754 1.000 0.838 UA 0.979 0.997 0.652 0.754 0.989 0.835  B. The Results of Analysis In terms of the efficiency and effectiveness, we compare  our method SOD with the method UA [7]. In all the datasets, Table 2 demonstrates that SOD is extremely efficient than the method UA. For example, in the large-scale NU, the     run-time of UA is around 186 seconds; by contrast, the run- time of our method SOD is just 0.028 seconds. Table 3 and Figure 4 show that SOD achieves perfect accuracy in the two cases (i.e., the datasets CE and NU). In addition, SOD is slightly accurate than UA in the majority of datasets.

KDD Cup 1999 Fig. 5. The Effectiveness of SOD in the large-scale case.

To test the scalability of SOD, we conduct experiments  with the extremely large-scale dataset KDD Cup 1999 (the file name: kddcup.data_10_percent.gz) [13]. The dataset KDD Cup 1999 is a record about network intrusions, and there are 494021 observations and 42 dimensionalities (note that each dimensionality has many attributes). In the data pre-processing, we delete the dimensionality (i.e., class label) and we select the observations that are satisfied with the two classes ?normal? and ?smurf?. Then, the number of observations is reduced from 494021 to 378068 and the number of dimensionalities is 41.

In the dataset KDD Cup 1999, our method SOD achieves perfect accuracy as shown in Figure 5. Regarding the efficiency, the total run time of SOD is around 4 seconds including: PART 1. Read Data into Memory and Build the Hash Table, 2.354 seconds; PART 2. Compute the score: 0.967 seconds; and PART 3. Save the Result in the File: 0.318 seconds. Regarding the usage of memory storage, SOD just costs around 4 megabytes including: 1 megabytes ( 518896 2  bits? ) for building the characteristics table and 3 megabytes ( 588473 2  bits? ) for storing the distinctive observations. In sum, these experiments suggest that SOD performs well even in the environment of large number of observations and high dimensionality. Against frequent itemsets mining, SOD provides significant evidence that the concept of relative patterns discovery is a pragmatic solution toward big data analytics.



V. CONCLUSION In this paper, we present the computational challenges of  big data analytics. For big data, the conventional association analysis is infeasible in extracting the distortionless information for patterns generation. From a new  perspective, we introduce the relative patterns discovery (RPD), which explores the behavior and relationship between observations. Based on the concept of RPD, we design a scalable outlier detection method (SOD).

According to the experiments with various real-world datasets, RPD is achievable toward big data analytics.

Specifically, our method possesses the following advantages.

1. Effectiveness: the RPD-based methods, UA [7] and  SOD, achieve remarkable accuracy. In addition, SOD is slightly accurate than UA in the majority of datasets.

2. Efficiency: In the worst case, the state-of-the-art methods of frequent itemsets mining run in exponential time. In contrast, RPD runs in quadratic time and SOD runs in linear time. Particularly, in the extremely large- scale dataset KDD Cup 1999 (i.e., 378068 observations and 41 dimensionalities), the total run time of SOD is around 4 seconds. It is satisfied with the requirement of big data analytics, immediate response.

3. Scalability: SOD is economical in terms of the usage of memory storage. In KDD Cup 1999, implementation of SOD just costs around 5 megabytes. The more important is that the characteristics table is in constant state. In other words, the size of characteristics table only increases linearly with the number of items. In the Wal- Mart case, even there are million transactions or more, SOD simply costs about 5 megabytes for the characteristics table.

On the other hand, we observe that the I/O operation is the main cost in SOD. For future work, the issue of I/O operation would be worthwhile to be enhanced by parallel distributed technology.

