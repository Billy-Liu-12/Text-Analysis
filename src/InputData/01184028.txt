On Incorporating Subjective Interestingness Into the Mining  Process

Abstract  Subjective inter~stingness is at the heart of the successful discovery of association rules. To deter- mine what is subjectively interesting, users? domain knowledge must be applied. [7] introduced an ap- proach that requires uery little domain knowledge and interaction to eliminate the majority uf the rdes  that are subjectively not interesting. In  this paper we investigate how this approach cun Ire in- corporated into the mining process, the Irenefits und disadvantages of doing so, and ezamine the results of its application to real databases.

1 Introduction  [4] defined Knowledge Discovery in Databases (KDD) as ?(. . .] the non-trivial process of identi- fying valid, novel, potentially useful, and ultimately understandable patterns in data.? Interesting- ness distinguishes the ?valid, novel, potentially use- ful, and ultimately understandable? patterns from those that are not. Since the problem of interest- ingness is ultimately subjective, subjective criteria that  explicitly employ users? domain knowledge are needed to  completely resolve this problem. Incor- porating subjective interestingness into the niining process has obvious advantages: less patterns will be mined, and those are more likely to  be inter- esting to  the users. However, a drawback of this tactic is that  the results may be tailored to match only the prespecified subjective criteria; exploring slightly different interestingness criteria could re- quire reexecuting the entire mining process.

To reduce the number of mined patterns to  make their post-processing easier, we need to  incorporate into the mining geneml subjective interestingness criteria that (1) are very easy to define, and, ( 2 ) apply to the interests of a wide audience base. In  this work we tackle this task by incorporating the method of [7] into the mining process to  create an algorithm that is simple, general and effective (see Section 3). We also examine the results and reper- cussions, both benefits and disadvantages, of the application of this method on the same databases used in 171. Note that as in 171 our goal is t o  reduce, rather than eliminate, the number of not-interesting rules mined.

Related are works such as [1 ,6,9,  51 that incorpo- rate subjective interestingness iuto the mining pro- cess; but rely on the availability of doniain experts to  express all the needed information in a predefined grammar. See (81 for a detailed review.

2 Definitions and Preliminaries  Let A be a set of attributes over the booleau domain, V he a set of transactions over A ,  and i- itemset denote an itemset of size i. For A :  B C 11; .4 n B = 0; the association rule A + B is defined [ 2 ]  to have support s% and confidence c% if 8% of 2) contain A U B: and c% of 2, that  contain .4 also contain B. We refer t o  A a3 the assumption of the rule and to  B as its consequent.  [2]k algorithm outputs all the association rules that have at  least predefined support and confidence thresholds. In this work we build on the AprioriTid algorithm [Z].

Let R be the list of association rules mined over A .  Let U :  b E A; and p = U + b. The family of p in R is defined 171 as: familyn(p) = { r  E Rjr = A -+ B, a E A,  b E B }  U { p } .  p is then defined as the ances tor  rule  of the family of p in R.

We now define the binuclear family of a 2- itemset {u,b} as: biFn({a:b}) Dsf furnilyn(u --f b) U famiryn(b+ a). {u ,b}  is referred to  as the an- cestor itemset and is said to  s p a n  biFn({a,b}).

A binuclear-family is also be defined to  be spanned by an ancestor rule: biFn(p) = hFn({a, b ) ) .  Note that biFn(u + b) = biFn(b + a).

Def  Dsf  0-7695-1754402 $17.00 Q 2002 IEEE 681    3 The Algorithm (1) T = ancestor itemsets classified for deletion; (2) Zr =new-attributes(T,Z); (3) Li = large 1-itemsets over VI; CI =database%; (4) C2 = apriori-gen(l1);  ( 6 )  delete-subjectively-not-interestin~(C~);  I\Z = 1\ U l r ;  - ^ ^  , (5) detennine-support(C*,c2,C1);  The goals of our algorithm are: (1) Simplicity: keep the process simple by asking users only a few, easy classification questions.

(2) Generality: the results of the mining process with the integrated subjective interestingness will be general enough to be applicable to a wide user base for further interestingness post-processing.

(3) Effectiveness: integrating this type of sub- jective interestingness into the mining process will result in mining of much fewer association rules?.

[q uses domain knowledge of the kind ?not  interested in Jamilyn(p)? for interestingness post- processing. The families of a + b and b + U cannot be distinguished during the mining process, when only itemsets are recognized. To integrate user pref- erences of the type used in [7] into the mining, we have to use generalized preferences of the type ?no t interested in biFn(p)?. We can decrease the num- her of mined patterns by eliminating one or more of the frequent itemsets discovered during the min- ing. T h e  challenge in deleting a frequent iternset I = {u , ;b , }  is in ensuring that even though I is removed; rules of the type:  .4 + D and D + A : a;, bi E .4 and a;, bi D (1) that would have been mined had I not been deleted, will still be mined. An example for a rule in Equation 1 is (tomatoes,cucumbers) + (sugar) if I,t ={cucumbers ,tomatoes} is deleted.

3.1 Selecting & Classifying Ancestor Itemsets  We only ask users a one-dimensioned classi- fication question: is the ancestor iternset inter- esting or not-interesting to the user: that is; is the binuclear-family spanned by the ancestor i t em set interesting/not-interesting. If a user classi- fies an ancestor itemset as not-interesting, it is marked for elimination (used in Section 3.2), 0th- erwise: no action is taken on that ancestor iteni- set. We select ancestor itemsets for user classi- fication to  be the 1-3 inost frequent 2-itemsets? in order of their frequency. These limited selec- tion and classification processes naturally guaran-  ?For example, integrating the first classification resulted, on average, io the mining of fever than 80% of the rules that would have been mined otherwise (see Section 4 for details).

tLarger iiernsets, itemsets with more than two attributes, are considered loo complicated for the naive clas~ification that we preserve: while the biFn({o, b } )  = fomrivn(o --t b l  U fomdyn(b +a], the biFomily of {a ,b ,c}  would be the union of the ?families? of a + bc, o b  t e ,  b + oc, bc --t a, ac + b , c 7- ob, and possibly aim of a --t 0 ,  (I 4 c, b i c, b -t a, e -t a, and e + b ,  with an even larger list far larger ancestor itemsets.

Figure 1. The Algorithm, Part I  tee our three goals: classifying an ancestor item- set is equivalent to classifying two ancestor rules as interesting/not-interesting. biFn({u, b}) = U is easy to classify because it consists simply of fumilyn(a + b) U familyn(b + a): and if U, b E X then X + Y, Y -$ XgU. The other reason the clas- sification is simple is that users only need to  clas- sify the 1-3 most frequent 2-itemsets. These item- sets are very likely to describe relationships known even to naive users, making them easy to classify.

Since these rnost frequent P-itemsets describe re- lationships that naive users are probably familiar with, a wide audience base is likely to classify them similarly, yielding generality. Finally, since the top three most frequent 2-itemsets are likely to have very large binuclear-families, likely to  be classified as not-interesting (known), the limit on the nuniber of ancestor itemsets also yields effectivenesst.

3.2 The Mining Process  The domain knowledge available to  us is a list of pairs ( I ;  c) where I is an ancestor iternset and c is its classification: interesting/not-interesting. For I classified as not-interesting we cannot simply limit the search space of large itemsets by deleting the iteniset {a,b} (see Equation l), but we can alter the search space. Figure 1 outlines the first part of our algorithm. In line (1) we initialize I? to be the set of ancestor iternsets classified by the user for deletion. Line (2) is described in Section 3.2.1, lines (3)-(5) in Section 3.2.2, and line (G) in Section 3.2.3. The secoxid part of the algorithm, consisting of iterations for k 2 3 and the rule generation, is performed as in 121. We use the notations L t :  Ck and ek defined in 121.

3.2.1 Creat ing N e w  Attr ibutes  For any iternset X = {Q,. . . :xn}, let v x  be a new attribute where the value of vx on any transac-  %In [?I] prior to each classification, users can rely on the R S C L ,  defined in [ i ]. as an indicator of the potential impact of  the next CLaSsifiCation they make. In our case, when interest- ingness i s  incorporated in one step, during the mining process, this kind of indication is not possible.

tion, T ,  will be TRUE if and only if the value of every xi E x is TRUE in 7.  For each element in Y = { { u s : 6 i ] ] ,  where u i ,  b, E A: we create a new attribute v y  defined for Y = {a,, 6 , )  and for n = 3 fur intersections as in YZ below. We now define AI D=" A UZr, where Zr = { u v } ~ ,  and 'DI to be the extension of 2) spanned by Ax.

We examine the creation of new attributes for two examples: for YI = { { ~ : 6 } ~ { c > d } ] ~  I r ,  = { c { a , b } :  V { e , d } ] .  For YZ = {{a:b}; {U:.]] three new attributes are created: I r ,  = { ya,+)> " { a , b , e } } .  The need for the third attribute will become clear in Section 3.2.3.

3.2.2 S tar t ing  t h e  Modified Mining Process  In line (3) of Figure 1 we initialiae L1: the set of large I-itemsets: from A I ;  the set of candidate large Litemsets. Since the ancestor iternsets in Y are the most frequent 2-itemsets, alniost all mining pro- cesses will discover all of IT as-large I-itemsets.

During tlie same pass on DE, C1 is constructed.

apriuri-gen on line (4) and the eliniination of the candidate large 2-iteinsets that do not have at  least the predefined support threshold on line (5) are ex- ecuted as in [2], but over 271 and A I .

3.2.3 Deleting Ancestor Iternsets  For TI = { ~ , , b ] :  where there is only one ancestor itemset to be deleted, = {U: b}: we delete the fol- lowing itemsets: (1) {u :b} ;  the itemset classified as not iuteresting, (2) {U: .ab}: since it is equivalent to { u : u ; 6 }  = e: and, (3) {6 ,21 .b} .  Note that if e is a large iteinset, all three of the above 2-itemsets are large, and will actually be deleted from Cz. This observation holds for all the iternsets below as well.

For YZ = {el el}, where el = {U: 6 ) ;  e2 = {c, d } , one of two cases will occur: Case 1: { u , b }  n {c :d }  = 0 then Yz = { v o b : u C d ] : and wedelete: (1 )  { u : b ) ,  (2) { U : v . b ] ,  (3) { b : w n b } : (4) { c : d } ;  ( 5 )  { c : v c d } ,  and (6) {d ;ued] ,  using the same reasoning as for Yl .

Case 2: 01 = { u : b ] ;  e 2  = { u ; d ) :  (U # 6 # d: U = c): theu IT = { ' U a b ; U , d : V , b d } :  and we delete the 14 2-itenISetS: (1-6) { U ; b } ,  {U,d}, { U ; V , b ) ;  { b ; v a b ) : { U > ' G a d ] :  { d : % d ] ;  (7-14) { U ; V a b d ) r  { b : U o b d ] : { d : ' G a b d ] :  { 'Uab: 'Uabd};  { u a d ; v a b d }  { d : V o b ) ;  ( 6 : t ~ a d )  { U . b , ' t i o d ] ;  where itenisets (7-14) are equivalent to %bd. Note that deletions (7-14) are not syninietric as 'ubd 6 YZ. We examine this case in Y3 below.

Case 2 clarifies the need for Dabd.  Without it: rules such as U, b: d -+ e will not be inined when users classify {U: 6 )  and {U: d }  as not-interesting.

Note that we do not need to  eliminate any item- sets in the third iteration, when the 3-itemsets are constructed. Itemsets such as {U, z , ~ . b ]  that we would want deleted are automatically deleted dur- ing the pruning stage of candidate itemsets since at least one of its 2-iternsets; for example, {U, vab}: is not a member of CZ as it has been deleted in one of the 14 deletions above.

Note that for T3 = { ( a ~ b } ~ { u , d ) , { b , d ] ) , Z ~  = { v G b >  % d ,  tJabdr U b d } ;  and 21 Z-iteniset deletions are required: the 14 deletions of Case 2; and, (15) { b ; d } ,  (16-17) { b , W b d } ,  { d : V b d ]  (equivalent t U u b d ) r  a i d  (18-21) { a ; . b d } ;  { '%b;z)bd) ,  { v a d l v b d ) and { tJabd:  W a d }  whicli are equivalent to ?Jabd.

4 Experiments With Real Data  We ran our algorithm on the same DBs used in [7] (1) The Grocery DB describes 67,470 shoppiug baskets, using 1,757 attributes: of an online Israeli grocery store (sparsest DB) (2) The WWW DB describes the accesses of tlie 2;336 heaviest users to 15 site categories (densest DB). (3) The Adult DB is based on tlie 45,222 entries with no missing values from the .4dult dataset [3], discretisized as in [7] into 171 boolean attributes. We mined each DB with zero, one, two and three ancestor itemsets classified for deletion. Nut surprisingly, the ancestor itenisets corresponded to the ancestor rules used in [7], allowing us to readily compare the results.

4.1 Reduction in Number of Rules Mined  Figure 2 depicts the re- sults of mining the U'WU'; the Grocery and the Adult DBs with 0-3 ancestor iteni- sets classified for deletion,  o > s ,  D I I I  -mz&---mm- and two mining thresholds each. Each histogram depicts the nuinher of rules mined with a specific threshold, and each bar represents the num- ber of rules mined when & 3 ancestor itenisets were used in the mining.

The reduction in tlie nun- ber of rules mined as result of eliminating the first few an- cestor itenisets is more sub-  -,.-A - stantial than that of elinii- ~ ~~. ~~~  Figure 2. nating latter ancestor item- sets (an average of more than 23% due to the dele- tion of the first ancestor itemset; just under 18%     and 12% for the second and third ancestor i t em sets). Tliis is parallel to the elimination of larger number of rules by the first few ancestor rules in [7] as compared to the elimination of latter ancestor rules. The explanation to both events is the same, captured by RSCL (71: and is the reason we limit the number of ancestor itemsets to a niaxiinuui of 3.

Overall, the reduction in the number of mined rules due to  the deletion of the three ancestor iteinsets was approximately 47% for the WWW DB: 36% and 40% for 3.5% and 6% mining thresholds for the Grocery DB: and 45% and 47% for the Adult DB.

4.2 Resource Consumption  1. Mining is normally dominated by the time it takes to calculate the support of the candidate iteni- set5and Figure 3 depicts the size of C k  for 0-3 ancestor itemsets deleted for the Adult DB. .4ft_er the fourth iteration the size of Ck  decreases more rapidly when more ancestor iternsets are deleted, requiring less iterations to coniplete the mining when more ancestor itemsets are used. This behavior was manifested after the secorid iteration in the WWW DB. See Section 5 for conclusions.

Cutnulative iteration run- .- -w,=- times in seconds on a Conipaq  ProLiant DL580 (4 700MHz Xeon CPUs) with 1GB R.4M: running Solaris 7 and Per1 5.005-03, are depicted in Figure  _n 4. Note the small nuniber of iterations for the sparse Grocery DB, compared to the large number of iterations for the dense WWW DB.

Note that the relatively short mining times for the WWW DB combined with the large nuni- ber of mined rules from this dense DB resulted in better ouernll runtimes when incorpo-  I rating the first ancestor item- set into the mining. Tlie mod- ified AprioriTid runtime with  3.5% thresholds was 115 seconds with 0 or 1 ancestor itemsets. The total runtime, including ap-genrules, was 140secs when no ancestor itein- sets were used; and 130secs wheii one ancestor iteni- set was used (with 29910 vs. 22932 rules). When mining with 6% thresholds AprioriTid took 51 sec- onds with 0 or 1 ancestor itemsets, 65 seconds total  I: i:  Figure 3'  I i  !

f   _un . .  5  d __,__  I--  Figure 4.

for 0 ancestor itemsets and 54 seconds total with 1 ancestor iteinset (with 9206 vs. 6972 rules mined).

5 Conclusions and Future Work  To fully address the problem of interestingness, subjective interestingness criteria must be used. In this work we investigated liow to incorporate [7] into a simple, general arid effective rnining process.

The reduction in the number of rules outputted by the altered mining process is similar to the post- processing elimination in 171. Tlie domain knowl- edge we incorporate can only be used to alter the search space by flattening it: resulting in niore item- sets discovered in the first few iterations; making those iterations' execution time longer. The execu- tion time of later iterations is reduced, and the last one or two iterations are often eliminated. How- ever: in general, the execution time of this new rnin- ing process, that outputs significantly fewer rules, is longer than that of mining the entire set of associa- tion rules. Our conclusion is that  there is not always a benefit to incorporating subjective interestingness into the mining process. The output of the new niiri- ing process is significantly reduced, almost by a half, inaking post-processing easier, but post-processing can often achieve similar results with shorter run- time. Future work in this area includes investigating and formalizing types of doniain knowledge that can be used to decrease the size of the search space.

