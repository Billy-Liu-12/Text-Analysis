

Hebbian Learning and Self-Association in Nonlinear Neural Networks  Francesco Palmieri, Member IEEE  Abatraci- A self-organizing feature map, based on a Hebbian paradigm, is proposed as a universal adaptive memory. The learn- ing paradigm, can be applied to arbitrary network topologies containing the standard sigmoidal nonlinearities at their nodes. The system generalizes the linear principal com- ponents by mapping the input space into a set of orthogonal nonlinear projections. Only localized learning rules are necessary for the adaptation. The size of the system is related to the desired accuracy and to the density of the examples.



I. INTRODUCTION A self-organizing neural network has to provide a useful alternate, or compressed, representation of the input space (feature map). Previous contribu- tions have shown how in artificial neural networks, simple learning rules, localized at the synaptic level, can lead to interesting patterns of organization (Lins ker, 1986). These structures bear strong resem- blance to patterns of activity observed in biological brains.

The emphasis of this work is on the search for suitable principles of self-organization, that applied to loosely constrained architectures, can lead to the emergence of representations useful for the appli- cations. For example, in the linear framework, the principal components provide compression for sources exhibiting marked linear correlations. Our goal is to provide an analysis framework, solidly rooted in the signal processing and system theory, that gener- alizes the well-studied adaptive linear filter theory (Haykin, 1986).

We came to focus our attention in our previous papers (Palmieri, Zhu and Chang, 1993) (Palmieri and Zhu, 1993), on two main criteria for the Heb- bian (excitatory) and the anti-Hebbian (inhibitory) synapses respectively. In particular, an Hebbian  The author is with the Dipartimento di Elettronica, Univer- rita? degli Studi di Napoli ?Federico 11,? via Claudio 21, 80125 NAPOLI (Italy) (Email: pdmieriOnadis.dis.unina.it), and with the Department of Electrical and System8 Eng., The University of Connecticut, S t o m ,  CT 06269-3157 (USA).

synapse attempts to excite the node to which it is attached in a way that guarantees the preservation of the source information. This is achieved by re- quiring the input to be maximally reconstructible from the node value (Principle of Maximum Inverse Linear Fieconstruction). An anti-Hebbian synapse instead, inhibits the node to which it is attached, by removing the correlation between its input and the node value.

These criteria are analyzed in detail for the lin- ear and the nonlinear neural networks, made up of nodes containing the familiar sigmoidal nonlineari- ties (Palmieri, 1993).

The principle of self-association in linear networks has been proposed by Bourland and Kamp (1988), Baldi and Kornik (1989), Hrycej (1990), Xu (1993), Karhunen and Joutsensalo (1993), that did not con- sider the hybrid architecture containing both Heb bian and anti-Hebbian synapses. Also the constraints on the locality of the learning rules was not al- ways imposed. Hybrid structures for the computa- tion of the principal components have been proposed by Foldiak (1988) and by Kung and Diamantaras (1990), that first postulated learning equations, and then showed how the principal components emerge from the self-organization.

We start from the objective functions, and study the learning behavior of a linear and a nonlinear neural network that contains an arbitrary mixture of Hebbian and anti-Hebbian synapses. The nonlinear projections that emerge from the self-organization provide a potentially universal multidimensional r e p resentation of the input space. The remarkable fea- ture of this paradigm is in the full locality of the learning algorithms. Even though the cost function (backward reconstruction) is formulated in global terms, the orthogonality that emerges from the anti- Hebbian connections only need Hebbian learning equa- tions completely localized to the synaptic level.

11. THE LINEAR NEURAL NODE  A .  The Anti-Hebbian Synapse Figure l(a) shows a linear neural node with an anti- Hebbian input 2, and another generic input z .  The  0-7803-1901-2U94 $4.00 01994 IEEE 1258    X.

Figure 1: The neural node  output value is y = coza + a where both zq and z are two arbitrary random variables. As already dis- cussed in (Palmieri, Zhu and Chang, 1993), the anti- Hebbian synapse "inhibits" the neuron by removing the correlation between xa  and y. The unique 80- lution to E[z,y] = 0, is clearly c, = -w. Note that the linear node with the anti-Hebbian synapse can be seen as a linear adaptive filter of order one, with the output y playing the role of the error in estimating --z linearly from 2,. In fact, the same solution can be found by minimizing E[g2] .  The classical adaptive linear filter theory (Haykin, 1986) is based on this formulation with the orthogonality pn'ncipIe ensuring that the input t o  and the error y are orthogonal in optimal conditions. We prefer us- ing the orthogonality condition as the starting point because, as we will see in the following, it becomes immediately applicable to the nonlinear scenario.

B.  The Hebbian Synapse Figure l(b) shows the same linear neural node of Figure 1(a) with one Hebbian input t . The Heb- bian synapse "excites" the neuron by forcing the neural node value to be "as related as possible" to z. We identified in our previous papers (Palmieri and Zhu, 1993; Palmieri, 1993) the following strat- egy for the Hebbbian synapse: learn the value c that corresponds to the best linear backward reconstruc- tion of t. That is, find the value of c that minimizes  ? = E[(x - cy)2].

? =' E[2]C' + PE[tr]c3 + ( E [ 4  (1) The c a t  function can be rewritten as  -2E[z2])c2 - ~E[zz]c + E[t2]. (2) By computing its derivatives, it can be easily seen that for arbitrary z ,  & it is not a purely convex func-  tion, but it may exhibit one global and one local minimum (Palmieri and Zhu, 1993). Typical cases arise when both inputs are orthogonal, E[zz] = 0, in which case there are two symmetric equivalent global minima, and when a = 0 which gives the two equivalent solutions c = fl.

C. The Hybrid Node A hybrid node is shown in Figure l(c) where both excitatory and inhibitory synapses are present. The node value is y = Cats + cz with the anti-Hebbian synapse trying to inhibit the node, and the Hebbian synapse trying to excite it. Since the anti-Hebbian synapse has a stationary point given by E[z,y] = 0 with c, = - c w ,  by substituting this expression in &, we have  & = (E[x2]  - w ) ( c 4  - c2) + E[z2] .  (3) E[zal Since E[+z,I2 <_ E[z$3[t2], & has a maximum at c = 0 and two equivalent global minima at c = fl.

The twostationary points are (c, c,) = f(1, -=).

A typical special case arise when x and 2, are orthogonal, E[z,t] = 0. The anti-Hebbian synapse does nor arise and the Hebbian synapse just copies the input to the output. Also interesting is the case with t a  = kz, with k being any real constant. The anti-Hebbian synapse becomes c, = -kc and g = 0. The anti-Hebbian synapse shuts off the neuron completely, with the Hebbim synapse left undefined (floating). The inverse reconstruction of t is ill- defined with ? = E [ t 2 ] ,  V c.

The above analysis can be easily generalized to an hybrid node accepting M anti-Hebbian inputs  ~ 0 2 ,  ..., CaM)T = ca, and N Hebbian inputs ( X I ,  2 2 ,  ..., Z N ) ~  = x through N synapses (c1 , c2, ..., C N ) ~  = c as shown in Figure l(d). The anti-Hebbian synapses guarantee orthogonality with E[zaj~] = 0, V i = 1, ..., M, with values given by  ( t a l ,  t a 2 ,  ..., taM)T = Xa through M synapses (col,  CO = - E [ x ~ x ~ ] - ' E [ x ~ x ~ ] c .  (4) The principle of linear backward reconstruction of x requires the simultaneous minimization of the cost function  which becomes using (4) E = E[llx - CY1i21, ( 5 )  8 = E[xTx] - 2cTR' + cTccTR', (6) with  and R = E[xxT] and pa = E[xxz]. We have al- ready proven (Palmieri and Zhu, 1993), and it is  R' = R - p , R , ' p z ,  (7)     Figure 2: The architecture for orthogonalization  easy to verify it by direct substitution, that equa- tion (6) has critical points at the eigenvectors of R'. The only two minima are at plus-or-minus the principal eigenvector. All the other ones are saddle points.

It is useful to examine a special cases that will be important in the study of the architecture of the next section. If the anti-Hebbian inputs are propor- tional to the projections of x on a set of M eigen- vectors of R (M < N ) ,  as za1  k l q Y 1 x ; ~ a 2  = k*qT,x; ...; z a ~  = kMqT,,x, with kl, k2, ..., kM,  ar- bitrary constants, we have that from (7) c will con- verge to plus-or-minus the principal eigenvector of the set ( q i , q 2 , . . . , ~ )  - { q i l , q i a , . . . , Q i N )  A more comprehensive analysis of the hybrid neural node would include a nonlearning input a as in Figures l(a) and l(b). This case may be of interest if we want to assume that each node in the network has an independent noise source, or accepts nonlearning inputs, or inputs that learn at a much lower speed to be considered stationary. We leave the more gen- eral analysis out for brevity. It will be discussed somewhere else.

D. A Linear Architectures The analysis of the general hybrid node allows us to establish a number of results for arbitrary ar- chitectures made up of linear neurons and mixtures of Hebbian and anti-Hebbian synapses. For exam- ple, the architecture of Figure 2 certainly gives the principal components of the input. The first out- put is the projection on the principal component.

The second one, using (6) and (7) is the projection on the second one and so on. Note that the crite- rion of maximum inverse reconstruction is applied in a decoupled fashion. In other words, each neu- ron maximally reconstruct its inputs from its out- put independently from the others. The solution is the same as that obtained from the more general cost function E = E[IIx - xbllq = E[llx- CCxl12, which would require coupling of the various outputs for xb (Palmieri and Zhu, 1993(b)). The equiva- lence is obvious if we observe that at equilibrium  the outputs are orthogonal and the minimization of E is the Bame as the independent minimization of ?i = E[IIx- qyill'], V i = 1, ..., M  111. THE NONLINEAR NODE The great interest we attribute to the paradigm studied above for the linear scenario, is in the fact that it can be immediately applied to neural nodes that have the common nonlinear structure with the sigmoidal nonlinearity. The sigmoid can be any strictly increasing function such that +a(O) = 0, limh,, &(h) = IC, ,  and lim,-m 4p(h) = kl with k, and kl being any two real constants. Even though most of results hold for these functions, in our calcu- lations and simulations we adopt the following func- tion:  ( 8 ) 1 ah  4a(h) = 2tgh-2-l  where the parameter Q is introduced to control the slope of the transition around aero. In fact for a -+ 00, da(h) -+ asgn(h), and for small a, &(h) N Zh.

According to the magnitude of the values at its input the sigmoid can behave either as a binary threshold, or as a linear unit, conveniently generalizing the lin- ear case. The derivative is easily computed to be 4h(h) = Q(f - 4%)).

A. The Anti-Hebbian Synapse  Let us begin by considering the simple architecture of Figure l(a) with only one anti-Hebbian input and an extra input z. The output is y = d(cat + z).

Just as in the linear case, the objective of the anti- Hebbian synapse is that of removing the correlation E[tay]. The following theorem guarantees that this is possible under the most general conditions.

Theorem: For any mndom variables xa and z ,  and any sigmoidal function 4(h) with b(0) = 0, there exist a unique value of c,, that satisfies the equation:  Proof: Since  V C O ,  the function E[z,y] is strictly increasing in Ca. Therefore if a solution to the equation exists, it is unique. But ca can take arbitrarily large values to bring 4a(caza + z )  to saturation. This means that there are values of ca at which the quantity E[Za#a(Caza + z) ]  is positive and values a t  which it is negative, which implies that there exist a value of Ca where E[Zada(Cata + z)]=O.

Figure 3: A generic  B. The Hebbian Synapse  hybrid architecture  Refer to Figure l(b). The cost function for the H e b bian synapse on the nonlinear node becomes  & = E [ ( z  - c$b(cz + 4 2 1 .  (11) The results on the linear case suggest that for arbi- trary distribution of z and z the function & is not necessarily a convex function of c. However, we veri- fied for special cases, that the unicity of the solution (two equivalent solutions) remains in a mixed node, just like in its linear counterpart.

C. The Hybrid Node For the mixed architecture of Figure l(c), even though we cannot write down an explicit expression for the value of co, we know from the above theorem that for any value of c a solution to the equation  E[zada(Caza + CZ)] = 0, (12) exists and it is unique. The cost function for the Hebbian synapse becomes & = E[( .  - c#(coio + CZ))~] ,  where to co we should substitute the solution to (12). We have not yet been able to study rigor- ously the convexity of & as in the linear case, but we conjecture from simulations that a result simi- lar to that of the linear case, should hold with one maximum and two symmetric minima.

The same criteria can be applied to the more general node of Figure l(d). The output is = I$(c~x,+c*x). For any c, the anti-Hebbian synapse will have a unique weight configuration given by the solution to the set of equations  E[Xa9(C;fXo + cTx)] = 0, (13) and the cost function for the Hebbian part will be  D. The Nonlinear Neural Network An arbitrarily complex architecture with both Heb- bian and anti-Hebbian synapses can be studied with  the tools provided above. To fix the ideas, let us look a the network of Figure 3. The node values {ii} (after the sigmoids) are the result of arbitrary mixtures of excitatory and inhibitory inputs. From the above diecueeion, we know that if two nodes lsre connected though an anti-Hebbian synapse, at equi- librium their node values will be orthogonal. Simul- taneously, for the Hebbian synapses the criterion of inverse reconstruction requires minimization of  &i = E [ ( z ~  - ~ b i ) ~ ]  = E [ ( z ~  - c j i ~ j ) ~ ] ,  (15) jEBi  applied to all the nodes in the network, where Bi is the set of nodes excited by the ith node through Hebbian synapses.

This criterion does not appear to be properly 10- cal because it requires the computation of 16 which is the backpmpugation of the values zj E rti through the Hebbian connections. However, if every pair of zj E S i  is orthogonal, the minimization of & is equivalent to the separate simultaneous minimisa- tion of  &jj = E [ ( z ~  - c j i ~ j ) ~ ] ,  V j E B. (16) Hence if each pair of nodes in Bi is connected through an anti-Hebbian synapse, the cost function is com- pletely local at the synaptic level!

A neural architecture composed by arbitrarily in- terconnected nodes leads to the emergence of a r e p resentation of the inputs that guarantees the exis- tence of a linear transformation for ?retrieval? from the network. This is the meaning we attribute to this system as an emerging physical memory.

Note that in a multi-layered feedforward topology, a perfect backward reconstruction at each node, im- plies a perfect reconstruction at the inputs.

The network architecture that follows, already in- troduced in (Palmieri, 1993), demonstrates how to obtain a set of nonlinear projections of the input space, that go from purely linear principal compo- nents, to binary networks.

E. An Architecture for Nonlinear Principal  The architecture shown in Figure 2 implements a generalization of the linear principal components.

If the sigmoids at each node are used in their lin- ear region (small net variance at the input of each node), the network essentially behaves as a linear fil- ter with the linear principal components emerging as the result of the self-organization. If instead the input energy is increased, or the sigmoidal functions are made increasingly sharp, the network begins to provide a progressively finer breakdown of the input space into orthogonal nonlinear projection.

Components     The following simulation performed with the ar- chitecture of Figure 2, on a one-dimensional input should be sufficient to demonstrate the potential of this idea.

Figures 4 and 5 show the resulting mappings pro- vided by the network of Figure 2 with N = 1, after self-organization with input examples uniformly dis- tributed in the interval [-0.5,0.5], The learning rule for the anti-Hebbian synapsea is the simple one:  Acaij = -pyiyj , j = 1, ..., M, i = j + 1, ..., M, (17)  where c,jj is the anti-Hebbian connection from the output j to the output i .  A number of variations to the rule would work as well with slightly different convergence properties. The Hebbian synapses are trained using to the rule  Acjj = p(xj - zbj)yi, j = 1, ..., N ;  i = 1, ..., M, (18)  where X b j  = cjjyi, is the output y backprop agated to the j t h  input. As we pointed out above, even though all the simulations that we report are performed using this rule, a decoupled version of it  1, ..., M, which is completely local at the synaptic level, would also work for the orthogonality of the outputs. Oja's rule would just exhibit worse missad- justment and generally slower convergence. A de- tailed analysis of the convergence behavior of these algorithms is beyond the purpose of this paper and it will be reported elsewhere.

Figure 4 shows the resulting orthogonal projec- tions {yl(t), ..., yd( t ) )  for M = 4, a = 100 and 1000 examples uniformly distributed in [0.5,0.5], presented 100 times with p = 0.003. Note how the sigmoid has still a linear region that is used by the various stages in the backward reconstruction of 2 .  The last graph in the figure shows the almost perfect reconstruc- tion on the whole range. Note that only 3 orthogo- nal projections, with an oscillatory behavior, emerge and are sufficient for an almost perfect reconstruc- tion of t. Figure 5 shows the results of the same simulation with M = 6, Q = lo7 and 1000 examples presented 300 times with p = 0.00003. The sigmoid is practically a step function and the orthogonal pro- jections that emerge show the oscillatory behavior necessary to reconstruct the input. The last graph shows the reconstruction of the input. It is impor- tant to note that the accuracy of the solution is due mainly to the density of the examples in the input space. The aelf-organizing network zooms-in the de- tails of the input space up to the resolution of the examples presented.

To demonstrate this effect more clearly we have performed two simulations with only 5 examples,  (Oja's rule): Acij = /.i(xjy~-cijy~)l j = 1, ..., N ;  i =  whose results are shown in Figure 6. The input space is now very sparse and the network converges to a configuration that guarantees almost perfect reconstruction of those points leaving the other r e gions filled in by the interpolation provided by the final network configuration. Figure S(a) is the result of 5 examples presented 20000 times to a network with A4 = 4, Q = 100 and p = 0.003. The result- ing functions, are orthogonal on the example set and are obviously less constrained providing good recon- struction only on the input points. The results of same simulation are repeated in Figure 5(b) for a very sharp sigmoid having Q = lo7.

More simulations have been performed on multi- dimensional examples confirming what we see from the one dimensional case. The orthogonal projec- tions that emerge guarantee an almost perfect re- construction at the sample points. More projections can be obtained at each stage by increasing the " degree of nonlinearity" of the sigmoids.



IV. CONCLUSIONS  The framework provided by this paper provides a solid approach to the design and implementation of a self-organizing feature map. The network struc- ture, based on standard linear combiners and sig- moidal functions, derives its learning algorithms from a clear definition of the cost functions, and uses only localized learning rules of the Hebbian type.

The network spans the whole spectrum ranging from linear principal components to nonlinear or- thogonal projections. The slope of the sigmoidal functions, or the net energy of the data at each node, determines the degree of nonlinearity. By increasing the size of the network, the feature map "zooms in" the details of the input space up to density of the examples.

The feature map can be used as a pre-processing stage followed by a linear filter (Palmieri, 1993), for any adaptive nonlinear mapping and provides a promising alternative to backpropagation networks.



V. REFERENCES Raldi P. and K. Kornik, (1989), "Neural Networks and Princi- pal Component Analysis: Learning from Examples Without Local Minima," Neural Neiworks, Vol. 2 ,  pp. 53-58.

Bourland H. and Y. Kamp (1988), " Auto-Association by Multi-Layer Perceptronsand Singular Value Decomposition," Biological Cybemctics, Vol. 59, pp. 291-294.

Foldiak P. (1988), "Adaptive Network for Optimal Linear Feature Extraction," Neural Networks, Vol. 2, pp. 459473.

Haykin S. (1986), Adaptive Filier Theory, Pnntice Hall.

Hrycej T. (1990), "Self-Organization by Delta Rule," Pro- ceeding# of the Intemaiional Joint Conference on Neural Neiworks, San Diego, CA, pp. 307-312.

J. Karhunen and J. Joutsensalo (1993), "Nonlinear General- ieation of Principal Component Learning Algorithms," Pmc.

I 1  -0.5 0 0.5  Figure 4: Resulting mappings for M = 4, Q = 100 and 1000 examples  of the International Joint Conference on Neural Networka, Nagoya, Japan, October.

Kung S. Y. and K. I. Dimantaras (1990), "A Neural Net- work Learning Algorithm for Adaptive Principal Component Extraction (APEX)," Proceedings o j  International Confer- ence on Acourtics Speech and Signal Processing, Vol. 2 ,  pp.

Lwker R. (l986), "!&om Basic Network Principles to Neural Architectm: Emergence of Spatial-Opponent Cells," Proc.

National Acodemi of Science, USA, Neurobiologq, Vol. 83, pp. 7508-7512.

Palmieri F., J. Zhu and C. Chang (1993), "Anti-Hebbian Learning in Topologically Constrained Linear Networks: A 'htorial," IEEE Trans. on Neural Networks, Vol. 4, N. 5, Sept.

Palmieri F., J. Zhu (1993a), "The Behavior of a Single Self- Associative Neuron," Proc. of the World Congress on Neural Networkr, Portland, OR, July.

Palmieri F., J. Zhu (1993b), "Hebbian Learning in Linear Networks: A Review," Tech. Rep. 5-93, Dept of Electrid and Systems Eng., The University of Connecticut, Storm, CT 062693157.

Palmid F. (1993), "Linear Self-Association for Universal Memory and Approximation," Proc. of the World Congrerr on Neural Networkr, Portland, OR, July.

Xu L. (1993). "Least Mean Square Error ReconstructionPrin- ciple for Self-organization," Neural Networks, Vol. 6.

861-864.

1 .  I 1.5 0 0.5  Figure 5:  Resulting mappings for M = 4, Q = lo7 and 1000 examples  I  . . . . . . .

