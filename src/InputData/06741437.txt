Distributed Online Similarity Search in High  Dimensional Space

Abstract?In this paper, we consider distributed on-line similarity search for big data in high dimensional spaces, for which Locality Sensitive Hashing (LSH) was the method of choice. But LSH scheme needs a rather large number of hash tables and optimal parameters. So, it is difficult for LSH to deal with big data in one machine.

To reduce the size of big data, we divide the dataset into well separated clusters with bounded aspect ratios, locating them in different peers in ring network, using random projection tree(RP-tree). To limit the number of network accesses, we put similar subgroups adjacent to each other. Then, we construct one LSH hash table for each subgroup using optimal parameters. It is shown by comprehensive performance evaluations using real world data that our approach decreases the network cost and brings major performance improvement, while maintaining a good load balance between different machines.

Keywords?distributed; similarrity search; high dimensional; locality sensitive search

I. INTRODUCTION The rapid growth of online information, triggered by the  popularity of the Internet and the huge amounts of user- generated content,  calls for efficient management of this data to improve usability and enable efficient and accurate access to the big data. Furthermore, the underlying applications use feature-rich data to avoid loss of information, ranging from simple text snippets to (semi-) structured documents and multimedia content, which are typically represented as high- dimensional feature vectors.

Most current exact approaches for similarity search are unable to satisfy the runtime requirements for high- dimensional datasets, for that ?concentration effect? tends to appear in high-dimensional datasets, i.e. the distances between the nearest and to the farthest neighbors become indiscernible when data dimensionality increases[1]. For example, tree-based  methods such as cover-tree[2], SR-tree[3] can compute accurate results, but are not time efficient for high dimensional data. This problem is well studied in literature, and is regarded as a challenging problem due to its intrinsic complexity and the quality or accuracy issues that arise in terms of computing the appropriate nearest neighbors[4, 5, 6].

Approximate nearest neighbor algorithms tend to compute neighbors that are close enough to the queries instead of the exact k-nearest neighbors, and have a lower runtime and memory overhead than the exact algorithms[7]. For high- dimensional k-nearest neighbor search, one of the widely used approximate methods is locality-sensitive hashing (LSH), which uses a family of hash functions to group or collect nearby items into the same bucket with a high probability[8].

But, for the reason that every hash table has to store all the dataset, it requires significant memory space, usually hundreds of hashing tables to produce good approximation. It is difficult to store all the hash tables stand-alone when facing a large data set. Its search quality is sensitive to several parameters that are quite data dependent, tuning parameters for a given dataset remains a tedious process. And, in the distributed setting, a large network load, as each hash bucket look up requires a communication over the network.

To address the demanding needs caused by this rapidly growing, large-scale, and high dimensional data space, we propose in the following an distributed online similarity search for high-dimensional data. The rest of the paper is organized as follows. We survey the background of LSH and Random Projection Tree in Section 2. Section 3 gives an overview of dividing large data set into well separated subgroups, generating one LSH table for each clusters and analysis its properties. In Section 4, we give a framework of distributing online similarity search. We validate our method through experiments and analysis in section 5, and give a conclusion in Section 6.



II. RELATED WORKS Similarity search often reduces to an instantiation of the  Nearest Neighbor search problem, and similarity search objects are characterized as points in a high dimensional space. In some applications, objects are considered in a metric space where only a distance function is defined among them the features of the objects are unknown. One popular way to perform similarity search on these points is via an exact or approximate k-nearest neighbor search in a high-dimensional feature space. Given a collection of points, a distance function between them and q query point d, the goal of the k-Nearest Neighbor  (KNN) query is to find the k closest (in terms of the distance function) points to it.

The approximate version is even more desirable when the data dimensionality is high, as similarity search is very expensive in such domains. Here, the goal is to find K objects whose distances are within a small factor (1+?) of the true K nearest neighbors? distances. The quality of similarity search is measured by the number of returned results, as well as the distances to the query for the K points returned compared to the corresponding distances of the true K nearest objects for KNN queries[14].

A. Locality Sensitive Hashing[9,10] Definition 1. For the space T with metric ?, given distance  threshold r, approximation ratio c>1, and probabilities p1>p2, a family of hash functions H = {h : T ?U } is said to be a ( r, cr, p1 ,p2 ) -LSH family if for all p,q?T,  If ?(p,q)<r then Pr[ h(p)=h(q) ] > P1,  If ?(p,q) >cr then Pr[ h(p)=h(q) ] < P2.

Hash functions drawn from H have the property that near points (with distance at most r) have a high likelihood (at least p1 ) of being hashed to the same value, while far away points (with distance at least cr ) are less likely (probability at most p2 ) to be hashed to the same value; hence the name locality sensitive.

We amplify the gap between the ?high? probability p1 and ?low? probability p2 by concatenating several functions as follows. First, for an integer k, let H? = {H : T ?Uk} be a family of hash functions in which any H?H? is the concatenation of k functions in H, i.e., H = ( h1 ,h2 ,...,hk), where hi ? H (1 ? i ? k ). Then, for an integer M, draw M hash functions from H?, independently and uniformly at random, and use them to construct the index consisting of M hash tables on the data points. With this index, given a query q, the similarity search is done by first generating the set of all data points mapping to the same bucket as q in at least one hash table, and then finding the closest point to q among those data points.

To utilize this indexing scheme, one needs an LSH family H to start with. Such families are known for a variety of metric  spaces, including the Hamming distance, the Earth Mover Distance, and the Jaccard measure. Furthermore, Datar et al.

proposed LSH families for lp norms, with 0 ? p ? 2, using p-stable distributions[9]. For any W>0, they consider a family of hash functions HW: {ha,b: Rd?Z } such that ha,b(v)=  where a?Rd is a d -dimensional vector each of whose en-tries is chosen independently from a p-stable distribution, and b?R is chosen uniformly from [0, W]. Further improvements have been obtained in various special settings. [11] proposed a Multi-probe method to improve the performance of LSH. This scheme seeks to make better use of a smaller number of hash tables (L). To accomplish this goal, it not only considers the main bucket, where the query point falls, but also examines other buckets that are ?close? to the main bucket.

In this paper, we will focus on the most widely used p- stable distribution, i.e., the 2-stable Gaussian distribution and used the Multi-probe method. For this case, Indyk and Motwani proved the following theorem:  Theorem 1. With n data points, choosing k=O (log n) and M=O( n1/c ), the LSH indexing scheme above solves the ( c, r) - NN problem with constant probability.

B. Random Projection Tree Dasgupta and Freund proposed space partitioning  algorithms that adapt to the intrinsic dimensionality of data and do not assume explicit knowledge of this parameter[12].

Their data structures are akin to the k-d tree structure and offer guaranteed reduction in the size of the cells after a bounded number of levels. Both k-d[13] trees and random projection (RP) trees are built by recursive binary splits. They differ only in the nature of the split, which we define in a subroutine ChooseRule. ChooseRule chooses a direction uniformly at random from the unit sphere SD?1 and splits the data into two roughly equal-sized sets using a hyper plane orthogonal to this direction. The core tree-building algorithm called RandProTree, taking as input a data set S?RD, is as follow:  Precedure RandProTree(S)  If |S| < MinSize Then  return (Leaf)  Else {  Rule?ChooseRule(S)  LeftTree?RandProTree({x?S:Rule(x)=true})  RightTree?RandProTree({x?S:Rule(x)=false})  Return ([Rule, LeftTree, RightTree])  }

III. DIVIDE THE LARGE DATA SET AND GENERATING ONE LSH TABLE FOR EACH CLUSTER  In this section, we present the details of our framework to divide the large data set and generate one LSH table for each cluster. As depicted in Figure 1, our algorithm contains three     steps.

Mapping  1 2 3 4                    ...

...

...

...

4 3   Generating LSH Table            Fig. 1. An example with 4 peers.

A. Divide the Dataset into Well-Seperated Clusters The RP-tree construction algorithm projects the data onto a  randomly chosen unit direction and then splits the set into two roughly equal-sized sets using new split rules, which are unlike K-d tree's method to choose the coordinate with the largest spread, and splits recursively until the resulting tree has a desired depth.

Theorem 2. For RP-tree, There is a constant c2 with the following property. For anys?2, with probability at least 1?1/4, every descendant C? which is more than c2*s*d*logd levels below C has radius (C?)?radius(C)/s.

RP-tree has many good properties including fast convergence speed, guaranteed ?roundness? of leaf nodes, et c, comparing with other methods such as a K-d tree[13] or K- means algorithm. And these properties are useful for generating compact LSH hash code, reducing the algorithm?s performance/quality variance and conducive to choose optimal parameters of LSH for each leaf node[15]. So, we choose RP- tree to partition a dataset into several subsets called leaf nodes so that each leaf node only contains similar data items, e.g.

images for the same or similar objects.

Dasgupta and Freund have proposed two rules for RP-trees: RP-tree max and RP-tree mean, which are adaptive to intrinsic dimension. In practice, we observe that RP-tree mean rule would compute K-nearest neighbor results with better quality.

To apply RP-tree mean rule, we use the iterative method proposed by Egecioglu and Kalantari[10], which converges fast to a diameter approximation with good precision to efficiently compute ?(S), the diameters for a given point set S in a high- dimensional space. The diameter computation algorithm uses a series of m values r1, ..., rm to approximate the diameter for a point set S, where m ? |S|. It can be proved that r1 < r2 < ... < rm ? ?(S) ? min( r1,  rm) [10]. In practice, we find ?3 (5 ? 2?3) that rm is usually a good enough approximation of ?(S) even  when m is small (e.g. 40). The time complexity of this approximate diameter algorithm is O(m|S|) and we can construct each level of the RP-tree in time that is linear in the size of the entire dataset. As a result, the overall complexity to partition the dataset into g groups is O(log(g)n).

B. Method of Locating Clusters to Peers When locate clusters to peers, more formally, the mapping  should satisfy the following two conditions:  1. assign clusters likely to hold similar data to the same or adjacent peer to reduce the number of network hops.

2. if there is a network access, it should be constrained in a small range, instead of extending to the entire network-wide to aggravate the burden of the whole network.

We fix such a pair of balls calling them B1 andB2, and the data has doubling dimension d. A split is said to be good with respect to this pair if it sends points inside B1 to one child of the cell and points inside B2 to the other, bad if it sends points from both balls to both children and neutral otherwise (See Figure 2). We have the following properties of a random split :    ?  B1  B2  neutral split Bad split  good split  Fig. 2. Balls B1 and B2 are of radius  and their centers are ?/s? apart.

Lemma 1. Let B1 and B2 be a pair of balls as described above contained in the cell C that contains data of doubling dimension d. Then a random split of the cell is a good split with respect to this pair with probability at least , and a bad split with probability at least [16].

That is to say, the probability of the data d1 and d2 in a ball B1 to be split in the same descendant cell is at least , and in  the different cells  320?? after s levels, in other words, the probability of d1 and d2 in different descendant nodes is smaller than in the same descendant. So, for any nodes {x, y, z}, if the nearest ancestor of x and z has the lower level than x and y, the data in x has bigger probability in the same ball included in their nearest ancestor with y than with z.

Theorem 3. Pick any three nodes in RP-tree x, y and z. if x and y has the lower level than x and z, then the similarity between x and y is bigger than x and z.

Based on the analysis above, we organize the peers as a ring, as depicted in Figure 3. For a RP-tree, we denote the leaf nodes from one side to the other 1-n. We map the buckets     generated by node i to peer j, and make sure that the buckets generated by node (i-1) mod n and (i+1) mod n mapping to peer (j-1) mod m and (j+1) mod m respectively. Figure 1 illustrate the mapping for 4 nodes among 4 peers.

Inference 1. Pick any peer peer_service, and the corresponding leaf node serial number is leaf. Suppose that peer_service?s neighbor peer is peer1 and peer2, and their corresponding leaf node serial number is leaf1 and leaf2 respectively. If the nearest ancestor for leaf and leaf1 is bigger than leaf and leaf2, than the similarity between the data stored in peer_service and peer1 is bigger than peer_service and peer2.

Proof: It is easy to be concluded by theorem 2.

C. Generating Hash Tables for Each Cluster with Different Parameters  As the data in a cluster partitioned by RP-tree have homogeneous properties, the estimated parameters for each cluster can improve the runtime performance and quality of the LSH scheme, as compared to the single set of parameters used for the entire dataset. We choose different LSH parameters that are optimal for each leaf node instead of choosing a single set of parameters for the entire dataset.

There are four parameters that affect the performance of Multi-probe LSH algorithm: number of projections per hash value (k), number of hash tables (l), the width of the projection (w) and the length of the probing sequence(t). The value k and l represent a tradeoff between the time spent in computing hash values and time spent in pruning false positives, i.e.

computing distances between the query and candidates; a bigger value increases the number of hash computations. We do a binary search over a large range to find the optimal value.

This binary search can be avoided if we have a good model of the relative times of hash computations to distance computations for the application at hand. The width of the projection (w) has great effect on the probability of collision for any two points, that is to say, decreasing the width means decreases the probability, and the same result as increasing w.

We set w as small as possible and in this way decrease the number of projections we need to make. However, decreasing w below a certain threshold increases the quantity , thereby requiring us to increase w. Thus we cannot decrease w by too much.

For a fixed k, the parameter m is chosen to be the smallest natural number satisfying; l is set to be k(k-1)/2. To choose k, we use the method proposed by Indyk to get Tg and Tc, which Tg represents computing the l functions gi for the query point q as well as retrieving the buckets gi(q) from hash tables and Tc for computing the distance to all points encountered in the retrieved buckets. And k is chosen such that Tc()+Tg is minimal, where Tc() is the mean of the times Tc for all points in the sample query set S.

The t parameter is related to the query processing algorithm, and we change it from query to query using the method of adaptive probing proposed by Wei Dong, et c:  maintaining an online prediction of the expected recall of current query, and keeping probing until the required value is reached.



IV. ONLINE SIMILARITY SEARCH PROCESSING Every machine in the network can process query data,  maintaining the RandoProTree when the hash tables created, the mapping table table_id_server between the leaf node serial number and peers and the neighbor machines?s IP address Neighbor_server.

Fig. 3. An example of distributed online similarity search.

Upon receiving a query data d, the server will calculate leaf node serial number leaf by the maintained RandoProTree, then search the table table_id_server, with the mapping between the leaf node serial number and peers address, to obtain the address of peer_obj stored the buckets generated by leaf. Then peer_obj performs Multi-probe search to systematically probe those buckets that are closest to main bucket to get the best result peer_obj_res, utilizing the parameters for the peer. For a single hash table, let d be the query point and H(d) its hash respectively. Recall that H(d) consists of the concatenation of M integral values, each produced by an atomic hash function on d. Buckets corresponding to hash values that differ from H(d) by ?1 in one or several components are also likely to contain points near the original query point d for that buckets corresponding to hash values that differ from H(d) by more than 1 in certain component are much less likely to contain points of interest.

Then the neighboring peer which has the nearest ancestor with leaf will be the next target peer peer_obj_next.

peer_obj_next will issue a local Multi-probe search compare their best result peer_obj_next_res to peer_obj_res. If no update, we stop forwarding the query d to its neighbor, or, we will go on searching until preset conditions full filled. Figure 3 shows an example of doing similarity search for a data point.

The formal description for generating buckets and mapping them to different peers is as follows:  Precedure Multi-Peer-Search(d)  leaf_id?RandoProTree(d);  peer_obj?find(table_id_server)  Flag?False  while (Flag==False) and (not all servers visited)  Multi-probe (peer_obj)  If (no update for peer_obj_res)  Flag?True     exit  Else  peer_obj_next?find_next(Neighber_server, RandoProTree)

V. EXPERIMENTS  A. Experimental Setup We have implemented the proposed system and algorithms  using C language running on eight machines connected as ring model. Every machine has eight Intel(R) Xeon(R) CPU of 2.33GHz and 64 GB RAM. We use Flickr data set, a crawl of Flickr consisting of 1, 000, 000 images represented by their MPEG7 visual descriptors. The total number of dimensions per image is 282 and contains descriptors such as Edge Histogram Type and Homogeneous Texture Type. We implemented the following methods plus the mechanism this paper prosed as comparison:  Random_method: the baseline algorithm, which distributed the dataset randomly on servers. At query time, the query data was also delivered to the random selected server.

We report on the following measures:  Number of Network Hops: We count the number of network hops during the query execution. Network hops are one of the most critical parameters and the dominating factor in making distributed algorithms work in large-scale wide-area networks. One single network hop in a wide-area costs in average around 100 ms, which overrules the I/O cost, induced by a standard hard disk, with approximately 8 ms for disk seek time plus rotation latency and 100 MB/s transfer rate for sequential accesses, in case of local disk access.

Recall Ratio: For the effectiveness metric, we report on the recall ratio, i.e., the percentage of the actual nearest neighbors  N(d) in the returned  results I(d): ?????? ? ??????  ?????? , where N(d) can be computed using any exact k-nearest neighbor approach and serves as the ground-truth. The relevance is defined by the full-scan run over all data points to determine points within k- nearest neighbor for a given query, where similarity is measured based on l2.

B. Experimental Results Figure 4 illustrated the recall ration and the number of  network hops. The height of random projection tree is 3, and the parameters   local LSH for every peer is generated automatically shown as follows:  Table 1.   Parameters each server taken  Number k t l w 1 20 2 595 4.0 2 26 2 2346 4.0 3 22 2 946 4.0 4 22 2 946 4.0 5 20 2 595 4.0 6 12 2 91 4.0  7 24 2 1485 4.0 8 24 2 1485 4.0  As illustrated in figure 4, the recall ratio of our method is better than random_method, especially for the previous servers, that is to say, the network hops will reduced greatly when not required the whole answers.

Fig. 4. Recall ratio versus number of network hops  In the second experiments, we compare the number of answers returned by random_method and the mechanism proposed by this paper for 15 query data in 3 network hops.

The parameters of each server are also as table 1.

As is shown in figure 5, our method?s performance are superior to random_method, with  recall ratio achieving 73%, which are superior than random method 37.4% obviously.

Fig. 5. Number of answers comparison among exact, random and the method we proposed for a fixed network hops

VI. CONCLUSION We have presented a robust and scalable solution to the  online distributed nearest neighbor search problem over high dimensional data. Considering the requirements that arise in distributed settings, we have shown how to partition the large data set into small ones, which are similar among them,  map them to the ring ordered set of peers to achieve high quality search results through LSH, balance the load among peers and limit the number of network accesses, which are verified by experiments.

There are many avenues for future work. We hope to test our algorithm on more real-world datasets, including images, textures, videos, etc.

