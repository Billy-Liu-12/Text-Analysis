Failover Pattern with a Self-Healing mechanism for high availability Cloud Solutions

Abstract?Cloud computing has already been adopted in a broad range of application domains and has become an established building block in IT landscapes. During the process of cloud middleware development, the companies have focused mainly on the high availability of data and end-user services, but unfortunately neglected the availability of middleware com- ponents. Therefore failures of the middleware components itself usually leads to a partial or even total blackout of the cloud.

In this paper, we present the design and implementation of a novel scalable and highly available multi-master pattern for cloud middlewares. In contrast to existing Infrastructure-as-a-Service cloud management frameworks, which are usually designed in a centralized tree topology composed in a three-tiered master- worker architecture, we introduce a concept for a multi tree with all tree roots connected in a fully connected mesh topology.

In this architecture user requests are load balanced over multiple failover servers. Furthermore, our concept includes an automatic self-healing mechanism for worker nodes of each tree.



I. INTRODUCTION  Running applications in the cloud on a large number of shared-nothing commodity computers makes fault tolerance an important issue. Not only the higher error-proneness of the hardware (compared to high availability hardware), but also the larger number of those easier failing components leads to a high failure probability. Therefore, failure prevention in Cloud Computing has become a well discussed topic in the last years and is differentiated in several types: power failures, network failures, server failures, and management software failures [1].

Especially on the Infrastructure-as-a-Service (IaaS) layer fault tolerance is an important issue, because failures in this layer result in poor quality of services. Depending on the ap- plication running on the infrastructure a poor quality of service may be for example data loss or a long off time of a virtual machine. For IaaS providers it is highly important to prevent failures or at least recover from failures transparent to the user and as fast as possible. At the same time the solution should be feasible without special hardware or complex changes in the cloud infrastructure.

The described approaches in this paper are based on our cloud middleware called CloudDisco [2]. CloudDisco is build similar to the known IaaS platforms, like Eucalyptus or Open- Stack. In contrast, it is designed to offer physical and virtual hardware through the internet. These hardware components are virtualized physical hardware elements, which can be shared  in the cloud and enable applications or virtual machines the access to geographical distributed hardware devices.

The availability of the hardware components is crucial for the quality of services in CloudDisco. Nevertheless the quality of service in IaaS clouds is based on the management software as well. Thus we focus on Management Software Failures in this paper, which can be subdivided in the following three levels:  ? Virtual Machine Level: In this level, a failure is a crashed VM, which results in data loss if no suitable replication mechanisms is applied.

? Cluster Level: Errors in this level have the conse- quence that a node or even a whole bunch of nodes are no longer available.

? Cloud Level: An error in this level is most severe, because it results in an unavailability of the entire cloud.

Common Iaas middleware they are typically built in a three-tier master-worker architecture, depicted in figure 1.

The single master instance is the cloud interface, it is called Cloud Manager or Cloud Controller. This interface between the cloud environment and a user (client) is a significant component, which should not only provide high availability support but also scalability mechanisms to achieve quality of service requirements. the described approach can be adapted to these systems.

In CloudDisco the Cloud Manager is the interface for the user to request a list of available hardware as well as to setup the connection to one of the hardware components. The Cloud Manager receiving a request for all available hardware components sent it to all connected Cluster Controller. The Cluster Controller collect the list of components and return it  User / Client  Common Cloud Middleware Stack  Cluster Controller Cluster Controller  Cloud Manager (Master)  Cluster Controller Cluster Controller Cluster Controller  (Worker)  Cluster Controller (Worker)  Cluster Controller Node Controller  (Information)  Node Controller (Information)  Fig. 1. Middleware components in a three-tiered master-worker architecture   DOI 10.1109/CLOUDCOM-ASIA.2013.63    DOI 10.1109/CLOUDCOM-ASIA.2013.63     to the Cloud Manager, which passes it back to the user. If the Cloud Manager gets unavailable the whole service becomes unavailable, even though the Cluster Controller and thus the hardware components are still available.

Similarly in Eucalyptus [3] for example, the Cloud Con- troller receives requests from the user (e.g. to list of available resources) and propagates them to its Cluster Controllers. The Cluster Controllers in turn propagate the request to the Node Controllers, which execute the request (provide the resource information). In such a architecture, a failure of the Cloud Controller is an error in the Cloud Level and will lead to an outage of the entire cloud. On the other hand, a failure in a Cluster Controller is an error in the Cluster Level and will affect only an individual part of the cloud.

In the further course of this text, we will restrict ourselves to the naming of CloudDisco components, as Cloud Manager is equivalent to Cloud Controller in Eucalyptus. Besides, the Cluster Controller and the Node Controller have the same names in Eucalyptus [3]. In OpenStack the components are called API Server (nova-api), Scheduler (nova-scheduler) and Compute Worker (nova-compute) [4]. Note that the mentioned components have similar or even equal functions within the cloud system and the architectures are alike. Therefore the pattern described in this paper can also be adapted to any other IaaS platform or any other platform of that kind.

CloudDisco Eucalyptus Openstack Cloud Manager Cloud Controller nova-api  Cluster Controller Cluster Controller nova-scheduler Node Controller Node Controller nova-compute  As the main benefit of the numerous computing nodes is the ability to parallelize applications a failed machine usually leads to a lose of information. This information might be crucial to the functionality of the application or the completeness of the results. In order to achieve fault tolerance it is necessary to make this information available to the system again. Depending on the application, there are several approaches for this issue, like replication in DDBMS [5] or saving of intermediate data in parallel processing systems like MapReduce [6]. In this paper we describe a technique to deal with machine failures in a multimaster cloud.

The substantial disadvantage of the common architectures is the non redundant structure, which cause each component to be a single point of failure, though on a different impact level. Especially failure of the single master node would cause a entire blackout of the system. Nevertheless a failing Cluster Controller would make a part of the Cloud unavailable.

To prevent a partial or even an entire blackout of the cloud, we introduce an architecture for a load balanced multi- master system in cloud middlewares. Due to the multiple Cloud Manager, an entire blackout of the cloud and thus failures in the Cloud Level are more improbable, because all other master nodes can take over the work of a failed node. Additionally the approach handles several master failures, because it is based on independent multiple Cloud Manager installation where each master instance is a further failover server. Therefore the cloud would still be available as long as one master server is still alive.

Besides the advantages of the higher availability due to the multi-master architecture, scalability issues are solved with this approach. As their are multiple master nodes available the user requests can be distributed over all available master nodes.

Furthermore, the multi-master architecture described in this paper, does not only prevent failures due to high availability and load distribution but can also recover from failures with an self healing mechanism. When some Cloud Managers fail and the Cluster Controllers lose the connection to the cloud management interface, a self-healing of the cloud middle- ware is initiated automatically. The self-healing accomplishes a reconnection of the Cluster Controllers. Thus, the cloud resources are still available after a Cloud Manager crashed, since the Cluster Controllers are remigrated by using another failover instance. Thereby a failure in the Cluster Level can be eliminated. Moreover, the reconnection attempts of multiple Cluster Controllers are load-balanced between all residual Cloud Managers in order to avoid inherited error and to raise the performance.

The rest of this paper is organized as follows: section II discusses related work in this area and will show the differences to other approaches. Section III describes the idea and implementation of the failover pattern in detail. The section IV explains how the self-healing is processed and in section V we conclude our results.



II. RELATED WORK  The importance for failover mechanism was also recog- nized by the developers of Eucalyptus. The CTO and co- founder of Eucalyptus Systems Inc., Rich Wolski, has noted in [7] that they will focus more on enterprise-quality features (e.g.

?. . . to develop a high-availability version of Eucalyptus that provides hot failover of internal components.?) in the platform.

Shortly thereafter, the first release of Eucalyptus Cloud 3.0 with high availability support has been published in February 2012. The concept of Eucalyptus for high availability is to have primary and secondary cloud and cluster components. The pri- mary component works in the same fashion as described above, but if a failure occurs, the secondary component becomes the primary component [8].

The technology behind this failover technique is a service called Arbitrator, which approximates reachability to a user by monitoring the connectivity between a user and the in- ternal components. This monitoring is performed by sending messages to an external entity or an external site periodically.

The primary/secondary server relationship of the Eucalyptus concept for high availability has some disadvantages: If the pri- mary server goes down, all user requests need to be redirected to the secondary instance and an administrator is obligated to repair or restart the primary node. During this time, the cloud installation is executed in the traditional way with only one management instance as a single point of failure and if a further error occurs, the whole cloud would be no longer available. Moreover, the primary/secondary approach does not solve the problem of scalability, because there is only one interface instance to the cloud at any time.

Another approach had been presented by Jamilson Dantas, who used the same principle of a primary and a secondary component with Eucalyptus in [9], but bundles the Cloud     Controller, Cluster Controller, Storage Controller, and Walrus on a single machine, which he called the General Controller (GA). By continuously monitoring and synchronization be- tween the primary GA and the secondary GA, he simulates in his evaluation with the aid of a Markov Reward Model a fast switch over in an error case. This approach has the same disadvantages as described above for the current Eucalyptus version with high availability support. Additional, the benefits of an faster switchover are irrelevant, because in an error case not only the interface to the cloud would be affected, but also additional components of the installation.

OpenNebula promotes a methodology called OpenNebula Failures as part of their fault tolerance feature [10], however, if the OpenNebula daemon crashes due to an error, the entire cloud is affected and can only be reactivated by an intervention of an administrator.

In contrast to the high availability principle for cloud platforms described above, many cloud middleware developer concentrate their work only on high availability for storage and virtual machines [11], [12], [13], [14]. In this context, check- pointing is another well known general technique to improve fault tolerance of a computer system. Whereas checkpointing can be either writing an application?s processing data to disk or to snapshot a virtual machine [15]. In case of a failure, a backup node can take over the failed nodes work by using the checkpoint. There are several approaches of optimization of checkpointing in this area. Zhou et al. introduce a method to mirror machine memory to a virtual memory were it can be used in case of a restart [16]. Singh et al. integrate a checkpointing algorithm into the load balancing to make the checkpointing more efficient [17]. Walters et al. using checkpointing for MPI computations in HPC environments [18]. However, checkpointing cannot be used to improve the availability of cloud middlewares, because the correction of errors is very time consuming and additional components are also required to create and store the checkpoints.

The work of Eugen Feller et al. is the most closest to our approach. In [19] the authors present a virtual machine management framework called Snooze, which is built on top of Apache ZooKeeper and prevent the single point of failure by a self-organization and self-healing mechanism. Snooze?s architecture is similar to the common architecture of figure 1, though they call the Node Controller - Local Controller, the Cluster Controller - Group Manager, and the Cloud Manager - Group Leader. The principle that they pursue is a fusion of Cloud Manager and Cluster Controller. In particular, the instance can have two states: either it runs in the Group Manager mode or in the Group Leader mode. If an error occurs at one of these components, an event is triggered. This event is followed by an election algorithm in which a new Group Leader will be elected by a given identifier.

During this reorganization the Group Manager which will be the new Group Leader terminates all its tasks and all connections to the underlying Local Controller. After that, all Local Controller need to rejoin the network and need to establish new connections to another Group Manager. This approach improves the availability of the middleware, but does not solve the scalability problem, because at any time only a single Group Leader is elected to provide its functionalities for upper layer requests and all its Group Managers. In contrast to  this approach the multi-master architecture in our middleware more is flexible and improves not only the availability of all middleware components but also the scalability of the interface to the cloud. Furthermore, in our approach it is unnecessary to terminate all connections to the underlying components. This avoids time-consuming rejoining of already connected Node Controllers.



III. FAILOVER PATTERN  The pattern described in this paper is based on a three tier multi-master cloud and is the foundation for communication in the middleware layer of our ongoing research project CloudDisco. The major difference to the traditional master- worker pattern is that there are multiple equal master nodes as depicted in figure 2. In contrast to a master-slave architecture, the different master nodes are not passive replicas of an active master, but are active nodes and in charge of a fraction of the cloud. These master nodes, called Cloud Manager (CM), are interconnected in a full-mesh topology and can communicate with each other. Each worker node, called Cluster Controller (CC), is connected to exactly one of the master nodes, which announces and updates a list of all known CMs in the system.

Since all CMs are equal, a user as well as a CC can connect to any CM of the cloud.

CCs do not have any knowledge of each other.They are connected to at least one resource provider (Node Controller (NC)). In our model, the connection between the components is setup from the NC to the CC and from the CC to the CM.

If a new NC was started, the instance registers itself at a given CC, which it has been assigned to. The CC controls a specific cluster of several NCs, which had been bundled due to the locality or characteristics of the NCs. A new CC in turn registers itself at one of the existing CMs; it does not matter to which CM the CC connects, because all CMs are equal multi- master nodes and act as a unit. Thus the components build a tree with one CM as the root node, which is in turn connected to all other CMs of the cloud. Therefore, the overlay network consists of a mixture between a tree topology and a full- mesh topology. Figure 2 depicts such an installation with four CMs and a user who requests the first instance. In this figure an schematic architecture is displayed, where the transparent colored components represent variety of nodes and additional underlying components.

A new CM has to know at least one other master node.

During start up, it connects to a CM and receives a list of all other master nodes in the system. It will then establish a connection to every CM in this list. Each CM, receiving a connection attempt updates its list of known master nodes and propagates the updated list to its connected CCs. Such a mechanism guarantees consistency of interconnected cloud components, and enables a failover technique in case of a CM crash. This way, an entire cloud blackout is prevented, because the user has another CM available at any time.

The task of the worker nodes is to collect and aggregate the information of the connected information providers when it is asked by its master node. The master nodes get requests from users and collect the requested information from its connected workers and request all other master nodes to start collecting from their workers. The original master node receives all information, aggregates it and gives it to the user.

Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cluster ControllerNode Controller  Cluster ControllerNode Controller  User / Client DNS  Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cloud Manager  Fig. 2. Schematic architecture and visualization of the failover pattern  A. Network and user requests  The Cloud Manager is the interface to the cloud. This interface enables various functionalities, e.g. to list all available resources, to reserve a time slot for a resource or to use a re- source in the desired manner. For each of these functionalities, the user submits a request over a client or directly by using the web interface of a CM. This request results in a job that the CM needs to process. Therefore, the CM creates a session for this job and needs to specify an appropriate addressing methodology according to the type of the job. The addressing methodology can be either an unicast or a multicast. If the user requests a list of all available resources, for example, this job needs to be propagated to all CCs and results in a multicast message. In contrast, a job for a specific NC is only transmitted to the corresponding CC by an unicast message.

The messages, which are sent in the overlay network, are called Session Datagrams. A Session Datagram is sent to all connected CCs and CMs in case of a multicast. If the Session Datagram is related to a directly connected CC, it is sent only to the corresponding CC. If the CC is not part of the CM?s bunch, the Session Datagram is sent to all CMs in the top- level mesh. Depending on the affiliation of the message the CMs decide to either reject or forward the Session Datagram.

Hence, if a Session Datagram is sent to a CC in another tree over a CM, this CM acts as a router for such a Session Datagram.

Each CC waits for incoming Session Datagrams and when it receives one, the associated job is executed (e.g. collecting data from the underlying Node Controllers). The result is than transmitted back to the connected CM in a new Session Datagram. If the CM is the same which emitted the Session Datagram, it collects and aggregates the information and gives it to the user. In any other case, the CM operates as a router again and transmits the Session Datagram to the source CM.

These addressing technique requires additional information, which are specified in a header of each Session Datagram.

This header includes, beside the source and destination address of the components, a session identifier, which enables the assignment of a Session Datagram to a corresponding job or request.

The interconnection between CM instances is based on a peer-to-peer overlay network. In Particular, all CMs are fully connected by the full-mesh topology, which is the most redundant and reliable network but is not feasible for a large number of peers. We opted for this topology, because firstly, the computationally intensive work is been done by the CCs which results in a rather smaller number of CMs compared to the well known master-worker pattern. And secondly, Session Datagrams which have been sent between CMs are only routed to the directly connected CCs or immediately rejected. There- fore, Session Datagrams can only be executed by the direct neighbor peers, which results in a maximum hop distance of one.

B. Scalability and Load Balancing  The scalability is one of the basic principles of the cloud, therefore the cloud should not only enable the scalability for resources but should also be able to scale itself. Adding and removing Cloud Manager and Cluster Controllers must be possible in a cloud system. Otherwise the capacity of a CM could be exceeded due to the number of connected CCs or the number of user requests, and the limit for a qualitative response time can no longer be guaranteed. On the other hand running CMs with spare capacity is causing waste of money.

For that reason, according to the number of users and the current workload, the number of available CMs can be increased or decreased. When a new CM is added to the cloud, it establishes connections to all CMs in the existing cloud. The details of the connection establishment is describte in Subsection III-C. Removing a CM from the cloud is similar to the self healing machanism: All connected CCs of this node have to be connected to other CMs in the cloud. The details of the self healing mechanism are described in Section IV.

In contrast to the self healing mechanism during the removal process the CM is able to wait for all pending user requests to be finished until all connected CCs have been reconnected.

In order to provide load balancing between the CMs the Round Robin DNS (RR-DNS) load balancing technique is used to distribute the requests of users over all available CMs [20].

C. Communication establishment  In order to build such an overlay network for high available cloud middleware components, an initial CM needs to be instantiated first. In order to secure the cloud system it is necessary to make sure that no unpermitted server connects to the mesh. Additionally the communication itself must be secured. This can be done in various ways, as the security is not the main focus of this work at this point the mesh is only secured with a simple password technique, which is briefly described within this chapter for the sake of completeness.

This CM is the initial node where a password for the mesh is set. This password is necessary to protect the overlay multi- master mesh against unauthorized access. When the initial node has been successful started, every other node, no matter if the node is a CC or a CM, can be started by assigning an IP- Address of any already running CM and the initially defined password.

If a new CC is instantiated to join the cloud environment, a connection between the CC and a CM needs to be established.

The figure 3 depicts a sequence diagram for a connection establishment between a CM and a CC. The first message is sent from the CC to the CM, because each connection must be made in the direction of a CM. The first message is the authentication message, which is the symmetrically encrypted IP address of the CC. The symmetric key for the encryption was generated from the initially defined password for the mesh.

At the CM site, the encrypted IP address is decrypted and compared to the given IP address of the new node. If both IP addresses correspond to each other, the connection to the new component is permitted. This authentication scheme does not only prevent most man-in-the-middle attack strategies, but enables also an encrypted data exchange during the continuing communication.

When the access to the overlay network is granted, the CM sends an announcement message back, which contains a list of all available and interconnected CMs in the multi-master mesh. This list represents all possible failover servers, which the CC can use for the self-healing mechanism described in section IV.

In the last message of the three way handshake, the CC signalizes the CM its readiness for work by sending a status message. This message simply consists of the number of NCs which are currently connected to the CC. During the  Cluster Controller Cloud Manager  Authentication Message  Announcement Message  Status Message  Fig. 3. Connection establishment between CC and CM  connection setup this number is just the information for the receivers that the CM is ready. It is discarded at this point.

However, this number gives valuable information: If the value is equal to zero it indicates a newly instantiated CC. In turn a number greater than zero means that the node is running a long time and due to an error the CC is rejoining the overlay network.

On the other hand, there is the case that a new CM is instantiated to extend the multi-master mesh as a further failover server or reconnecting after a failure. In order to connect to the mesh the CM first of all connects to one of the CMs in the mesh. The connection establishment between a CM and another CM, which is already part of the mesh, follows the same principle of the three way handshake. In contrast to the message exchange above, the new CM interprets the announcement message as a list of mesh nodes it needs to connect to next.

In particular, the new CM creates a new connection to each of the mesh nodes and this way initiates a self-registration in the failover lists of each master, and an update process where all CMs distribute new failover lists to their CCs. During the connection establishment to the other master nodes, the three way handshake is performed for every connection. This enables an overall comparison of all failover lists to check the consistency of the multi-master mesh. If an inconsistency is detected, an overall list update between the CMs is initiated.



IV. SELF-HEALING  All cloud components, the CMs, the CCs as well as the NCs, are interconnected by TCP/IP sockets. Thus, an interrupted connection can be identified and is interpreted as a failure of the connected component. Note that a slow connection to a CM is considered to be a failure too. In the context of CloudDisco there is a limit for a qualitative response time. Therefore a slow connection is treated as failure too. In case of a failed CM, the corresponding CCs and the other CMs lose the connection and therefore become aware of the failure.

The other CMs will then update their list of known CM nodes and pass it to their connected CCs.

As the CC is no longer connected to a master node, it has to establish a new connection to another CM. Therefore, the relevant CCs start the self-healing phase by reconnection attempts to the next failover server. In particular, each CC tries to connect to the first CM in its failover list. If the connection establishment to this CM fails, it tries to connect to the next CM in its failover list. As described in III-C, the connection attempt is done by a three way handshake analogous to the initial connection setup. Figure 4 shows the reconnection of two CCs from a failed CM to new CMs. To simplify the figure, we leave out the connected NCs at the CCs, nevertheless their might be various numbers of NCs connected to each CC. If the reconnection can be established, each CC has received a new failover list from its new CM during the connection setup and will stay connected to this CM even if the previous CM will be available again.

A. Failover list distribution  When a CM extends or leaves the overlay multi-master mesh, the list of possible failover servers needs to be updated     Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cloud Manager  Cluster Controller  Cluster Controller  Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cloud Manager Cluster Controller Cluster Controller Cluster Controller  Cluster Controller  Cluster Controller  x  Fig. 4. Schematic self-healing in the failover pattern  and distributed over all CCs. This list is sent from each CM to its currently connected CCs. If a CM fails, every connected CC will try to establish a new connection to the first available CM in the failover list. Hence, if each CC would get the same failover list, all CCs would try to connect to the same CM at the same time. To avoid this situation, the CMs transmit different failover lists to their connected CCs. In the following, we will describe the list sorting and distribution in a formal manner.

Every Cloud Manager CMi in the fully connected Multi- Master Mesh (MMM ) has an individual Mesh Connection List MCLCMi , which is a set of currently connected CMs in the sequence of their up times and is thus a proper subset of the whole multi-master mesh: MCLCMi ? MMM . In particular, the initial Cloud Manager on position one (CM1) is longest alive and is the first element in the Mesh Connection List of every connected CM. The number of all CMs in the overlay Multi-Master Mesh is |MMM | = j+1. The Mesh Connection List of a CM does not contain the CM itself, consequently, each Mesh Connection List has the length |MCLCMi | = j.

Therefore, individual MCLCMi can be described as:  ?CMi ? MMM : MCLCMi = {CMn|CMn ?= CMi} .

(1)  Every time, a list of possible failover server is sent from a Cloud Manager CMi to one of its Cluster Controller CCi,k, whether in an announcement message or in a list update, the customized Failover List FOLCCi,k needs to be calculated.

This Failover List contains the same elements as the Mesh Connection List, but in an alternating order for each Cluster Controller CCi,k. It is thus not a proper subset. Based on the Cluster Connection List CCLi, which contains all connected CCs ordered by uptime of an appropriated Cloud Manager CMi, and the corresponding Mesh Connection List MCLCMi , the individual Failover List FOLCCi,k for each CCi,k is calculated by shifting the elements of the Mesh Connection List MCLCMi for every connected CC:  ?CCi,k ? CCLi : FOLCCi,k = (MCLi,(k+x mod j)), x = [0, j]  (2)  This individual sorting of the Failover List for each CC results in a well balanced self-healing over all master nodes in the entire cloud, because all reconnecting CCs are uniformly distributed.

B. Request handling  As described above the multi-master mesh of CMs has knowledge of the fact that a CM failed. If this CM is responsible for a user request, all CCs and CMs discard its Session Datagram for this target and the user losses connection to the CM and has to trigger the request to the mesh once more.

Furthermore,for other user requests the other CMs do not wait for any Session Datagrams from the failed CM. Instead the CMs collected all answers from the rest of the master nodes and mark the result as incomplete before it is passed to the user. This way, the user is able to decide if the incomplete result is acceptable or if the request has to be started again. If the CM has already received all responses from the failed CM it is able to wait for all other missing responses and return a complete answer.

However, the CM has no knowledge of the cloud trees of the other CMs. To be able to recognize whether another CM has sent all answers, the CM needs to know how many responses are expected. As described above, a CM which is not in response of a request, routes the request and response massages to and from the CCs. Each CC sends a individual message to the CM. The number of responses to expect from another CM is therefore equal to the number of connected CCs in a case of a broadcast message.

To notify the requesting CM about the number of expected response messages, the routing CM adds the number of con- nected CC to the message header of a Session Datagram. The CM, which collects all responses for a particular session, com- pares this number to the count of already received messages from this CM.



V. CONCLUSION  In this paper we describe a failover pattern for cloud mid- dlewares. We show a multi-master architecture, which prevents an overall cloud failure in case of a failed master node. We specify the initial system setup and describe a communication model in this architecture. Additionally, we introduced a self- healing mechanism for this multi-master architecture, which leads to an automatic reconnection of worker nodes to another master of the cloud. Furthermore, the reconnection not only guarantee the network consistency, but is also balanced by an individual failover list sorting.

As described in section II, developers will focus more on high available middleware components in the future to improve the stability of cloud environments. Our approach is a step for- ward in this area, because we adopt the multi-master pattern for cloud middlewares. Future work will focus on redundancy for Cluster Controller to improve also its availability. Moreover, we need a better load balancing technique to distribute the user requests according to the load of the master nodes. We describe in this paper that the Cluster Controller discards all outstanding sessions, if the connection to the CM is broken.

Too avoid the loss of Session Datagrams during a switchover, an extended reconnecting needs to be designed, where the Session Datagrams are routed over the new Cloud Manager.

In addition, we will show performance measurements and comparions to other approaches in further publication.

