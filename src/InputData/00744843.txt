IBHYS: A New Approach to Learn Users Habits

Abstract  Learning interface agents search regularities in the user behavior and use them to predict user?s actions. We pro- pose a new inductive concept learning approach, called IBHYS, to learn such regularities. This approach limits the hypothesis search to a small portion of the hypothesis space by letting each training example build a local ap- proximation of the global target function. I t  allows to si- multaneously search several hypothesis spaces and to si- multaneously handle hypotheses described in different lan- guages. This approach is particularly suited for  learning interface agents because it provides an  incremental algo- rithm with low training time and decision time, which does not require, from the designer of the interjCace agent, to de- scribe in advance and quite carefully the repetitive patterns searched. We illustrate our approach with two autonomous software agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments and implemented in Objectworks-Smalltalk-80. The Ap- prentice learns user?s work habits using an IBHYS algo- rithm and the Assistant, based on what has been learnt, proposes to the programmer sequences of actions the user might want to redo. We show, with experimental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy.

1 Introduction  The discovery of repetitive patterns in data is a challeng- ing problem, which has a lot of applications in  various domains: fast text searching where patterns are strings of symbols, data mining where patterns are association rules and, particularly, learning interface agents where patterns are sequences of users actions.

Learning interface agents [ 131 are software agents that assist users of interactive environments by learning their habits and preferences from experience and predicting what they are going to do next. The problem of learning users habits can be modeled as the task of searching repeated  patterns in  the sequence of the actions the user has per- formed during work sessions. These repetitive patterns can be noisy (if the user does not perform each time exactly the same sequence of actions), unordered (if the user perform the same actions but not each time exactly in the same or- der) or both noisy and unordered. Learning interface agents would highly benefit from an incremental and low comput- ing time algorithm to learn such general regularities, able to predict user?s action in real time.

Although efficient algorithms have been proposed to find exact or noisy repetitive patterns in strings [9, 241, no algorithm, to our knowledge, solve the problem of finding both noisy and unordered repetitive patterns. On the other hand, machine-learning approaches like induc- tive concept learning should theoretically be able to learn such general regularities. However, most of the algorithms ([ 15,20,7, 63) search hypothesis spaces to acquire the def- inition of a target concept, and large and complex spaces critically slow down the learning process. Therefore, this approach fails to provide an efficient algorithm for learn- ing interface agents because user?s actions can be described with a lot of attributes with large set of possible values and training examples generally belong to large hypothesis spaces. Conversely, paradigms like instance-based iearn- ing [4] build a local approximation of the target function, and limit the hypotheses search to a small portion of the hypothesis space, but defer the processing of training ex- amples until a new instance must be classified, and require a lot of computing time to predict the target value for a new instance. These paradigms are unable to provide an algo- rithm to predict user?s actions in real time.

We provide here an alternative approach for learning interface agents called instance-based hypothesis search (IBHYS). As learning with radial basis functions [lo] which approximates the global target function by a com- bination of local approximations, our approach lets each training example build a set of hypotheses that locally ap- proximate the global target function, limiting the hypoth- esis search to a small portion of the hypothesis space.

However, as a generalization of learning with radial ba-  0-7803-5214-9/98/$10.00 0 1998 IEEE. 200  mailto:iirmm.fr   sis functions , it does not restrict the approximation to a combination of Gaussian functions and allows to han- dle simultaneously hypotheses described in different lan- guages. This approach is particularly suited for learning interface agents because it provides an incremental algo- rithm with low training time and decision time, which does not require, from the designer of the interface agent, to de- scribe in advance and quite carefully the repetitive patterns searched. By specifying several hypothesis spaces, he gives the algorithm the potential to find various repetitive pat- terns.

We illustrate our approach with two interface agents, the Apprentice and the Assistant, that actively assist users of the Smalltalk' interactive programming environment.

This work finds its roots in the idea of programmer appren- tices [23, 211 which were ambitious attempts to automat- ically assist programmers in the task of code production.

This produced remarkable results but the task was certainly too complex in whole generality and such apprentices are not really integrated in todays standard programming en- vironments. We propose to apply the techniques of au- tonomous software agents [IO, 141, to merge, eventually extend, the above ideas. The Apprentice and the Assistant aim at letting programmers focus on the essential part of programming (design and write code) by automating the achievement of repetitive tasks.

This paper summarizes the Apprentice-Assistant ar- chitecture and focuses more particularly on the issues re- lated to the learning task. Section 2 presents the Apprentice and the Assistant, shows how users actions are recorded and monitored, and defines the learning task. Section 3 de- scribes our IBHYS approach, proposes a formalism, gives a general procedure and shows how the Apprentice makes use of this procedure to learn users habits. Section 4 gives experimental results and shows that our IBHYS algorithm outperforms the well known ID3 algorithm [20].

Related works The idea of employing machine-learning in user-  modeling appeared with learning interface agents [22, 12, 14, 11 and begins to be studied in the user-modeling com- munity [ 16, 181. However, no incremental algorithm, with low computing time have been proposed to solve the prob- lem of learning repetitive patterns in whole generality.

Most of the existing learning interface agents [ll, 1,  2, 171 reduce the learning task to the prediction of a few attributes with small set of possible values, and not at all try to pre- dict complex actions like those performed in a program- ming environment.

[ 141 studies the question of employing ID3 [20], a de- cision tree algorithm, in a learning assistant for meeting calendar management. However, by allowing their assis-  tant to spend several hours learning each night, the authors do not propose a low computing time solution to learn users habits.

The closest work to our agents are Opensesame!.

Opensesame! runs in background on Macintosh system 7, and learns repetitive tasks in opening and closing files or applications, emptying trash, rebuilding desktop. Its first weakness is that it is disruptive and frequently solicits the user. Conversely, our Assistant only makes suggestions the user is free to ignore and never request the user. OpenS- esame! limits the learning task to a dozen of actions and only learns noisy habits. Unfortunately, it appeared unable to learn simple repetitive opening of folder when we have tested it and the paper describing this system does not bring any other information on this point.

Eager [3] is an interesting software that assists users of the Hypercard environment by anticipating actions. Eager, as Holte's assistant for browsing in information libraries [8], is unable to learn noisy or unordered repetitive patterns, does not build a base of habits and forgets habits after it has performed them.

Note that our multiple description languages approach, allows to integrate the graph-based induction technique used in [25], with benefit of low computing time and in- crementallity.

2 The Apprentice and the Assistant  We illustrate the IBHYS approach with two interface agents, the Apprentice and the Assistant, devoted to assist users of interactive programming environments. Our Ap- prentice learns user's habits i.e. the tasks the user performs repetitively for which he has not the opportunity or the will to write scripts or macros. The Assistant's task is to ac- celerate and facilitate the programmers tasks by automat- ing the achievement of repetitive tasks. Based on what the Apprentice has learnt, the Assistant proposes, in a non- obtrusive window the user is free to ignore, sequences of actions the user might want to redo. Both of them operate without explicit intervention of the user. The Apprentice and the Assistant have been developed in Smalltalk 4.02.

Figure 1 shows a snapshot of a Smalltalk screen including an assistant window in which the Assistant makes a sug- gestion triggered by the opening of an exception window.

2.1 Monitoring User's Actions  We define an action to be any interaction between the user and the interjiace that affects an interjiace tool. By interface tool we mean a software component of the Smalltalk envi- ronment (browsers, debuggers, inspectors and editors). We  'Objectworks Smalltalk, copyright Parc-Place s y s - tems.

'We are currently working to adapt our agents to the newest version of Smalltalk  20 1    Figure 1 : The user has executed a program that has raised an exception. The Assistant window displays a suggestion. It offers to open, move and resize a debugger. If the user accepts the suggestion by clicking on it, the Assistant will automatically open a debugger, move and resize it as the user uses to do  naturally represent actions with Smalltalk objects. For ex- ample, class Act ionMenu models selection of an item in a menu , class ActionList models selection of an item in a list, class Actionselect models text highlighting, class ActionError models the opening of an error noti- fication window and class Act ionBut t on models mouse click on a button. Each class of actions defines instance variables to store parameters for the action. In the follow- ing, we will note actions Class (Tool, Parameter).

Let us call truce the ordered collection of all the actions the user has performed during a work session. The figure 2  . . .

ActionSelecL(aStringHolder,'anObject cass') ActionMenu(aStringHolder,doIt) ActionError(ni1,doesNotUnderstand) ActionMenu(aDebugger,debug) ActionMenu(aDebugger,move) ActionMenu(aDebugger,resize) ActionList(aDebugger,learn) . . .

Figure 2: A sample of the trace  shows an example of a trace whrre the user opens a debug-  ger to correct an error.

2.2 The learning task  Our Assistant should be able to automatically select and propose sequences of actions that the user might want to redo. It has to detect situations in which these repetitive tasks are not fulfilled and it has to avoid them by offering to automate them. These repetitive tasks are all the - ex- act, noisy or unordered, both noisy and unordered - repeti- tive sequences of actions of the trace. The task of the Ap- prentice is to build knowledge that precisely characterize the situations in which these repetitive sequences should be proposed to the user.

Let us call situation, for such a repetitive sequence, the last n actions3 of the trace that immediately precede it. For a given repetitive sequence, there are as many situations as occurrences of this sequence. To characterize these situa- tions, we make the hypothesis that they may be different occurrences of a few situation patterns which characterize them. Therefore, in the machine-learning framework, the task of the Apprentice can be seen as a concept learning problem where each exact repetitive sequence of the trace is a concept c which training examples are all the pairs of  3The value of n clearly depends on the application field, and is called description length     the form 4 s ,  c +, where s is a situation associated to c.

The Apprentice has to induce the general definitions of sit- uation patterns given a set of training examples.

Let R be a repetitive sequence of actions, and let AI, A2, A3 and lowercase letters from a to f denote actions.

We can distinguish 3 kinds of interesting situation patterns:  1. Noisy: For instance, the 2 training examples 4 AlaAzbcA3,R + and + AldAzefA3,R + of the trace " ... AlaAzbcA3R ... AldA~efA3R ..." can be characterized by the situation pattern A1 * A2 * *A3 where the stars denote differences called errors or noise on the whole actions or on the values of the at- tributes of the actions.

2. Unordered: the training examples 4 A1A3A2, R +, 4 A2A3A1, R + and 4 A3A2A1, R + can be char- acterized by the pattern (AlAzA3).

3. Noisy and unordered: 4 AlaA2,R + and 4 A2bA1, R + can be characterized by the pattern {A1 * A2).

Figure 6 shows such examples of situation patterns.

The 3 kinds of patterns detailed above can be seen as 3 different kinds of hypotheses from 3 different hypothesis spaces which suppose 3 different description languages of the training example and the hypotheses. Therefore, the task of the Apprentice is to find in this different hypothe- sis spaces, the hypotheses that best explain the membership of each training example (situation) to the related concept (repetitive sequence).

3 IBHYS: the Instance-based Hypothesis Search approach  3.1 A formal framework  The general framework of our work is called inductive con- cept learning. Let C = {c l ,  c2, ..., cp} denote a set of con- cepts and & = E1 U 1 2  U ... U EP a set of training examples.

A training example is a pair of the form 4 x, c + where x is the description of the example and c E C the concept to which the example belongs. Our approach aims at ac- quiring the general definition of each concept ci E C from the set Ei of positive examples and the set E - Ei of neg- ative examples. Precisely, it builds approximations, called hypotheses, of each concept ci. Let U be the result of the learning process, i.e. the set of hypotheses that actually ap- proximate c1, 122, . .. , cp. A hypothesis can be seen as a set of constraints on the descriptions of the training examples. A hypothesis h is said to match a training example 4 x, c t, if x satisfies all the constraints of h (conversely, 4 x, c + is said to satisfy h). Besides, h explains the membership of a positive example 4 x, c + if h matches 4 z, c + and h  aims at approximating c. Our approach has two important advantages.

First, it does not explore a hypotheses space but builds local approximations4 of the concepts of C by letting each training example 4 x, c + choose the most relevant hy- potheses that correctly explain its membership to c. To do so, some hypotheses are successively submitted to + x, c >-. Using an evaluation criterion ([5]), 4 x, c + is able to compute the relevance of a hypothesis h for the concept c. As a consequence, a training example + x, c + has to keep xx the set of the most relevant hypotheses that cor- rectly explain its membership to e.

Second, hypotheses of U can be expressed in different description languages. The way the hypotheses are treated in the algorithm, described in section 3.2, is independent from their description; and the evaluation of :he relevance of the hypotheses is only based on the number of training examples they match. A hypothesis h keeps two impor- tant values: 1) na, the numbers of training examples that h matches in each c E C (these values are used by the training examples to measure the relevance of h). 2) T;,  the number of elements of E that judge h relevant (this attribute allows to remove from 3t a hypothesis that no training example has chosen).

Finally, let us describe the three following operators:  MATCH: U x & -+ IB Data: a hypothesis h, a training example  + z,c + Result: true if h matches 4 x ,c  +, and  false if h does not This is the classical subsumption operator. As stated above, our approach allows to handle sevecal hypothe- ses description languages, so the MATCH operator is strongly linked to these description languages. In fact, MATCH can be seen as a filter among several match- ing operators, one per description language: MATCH(^, 4 x, c +) = Matchi(h, 4 x, c +) where h is expressed in the ith description language, and Match, is the matching operator of this language.

SUBMIT: U x E x  IR -+ Void Data:  Result:  a hypothesis h, a twining example % x, c +, a threshold k updates the set of 23.1 regarding the hypothesis h  The training example 4 x , c  + computes the rele- vance of the hypothesis h (using [5]) for the concept c and updates its set xx. The hypothesis h may be: 1) relevant for + x, c + (h matches + x, c +), aitd is added to xx; 2) irrelevant for 4 z, c + and 4 x, c +  431 is a global approximation of the concepts of C, and each hypothesis of 31 is a local approximation of a concept of C     3.

3.2  We  rejects it. 4 x, c > may possibly remove the less rel- evant hypothesis in xz if it must keep the size of xx constant. Besides, r; is updated.

Regarding to the application field, the threshold IC can be used either to bound the size of xu, or to set the minimum relevance accepted to add any hypothesis in xx.

HYPGEN: & x 2 O  3 2 H y p Data: a training examplc x, c >, a set  Result: a set of hypotheses of objects 0  HYPGEN is the hypotheses generation operator. H y p denotes the space of all the hypotheses that can be generated. HYPGEN is strongly linked to the appli- cation field, and allows hypotheses to be formed by comparison between a training example 4 x, c > and a set of objects 0. Hypotheses can be generated by comparison with other training examples, hypotheses, or any other objects useful for the hypotheses gener- ation (in the procedure described after, 0 = & U 3-1).

The main advantage of our approach is that it makes possible to handle simultaneously several different de- scription languages of the hypotheses. HYPGEN can be seen as the combination of several hypothesis gen- erators, one per description language:  HYPGEN(+ x , c  > ,O)  = HypGen,(+ x ,c  + 7 0) where HypGen, is the hypothesis generator for the ith description language. Suppose 0 = &. Suppose the training examples are described both by directed graphs and conjunctions of attribute-value pairs. A simple example of hypothesis generator outputs the maximal tree included in < x, c + and in all the de- scription of the element of 0. Another hypothesis generator compares + z, c + to all the elements of 0.

For each pair (< x, c +,+ y,  d +) it returns a hypoth- esis which description is constituted of the attribute- value pairs that x and y shares in common.

One of the interest of using multiple hypothesis gen- erators can be evaluated easily. Suppose that train- ing examples are described with n boolean features.

This leads potentially to an hypothesis space of 2" ele- ments. Suppose now, this set of n features can be split in 3 disjoint sets of n / 3  features. These 3 sets lead to 3 hypothesis spaces of 2"13, and 3 sr 2"13 << 2n  A General Algorithm  following, & is the set of training examples currently avail- able, 3-1 the set of the currently relevant hypotheses, and the threshold k is used in the SUBMIT operator. Given a new training example + x, c +, the procedure updates the sets & and 31.

The most important steps of the IBHYS procedure are:  Step 1 Each hypothesis of 3-1 have to know the number of training examples it matches in each class. These val- ues allow the training examples to evaluate the rele- vance of the hypotheses of 3t.

Step 2 The numbers of training examples matched by each hypothesis have been modified (step 1). Some hy- potheses that were relevant for a training example y E & may not be relevant any more. This may happen if the hypotheses matching y are all almost as relevant as each other.

Step 3 Using + x, c >, & and 3t, the operator HYPGEN generates a set of hypotheses that will be individually studied only if they are not yet in 3-1. Whatever the de- scription languages of the hypotheses are, all the gen- erated hypotheses will be dealt with in the same way (described in the steps 4 and 5).

Step4 Each hypothesis generated by HYPGEN have to know the amount of training examples it matches in each class, allowing the training examples to evaluate i IS relevance.

Step 5 The currently studied hypothesis h must be evalu- ated by the training examples of & to measure its rel- evance. For each training example 4 y,  c +E E ,  this step updates the set y x  of its relevant hypotheses ( a d the value r;).

Step 6 This step aims at removing the irrelevant hypothe- ses of 3t. A hypothesis h is said to be irrelevant if none of the training examples of E has chosen it (i.e.

r; = 0).

,  3.3 The Apprentice algorithm  The Apprentice learns user's habits every loo5 actions of the user. It first searches the training examples that appear in the last 100 actions of the trace, and invokes the pro- cedure NewExample for each new training example it has found.

We defined 3 operators (taking 2 situations in input) to allow the Apprentice to learn the 3 kinds of situation pat- - terns defined-above: noisy which builds a pattern which has the commune characteristics, regarding the position, of now give an Of rhe IBHYS approach  through a general procedure, called NewExample. The main steps of this procedure are explained below. In the 'Again, this value depends on the application field.

I  \ \  Submit the hypothesis to the examples.

foreach + y, d +e E do SUBMIT(h,+ y, d +,k);  I\ Insert h in the set of hypotheses.

7-l c 3-1 U {h};  \ \  Delete of 3-1 the hypotheses \\ witch are not relevant.

foreach h E 3-1 do  \ \  Update the number of examples \ \  matched by each hypothesis.

foreach h E 31 do  if MATCH(h,+ z, c +) then L n; t n; + 1;  & + E U { < x , c + } ;  \ \  Insert < x, c + in the set of examples.

the two situations and stars (denoting noise) for their differ- ences, unordered: which returns the set of the actions of the first situation, if and only if, these actions all appear in the second situation, with no constraint on their positions, and noisyunordered. We also defined 3 classes of hy- potheses that define their own method MATCH to test cov- erage of training examples, and 3 hypothesis generators, based on the 3 operators defined above.

Let us call knowledge base the set of hypotheses pro- duced by our IBHYS algorithm. After each action of the user, the Assistant inspects the knowledge base and se- lects all habits which hypothesis cover the last actions of the user. The Apprentice then displays suggestions corre- sponding to the related concepts in an non-obtrusive win- dow. The user is free to take it into account or not. A sim- ple mouse-click on one of these suggestions automatically performs the related actions.

4 Experimental results  The Apprentice and the Assistant are currently used by the first author. Experiments reported here where conducted during the development of an ?ASCII to HTML? translator, on real data. We compare our IBHYS algorithm to ID3 which is the decision tree algorithm used in [ 141 to ?explore the potential of machine-learning methods to automatically create and maintain ... customized knowledge for personal software assistants?.

Time 450 1 50  0) 6 . .8 10 12 14 Description Length  lbhys ID3 +-- -  Figure 3: Computing time in minutes versus de- scription length I L ifr,h=Othen?He?H-hh;  Figure 3 and 4 show that IBHYS outperforms ID3 re- - end garding the computing time. The figure 3 plots the comput-  ing time versus the description length (cf. 2.2) on a trace of 1000 actions, and the figure 4, the computing time versus the size of the trace, for a description length of 10 actions.

Time is given in minutes.

Trace  Figure 5: Accuracy  1 2 3 4 Accuracy Excess Hypotheses  I & I ID3 I Ibhys ID3 I Ibhys ID3 I Ibhys  Time 450   300 350 t -  - t  - t -  Figure 4: Computing time in minutes versus trace length  Table in figure 5 is a direct comparison of the respec- tive accuracies of IBHYS and ID3. These tests were per- formed on a trace of 1000 actions, with 1 x~ I= 2 (see SUBMIT in section 3). These 1000 actions were split in a training set and a test set. The leftmost column lists the size of the training sets used, and column 1 lists the number of training examples (repetitive sequences) the algorithms have found in the training sets. Column 2 shows the predic- tive accuracy on new examples. It shows that IBHYS had correctly predicted a repetitive sequence in 52.58% of the case, versus 46.13% for ID3. Column 3 lists the ?excess rate? that is, the number of time the algorithms have pre- dicted erroneous repetitive sequences whereas no predic- tion were expected. This excess rate value is very impor- tant. Hight values means that the agent constantly bothers the user with useless suggestions. IBHYS and ID3 have almost the same excess rate. Finally, column 4 lists the number of hypotheses the algorithms have learnt. Note that both IBHYS and ID3 have an average decision time of 10 milliseconds.

Due to the fact that the Apprentice and the Assistant  have been implemented in Smalltalk 4.0, and that few pro- grammers still use this environment, we could not find pro- grammers to intensively test our agents. Of course, we are working to adapt our agents to the newest version of the Smalltalk environment. However, we can give examples of the habits learnt during the experiments reported here (fig- ure 6). Habit 1 means that the user systematically moves and resizes a debugger he has opened after an error; habit 2 shows that the user systematically removes system com- ments of new methods.

5 Conclusion  We have proposed a new approach, called IBMYS, and an incremental algorithm with low computing time, for induc- tive concept learning, particularly suited for learning inter- face agents. This approach lets each training example build a set of hypotheses that locally approximate the global tar- get function, limiting the hypothesis search to a small por- tion of the hypothesis space. Because training examples can choose among several description languages to form an hypothesis, and different description languages to form different hypotheses, it allows to handle simultaneously hy- potheses described in different languages. We presented an application of this approach to learn user?s habits of inter- active programming environments and propose an original assistance to programmers based on two software agents, the Apprentice and the Assistant. We showed, with exper- imental results on real data, that IBHYS outperforms ID3 both in computing time and predictive accuracy.

IBNYS seems a promising approach for data-mining.

Further studies will be conducted to evaluate our IB- HYS approach with respect to standard (Irvine collection) machine-learning datasets. In the context of the Appren- tice and the Assistant, an important limitation of IBHYS is that it bounds in advance the length of the description and, therefore, the length of the situation patterns searched. We are investigating to bypass this limitation.

We are currently working to adapt the Apprentice and     1.

S i t u a t i o n  p a t t e r n  R e p e t i t i v e  sequence ActionErreur(ni1, * )  ActionMenu(aDebugger,move) ActionMenu(aDebugger,debug) ActionMenu(aDebugger,resize)  Figure 6: Example of user?s habits  2.

the Assistant to  the newest version of the Smalltalk envi- [ 1 11 P. Maes. Agents that reduce work and information overload.

ActionSelect(aBrowser, message selector and argument names ActionMenu(aBrowser,cut) ?comment stating purpose of message? I temporary variable names I statements)  ronment. We hope that they will be soon available to full time programmers for intensive tests.

Communications of the ACM, Special Issue on Intelligent Agents, 37(7):31-40, July 1994.

1121 P. Maes. Social interface agents: Acquiring comoetence by  Acknowledgements  We would like to thank Christophe Fiorio for his Algo- rithm LaTeX style arld his help in the preparation of the final manuscript of this paper.

