Space-Efficient String Mining under Frequency Constraints

Abstract  Let D1 and D2 be two databases (i.e. multisets) of d strings, over an alphabet ?, with overall length n. We study the problem of mining discriminative patterns between D1 and D2 ? e.g., patterns that are frequent in one database but not in the other, emerging patterns, or patterns sat- isfying other frequency-related constraints. Using the al- gorithmic framework by Hui (CPM 1992), one can solve several variants of this problem in the optimal linear time with the aid of suffix trees or suffix arrays. This stands in high contrast to other pattern domains such as item- sets or subgraphs, where super-linear lower bounds are known. However, the space requirement of existing solu- tions is O(n logn) bits, which is not optimal for |?| << n (in particular for constant |?|), as the databases themselves occupy only n log |?| bits.

Because in many real-life applications space is a more critical resource than time, the aim of this article is to re- duce the space, at the cost of an increased running time. In particular, we give a solution for the above problems that uses O(n log |?|+ d logn) bits, while the time requirement is increased from the optimal linear time toO(n logn). Our new method is tested extensively on a biologically relevant datasets and shown to be usable even on a genome-scale data.

1. Introduction  In many applications, e.g., in computational biology, the goal is to find interesting string patterns that discriminate well between two classes of data. Application areas are, among others, finding discriminative features for sequence classification or segmentation [4], discovering new binding motifs of transcription factors [6], or computation of the classical ranking scores in Information Retrieval [3].

?Funded by the Academy of Finland under grant 119815.

In this paper, we focus on string mining under fre- quency constraints, i.e., predicates over patterns depending solely on the frequency of their occurrence in the data [13].

This category encompasses combined minimum/maximum support constraints, constraints concerning emerging sub- strings, and other constraints concerning statistically signif- icant substrings. While most of these problems have their motivation in itemset mining [1], data miners also consider them for the domain of strings [8], as plenty of naturally occurring data can be modeled as strings (biological se- quences, MIDI-data, etc.).

We concentrate on the fundamental instance of exact substring patterns, where optimal linear time algorithms can be obtained, which stands in high contrast to other pattern domains such as itemsets or sub-graphs, where super-linear lower bounds are known [33, 9]. Much of the related work in the domain of strings studies more complicated pattern classes, where the search space is typically of exponential size, and the objective is to optimize the time needed per each output element satisfying the frequency constraints, see e.g. [28, 2] for recent results on this line of research.

Our objective is to provide practical tools for the mining of very large data sets, such as the genome-scale sequences of molecular biology. For such data sets, one needs to pay special attention to the space usage. Even if the algorithm takes linear space proportional to the overall length n of se- quences in the database, this may be too much: Measured in bits, a data structure having O(1) integers per text char- acter occupies asymptotically O(n logn) bits, whereas the database can be stored in n log |?| bits, where ? is the un- derlying alphabet. Especially on DNA sequences (where |?| = 4) this is a significant difference.

1.1. Contributions of our work  In this paper, we show that frequency constrained min- ing tasks on exact substring patterns can be solved in much less space than previously known. We improve the known O(n log n) bits space usage into O(n log |?|+ d logn) bits   DOI 10.1109/ICDM.2008.32    DOI 10.1109/ICDM.2008.32     with a logarithmic penalty in computation time against the optimal linear time algorithm [13]. Here, d << n is the number of strings in the databases. We emphasize that our algorithmic framework is general enough to handle all data mining tasks whose predicates are based on the frequency of strings alone (e.g., frequent substrings, emerging sub- strings, strings passing the ?2-test, . . . ); this approach is much more general than designing individual solutions for each of these tasks seperately.

We have also tested our method empirically on realistically-sized data sets from computational biology and shown that in practice space is reduced by a factor of 6?7 compared to the optimal algorithm [13], while the running time is increased by a factor of about 80?90. Given that users are usually willing to wait longer if they can handle larger data sets in exchange, the increase in running time is compensated by the fact that due to the use of more input data (which is nowadays available in fast- evolving domains such as computational biology), the results of the mining tasks will be more significant in practice.

Sadakane [31] gives another succinct version for calcu- lating frequencies. However, his problem setting is quite different from ours, as he designs a succinct index that al- lows to answer frequency queries for a given pattern. Our work, on the other hand, is situated in the field of data min- ing, where the goal is to extract interesting strings from sta- tistical constraints alone. Because Sadakane?s index needs O(n logn) bits at construction time [31], we cannot use it for our task, as it would result in no advantage at all over the non-succinct version. Moreover, our algorithm can be modified to give a space-efficient algorithm to build a part of Sadakane?s succinct index.

1.2. Outline  In the following, we first give the formal definitions of the mining tasks we consider (?2). In ?3, we explain the existing optimal algorithm. Then we show how the optimal algorithm can be carefully re-engineered to use less space (?4). The paper continues with other space/time-tradeoffs that can be obtained for the problem (?5), and with appli- cations to Sadakane?s succinct index for storing document frequencies. Experiments are reported in ?6.

2. Preliminaries  For a finite ordered alphabet ?, a (text) string T ? ?? is a chain T1 . . . Tn of letters Ti ? ?. Here, ?? is the set of all strings over ?. We write Ti..j to denote the substring of T ranging from position i to j. We use |Ti..j| to denote the length j ? i + 1 of Ti..j . Substrings T1..j , 1 ? j ? n, are called prefixes and substrings Ti..n, 1 ? i ? n, are called suffixes of T . For strings ?, ? ? ?? we write ? ? ?  if ? is a substring of ?. Then lcp(?, ?) gives the length of the longest common prefix (lcp) of ? and ?. For exam- ple, lcp(aab, abab) = 1. When clear from the context, we also use lcp for the longest common prefix itself (not its length). Given a multiset D ? ?? with strings over ? (called database), we write |D| to denote the number of strings in D, and ?D? to denote their total length, i.e., ?D? = ???D |?|. We define the frequency of a pattern ? ? ?? in D as follows:  freq(?,D) := |{d ? D : ? ? d}|  Note that this is not the same as counting all occurrences of a ? in D, because one database entry could contain multiple occurrences of ?. In data mining applications it is important to use this definition of frequency, as one is usually looking for patterns that are highly relevant for the whole database, and not only for one or a few of its entries.

Now the support of a pattern ? ? ?? in D can be defined as  supp(?,D) := freq(?,D)|D| .

The first example problem that can be solved with our method is as follows (cf. [1]).

Problem 1 Givenm databases D1, . . . ,Dm of strings over ?, and m pairs of support thresholds (?i, ?i)i=1,...,m sat- isfying 0 < ?i ? ?i ? 1 for all i, the Frequent Pattern Mining Problem is to return all strings ? ? ?? that satisfy ?i ? supp(?,Di) ? ?i for all 1 ? i ? m. ?  As another example mining problem that can be solved with our algorithm, we consider a 2-class problem for a (positive) database D1 and a (negative) database D2. For this, we define the growth-rate from D2 to D1 of a string ? as  growthD2?D1(?) := supp(?,D1) supp(?,D2) , if supp(?,D2) ?= 0 ,  and growthD2?D1(?) = ? otherwise. The following def- inition is motivated by the problem of mining Emerging Itemsets [10]:  Problem 2 Given two databases D1 and D2 of strings over ?, a support threshold ?s (1/|D1| ? ?s ? 1), and a minimum growth rate ?g > 1, the Emerging Substrings Mining Problem is to find all strings ? ? ?? such that supp(?,D1) ? ?s and growthD2?D1(?) ? ?g. ?  The patterns satisfying both the support- and the growth- rate condition are called Emerging Substrings (ESs). ESs with an infinite growth-rate are called Jumping Emerging     #2#1  #1 #2 #1  #2  #2 #1  #2 #1  #2  #2 #1  #2  #4  #3 #4  #3 #4 #4  #3 #4  #3 #4  #3  #3  #3  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  LCP=  5 12 18 23 4 22 8 9 1 10 2 6 15 19 11 17 3 21 7 14 16 20  0 0 0 0 0 1 1 2 3 1 2 3 2 3 0 1 1 2 2 2 1 2 3  a b a a a ba a b a b b a b b a b b a   a a a a a b  a a b  a a b a  a b  a b a  a b a a a b  a b b  a b b a  b b b a  b a  b a a a b  b a b b  b b  b b a  b b a b b  SA=  T=  Figure 1. The suffix array for T and its lcp-table. Below position i we draw the string TSA[i]..n until reaching the first end-of-string marker. The solid line going through these strings indicates the lcp-values.

Substrings (JESs), because they are highly discriminative for the two databases.1  Example 1 Let D1 = {aaba, abaaab}, D2 = {bbabb, abba}, ?s = 1, and ?g = 2. Then the emerging substrings from D2 to D1 are aa, aab and aba. These are also the JESs. ?  2.1. Suffix- and lcp-arrays  This section introduces two fundamental data structures that we need for our algorithm.

The suffix array SA (see [26]) for T describes the lex- icographic order of T ?s suffixes, in the sense that it ?enu- merates? the suffixes from the smallest to the largest. More formally, SA[1, n] is an array of integers s.t. its entries form a permutation of [1 : n], and TSA[i]..n is lexicographically less than TSA[i+1]..n for all 1 ? i < n. The generalized suffix array of two databases D1 = {?1, . . . , ?|D1|} and D2 = {?1, . . . , ?|D2|} of strings is simply the suffix array built on top of the concatenated string  T := ?1#1 . . . ?|D1|#|D1|?1#|D1|+1 . . . ?|D2|#|D1|+|D2|.

The #j?s are (conceptual) new characters (lexicographi- cally smaller than other characters) to separate the individ- ual strings. See Fig. 1 for an example, which builds on the databases D1 and D2 from Ex. 1.

The suffix array for T can be computed inO(n) time, ei- ther indirectly by constructing a suffix tree for T , or directly with some recent methods, e.g. [21].

Instead of suffix array SA we can use a compressed suf- fix array [27]. Different tradeoffs between space and access  1Notice that to reduce the size of the output, it is customary to con- sider so-called left-maximal and right-maximal strings (those that cannot be extended to any direction to obtain another string satisfying the thresh- olds). For conciseness, we do not consider these enhancements. How- ever, right-maximality is trivial to accommodate in our algorithms, and left-maximality requires more care, but can be solved within the same time/space bounds.

time to SA[i] are possible, e.g. one can access any SA[i] within time tSA = O(log? n) for an arbitrary 0 < ? ? 1, us- ing an index of sizeO(n(H0 +log log |?|)) = O(n log |?|) bits [15, 29]. Here H0 = H0(T ) ? log |?| is the 0?th order empirical entropy of the text T (lower bound for the average number of bits needed to code a symbol using a fixed code table).

The lcp-array LCP[1, n] for T is defined by LCP[i] = lcp(TSA[i]..n, TSA[i?1]..n) for all 1 < i ? n, and LCP[1] = 0. That is, LCP contains the lengths of the longest com- mon prefixes of T ?s suffixes that are consecutive in lexico- graphic order. Kasai et al. [20] gave an algorithm to com- pute LCP in O(n) time. Sadakane [30] shows that repre- senting the LCP-array can be done using 2n + o(n) bits (and constructed within that space), while accessing LCP[i] then takes O(tSA) time.

2.2. Range Minimum Queries  Another tool we need for our approach is a prepro- cessing of the lcp-array such that range minimum queries (RMQs) can be answered in constant time. The reason for using RMQs on LCP is that they generalize the lcp- array, in the sense that we can compute the lcp between arbitrary suffixes, and not only between those that are lex- icographically adjacent. Formally, for two given indices i and j the query RMQLCP(i, j) asks for the position of the minimum element in LCP[i, j], i.e., RMQLCP(i, j) := argmink?{i,...,j}{LCP[k]}. We return the smallest index if the minimum is not unique.

Lemma 1 Let T ? ?? be a text and LCP be the lcp- array for T . Then for all 1 ? i < j ? |T |, lcp(TSA[i]..|T |, TSA[j]..|T |) is given by LCP[RMQLCP(i + 1, j)].

This follows immediately from the definition of the lcp- array. Stated differently, Lemma 1 says that the i?th-     and the j?th-smallest suffix of t are equal in exactly their LCP[RMQLCP(i+ 1, j)] first characters.

It is well known that a linear preprocessing of any input array A is sufficient to find RMQA(i, j) in time O(1). This method has recently been refined to use only 2n + o(n) bits in addition to the input array, also at construction time [12]. With the succinct representation of the lcp-array, we thus need 4n + o(n) space to answer arbitrary lcp-queries in O(tSA) time.

3. Basic Mining Algorithm  This section reviews the basic algorithm for computing the string frequencies. It is a tight integration of Kasai et al.?s algorithm for visiting all branching2 substrings of a text [20], and Hui?s color set size technique [18]. Note that it is enough to visit all branching substrings, as by definition the frequencies of all other strings are equal to the frequency of their longest branching prefix. From now on, let T denote the string formed from the (positive and negative) databases D1 and D2 as explained in ?2. Let d denote the total number of strings in the databases (d = |D1| + |D2|), and n denote T ?s length (n = ?D1? + ?D2? + d).

3.1. Visiting All Branching Substrings  First, we summarize the algorithm for visiting all branch- ing substrings [20]. The idea is to simulate a depth-first traversal of the (virtual) suffix tree, solely by means of the suffix- and lcp-array. This works by visiting the leaves of the suffix tree in lexicographic order (i.e., in the order of the suffix array), keeping on a stack R just the rightmost path of the part of the suffix-tree that has been seen so far. Step i first deletes the elements from R that are removed from the rightmost path, and then inserts new elements to R. The details are as follows.

Consider step i+1 of the algorithm, so we are just about to visit suffix SA[i+ 1] (see also Fig. 2). The stack R con- tains the lengths of the prefixes of TSA[i]..n that are branch- ing (the so-called string-depths of nodes on the rightmost path). Then we pop all elements from R whose string- depth is greater than LCP[i + 1], because LCP[i + 1] is the string-depth of the lowest common ancestor (lca) v of the leaves represented by SA[i] and SA[i + 1]. If v is not already present in R, we push it on R (with string-depth LCP[i+ 1]). Finally, we push SA[i+ 1] on R (with string- depth n? SA[i+ 1] + 1). It is shown in [20] that this algo- rithm visits all branching substrings of T . (The basic insight is that every internal node is the lca of at least one pair of leaves.)  2A substring ? ? T is called branching if there exist a, b ? ?, a ?= b, s.th. both ?a and ?b occur in T . These are exactly the strings that correspond to an internal node in the (virtual) suffix tree of T .

pop  (push)  push  SA[i+1]  vlca=  SA[i]  Figure 2. Going from suffix TSA[i]..n to TSA[i+1]..n when visiting all branching sub- strings. Solid nodes are on the stack, dashed nodes not yet. v is pushed if necessary, leaf SA[i+ 1] is always pushed.

3.2. Calculating Frequencies of Branching Substrings  Let us now describe Hui?s approach [18] to calculate freq(?,D) for all branching substrings ? of D. The idea is to calculate two counters S(?,D) and C(?,D) for each ? (simply S and C if clear from the context), such that freq = S ? C. S counts the number of all occurrences of ? in D, and C counts the number of duplicates of ? in the same string in D. More formally, S(?,D) =? ??D |{i ? [1 : |?|] : ? = ?i..i+|?|?1}|, and C(?,D) =? ?:S(?,D)>0(S(?,D) ? 1).

For what follows, we need to define an additional array  D[1, n] on top of the generalized suffix array such that D[i] gives the string number where suffix TSA[i]..n points to, i.e.

D[i] = j if the first string separator in TSA[i]..n is #j . By remembering the number h of the last string in D1, D also allows to infer whether a suffix points to D1 or D2.

The S-counters are easy to calculate during the simu- lated suffix-tree traversal: simply initialize them correctly for leaves, and when popping a node v from the stack, add v?s S-value to its parent-node on the stack. More formally, when pushing a leaf l = TSA[i+1]..n on R, we initialize S(l,D1) with 1 and S(l,D2) with 0 if D[i + 1] ? h, or vice versa if D[i + 1] > h. When popping a node u from R, we add S(u,Dj) to S(w,Dj) to the direct ancestor w of u for j = 1, 2. Note that w is either the predecessor of u on R (if the string-depth of this predecessor is ? LCP[i + 1]), or the newly pushed internal node v (otherwise).

Calculation of theC-counters is a little bit more intricate.

We will just describe how to calculate C(?,D1); the ideas forD2 are similar. Hui?s key insight is that if a substring ? is repeated within a string d ? D1 from D1, then ? must be a prefix of the lcp of two different suffixes from d. Even more, if we enumerate d?s suffixes in any order for all d ? D1 (say     RMQ# #j j  v?  v  propagate counters  Figure 3. Determining the C-counters. Node v? represents the longest common prefix of two suffixes from the same string j, so C[v?] has to be increased by 1.

in the order they appear in SA), then the number of times that ? is a prefix of the lcp of consecutive suffixes (in that order) gives C(?,D1).

With Lemma 1, this gives us all the tools we need to cal- culate the C-counters ?on the fly? while visiting all branch- ing substrings (see also Fig. 3): remember the position of the previous suffix of string j before position i for each j ? [1 : d] in an array P [1, d] (i.e., P [j] = max{p ? i : D[p] = j}). Then when at position i + 1, calculate the desired lcp as l = LCP[RMQLCP(P [D[i + 1]] + 1, i + 1)], and increment by 1 the C-counter of the node v? on R that has string-depth exactly l. (Note that such a node must be on R, as it is on the path from SA[i + 1] to the root.) The easiest way to find v? in R is to use another array of size n.

Like with the S-counters, when popping a node from R, we need to add the C-counters to its parent node. This step takes care of the fact that the RMQs from the above para- graph just locate the longest duplicates; but all prefixes of duplicates are duplicates as well.

3.3. Determining Interesting Patterns  In total, the integration of the above two techniques im- plies that when a node v is popped from the stack, the fre- quency of the respective substring is given by freq = S?C.

From this, we check if the string passes the frequency-based predicate (e.g., if it is an emerging substring). If so, we out- put TSA[i+1]+d?1 for all values d between the string-depth of v (inclusive) and that of its parent-node (exclusive).

4. Space-Efficient Version  The problem with the algorithm from the previous para- graph is that it still needs O(n log n) bits of space in the worst case, even if we take compressed representations of  suffix- and lcp-arrays. This is because for degenerated suf- fix trees, the rightmost path could containO(n) nodes from the suffix tree; hence the space for stackR and all the coun- ters would be O(n log n) bits. We show in this section how to achieve O(n log |?| + d log n) bits of space.

4.1. New Data Structures  We now step through the data structures from ?3 and show how to reduce the space for each of these.

4.1.1 Representing D and P  We use array P as is, but we will represent array D im- plicitly. Array P occupies d logn bits. During the algo- rithm one needs to inquery P [D[i + 1]] when inserting the (i+1)-th suffix array element, after which one needs to up- date P [D[i+ 1]] = i+ 1. Value D[i+ 1] can be computed in time O(tSA) as follows: Store a bit-vector B that marks the boundaries of documents in the concatenation T by set- ting B[j] = 1 if position j starts a new document in T , otherwise B[j] = 0. Preprocess B for rank-queries, where rank(B, j) gives the number of bits set in B[1, j]. That is, rank(B, j) gives the document number in which the j- th position in T belongs to. It is possible attach to B an auxiliary structure of size o(n) bits so that rank(B, j) can be answered in constant time for any j [19]. Now we have D[i+ 1] = rank(B,SA[i + 1]). Using compressed suffix array, the computation of SA[i + 1] takes time O(tSA) and the rank-query on B takes constant time. The space used in addition to the already used compressed suffix array is n+ o(n) bits for the bit-vectorB and its rank-structure.

4.1.2 Representing R  Stack R needs more functionality than being accessed only from top, as in order to increase the correct C-counter as described in ?3.2, we need to quickly find the node on R with string-depth l = LCP[RMQLCP(P [D[i+1]]+1, i+1)].

Thus, storing R as a difference-encoded list in O(n) bits [16] would result in having to scan R in O(n) time in the worst case after each RMQ.

Instead, we represent R via a dynamic succinct data structure for searchable partial sums. This is a data struc- ture maintaining a sequence of symbols A = a1 . . . am, supporting the following operations:  ? sum(A, i): returns ?j?i aj ? search(A, j): returns the smallest i such that  sum(A, i) ? j ? update(A, i,?): adds ? to ai (? = O(polylog(n))) ? insert(A, i, x): inserts x between ai and ai+1     ? delete(A, i): deletes ai It has been shown in [25] that each of these operations can be supported in O(log n) time, by using an extension of the data structure from [5]. The space occupied by this data structure is only n + o(n) + O(m) = O(n) bits, provided that the sum of the numbers in A is always ? n.

In our case, if the elements in A represent the number of letters on the incoming edges of the nodes on R (and 0 for the root of the suffix tree), then the condition  ?m i=1 ai ? n  is naturally satisfied (because the longest suffix has length n). A query sum(A, i) gives the string-depth of the internal nodes (needed for popping all nodes with a string-depth ? LCP[i+1]). The index (onR) of the node with string-depth l can be found by r = search(A, l).

Note that we do not need the full functionality of the dynamic searchable partial sum structure, as the function update() is not used at all, and we only have to insert and delete at the end of the sequence (corresponding to pushes and pops on R).

4.1.3 Representing C-counters  The C-counters (for counting the duplicates in a string) are also stored in a searchable partial sum data structure (see the previous section). This time, we only need the functions insert (when a new node is pushed on the stack), delete (when a node is popped), and update(A, i,?) (with ? = 1 when updating ar after an RMQ, or with ? being the C- value of the node that has just been popped from the stack).

This structure needs again n+o(n)+O(m) = O(n) bits if we can assure that that the sum of all C-counters on the stack is always less than n. But this is true, as a C-counter of u is added to its parent node if and only if u?s subtree has been traversed completely. So each suffix can contribute at most 1 to the set of all C-counters, hence the bound on their sum.

4.1.4 Representing S-counters  The S-counters (for counting the total number of occur- rences of a string) are easier to handle, as they only need to be accessed from top of the stack. The only prop- erty we need to know is that the sum of the S-counters is never greater than n, as they ?cover? disjoint sub-arrays in SA. Thus, we can encode them with variable-length prefix codes, e.g., Elias-?-code [11]. This takes again n+ o(n) +O(m) = O(n) bits, while supporting deletions and insertions at the ends in O(1) time.

4.2. Space and Time Analysis  The Compressed Suffix Array takes O(n(H0 + log log |?|)) bits of space. Compressed LCP and RMQ val-  ues take overall 4n + o(n) bits of space. The database oc- cupies n log |?| bits. Array P takes d log n bits. Bit-vector B and its rank-structure take n+ o(n) bits.

Interesting points are the peak space consumption of the data structures we use and their construction time.

The Compressed Suffix Array can be constructed using O(n log |?|) bits of space in O(n log log |?|) time [17], or even within space of the final structure [23] but us- ing O(n log n) time. Once the Compressed Suffix Ar- ray is given, the lcp-representation can be constructed in O(n log? n) time using no extra space in addition to the fi- nal structure [16]. Then, given the Compressed Suffix Array and the compressed lcp-representation, the linear time algo- rithm to construct the RMQ structure [12] takesO(n log? n) time using no extra space.

During the main algorithm, we also allocate space forR, C and S. This space is bounded by O(n) bits as analyzed earlier. The algorithm makes O(n) queries and updates to the data structures. The most costly operations are the searches on R and updates on C which both take O(log n) time.

Theorem 1 There is an algorithm for determining all F strings satisfying a frequency-based predicate in O(n log |?| + d logn) bit of space and O(n log n) time.

Writing the output takes additional O(|F | log? n + ?F?) time, where 0 < ? ? 1 affects the constant of the space usage and ?F? is the total length of the output.

5. Extensions and Applications  5.1. Less Space, More Time  Other tradeoffs are possible in Theorem 1 by using dif- ferent variants of compressed suffix arrays. It is possible to obtain nHk + o(n log |?|) + d logn bits of space with the running time increasing to O(n log n(1 + log |?|log logn )).Here k = o(log|?| n) and Hk is the order-k entropy of D.

This is achieved by using the FM-index variant in [25, 14], and building on top of it the additional structures needed for the full functionality of compressed suffix arrays as in [32]. In this case, the text is not required to be stored at all (after the construction), as the structure is self-index and supports displaying any text substring of length 	 in O((	 + log1+? n) log |?|) time. That is, outputting the result of the algorithm in Theorem 1 takes in this case O(|F | log1+? n log |?| + ?F? log |?|) time, for any ? > 0 affecting the constants of the sub-linear structures.

5.2. Application to Storing Document Fre- quencies  Sadakane [31, Section 5.2] gives a succinct index struc- ture that stores the document frequencies, i.e. values S[i]?     C[i] that we compute on-the-fly in our algorithm. His struc- ture consists basically of the compressed suffix array and a unary coding of the frequency values in the inorder of the virtual suffix tree. Sadakane shows that the final structure occupies |CSA| + 2n+ o(n) bits, where |CSA| is the size of the compressed suffix array used. He does not give a space-efficient construction algorithm (a suffix tree is used as an intermediate structure, hence taking O(n log n) bits).

We can construct the required unary coding during our algorithm as follows: We maintain the balanced parenthe- ses (BP) representation as in [32] using a dynamic bit-vector occupying n + o(n) bits. This gives us the preorder of the virtual suffix tree nodes at each step. We use another dy- namic bit-vector to store the C-counter values in the same unary coding as Sadakane uses. Using rank and select (select(i) returns the position of the i?th 1 [19]) on both bit-vectors gives us a mapping between BP bit-vector and the C-counter bit-vector. Inserting a new node in BP means inserting 1 in the corresponding place of the C-counter bit- vector. Incrementing a C-counter works by finding the cor- responding node in BP, mapping the position to C-counter bit-vector, and inserting 0 there. Finally, after all values are computed, the preorder of C-counter bit-vector can be turned into the inorder used in Sadakane?s scheme, and the intermediate structures can be deleted. The algorithm uses the same space and time as reported in Theorem 1.

6. Experimental Results  We have implemented the algorithm from ?4 in C++3 and compared it to the C++-implementation of the optimal algorithm from ?3. Our implementation deviates from our theoretical proposal as we use a compressed suffix array that is based on sampling. We use a standard sampling rate of logn that minimize the space usage, as this is the main ob- jective of our approach. However, we will also see that a smaller (fixed) sampling rate drastically decreases the ex- ecution time, while leading only to a moderate increase in space usage. We present test results of time and maximum memory usage for different datasets of protein and genome data.

6.1. Protein Datasets  We used two datasets consisting of the primary struc- ture of all protein data from human and mouse, which were obtained from Swissprot using the keywords HUMAN and MOUSE in the NEWT taxonomy browser. The hu- man dataset (D1) contained 71,622 proteins of total length ?27.3MB, and the mouse dataset (D2) contained 62,562  3The implementation can be downloaded from http://www.cs.helsinki.fi/group/suds/  proteins of total length ?26.3MB. It is interesting to com- pare these data sets because the genome of human and mouse is known to be largely the same, but (given the dif- ferent phenotype of the two species!) must contain some significant differences.

All tests on protein data were run on a 3.0GHz Intel Pen- tium 4 CPU with 3GB of main memory. All output was redirected to the ?null?-device in order to remove influences from secondary storage units.

The programs were tested both on mining frequent sub- strings (Probl. 1) and emerging substrings (Probl. 2). The results for the frequent substring mining are reported in Tbl.

1. Here, the maximum support threshold ? for D2 was fixed to .95. The more striking property of the space-efficient al- gorithm is that it uses only 183.1MB of memory, while the optimal algorithm needs 1,267.3MB. This means a space reduction of a factor of ? 6.9. As already mentioned in the introduction, space is often a more critical resource than time; e.g., users are often willing to wait 3?4 hours instead of 3 minutes, if this allows them to apply their methods to much bigger data sets.

Table 1. Running times (seconds) for mining frequent strings for the optimal (?3) and the space-efficient (?4) algorithm. ? is the mini- mum support threshold for D1. The maximum support threshold ? was held fixed at .95. The last column denotes the size of the output.

? optimal space-efficient |output| .1 154.4 12,426 3,559 .2 151.5 12,403 1,211 .3 154.9 12,565 953 .4 156.5 12,442 694 .5 155.8 12,558 436 .6 152.6 13,420 196 .7 154.1 12,445 49 .8 155.2 12,524 7 .9 151.3 12,672 2  avg 154.0 12,606.1 ?  The running times for the emerging substring tasks are reported in Tbl. 2 (for ?g = 1.3333) and Tbl. 3 (for ?g = 2.0). The results are largely along the lines of the frequent string mining tasks: space is reduced by a factor of 6?7, and the running time is increased by about two orders of magnitude.

To give an idea on how the dataset length affects compu- tation time and space usage, we tested mining frequent sub- strings from different size prefixes of the protein data. Tests were run on dataset prefixes of total length 2?50MB and on the whole dataset of length ?53.6MB. Here, the maxi-     Table 2. Running times (seconds) for mining emerging strings for the optimal (?3) and the space-efficient (?4) algorithm. ?s is the min- imum support threshold. The growth rate ?g was held fixed at 1.3333. The last column de- notes the size of the output.

?s optimal space-efficient |output| .1 154.2 12,597 12  .05 151.9 12,636 233  .01 155.1 12,734 35,839 .005 152.8 13,137 225,198 .001 156.0 12,679 60,449,586  Table 3. Running times (seconds) for mining emerging strings for the optimal (?3) and the space-efficient (?4) algorithm. ?s is the mini- mum support threshold. The growth rate ?g was held fixed at 2.0. The last column de- notes the size of the output.

?s optimal space-efficient |output| .1 154.2 12,380 0  .05 155.9 12,538 64  .01 154.4 12,637 34,343 .005 157.1 12,674 220,585 .001 159.1 13,228 60,410,579  mum support threshold ? was fixed to .95 and the minimum threshold ? to .40. Maximum memory usage for the opti- mal (?3) and the space-efficient (?4) algorithm on different size datasets are reported in Fig. 4. Running times for the same datasets are shown in Fig. 5. Both figures show also a time-space tradeoff for the space-efficent algorithm by us- ing a fixed samplerate (= 3) inside CSA ? we see that a modest increase in memory consumption (Fig. 4) decreases the running time by one order of magnitude (Fig. 5)!

Inspired by this, we tested various time-space tradeoffs for the space-efficient algorithm by using a denser sampling inside CSA. The test was run on the whole protein dataset with the same parameteres as above. The samplerate of CSA was given values between 3?25. The resulting time and space usage for each samplerate is reported in Fig. 6.

Note that the default samplerate used by the algorithm is ?logn (= 25 for the whole dataset). We thus conclude that in practice we should choose a denser sampling.

0 10 20 30 40 50        Input size (MB)  M em  or y  us ag  e (M  B )  Optimal Space?efficient, default Space?efficient, dense  Figure 4. Maximum memory usage while min- ing frequent substrings from protein data for the optimal (?3) and the space-efficient (?4) algorithm. A time-space tradeoff with dense sampling is shown for the space-efficent al- gorithm.

6.2. Human Genome  To show that the space-efficient algorithm works also on genome-scale data, we used a DNA dataset of 22 human chromosomes (build NCBI34) of total length 2.9 billion base pairs. We measured time and maximum memory us- age to solve the frequent substrings and emerging substrings problems. Time to output the result?s substrings was ex- cluded from these test results. Tests were run on a 3.0GHz Intel Xeon CPU with 128GB of main memory.

The space-efficient algorithm used 39.8 hours and re- quired maximum of 9.3GB of memory for mining the whole genome. With a different time-space tradeoff (by denser sampling inside CSA) we achieved a running time of only 22.6 hours with 14.9GB of maximum memory usage.

The implementation of the optimal-time algorithm was tuned for 32 bit word length which is not enough for genome-scale data. However, we can estimate the time and space requirements from tests with 9 small chromosomes (a quarter of the whole genome). For this small portion of the genome, the optimal time algorithm used?17.3 minutes and required maximum of ?13.1GB of heap. This suggests that genome-scale mining would require about an hour of time and about 50GB of memory. Note that the integer ar-     0 10 20 30 40 50       Input size (MB)  T im  e (s  )  Optimal Space?efficient, default Space?efficient, dense  Figure 5. Running times for frequent sub- string mining with the optimal (?3) and the space-efficient (?4) algorithm on a logarith- mic scale. A time-space tradeoff with dense sampling is shown for the space-efficent al- gorithm.

rays for SA, LCP and C-counters already require 3n logn bytes of memory (?32.0GB for the whole genome).

6.3. Comparison to Other Algorithms  Let us briefly describe other algorithms for mining sub- strings. Algorithms VST [8] and FAVST [24] rely on a data structure called Version Space Tree. Because this tree is basically a suffix trie with O(n2) nodes in the worst case, these algorithms suffer from high memory requirement. In practice, we could not test these algorithms on any of the datasets above, as they are only applicable to input sizes up to several hundred kilobytes.

Chan et al. [7] presented an algorithm for the emerging substrings problem, but we could not find an implementa- tion of their approach. Furthermore, no information about the practical memory requirement is reported. The algo- rithm itself is based on a merged suffix tree withO(n) nodes that suggests a space requirement of O(n log n) bits. It has already been shown in a previous study [13] that suffix- array based methods are superior to those built on suffix trees or tries both in terms of time and space.

Two very recent improvements [22, 34] of the origi- nal linear-time algorithm [13] also addresses the problem  2000 4000 6000 8000 10000 12000        Time (s)  M em  or y  us ag  e (M  B )     Space?efficient  Figure 6. Time-space tradeoff for the space- effient algorithm with samplerates 3?25 on protein data. Decreasing the samplerate in- creases memory usage but makes our algo- rithm significantly faster.

of lowering the space consumption; however, they only achieve about half the space of the orignal algorithm. Fur- thermore, their theoretical space guarantee is no better than the O(n logn) bits of the original solution. Nevertheless, it must be said that due to the simplicity of these two algo- rithms [22, 34] they work faster in practice.

7. Conclusions  We hence conclude from the experiments that the prac- tical performance of the two algorithms are in accordance with their theoretical guarantees. However, we must also say that the constants involved in the time-performance of the succinct data structures (?4) are still large.

