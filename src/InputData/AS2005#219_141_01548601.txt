NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society

Abstract  This paper studies the use of a rough set based learning program for predicting web usage. In our approach, web usage patterns are represented as rules generated by the inductive learning program, BLEM2.

Inputs to BLEM2 are clusters generated by a hierarchical clustering algorithm applied to preprocessed web log records. Empirical results show that the prediction accuracy of rules induced by the learning program is better than a centroid based method. In addition, the use of a learning program can generate shorter cluster descriptions.

1. Introduction  One objective ofweb usage mining is to extract sequential usage patterns from a large collection of web logs [1].

These patterns can be used to predict users' access patterns, to identify users' intention, and to provide timely help for using features available on a web site. Since web log records are usually designed for debugging purposes, they need to be preprocessed before applying data mining techniques [2]. Five preprocessing steps have been identified [3]:

I. Data Cleaning: Remove irrelevant data such as log records for images, scripts, help files, cascade style sheet, etc.

2. User Identification: To group together records for the same user. Because web logs are recorded in a sequential manner as they arrive, therefore, records for a specific user are not necessary recorded in consecutive order rather they could be separated by records from other users.

3. Session Identification: To divide pages accessed by each user into individual sessions. A session is a sequence of pages visisted by a user. We also call it as a usage sequence.

4. Path Completion: To determine if there are important accesses which are not recorded in the access log due to caching on several levels.

5. Formatting: Format the data to be readable by data mining systems.

Once web logs are preprocessed, useful web usage patterns may be generated by applying data mining techniques such as mining association rules, mining clusters, and mining classification rules [4, 5, 6]. In this work, we study web usage mining by using a hierarchal clustering algorithm to generate clusters from preprocessed web logs, then, these clusters are used as training examples for a rough set based inductive learning program BLEM2 [7] to generate rules, which represent usage patterns. Rules generated by BLEM2 are associated with four quantities: support, certainty, strength, and coverage. In the study, support of a rule denotes the number of usage sequences matches the rule. Certainty of a rule is the conditional probability of an activity given a usage sequence satisfies the condition of the rule.

Activities are labels of clusters generated by the hierarchical clustering algorithm. Strength of a rule is the probability of the usage sequence denoted by the left hand side of the rule. Coverage of the rule is the probability of an activity matched by the usage sequence. The BLEM2 rules are used for predicting users' activities based on their certainties. Effectiveness of the proposed mining system is evaluated by using log files collected from the University of Akron registration web server.

In the following section, we present the workflow of the proposed mining system. Section 3 describes the preparation of data set. Empirical results are presented in Section 4. Conclusions are given in Section 5, followed by references in Section 6.

2. Mining System Components  There are two major component of the mining system used in this study: a data preprocessing component and a  0-7803-9187-X/05/$20.00 02005 IEEE. 580    clustering and classifier component. They will be presented in the following subsections.

2.1 Data Preprocessing  These are general rules, and they may change according to the purpose of the mining system.

2.1.2 Users Identification  As shown in Figure 1, there are four steps in data preprocessing: data cleaning, user identification, session identification, and data filtering. Path completion is not considered in this study.

r----------------------------  Records from Data Cleaning User Web Log Identification  Web Data Filtering Session Sessions _ Identification  .----------------------------  Figure 1. Data preprocessing component  2.1.1 Data Cleaning  Web logs are designed for debugging purposes in a way the web accesses are recorded in the order they come [2] and due to the connectionless nature of the HTTP (i.e.

each request is handled in a separate connection), for a single user to surf a - website that will result in huge number of records in the web page that are not necessary in sequence rather they could be mixed with records for other users. So for each page component such as image, cascading style sheets file, html, scripting file or java script a separate record is recorded in the web log file.

Usually each record in the web log file has the following standard format [8]:  * remotehost is the remote hostname or its IP address,  * logname is the remote logname of the user, * date is the date and time of the request, * request is the exact request line as it came from  the user, * status is the HTTP status code returned to the  client, and * byte is the content-length of the document  transferred.

Usually for web mining purposes the only interesting element are the HTML pages and the scripting pages (such as jsp, asp or php pages) unless the other file types are playing a navigation role in the web application and they are part of the web structure. In the cleaning phase the pages types that are related to the navigation structure are kept and the other pages are eliminated. The status filed in the web log can be used to keep the successfully fulfilled request and delete the others. Finally the mining process can be limited to certain time or date so only transaction during such time and date will be considered.

User is defmed as a unique client to the server during a specific period of time. The relationship between users and records is one to many. Each user has one or more records to identify it. The users are found based on two assumptions:  1. Each user has a unique IP address during his browsing time span that means other users can have the same IP address during different time span.

2. Maximum of 30 minutes for the user to be in ideal state after that the user will be considered that he left the website.

2.1.3 Session Identification:  Using the website ontology we can identify different sessions in a single user's visit. We assume that the website ontology is already available. Work on retrieving web site ontology can be found in [9, 10, 11].

A web site is defined as a triple W = (P, L, F),  where P = (P ...** Pk) web site pages, which is a list of  k pages, and k is an arbitrary integer.

L: web site links, which is the group of links for the  web site. Each link 1 = (P, Pd ) is defmed by two pages the source page (ps) where the link starts from  and the destination page (Pd) where the links ends to.

F =< fo, f1,. . ., fn,l >, web site functionalities,  where Vf F f=<Pss Pe >. The web functionality f consist of at least two pages the start page and the end page, and zero or more pages in between.

The session identification algorithm will take the users identified in the previous section and divide them into different sessions using the website functionalities. From the website functionality we can identify some pages that are considered as breaking point for the session such as sign in or sign out page.

2.1.4 Data Filtering  After the data is been separated into different sessions, filtering is done based on removing the house keeping pages, the house keeping pages are the pages that are necessary for the web application to run properly, they are not called directly by the user rather they are called internally by the requested page. These pages shall be identified by the website designer and can be found using     the website ontology. Removing the pages can result in redundant pages which can be misleading to the distance function.

2.2 Clustering and Classification  In this section, we present web usage classification and prediction using hierarchical clustering and BLEM2 learning algorithm. An inference engine is used to evaluate the overall results based on the holdout classifier accuracy estimator [12]. The workflow is shown in Figure 2.

Cluster Estimation Results  Figure 2. Web sessions clustering and classification  Figure 2 shows that web sessions along with similarity/dissimilarity matrix are used in the clustering process to generate examples that represent users' behavior over the web. Major part of the examples are used to generate rules that describe the usage patterns, the generated rules along with the rest of the examples are used by an inference engine to estimate accuracy of the mining results. The rules are used for both predicting users' behavior based on past navigation history and describing the cluster of users' usage sequences in terms ofwhat functionality the user is trying to do. For example, one cluster may consist of predominantly freshman students who register classes, while another may consist ofprofessors who upload their classes' grades.

2.2.1 Hierarchical Web Sessions Clustering  Clustering web sessions is to group similar website usage behavior together. We chose to implement hierarchal clustering algorithm for its ability to deal with nominal attributes and to deal with different sequence sizes, and its ability to adopt different similarity/dissimilarity functions.

It starts by placing each web session in one cluster, then,  it merges similar web sessions together until forming one cluster that has all the web sessions in it or other termination condition exist. At the first step, the dissimilarity measure between web sessions is computed by multidimensional session comparison method (MSCM) [13]. After the first step is completed and the first level of clusters is generated, new clusters are formed using Ward's distance measure [14], where the distance between two clusters is the sum of squares between the two clusters summed over all sessions in the cluster. This procedure tends to combine clusters with a small number of observations. It is also biased toward the production of clusters with approximately the same number of sessions.

2.2.2 Cluster Description and Classification  We used two methods for cluster description, one is based on the centroid, and the other is based on rules generated by BLEM2. The centroid for each cluster was selected based on Ward's method [14]. By using the centroid as a way to describe the clusters, each cluster will then be described by one usage sequence. For example if cluster Ck has session si as its centroid, which represents the following page sequence:  Si (P45,P84 P204 From the page names look up the sequence has the following page physical names: Si = (signon, BeforeClassSearch, ClassSearch) The cluster can be described as Class Search cluster, meaning it contains group of users who are searching for classes. Besides using this description for labeling the clusters, this description was used as a way to predict users' behavior. From the above example, the following rule can be generated if Si. =p45 andsi2 =P84 andsi.3 =p204 thensi E ck The BLEM2 method is to use rules generated by a  learning program to desribe the clusters. For each cluster, we may have more than one rule, so we may have more than one description. Or, we can keep the rule with maximum support ifone description is desired.

3. Data Preparation  In our experiments, the data is collected from the University of Akron registration website log files from October 2003 to September 2004. General statistics about the data is presented in the following.

3.1 Data Overview  The total number ofrecords recorded in the web server for the time period mentioned was 28,294,229 records. Each record in the web log file represents a page request     processed by the web server, the monthly records hit is shown in Figure 3. The figure shows high web traffic on the month where there is a major activity such as registration, release of final grades or starting of semester.

For example, it can be seen from the figure that there is a high traffic volume in January since this is when the final grades for fall semester are released and the spring semester starts.

Figure 3. Monthly record counts in the web log  3.2 Data Selection  For experimental purposes, we have selected the data records for few days in which there is major activity on the web server. Table I shows the selected dates along with the major activity. The total records for the selected dates were 1,582,292 records which represent 5.6% of the total records.

Table 1. Selected dates and major activity  Date Major Activity  Monday, Dec. 15, 2003 Teachers insert the grades  Tuesday, Dec.16, 2003 Final grades due for Fall 2003  Monday, May 10, 2004 Teachers insert the grades  Tuesday, May 11, 2004 Final grades due for Spring 2004  Friday, Feb. 20, 2004 Summer Registration begin  Friday, Oct. 24, 2003 Sprint Registeration begin  Friday, April 02, 2004 Fall 2004 Registration begin  3.3 Data Cleaning  In this step we identified several page types and their percentages as shown in Table 2. Since we are interested in the scripting files that imply direct requests by the user,  we kept the asp and html files. We removed files with other extensions. The second cleaning step was to remove incomplete requests. This can be tracked using the status code in the log file. Table 3 shows the status code, the code description, the total number of records and the percentage. We kept the records with status code ok and we removed other records, so we are left with 76% of the data.

Table 2. Different file extension in the selected data set Page Extension Count Percentage Html 33307 2.10% dll 186 0.01% No extension 2148 0.14% php 2 0.00% htm 6938 0.44% txt 68 0.00% ico 645 0.04% jpg 1510 0.10% asp 1537485 97.17% js 1 0.00% xml 2 0.00% Total 1582292  Table 3. Requests status Code  Status Code Description Page Count Percentage  206 Unknown 20 0.00%  207 Unknown 3 0.00%  304 No Change 17826 1.13%  302 Not Found 322728 20.40%  400 Bad request 74 0.00%  200 Ok 1206982 76.28%  403 Forbidden 17491 1.11% Matching not  404 found 1536 0.10% Facility not  501 supported 10 0.00% Unexpected  500 condition 15622 0.99%  Total 1582292 100.00%  3.4 User and Session Identification  The selected data is loaded into a SQL database table in which, each row has three fields (data, time, url), then, we run a session identification script which is based on the algorithm described in [13]. Each session is a sequence of accessed pages, and each user may have more than one   Monthly Record Counts Recorded in the Web Log  . 450 - 400 =L 350 0 -300C. 25  a?- f150.200 O*l5050  C- - or_ -r - _, 0 e - -c . , O= W  . C ) 0)M - - 0)al Data    session. Figure 4 shows the histogram of the generated sequences length  150DO  G 10000 c  5000-  0- -I 0 10 20 30 40 50 60 70 50 90  Session Length  Figure 4. Histogram for page length before filtering  3.5 Data Filtering  We implemented a java program for data filtering and session identification. House keeping and redundant pages are removed. Session identification is reapplied by using break pages. Finally, sessions are grouped based on session length. Figure 5 shows the histogram for the page length, after running the filtering process, notice that most of the sequences have length of 15 or less, so we filtered the data in 14 different files on which each file has a sequences with the same length. We considered sequences of length two or more, since sequence of length one are not considered sequences.

10000 -  c  ) 5000-  0 -  10 20 30 40 Session Length  so 60  Figure 5. Histogram for page length after filtering  4. Experimental Results  The web sessions generated from the clustering process were randomly partitioned into two independent sets, training set (73%) and test set (27%). The training set was used to generate rules by centroid method and BLEM2 learning program. An inference written in Tcl scripting language is used for testing accuracies for both classifiers.

Performance of different methods has been evaluated four  times for session length range from 3 to 15. The resulting average accuracies are shown in Figure 6.

Figure 6. Average accuracy for different session length  The comparison of cluster description length for different session length range from 3 to 15 is shown in Figure 7. It shows that the average cluster description length based on BLEM2 rules is around length 2 and it is almost staying unchanged with increasing session length. In contrast, cluster description length using centroid method grows linearly.

Cluster Description Length  16 |g_l  o 12  -.-BLEM2 Maxl: 'c6 BEM2Max X 4  0 5 10 1s 20  Session Length  Figure 7. Average cluster description length for different session length  5. Conclusions  In this paper we present the experiments of using hierarchical clustering algorithm to generate web sessions further processed by centroid and BLEM2 classifiers.

Holdout classifier accuracy estimator is used to evaluate accuracies of both methods with different session length range from 3 to 15. It shows that rough set based BLEM2 rules perform better than centroid based method in predicting usage patterns. In addition, BLEM2 based cluster description length is shorter than centroid based.

6. References  [1] Cooley, R., P.-N. Tan, and J. Srivastava, "Discovery of intersting usage patterns from Web data," presented at WEBKDD, 1999.

Average Accurcy  0.8 -.-.-Average Accurcy Using Centroid  :0.6 -un-Avrage Accurcy <0O.4 111 lill 1 - _li i | 111 Using BLEM (all)  er ll ilX lil | _ i | | | Average Accurcy0.2 l ll ll 11 | 11 | llilUsing BLEM (max)  0 5 10 15 20 Session Length  .nuwmuamuuaem.ee................

[2] Kohavi, R., "Mining e-commerce data: The good, the bad, and the ugly," presented at 7th ACM SIGKDD San Francisco, California, 2001.

[3] Cooley, R., B. Mobasher, and J. Srivastava, "Data Preparation for Mining World Wide Web Browsing Patterns," Knowledge and Information Systems, vol.

1, pp. 5-32, 1999.

[4] Fu, X., J. Budzik, and K. J. Hammond, "Mining navigation history for recommendation," In Proceedings of the 2000 Conference on Intelligent User Interfaces, 2000, pp. 106-112.

[5] Pazzani, M., J. Muramatsu, and D. Billsus. Syskill & Webert, "Identifying interesting Web sites," In Proceedings of the Thirteenth National Conference on Artificial Intelligence, 1996, pp. 54-61.

[6] Mobasher , B., R. Cooley, and J. Srivastava, "Creating adaptive web sites through usage-based clustering of URLs," In Proceedings of the 1999 IEEE Knowledge and Data Engineering Exchange Workshop (KDEX'99), 1999, pp. 19-25.

[7] Chan, C.-C. and S. Santhosh, "BLEM2: Learning Bayes' rules from examples using rough sets," Proc.

NAFIPS 2003, 22nd Int. Conf. of the North American Fuzzy Information Processing Society, July 24 - 26, 2003, Chicago, Illinois, pp. 187-190.

[8] W. W. W. Consortium, "The common logfile format.

Available at: http://www.w3.org/Daemon/User/Config/Loggining.

html#common-logfile-format," 1995.

[9] P. Clerkin, P. Cunningham, and C. Hayes, "Ontology discovery for the semantic Web using hierarchical clustering," presented at Semantic Web Mining Workshop at ECML/PKDD-2001, Freiburg, Germany, 2001.

[10] Crave, M., D. DiPasquo, D. Freitag, A. McCallum, T.

Mitchell, K. Nigam, and S. Slattery, "Learning to construct knowledge bases from the World Wide Web," Artificial Intelligence, vol. 118, pp. 69-113, 2000.

[11] Maedche, A. and S. Staab, "Discovering conceptual relation from text," presented at Euorpean Conference on Artificial Intelligence (ECAIOO), Berlin, 2000.

[12] Weiss, S.M. and C.A. Kulikowski. Computer Systems That Learn: Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. San Mateo, CA: Morgan Kaufmann, 1991.

[13] Khasawneh, N. Manuscript of Ph.D. Thesis, Department of Elelctrical and Computer Engineering, the University ofAkron, August, 2005.

[14]Kaufmann, Learnard and Peter J. Rousseeuw.

Finding Groups in Data: An Introduction to Cluster Analysis. Wiley, New York, 1990.

