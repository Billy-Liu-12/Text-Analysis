2009 ISECS International Colloquium on Computing, Communication, Control, and Management

Abstract?Mining maximum frequent item sets is crucial for mining association rules. This paper proposes a novel algorithm, Interest Frequent Pattern Matrix(IFPM), which is based on user?s interests, and illustrates the algorithm?s execution process. IFPM preprocesses and filters the transaction database according to the level of data item and user's interests, making the handling data reduce an order of magnitude. And then scans the filtered database to create a FP-Matrix, searches the FP-Matrix by top-down depth-first, which can produce maximum frequent item sets(MFI), frequent item sets(FI)and Closed frequent item set(CFI) by vector operation, thus greatly improves the algorithm?s efficiency.

Keywords:Association rules; Maximum Frequent Itemsets; FP-Matrix; FP-Tree; Multi-level Association Rules

I. INTRODUCTION Association rule mining, as an important technique of  data mining, was first developed to find the customer purchase habit by Agrawal et al. in 1993 at the problem of Market Basket Analysis. Its essential idea, according to the principle that any subsets of frequent item set is frequent, and the superset of non-frequent item set are non-frequent, is scanning transaction database repeatedly[1], to generate a length of k large item sets Lk in the k times scan and get a length of k+1 large item sets iteratively from k large item sets. And just consider the length of k+1 candidate sets Ck+1 generated by k item sets in Lk, then scan the database to verify its frequency. However, the candidate generation costs highly, especially when the frequent sets are large or the minimum support is too small. For example, when there are 104 frequent 1-itemsets, Apriori algorithm will result in more than 107 candidate 2-itemsets, and it needs to scan the database repeatedly; if the maximum length of frequent sets is 10, it needs to scan database for 10 times. In response to this disadvantage, there has been a series of improved algorithms, such as hash-based method[2], sampling method[3], Lattice of Closed Item sets[4], FP-tree algorithm[5]. And Fp-tree method directly create a frequent pattern tree to generate association rules by compressing the database, so that Fp-tree avoids the pattern that generating k large frequent item sets iteratively and cutting item sets space that are adopted by Apriori.

Just like milk and bread are both food, some items in transaction database imply level information. It is likely that different users probably prefer association rules in a certain  level. Traditional algorithms try to find all the association rules meeting minimum confidence, but users usually concentrate on how to improve certain items in different promotion. Take supermarket manager for example, he may want to sell more Winter Coming when it?s late autumn.

Therefore, this paper proposes an algorithm which preprocesses and filters the transaction database according to data item level and the user's interest, scans the filtered database to create a FP-Matrix, searches the  FP-Matrix by top-down depth-first, and produces maximum frequent item sets by vector operation. The experimental results show that it has greatly improved the time complexity.



II. APPROACH TO MINING MAXIMUM FREQUENT ITEMSETS  A. Basic Concepts Definition 1 Let I={i1,i2,i3?,im} be a set of items and  D={ t1,t2,t3?,tn } be a multi-set of transactions, where each transaction ti is a set of items such that ti ? I. For any X? I, we say that a transaction ti  contains X if X?  ti, the set X is called an item set. The count of an item set X is the number of transaction in D that contain X. The support of an item set X, Sup(X), is the proportion of transactions in D that contain X. So Sup(X)= ||{t D | X ? t}||/ ||D||, ||D|| is defined as the number of transaction in D.

Definition 2 An association rule is an implication of the form X=>Y, where X ? I, Y ? I, and X ? Y= ? . The rule X=>Y holds in the transaction set D with confidence Conf(X=>Y), that is defined as Conf(X=>Y)= Sup( X Y) / Sup( X)  Definition 3 An item set X is called frequent if its support, Sup(X), is greater than or equal to some given percentage s, where s is called minimum support.

A frequent item set X is called maximal if there does not exist frequent item set Y such that X?Y.

Definition 4 A association rule X=>Y is called strong association rule if its Sup(X ? Y) is greater or equal to a given minimum support and Conf(X=>Y) greater or equal to a given minimum confidence.

Definition 5 the user? interest can be described by a set Interest-Item, which contains all the data items user is interested in. But the difficult is the items may contain hierarchy data which need to process hierarchy.

B. Description of the process of Algorithm 1) Preprocessing of the Transation Database D    As transaction data are stored in relational database, which doesn?t contain high level data that users are interested in. Firstly, we construct a hash map HMapLevel, which map keys in relational database to high level data. At the same time, let set Interest-Item store the data, that user are concerned and may exist in relational database and high level data.

PreParingData(DB, HMapLevel, Interest-Item,newDB,FL, min-support)  Input: transaction database DB, HMapLevel, Interest-Item, min-support  Output: the database generated newDB, descending order of support number 1-frequent item sets FL  Step 1 construct Hash Map HMapLevel according to user?s input data  Step 2 construct set Interest-Item according to user?s input data  Step 3 if (Interest-Item is null) goto Step 4 for(all item set in DB) do{ if(item set ?  HMapLevel.key <> ? ) replace the item in item set with the corresponding value  HMapLevel[key].value; if(item set ?  Interest-Item <> ? ) {  Insert_Record(newDB, item set); FL[item].count ++;  } } Step 4 for(all itemset in DB) do { if(item set ?  HMapLevel.key <> ? ) replace the item in item set with the corresponding value  HMapLevel[key].value; if(item set ?  Interest-Item <> ? ) { Update_Record(DB, item set); FL[item].count ++; } } Step 5 Sort FL in decreasing order by support number,  and only retain the frequent item set.

2) IFTM:An novel algorithm for Mining Maximal  Frequent Itemsets Input: transaction database newDB, Interest-Item,  min-support Output: maximum frequent item set MFI Step 1 if (Interest-Item is null) let DB replace newDB; for(all item set in newDB) do{ sort item set according to the sequence of item in  FL and just retain the item in FL; map item set to a bit-vector and a bit is set if its  corresponding value exist in FL, otherwise, the bit is not set; merge bit-vector to frequent pattern Matrix; } Step 2 if (Interest-Item is null) let Interest-Item=FL; Step 3 for(all item in Interest-Item) do{ if (item ? FL)  let MFI[item]= ? ;  else{ for (i=0; i < Matrix.rownum ;i++) if(Maxtrix[i, item] ?  min-support) { let S = all item in Matrix[i], on condition that its value is  greater or equal to min-support; merge S to MFI[item]; } } for(j=0;j<Matrix.colnum && j<> item;j++) for(k=0;k<Matrix.rownum;k++)  if( in( ( , ), ( , ))M Matrix k j Matrix k item	 ? min-sup port)  merge item j to MFI[item]; } Step 4 Output maximum frequent item sets in the form  of: item, the maximum  frequent item sets, and its support.



III. AN EXAMPLE Table I a shows an example of a database, Table II the level  mapping HMapLevel. Suppose user are interested in item set Interest-Item ={b, k}  TABLE I.  THE TRANSACTION DATABASE  Transaction ID Item sets 001 a,c,d 002 bce 003 a.b,c,f,m,p 004 a,b.c.f,I,m,o 005 b,f,h,j,o 006 a,f,e,l,p,m.n 007 b,c,k,s,p 008 f,a.c,d,g,i,m,p  TABLE II.  THE HASH MAPPING HMAPLEVEL  item Mapping to item c k d k  (1) Preprocessing of the transaction database, the result database newDB is shown in Table III frequent item set FL in decreasing order of their count is shown in Table IV.

TABLE III.  THE RESULT DATABASE NEWDB  Transaction ID Item sets 001 a,k 002 bke 003 a.b,k,f,m,p 004 a,b.k.f,I,m,o 005 b,f,h,j,o 007 b, k,s,p 008 f,a.k,d,g,i,m,p  TABLE IV.  THE FREQUENT ITEM SET FL  Item Support num k 6 b 5 f 4 a 4 m 3 p 3    (2) the process of IFTM. According step 1, scan the newDB, for each item sets, we use a bit-vector to represent it, take transaction 001 and 002 for example, we use [1,0,0,1,0,0]and[1,1,0,0,0,0] then merge the bit-vector to Matrix. The process is shown in next tables.

TABLE V.  THE MATRIX AFTER TRANSATION 001 ARE MERGED.

k:6 b:5 f:4 a:4 m:3 p:3 V1 1 0 0 1 0 0  TABLE VI.   THE MATRIX AFTER TRANSATION 002 ARE MERGED.

k:6 b:5 f:4 a:4 m:3 p:3 V1 2 0 0 1 0 0 V2 2 1 0 0 0 0  TABLE VII.  THE MATRIX AFTER ALL TRANSATION ARE MERGED.

k:6 b:5 f:4 a:4 m:3 p:3 V1 6 2 2 2 2 1 V2 6 0 0 1 0 0 V3 6 2 0 0 0 1 V4 6 0 1 1 1 1 V5 0 1 1 0 0 0   Interest-Item={b, k} we first search the maximum  frequent item sets for item b.

As all the value of b in Matrix is less than the minimum  support number, S = MFI[b]= ? ; For the other items, take item k for example, we first  calculate the sum if item k and item b are contained by the formula n=  0 5 min( , )  i b k     =4, as 4 is greater than the  min-support 3, we add item k to the maximum item sets of b, MFI[b]={k }.By the same method, we can get  0 5 min( , )  i b f     =3, but min( , , )  i h b k f    =2,which is less  than 3, so we just merge item f to MFI[b], MFI[b]={{k, b},{f, b}}. As  0 5 min( , )  i b a     =2 and  0 5 min( , )  i b m     , we  just drop item a and m .

Follow the same way, we can get MFI[b]=  { {b,f,k:3},{a,f,m,k:3 }{p,k:3}};

IV. ALGORITHM PERFORMANCE ANALYSIS According to the process of IFPM, we can clearly see  that IFPM only needs scanning the database once, its filter according to the user?s interests can improve the performance from Apriori? O(f(n)) to O(f(n/m)). Here, n is the quantity of transaction items, O(f(n)) is its time consuming scale. As the experience of Supermarket data and other data process, we find m is greater than 10, so the performance of IFPM is ten times more than traditional algorithm.

To evaluate the effectiveness and performance of IFPM, we first implement IFPM in C++, then compare IFPM to FP-Tree based on datasets that are from a Supermarket. The experiment was executed on Intel(R)D CPU 3.0 GHz  machine and the software was VC++. The experiment results are shown in Fig.1 and Fig.2.

From Fig.1, it can be seen that the execution times of IFPM is always smaller than FP-Tree when Minimum Support are small. This demonstrates that the performance of IFPM is better.

Figure 1. The time cost of IFPM and FP-tree  Fig.2 shows that the execution times of IFPM and FP-Tree increase rapidly when Minimum Support decreases in high Minimum Support, and the rate of increase of IFPM is lower than FP-Tree.

Both experiments indicate that IFPM is efficient.

Figure 2. The time cost of IFPM and FP-tree

V. CONCLUSION In this paper, we have introduced a new algorithm  IFPM for mining frequent item sets based on user?s Interests, and the experimental results given in this paper show that our technique works especially well for both high Minimum Support and low Minimum Support. Though the experimental results given in this paper show the performance of IFPM, the problem of IFPM is that it consume too much memory. So, in the future we plan to extend the work to a low memory cost.

REFERANCES [1] Agrawal R , Imielinski T ,Swami A. Mining Associations Between  Sets of Items in Massive Databases. In :Proc. Of the 1993 ACM- SIGMOD Int?l Conf . on Management of Data. NewYork :ACM Press ,1993.

[2] Jong Soo Park , Ming-Syan Chen , Philip S. Yu, An effective  hash-based algorithm for mining association rules, Proceedings of of data, p.175-186, May 22-25, 1995, San Jose, California, United States  [3] S. D. Lee , David W. Cheung , Ben Kao, Is Sampling Useful in Data Mining? A Case in the Maintenance of Discovered Association Rules, Data Mining and Knowledge Discovery, v.2 n.3, p.233-262, September 1998  [4] Nicolas Pasquier , Yves Bastide , Rafik Taouil , Lotfi Lakhal, Efficient mining of association rules using closed itemset lattices, Information Systems, v.24 n.1, p.25-46, March 1999  [5] J. Han, J. Pei, and Y.Yin, ?Mining Frequent Patterns without Candidate Generation? Proc. ACM-SIGMOD Int?l Conf.

Management of Data, p1-12, May 2000.

