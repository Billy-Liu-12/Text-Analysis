On-Line Learning Gossip Algorithm in Multi-Agent Systems with Local Decision Rules

Abstract?This paper is devoted to investigate binary clas- sification in a distributed and on-line setting. In the Big Data era, datasets can be so large that it may be impossible to process them using a single processor. The framework considered accounts for situations where both the training and test phases have to be performed by taking advantage of a network architecture by the means of local computations and exchange of limited information between neighbor nodes.

An online learning gossip algorithm (OLGA) is introduced, together with a variant which implements a node selection procedure. Beyond a discussion of the practical advantages of the algorithm we promote, the paper proposes an asymptotic analysis of the accuracy of the rules it produces, together with preliminary experimental results.

Keywords-online statistical learning; distributed learning algorithm; gossip algorithm

I. INTRODUCTION  In most analyses carried out in the field of statistical learning theory, the practical constraints related to the data acquisition/storage/access system and inherent to processing speed, memory and computing capacity are generally ig- nored or incorporated into the mathematical framework in a very stylized manner so far. With the advent of highly complex digital network infrastructures and the pressing necessity of sharing resources and distributing computing power (1; 2), this facet of the machine-learning environment is however becoming more and more essential from a technological perspective and is receiving now increasing attention, see (3; 4; 5; 6; 7; 8; 9; 10; 11; 12) for instance.

Motivated by the recent developments in the architecture of data repositories and computer systems, it is the main purpose of this paper to investigate the binary classification problem, the ?flagship? problem in statistical learning, in a distributed and on-line context, accounting for certain real- life situations, possibly more and more currently encoun- tered in the near future.

Throughout the article, we consider the case where the training data are not stored in some central memory but split into distinct clusters, individually processed by independent  agents (e.g. processors). To process Big Data, one generally distribute data subsamples over a network of processors communicating with each other. Precisely, it is assumed that the agents can exchange a limited amount of information per unit of time only, through a communication structure modeled by a graph of which they form the nodes. Hence, due to these capacity constraints, merging all training sets at any node is unfeasible and a distributed approach, limiting the network overhead, is required. Here, by ?distributed?, it is meant that both the learning and prediction stages are performed by the means of local computations of the agents and sparse communications between them: each agent simultaneously processes the data set it has been assigned to and shares some information with its neighbors in order to build a local classifier.

In (7; 8), a specific view to distributed learning has been developed, where the goal is to reach a consensus among local classifiers. In this setting, all agents originally dispose of the same collection of classification rules and a local gradient descent technique, jointly performed with a gossip step, is used to drive them to a consensus. At the end of the learning procedure, all agents use the consensus classifier to predict labels assigned to test data, with no need for further communications. The nature of the problem we investigate through this paper is very different, it is not of the type ?distributed consensus?. It should be noticed that, unlike most works on ?distributed classification?, agents are not assumed exchangeable in the framework we consider. First, we assume that the collection of classifiers may vary from an agent to another. This situation encompasses the case where each agent is an expert in the recognition based on a specific feature of the input observation for instance. Additionally, the issue at stake is not to seek to achieve a consensus between the agents but to learn how to aggregate efficiently the local decisions, typically through a a majority vote or a well-chosen weighted average. Hence, in the classification problem we consider, both learning and test phases require distributed computations, relying on the whole network of        agents. In addition, it is expected that, unlike consensus- based approaches that drive all nodes to a common classifier, our scheme should preserve and take full advantage of the peculiar skills of the local classifiers, being therefore closer to the spirit of ensemble learning algorithms.

The paper is organized as follows. Section II describes the specific framework of the learning problem considered.

In section III, the principles of the algorithm promoted are described at length. The performance of the procedure proposed is analyzed in section IV, while section V focuses on a specific situation. Finally, numerical experiments are displayed in section VI, in order to provide some preliminary empirical evidence of the efficiency of the methods proposed in this paper. Section VII collects some concluding remarks and technical details are deferred to the Appendix section.



II. BACKGROUND  We start off with setting out the notations and describing the key ingredients of the learning problem subsequently analyzed. Here and throughout, the indicator of any event E is denoted by I{E}.

A. Objective  Suppose we have a ?black-box? system where Y is a binary output, taking its values in {?1, +1} say, and X is an input random vector valued in a high-dimensional space X, modeling some (hopefully) useful observation for predicting Y . Based on training data, the goal is to build a prediction rule sign(h(X)), where h : X ? R is some measurable function, which minimizes the risk  R?(h) = E [?(?Y h(X))] , where expectation is taken over the unknown distribution of the pair of r.v.?s (X, Y ) and ? : R ? [0, +?) denotes a cost function (i.e. a measurable function such that ?(u) ? I{u ? 0} for any u ? R). For reasons which will appear obvious in the sequel (see Remark 3), we focus on the cost function ?(u) = (u+1)2/2. Notice that, in this case, the optimal decision function is given by: ?x ? X, h?(x) = 2P{Y = +1 | X = x} ? 1. The classification rule H?(x) = sign(h?(x)) thus coincides with the naive Bayes classifier. For this specific choice, decision function candidates h(x) will be assumed to be square integrable with respect to X?s distribution. The learning environment under study is non standard. Here we consider a model of distributed classification device composed of a set V of N ? 1 connected agents, which process independent databases: each agent v ? V disposes of a training dataset Dv = {(X1,v, Y1,v), . . . , (Xnv,v, Ynv,v)} of size nv ? 1 and made of independent copies of the pair (X, Y ). In addition, each agent v ? V must select a weak classifier function among a given parametric class possibly depending on v, namely {hv(?, ?v)}?v?Rdv , where dv ? 1. We set  D = ?  v dv . For any vector ? = (?1, ? ? ? , ?N ) ? ? = R  d1 ? ? ? ? ? RdN , we define the global (soft) classifier as: H(x,?) =  ? v?V  hv(x, ?v) forx ? X,  the label related to an observation X being estimated by sign(H(X, ?)). To lighten notation, we set R?(?) = R?(H(?, ?)). This paper investigates the problem of finding a ?global classification rule?, as defined above, with mini- mum risk, i.e. the optimization problem  min ???  R?(?), (1)  while fulfilling some capacity constraints, which shall be described in the next subsection.

Remark 1. (MIXTURE OF EXPERTS.) A typical example of the framework above stipulates that a fixed weak classifier hv : X ? {?1,+1} is assigned to each agent v. For any (?v, x) ? R?X, we set hv(x, ?v) = ?vhv(x) and the global classifier then reduces to a weighted sum of the local weak classifiers. In the learning phase, the issue is to determine the optimal weights using a distributed algorithm. In the test phase, it is compute a weighted sum of the local decisions, by using standard average consensus algorithms such as those studied in (13) for instance.

Remark 2. (MAJORITY VOTE.) Another useful example is given by the case where each ?v corresponds to some local parameter of a local classifier x ?? hv(x, ?v) ? R.

In this case, the global classifier output sign(H(x,?)) can be evaluated by a simple majority vote between agents, see (14).

B. Distributed Learning  In order to give an insight into the approach we pro- pose, we consider first the ideal case where a standard gradient descent for solving (1) could be applied. One would then generate in an iterative manner a sequence ?(t) = (?(t)1 , ? ? ? , ?(t)N ), t ? 1, satisfying the following update equation for each v ? V: ?(t+1)v = ?  (t) v + ?t E  [ Y?vhv(X, ?(t)v )??(?Y H(X, ?(t)))  ] ,  (2) where ?t > 0 is a step size and ?v represents the gradient operator w.r.t. the argument ?v . Naturally, as (X, Y )?s distribution is unknown, the expectation involved in (2) cannot be computed and must be replaced by a statistical version, in accordance with the Empirical Risk Minimization paradigm. It is assumed that each agent v ? V must rely on the local dataset Dv only to update its estimate, in a one- pass fashion: each observation (Xk,v, Yk,v) must be used only once by agent v and is not stored into the agent?s memory. This ?on-line? framework is especially relevant in the context of large data sets, where it is generally hopeless to process the whole training sequence as a block. It shall     also be revealed useful in a distributed optimization setting, as will be discussed later on. The expectation in (2) can be then replaced by the following unbiased estimate  Yt+1,v?vhv(Xt+1,v, ?(t)v )??(?Yt+1,vH(Xt+1,v, ?(t))).

The second issue is related to the distributed setting. In the estimate of the gradient above, the evaluation of the quantity H(Xt+1,v, ?(t)) requires that: i) agent v sends the input Xt+1,v to all the other nodes w 	= v, ii) each node w computes its local decision hw(Xt+1,v, ?  (t) w ) and returns  the result to node v. Needless to say, such a procedure can be revealed overwhelmingly complex when the number of nodes is significant and/or when the dimensionality of the input X is large. It is therefore crucial to reduce the amount of information exchanged in the network. To formalize this constraint, we define the network throughput ? as the average number of information bits successfully carried by the network during each unit of time. Formally, we require that the sum over all pairs of agents (v, w) of the number bits send by v to w does not exceed ? in expectation.



III. THE ONLINE LEARNING GOSSIP ALGORITHM (OLGA)  We now describe at length the general algorithm we propose, in order to solve the constrained optimization problem (1) in the general on-line and distributed framework described in section II. Suppose that, at step t ? 1, for each agent v ? V , the current parameter value is ?t,v . Set ?t = (?t,1, ? ? ? , ?t,N ). The update is performed as follows.

Agent v observes the pair (Xt+1,v, Yt+1,v) and evaluates its local decision hv(Xt+1,v, ?t,v) using the former value of the parameter ?t,v . Next, it searches for an estimate of the global decision H(Xt+1,v, ?t,v) as follows, by selecting some neighbors at random and sending its training input Xt+1,v to the selected nodes. Let ?t+1,v = {?wt+1,v}w?V,w ?=v be a collection of N ? 1 independent Bernoulli r.v.?s B(p), with parameter p ? (0, 1], independent from Xt+1,v given ?t.

Agent v sends the input Xt+1,v to node w if and only if ?wt+1,v = 1. An estimate of the global decision is then given by:  Y? (V) t+1,v = hv(Xt+1,v, ?t,v)  + p?1 ?  w?V\{v} ?wt+1,v hw(Xt+1,v, ?t,w) , (3)  where the superscript emphasizes the fact that the esti- mate is computed by means of communications in the whole network V . It is worth noticing that (3) is an unbiased estimate of the global decision in the sense that E  [ Y?  (V) t+1,v |Xt+1,v, ?t  ] = H(Xt+1,v, ?t,v). If B represent  the number of bits required to represent an arbitrary input x ? X, then each link v ? w carries in average pB bits per unit of time. In order not to exceed the network throughput ? , one must pick the sampling parameter p so  that p ? ?/BN(N ? 1). Finally, agent v performs a local gradient descent as follows:  ?t+1,v = ?t,v+  ?t Yt+1,v?vhv(Xt+1,v, ?t,v)??(?Yt+1,vY? (V)t+1,v) . (4) As mentioned above, we shall pay a particular attention to the case ?(x) = 12 (x + 1)  2. In that case, the update equation (4) boils down to:  ?t+1,v = ?t,v + ?t?vhv(Xt+1,v, ?t,v)(Yt+1,v ? Y? (V)t+1,v) .

(5)  Remark 3. (ON THE COST FUNCTION.) The quadratic nature of the cost functional is essential in the subsequent analysis. It guarantees that the OLGA output remains unbiased at each iteration, in spite of its on-line nature and the randomness incurred by the gossip phase.

The algorithm is summarized in Table 1 below.

Algorithm 1 OLGA  Initialize: Set arbitrary initial values ?0,v for each node v ? V .

Update: At each time t = 1, 2, ? ? ? do For each v ? V do  Neighbors selection: Draw independent Bernoulli r.v.?s ?wt+1,v ? B(p) for any w 	= v  Gossip step: Transmit Xt+1,v to all w such that ?wt+1,v = 1 and  obtain hw(Xt+1,v, ?t,w) in return Local descent: Update the parameter value ?t+1,v  using (4)  As the algorithm is single-pass, the number of iterations is necessary smaller than the size of the full data sample, n =  ? v?V nv . Hence, in the asymptotic framework that  stipulates t ? +?, it is implicitly assumed that n ? +?.



IV. PERFORMANCE ANALYSIS  In this section, we investigate the asymptotic behavior of the predictor output by OLGA as t ? ?. First, we establish its almost-sure convergence to the set of minimizers of R?(?). Next, we provide a Central Limit Theorem which characterizes the fluctuations of the excess of risk as t ??.

This result determines the convergence rate of the algorithm and explicitly characterizes the impact of the ?sparsifying? parameter p on the performance of the algorithm. Finally, using the results of (15), we provide a uniform bound on the error probability of the proposed classifier.

The following assumption is rather standard in stochastic approximation.

Assumption 1. The step size ?t decays to 0 as t ??, so that:  ? t?1 ?t =? and  ? t?1 ?  t < ?.

Additionally, some classical regularity conditions on the weak classifier functions hv are required.

Assumption 2. The conditions below hold true for any v ? {1, ? ? ? , N} and any compact set K ? Rdv .

(a) For any x ? X, the function ?v ?? hv(x, ?v) is  continuously differentiable.

(b) For any ?v ? Rdv , E  [ hv(X, ?v)2  ] < ?.

(c) We have:  E  [ sup ?v?K  ??vhv(X, ?v)?2 ]  < ?,  E  [ sup ?v?K  ???vh2v(X, ?v)?? ]  < ?.

(d) The mappings ?v ?? hv(X, ?v) and ?v ?? ?vhv(X, ?v) on L2(P) are both continuous.

(e) We have:  sup ?v?K  E  [ ??vhv(X, ?v)?4  ] < ?,  sup ?v?K  E [ h4v(X, ?v)  ] < ?.

(f) The set of stationary points  L = {? : ?R?(?) = 0} is finite.

Assumption 2 is clearly satisfied in the example described in Remark 1, i.e. when hv(x, ?v) = ?vhv(x) for some fixed local weak classifier hv such that the fourth moment of hv(X) is finite.

Recall that the algorithm is said to be stable if there exists a compact set K ? Rdv such that the sequence (?t,v)t?1 remains in K for any v ? V , with probability one: P  {?K > 0, supt?1 ??t? < K} = 1. The next result reveals that, provided that it is stable, the algorithm produces a consistent decision rule as the number of iterations grows to infinity.

Theorem 1. (CONSISTENCY) Assume that the algorithm is stable. Under Assumptions 1 and 2, the sequence (?t)t?1 almost-surely converges to the set of stationary points L of R?.

The stability condition may not be easy to check in practice. There are several ways to guarantee stability. A possible approach is to confine the sequence to a predeter- mined bounded set. This can be achieved by introducing a projection step at each iteration of the stochastic gradient algorithm. Each time an estimate ?t,v falls outside some convex compact set Kv , agent v brings the estimate back into Kv by replacing ?t,v with the nearest point in Kv . In that case, differential inclusion arguments may show that the conclusions of Theorem 1 remain true: ?t converges to the set of Karush-Kuhn Tucker points of the functional R?(?) over the set  ? v Kv . Refer to (16) or (17) for further details  on projected stochastic approximation algorithms. Alterna- tively, one can stipulate additional assumptions for the weak classifier functions, see for instance (18). The following result focuses on the situation described in Remark 1.

Theorem 2. (CONSISTENCY (BIS)) Suppose that, for all v ? V , hv(x, ?v) = ?v hv(x) for some given function hv such that E[(hv(X))4] < ?. Then, OLGA is stable and the sequence (?t)t?1 almost-surely converges to the set of minimizers of R? as t ? +?.

In the sequel, notation ?2 (resp. ?2v) denotes the Hessian operator w.r.t. ? (resp. ?v). We also use notation ?1v for ?v , and ?0v stands for the identity i.e., ?0vf(?v) = f(?v). Su- perscript T represents transposition. Let ?? = (??1 , ? ? ? , ??N ) be an arbitrary point. We make the following assumption.

Assumption 3. Suppose that ?? ? L and that the following conditions hold true for any v ? V .

(a) There exists a neighborhood Nv of ??v such that for any  x ? X, function ?v ?? hv(x, ?v) is twice continuously differentiable on Nv .

(b) We have: E [???2vhv(X, ??v)??2] < ? where ? . ? repre-  sents any matrix norm.

(c) There exists a square-integrable random variable C(X)  s.t. for any i ? {0, 1, 2} and ?v ? Nv ,???ivhv(X, ?v)??ivhv(X, ??v)?? ? C(X) ??v ? ??v? .

(d) The matrix  Q? = E [?H(X, ??)?T H(X, ??)]  + E [ (H(X, ??)? Y )?2H(X, ??)]  is a Hurwitz matrix, i.e. the largest real part of its eigenvalues is ?L with L > 0.

(e) There exists b > 4 such that for any i ? {0, 1}, sup  ?v?Nv E [??ivhv(X, ?v)?b] < ?.

(f) The mapping ? ?? ?v(?) is continuous at point ??, where ?v(?) is defined by:  E [ (H(X, ?)? Y )2?vhv(X, ?v)?Tv hv(X, ?v)  ] + 1? p  p ??  w?V\{v} E [ hw(X, ?w)2?vhv(X, ?v)?Tv hv(X, ?v)  ] .

(6)  (g) The block-diagonal matrix ?? = diag(?v(??))v?V is positive definite.

Observe that the mapping (6) is well-defined in a neigh- borhood of ??, by virtue of Assumption 3(e).

Theorem 3. (A CONDITIONAL CLT) Suppose that Assump- tions 2 and 3 hold true and that ?t = ?0t?? for some     constants ?0 > 0 and ? ? (1/2, 1]. When ? = 1, take ?0 > (2L)?1 and ? = 1/(2?0). Otherwise, set ? = 0. Conditioned upon the event {limt?? ?t = ??}, the sequence  ? ?t(?t ? ??) converges in distribution to a  centered Gaussian distribution N (0,?) whose covariance matrix ? is the unique solution to the Lyapunov equation: (Q? ? ?I)? + ?(Q? ? ?I)T = ?? .

Theorem 3 still holds true under milder assumptions on the step size, see (19) for more general conditions.

The effect of the Bernoulli sampling parameter p on the asymptotic behavior of the estimation error deserves some attention. The case p = 1 corresponds to a centralized setting where all nodes communicate without restriction at any time. The matrix ?v(??) then boils down to the first term in (6) solely. This gives the insight that the second term of (6) corresponds to the additional noise covariance induced by the distributed setting, as opposed to a central- ized situation. In effect, when p becomes close to zero i.e.

when communications become rare, the second term of (6) becomes significant and produces a dramatic increase of the asymptotic covariance matrix ?. In that sense, Theorem 3 quantifies the unavoidable tradeoff between throughput and accuracy.

Corollary 1. (ERROR RATE) Let U be a D ? 1 vector of independent centered Gaussian r.v.?s with unit variance.

Under Theorem 3?s assumptions and conditioned upon the event {limt?? ?t = ??}, ?t(R?(?t)? R?(??)) converges in distribution to the noncentral ?2 r.v. 12U  T?1/2Q??1/2U .

Remark 4. (ON THE COST FUNCTION (BIS).) We re- call that the excess of probability of error of a classifier sign(H(x)) is bounded by (R?(H) ? R??)1/2, see (15).

However, the damage to the rate of the excess of risk caused by the use of a quadratic convex surrogate for the cost is somehow compensated by the (possibly parametric) rate stated in Corollary 1.



V. DISTRIBUTED SELECTION This section investigates more specifically the situation  where for any v ? V , hv(x, ?v) = ?v hv(x, ?v), the local parameter ?v being of the form ?v = (?v, ?v) with ?v ? R, ?v ? Rdv?1 and hv : X ? Rdv?1 ? R being a local weak classifier function. For any agent v, the aim is to jointly determine the value of ?v parametrizing the local classifier and the weight ?v of agent v in the sum:  H(x,?) = ? v?V  ?v hv(x, ?v) , (7)  for ? = (?1, ? ? ? , ?N ). In this scenario, it is natural to include a nonnegativity constraint on the weights: ?v ? 0 for any v ? V . Clearly, the vector ? can be achieved by using a distributed algorithm as proposed in Section III.

However, when the number N of nodes is very large, the implementation of OLGA can be difficult or even unfeasible.

Indeed, in the learning phase, a significant amount of information should be exchanged by all nodes and, in the test phase, all N nodes are involved in the decision process. It is therefore desirable to restrict the number of nodes in order to simplify the optimization stage and the prediction process both at the same time. This remark is also motivated by the fact that in practice, different nodes might generate quite similar outputs. For such nodes, it is useless to duplicate the information in the sum (7). In this section, we propose an online method to jointly i) learn the parameters ? and ii) withdraw the nodes which are not essential for classification.

Note that the withdrawal of a node v can be achieved by setting ?v to zero in (7). Based on this remark, we propose to include a ?1-penalization term to the initial cost function.

For some fixed constant ? > 0, this yields the following optimization problem:  min ???  R?(?) + ? ? v?V  |?v| , (8)  The ?lasso? penalization term ?  v |?v| is introduced in order that the minimizers exhibit a certain level of sparsity, i.e.

are such that a significant number of coefficients ?v are exactly equal to zero. Here, ? is a tuning parameter which allows to set the tradeoff between the minimization of the cost and the sparsity of the minimizers. The following modifications should be brought to OLGA in order to pro- duce an efficient distributed on-line algorithm for selecting the relevant experts. At each iteration t, we assume that certain nodes have been definitively declared as idle, and we denote by St ? V the remaining subset of active nodes.

Following in the footsteps of the approach described in section III, a given active node v ? St observing a pair of the training sample (Xt+1,v, Yt+1,v) can obtain a noisy estimation of the output classifier by i) drawing independent Bernoulli distributed r.v.?s {?(w)t+1,v}w?St\{v} with parameter pt ? (0, 1], ii) computing the sum:  Y? (St) t+1,v = hv(Xt+1,v, ?t,v)  + p?1t ?  w?St\{v} ?wt+1,v hw(Xt+1,v, ?t,w) .

The Bernoulli parameter pt should be chosen in a way that the network throughput does not exceed ? . Thus, if |St| denotes the cardinal of the set St and B the number of bits required to represent an arbitrary input x ? X, the Bernoulli parameter should be such that:  pt ? ? B |St|(|St| ? 1) . (9)  Next, agent v updates its local parameters ?t,v = (?t,v, ?t,v) using a stochastic gradient descent. Unlike the algorithm of section III, the update should include the ?1-penalization term in (8) and keep the nonnegativity constraints satisfied.

We thus propose the following update equations:  ?t+1,v = [?t,v + ?t hv(Xt+1,v, ?t,v)(Yt+1,v ? Y? (St)t+1,v) ? ?sign(?t,v)?t]+, (10)  ?t+1,v = ?t,v+  ?t ?t,v??vhv(Xt+1,v, ?t,v)(Yt+1,v ? Y? (St)t+1,v), (11) with the notation [u]+ = max(u, 0) and where ??v repre- sents the gradient w.r.t. ?v . Finally, we need a criterion to decide whether agent v should declare itself as idle at step t+ 1 or should be kept active. Here, we propose to declare a node as idle at iteration t+1 if the current value ?t+1,v of the parameter ?v is zero for the M -th time, where M is an integer fixed in advance. Formally, a node v declares itself as idle if the sequence (?k,v)1?k?t+1 contains at least M zeros.



VI. NUMERICAL RESULTS  The algorithms proposed have been tested on toy exam- ples based on simulated data and on public datasets. Due to space limitations, only a few experiments are reported below: OLGA with experts selection is evaluated on a toy example since its usefulness can be simply illustrated and tested OLGA on real data.

Simulation data. We placed ourselves in the mixture of experts case, using randomly placed affine experts as weak classifiers namely hv(x1, x2; ?v) = ?vsign(cos avx1+ sin avx2 ? ?v), where av and ?v are considered fixed for each agent. We then ran OLGA with experts selection and kept v for which ?v 	= 0. Notice below how the algorithm selects mostly affine separators relevant to the distribution (X, Y ).

Real data. In this section, we compare the performances of GentleBoost (20) and OLGA on some benchmark datasets for binary classification: banana and twonorm. Detailed information about these datasets can be found in (21), see also (22) for a distributed boosting. We split each data sample into a training set and a test set using a 80%? 20% rule. For both GentleBoost and OLGA, the classifier is of the form H(x) =  ? 1?m?M ?mhm(x, ?m), based on linear  combinations of weak classifiers h(x, ?), where ? are the target parameters for the algorithm. For GentleBoost, the (?m, ?m)?s are estimated using a stagewise block procedure.

This means that ?1h1(?1, ?) is added first, then ?2h(?2, ?), etc. and, for each ?mhm(?m, ?) to be added, a pass over the whole block of training data is required. For OLGA, the algorithm is online and distributed; meaning that each data is processed only once and then forgotten. In addition, each ?mh(?m, ?) is computed simultaneously by separate agents forming a network. For GentleBoost, the form of the weak classifier is arbitrary, but a widespread choice is to use stumps, i.e. rules of the form I{x(j) ? ?},  Figure 1. Left: Data av , ?v are represented by lines in red and sampling points (X, Y ) by ?+? in blue when Y = ?1 and by ?o? in green when Y = 1. Right: Only v having a non-zero weight ?v ?= 0 at the end of the iterations are represented.

where x(j) denote the x?s j-th component. For OLGA we used ?smooth stumps?, of the form F  ( ?(x(j) ? ?)) where  F (x) = 1? 2/(1 + exp(?x)). The smoothness is required by the gradient algorithm approach. In the case of OLGA, weak classifiers are in one-to-one correspondence with the agents: V = {1, . . . , M}. Each agent v starts by uniformly randomly selecting an axis j(v) ? {1, . . . , d}, independently from all other agents, and applies next the algorithm de- scribed in section V, using ?t,w = (?t,w, ?t,w, ?t,w) and hw(x, ?t,w) = F  ( ?t,w(xj(v) ? ?t,w)  ) . On both examples,  one can see that OLGA does not outperform GentleBoost and has a more erratic error curve, due do its stochastic nature. However, it should be emphasized that: 1) both algorithms lead to comparable results and 2) OLGA is online and distributed, thus far less demanding in storage and power capability, which are crucial properties in a wide variety of applications.

Figure 2. Comparison between GentleBoost and OLGA on datasets banana (left) and twonorm (right).



VII. CONCLUSION  In this paper, two variants of an online learning gossip algorithm (OLGA) for binary classification, very different in nature from ?distributed consensus? approaches, are pro- posed and investigated. The main strength of OLGA lies in its ability to process data ?on the fly? and then forget about it forever. Being distributed, datasets can be split and partially processed by several agents, while the network is able to benefit from the whole dataset. On real datasets tested in this paper, OLGA performs nearly as well as GentleBoost (20), a block centralized robust version of boosting that needs to store the whole dataset in order to process it. OLGA is well suited to underlying complete graphs, while subsampling edges to perform sparsification (23). Even this assumption seems realistic for IP networks, one might want to alleviate it and introduce hierarchies. Sophisticated versions of OLGA should be thus designed and analyzed in the near future.

APPENDIX - TECHNICAL DETAILS  Proof of Theorem 1 (sketch of)  By Assumption 2(a), R?(?) is finite for any ?. Let us write its derivative. For any ?, any v ? V and any (x, y) ? X? {?1}, ?v?(?yH(x,?)) = (H(x,?)? y)?vhv(x, ?v)  =  ? ??  w ?=v hw(x, ?w)? y  ? ??vhv(x, ?v)  + ?vh2v(x, ?v) .

In particular, for any fixed value of ? and any ??v ? B(?v, 1) := {?? : ??? ? ?v? ? 1}, we obtain that  ??v?(?yH(x,??))? ? (  sup ???B(?v,1)  ??vhv(x, ??)? ) ?  (1 + ? w ?=v  |hw(x, ?w))|) + sup ???B(?v,1)  ??vh2v(x, ??)?,  where we set ?? = (?1 ? ? ? ?v?1, ??v, ?v+1 ? ? ? ?N ). Thus, ?v?(?yH(x,??)) is bounded with a r.v. which does not depend on ??v and which can be proved to be integrable by straightforward application of Cauchy-Schwartz? inequality along with Assumptions 2(b,c). Using Lebesgue?s domi- nated convergence theorem, R? is differentiable w.r.t. ?v and its gradient coincides with  ?vR?(?) = E [ (H(X, ?)? Y )?vhv(X, ?v)  ] The next step is to prove that ?vR? is continuous, and thus that R?(?) is continuously differentiable w.r.t. ?. This is a direct consequence of Assumption 2(d): the proof is left to the reader.

We are now in position to prove Theorem 1. Let us write our algorithm under the form ?t+1,v = ?t,v+?tZt+1,v where we set:  Zt+1,v = ?vhv(Xt+1,v, ?t,v)(Yt+1,v ? Y? (V)t+1,v) . (12) Let (Ft : t ? 0) represent the natural filtration Ft = ?(Ft?1, Xt,1, ? ? ? , Xt,N , Yt,1, ? ? ? , Yt,N ). From the previous statement, it follows that E(Zt+1,v |Ft) = ?vR?(?t). Using Minkowski?s inequality followed by Cauchy-Schwartz? inequality, we obtain:  E [?Zt+1,v?2 |Ft] 12 ? E [??vhv(X, ?v?2] 12 +?  w  ( E [?hw(X, ?w)?vhv(X, ?v)?2]) 12 ? E [??vhv(X, ?v?4] 14 ?(  1 + ? w  E [ hw(X, ?w)4  ] 1  ) .

Therefore, Assumption 2(d) implies that for any compact set K ? ?,  sup ??K  E [?Zt+1,v?2 |Ft] < ? .

The proof is concluded by direct application of (18).

Proof of Theorem 2 (sketch of)  The proof relies on the fact R? is a Lyapunov function R? for the mean field of our algorithm, and that it is well- behaved as ??? ? ?. More precisely, we prove that ?R? is Lipschitz-continuous and satisfies ??R??2 ? C (1+R?) for some constant C > 0. Using these conditions along with adequate estimates of the conditional moments of the noise sequence ?t, standard stochastic approximation results imply that sequence ?t remains in a compact set (see for instance (18)). Moreover, R? is convex under the assumptions of Theorem 2. Thus the stationary points coincide with the global minimizers.

Proof of Theorem 3 (sketch of)  Define ?t+1 = Zt+1+?R?(?t) where Zt+1 is the vector whose vth block-component coincides with Zt+1,v defined by (12). As already seen in the proof of Theorem 1, the sequence ?t is a martingale increment sequence adapted to the natural filtration, meaning that E[?t+1 |Ft] = 0 for any t. Algorithm 1 writes ?t+1 = ?t ? ?t?R?(?t) + ?t?t+1.

Function ??R? is the so-called mean field of the algorithm, whereas ?t plays the role of a noise sequence.

Theorem 3 is a consequence of (19, Theorem 1). We only need to show that the assumptions of (19) are satisfied. To that end, we prove the following two technical lemmas. The first Lemma provides some conditions on the mean field of the algorithm. Due to space limitations, its proof is omitted.

Lemma 1. Set ?? ? L. Under Assumptions 2b?c and 3a?c, function R? is twice continuously differentiable on N :=?  vNv and satisfies:  ?2R?(?) = E [?H(X, ?)?T H(X, ?)]  + E [ (H(X, ?)? Y )?2H(X, ?)] .

Moreover, ?R?(?) = Q?(? ? ??) +O(?? ? ???2).

The second Lemma yields the required conditions on the  probabilistic behavior of noise sequence.

Lemma 2. Set ?? ? L. Let Assumptions 2(a-d) and 3(e-f) hold true. Then, we have:  sup t?0  E(??t+1?b/2 |Ft)I?t?N < ?.

Moreover, almost surely on the event {?t ? ??}, E(?t+1?Tt+1 |Ft)? ?? as t ??.

Theorem 3 directly follows from Lemmas 1 and 2 by straightforward application of (19).

PROOF OF LEMMA 2.

Since?R? is continuous, it is bounded in a neighborhood of ??. Therefore, it is quite immediate to see that the statement supt?0 E[??t+1?b/2 |Ft]I{?t ? N} < ? is in fact equivalent to supt?0 E[?Zt+1,v?b/2 |Ft]I{?t ? N} < ? for any v ? V . Recalling the definition (12) of Zt+1,v , we obtain using Cauchy-Schwartz? inequality:  E[?Zt+1,v?b/2 |Ft] ? E [??vhv(Xt+1,v, ?t,v)?b |Ft] 12  ? E[|Y? (V)t+1,v ? Yt+1,v|b |Ft]  ? C E [??vhv(X, ?t,v)?b] 12 (1 + E[|Y? (V)t+1,v|b |Ft]) 12 for some constant C > 0 which depends on b. Assump- tion 3(e) ensures that the first factor in the righthand side of the above inequality is bounded uniformly in t when mul- tiplied by the indicator of event {?t ? N}. The remaining task is thus to estimate E[|Y? (V)t+1,v|b |Ft]. Recalling (3), one can prove after some algebra that:  E[|Y? (V)t+1,v|b |Ft]1/b ? C??? w?V  ( sup  ?w?Nw E [?hw(X, ?w)?b]  )1/b .

for some constant C? > 0. Using again Assumption 3(e), we obtain that the righthand side is bounded. Putting all pieces together, this proves supt?0 E[??t+1?b/2 |Ft]I{?t ? N} < ?. Consider the second statement of Lemma 2. For any v 	= w, Zt+1,v and Zt+1,w are independent conditionally to Ft. Thus, it is sufficient to study the conditional covariance of ?t+1,v for a given v. The latter covariance matrix coin- cides with Uv(?t)??vR?(?t)?vR?(?t)T where Uv(?t) = E[Zt+1,vZTt+1,v |Ft]. Upon noting that ?vR? is continuous and ?vR?(??) = 0, it is therefore sufficient to show that ? ?? Uv(?) is continuous and that Uv(??) = ?v(??), in order to complete the proof of Lemma 2. Note that Uv(?t) coincides with the conditional expectation of (Y? (V)t+1,v ? Yt+1,v)2?vhv(Xt+1,v, ?t,v)?vhv(Xt+1,v, ?t,v)T , given Ft.

After some tedious but straightforward derivations, one is able to show that Uv(?) = ?v(?). By Assumption 3(f), the proof of Lemma 2 is complete.

Proof of Corollary 1  We use a second-order Taylor-Lagrange expansion of R? at ??. As ?vR?(??) = 0, R?(?t) ? R?(??) is equal to 2 (?t ? ??)T?2R?(??)(?t ? ??) up to a negligible term.

Upon noting that ?2R?(??) = Q?, the result follows from Theorem 3.

Pseudo-code - Penalized OLGA  The method proposed in Section V is summarized by Algorithm 2 below.

Algorithm 2 OLGA  Initialize: Set S = V . For each node v ? V , set initial values ?0,v and ?0,v = 1. Set counterv = 0 .

