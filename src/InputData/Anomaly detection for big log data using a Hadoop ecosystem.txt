Anomaly Detection for Big Log Data  Using a Hadoop Ecosystem

Abstract?In this paper, we address a novel method to efficiently manage and analyze a large amount of log data. First, we present a new Apache Hive-based data storage and analysis architecture to process a large volume of Hadoop log data, which rapidly occur in multiple nodes. Second, we design and implement three simple but efficient anomaly detection methods.

These methods use moving average and 3-sigma techniques to detect anomalies in log data. Finally, we show that all the three methods detect abnormal intervals properly, and the weighted anomaly detection methods are more precise than the basic one.

These results indicate that our research is an excellent and simple approach in detecting anomalies of log data on a Hadoop ecosystem.

Keywords?Anomaly Detection; Big Data; Log Data; Apache Hadoop; Apache Hive; Moving Average; 3-Sigma

I. INTRODUCTION Recently, big data [1] attract a great attention with the  advancement of mobile devices and various IoT sensing technologies. Big data have appeared to handle a large volume of data that are difficult to manage with existing technologies. Most of big data techniques manage the data through the distributed environment, and Apache Hadoop [2] is one of the most popular systems in the big data field. Hadoop is a distributed software framework that stores and processes a large volume of data in multiple servers with low specification and low cost.

In the distributed environment like Hadoop, resource management and system monitoring of each server are highly required. Monitoring systems such as Ganglia [3] and Nagios collect and manage server status as a form of log data. The collected log data represent the system resources and the status of applications. However, analyzing log data is getting more difficult because the log data rapidly increase with the scale-out of clusters.

In this paper, we present anomaly detection methods using Apache Hive [4, 5] to correctly detect the unusual status of big log data collected from the distributed environment. Hive is an analysis platform that processes big data by the SQL-like query language. To detect anomalies of the rapidly changing log data, we first implement moving average [6-8] and 3-sigma [9] techniques using Hive. Second, we improve this basic moving average and 3-sigma method to eliminate the repeated anomalies  using linear weight and exponential weight methods. We finally validate the effectiveness and efficiency of the proposed method through the experiments on real log data.



II. RELATED WORK Hadoop [2] is composed of two major components: HDFS  [10] as a storage model and MapReduce [11] as a computing model. However, implementing MapReduce programs for big data management and analysis in the Hadoop environment is very complicated in general. Apache Hive [4, 5] has been developed to provide an easy programming model in Hadoop. It presents big data in the same format as RDBMS on HDFS, and we can access those tables using HiveQL, an SQL-like query language for Hive.

More specifically, to process HiveQL queries, Hive first converts those queries into MapReduce algorithms and then executes the algorithms in the Hadoop MapReduce framework.

In general, the size of record items in log data is small, but their generation speed is very fast, and we regard the log data as a typical example of big data. We can collect such log data through a log monitoring or log collecting system, and Ganglia [3] is one of such representative log monitoring systems. In this paper, we use the Ganglia monitoring system to collect Hadoop log data, store the collected log data into HDFS through Hive, and detect Hadoop system anomalies from the stored data. As the technical background of anomaly detection, we exploit moving average [6- 8] and 3-sigma [9] techniques, which are simple but efficient in identifying anomalies. First, the moving average technique converts a time-series into an average sequence of the fixed time interval, and it is widely used to understand the changing trend of time-series data. Second, the 3-sigma technique is an empirical rule that, if the measured values follow the normal distribution of mean ? and standard deviation ?, 99.7% of the data values are included in [?-3?, ?+3?]. Using these moving average and 3- sigma techniques, we can identify the values which are not in [?- 3?, ?+3?] as anomalies, when we get ? and ? from the moving averaged sequences.



III. HIVE-BASED STORAGE ARCHITECTURE FOR ANOMALY DETECTION OF LOG DATA  In this section, we design a storage and analysis architecture for anomaly detection of log data based on Hive. Fig. 1 depicts     the proposed Hive-based log management and anomaly detection system. As shown in the figure, the whole system consists of four major components: Hadoop name or data nodes, Ganglia, Hive, and anomaly detection program, which work as follows. First, the Ganglia monitoring system collects system log data from each Hadoop name or data nodes and delivers the log data to Hive.

Second, the Hive system stores the Hadoop log data into the predefined tables of HDFS. Third, the anomaly detection program detects system anomalies from the log data stored in tables of Hive and reports the anomalies to the users.

Fig. 1. Architecture of the proposed anomaly detection system.

Fig. 2 shows a part of real log data collected by the Ganglia system. Ganglia first groups multiple nodes in a cluster to be monitored, and it then collects Hadoop log data from the cluster.

Explanation on each column in Fig. 2 is as follows. The first column ?etc? of each row represents the cluster in which the log data are collected. The second column ?agent1? represents the hostname of the corresponding server. The third column represents the type of log data, and in this case, the ?bytes_out? type means the size of network output data. The fourth column represents the measured value. Finally, the fifth column means the measured time. If we rearrange the values of each column into a sequence, we can regard them as a time-series of the specific column.

Fig. 2. Example of log data collected by the Ganglia monitoring system.

As shown in Fig. 2, the log data collected by Ganglia are text files of CSV (comma separated value) format, and we need to convert those text format into table format to be handled in Hive.

For this, we first design a table scheme to store the log data in Hive. Fig. 3 shows the Hive table scheme to store Hadoop log data. As shown in the figure, the table structure follows the CSV formatted log data of Fig. 2, so it consists of five attributes, each of which corresponds to each column of Fig. 2.

Fig. 3. A table scheme for storing log data in Hive.



IV. ANOMALY DETECTION METHODS BASED ON MOVING AVERAGE AND 3-SIGMA  In this section, we design a HiveQL query to use anomaly detection methods based on moving average and 3-sigma in Hive.

We present three methods: basic, linear weight, and exponential weight methods.

A. HiveQL-based Query for Anomaly Detection Now we define a HiveQL query to detect system anomalies  of Hadoop log data stored in Hive. Fig. 4 shows a query that implements the anomaly detection exploiting moving average and 3-sigma techniques. First, an inner query statement of Lines 4 to 8 computes the moving average and standard deviation from ?log_table? which stores the actual log data. Here, MOVING_AVG() and MOVING_STD() as user defined functions (UDFs) compute the moving average and standard deviation, respectively, over a specific window, and the window size can be changed by a given parameter. For simplicity, in this paper we use one hour, i.e., 60 minutes, as the window size. Next, an outer query statement of Lines 1 to 3 executes the anomaly detection process by using the computed moving average and standard deviation. More precisely, in Line 3, we return FALSE for the normal values. Line 2 is a core part that determines an anomaly based on the moving average and standard deviation. Here we note that, if a log value is not in [moving_avg ? 3?moving_std], we determine it as an anomaly and return TRUE. The anomaly detection methods in Sections 4.2 and 4.3 are plugged in the query of Fig. 4.

Fig. 4. HiveQL query for anomaly detection based on moving average and 3- sigma.

B. Basic Anomaly Detection Method Among three anomaly detection methods, we first introduce a  basic one. It first calculates moving average and standard deviation for anomaly detection. The following Eq. (1) is about the simple moving average and standard deviation. In the equation, x  is a vector of log values included in a window [8].

? 1...nx x? ??? 11( ) n  iix xn? ?? ? ?? 2 2( ) ( ) ( )x x x? ? ?? ? ? ????  Two procedures of Fig. 5 show the algorithms that calculate moving average and standard deviation of each window. Both algorithms compute moving average and standard deviation by using a previous window, a current value of log data, and the window size. In Lines 2 to 5, the old log item is removed from the window, and the current value is added to the window when the previous window size is equal to that of the current window.

After that, the rest steps calculate new moving average and standard deviation by Eq. (1).

Fig. 5. Basic anomaly detection method.

C. Improved Anomaly Detection Methods In basic anomaly detection, there is a problem that repeated  values are detected as duplicated anomalies because each log value is treated with equal importance. To solve this problem, we present two weighted anomaly detection methods by improving the basic one. Basically, weighted anomaly detection is an improved approach that adds different weight to each log item in a window. In other words, this approach is to treat the importance of old data as low and that of new data as high. Based on the basic method, we present linear weight and exponential weight methods.

First, we design linear weight anomaly detection, where the weight increases linearly. For this, we use Eq. (2) to calculate the linear weight moving average and linear weight standard deviation.

? 1...nx x? ??? 11( ) ( ) n  iiW x i xn? ?? ?? ?? 2 2( ) ( ) ( )W x W x W x? ? ?? ? ? ????  In Eq. (2), W is the weight, and we give W linearly proportional according to positions of log items. Fig. 6 shows linear weight anomaly detection based on Eq. (2). The algorithms are similar to Fig. 5 excepting how to get ? and ?, that is, the linearly increasing weight is applied to log data.

Fig. 6. Linear weight anomaly detection method.

Next, we explain exponential weight anomaly detection, which adds weights exponentially to the basic method. We use Eq. (3) to calculate exponential weight moving average and exponential weight standard deviation. In Eq. (3), w means the size of a window, and nE?  and nE?  mean the exponential weight moving average and standard deviation of n-th log value, respectively [8]. Unlike Eq. (2), Eq. (3) gives much higher weights to recent log items exponentially proportional to their positions.

? 2 1w? ? ? ??? 1(1 )n n nE x E? ? ? ? ?? ? ? ? ? ??  ? 2 21 1(1 )( ( ) )n n n nE E x E? ? ? ?? ?? ? ? ? ? ????  Fig. 7 shows the algorithms that calculate exponential weight moving average and standard deviation using Eq. (3). Like Figs.

5 and 6, Fig. 7 needs a previous window, a current value of log data, and the window size. In addition, it requires a standard deviation of the previous step. In Lines 2 and 3, we assign the current log value to moving average and ?0? to moving standard deviation in the first window since there is only one log data item in the window. Then, the rest lines compute moving average and standard deviation by Eq. (3).

Fig. 7. Exponential weight anomaly detection method.



V. EXPERIMENTAL EVALUATION In this section, we evaluate the effectiveness of the proposed  methods through actual experiments. The experimental environment of a Hadoop cluster is configured by a name node and four data nodes. The detailed hardware and software specifications are as follows: Intel Xeon CPU E3-1240 v3 3.40GHz, 16GB RAM for the name node, Intel Core i3-4350 CPU 3.50GHz, 4GB RAM for the data nodes, and Hadoop-2.6.0, Hive-1.1.0 for software platforms. We use a ?bytes_out? log item, which contains one day log data collected from the ?agent1? server in the ?etc? cluster for demonstration.

Fig. 8 shows a visualization graph of anomaly detection results. In the figure, x-axis represents the measured time from 00:00 to 24:00, and y-axis represents the measured value, i.e., ?LOG_VALUE? in Fig. 3. The solid line represents log values, and three pairs of dotted lines represent the 3-sigma bands. That is, if the solid line deviates the dotted band, we determine those points are highly to be abnormal, i.e., they might be anomalies. As shown in the figure, we find four abnormal intervals, which are depicted as circles (?) for the basic approach, triangles (?) for the linear weight approach, and crosses (?) for the exponential     weight approach in the chart. In particular, we note that there are rapid changes in between 07:00 and 09:00, and we can say that there might be some abnormal status at that time interval.

We evaluate the basic approach from the anomaly detection results as follows. First, the basic detection method correctly identifies abnormal points in which the ratio of log data values are rapidly increased. For example, the change ratio before 15:00 is not large, but the change ratio around 15:00 is very large, i.e., the log values rapidly increase around 15:00, and we identify a rapid change time point as an anomaly. Next, let us discuss another time interval in between 11:00 and 13:00. There is also a big change at that interval, but the change ratio itself is not large, and thus, we regard those time points as normal changes. However, the basic method has a problem of reporting unnecessary and duplicated anomalies in the case where there are rapid changes.

For example, seven anomalies are reported around 03:00, and five anomalies are reported around 09:00. This is a limitation of the basic anomaly detection method.

The linear and exponential weight methods differ from the basic one by solving the duplicated anomaly problem. Table 1 shows the number of anomalies by time points for each anomaly detection method. The table says that the number of anomalies in each interval of the weighted methods is quite less than that of the basic method, but all methods report multiple anomalies in the same intervals. For example, the basic method detects seven anomalies around 03:00 and five anomalies around 09:00.

However, exponential weight approach detects only one anomaly around 03:00 and only two anomalies around 09:00. Therefore, we can say that the weighted methods detect the same anomaly intervals with the basic method, but they eliminate unnecessarily duplicated anomalies from the basic one.

TABLE I.  NUMBER OF ANOMALIES DETECTED BY THE PROPOSED METHODS.

Time Method 03:00 09:00 15:00 22:00  Basic method 7 5 3 2  Linearly weighted method 2 2 1 1  Exponentially weighted method 1 2 1 1

VI. CONCLUSIONS In this paper, we designed an anomaly detecting system in the  Hadoop environment. For this, we first presented a log data management and analysis architecture. We then proposed a basic anomaly detection method using HiveQL, where we adopted  moving average and 3-sigma techniques based on Hive. We next presented the enhanced detection methods by applying the concept of weights to the basic method. That is, we proposed two weight-based anomaly detection methods as the improved version of the basic detection method. We also performed the experiment using the actual Hadoop log data and visualized the anomaly detection results as a graphical chart. Experimental results showed that our methods correctly identified abnormal points as anomalies, and the enhanced weighted methods improved the accuracy of the anomaly detection compared with the basic one. As the future work, we will apply the presented methods to big log data of multiple Hadoop nodes, and we will redesign the Hive storage structure to improve its maintenance and analysis performance.

