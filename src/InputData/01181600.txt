Web Search with Personalization and Knowledge

Abstract   Although many search engines provide relevantly good search results to the user, they do not consider personal, domain-specific preferences in their searching or ranking algorithms. In an intranet environment we could collect the background information about the users such as their expertise. If we can accumulate, categorize and personalize web usage information, it can be used to help the user search web pages efficiently and effectively. Data analysis and mining can further facilitate web searching in an intelligent way. This paper describes Internet Search Advisor (ISA), a personalized, knowledge-driven search system that helps the user find the informative web sites. The ISA supports multi-dimensional data analysis and data mining based on association rules and sequential patterns.

1. Introduction   The Internet has impacted almost every aspect of our society. The number of web sites and web pages has grown rapidly so that it is almost impossible for a user to locate unknown web sites rapidly and accurately without search engines or web directories. Most of the search engines employ their own algorithms to provide relevant web sites given a set of input keywords. They are also scalable so that a user can select a specific search domain.

What a meta-searching engine does is to send input keywords to multiple search engines, collect outputs from them, format outputs and generate final results to the user.

In general, search engines decide the web page relevance using several factors. These factors are ?general? in the sense that they do not consider personal preferences. In other words, given input keywords, the search output is identical to every user. On the other hand if we collect information of each user, for example, his expertise, experience and web usages we can provide better search results. To our knowledge little approach has been done on this subject.

In the following, Section 2 summarizes work related to this research. Section 3 describes the Internet Search Advisor (ISA) that we have developed to support personalized web search. ISA consists of the Process  Module, Parser Module, Data Analysis Module, and Data Mining Module. The paper is concluded in Section 4.

2. Related Work   Due to the explosive growth of web pages it is impossible for web users to locate web sites or web pages of interest without a web search engine or web directory.

Usually a search engine crawls all websites periodically and builds an index structure for a full-text database. Web directories classify the selected web sites by subjects. It is common that a portal provides both a search engine and a web directory.

The Internet or an intranet can be considered as a distributed database with the following characteristics:  ? Distributed: Web pages are distributed across multiple web servers.

? Volatile: Web masters continue to update, add and delete web pages.

? Unstructured: Web contents include HTML pages, text files, PDF files, Word files, and other document files, which are semi-structured or unstructured.

? Large: A web search engine needs efficient methods for crawling, indexing, storage, and query processing.

The Internet or an intranet can be modeled as a directed graph G:(V, E), where V is a set of vertices (pages) and E is a set of edges (hyperlinks). For example, Figure 1 shows the graph model of an example web site.

If we remove the back-links a graph becomes a tree, i.e., a connected, acyclic and undirected graph. These back-links can be used as an important factor to decide the page ranks [12]. For instance, if a web page x has more back links pointing to it than page y, the rank of x should be higher than that of y.

To construct a search engine several steps are needed; these include building a crawler, constructing page ranks and building an index. Reference [9] explains how to crawl web pages efficiently. Reference [12] describes the structure of Google which is one of the most popular web search engines. There are many commercial search engines available right now and most of them have the generic architecture as shown in Figure 2 [6].

Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)        Admissions Alumni  Home  Libraries Students ??  ??  ?? ??  ??    Figure 1. The graph model of an example web site      InterfaceUsers Query engine Index  Indexer Crawler  Web page description  Web     Figure 2. Architecture of a generic search engine    A crawler is a software agent that visits different web  sites, downloads the web pages, parses the web pages and stores the parsed information into a database. It starts with an entry URL and extracts a set of URLs from the download pages recursively in a breadth-first or depth-first manner. With breadth-first search, the crawler visits all the nodes at level n before visiting any nodes at level n+1. On the other hand, with depth-first search, the crawler follows a path until it cannot go deeper and it returns recursively. Reference [9] addresses the problem of ordering the URLs for crawling and it concludes that a good ordering strategy enables the crawler to collect ?more important? pages when the buffer size is limited.

An inverted index data structure can be employed to store the words extracted from the downloaded pages. It usually consists of a list of keywords, each having a set of pointers to the pages in which the keyword appears.

Consequently, given a keyword, this data structure can be used to locate those pages that contain the keyword.

Some ?stopwords? (e.g., articles, prepositions, etc.) are not searchable and should not be included in the index structure.

Given a set of keywords (or a query), the query engine retrieves the web pages (?output? pages) in which all the keywords are included. The ranking algorithm that decides in what order the output pages are displayed is the key factor to decide the quality of the search engine. The ranking algorithm usually considers several factors including the proximity of the keywords to ?rank? the output pages [12].

The importance of data analysis can be understood if large amounts of data can be stored over the time. Data analysis tools provide summarization, understanding and organization over a data set. On-line analytical process (OLAP) was first introduced by [10]. There are several  Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)    differences between OLAP and on-line transaction process (OLTP). For instance, OLTP is transaction- oriented, application-oriented and simple query- oriented. On the other hand, OLAP is analysis-oriented, subject-oriented and complex query-oriented. Reference [8] provides a detailed comparison between OLAP and OLTP systems.

Most OLAP tools employ a multidimensional data model, which views data in the form of a data cube.

Reference [2] proposes several methods of multidimensional aggregations for ROLAP servers.

Reference [11] explains the use of a data cube as a relational aggregation operator. A data cube models an n-dimensional data set with a set of dimensions and facts. A fact table usually stores the subjects, and dimensions are the different perspectives of the facts.

The core of multidimensional data analysis aggregates the facts with a set of dimensions. A ?star? schema is often used to model a multidimensional data analysis.

With the availability of user access patterns, data mining can be applied to extract rules that relate input keywords, users, and visited URLs. In particular, association rules [3][4] can be identified to correlate different dimensions, and sequential patterns [1][5] can be used to derive search patterns of given users or a set of keywords.

3. Internet Search Advisor   Today, most search engines are not personalized, which means that given a set of keywords, the output pages and their ranks are the same to every user.

However, there are many situations that personalization can significantly facilitate web search if we can consider user profiles and their web usage histories. As a simple example, seniors of Computer Science are usually more familiar with Java than freshmen. If a freshman student wants to obtain some information about Java she/he can first consult the ?hot list? (i.e., a list of URLs that have been visited most frequently over a time period) for ?Java? from the seniors. Thus the knowledge of one group of users can play a role in guiding the other users to reach useful URLs sooner.

When a user visits the web sites his web usages information can be collected. We have developed the Internet Search Advisor (ISA) to accumulate the information about users? activities in a way transparent to the user. For instance, when a user downloads several web pages that are related to ?Java Programming?, the ISA stores the web usage information. Later on,  whenever the user needs to go to web sites related to ?Java Programming? she/he can be directed to these sites on the ?hot list?. By applying data analysis functions, the user can post more useful queries such as ?Which URLs were visited for input keyword ?Java Programming? in the last month??  The ISA is designed to work on top of any existing search engines (currently Google and Alta Vista). To determine the ?hot lists? we consider several factors such as number of clicks, duration of each visit and user evaluations of a specific set of users  As shown in Figure 3, ISA largely consists of four components: the Parser Module, the Process Module, the Analysis Module, and the Mining Module.

3.1 Parser Module   The Parser Module connects a user to a target search engine, passes the input keyword(s) to the search engine(s), downloads the web pages, reformats the output pages, and passes the output pages to the user.

Each target search engine has its corresponding parser.

The parser can be implemented using the Common Object Model (COM) programming expertise to accomplish a client and server relationship in the Windows environment.

3.2 Process Module   The Deliverer Process takes an input query from the user through the web browser and passes the keywords to the corresponding parser. It receives the formatted output pages from the Parser Module and generates the hot URL lists. The Bookkeeper Process is responsible for updating the database to track users? behaviors after the Deliverer Process generates the search output. The Reporter Process generates the hot URL lists based on a classification of users.

Figure 4 shows a snapshot of ISA given the input keyword ?UCI?. The leftmost column displays the reformatted Google output and the middle column shows (1) the user?s rank for UCI related pages, and (2) a recommendation rank for these pages from all users by adding up the visiting counts. The rightmost column allows the user to select the preferred pages of other users. Therefore we can utilize other users? preferences if their expertise is known.

Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)      User Interface    Process module  Parser module  Deliverer Bookkeeper Reporter  ? Database  Google Alta Vista  Google Parser  Alta Vista Parser  Analyzer Miner  Analysis and mining modules  ?    Figure 3. Architecture of ISA       Figure 4. A snapshot of ISA given the input keyword ?UCI?   Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)    3.3 Data Analysis Module   Once historical data are accumulated we can post queries. Some example queries include:  ? Which keywords were demanded the most in the last two months?

? Who have used the following keywords the most in the last two months?

? Which web sites were visited the most in the last three months?

In general a data warehouse should be separated from the operational database. However, in this project we have used the same database for both purposes. We can easily separate these two when the size of the database increases.

Four dimensions of facts are of interest: users (IP addresses), keywords, URLs, and time. The measures are also user, keywords and URLs.

3.3.1 Multidimensional Data Model   Considering the schema shown in Figure 5 we can create three types of 3-dimensional cubes. Note that the time-dimension is implied and it is not explicitly shown in Figure 5. Figure 6 shows the user interface for the data analysis module and its output. We can post three types of queries based on three dimensions:   Q1: Which URLs were visited the most (given a keyword) in the last two months (by a user)?

A1:  A list of URLs.

Q2: Which keywords were demanded the most (given a URL) from January 2001 to May 2002 (by a user)?

A2:  A list of keywords Q3: Who was the most frequent visitor, given a keyword, on a URL in the last three months?

A3:  A list of users.

Using the multidimensional data structure we can offer  a ?recommendation? function and a ?similar keyword? function. The recommendation function is similar to the global URL hot list in that given a set of keywords, ISA recommends a list of URLs based on the number of visits.

It also provides a list of similar keywords for a set of keywords given. For example if a user searches for ?java design?, ISA would provide the keywords related to the URLs corresponding to ?java design?.

3.4 Data Mining   Once ISA is operational the database size will keep increasing. It is likely that we can discover some interesting rules or correlations hidden in the large data set from the ISA database. In the ISA environment there are  two kinds of knowledge to be mined: association rules and sequential patterns.

3.4.1 Mining Association Rules from ISA Database   Using the association rule algorithm we can derive rules like: ?Which URLs are likely to be visited for a specific keyword?? and ?Which keywords are likely to be used on a specific URL?? The association rule approach can find some correlations from a large data set [3].

Basically there are two steps involved to derive an association rule from a data set: (1) find frequent items, and (2) derive the association rules from the frequent items [4].

We have employed the DHP (Direct Hashing and Pruning) algorithm [13] in the Data Miner. This algorithm utilizes hash techniques to accelerate the generation of frequent item sets.

We have also made some modifications on the original DHP algorithm to make it more efficient. The original DHP algorithm makes several scans on the transactional data. During each scan, new frequent item sets are discovered and some items in the transactional data are pruned if they will not be in any frequent item sets during the next scan. The original DHP algorithm counts the number of occurrences of an item set twice, one to count if that item set is frequent or not, and one to check whether any item in the transactional data can be deleted.

However, we propose that one count is enough. In fact, during the first count, we will already find out the items that will not appear in frequent item sets in future scans.

Before the starting of the k-th scan of the algorithm, we have already generated k-1 item sets. During the k-th scan, we generate the candidate frequent k-item sets by selectively combining two frequent k-1 item sets into one k-item set using the a priori rules. Here we also employ a hash technique to discover quickly whether all the k-1 item subsets of a candidate k-item set are frequent. If not, we can delete this candidate k-item set since it cannot be a frequent set. After then the remaining candidate k-item sets will be inserted into a hash table, where they are counted to satisfy the minimum support. As shown in Figure 7, assuming a database size of 10,000 transactions, the time consumed by the improved DHP algorithm decreases rapidly as we increase the minimum supporting percentage.

3.4.2 Mining Sequential Patterns from the ISA Database   Whenever a client uses ISA the transaction time is recorded into the database. Using the database we can obtain some knowledge about how and when different web sites were visited. In other words, we can extract some sequential patterns based on the access information.

Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)     Figure 5. The ISA database schema      Figure 6. User interface for the ISA data analysis tool    Time vs. Support Percentage (on mining 10,000 transactions)        0 1 2 3 4 5 6  Support Percentage  Ti m  e (s  )     Figure 7. Sample output of the ISA data analysis tool        Proceedings of the IEEE Fourth International Symposium on Multimedia Software Engineering (MSE?02)    For example assuming the data in Table 1 and the minimum support, s, is 2, we can extract the sequential pattern 23->38->49, i.e. users often tend to visit 23, then 38, followed by 49.

Table 1. A part of the table for URL data set   User ID URL 1 <5 23 49> 1 <5 38> 1 <5 45 49> 2 <23> 2 <19 38> 2 <23 47 49>    Extracting sequential patterns typically needs much more computational power than finding association rules.

The sequential discovery techniques we use involve 4 phases.

? Phase 1 finds the frequent item sets from the data sequences. This is almost identical to finding association rules. However, each item set is only counted once for every user. Therefore, although item 5 appears 3 times for user 1, it is only counted as appearing once.

? Phase 2 enumerates all the frequent items sets.

1: <23> 2: <38> 3: <49> 4: <23 49>  ? Phase 3 transforms the original data sequences by replacing the items with the enumerated item sets.

Table 2.  Transformed data   User ID URL 1 1 3 4 1 2 1 3 2 1 2 2 2 1 3 4   ? Phase 4 finds the frequent patterns in the transformed  data by the counting algorithm similar to DHP. We can obtain the frequent sequential patterns that satisfy the minimum support. In this case, the sequences <1 2> <1 3> <2 3> <1 2 3> appear twice, so they are all frequent sequences.

We can employ a lexical tree to count the frequent sequential patterns efficiently during phase 4. Figure 8 shows the paths of the lexical tree. Figure 9 shows the performance of the sequential pattern search algorithm along with the support percentage and the execution time of a 10,000-transaction database.

4. Conclusions   We have introduced the Internet Search Advisor (ISA), a personalized, knowledge-driven search system that helps the user to find web information based on individual preferences. To our knowledge, no existing search engines provide such personalized search results.

We have also implemented a data miner to extract association rules and sequential patterns based on an improved DHP algorithm that reduces the cost of the trim process. Finally we have introduced a data structure, called lexical tree, to find sequential patterns efficiently.

In the future, we need to develop an efficient sequential pattern data-mining algorithm that avoids generating all the candidate sets. We also need an efficient similarity-matching algorithm to find the similar URLs related to a specific keyword.

