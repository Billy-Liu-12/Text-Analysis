

Market-Basket Analysis with Principal Component Analysis: An Exploration  Kitty S.Y. Chiu, Robeti W.P. Luk, Keith C.C. Chan and Koms F.L. Chung Department of Computing, Hong Kong Polytechnic University  Hong Kong, China  {cssychiu, csrluk, cskcchan, cskchung)@comp.polyu.edu.hk  Abshee(-- Market-basket aonlyiir Is a well-known business problem, which can be (partially) solved computationally using association roles, mined from transaction data to maximize cross-selling effeets. Here, we model the market-basket aaalyrls 81 a finite mixtnre density of human eoasomptian behavior according to social and cultural events. This leads to thc use of principle component analysis and possibly miitore density analysis of transaction data, wbleh was not apparent before. W e  compare PCA and association ruler mined from B set of benebmark transaction data, to explore similarities and differences between these two data esploration tools.

KeywordF: market-basket, association rules, PCA, mixture density

I. INTRODUCTION  The market-basket problem is a well known business problem in which consumers buying behavior is influenced by alternative prcducts and related products. With the availabiliiy of accurate and efficient capture of transaction data, wmputational analysis of hansaction data can discover interesting consumer spending patterns. Mining association rules [ I 2 1  is a well known and important tool to discover dependencies in product sales. By discovering these dependencies, it is possible to maximize the cross-selling effect [3] of related prodwts, increasing transaction volume and therefore increasing total profit. The appropriate use of cross-selling capabilities has implications in other business activities like inventoryharehouse management. enhancing user satisfaction, etc. Hence, it is important to discover prohct dependencies (41. While association rules have been quite successful in solving the market-basket problem and dismvering dcpendencie$ higher-order dependencies are hard to rind because of the nature of the discovery algorithm (e.g.

Apricri), in which successive higher order association tules are pruned [SI due to computational cost and the lack of evidence to supput such higher order association rules.

hincipal componen analysis (PCA) and the more general techniques of finding mixture densities are well know multivan'ate data exploration techniques [6] but it is not apparent how they could be applied to the market basket problem. Previous work used PCA for quantitative association d e s  for estimating errors [7], as well as for load balancing parallel association rule mining mechanisms [SI.

Recently, Cadez, Smfi and Mauuila [9] examined the use of mix- models for profiling the bansaction data of indivi&als Here, we will suggest a model of consumer spending behavior originated from (social or cultural) events.

These lend themselves to the use of PCA.

11. MOOELING GNSUMER SPENDING PATTERNS  A market is based on supply and demaud. Shops are typical market places where suppliers provide products to satisfy the demand of consumers. We conjecture that the demand of consumers typically depend on (social or cultural) events. For example, almost every family participates in the "breakfast" event, in which t h m  is a n a W  distribution of products desired for those events. Certain events are seasonally, like Christmas. while other events may recur like breakfast. Each household will require certain amount of products to be bought in order to satisfy the participation of the events by the househdd. We assume that each event has a (quasi- stationary) distribution of how likely certain product is bought. For example, suppov there are three products: bread, butter and hammer. The likelihood of a household to buy bread and butter for the breakfast event is much higher than that to buy beer for breakfast Hence, the spending pattern is dependent on the nature of the event Obviously, the spending pattern of each household depend on many other factors but for a large numba of households, we assume that the probability distribution de = <p(iJ ,..., p(iJ> of products ti,, .... iJ for a particular event will k reflecled in the aggregate demand of pro&cts. Since there are many events that a household is participatilg, the aggregate demand Dp) of these households is:  h.r  This demand may be reflected in the transaction data, representing the eventual spending pattern. Obviously, there are product substitution effects, etc. However, for simplicity, we assume that the consumer only buys the desired product or not Hence, one cansider that dh,* is basically an influence on the consumption, reflected in the transaction data T, of a consumer h. In general, the influence of such a distribuion di,c is sumnmizcd in some function say GO. However, for simplicity, the influence is simply modeled as additive components, i.e.:  0 2002 IEEE SMC TAlFZ    although more sophisticatedmodels using bgit transformatbn for discrete data can be used Since the events are those that may induce cross-selling effects, it would be important to discover the events and the associated likely products to buy.

Hence, it would be interesting to find  de = x p ( h ) x d h , h  Assuming that each household is equally likely to consume (i.e.p(h) is a constant),  However, what we can observe is only TA. Hence, w r  task is to discover the underlying distribution de.

Figure 1 is the schematic diagram of our suggested transaction generation process Notice that the discovering process may not be able to identify the original numbu n of events (i.e. rn f n). Note that it is possible to verify which social event that a product is used for by asking the consumer. It is also possible that a single product is used for a number of social events so that the decision to buy a 'ype of product may be based on some aggregate influence for all events that needed the oroduct.

Figure 1: Schematic diagram of discovering the underlying distribution (d), .._. 4J of consumer spending patterns to participate in different social and/or cultural events.

From a practical point of view, it may not be disastrous even if the discovered distribution differed from the underlying distributbn, as long as the distribution found can readily identify products that are sold together in the same transmion for maximizing cross-selling effects. In this vais it does not matter if two underlying distributions of two different events merged together as long as in the transactions, the consumers bought products to support both events. From the point of view of scientific modeling and fmm other practical point of views, obtaining the actual underbing distribution is i m p o m t  to predict when cetiain events are known to occur, for example seasonal events. However, as an  initial exploration in this area. we will confine to the case whae readily avdilable techniques are used  111. PRINCIPAL COMPONENT ANALYSIS  Principle component analysis (PCA) is a well-known method in statistical multivariate data analysis and it has been applied widely. First, it was used as a means to reduce the dimensionality of the problem [IO] by reducing the number of variables to a few components or latent variables at certain level of accepted information loss. Dimension reduction, however, is not the main objective to apply PCA to the market-basket analysis problem, here.

Second, PCA was used to project data onto a transformed space that is invariant to certain linear transformation, to measure better similarity or dissimilarity of two points. This has been applied in infomation retrieval and is called latent semantic indexing [ I  I]. Again, this is not our objective to apply PCA to the market-basket analysis problem.

We apply it (i.e. the discovery process in Figure 1 is the PCA) in the market-basket analysis problem (see Figure 2)  as a means to discover not the underlying distributions but the significant variables, which are associated positively or negatively to certain events. These events are interpreted as the principal components of the data (i.e. they account for the variability that we o b m e  fmm the data). The acceptable level of informdon loss can be c o n s i d d  as noise or underlying uncenainty in the data that cannot be accounted by the principal components or social events. These could be interpted as a kind of impulsive buying behavior althmgh this type of explanation is anecdote in nature. Figure 2 shows the updated discovely process with a noise component Note that the noise source is added to the function G(.) since it may not be an additive noise.

Figure 2: PCA discovery process with noise.

Formally, let us define PCA and interpret the PCA in the context of market basket analysis. PCA assumes that a principal component Z, is a linear combination of a set of random variables. In this case, these random vanables are    categorical variables indicating the presence or absence of an item i, bougld in a transaction TA. Hence,  2, =cl* x i l  +...+end x i , where {cJ are the coefficients in he linear combination of the items. In principal component analysis, the number of components is at most n (i.e. 0 < k 5 n). These principal components can be expressedin matrix notation form, i.e.

Z =CXT,' where C is the coefficient matrix {cj,h), 7' is the transpose of T. The covariance of Z is:  COvar(Z) =CxZ(T' )xC' where Zm r e m s  the square matrix with the main diagonal equals to the eigenvalues {,I,) of X and all o f f d i a g d elements are zero.

In PCA, the coefficients {q,d are related to the correlation between the obenable random variables {g and the latent Variables {ZJ in the following manner  simulated transaction data is generated by Quest [I21 with 20 items. A set of association rules are obtained by the Apriori a l g o r i b  [I31 with minimum support of 20% and minimum confidence of 30%. In total, 1,157 association rules were discovered and there are 320 unique frequent itemsets. Figure 3 is the scatter diagmn of thc wntidence values and the suppolt values of the mined association rules. n e  confidence values of all the association rules are larger than or equals to their corresponding support values. Only a number (< 10) of cases (called outlien in the Figure) that their support values and ther corresponding conli&nce values are the same  For PCA, the standardized variables 2, are used which requires the computation of correlations between two categorical variables. Since each variable is a probability of occumfy'e, the conelation r(.,.) between the presence and absence of itemi, and i, in a transaction is:  If 25 are standvdizcd variabls, Covar(Z) is the correlation matrix between the latent variables and the correlation betwen observable random variables (i,) and latent variables  We refer to this correlation as the principal component or PC correlation m e  h i g h  the FC correlation r(ij 2,) is the higher the co-ocnmence of item $ with the latent variable Zh. If there are a number of itans or obsavable raniam variables carrehe with the same principal component, then these items would likely cocorrelate or CO-occur with each other as well. Hence, we expea that there should be a significant amoum of transactions with these co-correlate items. Specifically, if a transaction is classified as being influenced by the principal component or observatde event, then the co-correlate items should appear in the transaction.

In PCA, a principal component 2, is uncorrelatd with any other principal component Z.. From the point of view of modeliug, this suggested that social and c d u  events are uncorrelated, which is unlikely to be the case. However, for certain general events like those related to a meal and those related to same mechanical work may have little correlation Hence. this is a strong assumption that PCA made, which are unlikely to be the case in practice.

w. ~NlTlALEXPLORATlON A. Set Up We carried out a pilot study to examine the potential of using PCA for mining association patterns. A set of 10,000  90% .

80%  -70% $eo%  540% 530%  20% 10%  ?50%   O % - l  -" I 0% 20% 40% 60% 80%  SuPllOsn  Figure 3: k a n a  diagram of the supott and confidence values of the sct af association mles mined by the Apriori algorithm  Both thejointprobabilitiesp(in iJ and indinidual probabilities p(iJ are estimated by relative frequency counts. The eigenvalues {U of the carrelationmatrix is shown in Table 1.

The information gain IG refers to the amount of data that can be explained by a particular principal component (say i-th), which is:  IC, =-xlOO%. 4 .

D  According to the initial results, the largest principal component is 20, which accounts for 12.3% of the data, and which can be expressed as a linear weighted s u m  of the items as follows:  Zzo = -O.21SXt -0.OlSXz +0.046X3 -0.339& -0.242XI -O.lOO& +0.289X7 +0.085& +0.45X9 +0.035XlO +0.190X,l +0.277Xlz -0.262Xt3 +0.319X14 -0.099X1~ -0.124X16 +0.3S7X17 +O0.16SXls -0.097X19 -0.037Xzo    The coefficients in the above equation can be converted into a set of correlation values between Z, and indiv iU items, using equation (I) ,  as in Table 2. If we consider those correlation wefficierts larger than 20% as strong then item 7, 9, I I ,  12, 14, 17 and 18 are strongly correlated with the largest principal component. This suggest that if a transaction i5 wincipally influenced by the largest principal component, then it is likely to observe these correlated items although they may not necessarily simultaneously OCNI in the same transaction.

related frequent itemsets, since we conjecture that the smng PC correlations suggest cross buying behavior. Related to the data generation model, ow assumption is that the buyer driven by some social or cultural event needs to buy (multiple) items to participate in the event. Hence, the bansadion would reflect the C O - O ~ ( U ~ ~ ~ ~ C E  of items supporting specific events. These co-occurrences would be related to the frequent itemsets during the association rule mining process and individual association rules are simply the ratio of the probability of two itemsets where one itemset is a subset of the other, i.e. confidence value confc) of the association rule for itemsetX + Y is:  Component Eigenvaiue Informatwn Gain I 0.876 4.38% 2 0.944 4.72% 3 0.942 4.71% 4 0.881 4.40% 5 0.89 4.45% 6 0.901 4.50% 7 0.955 4.77% 8 0.917 4.58% 9 0.963 4.81% 10 0.861 4.30% I I  0.856 4.28% 12 0.982 4.91% 13 0.837 4.18% 14 0.988 4.94% I5 0.817 4.08% 16 1.007 5.03% 17 0.809 4.04% I8 1.034 5.17% 19 1.076 5.38% 20 2.465 1232%  Table I : Eigenvalues of the principal components.

Table 2 Correlation between items and the largest principal component.

B. Comparison To compare whether there are any relationships between mined association rules and the principal components, we examine whether the strong PC correlations for a given component match with any of the association rules and  where Xand Y are disjoint (for the Apriori algorithm).

Therefore, an initial comparison between the principal components and the mining of assodation rules is to examine whether the itemsets of both algorithms are the same. Since the iiequent itemsets of the association rules are always correct, they are used as a reference for comparison  For PCA, there is a need to define what are strong PC- correlations since those are considered to be Co-ocCUrring for a givm event. 1nste.d of defining a threshold for strong PC correlatiors, we examine how the matching performance varies with different level of threshold values so that the tendency and characteristics of matching in relation to the different threshold values can be examined.

For matching, we expect that as long as all the items in a fiequent itemset has strong correlations, then there is a match (i.e. for all items i in th itemset X, r(i, Z) is strong for some principal component Z), since it is possible to recover the frequent itemset by funher analysis. Since there is no threshold to define smng correlation, for the itemset X to be discovered by PCA. the threshold has to be below or equals to the minimm PC correlation value of all the items in X for the principal compnent 2. Since thee are more than one principal component, if any principal component, can match with the itemset X, then X can be discovered by funher analysis. Hence, the threshold can set to the maximum of all the different minimum PC correlation values of the itemsets of different principal componens. We call this threshold below which the itemset X can be discovered by PCA the min-max correlation value for itemset X and formally, it is defined as:  min -max(m = r n a x b { r ( i , ~ ) } j  (2) where Z is a principal component  Figure 4 shows the min-max conelation values against the maximum confidence value of the frequent itemsets This maximum confidence value is the maximum confidence values of all assodation rules, X + Y, such that itemset W = X U Y, i.e.

0% +-- I Figun 4: S c a m  diagram of the min-max melation values (definition 2) of itemsets against their corresponding maximum confidence value (def~tion 3) derived from related associatan rules (see text).

rm  $2 8% 862%  B m  E m  - 0-  - -  lo* ox  0% 10% 20% 30% 40% 50% PC CorrslaUDn Threshold  Figure 5: The effect of PC correlatim theshold on the recall of itemsets.

The maximm was used because if the itemset W leads to any quality association rules, W should be retained rather than filtered According to Figure 4, in almost all cases, except 2, that mar-conflry) > min-mar(r(l. Hence, we expect the discovered itemsets to have a better confidenee value than the min-max correlation value, which may be used as a kind of approximate lower bound to discover quality association rules.

Figure 5 shows the impact of filteringPC correlation values using different threshdds on the recall of itemsets as discovered hy the association rules. As the threshold increases, the recall dropped dramatically. Whether this impact is desirable depends on whether the filtered itemsets are low quality, i.e. have low mar-confvalues.

We observe the effects of setting different PC carrelation threshold in Figure 6 on fdtering the (quality) itemsets. As the thresholdvalue increases the minimumma-confvalues for all the itemsets increass semi-monotonically and drastically after 35%. and the maximum mar-conf values for all the  itemsets dropped by I% (from 84% to 83%). This shows that certain quality assmiation rules are retained with increasing threshold value. In additios the average m-conf values steadily wnverge towards the maximum mar-cod values as &e threshold increases.

Average  F  :"s( minimum g !7zzzE3 10% 0% 0% 1W 20% 30% 40% 50% 60%  PC Comlatlon Threshold  Figure 6 PC correlation threshold against the itemsets discovered.

Figure 7 shows that the amount of PC correlation values that are filtered against the quality ofthe itemsets measured by the ma*-co$values set by different threshold. With just 10% of the top correlation values retamed the itemsets leading to the quality association rules can be found using PCA. In this case, since there are 400 (i.e. 20 x 20) PC correlations, IO% represents retaining only 40 PC correlation coefficients and only 2 correlation coefficients per principal component  90% 80% e 70% 60%  50%  40% 30%  I Minimum e 20% : 10%  0% + I 0% 20% 40% 60% 80% 100%  Y. NoRI.Io (PC) Cwr.Ulb".

Figure 7 The amount of nonzero PC correlations filtered against the quality o f  the itemsets found, measured by the mer-conf values.

Figure 8 shows the size of itemsets discovered hy PCA against the different percentages of n o m m  PC correlations of all the principal components. The general trend is that the less the amount ofpercentages the smaller the itemset size. In panicular, even though the percentage of nonzero PC correlatims is close to 0%. the maximum item sim discovered by PCA can still be as large as 4. Since certain principl components have no n o m s 0  PC correlations when the    percentage is close to 0%, these principal componenb can be discarded without any fwthpr data exploration  ; ;; 2 I4  i s  6 16  ( 2  6 10 ? 6  b 4  8 2  0% 20% xM4&%mpc&4&. BOX IWY  Figure 8: The maxitnum, minimum and average munber of nonzero PC correlations per principal component against the different percentages of n o m  PC correlations for all the principal components.

Another enmuraging aspect of using PCA is that the average number of itemset size is only 15 compared with 20 because some of the PC correlations are zems inherently. When the PC correlation threshold is set at 0.0, the recall of items- is 100%. Hence. there may be some potential in saving processingspgd using less number of items.



V. SUMMARY  In this paper, we have explored the use of finite mixture densities to model the consumer spending patterns. Products hcught for specific social and/or cultural events were considaed to be the underlying driving forces of cross s e l l i effects as multiple related items are needed for consumrs to participate into those events. We applied the principal component analysis (PCA) to discover these events as principal components, taking a simplistic view of the discovery process, for this initial exploration. We used a set of 10,000 tansaction data generated by Quest [ 121 to examine haw PCA discovered c-occurring items may relate to associatim rules mined using the Apriai algorithm.

Our initial understanding is that the frequent itemsets of association rules may relate to the C O - o d n g  items discovered by PCA. These co-occuning items are thought to he the smng correlations between the principal component and the specific item. Instead of using a threshold to define s-g cornlatiom, we use a novel measure called the min- max correlation to illushate graphically how PCA mining of patterns relate to the fiquent itemsets discovered by the Apriori algorithm. Our initial study show that as the conelation threshold is increased, the average, minimum and maximm confidenoe values of the itemsets discovered by the Aprim algorithm and the PCA converges to high confidence value (around 80%). However, this process would loose some high mfidence value rules. Nevertheles., certain high  confidence and nontrivial association rules related to the itemsets remain. Further work is nccessary to demonstrate the extend of the utility of PCA.

Aekoowledgement  This research is supported in part by project funding for the postgraduate study.

Reference  [I] Agrawal, R, T. Imielinsld and A Swami. Mining associationrules between sets of items in large database.

In Proceedings of fhe I993 Infenarional Conferrpnce on Management of Dora (SIGMOD 93). pages 207-216, May 1993.

[2] Agrawal R and R Srikant. Fasf ulgorifhmsfor mining arrocimion mla in large daatases. In YLDB' 94.

September 1994.

(31 Russell, G.J. and A. Petersen. Analysis of cross category dependence in market basket selection, Journd of Refai!ing. 67(3): 367-392.2000.

[4] Meo, R Theory of dependence values, ACM Trans. on Dofabase Sysremr, 25(3): 380406,2000.

[5] Bayardo Jr., R. Ef?iciently mining long patterns k m databases, In Proc. I998 Infsnmional Confexnce on Management of Dofa (SIGMOD 98), pages 85-93.1998.

[6] Bryan F.J. Manly. Mdtiwniufe Sfarirtiml Mefhds  A Primer, Chapman and Hall, 1986  [7] F. Korn, A. Labrinidis, Y. Kotidis, C. Faloutsas, A.

Kaplunwich, and D. Perkovic. Quantifiable data mining using principal component analysis, CS-TR-3754 and UMUCSTR-97-13 techniculreportr. Febmuy 1997.

[SI Manning, A.M. and J.A. Keane. Induing load balancing and efficient data distribution prior to association Rule Discovery in a Parallel Ennvimment, In Pro=. Europron Co&ence on Puralld Processing, pages 1460-1463, 1999.

[9] Cadez, I.V., Smyul P. and Mannila H. Probabilistic modeling of transaction data with applications of profiling, visualization and prediction. In Proc ACM KDD Confereno?, pages 37-46.2001,  [IO] Levin, A.U., k e n  T.K and Moody, I.E. Fast pruning using principal components, Advances in Neural Information Processmg Systems, 6: 3542.1994.

[ I 1 1  AgganvaZ C.C. On the effects of dimensionality reduction on high dimmsimal similarityearch. In ACM PODS Cmfererce, 2001.

[I21 Agrawal, R. M. Mehta, J. Shafer, R. Srikant, A. Amhg and T. Bollinger. The Quest data mining system, In Proc 2nd Int. Con5 Knowledge Discovery and Dam Mining, pages 244249,1996.

[I31 Han, J. M Kamber. Data mining: concepts and techniques. Morgnnffiufmnnn Publishers, 2001.

