Improving Performance of the K-Nearest Neighbor Classifier  by Tolerant Rough Sets

Abstract  In this paper, we report our efforts in improving the pelformance of the k-nearest neighbor classification by introducing the tolerant rough set. We relute the tolerant rough relation with object similarity. Two objects are called similar if and only if these two objects satisfy the requirements of the tolerant rough relation. Hence, the tolerant rough set is used to select objects from the training data and constructing the similarity function. The GA algorithm is used for  seeking the optimal similarity metrics. Experiments have been conducted on some artificial and real world data, the results show that our ulgorithni can improve the performance of the k-nearest neighbor classification, and get the higher accuracy compared with the C4.5 system.

1. Introduction  Classification is a primary data mining method. Given a set of classes, i t  assigns a point XER" to one of those classes. Classification has been applied in many applications, such as the credit approval, pattern recognition, part classification in computer vision, and so on. Many induction algorithms have been proposed for pattern classification problem. For example, ID3, C4.5, k- Nearest Neighbor, Naive-Bayes, T2, Neural-network, association rules etc. However, improving accuracy and performance of classifiers is still attractive to many researchers.

In this paper, we focus on the k-nearest neighbor classification method. The k-nearest neighbor method is a predictive technique. Given a decision table, i t  scans the all table to find the most similar object with a new one, and then assign the class label of the selected object to the new object. Some algorithms use training data as the decision table directly. But in many cases, i t  is not appropriate, because there may exist noise, missing or  redundant data in the training data. Moreover, a big decision table will reduce the efficiency of the classification. Hence, conceptually, a k-NN classification algorithm can be viewed as two parts: constructing a decision table from the training data and finding the k- nearest neighbor for an unseen object. Theoretically, these two steps are independent. Many researchers have developed extensions to the nearest neighbor algorithm [ 171. In this paper, we use the tolerant rough set to select a set of objects from the training data that have the same classification power as the original data set. It can reduce the size of decision table and make the algorithm no sensitive to irrelevant and redundant attributes.

The k-nearest neighbor algorithm needed to construct a similarity metrics. Similarity metrics greatly affects the accuracy of the nearest neighbor classification. Even though we use the same training data set, the algorithms with different distance function will result in completely different predictions. Similarity metrics is usually represented by a distance function based on the attribute values. Hence, determination of the weight of each attribute and the similarity threshold are the most important for the nearest neighbor method. In this paper, we use tolerant rough set to measure the classification power of each attribute and CA algorithm to find the optimal threshold for similarity and weights of each attribute. It means also that we find the optimal tolerant rough relations between objects that provide as small classification error as possible. Experiments have been conducted on some artificial and real world data. The results show that our method can improve the accuracy of k-nearest neighbor classification, and get the higher accuracy compared with C4.5 system.

The remainder of this paper is organized as follows: Section 2 introduces the relevant concept of the tolerant rough set and similarity measure used for classification. In Section 3, we describe how to determine the features weight and similarity threshold optimally by using the CA. Section 4 presents our classification method, and  0-7695-1 128-7/01 $10.00 0 2901 IEEE 167  http://ics.nitech.ac.jp   Section 5 presents the results of the experiments that compare our algorithm with the C4.5 system and no- weighted k-nearest neighbor method. A short conclusion is given in the final section.

2. Tolerant Rough Set  Rough set theory introduced by Z .  Pawlak in the early 198Os[l], is a new mathematical tool to deal with vagueness and uncertainty in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, decision support systems, inductive reasoning, and pattern recognition. The rough set is based on the indiscernibility relation that satisfies reflexivity, symmetry, and transiti- vity. However, in the problem of data classification, it is inconvenient to describe the similarity among data with the indiscerniblity relation, because similarity relation is not transitive. Two data x and z cannot be guaranteed in the same class even though x and y are in one class and y and z are in other class. So, we introduce a tolerant relation as follows:  Let R = (U, A u ( d ) )  be a decision table, where U is a set of elements (objects, examples), A is a set of condition attributes, d is the decision attribute. Each attribute aEA has a set of values V,, called domain of the attribute, and let r(d) be a number of decision values. Let SA = {R, : R,E V, x V, n a? A )  be a set of tolerant relations, where each such tolerance relation satisfies  Reflexive : V VE V, , v R, v Symmetric : vI  R,vz +v2 R, V I  We say that two objects x and y are similar with respect to the attribute a when the attribute values a(x) and a(y) satisfy a(x)R,a(y). Further, we say that two objects x and y are similar with respect to all attributes A, if a(x)R,a(y), denoted as x r A  y.

A tolerance set TS(x) of an object x is defined as  Now we can define the lower approximation r A  *(Y ) and the upper approximation r A  *(Y ) of a set YE U that have the tolerance relation with respect to all attributes A as  TS(X) = ()'?U: X r A  y }  r A  * ( Y )  = (YEU:  TS(y) E Y } r A * ( Y )  = ( y E U : T S ( y )  n Y # @ }  The meaning of two approximations in the tolerant rela- tion is similar to that in the indiscernibility relation. Let Yi=(x?U : d(x)=i }. The set  is called r A  -positive region of partition ( Yi ,i=1,2,. . ., r(d)}. The coefficient  is called the quality of approximation of classification { Yi ,i=l,z,...,r(d)}. It expresses the ratio of all F A  -correctly classified objects to all objects in the table.

POS(TA ,{ d})  = U( TS(X) : 3 i TS(X)C Yi ]  y ( r ,  , ( d} )  = card(POS(TA ,(d)))/Card(U)  To construct a tolerance relation among the data, we need to define a similarity measure that can quantify the closeness between attribute values of objects. Let the similarity measure with respective to the attribute a between two objects x and y be S,(X, y). Then, two objects x and y are called similar with respect to the attribute a if S,(x, y) 5 t(a), where t(a) is a similarity threshold value of the attribute a whose value is in the interval of [0,1]. So, we can relate the tolerance relation with the similarity measure as  a(x)R,a(y) iff &(X, Y) 5 t ( 4 In the classification problem, the commonly used  similarity measure is based on a normalized distance function as  Sa(x, Y) = d(a(x),a(y))/dm,x where d,,, is the maximum value of distance between  two attribute values a(x) and a(y). The choice of distance function depends on the type of application. In this work, we select the absolute difference between attribute values as d(a(x),a(y)) = la(x) - a(y)l due to its computational simplicity. Next, we can extend the similarity measure SA(x,y) between two objects x and y with respect to all attributes by an weight average of similarity measures of all attributes as  SA^, y) = Ca,Sa(x,y), where a, is the number in [0,1] and C a ,  = l .  In the case  of considering all attributes A at the same time, we can relate the tolerance relation with the similarity measure as  X r A  y SA(& y) 5 t(A) where t(A)E [0,1] is a similarity threshold. One of the  most important tasks in the pattern classification using the similarity measure defined above is the optimal determination of the similarity threshold a, and t(A), because its proper determination affects the classification performance greatly. In this work, we apply the GA to solve this optimization problem.

3. Determination of Similarity Thresholds  GAS is any population-based iterative adaptive algori- thm that uses selection, recombination, and mutation operations based on natural selection and biological genetics. GAS has been proven to be a powerful method in search, optimization and machine learning. They encode a potential solution to a specific problem on a simple chromosome-like data structure and apply recombination operators to these structures to achieve optimization.

3.1. Chromosome Representation  When we apply the C A  to determine the optimal similarity threshold values, the inputs into the GA are the information table A = (U, A u ( d ) )  and the similarity measure, and the output from the G A  is a set of optimal     similarity features weight a, and threshold values t(A).

So, when an object is represented by n attributes, the chromosome for the GA consists of n+l consecutive real numbers (aa1 , aa2 ,..., aan, t(A)), where aai (i = 1, 2 ,..., n) represents the feature weight for the ith attribute, the sum of all a, is equal to 1. The last value t(A) represents the similarity threshold that defines the tolerance relation when all attributes A are considered together.

3.2. Initial Population Generation and Fitness Function  The initial gene values in the chromosome are obtained by generating n random real numbers in the interval of [0,1] as the initial weights, and one random real numbers in [0.001 , O S ]  as the initial similarity threshold. We complete initial population by repeating the above operation 21AI times(we set the population size to 21AI), where IAl denote the number of attributes in A. For preventing card(( TS(x) 3 i TS(X)G Y, ) ) = I  for most or all of objects, we set the fitness as: Fimess(rA) =(card(u(TS(x) : 3 i TS(X)G Y, and card(TS(x))>l ) )  + E x card(u(  TS(x) : cnrd(TS(x)). 1 ) ) )/card(U) (O<E< 1 )  We can see Fitness(TA) 5 y ( r A  , ( d ] ) .

3.3. Genetic Operations  The initial populations are then evolved by appropriate genetic operations in order to find a set of optimal similarity thresholds for the pattern classification. The detailed explanation about the genetic operations used for the determination of the optimal similarity thresholds is given as follows.

3.3.1. Selection. All individuals in the population are sorted by their fitness values, and the first individual is the best. In our method, all individuals are selected for mating. We perform crossover to the ith individual and the (i+l)th individual to generate IAl individuals of the next population, where i=l,2,..IAI. If the fitness value offspring is less than that of ith parent, then adding the parent to the next population instead of the offspring. By this way, the individuals with higher fitness could be generated because the fitness of parents is higher. Also, we perform crossover to the jth individual and the (2IAl+l-j)th individual to generate IAl individuals of the next population, where j= 1,2,..,lAl, then the average fitness of individuals of the next population could be increased because the fitness of one parent is higher and the other is lower.

3.3.2. Crossover. Two selected individuals are called parent-] and parent-2, and assume the fitness value of  parent-1 is higher or equal to that of parent-2. For the t(A), set the average value of that in two parents to the offspring. For weights, every crossover results in one offspring by the following steps: 1. For a gene o, in parent-1 and the a gene q, at the same  position as a gene o, in parent-2, if q, =c&, then set q, to the same position in the offspring.

2. Let the number of unset genes in the offspring be m (a) if m<4 then for every unset position in the offspring,  set the average value of genes at the same position in parents to the offspring.

(b) If m(4 then: select [m+1]/2 genes from parent-1  randomly and set them to the offspring. Let the sum of these genes and the genes set by step 1 be sum1 and then, let the number and sum of genes in parent-2 at the unset position be n? and sum2. For any unset position in the offspring, if sum2 = 0 then set (1-suml)/n? to the position else set g( 1 -suml)/sum2 where g is the gene in parent-2 at the unset position, so that the sum of all weigh genes in offspring is 1.

3.3.3. Mutation. Select two weight genes for mutation randomly, replacing a gene pair (gl,g2) with a new one (gl?,g??) where gl+g2=gl?+g2? for eeping the sum of weighs unchanged. For the t(A), we generate a random number t in [-0.02,0.02] and set i t  as t + t(A).

3.3.4. Differentiation of the same individuals. After crossover and mutation, the differentiation of the same individuals will be carried out for variety of individuals in the population. Mutation will operate on the individuals in the population that is same as another until it is different from all other individuals.

4. Classification based on the Tolerant Rough Set and Nearest Neighbor(TRS-NN)  Now, we describe our hybrid algorithm in detail. For convenience, we denote DTS(x)={ y(TS(x): d(x)=d(y)) in the following. Let U be all of training instances, p be an threshold in [0,1] for generate objects(T0) based on tolerant relation, and PRUNE the flag of pruning for TO.

Step 1 Determine the optimal similarity threshold values using GA.

( 1 )  Read training data; (2) Define the similarity measure; ( 3 )  Generate initial population; (4) Perform the genetic algorithm; ( 5 )  Determine the optimal similarity threshold values.

Step 2 Determine the objects for the k-nearest neighbor classification TO1 = ( XE U : card(DTS(x))/card(TS(x)) =1) ;  ,  TO2 = { X E U  : I>card(DTS(x))/card(TS(x)) 2 p ) ;     If PRUNE TO1 = { xeTO1: no y ~ T 0 1  s.t. DTS(X)G DTS(y)) TO2 = { XE T 0 2 :  no YE TO2 s.t. DTS(X)G DTS(y))  T O  = T O  1 u T 0 2 Step 3 Classification For unseen object U, we use the smallest k as possible to classify U by using k-nearest neighbor. That is mean, if there is only one instance x with the minimal value SA(x, U) in TO, then set U to class d(x). And if there is two instances with the minimal value SA(x, U) , and this two instances is in different class, we should check instances with the secondary minimal value SA(x, U) in TO. If we cannot determine class too we should continue the process until we can determine the class of the object U.

End if  Reproduction ( Pselect ) Crossover probability ( P ) Mutation urobabilitv P, 1  5. Experiment Results  0.10 0.70 0.20  For evaluating the classification accuracy of our algorithm, the TRS-NN algorithm was compared with the no-weighted k-nearest neighbor classification ( N W - K " ) and the C4.5 system (release 8). The data sets used in experiments are downloaded from the UCI Machine Learning Repository. Each dataset (with no test data set) was divided into two parts: training data set (two-third of the original size in every class) and testing data set. The basic characteristics of the data sets contains the size of training data and test data, class number, feature number, missing instance number and noise instance number in the following table.

Tablel: Basic characteristics of the data sets used in  Table 2 shows the execution parameters of the C A  that is used for the optimal similarity threshold value. And for the fitness, we set E = 0 at firstly. After evolving 100 times, we get the fitness. If fitness >=0.8 we set E = 0.67 otherwise we set E = 0 . 6 7 ~  fitness.

Table 3 shows the execution results. Because for the same fitness the similarity threshold may be little different, so we run the TRS-NN 10 times for each data se t ,  and  Table 2: Execution parameters for GA.

Excution parameters I Values Pomlation I .xlAl  Number of generations I 2x~ize(train) &Der boundarv of t(A) I 0.50  take the average. In the process of simulation, for the nominal attributes we just change them to number ones artificially. For example, { a,b,c}==>( 1,2,3).And for the missing value feature, we take the possible biggest distance as the distance between this two objects. To analysis the TRS-NN, we gave the result of TRS-NN in the case of p=O, 0.66 and 1, respectively. When compared with NW-NN and C4.5 system (tree, pruning tree, rules),     in the most of datasets we get the higher accuracy, and in the average we get the higher accuracy too.

Now let us analysis the affection of p. In the case of p = 0, it is mean that we use all the objects to nearest-neighbor selection. And in the case of p = 1, we just use the objects of T01 .  We can see that in most of case the set TO1 is just parts of all objects. On the average, when p=1 we can get 85.28% accuracy use 64.39% objects, and get 85.08% accuracy use 37.9% objects after pruning. And when p=0.66 we can get 86.03% accuracy use 90.22% objects, and get 86.09% accuracy use 55.82% objects. And use all objects we get 85.16%. So we can get the same higher classification accuracy just used some necessary objects.

And from the table 3, we can see it is it is no sensitive to irrelevant and redundant attributes, and it is also useful for some missing or noise data too. As the result, it is showed that TRS-NN developed in this paper is effectual.

6. Conclusions  In this paper, we have presented a new approach for classification. It is based on the tolerant rough set and nearest neighbor method. For the better classification, it is important to find the optimal tolerant relations between objects that provide as small classification error as possible. We use a very simple distance function like the absolute difference of attribute between two objects due to its low computational cost and use the genetic algorithm to determine the optimal similarity threshold values. After finding the optimal features weight and similarity threshold, we compute the objects reduction. Based on these well-selected objects, we determine the class for unseen objects using the nearest neighbor approach.

Simulation results show that we improve the performance of the k-nearest neighbor classification and get higher classification accuracy than C4.5 system on some artificial and real word data. One important feature of TRS-NN, in our view, is that we can just use objects reduction for prediction using K". It can be used for continuous feature directly, it is no sensitive to irrelevant and redundant attributes, and it is also useful for some missing or noise data too. Moreover, from the feature weights in tolerant relations we can know the importance of feature.

And when applied for real project, you can select the adequate distance function. We believed that increasing the generation number of C A  can improved the perfor- mance. As the result, we can say that TRS-NN developed in this paper is an efficient algorithm.

Acknowledgement  We would like to thank Professor J.R. Quinlan for the newest C4.5 system.

