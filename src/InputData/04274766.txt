Systems, Man, and Cybernetics October 8-11, 2006, Taipei, Taiwan

Abstract- An efficient algorithm for important class associ- ation rule mining using Genetic Network Programming (GNP) is proposed. GNP is one of the evolutionary optimization tech- niques, which uses directed graph structures as genes. Instead of generating a large number of candidate rules, the method can obtain a sufficient number of important association rules for classification. The proposed method measures the significance of the association via the chi-squared test. Therefore, all the extracted important rules can be used for classification directly.

In addition, the method suits class association rule mining from dense databases, where many frequently occurring items are found in each tuple. Users can define conditions of extracting important class association rules. In this paper, we describe an algorithm for class association rule mining with chi-squared test using GNP and present a classifier using these extracted rules.



I. INTRODUCTION  Association rule mining is the discovery of association relationships or correlations among a set of attributes (items) in a database [1]. Association rule in the form of 'If X then Y (X =4 Y)' is interpreted as "database tuples satisfying that X are likely to satisfy Y". Association rules are widely used in marketing, decision making and business manage- ment. The most popular model for mining association rules from databases is a priori algorithm, the support-confidence framework proposed by Agrawal et al. [2]. This model measures the uncertainty of an association rule with two factors: support and confidence. An association rule is ex- pected to capture a certain type of dependence among items in a database. Brin et al. made the suggestion to measure the significance of association via the x2 (chi-squared) test for correlation used in classical statistics [3]. There have been published numerous papers showing improvements on association rule mining [4]-[7].

A method of association rule mining using Genetic Net-  work Programming (GNP) has already been proposed [8]- [10]. The algorithm using GNP could extract association rules whose consequent are not fixed. The method calculates not only support and confidence of association rules, but also x2 value using GNP. GNP is one of the evolutionary opti- mization techniques, which uses the directed graph structures as genes [11], [12]. GNP is composed of two kinds of nodes: judgement node and processing node. Attributes in database correspond to judgement nodes in GNP. Association rules  Kaoru Shimada, Kotaro Hirasawa and Jinglu Hu are with the Graduate School of Information, Production and Systems, Waseda University, 2-7 Hibikino, Wakamatsu-ku, Kitakyushu-shi, Fukuoka, 808-0135, Japan k.shimada@ruri.waseda. j  {hirasawa,jinglu}@waseda.jp  are represented by the connections of nodes. Candidates of important rules are obtained by genetic operations.

The features of the conventional method using GNP are  as follows:  . Rule extraction is done without identifying frequent itemsets used in a priori-like methods. The method can be applied to rule extraction from dense databases, where many frequently occurring items are found in each tuple.

. Conditions of important association rules are defined flexibly by user. The definition can include the minimum threshold x2 value. x2 value of each rule is measured by the feature of GNP's structure.

. Extracted rules are stored in a pool through generations.

GNP evolves in order to store new interesting rules in the pool, not to obtain the individual with high fitness value. The method cannot extract all the rules meeting the given definitions of importance, but extracts important rules sufficiently enough for user's purpose.

. Extracted important association rules in a pool are reflected in genetic operators as acquired information.

Mutation changes the contents of the nodes using the knowledge of interesting rules acquired in the pool.

. The proposed method can easily extract positive and negative association rules just by defining the judgement nodes including negative attributes.

Rule based classification systems have been widely used in real world applications because of the easy interpretation of rules. Classification rule mining aims to discover a small set of rules in the database that form an accurate classifier. Liu et al have proposed a framework to integrate classification and association rule mining [13]. When the consequent item in the rule is the classification class attribute, they can be used for classification. There have been proposed associative classifiers mining the training set as an a priori-like method or using FP-tree as association rule discovery algorithm [14].

These methods generate the association rules in the support- confidence framework. A training data set often generates a huge set of rules. So, it is challenging to store, retrieve, prune and sort a large number of rules for classification efficiently.

In addition, in most conventional associative classification methods, the rule with the highest confidence is used for classification. However, such a classification may not always be the correct one, because it is not easy to identify the most effective rule.

The association rule mining using GNP measures the  significance of the association via the x2 test. Users can  1-4244-0100-3/06/$20.00 C2006 IEEE 5338    define conditions of important association rules. Therefore, all the extracted important rules can be used for classification directly. In this paper, we propose an efficient algorithm for important class association rule mining using GNP. The con- ventional algorithm using GNP could extract the important association rules in the case of free consequent. However, the proposed method extracts the class association rules whose consequent are restricted to the classification class attribute.

In addition, we also propose methods for building a classifier using extracted important class association rules.

This paper is organized as follows: In the next section, the outline of GNP is reviewed briefly. Some related concepts and definitions on association rules are presented in Section

III. In section IV, a new algorithm capable of finding the important class association rules using GNP and the methods for building classifiers are described. Performance results are presented in Section V, and conclusions are given in Section VI.



II. GENETIC NETWORK PROGRAMMING (GNP)  In this section, the outline of conventional GNP [11], [12] is reviewed. GNP is one of the evolutionary optimization techniques, which uses the directed graph structures as solutions. The basic structure of GNP is shown in Fig. 1.

GNP is composed of two kinds of nodes: judgement node and processing node. Judgement nodes correspond nearly to elementary functions of Genetic Programming (GP). Judge- ment nodes are the set of Ji, J2, ..., Jp, which work as if-then type decision making functions. On the other hand, processing nodes are the set of P1, P2, ..., Pq, which work as some kind of action/processing functions. The practical roles of these nodes are predefined and stored in the library by supervisors. Once GNP is booted up, firstly the execution starts from the start node, secondly the next node to be executed is determined according to the connection from the current activated node.

The genotype expression of GNP node is shown in Fig. 2.

This describes the gene of node i, then the set of these genes represents the genotype of GNP individuals. NTi describes the node type, NTi = 0 when node i is start node, NTi 1 when node i is judgement node and NTi = 2 when node i is processing node. IDi is an identification number, for example, NTi= I and IDi= 1 mean node i is JI. Cil, Ci2,  denote the nodes which are connected from node i firstly, secondly, ..., and so on depending on the arguments of node i. All individuals in a population have the same number of nodes. The following genetic operators are used in conventional GNP. Crossover operator affects two parent individuals. All the connections or contents of the uniformly selected corresponding nodes in two parents are swapped each other by crossover rate P, Mutation operator affects one individual. All the connections of each node are changed randomly by mutation rate of P,  D Start Nodc  0 : Judgement Node  0 : Processing Node  Fig. 1. Basic structure of GNP  INT,I J)i || C-, I . . . C | |  Fig. 2. Gene structure of GNP (node i)

III. ASSOCIATION RULES AND CLASSIFICATION  A. Association Rules  The following is a formal statement of the problem of mining association rules. Let I {i l, i2,...,il} be a set of literals, called items or attributes. Let G be a set of transactions, where each transaction T is a set of items such that T C I. Associated with each transaction is a unique identifier whose set is called TID. We say that a transaction T contains X, a set of some items in I, if X C T. An association rule is an implication of the form X 4 Y, where X cI, Y CI, and XnY =0. X is called antecedent and Y is called consequent of the rule. In general, a set of items is called an itemset. Each itemset has an associated measure of statistical significance called support.

If the fraction of transactions containing X in G equals t, then we say that support(X) = t. The rule X X- Y has a measure of its strength called confidence defined as the ratio of support (XU Y) /support (X). An example is shown below using Table I. Let item universe be I {A=,A2,A3,A4,Z} and transaction universe be TID {1,2,3,4}. In order to extend our research not only to market basket problems but also to dense databases, we indicate the items of the transaction by 1 or 0 as shown in Table I. In Table I, itemset {Al ,A3} occurs in two transactions of 1 and 3 in TID. So, its frequency is 2, therefore, its support, that is, support((Al 1) A (A3 1)) becomes 0.5. Itemset {A1,A3,Z} occurs in the transaction of 3 in TID. Its frequency is 1, so its support, i.e., support((AI = 1) A (A3 = 1) A (Z= 1)) becomes 0.25.

Therefore, support((AIl 1) A (A3 = 1) X (Z 1)) = 0.25, and confidence((AI = 1) A (A3 = 1) X (Z= 1)) = 0.5.

The a priori algorithm searches large (frequent) itemsets  in databases [2]. This algorithm finds all the rules meeting user-specified constraints such as minimum support or min- imum confidence. However, this algorithm may suffer from large computational overheads when the number of frequent     TABLE I  AN EXAMPLE OF0)ATABASI.

TID A1 A2 A3 A4 Z I 1 0 1 0 0 2 0 1 1 1 1 3 1 1 1 0 1 4 0 0 1 1  TABLE II  THE CONTINGFENCY OF X AND Y.

Y LaMy X Nxv N(x- xv)  Nz N(x -) Nx --X N(y-xy) N(l-x-y+xv) N(1-x)  N(y- z) N(1-x-y+z) -col Ny N( - v) N  ( N: the number of tuples ( ITIDi) )  itemsets is very large. Many variations of this approach, such as the hash-based algorithm have been studied for efficiency [4]. There has been also a recent trend in applying the rule mining to dense databases, such as census data analysis [3], where any or all of the following properties are found: many frequently occurring items; strong correlations between several items; many items in each tuple [5].

Calculation of x2 value of the rule X at Y is described as follows. Let support(X) = x, support(Y) = y, support(X U Y) = z and the number of database tuples equal N. If events X and Y are independent then support(XU Y) xy.

Table II is the contingency of X and Y: the upper parts are the expectation values under the assumption of their independence, and the lower parts are observational. Now, let E denote the value of the expectation under the assumption of independence and 0 the value of the observation. Then the x2 statistic is defined as follows:  X2= x (O E)2 Al/Cells (  We can calculate x2 using x, y, z and N of Table II as follows:  2 _ N(z _Xy)2 (2) Xxy(l- x)(1 -y).

This has 1 degree of freedom. If it is higher than a cutoff value (3.84 at the 95% significance level, or 6.63 at the 99% significance level), we should reject the independence assumption.

B. Classification Using Association Rules  Building accurate and efficient classifiers is one of the essential subjects of data mining. The main idea of the classification is to discover interesting patterns in training set of data to build a classifier model able to predict the unknown classification class attribute. Association rules can be applied to classification, when the consequent item in the rule is the classification class attribute.

Rule based classification usually involves two stages: training and testing. In the training stage, an important rule set is generated for classification by training set of data.

Then, in the testing stage, obtained rule set is applied to test data without class information. Then, unknown classifi- cation class attributes in the test data set are predicted. The proportion of predicting test data incorrectly is the error rate of classification.

Liu et al have proposed to integrate classification rule mining and association rule mining techniques [13]. The algorithm is called CBA (Classification Based on Asso- ciations). The integration is done by focusing on mining a special subset of association rules whose consequent is restricted to the classification class attribute. The rules are called class association rules (CARs). The associative clas- sification framework consists of the following three steps:  * discretizing continuous attributes, if any * generating all the class association rules (CARs), and * building a classifier based on the generated CARs.

They generate the complete set of CARs that satisfy  the user-specified minimum support (minsup) and minimum confidence (minconf) constraints. minsup has a strong effect on the quality of the classifier produced. If minsup is set too high, possible rules satisfying minsup, but with high confidences will not be included, and the CARs may fail to abstract the effective rules. Thus, the accuracy of the classifier suffers.

There are roughly two types of models for building classifiers based on association rules: ordered rule based and unordered rule based. CBA is the ordered rule based model.

Rules are organized in the descending accuracy order. The first matching rule in the sequence makes the prediction.

When there is no matching rule, the class of the class attribute is predicted on the default base. On the other hand, unordered rule based model is to compare the accuracy of all possible classes obtained from the multiple rules. The class getting the highest accuracy is the prediction.

Antonie et al have proposed a new way to prune irrelevant classification rules using a correlation coefficient without jeopardizing the accuracy of the associative classifier model [15]. They use the scoring scheme of confidence in the classification. Although the confidence averaging seems to work well in many cases, there are some examples where the classification is inaccurate while good rules exist.



IV. CLASS ASSOCIATION RULE MINING USING GNP AND BUILDING A CLASSIFIER  In this section, a method of class association rule mining using GNP is proposed. The extracted rules can be used for classification directly, because they satisfy the conditions of importance given by users.

Extracted rules are stored in a pool all together through the generations. GNP individuals evolve in order to store new interesting rules in a pool, not to obtain the individual with high fitness value. Therefore, the method is fundamentally different from other evolutionary algorithms in its evolution- ary way.

Let Ai be an attribute (item) in a database and its value be 1 or 0, and Z be the set of class labels. The proposed     method extracts the following association rules with x2 test: (Am = 1) A A (An = 1) 4 (Z = C), (C = 0,1,2, ... I,K) A. GNP for Class Association Rule Mining  Attributes and their values correspond to the functions of judgement nodes in GNP. The connections of nodes are represented as association rules. Fig. 3 shows a sample of the connection of nodes in GNP for class association rule mining. Pi is a processing node and is a starting point of association rules. "Al 1", "A2= 1", "A3= 1" and "A4 =1" in Fig. 3 denote the functions of judgement nodes.

The connections of these nodes represent association rules, for example, (AI = 1) X (Z = C), (AI = 1) A (A2 = l) X (Z C), (A1 1)A(A2 = l)A(A3 = l) (Z = C) and (Al = 1)A(A2 1)A(A3 = l)A(A4= 1)#= (Z C). The details of it are described later.

GNP examines the attribute values of database tuples  using judgement nodes and calculates the measurements of association rules using processing nodes. Judgement node determines the next node by a judgement result of Yes or No corresponding to Yes-side or No-side. For example, in Table I, the tuple 1 C TID satisfies A1 1 and A2 i 1, therefore, the node transition from PI to P2 occurs in Fig. 3. In addition, each judgement node examines the 'Z = C' at the same time.

Yes-side of the judgement node is connected to another judgement node. Judgement nodes can be reused and shared with some other association rules because of GNP's feature.

Fig. 4 shows a basic structure of GNP for class association rule mining. In Fig.4, No-side of judgement nodes are abbreviated. The kinds of the judgement node functions equal the number of attributes in a database.

Each processing node has an inherent numeric order (PI, P2, ..., P,) and is connected to a judgement node. Start node connects to P1. Examinations of attribute values start at each processing node. No-side of the judgement node is connected to the next numbered processing node. If the examination of attribute values from the starting point P, ends, then GNP examines the tuple 2 C TID from P1 likewise. Thus, all tuples in the database are examined.

When the examination ends, then we can obtain mea-  surements of rules described as follows. The total number of tuples moving to Yes-side at each judgement node is calculated for every processing node, which is a starting point for calculating association rules. In Fig. 3, N is the number of total tuples, and a, b, c and d are the numbers of tuples moving to Yes-side at each judgement node. For example, in Fig. 3, the number of tuples 'b' indicates support((AI = )A (A2= 1)), and GNP also calculates such as support((AI = I)A(A2= 1)A(Z= 1)) at the same time.

Using support((Al = l)A (A2 = l) A (Z 1)) = b(Z = 1), support and confidence of the rule (A1 1) A (A2 = 1) =# (Z= 1) become b(Z= 1)/N and b(Z= 1)/b, respectively.

As support(Z= 1) is known in advance, we can also obtain the X2 value of the rule (A1 = 1) A (A2 = 1) => (Z = 1).

If the transition using Yes-side connection of judgement nodes continues and the number of the judgement nodes from the processing node becomes a cutoff value (given maximum  pIA= A2= A 1 A4=I  -. Yes .......P. No  0 : Judgement Node, 0 : Processing Node  Fig. 3. A connection of nodes in GNP for class association rule mining  GNP (individual)  00 0<X O  Fig. 4. Basic structure of GNP for class association rule mining  number of attributes in extracted association rules), then Yes- side connection is transferred to the next processing node obligatorily.

Now, we define important association rules satisfying the  following: 2 2  X >Xmin, support > minsup,  confidence > support (Z = C)  (3) (4) (5)  X~iin and minsup are the minimum threshold X2 and support value given by users. We use the condition of (5), when the consequent of the rule is (Z = C), (C = 0, 1, 2,...,K).

The extracted important class association rules are stored  in a pool all together through generations. When an important rule is extracted by GNP, the overlap of the attributes is checked and it is also checked whether the important rule is new or not, i.e., whether it is in the pool or not.

B. Fitness and Genetic Operators  The number of processing nodes and judgement nodes in each GNP individual are determined by users. All individuals     in a population have the same number of nodes. The connec- tions of the nodes and the functions of the judgement nodes at an initial generation are determined randomly for each GNP individual. GNP needs not include all the functions of judgement nodes and the number of each function is not fixed.

Fitness of GNP is defined by  F = JIX2(i) + 1O(n(i) -1) + anew(i)} (6) iel  The terms in (6) are as follows: I: set of suffixes of extracted important association rules  satisfying (3), (4) and (5) in a GNP individual X2(i): x2 value of rule i.

n(i): the number of attributes in the antecedent of rule i.

anew (i): additional constant defined by  a<ne W { an, (rule i is new) 0 (rule i has been already extracted)  72(i), n(i) and anew(i) are concerned with the importance, complexity and novelty of rule i, respectively. We consider that the fitness represents the potential to extract new rules.

The rule changing an attribute to another one or the  rule adding some attributes can be candidates of important rules. We can obtain these rules effectively by GNP genetic operations, because mutation and crossover will change the connections or contents of the nodes. We use three kinds of genetic operators; crossover, mutation-I (change the con- nection of nodes) and mutation-2 (change the function of judgement nodes).

At each generation, individuals are replaced with new ones by a selection rule. The individuals are ranked by their fitnesses and upper 1/3 individuals are selected. After that, they are reproduced three times for the next generation, then three kinds of genetic operators are executed to them. These operators are executed for the gene of judgement nodes of GNP. All the connections of the processing nodes are changed randomly in order to extract rules efficiently. The above number 1/3 is determined experimentally, which is not so sensitive to the results.

We demonstrate the above more concretely using 120  individuals at each generation. The individuals are ranked by fitness values, after that, the top 40 individuals are selected, reproduced three times and the following genetic operators are executed to them.

Crossover: crossover we used is the uniform crossover.

Judgement nodes are selected as the crossover nodes with the probability of PC. Two parents exchange the gene of the corresponding crossover nodes. 40 parent individuals for crossover operation are divided into 20 pairs of parents and new 40 offspring are produced.

Mutation-1: Mutation-i operator affects one individual. The connection of the judgement nodes is changed randomly by mutation rate of Pm1. The 40 parent individuals for mutation- 1 reproduce new 40 offspring.

Mutation-2: Mutation-2 operator also affects one individual.

This operator changes the functions of the judgement nodes  by a given mutation rate &m2. New 40 offspring are repro- duced from 40 parent individuals by mutation-2.

If the probabilities of crossover and mutation are set at small values, then the same rules as in the pool are extracted repeatedly and GNP tends to converge at an early stage.

These parameter values are chosen experimentally.

As one of the features of the proposed method, we can  select the new functions of the mutated judgement nodes using the frequency of attributes of the extracted rules in the pool. When mutation-2 is carried out, we can define the probability of selecting the attribute Aj for judgement nodes by the following PJ:  pg ng(Aj) + c (8) XkEK (ng (Ak) + C)  where ng(Aj) is the frequency of the attribute Aj in the rules extracted in the latest g generations. K is the set of suffixes of attributes. c is a constant given by users. If no rules are extracted in the latest g generations, then Pg is equal to the inverse of the number of attributes. We could use all the extracted rules in the pool, however, it has been found that using extracted rules in some of the latest generations is better than using all rules [10].

C. Building a Classifier This subsection presents the methods for building clas-  sifiers using class association rules extracted by GNP. The extracted rules in the pool can be used for classification di- rectly, because they satisfy the conditions of importance. The following two methods for building classifiers are proposed: Input The associative classifier, i.e., a set of extracted rules  by GNP, A new data to be classified Output Class predicted by the classifier  . Method 1 1) total(C): compute the total number of rules satisfying Z = C in the classifier (C = 0, 1,2, ...,K) 2) predict1(C): compute the number of rules in the classifier, whose antecedent matches the items of the new data and satisfy Z = C (C = 0, 1, 2,..., K) 3) score l (C) p,dictl(C)) (C = Or 1, 2 , ...., K) 4) predict that the new data belong to the class having the highest scoreI  . Method 2 1) total(C): compute the total number of rules satisfying Z = C in the classifier (C = 0, 1, 2, . . ., K) 2) calculate the distance Di(C) between the items of the new data and rule i which has the following m attributes in antecedent: (Aj, il) AA\ (Aj. = l) => (Z C).

Now, when the new data have t attributes which satisfies Aj4 = 1(1 < k < m), then, the new data have m-t attributes whose value is Aj, $& 1.

So, the distance Di(C) is calculated as max{t (r t) o} 3) sum Di(C) to calculate predictd (C): predict2(C) = EiDi(C) 4) score2(C)= pdiCt$2(C) (C=0,1,2,...,K) 5) predict that the new data belong to the class having the highest score2

V. EXPERIMENTAL RESULTS  We tested our methods on some datasets from UCI ML Repository [16]. Class association rules with x2 test were extracted by GNP, then built classifiers using extracted rules.

Descretization of continuous attributes is done using the Entropy method. All the categorical attribute values are transformed to a set of attributes, whose attribute value is 1 or 0. A 10-fold cross validation was performed on each dataset and results are given as the average of the errors over 10-fold cross validation.

We use (3) ( 2, = 6.63 ), (4) (minsup = 0.05), (5),  (7) (anew = 150) and 1 < n(i) < 8. Class association rules are extracted for each class using each specific GNP. The population size is 120. The number of processing nodes and judgement nodes in each GNP individual are 10 and 75, respectively. The conditions of crossover and mutation are P,c= 1/5, Pml = 1/3 and Pm2= 1/5. We used the probability P? of (8), i.e., the inverse of the number of attributes, for selecting the attributes of judgement nodes in mutation- 2. The condition of termination is 500 generations. All algorithms were coded in C. Experiments were done on a 1.50GHz Pentium M with 504MB RAM.

The results of classification using test data are shown in  Table III. cleveland and breast-w have 2 classes for predic- tion. The transformed cleveland and breast-w dataset consist of 25 attributes, 297 tuples and 18 attributes, 683 tuples, respectively. allp,,I, 0.8 and 0.9 in Table III denote the cases using all rules, the rules satisfying confidence > 0.8 and the rules satisfying confidence > 0.9 in the pool, respectively.

The results for CBA and C4.5 are taken from [13]. The results show that the proposed method has comparable accuracy with conventional models. It is found that using the condition of minimum confidence affects the accuracy of classification a bit. Especially, the error rate increases in cleveland by the increase of the threshold of confidence. Medical dataset, sometimes, includes different characteristics individual by individual in data set, so we can say that it is not good to use too strict rules for classification of medical data. In this simulation, minsup we used is 5%, however minsup in conventional models such as CBA is 1 %. Therefore, our method can avoid over fitting and can acquire the rules with high reliability for classification.

Table IV shows the error rates versus the number of used rules for classification. Each extracted rules in the pool has an inherent numeric order by its appearance. We examined 6 cases of using 10, 30, 100, 300, 1000 rules by numeric order and all the extracted rules for each class. It is found that using 300 rules is almost equivalent to using all rules in the pool. Table V shows averaged run-time versus the number of extracted rules in the pool. This shows that the proposed method extracts the rules sufficiently enough for classification in a short time.

Table VI shows the error rates of using long rules, whose antecedent contains 7 or 8 attributes. A small number of rules in method 1 cannot predict the class accurately. On the other hand, method 2 is effective even in the case of a small  number of rules. Using a sufficient number of rules reduces the error rate in both methods.

In the proposed method, all the extracted rules satisfy the conditions of importance and have the qualification for the classifier. Therefore, we need not store, retrieve, prune and sort a large number of rules for classification as conventional methods. This is one of the features of our method. The proposed method cannot extract all the rules meeting the given definition of importance, but extracts important rules sufficiently enough for user's purpose in a short time. As a result, the proposed method suits the classification problems that need real-time processing. In addition, the method can be used for incomplete test dataset such as having missing data.



VI. CONCLUSIONS  In this paper, we propose an efficient algorithm for im- portant class association rule mining using GNP. Instead of generating a large number of candidate rules, the method can obtain a sufficient number of important association rules for classification. The method measures the significance of the association via the y2 test. Therefore, all the extracted im- portant rules can be used for classification directly. We have performed some experiments and estimated the performances of the method. The results showed that the proposed method extracts the important class association rules effectively. The proposed method suits class association rule mining in dense dataset and uses x2 value for importance, so it could be applied to medical dataset. We are currently studying the applications of our method to real world databases.

