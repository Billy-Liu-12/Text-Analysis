Support IR Query Refinement by Partial Keyword Set

Abstract  As our early work, we proposed a data mining model for query refinement using Association Rules (ARs) mined among keywords of a large document database. In this pa- per, first, we explicitly identify principles of a data mining model for  query refinement, and show that our model satis- fies the principles. Second, we propose a mechanism to re- duce the large number ofARs using a concept called ?prime keyword?. The effectiveness of our approach will be dis- cussed. Considering the fact that most of today?s searching engines on the Internet are keyword based, our approach can be easily extended to support Web applications.

1 Introduction  Due to the rapid growth of on-line documents, the dif- ficulty users are facing is that a large number of irrelevant documents will be returned when a query is under-specified or contains ambiguous keywords. However, the task of for- mulating an effective query is difficult in the sense that it requires users, without any knowledge about the collection of documents, to predict the keywords that will appear in the documents the users want to retrieve. Therefore, i t  is highly desirable that a system can provide some mechanisms to help users to refine their queries.

Most of today?s searching engines on the Internet are keyword based. Some of them allow users to refine their queries using additional keyword(s). However, the issue we argue in this paper is how likely a user can choose a set of proper keywords. If a user knows what keywords are good  Nobuo Ohbo Institute of Electronics & Information Sciences,  University of Tsukuba, Tsukuba, Japan 305.

ohbo @is. tsukuba.ac.jp  for refinement, hekhe would possibly use them in hisher original query. It is uneasy to refine queries without any knowledge about the set of documents. A possible solution is to use categories to categorize all documents. However, the categorization itself is a difficult task to systematically categorize a huge number of growing documents. As our early work in [LCYO98, LCN0991, we proposed to pro- vide users with a candidate list of keywords, in order to help users to refine their queries step-by-step. The candidate list is systematically constructed using association rule (AR) mining techniques. ARs are based on a statistical method in which the support makes rules meet certain frequency and the confidence describes degree of relation. The confidence also indicates that relationships are not symmetric, as con- fidences of X * Y and Y * X are often different from each other. It is very important that, unlike the similarity and co-occurrence, ARs are not symmetric.

As related work, query expansion [Peat911 and query feedback [Salt901 were proposed to make use of relation- ships such as similarity between keywords. In brief, the technique of query expansion is based on an association hypothesis which states: if an index term is good at dis- criminating relevant from non-relevant documents then any closely associated index term is also likely to be good at this. On the other hand, the relevance feedback in [Salt901 is a method in which queries are modified using the key- words obtained from the result set of documents if they have high similarity to the keywords used in the queries.

However, these approaches have the difficulties to differ- entiate between relevant and non-relevant documents, be- cause the keywords with higher similarities intend to in- clude the similar documents. LChen971 and IVde971 in- dependently noticed the problem that too many documents  0-7695-1393-UO2 $17.00 0 2002 IEEE 245  http://tsukuba.ac.jp http://tsukuba.ac.jp http://tsukuba.ac.jp   will be retrieved by a query which is under-specified or con- tains ambiguous terms. The solution proposed in [Vde97] is an algorithm called RMAP which provides suggestions to refine the user query by calculating ranks of terms. How- ever, RMAP provides one-step refinement. In contrary, we provide systematic stepwise query refinement. Our system reduces the number of candidates that a user need choose to refine hisher query, and guarantees no lost of docu- ments helshe wants to access. Like the methods proposed in [Salt90, Peat911, our approach is based on co-occurrence of keywords. Unlike these approaches, we use ARs, and we focus on screening retrieval results. In addition, different from the work [Peat91, Salt901, which automatically refines a user query, our interactive interface enhances the precision because the user interactively chooses what hdshe needs to refine queries.

The rest of this paper is organized as follows. Section 2 gives an overview. The preliminaries of this study is given in Section 3. Section 4 introduces our early work. Sec- tion 5 identifies five principles of a data mining model for query refinement. The main contribution of this paper is discussed in Section 6, in which we introduce a new con- cept called ?prime keyword?, and describe how to use prime keywords to significantly reduce keyword candidates. Sec- tion 7 concludes our discussion.

2 Anoverview  The overview of our query refinement approach is illus- trated in Figure 1. First, the keywords will be extracted from a document database. ARs are mined for keywords, and are stored and managed in a rule base. Second, from the rule base, the system selects ARs with respect to the user?s query. For an AR, Q + R, Q is a set of keywords in the query to be refined, and R is a set of keywords for refinement. We call R as refinement candidate. All re- finement candidates will be displayed including statistical information such as support and conjidence which indicate the screening effect. Third, a user interactively selects a re- finement candidate, based on the statistics information, to refine hisher query. This process will be carried out repeat- edly until the user satisfies the results of the query.

Figure 2 illustrates an example of query refinement. To make the figure simple, each node shows only the differ- ence from its parent node. In other words, each node in fact contains all the keywords of its ancestor nodes. In this ex- ample, suppose that a user issues a query by keyword ?Pic- ture communication?. The system shows that there are 784 hits for the query, and further provides both refinement can- didates and the number of document hits. The user query is then refined by adding any of the candidates to the orig- inal query. Suppose that the user chooses ?Picture com- pression?. Then by adding this keyword the number of hit  documents will be reduced to 177. In the same way, if the user adds ?Picture quality? further to the query, the final query becomes {Picture communication, Picture compres- sion, Picture quality} and 38 documents will be returned.

This means that any of the 38 documents contain all these three keywords. In Figure 2, the dot line arrows show some other possible ways of query refinement.

3 Preliminaries  In this section, in brief, we give the notations that will be used in the following discussions. Let D and lK be a set of documents and a set of keywords, respectively.

An operation p ,  which extracts keywords from a docu- ment d E D, is defined as p ( d )  = { I C  I ( I C  E K) A ( I C  is a keyword included in d ) } .  Furthermore, let D c D.

U d E D  p ( d )  is denoted by p ( D ) ,  and in particular p(D) = K A query is a set of keywords Q (C K). A query evaluation retrieving all the documents that contain all the given key- words in Q is defined as o(Q) = { d  [ d E D A Q p(d) ) .

Let X and Y be subsets of I[& The support and the confi- dence of a rule, X + Y ,  are calculated as Sp t (X  3 Y )  = Io(X U Y ) ]  and C n f ( X  + Y )  = In(X U Y)l / lo(X)l , respectively.

In addition to the minimal support as used in [Fayy96, Han95, Sava95, Srik961, we defined the maximum support and the maximum confidence for the purposes of query re- finement, and call it a base condition as follows. An AR X 3 Y satisfies a base condition if and only if Os, < S p t ( X  =+ Y )  < Os, and C n f ( X  =+ Y )  < e=,. Here, Os,, Os, and Oc, are minimum support, maximum support and maximum confidence of the rule, respectively. Thresh- olds 6?s affect the refinement hence must be chosen care- fully. Our principles proposed in section 5 will be good heuristics in making the choice.

4 Early Work: Stepwise Refinement  A stepwise refinement is to refine a query step-by-step.

In each single step a keyword is added. In this section we introduce two essential concepts, stem rules and coverage, which form the fundamentals of stepwise refinement. Stem rules are the only rules that we need to store in the rule base. All the other applicable ARs can be generated from the stem rules at run time. The idea behind the stem rules is to significantly reduce the number of candidates stored in the rule base. Consequently, as a trade-off, a few addition steps of refinement might be needed. For example, suppose a user issues a query with a keyword which has 100 co- occurrence keywords. If the user refines hisher query by adding one keyword at one step, there are at most 100 steps and at each step one of the 100 keywords can be selected.

.- Original c 4 u y  / ?  Rehnement \ \ \ s?upport system \  I - - / -  \ \ I  I I  I I  \ \  \ I \ / \ document base  _ _ - - #  \ - - - - - - -  Figure 1. Overview of query refinement support  Multi-media (+=295)  Digital communication Documents f ----- - --  ,_._.-.I- docl ? I  ...... /?(+=91) /! doc3 i standards ,,,,,I Coding standards -.. ....... ;  .............

Picture  (=784)  Figure 2. An example of refinement  Totally there are at most (100 + 99 + ... + 1) ARs gen- erated and stored. On the other hand, if we store all rules of any subset of the 100 keywords, then the user may have to choose candidates from up to 21?0 rules, although the re- finement can be finished in only one step. Our solution for avoiding the problem of combination is to use rules with only a single keyword in its right hand side. In the example of Figure 2, this means that all refinements indicated by dot line arrows become impossible. Coverage ensures that all documents can be possibly retrieved using the keywords in the stem rules.

in keyword-based retrievals, if a set of keywords does not appear in any documents, then the retrievals by this set of keywords will get no hits.

To extract a set of stem rules from a document database UD, the algorithm of [LCYO98] is adopted. Unlike Agrawal?s algorithm in ([Agra93]) which computes 1- itemsets, 2-itemsets, .... n-itemsets from a set of items K, [LCYO98] generates a set of stem rules from each docu- ment instead. At the step of computing n-itemsets for each n, the time complexity of [Agra93] is 0(,Cl~\), because all combinations of items (keywords) K must be checked.

On the other hand, the time complexity of our algorithm is O ( , C ~ , , ~ ~ ~ ~ ) ,  because we only need to check keywords in a document.

We explain the concept of coverage using the example of Figure 2. The un-arrowed dot lines link the documents with the keywords (on the path to the root) they contain. For  Definition 1 Stem Rule: Given two rules TI and 7-2. If the fact that T I satisfies the base condition implies that ~2 satisfies the same base condition, we say that rule r2 is derived from rule T I .

A stem rule is a rule that can not be derived by any rules.

This definition restricts the keywords of a rule to appear in at least one document. This restriction is natural because,  example, ??doc4? contains three keywords: ?Picture qual- ity?, ?Picture compression? and ?Picture communication?.

In this example, if the keyword ?Coding standards? does not appear in the candidate list, then however {?Picture commu- nication?, ?Picture compression?) be further refined, docu- ment ?docl? can never be retrieved.

To guarantee that the system does not cut off any part of the results that should be retrieved from the original query, the concept of ?coverage? was introduced. The general def- inition of coverage is as the follows given in [LCN099].

Definition 2 Coverage: Let K(C 2?) be a powerset of W and D(C D) a set of documents. If  D E  U a(&) QEK  then we say that IC covers D ,  or K is a coverage of D. In a similar fashion, K is also called a coverage of K? (2 2w) iff  By the definition, 2K is a coverage of D, and 2 P ( D )  is a coverage of D. We also say a set of rules, R, covers D if {X U Y I X + Y E R} covers D. Naturally, coverage IC of D is a minimum coverage if and only if D can not be covered by any pure subset of K. Minimum coverage is defined in order to exclude redundant keywords from the rule space. Especially, union of all the sets in a minimum coverage of D is called prime keywords. Based on prime keywords, in Section 6, we will discuss prime graph and transfer graph and develop constructing algorithms. The algorithm of generating minimum coverages [LCN099], as well as the algorithm of generating stem rules, will be inte- grated.

5 Five Principles  In this section, we identify five principles of our mining- based query refinement approach.

0 pl  (Screening effect): The first issue is about screen- ing effect. In [Agra93, Fayy96, Han95, Sava95, Srik961, ARs are defined as ones whose support and confidence exceed given thresholds called a minimum support and a minimum confidence. Given a query containing a set of keywords Q. When an AR {Q + Y} is picked up, the keywords included in Y will be added to a submitted query Q. However, if the AR has a high confidence, then the effectiveness of screening the documents being retrieved is very small. To ad- dress this issue, maximum support and maximum con- fidence are proposed to be used in conjunction with minimum support and minimum confidence.

p2 (Appropriate number of refinement candidates): One extreme example is, the number of refinement candidates should never exceed the number of hits of the original query. If the number of keywords in a doc- ument database is t ,  then a naive method must check t x ( t  - 1) combinations to generate all ARs between two keywords, and so on. Finally, it must check expo- nential combinations for generating all ARs. To avoid a large number of rules being generated, a technique called stepwise re$nement is adopted based on stem rules([LCY098]) by which the other ARs can be de- rived.

p3 (No oversight of documents (coverage)): Unlike other approaches, we believe that it is hard for a sys- tem to predict what users want. Therefore, in our inter- active refinement, refinement candidates are carefully chosen. By displaying inappropriate refinement candi- dates, some of the documents can never be retrieved.

A co-related concept, called coverage, was proposed in our early work ([LCN099]). A set of keywords is said to be a coverage of a set of documents if these documents can be retrieved using the same set of key- words. With the concept of stem rules and coverage, the number of ARs can be reduced, and it can ensure that all documents can be covered.

p4 (Uniform screening): This means that all refine- ment candidates have similar screening effect to the original query. Otherwise some candidates have small screening effect hence are not proper to be candidates.

Indeed, this principle is implied by p l  and p3 (sim- ilarly p l  is implied by p3 and p4) under certain con- ditions. However, the main condition is that the set of keywords is not large. Otherwise, the stem rule and coverage are not enough to even guarantee pl .  To ad- dress this, a new concept prime keyword is introduced in this paper and will be discussed later. The prime keywords are such keywords that any documents can be retrieved from some of them.

p5 (Appropriate number of steps of refining): Nat- urally, no user wants to spend too much time in the process of refinement. Instead, he/she will prefer to get to an appropriate size quickly. Again, when a doc- ument database is large, the retrieval result becomes larger, and accordingly the number of keywords be- comes large, too. Therefore, the co-occurrence be- tween keywords becomes much more frequent, then it takes more steps to screening the retrieval result.

All the five principles are important in  that they are users?s natural requirements. We also find that they are not independent from each other, under certain conditions. Our     early work only addressed p2 and p3 for not very large doc- ument databases. We add pl,  and show that the first three principles are the basis and the last two can be derived from the first three. Note: p2 can be considered as a tradeoff with pl.  We explain how p4 and p5 are derived from the first three below.

First, the principle p4 of uniform screening means that if k l  and k2 are the refinement candidates of kO then la({kO, kl})[ is about la({kO, k2})1.

We show that p4 is implied by p l  and p3. By P3, Ifl({lco, kl})l + la({kO, k2})1=(o((kO})(, then ( o ( ( k O , k 2 } ) (  << la({kO, kl})l implies that Ia({kO, kl})( is almost Ic({kO})l, which means that the screening effect of k l  is ?bad? hence contradicts pl.

Second, p5 is satisfied if p l  through p3 are well satis- fied. Because of p2 and p4, after n steps of refinement, the number of documents retrieved by the refined query Q? will be approximately la(Q?)l = Ia(&)I/Cn where C is the average number of refinement candidates. For instance, let Io(&)[ be 1000 and C be 10, then ideally after the sec- ond step of refinement, la(&?)[ will be about 10. The user can be comfortable with the retrieval by Q?, rather than to further refine it.

In our system, several concepts are introduced for sat- isfying the principles mentioned above. In particular, the coverage mainly guarantees p3, the stepwise helps to re- duce refinement candidates, and prime keywords help to re- duce candidates, significantly in large document databases with a large number of keywords. It is worthwhile noting that a concept does not correspond one single principle. In- stead, as shown in p4 and p5, they work together. Further- more, without prime keywords, coverage and stepwise can become powerless for large document databases.

6 Refinement Model Using Prime Keywords  In this section we discuss the detail of ?prime keyword?.

The approach of using both stem rule and coverage in our early work is efficient. However, when the document database gets large and the number of keywords grows, the number of rules becomes large. This not only requires more storage space and maintenance, but also longer response time during the query refinement. To address this problem, we propose a new concept called prime keywords.

Definition 3 prime keyword: Let D and K be the set of all documents and all keywords, respectively. R is called the prime key- words o f D  (or K> i # { {k }  I IC E E} is a minimum cover of D (or 2?).

By the definition of coverage, prime keywords satisfy the following expression.

D = U a ( { k } )  3 U a ( K ) k E g  KCW  In other words, any documents can be retrieved from a cer- tain subset of the prime keywords. Therefore, if all users formulate their query using only keywords of prime key- words, then it will be very effective and efficient to build a refinement supporting system based on prime keywords.

We construct a prime graph to treat queries containing only prime keywords. However, it is difficult to require users to remember these prime keywords and non-prime keywords.

Our solution is to map a non-prime keywords query to a prime graph by searching their coverage from prime key- words, and construct a transfer graph. Before giving the formal description, we will explain these notations by the following example.

Table 1 gives an example of a document database. In this example, each line of the table describes a document with the keywords contained in it. D = {di};?, K = {ki};?, and one example of prime keywords of I& is = { I C * } : because it satisfies U k E g  a ( { k } )  = D. Note that prime keyword is not unique.

Keywords in d(C K) D l  Table 1. A document database example  The prime graph based on prime keywords = {ki}: is given in the left part of Figure 3, where DK means search- ing the documents contains no other keywords than those in K .  As an instance, if the user?s query is Q = { k l } , then as shown in the figure, the user ca_n choose adding k2 or k3 to refine Q. Another choice is D { k l ) ,  which means he/she terminates the refinement and retrieve the documents containing exactly the keyword k l .  For non-prime key- words K - E, the transfer graph is constructed as shown in the right part of Figure 3. The transfer graph has only two levels. Each node in the first level is a non-prime key- word, and its children nodes, which are all prime keywords, form one of its minimum coverage.

-$kZ?k3k4  -bk2k3k5  Figure 3. A prime graph and a transfer graph  6.1 The query refinement system with prime key- words  Now we give the formal definition of the refinement sup- port system.

Definition 4 refinement base: the refinement base consists of a directed acyclic prime graph = (g7 E ,  w) and a directed acyclic transfer graph I;? = (N7 E? ,  w), where  N = {Q I Q E U 2p(dlnE A (0 < w ( ~ )  5 es)} d ? D  N? = { { k }  1 k E K}  &? = {{k ? ,E}  1 k? E K -  R A  E E}  8, and Bc are maximum support and maximum confidence, respectively. w is the weight of nodes and edges. For P, Q E N U N?, e = (P,  Q) E .?? U E ? ,  w is defined using CJ as w(Q)) ?Zf Io(&)!, and w(e) ?Zf = #, respec- tively.

Figure 3 shows a prime graph and a transfer graph.

These graphs are constructed using three algorithms. The first algorithm (Algorithm 1 )  extracts prime keywords, the  second and third (Algorithm 2 and Algorithm 3) construct a prime graph and a transfer graph, respectively. For ex- ample, d l ,  d2, d3, dg contains k l ,  and by adding kz and k3 to k l ,  d l ,  d2 and d3 can be covered, respectively. dg cannot be covered by adding any keyword to k l ,  and hence is treated as Dkl which means that no further refinement is available. The relationship between k6 and kl  (k2)  in the transfer graph is due to the fact that { {k l } , {k2}}  is a prime keyword coverage of {kg}.

Considering the principles, in Algorithm 1 of extracting prime keywords, a heuristic of using keywords of medium support (A) is adopted. Hence the resulting prime key- words can be expected to have an appropriate size and contain keywords of similar support values. Reevaluating c( { k } )  in D or D is another heuristic. (Referenced in Fig- ure 5 as ?fix spt? and ?re-counted spt?, respectively.) If D is adopted, then a ( { k } )  in the line ?(*)? of Algorithm 1 changes. If III, is used, a ( { k } ) ,  for any k ,  is a fixed value from the beginning.

covers IID, and hence covers any subset K of K M C  is a function that returns two things, f l ~  and C ( R , ~ ) .  Here, f l ~  which is not necessarily empty here, is the documents contains exactly those keywords in Q. C ( R , ~ )  is a minimum coverage of Q within lk. M C  is called by the second and the third algorithm commonly so is described separately.

Due to space limitation, we give the following remark without proof.

Property 1 The algorithms are sound and complete. By sound, we mean that any output satisfies the corresponding definition.

By complete, we mean that they terminate and all results of  Note that by definition     Algorithm 1 Generating prime keywords Input: document database D, all keywords IK and a center support Ao.

Output: prime keywords and a support for each keyword in the set.

l e t E t 0 ,  D t D ,  A t A + + A - t A o ; while D # 0 do (*) K + { I C  I IC E K b({~))l = A)  D t D - . ( { I C ) ) for k E K do  endfor if ( \ A  - Acl = lA - A-I) or (A- = 1) then  A t A+ t A + + l ; else  A t A- t A- - 1; endif  endwhile  Algorithm 2 Generating a prime graph Input: prime keywords E, maxCnf 8, Output: N ,  E  Algorithm 3 Generating a transfer graph Input: E, maxCnf 8, output: N ,  &  the definition are constructed.

function MC(Q, e,) begin  c(R,Q) K c { I C  I IC E lk, spt(Q + {k}) > 0) D + U(&) for ko E K do  if D = 0 then return(D, C(R,Q~) if mf(Q * { I C , , ) )  5 Bc then  C(R,Q) c(w,Q) { I C O ) K t K - { I C O ) D 4- D - .({kO})  return(D, C(R,Q)) endfor  end  6.2 Query refinement with prime keywords  Now with the given prime graph and transfer graph, any retrieving query can be processed. Basically, there are three types of queries, the first contains only prime keywords, the second contains only non-prime keywords, and the third contains both prime keywords and non-prime keywords.

These cases exactly correspond to the Case 1, 2 and 3, and 4 in the following, respectively.

The document database is D = { d } ,  and all keywords in D is IK = {k}. Here, prime keywords K, and non-prime keywords K? = IK - Ik.

Case 1: Q = E%.

Q is simply refined by searching the prime graph. Children nodes of Q in the graph are refined candidates.

Case2: Q? = {k?},lc E K? .

Q is refined by searching the transfer graph followed by prime graph, as in the previous case.

Case3: Q? = K?o = { k ? l , . . .  , k k } For this case, 0? is constructed as Q? = ,?I Ki, then Q? can be treated as in Case 1. Here, Ki is given by Ki = { i i j  1 i i j  E lR. A g( {-iijl k : ) ) # 0 ) ) Case 4: Q = Q? U KO.

By transferring Q? as in Case 3, this is also the case of Case 1.

The following properties guarantee that our processing to Case 3 gives proper result. In detail, the former property guarantees that there exists prime keywords for Q? thus the query refinement can be further processed. The latter one guarantees that Q? can be covered, therefore no document will be lost, Having Case 3, the soundness of Case 4 be- comes trivial.

Property 2 v IC: E K? Zi = {Icij  I E i j  E EACT({&~, ki))#@)} as given in Case 3, is not empty.

K ? .

z=1     Proof: Note that R is extracted from ED, thus, for any k E IK, o ( { k } )  # 0. Now suppose that 3kA E K?, Ro = {Lo3 I rCoj E R A o ( { L o j , k ~ } ) # ~ )  = 0 .  In other words, such & j  does not exist. This means that V z j  E E, o( { kj, kb}) = 0 ,  hence the non-empty o( { I C ; } ) cannot be covered by E. On the other hand, by the defi- nition of prime keyword, must cover any document set, which leads to contradiction.

Property 3 In Case 3, o(Q?) C_ o(Q?)  Proof: Note that d E a(&?) equals to p(d)  1 Q? and d E o(Q?) equals to p(d)  2 Q?, we only need to show that Vd E o(Q?),  p(d)  2 Q? holds. Assume that 3d, p ( d )  2 Q? but p(d) 2 Q? does not hold. Then,  2= 1 3ko E p ( d ) ,  Lo $! Q? = .: Ki - o(k0,kt?) = 0 3t, 1 5 t 5 X ,  ko Kt =+ ko $! { & j  I Ictj E B A o { ~ t j , k t ? } # 0 } However, kt? E Q? g p(d) ,  in addition with ko E p ( d ) , {ko ,  k t ? }  C p(d) .  Hence d E ~ ( { k o ,  kt?}) holds. This con- tradicts the conclusion o( (k0 ,  kt?})  = 0 that we have just led. Therefore, Vd,  p ( d )  _> Q? ==+ p ( d )  _> 0? meanwhile d E U ( & ? )  + d E U ( @ )  must hold.

Example 1 A running example of query refinement support is given in Figure 4 in which QI, Q2, and 4 3  correspond to the three types of queries, only prime keywords, one non- prime keyword, and more than one non-prime keyword, respectively. Q1 is treated easily by searching the prime graph as described in Case 1. Q2 is just a variant of Case 2. First the non-prime keyword, k8, is transfered as described in Case 2 and then the prime keyword, k3, is added to each node, which results in the nodes of prime graph. Now as the result, Q2 is mapped to { k l , k 3 }  and ( k 2 ,  k 3 } ,  and the documents retrieved change from { d 5 } to {d2,d3} and {d4, d5} ,  respectively. For Q3, first the co-occurrence prime keywords sets of each of non-prime keywords, k7 and k10, are calculated, which are {k3 ,  k4} and {k4, k5}. Then the intersection of these two sets is ob- tained. Finally the remaining prime keywords subset of Q3, {k4}(= Q3 - (k7 ,  klO}), is identified. The result must be a node of a prime graph. The soundness is guaranteed as has been discussed in the properties above.

7 Conclusion  We consider all possible requirements in query refine- ment, and formally identify them by five principles. A  I . .  . . . . . .  I to 20 30 U) 50 M 70 en 90 $03  Figure 5. Sizes of prime keywords  model of supporting IR query refinement based on prime keywords is proposed and is confirmed to be sufficient in the sense that it  satisfies the principles.

We built a protocol system and used the JICST docu- ment database for testing which contains 120,000 docu- ments. About 22,000 keywords are extracted. Figure 5 shows the effectiveness of the center support and fixed or re-counted support for Algorithm 1. The center support of around 80, with re-counted support, gives a reasonable set of prime keywords of size 2,829, that is one eighth of the number of all keywords. Table 2 shows the number of the co-occurrences of keywords with support greater than or equal to 10. The numbers give upper-bound of the sizes of the two graphs (prime graph and transfer graph), and confirm the effectiveness of our approach. As an instance, the number of co-occurrences of 2-keywords for prime key- words is 9,061, while the number is 60,048 for all key- words. The size of prime graph will never exceed the total number of 12,718 which is less than one tenth of that of all keywords. This means that our approach can efficiently support query refinement. More importantly, there is a few.

co-occurrence among more than 4 keywords, which implies that, with prime graph, the process of refinement will finish in 2 or 3 steps, and hence satisfies p5.

As for the retrieval quality, we ensure the same results obtained in our early work ([LCYO98, LCN099]), and therefore omit the testing results in this paper. Interactive refinement enables users to enhance precise while keeping high recall.

As future work, we will investigate the sizes of rules and the effectiveness of these rules for precision. We are plan- ning to use clustering as a technique for query refinement in conjunction with association rules. Also, we are extend- ing our work to support query refinement in a distributed environment, as well to cope with dynamical increment of documents.

Figure 4. Some examples of query refinement support  n primekeywords  All keywords  1 2 3 4 5 6 7 8 9 10 total 2829 9061 773 46 8 1 0 0 0 0 12718 22089 60048 35237 15560 8426 4604 1983 616 125 15 148703  Table 2. Counting co-occurrence keywords.

