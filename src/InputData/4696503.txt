A Novel Anti-Data Mining Technique Based On Hierarchical Anti-Clustering  (HAC)

Abstract  Data Mining is a technique to search potential valuable information from databases. Preventing personal data and high security data therefore pose a difficult task to IT experts. In this paper, we propose a novel anti-data mining (ADM) database security scheme, that protect against data mining. The scheme makes use of hierarchical clustering where noise is added to change the cluster structure of data. The proposed hierarchical anti-clustering (HAC) scheme modifies the cluster structure of the original data.

Experimented results show that data may be protected against during the HAC key can be used reverse the cluster structure to its original.  At the meantime, HAC also designs the key value to restore correctly the protected database.

Keywords: Data Mining, Anti-Data Mining, Hierarchical Clustering, Anti-Clustering, Noise data  1. Introduction   Data Mining is a technique using information technology and statistics method to search potential valuable information from a large database which can be used to support administrative decisions [21].

Examples of which include the data mining of on-line database, ERP database or Data Warehouse using techniques like association rules, clustering, classification, sequential pattern analysis and prediction [21,22].

In addition enterprise uses data mining to know companies, working status and to analyze potential information values. Information mined must be protected from disclosure of company secrets.

In this internet era, database of companies are web based to provide employees and clients a convenient want to accept information. However, this kind of  services results in security problems. Techniques have been developed to protect data such as symmetric key algorithm and asymmetric key algorithm to protect the access and deliver of data [18,19], and anonymous user identity [2,3,14]. However, these methods can not efficiently protect data from being mined.

Clifton [7,8] proposed the threatening and clash of data mining to protect database. He considered that mining of database will exposed sensitive data or rules.

Some mining techniques were aimed at processing with association rules [1,9,12], transactions decompositions [13], victim items [17] or other methods to prevent mining sensitive data. Basically, sensitive data could be mined and important knowledge pertaining to a company?s competition ness could be exposed.

In this paper, we proposed a novel Anti-Data Mining (ADM) database security concept by adding interference data to harass knowledge content of database. Illegal users can not use data mining technique to analyze the knowledge value in database.

This scheme is used to negate data mining, but will not destroy the original data. The protected database can be retrieve to its original by reversing the harass rule.

2. A Novel Anti-Data Mining Technique of Protected Database   The clustering technique used in Data Mining.

Group data according to feature attribution of data where each object within group has high intra-group similarity, and low inter-group similarity [5,6]. In the process of clustering analysis, there is neither advance designate class information, nor any information showing records relationship. Therefore, clustering is regarded as unsurpervised learning.

The anti-data mining (ADM) method proposed in this paper is a novel method to protect content of data based on clustering of data mining that complements   DOI 10.1109/ISDA.2008.155     we proposed, Hierarchical Clustering (HC) [15], Hierarchical Anti-Clustering (HAC), which protects the analyses result of clustering of database. Our research produces noises by random seed and adds certain amount of noise data into database to HAC in order to disturb the clustering result and to destroy the original clusters. Therefore similar data become dissimilar ones and belongs to the wrong clusters.

2.1. Hierarchical Clustering (HC)   Clustering Algorithm is divided into two parts composing of Hierarchical Clustering (HC) and Non- Hierarchical Clustering [11,22]. HC forms a tree or hierarchical structure for the clustering process. In the tree structure, the height of nodes represents the similar degree between clusters, users can use this characteristic to observe the clustering results of data.

HC can also be further divided into two parts which are Agglomerative algorithm an Divisive algorithm [5,6,22]. Agglomerative Algorithm regards every data in dataset as a cluster, then into two clusters with feature and attribute that are most similar repeatedly until all data become one. Divisive algorithm regards the whole dataset as a cluster, then divides them into two smaller cluster by lowest similarity repeatedly until every data is independent. (See Figure 1)   Figure 1. Hierarchical Clustering Algorithm [22]   Generally speaking, the evaluation method of data  similarity mainly regard the distance of two data records distance calculation methods include Euclidian distance, Manhattan distance  and Chebychev distance [4]. The different calculating methods result in different clustering results, and are dependent on data characters and spread conditions. HC provides four methods for similar evaluation [10]. The first method is Centroid Linkage, which evaluates the distance of two clusters centroids. The second is Complete Linkage, which evaluates the distance of two farthest data points between two clusters. The third is Average  Linkage, which evaluates the average of sum distance of any two data between two clusters. The fourth is Single Linkage, which evaluates the distance of two nearest data in different cluster.

In this paper, we will add noise to data columns to produce wrong clustering results with HC clustering.

The cluster tree structure is HC where the data of similar height original nodes, offer adding noise to data columns method will let the height distance of data nodes to become dissimilar. Wrong cluster structure will protect the clustering knowledge of HC.

2.2. Hierarchical Anti-Clustering (HAC) Algorithm   In this paper, we design HAC Algorithm to be a new protection method of database which change the cluster structure of HC, and be the main idea of ADM.

The essence of this method is to add the noise to data to change the analysis result of HC, then producing the wrong clustering information to mislead the person who get database illegally.

This method set up the dataset which needs protection to be D initially, the column numbers of dataset to be c, data sum records to be n, Then  },...,3,2,1,{ nidD i ?? , after analyzing by HC that clustering result composed of a Tree, which represent by H, H include n-1 nodes, dividing up different nodes acquires different clusters iH ,  }1,1,{ iiji njkidH ????? , ijd is the data included by iH , k  is the number of clustering, in  is each data of iH . HAC is the method which adding the noise to column data to make original data D become D? , then using HC to analyze D?  to get the clustering result H ? which added the noise to column data, and calculating simultaneously clustering result iH  and iH ?  , both include the same data numbers s, which is the data numbers of  iij Hd ?  and iij Hd ?? , the lower the  values of unchanged part of iH  in iH ?  is the better.

Show as the formula (1).

||||min  iiji  k  i ij HdandHds ???? ?  ?  (1)  After acquiring s, then divide by data sum records n equals HR which is a rate of similarity of H ? to H, also is a rate of clustering result of HC changed by HAC, if the rate of HR is lower which means the structure of HC changed by HAC is higher, it also means the effect of HAC is better. Finally, use cSeedRandom ?)( to acquire the position of noise column data which adding     the dataset finally, and adding the noise column data into the original dataset to output the dataset PD which protected by HAC. The following of HAC Algorithm is: Step 1: Set up the initial date D, and make D be  clustering analyzed by HC which result in H.

Step 2: Set the range of noise data q and noise data  increasing rate r, and Seed value be the key value of HAC, and set two threshold values, set noise rate rT  and executing frequency tT be the conditions of executing, and set t = 1.

Step 3: Use the formula (2) to produce the noise data iZ , ],1[ ni? , and let iZ  join D  acquiring  D? .

qSeedRandomZi ?? )(   (2) Step 4: Make D?  be clustering analyzed by HC which  result in H ? .

Step 5: Calculate the same data number S between iH  and iH ?  , and nsHR ?? .

Step 6: If rTHR 	  and tTt ?  then drop D? and set  rqq ??  and t = t + 1 repeat Step 3 ~ Step 6.

Step 7: Run cSeedRandom ?)( , then acquire the  column position of noise column data, then output the dataset PD which protected by HAC.

Through the method above, it let users decide producing the range of noise data, noise data increasing rate and long for the effect of protection by depending data characters. Thus, the protected data content and possible clustering knowledge of HC can be adjusted flexibly, then increasing the trustworthy and protection effect of database protected by HAC.

In the other hand, about the setting of noise data range and adding column, the character of original data must be considered, in order to avoid the removing noise data columns because of guessing easily, including the attribution, maximum, minimum, the number of decimal, the name of columns, the number of columns of original data column, etc, which should be considered, therefore, the adding noise column data are similar with original dataset, that is difficult to recognized. In the meantime, in order to increase the difficulty of recognizing, it can use more than one noise column data to increase the difficulty of breaking HAC and the effect of noise.

2.3. Restoring the Protected Database by HAC   In section 2.2, we describe the database protection method of HAC, and we will describe how to restore  the protected database PD in this section, let legal users can use and analyze data correctly.

When the noise data produced, then run it by cSeedRandom ?)(  to acquire a position, then insert  the position into noise column, therefore, when restoring data set, it should be produced correct noise data then calculating the position of noise column and remove it, the restore flow as following: Step 1: Pick up the same key value as being protect:  the range of noise data q and noise data increasing rate r and Seed value.

Step 2: Using formula (2) to produce the same noise data iZ ? .

Step 3: Run )1()( ?? cSeedRandom to find out the position of noise column in dataset.

Step 4: Remove the noise column and output restored dataset D.

Due to the function Random(Seed) produce the same random values, when repeating execute, as long as set up the same Seed value that will produce the same random values. Therefore, this research set up the Seed value and the range of noise data and noise data increasing rate to be the key value of HAC method, then only the users who have correct key value can restore the protected database by HAC correctly, and analyze database by clustering technique, and acquire the correct knowledge content of database.

3. Experimental Results   In order to proceed the experiment exactly, this research use the famous Iris dataset in UCI Machine Learning Repository [23] website to be the original dataset for experiment, proceeding the knowledge protection experiment of HAC, and illustrating the experiment result as following:  Iris is a dataset which has 150 four-column records, they are divided into three clusters by data character [23]. When experiment proceeding, set Iris dataset is D then do HC to acquire correct clustering result  },,{ 321 HHHHi ? , Example, Table 1. HC Analysis Result of Iris Dataset. The value in Table 1 is the data number of cluster. Simultaneously Picking the maximum and minimum of Iris dataset to be the range of noise information q, noise data increasing rate r = 1.1, noise rate %50?rT  and execution time limit  10?tT .

Table 1. HC Analysis Result of Iris Dataset 1H  2H  3H  n 13 87 50     In the next, we proceed the protection flow of HAC, adding noise column data by qSeedRandom ?)(  to destroy the original cluster structure, and re-analyzing the protected dataset D?  which added the noise column data by HC to produce the protected clustering result  },,{ 321 HHHHi ????? , and compare with the unprotected clusters H , calculating the data numbers of iij Hd ?  and iij Hd ??  to acquire the effect of clustering knowledge of Iris dataset protected by HAC, See Table 2.

Table 2. The Clustering Result of Iris Protected by  HAC 1H ?  2H ?  3H ?  n?  48 62 40  1H 8 5 0  2H 32 37 18  3H 8 20 22  The experiment result is analyzed by three clusters  characters of Iris dataset, In this paper, we divide the clustering result of Iris dataset into three clusters, then calculating the difference of unprotected data by HAC and protected ones of each cluster data. n?  in Table 2 is the cluster data number of HC which added noise column data by HAC.

Finally, acquiring s which is the data number of iij Hd ?  and iij Hd ?? , and the effect of clusters  structure changed by HAC. For example: Table 3.

HAC Experimental Result of Iris Dataset. iHR  in Table 3 is the data rate which reserve the original data clusters iH  in iH ? , Finally, HR is the average of iHR .

It also is the rate of HC result of Iris dataset changed by HAC. In this experiment, when t = 9, which meet the need that %50?rT , then the cluster structure of  iH ?  is exactly different from iH . This proves that HAC is the solution to protect HC knowledge of Data Mining.

Table 3. HAC Experimental Result of Iris Dataset  1H  2H  3H  n 13 87 50  s 8 37 22  iHR  61.53% 42.52% 44%  HR 49.35%   4. Conclusions   In this paper, we propose an anti-data mining (ADM) scheme to protect data from mining.

Hierarchical anti-clustering (HAC) is used to modify the structure of clusters example, for protecting the knowledge of database clustering analysis, not only considering the security of system frame.

The ADM method is different from traditional mode of data encryption and decryption, it won?t affect normal access of data, only base on data mining knowledge to protect principal part. The main function is adding noise data to disturb information structure of database, then users can?t use the clustering technique of data mining to analyze correct knowledge. In the meantime, this method also can disturb the clustering result of data mining and approaching effect of cheating enemy.

5. References  [1] R. Agrawal, and R. Srikant, ?Fast Algorithms for Mining Association Rules?, In proceedings of the IEEE International Conference, 1998, pp. 402-411.

[2] E. Bertino, S. Castano, E. Ferrari, and M. Mesiti, ?Protection and Administration of XML Data Sources?, Data & Knowledge Engineering, Vol. 43, 2002, pp. 237-260.

[3] E. Bertino, ?Data Security?, Data & Knowledge Engineering, Vol. 25, 1998, pp. 199-216.

[4] N. Bolshakova, F. Azuaje, and P. Cunningham, ?An Integrated Tool for Microarray Data Clustering and Cluster Validity Assessment?, Bioinformatics, Vol. 21, 2005, pp.

451-455.

[5] T.S. Chen, C.C. Lin, Y.H. Chiu, and R.C. Chen, ?Combined Density-based and Constraint-based Algorithm for Clustering?, Journal of Donghua University, Vol. 23, 2006, pp. 36-38.

[6] T.S. Chen, T.H. Tsai, Y.T. Chen, C.C. Lin, R.C. Chen, S.Y. Li and, H.Y. Chen, ?A Combined K-Means and Hierarchical Clustering Method For Improving the Clustering Efficiency of Microarray?, In Proceedings of 2005 International Symposium on Intelligent Signal Processing and Communications Systems, 2005, pp. 406-408.

[7] C. Clifton, and B. Thuraisingham, ?Emerging Standards for Data Mining?, Computer Standards & Interfaces, Vol. 23, 2001, pp. 187-193.

[8] C. Clifton, and D. Marks, ?Security and Privacy Implications of Data Mining?, Proceedings of the ACM SIGMOD Workshop on Data Mining and Knowledge Discovery, 1996, pp. 15-19.

[9] F. Coenen, P. Leng, and S. Ahmed, ?Data Structure for Association Rule Mining: T-trees and P-trees?, IEEE Transactions on Knowledge and Data Engineering, Vol. 16, 2004, pp. 774-778.

[10] A. Dragut, and C.M. Nichitiu, ?A Monotonic On-Line Linear Algorithm for Hierarchical Agglomerative Classification?, Information Technology and Management, Vol. 5, 2004, pp. 111?141.

[11] G. Gautam, and B.B Chaudhuri, ?A Novel Genetic Algorithm for Automatic Clustering?, Pattern Recognition Letters, Vol. 25, 2004, pp. 173?187.

[12] L. Glimcher, R. Jin, and G. Agrawal, ?Middleware for Data Mining Applications on Clusters and Grids?, Journal of Parallel and Distributed Computing, Vol. 68, 2008, pp. 37- 53.

[13] W. Jiang, C. Clifton, and M. Kantarc?o?lu, ?Transforming Semi-honest Protocols to Ensure Accountability?, Data & Knowledge Engineering, Vol. 65, 2008, pp. 57-74.

[14] M.S. Hwang, and C.H. Lee, ?Secure Access Schemes in Mobile Database Systems?, European Transactions on Telecommunications, Vol. 12, 2001, pp. 303-310.

[15] M.J.L. de Hoon, S. Imoto, J. Nolan, and S. Miyano, ?Open Source Clustering Software?, Bioinformatics, 2004, pp. 1453?1454.

[16] U. Maulik, and S. Bandyopadhyay, ?Performance Evaluation of Some Clustering Algorithms and Validity Indices?, Pattern Analysis and Machine Intelligence, IEEE Transactions, Vol. 24, 2002, pp. 1650-1654.

[17] S. Oliveira, and O. Za?ane, ?Privacy Preserving Frequent Itemset Mining?, Proceedings of the IEEE Mining, 2002, pp. 43-54.

[18] S. Rapuano, and E. Zimeo, ?Measurement of Performance Impact of SSL on IP Data Transmissions?, Measurement, Vol. 41, 2008, pp. 481-490.

[19] T. Rowan, ?VPN Technology: IPSEC VS SSL?, Network Security, Vol. 2007, 2007, pp. 13-17.

[20] A.J. Saldanha, ?Java Treeview-Extensible Visualization of Microarray Data?, Bioinformatics, 2004, pp. 3246-3248.

[21] M.H. Dunham, Data Ming: Introductory and Advanced Topics, Prentice Hall, 2003.

[22] J. Han, and M. Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann, 2006.

[23] A. Asuncion, and D.J. Newman, ?UCI Machine Learning Repository?, http://www.ics.uci.edu/~mlearn/MLRepository.html, 2007.

