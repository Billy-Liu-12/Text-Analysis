Mining Association Rules with Weighted Items

Abstract  Discovery of association rules has been found use- ful in many applications. In previous work, all items in a basket database are treated uniformly. We gen- eralize this to the case where items are given weights to reect their importance to the user. The weights may correspond to special promotions on some prod- ucts, or the pro?tability of di?erent items. We can mine the weighted association rules with weights. The downward closure property of the support measure in the unweighted case no longer exist and previous al- gorithms cannot be applied. In this paper, two new algorithms will be introduced to handle this problem.

In these algorithms we make use of a metric called the k-support bound in the mining process. Experimental results show the e?ciency of the algorithms for large databases.

Keywords: data mining, association rules, basket  data, support, con?dence, weighted items.

1 Introduction Computers store large amounts of retailing trans-  actions in a retailing business. Marketing managers will be interested in useful information that can be extracted from these large databases. As the amount of retailing information increases dramatically, there is a new challenge of ?nding interested information e?ciently. One promising approach is the mining of association rules for basket databases, which has been investigated by [1], [2], [6], [7], [5], etc. Most of these works are focused on mining binary association rules.

A binary association rule  Buys <Bread> ) Buys <Ham> with support = 0.6, confidence = 0.8  says that the probability of buying bread and ham is 0.6, and the probability of buying ham is 0.8 given that a transaction contains bread. The interestingness of the rule depends on the number of occurrences of bread and ham.

In this paper, we introduce the notion of weighted items to represent the importance of individual items.

In a retailing business, a marketingmanagermay want to mine the association rules with more emphasis on some particular products in mind, and less emphasis on other products. For example, some products may be under promotion and hence are more interesting, or some products are more pro?table and hence rules concerning them are of greater values. This results in a generalized version of association rule mining problem, which we call weighted association rule mining.

For example, if the pro?t of the sofa is much higher than the bed, then the rule  Buys <Pillow> ) Buys <Sofa>  is more interesting than  Buys <Pillow> ) Buys <Bed>  When we compute the weighted support of the rule, we can consider both the support and the important ratio (weights) factors.

A simple attempt to solve the problem is to elimi- nate the entries of items with small weights. However, a rule for a heavy weighted item may also consist of low weighted items. For example, we may be pro- moting a product A, and ?nd that it is a?ected by another product B, for which we have initially no in- terest. Hence the simple approach does not work in this case.

Another approach is adopting the existing fast al- gorithms for ?nding binary association rules, such as the Apriori Gen Algorithm [1]. Such algorithms de- pend on the downward closure property which governs that subsets of a large itemset are also large. However, it is not true for the weighted case in our de?nition, and the Apriori Algorithm cannot be applied.

In this paper, we propose new algorithms to mine weighted binary association rules. Two al- gorithms, MINWAL(O) and MINWAL(W) are de- signed for this purpose. Experimental result shows    that these algorithms have reasonable performance for large databases and MINWAL(O) performs bet- ter than MINWAL(W) in most cases.

The paper is organized as follows. In Section 2, the de?nitions of mining weighted association rules will be introduced. In Sections 3 and 4 algorithms will be described for two di?erent de?nitions of weighted sup- port. Performance study will be reported in Section 5. Finally, Section 6 is a conclusion.

2 Weighted Association Rules  Similar to [1] and [5], we consider a database with a set of transactions D, and a set of items I = fi1; i2; :::; ing. Each transaction is a subset of I, and is assigned a transaction identi?er hTIDi.

De?nition 1 An association rule has the form of X ) Y , where X ? I, Y ? I, and X \ Y = ;.

We de?ne the terms of support and the con?dence as in [1].

De?nition 2 The support of the association rule X ) Y is the probability that X [Y exists in a trans- action in the database D.

De?nition 3 The con?dence of the association rule X ) Y is the probability that Y exists given that a transaction contains X, i.e.,  Pr(Y nX) = Pr(X [ Y )  Pr(X) (1)  In large databases, the support of X ) Y is taken as the fraction of transactions that containX[Y . The con?dence of X ) Y is the number of transactions containing both X and Y divided by the number of transactions containing X.

Given a set of items I = fi1; i2; ? ? ? ; ing, we assign a weight wj for each item ij , with 0 ? wj ? 1, where j = f1; 2; ? ? ?; ng, to show the importance of the item.

According to De?nition 2, we can de?ne the weighted support for the weighted association rules.

De?nition 4 The weighted support of a rule X ) Y is 0  @ X ij2(X[Y )  wj  A (Support(X [ Y )) (2)  Similar to [1], a support threshold and a con?dence threshold will be assigned to measure the strength of the association rules.

Bar code Item Total Pro?t weights ....

1 Apple 100 0.1 ....

2 Orange 300 0.3 ....

3 Banana 400 0.4 ....

4 Milk 800 0.8 ....

5 Coca-cola 900 0.9 ....

Table 1: Example of product database  TID Bar codes TID Bar codes  1 1 2 4 5 2 1 4 5 3 2 4 5 4 1 2 4 5 5 1 3 5 6 2 4 5 7 2 3 4 5  Table 2: Transaction database  De?nition 5 A k-itemset X is called a small item- set if the weighted support of such itemset is less than the minimum weighted support (wminsup) threshold, or 0  @X ij2X  wj  A (Support(X)) < wminsup (3)  Otherwise, X is a large k-itemset.

De?nition 6 An association rule X ) Y is called an interesting rule if X [ Y is a large itemset and the con?dence, de?ned in de?nition (3), of the rule is greater than or equal to a minimum con?dence thresh- old.

Example 1 Suppose in a retailing store, a database is shown in  Tables 1 and 2. Table 1 shows the information about the items in the retailing store. The information in- cludes the bar code number of the items, the names of such item, the total pro?ts of the items, the given weights, etc. Table 2 shows the transaction database.

For each transaction, there will be a transaction iden- ti?er hTIDi, and the bar code numbers of the items.

For simplicity, the bar code numbers will be repre- sented in the form of natural numbers f1; ? ? ? ; 5g.

Suppose that there are 5 items and totally 7 trans- actions in the transaction database. If the value of wminsup is 0.4, then f2, 5g will be a large itemset since  (0.3+0.9)?57 = 0.86 ?0.4    By the same argument, f4, 5g and f2, 4, 5g will be large itemsets. 2  2.1 Reasoning behind the problem de?ni- tion  In this problem, we want to get a balance between the two measures, which are weight and support. We have three parameters to consider in the weighted as- sociation rule: weights of items, support of itemsets and the con?dence factor. We have chosen to com- pute a weighted support of an itemset which is the product of the total weight of items in the itemset and the support of the itemset.

Suppose we separate the supports and weights. We can only ?nd itemsets having both su?cient support and weight. However, this may ignore some interest- ing knowledge. The semantics of weight is a measure of the importance of an itemset. If an itemset is very important, for example, it is under promotion, or it is highly pro?table, then even if not many customers have bought it, it is still an interesting itemset to the user. On the other hand, if an itemset is not consid- ered very important in terms of the weights, but it is very popular so that many transactions contain it, it is also an interesting itemset.

Another feasible alternative is to ?nd itemsets that have either su?cient support or weight. However, this may not allow us to handle zero weight (no interest) items e?ciently, no matter how high the support may be.

There is one possible problem with our de?nition.

Even if each item has a small weight, when the number of items in an itemset is large, the total weight may be large. Depending on the application requirements this may or may not be desirable. It may be desirable if the user considers a rule with a number of items, which contribute to a su?cient pro?tability together, is interesting. It may not be desirable if a rule with many light weighted items should not be considered interesting. Here we also consider another problem de?nition in which the weighted support of an itemset is normalized by the size of the itemset. This will be discussed in Section 4. The semantics of the rules will be di?erent, and it will depend on the need of each application to consider the applicability of this normalization.

3 Mining Weighted Association Rules  A new algorithm is needed to solve the mining of the weighted association rules. An e?cient algorithm for mining binary association rules has been proposed  in [1], called Apriori Gen. The reason why Apriori Gen works is because if an itemset is large, all the subsets of that itemset must be large. However, for the weighted case, the meaning of large itemset is modi?ed to han- dle weighted support. It is not necessary true for all subsets of a large itemset being large. In Example 1, f2, 4g is a subset of the large itemset f2, 4, 5g, but it is not a large itemset.

3.1 k-Support Bound  Given a database with T transactions, we de?ne the support count (SC) of a large k-itemset X to be the transaction number containing X, and it must satisfy:  SC(X) ? wminsup ? TP  ij2X wj  (4)  Let I be the set of all items. Suppose that Y is a q-itemset, where q < k. In the set of the remaining items (I ?Y ), let the items with the (k ? q) greatest weights ir1 ; ir2 ; :::; irk?q . We can say the maximum possible weight for any k-itemset containing Y is  W (Y; k) = X ij2Y  wj +  k?qX j=1  wrj (5)  in which the ?rst sum is the sum of the weights for the q-itemset Y , and the second sum is the sum of the (k ? q) maximum remaining weights.

From Inequality (4), the minimumcount for a large k-itemset containing Y is given by  B(Y; k) =  ? wminsup ? T  W (Y; k)  ? (6)  We called this the k-support bound of Y . We take an upper bound of the value since the function B(Y; k) is an integer.

Example 2  Refer to Tables 1 and 2, the 3-support bound for the itemset f2, 4g is  ? 0:4? 7  (0:3 + 0:8) + 0:9  ? = 2 (7)  It means if the itemset f2, 4g is the subset of any large 3-itemsets, the count of the itemset f2, 4g must be greater than or equal to 2. 2  The algorithmfor miningweighted association rules can be established according to the above properties of the k-support bound for all possible k-itemsets.

3.2 Algorithm for Mining Weighted As- sociation Rules  An algorithm for mining weighted association rules has the following inputs and output.

Inputs: A database D with the transactions T ,  two threshold values wminsup and minconf , weights of the items wi, with ascending order, total number of transactions and the total number of the items.

Output: A list of interesting rules.

Notations:  D The database w the set of item weights Lk set of large k-itemsets Ck set of k-itemsets which may be k-subsets  of large j-itemsets for j ? k SC(X) no. of transactions containing itemset X wminsup weighted support threshold minconf con?dence threshold  Algorithm 1 MINWAL(O) 1 Main Algorithm (wminsup;minconf ,D, w) 2 size=Search(D); 3 L=;; 4 for (i=1;i?size;i++) 5 Ci=Li=;; 6 for each transaction do 7 (SC,C1)=Counting(D,w); 8 k=1; 9 while (jCkj ? k) 10 k++; 11 Ck=Join (Ck?1); 12 Ck=Prune (Ck); 13 (Ck,Lk)=Checking (Ck ,D); 14 L= L  S Lk;  15 Rules (SC, L); 16 ends;  The subroutines are outlined as follows:  1. Search(D): The subroutine accepts the database, ?nds out the maximum size of the large itemset in that transaction database D, and returns the maximum size.

2. Counting(D,w): This subroutine cumulates the support counts of the 1-itemsets. The k-support bounds of each 1-itemset will be calculated, and the 1-itemsets with support counts greater than any of the k-support bounds will be kept in C1.

3. Join (Ck?1): The Join step generates Ck from Ck?1 as in [1]. For example, if we have f1, 2, 3g and f1, 2, 4g in Ck?1, f1, 2, 3, 4g will be generated in Ck.

4. Prune (Ck): During the prune step, the itemset will be pruned in either of the following cases :  (a) A subset of the candidate itemset in Ck does not exist in Ck?1.

(b) Estimate an upper bound on the sup- port count (SC) of the joined itemset X, which is the minimum support count among the k di?erent (k? 1)-subsets of X in Ck?1.

If the estimated upper bound on the sup- port count shows that the itemset X cannot be a subset of any large itemset in the com- ing passes (from the calculation of k-support bounds for all itemsets), that itemset will be pruned.

5. Checking (Ck,D): The checking procedure scans the transaction database, updating the counts of all candidate itemsets in Ck. By the simi- lar method in the prune step, prune the candi- date itemsets for those not satisfying the sup- port count bounds for all possible large itemsets.

The remaining candidate itemsets will be kept in Ck. At the same time, the large itemsets Lk will be generated from Ck by checking the exact weighted support of the itemsets.

6. Rules (Support count; L): Find the rules from the large itemsets L, similar to [1].

The framework of our proposed algorithm for min- ing weighted association rules is similar to the Apri- ori Gen Algorithm [1], but the detailed steps contain some signi?cant di?erences. To begin with, we also generate large itemsets with increasing sizes. How- ever, since the subset of a large itemset may not be large, we cannot generate candidate k-itemsets sim- ply from the large (k ? 1)-itemsets as in Apriori Gen.

We shall ?nd a way to keep the k-itemsets which may generate large j-itemsets, for j ? k, in the following passes. In order to extract such k-itemsets from the database, we use the j-support bound values. During the operation, the j-support bounds will be calculated for all the candidate k-itemsets, where j is any num- ber between k and the maximum possible size of the large itemset. If the count of the existing k-itemset is less than all of the j-support bounds, we can say that it cannot be the subset of any large itemsets in the coming passes, and it can be pruned. The k-itemset, which may contribute to (be subsets of) future large itemsets, will be kept in Ck.

Example 3 From the Tables 1 and 2, we will show how the large  itemsets are generated from the transaction database.

Suppose the wminsup (weighted minimum support threshold) is 1.

1. During the search process, the algorithmwill scan the size of each transaction only, and returns the maximum possible size of the large itemsets, which is 4 in this example.

2. Pass I (k=1, where k is the size of the itemset)  During the counting step, the transaction database will be scanned once, similar in Pass I of the Apriori Gen [1]. The counts of the items (1-itemsets) will be found during this stage. For each item, we can calculate the support bounds for all coming passes from the information (item supports count and weights).

For this example, the counts of the items f1,2,3,4,5g are f4,5,1,6,7g respectively. Let us de- note the 1-itemset containing item 1 by Ia. The k-support bounds of the k-itemsets containing Ia are given by B(Ia; k):  B(Ia; 2) = l  1?7 0:1+0:9  m = 8.

B(Ia; 3) = l  1?7 0:1+(0:8+0:9)  m = 4.

B(Ia; 4) = l  1?7 0:1+(0:4+0:8+0:9)  m = 4.

The k-support bounds of the other items are cal- culated as similar as above.

The count of the item 1 is 4, which implies that it may be the subset of the large 3 or 4-itemsets. We should keep item 1 in C1. By similar argument, items 2, 4 and 5 will be kept in C1. Item 3 will be pruned because none of the k-support bounds for item 3 is less than or equal to the count of item 3. Therefore, C1 will become ff1g,f2g,f4g,f5gg.

By similar method, all the candidate and large itemsets will be generated as the following.

Pass II (k=2)  During the join step, the algorithm will generate the following potentially large itemsets:  f1, 2g,f1, 4g,f1, 5g,f2, 4g,f2, 5g,f4, 5g  During the prune step, the estimated upper bounds for the above corresponding itemsets will be (4,4,4,5,5,6) respectively. All the support bounds are calculated as above Pass I.

After calculating all the support bounds, all the itemsets in C2 will remain, as all of them may be large in the coming passes.

During the checking step, the updated counts for the itemsets will be (2,3,4,5,5,6) respectively.

From the support bounds calculated in the prune step, f1, 2g cannot contribute to any large item- sets in the current or future passes, and it will be pruned in this stage.

After calculating the weighted support in the re- maining candidate set  C2 = ff1; 4g; f1; 5g; f2;4g;f2;5g;f4;5gg  we found that the large itemset is f4, 5g, and we put f4, 5g into L2. The itemsets in C2 will be used in next pass.

We will use the above method in the remaining pass.

3. Rule generation from the L = L [ Lk is exactly the same as the Apriori Gen [1]. 2  4 Mining NormalizedWeighted associ- ation rules  In this section, we focus on the mining of weighted association rules for which the weight of an itemset is normalized by the size of the itemset. We can still apply the previous algorithm MINWAL(O) in Section 3 for this case simply, with a modi?cation of the de?- nitions of large itemsets and k-support bound (see be- low). However, we will design another new algorithm, and we shall present this, called MINWAL(W), in Sec- tion 4.1.

De?nition 7 The normalized weighted support of a rule X ) Y is given by   k  @ X  ij2(X[Y )  wj  A (Support(X [ Y )) (8)  where k = size of the itemset (X [ Y )  De?nition 8 A k-itemset X is called a small item- set if the normalized weighted support of such an itemset is less than the minimum weighted support (wminsup), or  P ij2(X)  wj  k  !

Support(X) < wminsup (9)  Otherwise, it is a large k-itemset.

We can de?ne the k-support bound for the normal- ized case.

De?nition 9 The minimum support count for a large k-itemset which contains Y is called the k-support bound of the itemset Y with normalized weight and it is given by  B(Y; k) =  ? k wminsup  W (Y; k) ? T  ? (10)  4.1 Another approach for normalized weighted case  In the following, we will design an algorithm which generate large itemsets in an iterative manner as in Apriori-Gen and also e?ectively pruning candidate itemsets in each iteration. Although the closure prop- erty of \the subset of a large itemset must be large" still does not hold in the normalized weighted case, we can ?nd the closure property in a di?erent manner.

De?nition 10 For an itemset X = fx1 ? ? ?xng, let the smallest weight of the items be wi. An itemset Y = X [Z, where the items in Z have weights all less than wi, is called a low-order superset of X.

De?nition 11 An itemset Y ? X, such that each item in Y has a weight greater than or equal to each item in X ? Y , is called a high-order subset of X.

Lemma 1 If an itemset Y is large, then any high- order subset of Y must also be large.

Proof Let X be a high-order subset of Y . The average weight of X is greater than or equal to the average weight of Y . The support of X is also greater than or equal to that of Y . Hence the weighted sup- port of X is greater than or equal to that of Y . If Y is large, then X will be also large.

For example, if weights of items 1,2,4,5 are in as- cending order, and f1, 2, 4, 5g is a large 4-itemset, then itemsets f5g, f4, 5g and f2, 4, 5g must also be large.

Lemma 2 A large (k + 1)-itemset X must be a low- order superset of some large k-itemset Y .

Proof: IfX is large, then from Lemma 1, any high- order subset ofX must also be large. Let x be the item in X with the lowest weight. Then Y = X ? x is a high-order subset of X and it must be large. Hence X is a low-order superset of Y .

With the above ?ndings, we can modify some steps in the previous algorithm. The modi?ed algorithm is presented in the following.

4.2 Algorithm for Mining Normalized Weighted Association Rules  The inputs and output for an algorithm for mining normalized weighted association rules are the follow- ing.

Inputs: Same as Algorithm 1 Output: Interesting normalized weighted associa-  tion rules.

Most of the notations are similar to that of Algo-  rithm 1, except for the following:  Notations: Ck set of candidate k-itemsets.

wminsup normalized weighted support threshold.

Algorithm 2 MINWAL(W) 1 Main Algorithm (wminsup;minconf ,D, w) 2 size=Search(D); 3 L=;; 4 for (i=1;i?size;i++) 5 Ci=Li=;; 6 for each transaction do 7 (SC,C1)=Counting(D,w); 8 k=1; 9 while (Ck 6= ?) 10 k++; 11 Ck=Join (Lk?1); 12 Ck=Prune (Ck); 13 (Lk)=Checking (Ck,D); 14 L=L  S Lk;  15 Rules (SC;L); 16 ends;  The subroutines of MINWAL(W) are outlined as follows:  1. Search(D): Same as in Algorithm 1.

2. Counting(D,w): Same as in Algorithm 1.

3. Join (Lk?1): The subroutines Join, Prune, and Checking generate Lk and Ck. The main job in the join step is to generate Ck. From Lemma 2, a candidate k-itemset must be a low-order superset of some large (k ? 1)-itemset. In this step, we join the large itemsets in Lk?1 with one of the items that have lower weights to form a low-order superset. For example, if w1 ? w2 ? ? ? ? ? w5 are the weights of the item f1,2,3,4,5g, and f3, 5g is a large itemset found in pass II, the join step will construct f1, 3, 5g and f2, 3, 5g itemsets only.

Those joined itemset will be put in Ck. f3, 4, 5g cannot be a large itemset because if it is, then f4, 5g should be a large itemset.

4. Prune (Ck): During the prune step, a candidate k-itemset X will be pruned if all the j-support    bounds of the X, j ? k, are greater than the smallest known support count among the (k?1)- subsets ofX, which is an estimation and an upper bound of the support count of the k-itemset X.

5. Checking (Ck,D): The checking procedure will be done similar to Algorithm 1, except that the remaining candidate itemset will be the set of large itemset Lk, and the next pass will be based on the Lk to generate the candidate sets.

6. Rules (SC;L): Same as in Algorithm 1.

The framework for our proposed algorithm for mining weighted association rules (with averaging of weights) is similar to Apriori Gen [1] and Algorithm 1, but with some major di?erences in the details. Al- though the large k-itemsets can be generated from large (k ? 1)-itemsets, but it is not true that all the subsets of a large itemset should be large. The modi- ?cation is on the generation of candidate sets. In the Apriori Gen process [1], generating the candidate sets Ck is based on the large itemsets Lk?1. Here we also generate Ck from Lk?1. However, we would not con- sider k-itemsets where all (k? 1)-subsets are in Lk?1, since not all (k?1)-subsets of a large k-itemset should be large. Instead, we shall consider the low-order su- persets of the itemsets in Lk?1 (from Lemma 2).

During the prune step, we check the estimated weighted support of the itemsets. The di?erence be- tween this and the Apriori Gen is not to check the subsets of the itemsets being large. We use the sup- port bound values instead.

Example 4 Refer to Tables 1 and 2, we will show how the large  itemsets are generated from the transaction database.

Suppose that the wminsup (weighted minimum sup- port) is 0.45. Similar to Algorithm 1, we ?nd the maximum possible size of the large itemsets, which is 4.

Pass I (k=1, where k is the size of the itemset) The support counts (SC) of the 1-itemset  f1,2,3,4,5g will be f4,5,1,6,7g. For these counts, for example, the weighted support of item f1g will be  0:1? 47 = 0.06  By the closure property, the high-order subset of a large k-itemset should be large. Therefore, items f3g, f2g and f1g will be pruned. Hence C1 = ff4g; f5gg.

Pass II (k=2) During the join step, the algorithm will generate  the following potentially large itemsets:  D Number of transactions T Average size of the transactions I Ave. size of the max. potentially large itemsets L Number of maximal potentially large itemsets N Number of items  Table 3: Parameters of the synthetic database  f5, 4g,f5, 3g,f5, 2g,f5, 1g,f4, 3g,f4, 2g,f4, 1g During the prune step, the estimated weighted sup-  port of itemset f5, 4g will be (0:8+0:9)2 ? 7 = 0.72  Others will be calculated as the above.

During the checking step, the updated count for the  itemset f5, 4g will be 6. From the calculation of the weighted support, f4, 5g is found to be a large itemset, and is put in L2.

We will use the same method for the remaining passes, until no itemset found in candidate set. 2  5 Performance Study A performance study is carried out for the two algo-  rithms, MINWAL(O) and MINWAL(W). A series of experiments are done to show the performance of these algorithms on an UltraSparc 1 machine with 256MB of main memory. The algorithms are written in C and the timing is measured by the cpu time calculated from the built-in timing functions of UNIX Shell. In order to control di?erent parameters in the experimen- tal setup, we use synthetic databases and weights in the experiment. The method of generating such syn- thetic data will be explained in Section 5.1.

In the following experiments, we use di?erent trans- action databases with the same set of item weights.

The reason for using the same item weights is to keep this factor constant in order to compare the e?ciency of the algorithms.

0 0.2 0.4 0.6 0.8 1  freq uen  cy  weights  ?frequency?  Figure 1: Distribution of Weights  5.1 Generation of Synthetic Data and Weights  During the experiment, we will use synthetic data to evaluate the performance. The data generation pro- cedure is similar to [1].

Before the generation of the database, the parame- ters D;L, N , T and I are set to be 100K, 2000, 1000, 5 and 2 respectively, with 2.4MB of database.

In the generation of the weights, we assume that the number of the low weight products will be much more than the high-weight products. We generate the weights according to an exponential distribution, and the result is shown in Figure 1.

5.2 Performance Evaluation  We study the e?ect of di?erent values of wminsup, number of transactions and items, etc., on the pro- cessing time, for algorithms MINWAL(O) and MIN- WAL(W). We use the hash-tree data structures [1] in the following experiments.

5.2.1 Performance Evaluation of the two al- gorithms  The experiment will be done on the two algorithms (MINWAL(O) and MINWAL(W)) under two condi- tions: the unnormalized case (as describe in Section 2) and the normalized case (as discussed in Section 4). Since MINWAL(W) cannot be applied to the un- normalized case, we need to consider only three cases: MINWAL(O) for cases with and without normaliza- tion, and MINWAL(W) for the normalized case.

The test will be based on the synthetic databases.

We use 5 values of thresholds (wminsup) for each test.

We use f0.0016, 0.0017, 0.0018, 0.0019, 0.0020g in the mining of unnormalized weighted association rules, while wminsup of f0.0006, 0.0007, 0.0008, 0.0009, 0.0010g are used in the mining for the normalized weighted case. These threshold values are chosen to generate a reasonable number of large itemsets and rules. For simplicity we use the values of f1,2,3,4,5g to represents these sets of threshold values in most of the ?gures.

Figure 2 shows the decreasing trend of the execu- tion time when the support threshold increases. As the threshold increases, the candidate itemsets de- creases. The execution time would decrease for the smaller resulting hash-tree because of the decreasing searching time.

In Figure 3, it is noted that the time needed for each pass for the MINWAL(O) is much less than the MINWAL(W), especially from pass 2 to pass 4. Furthermore, when comparing the two algorithms MINWAL(W) and MINWAL(O) in the normalized weighted case, it is noticed that the execution time of MINWAL(O) is much less than the algorithmMIN- WAL(W), especially for the cases with smaller thresh-        1 1.5 2 2.5 3 3.5 4 4.5 5  Ti m  e (in  s ec  on ds  )  Threshold (wminsup)  MINWAL(O) unnormalized weighted support        1 1.5 2 2.5 3 3.5 4 4.5 5  Ti m  e (in  s ec  on ds  )  Threshold (wminsup)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support        1 1.5 2 2.5 3 3.5 4 4.5 5  Ti m  e (in  s ec  on ds  )  Threshold (wminsup)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 2: Overall execution time            1 2 3 4 5 6 7  Ti m  e (s  ec )  Pass Number  MINWAL(O) unnormalized weighted support            1 2 3 4 5 6 7  Ti m  e (s  ec )  Pass Number  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support            1 2 3 4 5 6 7  Ti m  e (s  ec )  Pass Number  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 3: Execution time for each pass          1 1.5 2 2.5 3 3.5 4 4.5 5  Pa ss  Threshold (wminsup)  MINWAL(O) unnormalized weighted support          1 1.5 2 2.5 3 3.5 4 4.5 5  Pa ss  Threshold (wminsup)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support          1 1.5 2 2.5 3 3.5 4 4.5 5  Pa ss  Threshold (wminsup)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 4: Passes needed           1 2 3 4 5 6 7  N um  be r o  f C an  di da  te It  em se  ts (i  n lo  g)  Pass Number  MINWAL(O) unnormalized weighted support         1 2 3 4 5 6 7  N um  be r o  f C an  di da  te It  em se  ts (i  n lo  g)  Pass Number  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support         1 2 3 4 5 6 7  N um  be r o  f C an  di da  te It  em se  ts (i  n lo  g)  Pass Number  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 5: Size of the candidate itemsets  olds. It was because the number of the candidate item- sets needed to be checked in MINWAL(W) is much more than in MINWAL(O).

Figure 4 shows the number of passes needed in mining of the association rules. From the graph, the number of passes needed for mining the unnormalized weighted case is more than the normalized case.

The number of candidate itemsets mined is shown in Figure 5 , where we set wminsup = 0:006 for the normalized weighted case and wminwup = 0:0016 for the unnormalized case. The number of candidate itemsets generated by MINWAL(W) is greater than that ofMINWAL(O) for the normalized weighted case.

Based on the all the above ?gures, we can see that MINWAL(O) is in general more e?cient in the min- ing of normalized weighted association rules. This is because that the time needed to check the candidate itemsets is much less than MINWAL(W).

5.2.2 Scale-Up Experiment  In the following, we examine how the performance varies with the number of items and transactions. All other things being equal.

With wminsup = 0:002 for MINWAL(O) for the unnormalized case, and wminsup = 0:001 for other cases, the execution time increases with the number of items since the more the items, the larger the hash tree. As the hash tree grows directly with the items, the execution time in the hash tree searching would be increased, which is shown in Figure 6.

In Figure 7, we are interested in the relation be- tween the number of transactions and the execution time. Keeping other things equal (1000 items and weights), we generate di?erent transaction database   5.5   6.5   7.5   300 400 500 600 700 800 900 1000  Ti m  e (In  L n  Se c)  Number of Items  MINWAL(O) unnormalized weighted support   5.5   6.5   7.5   300 400 500 600 700 800 900 1000  Ti m  e (In  L n  Se c)  Number of Items  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support   5.5   6.5   7.5   300 400 500 600 700 800 900 1000  Ti m  e (In  L n  Se c)  Number of Items  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 6: Number of items scale-up   5.5   6.5   7.5  30 40 50 60 70 80 90 100  Ti m  e (In  L n  Se c)  Number of Transaction (K)  MINWAL(O) unnormalized weighted support   5.5   6.5   7.5  30 40 50 60 70 80 90 100  Ti m  e (In  L n  Se c)  Number of Transaction (K)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support   5.5   6.5   7.5  30 40 50 60 70 80 90 100  Ti m  e (In  L n  Se c)  Number of Transaction (K)  MINWAL(O) unnormalized weighted support MINWAL(W) normalized weighted support MINWAL(O) normalized weighted support  Figure 7: Number of transactions scale-up         1 1.5 2 2.5 3 3.5 4  Ti m  e (s  ec )  Pass Number  MINWAL(W) for w=0.0007         1 1.5 2 2.5 3 3.5 4  Ti m  e (s  ec )  Pass Number  MINWAL(W) for w=0.0007 MINWAL(W) for w=0.0006         1 1.5 2 2.5 3 3.5 4  Ti m  e (s  ec )  Pass Number  MINWAL(W) for w=0.0007 MINWAL(W) for w=0.0006 MINWAL(O) for w=0.0007         1 1.5 2 2.5 3 3.5 4  Ti m  e (s  ec )  Pass Number  MINWAL(W) for w=0.0007 MINWAL(W) for w=0.0006 MINWAL(O) for w=0.0007 MINWAL(O) for w=0.0006  Figure 8: Special case with 0/1 weights for normalized case    with the same parameters but di?erent in size, rang- ing from 25K to 100K. The values of wminsup are set as the above scale-up experiment. In this ?gure, the time is given in ln(sec). From the ?gure, the execu- tion time increases with the number of transactions linearly with ln scale, implying that the complexity of the algorithms is exponential in the number of trans- actions [1].

5.2.3 Experiment for special case  In this section, we are interested in the performance in the special case, which is the item weights equal to 0 or 1 only. In this case, we make the ?rst 900 weights be 0 and the remaining weights be 1. Other things, including database and threshold, equal as above sec- tion. We carried out the experiment for the normal- ized weighted case to compare the two algorithms.

There are two major ?ndings:  1. The performance of the special case is much bet- ter than the general case where item weights fol- low a distribution between 0 and 1.

2. Contrary to the previous cases, MINWAL(W) performs better than MINWAL(O). From Fig- ure 8, we notice that the time needed in MIN- WAL(W) is much less than the MINWAL(O), for all the thresholds. This is because in the join- ing step, the number of starting seed (candidate itemsets in C1 to generate itemsets in C2) is less than MINWAL(O) case. In this situation, the 0/1 weights give the advantage to MINWAL(W).

During the ?rst step, the algorithmMINWAL(W) will easily prune all the small itemsets with 0 weights, while MINWAL(O) will keep those small itemsets with 0 weights. As the starting seed is smaller in size, MINWAL(W) would perform well in this case.

6 Conclusion We have proposed to study a new problem of min-  ing weighted association rule. This is a generalization of the association rule mining problem. In this gen- eralization, the items are assigned weights to reect their importance to the user. The main di?erence be- tween mining weighted association rules and the min- ing unweighted association rules is the downward clo- sure property.

We proposed two di?erent de?nition of weighted support: without normalization, and with normaliza- tion. We proposed new algorithms based on the sup- port bounds: the algorithms MINWAL(O) and MIN-  WAL(W). MINWAL(O) is applicable to both normal- ized and unnormalized cases, and MINWAL(W) is ap- plicable to the normalized case only. The performance evaluation has been done on these two algorithms. We found that MINWAL(O) outperforms MINWAL(W) in most cases, but MINWAL(W) performs better for the special case with only 0/1 item weights.

So far we have only considered the mining of binary weighted association rules. Some of the researchers did the research for the problem of the quantitative assoication rules, such as [4], [3]. We may investi- gate the problem of quantitative association rules with weighted items, which is an interesting topic in the fu- ture.

