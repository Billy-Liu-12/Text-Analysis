Application National Taiwan University of Science and Technology, Taipei, Taiwan,  Dec. 6-8, 2013

Abstract?Aiming at the problems of increasing energy  consumption of cloud storage, big environmental resources  pressure, exist research on replica placement lacking of  focus on energy consumption, this paper proposes a energy  conservation replica placement method ECRP(Energy  Conservation Replica Placement, ECRP). Based on the  three main different levels of energy consumption of  hardware components, the data node is divided into hot  zone, warm zone and cold zone three logical zone, each zone  uses different power management strategies and replica  placement strategies, and after building a file activity pre-  diction model which based on the simulated annealing BP  neural network, files are stored in the data nodes on dif-  ferent zone according to the type of file activity, and then  calculate the file activity periodically, when file activity  exceeds a certain threshold, the file begins flow among the  hot zone, warm zone and cold zone step by step in the  two-way. Experiment results show that this method better  solve the problems of replica placement and energy con-  sumption optimization, have good energy saving effect.

Keywords: replica placement; simulated anneal; neural  network; file activity; energy conserving.



I. INTRODUCTION  With the continuous improvement of the whole so-  cial information degree, especially under the push of  Cloud Computing, the Internet of Things and Mobile  Internet, the scale of applied information processing and  data service in various industry is more and more large,  various types of data-intensive applications emerging, the  amount of data generated by the application presents  geometric growth. IDC?s research report pointed out that  the world in 2010 produced 1.2 ZB (1 ZB = 1 trillion GB)  data [1], the explosion of digital information scale pro-  motes the people to the huge demand for information  storage. Turing Award winner Jim Gray presented a new  Moore's Law in storage industry, that every 18 months the  amount of additional memory storage capacity is equal to  the sum ever. UC Berkley study shows that the data  generated from the next three years will be more than the  sum of the past 40000 years, and 93% of the new gener-  ation information will be existed in digital form. Such  vast amounts of data, makes the global data center's scale  expands, the consumption of energy is also increasing.

Pike Research points out that the data centers consumes  20.18 billion degrees of electricity in 2010, electricity  expenses as much as $23.3 billion. Electricity consump-  tion of data centers has been close to a total generating  capacity of the Three Gorges power station. Huge  amounts of data in the cloud provide more intelligent,  more convenient services for people at the same time, and  also brought the bottleneck and confused of data appli-  cation, and greatly increasing the difficulty for people to  find useful knowledge. How to effectively storage, pro-  cess and manage these massive data has been a new  challenge facing in cloud computing era, but also become  the driving force for many researchers to explore new  concepts, new methods and new technology of cloud  computing.

Now usually adopts the replica placement method to  solve the problem of availability and reliability of the data  in cloud computing environment. How to efficiently  place the replica, making the highest income obtained  under the minimum cost, is one of the key problems of  cloud storage system needs further study[2,3].Research  suggests that by suitable replica placement strategy will  keeping multiple copies of a data item in a distributed  system of multiple nodes, can effectively improve the  reliability, throughput, and performance[4] of system by  smaller cost, but also has proved that the replica place-  ment problem is a NP-complete problem[5, 6], it's impos-  sible to find a universal optimal solution, can only obtain  a nearly optimal solution by heuristic method. Therefore,  the research of the replica placement problem in cloud  computing, finding nearly optimal solution which suita-  ble for cloud storage environment, has important signif-  icance.

The current study of replica placement mainly di-  vided into two categories, one category is to establish  mathematical model according to the specific optimiza-  tion goal, optimize replica placement by mathematical  models.

Among them, Liu Tian Tian, etc.[7] discussed the  research status of replica management from five aspects  of replica?s creation, positioning, selection, delete and  consistency, and analyzed that in different network en-  vironment, different file system requirements and dif-  ferent application requirements environment, can adopt  management methods in different emphasis. Wang et  etc.[8] proposed a RPRTM model, the model consider the  response time in each node, and use the genetic algorithm  to minimize the replica number while ensuring the re-  sponse time of node. Wei[9] modeled the relationship  between the usability of files and the replica number us-  ing queuing theory, puts forward a dynamic adjustment  mechanism for replica. Zhu Jiayu, etc.[10] proposed a  dynamic replication management strategy, by establish-  ing the relationship model between the availability of  Xiao-Yong Zhao Lei Wang  School of Information Management, School of Information Management, Beijing Information Science and Technology University Beijing Information Science and Technology University  Beijing , China Beijing , China  e-mail:  zhaoxiaoyong@bistu.edu.cn e-mail:   wanglei575882@163.com  An Activity-based Replica Placement Method of  Energy-conservation  National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013   maxim ?????     Application National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013  files and the replica number to calculate the minimum  number of replica which systems should maintain while  satisfy the requirement of file availability at the same time,  and then place the replicas according to the number of  copies data is accessed and transport costs, to achieve  optimal system performance and load balancing; FuWei,  etc.[11] in view of the application scenario with strict re-  quirement of the quality of service, established an ab-  stract replica placement model which based on graph  theory on the basis of service quality distance, and make  the system to meet all user's QoS requirements by looking  for approximate optimal QoS-aware replica placement  method. Agneeswaran etc.[12] establish relationship  model between data replica and availability, and then set  a threshold according to the need, and calculate minimum  number of replica that each file block needs.

The other is based on historical information, build a  statistical prediction model, and thus predict the optimal  layout of replica placement. Among them, Li Jinmiao etc.

[13] put forward a dynamic mechanism based on user be-  havior analysis, by analyzing the replica visitor, access  time, file size, access to reply, delay and other relevant  information, use the file heat to determine the dynamic  changes of the number of replica, and adopt the  best-effort approach to determine the location of the  replica, so as to achieve the target of dynamically change  the number of replicas and confirm the location of copies,  to provide users with more flexible, efficient and reliable  safeguard mechanism. Zou Li Da etc.[14] proposed replica  placement policy based on traffic prediction, and intro-  duced two kind of traffic forecast methods, one is based  on historical data and the other one is based on artificial  neural network.

Now these studies mainly focus on how to efficient  use of storage space, improve the efficiency of file access,  enhance the system reliability and guarantee the data  consistency and so on, but usually there are two main  problems:  (1) Does not distinguish between data nodes in the  cluster, and think that all nodes are homogeneous; But  real data center?s servers are usually batch purchased in  different period, even if the new data center, as time goes  on, later will also join the cluster server which has better  computing performance, I/O performance and more  storage space than previous server.

(2) Does not distinguish between data, but treat the  data in the same position; In fact, the data have a life cycle,  show the obvious time activity associated characteristics;  This paper based on the heterogeneous of servers,  the three main working status in different levels of energy  consumption and the characteristics of different activity  level in data lifecycle, puts forward an energy-saving  replica placement algorithm based on file activity. First,  based on three main different levels of energy consump-  tion of hardware component?s working state, divide the  data node into three logical zones: Hot Zone, Warm Zone  and Cold Zone, each zone adopt corresponding power  management strategy and replication strategy. Using  historical data as the training samples to construct the file  activity prediction model which based on simulated an-  nealing BP neural network, and store the files in different  parts of the data node depending on the activity of files,  calculate the activity of files in each zone periodically, if  the file activity over (below/above) a certain threshold,  the file will start two-way flow among hot zone, warm  zone and cold zone. Experimental results show that  compared with HDFS built-in random replica placement  strategy, this method can 28% reduction in energy con-  sumption, at the same time, reduce 7% of the storage  space.

Section II is the construction of the file activity pre-  dicted model. Section III is the toward ener-  gy-conservation replica placement method. Section IV is  the actual simulation results analysis. Finally, we give a  conclusion and proposed future research recommenda-  tions.



II. FILE ACTIVITY PREDICTED MODEL  Studies show that the newly created file is accessed  most frequently, with the passage of time, the access  frequency drops. In large-scale file system, most files  unused for long, a few files frequently used[15]; In the  Yahoo 5PB data Hadoop cluster constituted by 2600  servers,90.26% of the data within 2 days after creation  will be accessed, and 89.61% after 10 days of data access  in creating trend is clearly down, and 60% The data has  never been accessed in 20 days[16]. The file prediction  model has been extensively studied by many researchers  based on this statistical properties, at present, there are  two models, a model based on global accessing infor-  mation and the other is  based on the recently accessing  information.

Among them, the model based on global accessing  information typical representatives: Griffioen, etc.[17]  proposed methods based on probabilistic graphical model  built with the greatest probability of succeeding the file as  predictions; Kroeger, etc.[18]proposed ideas based on data  compression multi-tree prediction model FMOC, using  the higher probability of occurrence matching path as the  prediction results.

The other, the model based on recently accessing  information typical representatives: LS (Last Successor)  model using the latest successor to the current accessing  file as the prediction results, based on LS models con-  structed PLS[19] model, ULS[20] model and PULS[21]  model. In these prediction models, the main use of the file  attributes include the number of users to access the file,  the size of the requested data, the time characteristic, file  access times (repeatability) and I/O operation types etc.

As the cloud computing environment files accessing  variability and complexity, define a precise mathematical  model to describe the multiple nonlinear relationship of  file activity with these factor is very difficult, so this  paper used simulated Annealing BP neural network  method with the self-learning characteristics, through  historical data training, obtained the file activity predic-  tion model to predict the activity of the file. In order to  simplify the complexity of the model, the paper select the  National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013      Application National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013  file access times, number of users, and the average access  time as the factors of file activity model.

A. Sample data selection  File activity is defined as the file being accessed  probability within the next cycle; this paper use Microsoft  MSN Storage File Server Traces of IOTTA as sample  data, which contains 6 hours of file access logs at 10  March 2008. Samples were divided into two groups, the  first five hours as training samples, the last hour of data as  the prediction sample.

B. BP network structure design  BP neural network can approximate any nonlinear  function that can self-learning and adaptive, with the  ability to generalize the learning results, but also has  certain degree of fault tolerance, has been widely used.

Therefore, the paper uses BP neural network prediction  model. BP network structure design needs to determine  the network layers, each layer of nodes, transfer function,  the initial weights and learning algorithms, still no  common approach for the clearly calculation , more so in  certain guiding principles, relying on human experience  and continuous trying. Its construction process is as fol-  lows:  (1) Hidden layers  According to Kolmogorov theorem, for any con-  tinuous functions in closed interval, a three-layer BP  network including one hidden layer sufficient for any  n-dimensional to m-dimensional mapping, combined  with our problems scene in this paper, neural network  hidden layer  selected as one layer.

(2) Transfer function  As the network output value meaning file activity  was the probability value, so the output layer transfer  function selection logsig function, limited its value in the  range 0-1. The transfer function of hidden layer   used the  try and error method to determine, selected respectively  tan-sigmoid function and log-sigmoid function, using the  sample data for training the results, the system error will  be smaller while tan-sigmoid function be used, so choose  tan-sigmoid function as the transfer function of the hid-  den layer.

(3) The number of nodes in each layer  Based on the file average access time, the number of  users, the file history accessing times these three indica-  tors, used t, t-1, t-2 of 3 cycles of statistical data to build  file activity predictive models , so BP network input layer  node had 9, the output layer node had 1, the output value  was the file activity.

So far, people still did not find a common theoretical  approach to determine the number of hidden nodes, the  general principle was: under the premise to meet the  accuracy requirements and accurately reflect the in-  put-output relationship, used as little as possible hidden  layer nodes. More representative of the empirical formula  were:  ? ?? ?  ? ?? ?  ? ?  ?  n  i i  n Ck   1      ?1?  1011 ????? aamnn    (2)  11 ?? nn   (3)  nn 21 log?    (4)  Reference to the above empirical formula to deter-  mine the number of nodes in the hidden layer ranges from  5 to 13, and then continued to use the node cumulative try  and error method, comprehensive analysis of comparison,  determining 7 nodes in the hidden layer to be most ap-  propriate.

C. Simulated Annealing BP Neural Network  SA algorithm (Simulated Annealing) from solid  annealing principle has been proved theoretically to be a  probability of 1 to converge to the global optimal solution  of global optimization algorithm, the algorithm obtained  solution has nothing to do with the initial state, with  gradual convergence. Since the standard BP neural net-  work in the presence of gradient descent slow conver-  gence and easy to fall into local minima problem, we used  simulated annealing algorithm to train the BP network,  the mean square error:  ? ?  1 1  ?? ? ?  ?? p  p  m  j pjpj yymp  E ?  (5)  as the objective function, the problem was converted  to the use of simulated annealing algorithm to looking  optimal solution of BP network weights W and thresholds  B  to make E reaches the minimum, the algorithm was  described as follows:  Input: training sample set  Output: the network weights W and thresholds B  Algorithm steps:  Step 1: Initialized a higher temperature T0> 0, and  the network weights W and the threshold value B ini-  tialized;  Step 2: For each training sample, loop steps 3 to 8  until the annealing process termination condition be sat-  isfied, that is Ti reduced to a predetermined minimum  value;  Step 3: Calculate the sample mean square error  value E;  Step 4: Use obey Cauchy uniformly distributed  random perturbations generate new network weights W*  and thresholds B*, at this  time, BBBWWW ?? ???? ** , , W? , B?  which is a  random disturbance;  Step 5: Based on the new network weights W* and  the threshold value B*, for each sample to recalculate a  new mean square error E* and the error increment val-  ue EEE ?? *'? ;  Step 6: Use the Metropolis rule to determine  whether to accept the new solution: If 0? ?'E , then accept  the new W* and B* as a new network weights and  thresholds; otherwise under acceptance probability  ) '  exp( iT  E r  ?? ?  to determine whether to accept the new  solution, set p=random(0,1) as [0,1] interval uniformly  distributed random function, if r>p, then accept the new  National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013      Application National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013  W* and B* as a new network weights and thresholds, or  do not accept the new solution;  Step 7: Loop steps 3 to 7 until the system reaches  equilibrium, this time performed Markov chain length jl  iterations, j++;  Step 8: cooling with cooling schedule ii TT ??? ?1 , T  = Ti +1, Ti +1 <Ti, i ++, where ?  is a value between 0.8 and 0.9999;  Step 9: Used the current solution as the optimal  solution, output the network weights W* and threshold  value B*.

D. File activity prediction model  Using 2.2 to establish the BP network, and using the  simulated annealing algorithm to training, the predictive  model is:  ? ? ?  ? ? ? ?  ? ??  ?  ? ? ?  ? ????  ??      BBxWgVgy ji i  ji j  j ?6?  Thereinto, the input weights W:  ? ? ? ? ? ? ? ? ? ?  ?  ?  ????  ??  ??????  ????  ????  ???????  ????????  ? ? ? ? ? ? ? ? ? ?  ?  ?  ?         .........

.........

.........

.........

.........

.........

.........

w  Output weights:  V= (0.861 0.492 1.126 -1.189 0.669 0.869 0.776)  Input threshold  B1= (0.673 0.177 -0.237 -0.178 -0.232 0.504 -0.382)  Output threshold B2=0.587.



III. TOWARD ENERGY-CONSERVING REPLICA  PLACEMENT METHOD  Within the current server, the main power consump-  tion components including CPU, hard drive, memory,  fans, and power supply unit (PDU), these components  have different working state, corresponding the different  power. There are three major states: active state, idle state  and sleep state. The major hardware components of  mainstream server power consumption and powerup la-  tency case in different states [22] as shown in Table 1: Table I: Hardware component power consumption and  powerup-latency in different states.

A. Zone definition  Based on the above analysis, the data nodes parti-  tioned into three logic zone: the hot zone (Hot Zone),  warm zone (Warm Zone) and cold zone (Cold Zone), to  add Zone file metadata attribute representing the files  currently zone, you can set values for the {Hot, Warm,  Cold}. Each zone used different power management  strategy and replication strategy, including:  (1) Hot zone Zhot  Dhot: This zone stores frequently accessed data, the  activity of Y> = Thresholdhot;  Rhot: Using Rack-aware data replication policy, the  default replication is 3;  Phot: The nodes were Actived state, providing maxi-  mum performance and throughput.

(2) Warm zone Zwarm  Dwarm: This zone stores greater probability again re-  cently accessed data;Thresholdhot> Y> = Thresholdwarm;  Rwarm: Using Rack-aware data replication policy, but  the default replication is set to 2, the replication reduction  can be corresponding increase  disk space utilization;  Pwarm: The nodes were the Idle state, the energy con-  sumption is equal to 58% of Actived state, can return to  Actived state within 10 milliseconds; after files be ac-  cessed, the corresponding node was holding Actived state  for some time Twarm (default 1 hour) to respond to pos-  sible subsequent arrival access, Twarm of time without any  access, then re-enter the Idle state. The node transition  between the idle state and actived state, the main con-  sideration was to wake up around 10 milliseconds state  transition latency;  (3) Cold zone Zcold  Dcold: This zone stores rarely accessed data, Thresh-  oldwarm> Y;  Rcold: This zone files did not chunking, using Rack  Aware replica placement strategy, but the replication is  set to 2. Regardless of the chunk can guarantee access to  files in the zone, only need to activate a node, reducing  the replication also improves disk utilization;  Pcold: Nodes usually were Slept state, the energy equal  Actived state 1/30, in about 10 seconds to return to Ac-  tived state, to maximize energy savings. After files be  accessed, the corresponding node was holding Actived  state for some time Tcold (default is 30 minutes), to re-  spond to possible subsequent arrival access, Tcold of time  without any access, then re-enter Slept state.

Metadata node by detecting heartbeat signals to de-  termine whether the data node is available, but the cold  zone node is Slept state, can not send heartbeat, heartbeat  detection method therefore needs to be modified, usually  not detect the heartbeat of the zone nodes, only when the  node in Actived state, exchanged heartbeat information  with the metadata node.

B. Data Migration Algorithm  Research shows that file accessing was sudden load  characteristics, most access was concentrated on a cer-  tain period of time, while a smaller part of the dispersed,  Wang et al[23] obtained by experiment that most appli-  cations having I/O access burst characteristic, and the  65% of the write request focused on a period of time.

Therefore, in this paper ,at I/O idle time window t (0:00  am by default), then the metadata node MDS is relatively  idle, the background worker threads read file metadata,  then migrated data using the following algorithm:  Input: Zhot, Zwarm and Zcold metadata;  Output: Zhot successfully migrated files Chot, Zwarm  Hardware  Component  Active  Power  (W)  Idle  Power  (W)  Sleep  Power  (W)  Powerup  Latency  CPU 95 15 3 30us  Memory 4 2 0.2 <1us  Hard disk 11 9 1 10s  PDU 50 25 0.5 300us  FAN 10 1 - -  National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013      Application National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013  successfully migrated files Cwarm, Zcold successfully mi-  grated files Ccold;  Step 1: Extracted access times, creation time, last  access time, file block number from metadata, the number  of files migrated successfully initialized Chot = 0, Cwarm =  0, Ccold = 0;  Step 2: Using formula (6) file activity prediction  model to calculate next time period of activity of Zhot,  Zwarm and Zcold zone files;  Step 3: Joined files of hot zone below the threshold  Thresholdhot to be migrated list Queuehot; Joined files of  warm zone below the threshold Thresholdwarm to be mi-  grated list Queuewarm; Joined files of warm zone above the  threshold Thresholdhot to be migrated list Reverse-  Queuewarm; Joined files of cold zone above the threshold  Thresholdwarm to be migrated list ReverseQueuecold;  Step 4: Batched migrated Queuehot,Queuewarm, Re-  verseQueuewarm and ReverseQueuecold files, according the  corresponding zone replica placement strategy to place  files, once file for each migrated successfully, the cor-  responding Chot, Cwarm, Ccold plus 1;.



IV. EXPERIMENT RESULTS AND ANALYSIS  A.Experiment environment  In order to validate the replica placement method in  this paper, we implemented a set of prototype based on  the Hadoop platform, and compared it with unmodified  Hadoop , Hadoop which using standard BP neural net-  work and Hadoop which using SA BP neural network in  system energy consumption, performance, space utiliza-  tion, hard Life and other aspects. Experiment platform  was a Hadoop cluster which consisted of 31 servers, the  proportion of the number of nodes in each region refer-  ences the proportion in paper[24], zone division and the  hardware configuration of nodes in each zone is shown in  Table 2: Table 2.  Configuration of Nodes in Each Zone  Software Environment: operating system is 64-bit  CentOS 5.5 (kernel version 2.6.18), the original version  of Hadoop is 1.0.0, JDK version 1.6.0.

B. Experiment results  The comparison in system energy consumption was  shown in figure 1:  Figure 1: Comparison of energy consumption  As we can see from Figure 1, thanks to the low energy  consumption in warm zone and cold zone, the system's  overall energy consumption was reduced by 28%. A  cluster node that composed of 1,000 nodes of this con-  figuration, according to the Beijing electricity price  0.6625 Yuan/kW.h, can save cost about $430000 a year.

In addition, the equipment energy consumption reducing  corresponding will also reduce cooling costs.

As the number of copies in warm zone and cold zone is  lower than traditional way, and the lower number of  copies also results in a corresponding reduction of the  disk space occupied, the prediction model established by  simulated annealing BP neural network is more accurate  than the standard BP neural network. So the disk space  occupancy rate is lower when using simulated annealing  BP neural network, the overall disk space utilization in-  creased by 7%. The Comparison of space occupied is  shown in figure 2:  Figure 2: Disk space usage  Proportion of number of visits in each zone as shown  in figure 3:  Figure 3: The proportion of visits in each zone As can be seen from Figure 3, in the way of standard  BP neural network and simulated annealing neural  network, more than 96% of the accesses are performed  on the hot zone files, the performance impact is negli-  gible, and, because of the division of logical partitions, it  can optimize the strategy of hot zone targeted, to gain  higher throughput; 2.5~3.0% access to warm zone have  10 milliseconds delay; Only 0.42 to 0.62% for cold zone  have greater access delay, and for the cold zone file ac-  cess probability is very low; Moreover, since the simu-  lated annealing BP neural network prediction model?s  prediction accuracy rate is higher, so the hot zone file  access has higher proportion, and the warm and cold  zones have fewer visits. In general, the improvement way  has little impact on the performance for the whole system.

In the ways of standard BP network and SA BP net-  work, the amount of transfer data between three zones,  was shown in Figure 4:  Zone Num-  ber  CPU Memory Hard  disk  Power  MDS 1 X3430 4G 500G 450  Hot 18 X3430 4G 2T 450  Warm 6 Core i5 2G 2T 270  Cold 6 Core i5 1G 2T 10  National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013      Application National Taiwan University of Science and Technology, Taipei, Taiwan, Dec. 6-8, 2013  Figure 4: The amount of transfer data between zones As can be seen from the Figure 4, the average amount  of data transferred daily between hot zone with warm  zone is 48 GB, and transferred between warm zone with  cold zone is 27.2 GB, in the condition of 80MB/s hard  disk speed and Gigabit Ethernet network environment, on  average, they need 10 minutes and 6 minutes to complete  separately, and the network will not result in a significant  effect. Under the simulated annealing neural network  approach, the improvement of the accuracy of prediction  model makes data transfer becomes smoother.



V. CONCLUSION  This paper proposes an energy conservation replica  placement method ECRP(Energy Conservation Replica  Placement, ECRP),using simulated annealing BP neural  network with self-learning feature, through historical data  training to obtained file activity prediction model ,then  predict the activity of the file. This method?s essence was  based on the data load scheduling, clustered files together  by heat and thus affect server loads. On the other hand,  based on the three major levels of energy consumption of  hardware components, the data node were divided into  hot, warm and cold three logical zone, each zone uses  different power management strategies and replica  placement strategies ,files are stored in the data nodes on  different zone according to the type of file activity, and  then calculate the file activity periodically, when file ac-  tivity exceeds a certain threshold, the file begins flow  among the hot zone, warm zone and cold zone step by  step in the two-way. Experimental results show that the  paper proposed method better use of heterogeneous  servers and file life cycle characteristics of the different  level of activity, better solved the problems of replica  placement and energy consumption optimization, have  good energy saving effect. This paper did not depth re-  search the number of nodes ratio in each zone, how to  effectively divided zone, recommendations for follow-up  research.

