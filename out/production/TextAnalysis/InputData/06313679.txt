Mining Associations Using Directed Hypergraphs Ramanuja Simha #1, Rahul Tripathi #2, Mayur Thakur ?3

Abstract? We introduce the notion of association rules for multi-valued attributes, which is an adaptation of the definition of quantitative association rules known in the literature. The association rules for multi-valued attributes are integrated in building a novel directed hypergraph based model for databases that allows to capture attribute-level associations and their strength. Basing on this model, we provide association-based similarity notions between any two attributes and present a method for finding clusters of similar attributes. We then propose an algorithm to identify a subset of attributes known as a leading indicator that influences the values of almost all other attributes.

Finally, we present an association-based classifier that can be used to predict values of attributes. We demonstrate the effectiveness of our proposed model through experiments on a financial time- series data set (S&P 500).



I. INTRODUCTION  Data Mining involves searching interesting patterns and/or classifying data. Association rules help to discover interest- ing patterns by identifying implication relationships among attribute-value pairs present in the data. An example of an association rule is: ?If a customer buys milk and diapers, then the customer also buys beer and eggs.? Here, milk, diapers, beer, and eggs are attributes, and they each take value ?1? (where ?1? denotes present and ?0? denotes absent) in the association rule. Attributes can be either categorical, e.g., days of a week, or quantitative, e.g., stock price. Agrawal et al. [1] presented association rules that target identifying implication relationships among categorical attributes that take only 0/1 values. Such association rules are called boolean association rules. Srikant et al. [2] introduced the more general quan- titative association rules that accommodate both categorical and quantitative attributes. Henceforward, we will refer to quantitative association rules as association rules.

Much work [3], [4], [5] has been done in mining association rules that satisfy constraints, such as minimum support (a measure of significance) and minimum confidence (a measure of predictive ability). Association rules can be used to solve the following problems: finding clusters of similar attributes, find- ing a small subset of attributes that influences a large section of other attributes, and finding classification rules to predict values of attributes. Some domains where association rules have found important applications are as follows: in market- basket type data for identifying sales patterns, in medicine  for identifying relationships among medical conditions and diseases [6], in bioinformatics for identifying interrelationships among genes [7], [8], and in finance for identifying prediction relationships among stocks.

Knobbe et al. [9] proposed an approach to mine a small set of binary attributes that help differentiate observations in a database. Siebes et al. [10] and Bringmann et al. [11] proposed techniques for compressing a database. The compressed set of binary attributes could then be used in a data mining classifier to avoid overfitting. The above methods mainly attempt to identify informative sets of binary attributes, whereas our approach attempts to build a generic model for any database containing multi-valued attributes and address a variety of problems such as similarity, clustering, leading indicators, and classification.

We propose a novel model for any database using a di- rected hypergraph in which the nodes represent attributes and the directed hyperedges represent many-to-many relationships among the attributes. The weights on directed hyperedges capture the likeliness of association in a particular direc- tion. We introduce the notion of association rules for multi- valued attributes, which is an adaptation of the definition of quantitative association rules known in the literature [2]. The association rules for multi-valued attributes are integrated in building the directed hypergraph model. Basing on this model, we provide association-based similarity notions between any two attributes, present a method for finding clusters of similar attributes, and propose an algorithm to identify a subset of attributes known as a leading indicator that influences the values of almost all other attributes. Finally, we present an association-based classifier that can be used to predict values of attributes. We demonstrate the effectiveness of our proposed model, algorithm, and classifier through experiments on a financial time-series data set (S&P 500).



II. MODELING ASSOCIATIONS USING DIRECTED HYPERGRAPHS  A. Associations Between Multi-Valued Attributes  Let D be a database in the form of a m?n table, where the rows correspond to observations and the columns correspond to multi-valued attributes. Let O = {O1, O2, . . . , Om} be the set of observations and A = {A1, A2, . . . , An} be the   DOI 10.1109/ICDEW.2012.56    DOI 10.1109/ICDEW.2012.56     set of attributes. The table entry for each attribute Ai and each observation Oj is a value from a fixed finite set V = {v1, v2, . . . , vk}. We denote such a database D as D(A,O,V).

For any X ? A?V , let ?1(X) denote {Ai | ?vj(Ai, vj) ? X}.

Definition 1: An association rule for multi-valued attributes (or mva-type association rule) in D(A,O,V) is an implication relationship of the form Xmva=?Y , where X, Y ? A ? V and ?1(X) and ?1(Y ) are disjoint subsets of A.

Definition 2: The support and confidence measures are gen- eralized for multi-valued attributes in a database D(A,O,V) as follows:  1) Let X = {(Ai1, vj1), (Ai2, vj2), . . . , (Air, vjr)} ? A ? V . The support of X , denoted by Supp(X), is defined as the fraction of observations in D for which Ai1 takes value vj1, Ai2 takes value vj2, . . ., and Air takes value vjr.

2) Let Xmva=?Y be a mva-type association rule. Then, its confidence, denoted by Conf(Xmva=?Y ), is defined as follows: Conf(Xmva=?Y ) = Supp(X ? Y )/Supp(X).

In an mva-type association rule, attributes are associated with values from a fixed finite set, whereas in a quantitative asso- ciation rule, attributes are associated with either categorical values (e.g. zip code, make of car) or intervals (e.g. age, income). During the process of discovering quantitative rules, the attribute values are then mapped to discrete values. On the other hand, our definition of database D (in particular, the set of values V) assumes that the attribute values are already mapped to discrete values. In this sense, our definition of mva- type association rules simplifies the definition of quantitative association rules given in [2].

The definitions of support and confidence in the market- basket type data can be viewed as a special case of Defini- tion 2. For instance, let A1, A2, and A3 be 0/1-valued (i.e., binary) attributes. Then, the measure ?support of {A1, A2}? in the market basket type data can be seen as equivalent to Supp({(A1, 1), (A2, 1)}) and the measure ?confidence of {A1, A2} =? {A3}? can be seen as equivalent to Conf({(A1, 1), (A2, 1)}mva=?{(A3, 1)}), given by Definition 2.

B. Association Hypergraphs  Directed hypergraphs are a generalization of directed graphs in which each directed hyperedge has one or more source (tail) vertices and has one or more destination (head) vertices. They have found a variety of applications in Computer Science, e.g., in databases [12], scheduling [13], bioinformatics [14], and data mining [15]. Directed hypergraphs are different from undirected hypergraphs as the former generalize directed graphs, whereas the latter generalize undirected graphs.

Definition 3: [16] A directed hypergraphH is a pair (V,E), where E ? 2V ? 2V such that, for every hyperedge e = (T, H) ? E, T ?= ?, H ?= ?, and T ? H = ?. (Here, T is called the tail set and H is called the head set of e.) Using the notion of directed hypergraphs, we define associa- tion hypergraphs that are tailored for database applications.

Definition 4: An association hypergraph H for a database D(A,O,V) is a directed hypergraph whose vertex set V is A  and hyperedge set E consists of directed hyperedges (T, H), where T and H are disjoint subsets of A. Each directed hyperedge e = (T, H) has an association confidence value in the range [0, 1], denoted ACV (e) or ACV (T, H), and an association table, denoted AT (e) or AT (T, H), that are defined as follows:  1) The ACV (e) of a directed hyperedge e = ({t1, . . . , tr}, {h1, . . . , hs}) equals  ?  v1,...,vr?V Supp({(t1, v1), . . . , (tr, vr)})  ? Conf({(t1, v1), . . . , (tr, vr)}mva=?H?), where H? = {(h1, v?1), . . . , (hs, v?s )} and v?1 , . . ., v?s depend on v1, . . ., vr such that they maximize the confidence of the mva-type association rule  {(t1, v1), . . . , (tr, vr)}mva=?{(h1, v?1), . . . , (hs, v?s)} over all choices of v?1, . . ., v  ? s ? V .

2) The association table of a directed hyperedge ({t1, t2, . . . , tr}, {h1, h2, . . . , hs}) has rows corresponding to the set of all possible values that t1, . . ., tr can take from V . The row corresponding to t1 = v1, . . ., tr = vr, where v1, . . ., vr ? V , is a list that contains  (a) Supp({(t1, v1), . . . , (tr, vr)}), (b) the values v?1 , . . ., v  ? s ? V defined in part (1)  above, and (c) the confidence of the mva-type association rule  {(t1, v1), . . . , (tr, vr)}mva=?{(h1, v?1), . . . , (hs, v?s )}.

In other words, every row corresponds to an mva-type association rule.

The motivation for representing a database D using a directed hypergraph is to capture a more general implication relation- ship between attributes than the one identified by mva-type association rules. For example, we now want to answer the following question: ?Regardless of the values the attributes in T take, what is the likeliness of predicting the values of attributes in H??. The above intuition helps us to model such a relationship between attributes in T and attributes in H using a directed hyperedge (T, H) and to capture the likeliness as the association confidence value ACV (T, H) of this directed hyperedge.

In this work, we consider only associations of the form (T, H), where T and H are disjoint subsets of attributes, |T | ? 2, and |H| ? 1. Having no constraint on |T | and |H| adds complexity. Henceforth, we use the term association hypergraph to refer to this restricted case, and call any directed hyperedge (T, H) in which |T | = 1 a directed edge and one in which |T | = 2 a 2-to-1 directed hyperedge.

Constructing Association Hypergraphs. The association hy- pergraph H for D(A,O,V) has node set A and the hyperedge set consists of directed hyperedges of the form (T, H), where T and H are disjoint subsets of A. We construct directed hy- peredges in the order of their head set. For a fixed combination     of two or fewer attributes, say {A1, A2}, and any other at- tribute, say A3, we determine whether ({A1, A2}, {A3}) could be included as a directed hyperedge of H by checking whether the combination is ?-significant according to Definition 5.

Definition 5: Consider a combination (T, H) for inclusion as a directed hyperedge of the association hypergraph H, where |T | ? 1. For ? ? 1, we say that (T, H) is ?-significant if ACV (T, H) ? ? ?maxv?T {ACV (T ? {v}, H)}.

We include the directed hyperedge (T, H) in H if and only if it is ?-significant, and set its weight to ACV (T, H). The association table for the directed hyperedge ({A1, A2}, {A3}) is constructed as follows. Supp({(A1, v1), (A2, v2)}) is com- puted by counting observations in the database for which A1?s value is v1 and A2?s value is v2. Conf({(A1, v1), (A2, v2)} mva=? {(A3, v?3)}) is then computed by counting such observa- tions in the database for which A3?s value is v?3 , where v  ? 3 is  the most frequent value for A3. This process is repeated for all possible values that A1 and A2 can take from V .



III. ASSOCIATION-BASED SIMILARITY  Han et al. [17] used undirected hypergraphs, where nodes represent binary attributes and hyperedges represent subsets of attributes, for identifying clusters of similar attributes. Ozdal et al. [18] also used undirected hypergraphs, where nodes rep- resent patterns and hyperedges represent relationships among patterns, and proposed a clustering approach. Lent et al. [19] used clustering to group association rules based on certain attribute conditions.

We next propose similarity notions that measure association- based similarity between any two nodes of the association hypergraph. We can imagine two nodes A and B to be in- similar if significantly many directed hyperedges that contain A in their headset lead to valid directed hyperedges that contain B in their headset when A is replaced by B. Here, A and B are in-similar since they both share similar incoming directed hyperedges. Likewise, A and B can be regarded as out-similar by relating to the tailset instead of the headset of directed hyperedges.

Notation 1: Let H = (V,E) be an association hypergraph.

1) For any A ? V , outH(A) denotes the set of all directed  hyperedges of H whose tail set contains A.

2) For any A ? V , inH(A) denotes the set of all directed  hyperedges of H whose head set contains A.

3) For any A1, A2 ? V and e = (T, H) ? outH(A1),  e|T:A1?A2 denotes the directed hyperedge (T ?, H ?) whose head set H ? is H and whose tail set T ? is formed from T by replacing node A1 by node A2 (i.e., H ? = H and T ? = (T ? {A1}) ? {A2}). For any set of directed hyperedges S and nodes A1 and A2, S|T:A1?A2 denotes the set  ? e?S{e|T:A1?A2}.

4) For any A1, A2 ? V and e = (T, H) ? inH(A1), e|H:A1?A2 denotes the directed hyperedge (T ?, H ?) whose tail set T ? is T and whose head set H ? is formed from H by replacing node A1 by node A2 (i.e., T ? = T and H ? = (H ?{A1})? {A2}). For any set of directed  hyperedges S and nodes A1 and A2, S|H:A1?A2 denotes the set  ? e?S{e|H:A1?A2}.

Notation 2: Let A1 and A2 be attributes and H = (V,E) be an association hypergraph. Let ? denote the empty directed hyperedge.

1) outH(A1)?outH(A2) is the set of directed hyperedge pairs (e, f) s.t. e ? outH(A1), f ? outH(A2), and e = f |T:A2?A1 . The definition of inH(A1)?inH(A2) is similar.

2) outH(A1)?outH(A2) is the union of the following sets of directed hyperedge pairs: (1) outH(A1)?outH(A2), (2) (e, ?) s.t. e ? outH(A1) and e ?= f |T:A2?A1 for each f ? outH(A2), and (3) (?, f) s.t. f ? outH(A2) and e|T:A1?A2 ?= f for each e ? outH(A1). The definition of inH(A1)?inH(A2) is similar.

The in-similarity of attributes A1 and A2, denoted by InSimH(A1, A2), is the weighted fraction of directed hyper- edges e ? inH(A1) ? inH(A2) such that switching A1 to A2 in in the head set of e results in another directed hyperedge; the weights are the association confidence values of directed hyperedges. Similarly, the out-similarity of attributes A1 and A2, denoted by OutSimH(A1, A2), is the weighted fraction of directed hyperedges e ? outH(A1) ? outH(A2) such that switching A1 to A2 in the tail set of e results in another directed hyperedge.

Definition 6: Let A1 and A2 be attributes and H = (V,E) be an association hypergraph. The following similarity notions are defined for A1 and A2:  1) OutSimH(A1, A2) =P (e,f)?outH(A1)?outH(A2) min{ACV (e), ACV (f)}P (e,f)?outH(A1)?outH(A2) max{ACV (e), ACV (f)}  .

2) InSimH(A1, A2) =P (e,f)?inH(A1)?inH(A2) min{ACV (e), ACV (f)}P (e,f)?inH(A1)?inH(A2) max{ACV (e), ACV (f)}  .

A. Clusters of Similar Attributes  We define below the notion of a similarity graph induced by any subset S of attributes that assigns every attribute pair {A1, A2} in S an undirected edge whose weight depends on the in-simialrity and the out-similarity of the pair.

Definition 7: Let H = (V,E) be an association hyper- graph. Given any collection S of attributes, a similarity graph SGS = (V ?, E?) induced by S in H is an undirected, weighted, complete graph whose node set V ? is S and edge set E? contains all attribute pairs in S such that, for every edge {A1, A2} ? E?, its weight d(A1, A2) is defined as 1? (InSimH(A1, A2) + OutSimH(A1, A2))/2.

Our objective is to determine a partition of S into subsets of attributes such that attributes within each subset are highly similar in their associative characteristics. For this, we con- struct the similarity graph SGS induced by S in H and apply the t-clustering algorithm by Gonzalez [20]. This algorithm finds such a partition of S by designating some t attributes as cluster centers. The algorithm takes the parameter t in the input and assigns each attribute to its closest cluster center.

This is a factor 2-approximation algorithm for minimizing the     diameter of the t-clustering, assuming that the distances (i.e., weights) satisfy the metric properties. (Here, the diameter of a t-clustering is the maximum distance between any two data points that are within the same cluster.)

IV. COMPUTATIONAL PROBLEMS  Having defined mva-type association rules, the next step is to devise a methodology for predicting values of an attribute given the values of other attributes. Such prediction rules are called classification rules. Liu et al. [21] and Bayardo [22] proposed various methods that use pruning of association rules to generate classification rules. Liu et al. [23] introduced multiple minimum supports during mining of association rules to identify potential classification rules. In this section, we use the directed hypergraph based model to first propose an algorithm for identifying leading indicators and then present the association-based classifier for predicting the values of attributes.

A. Leading Indicators  A leading indicator X for any set S of attributes is a subset of S such that knowing only the values for the attributes in X allows us to infer the value for all attributes in S ? X .

Motivated by the notion of a dominating set for any graph, we define below the notion of a dominator for a set of vertices of any association hypergraph. Our hypothesis is that a dominator for nodes corresponding to the set S of attributes in the association hypergraph modeling attribute relationships gives a leading indicator for S.

Definition 8: A dominator for a set S of vertices in an association hypergraph H = (V,E) is a set X ? V such that, for every u ? S?X , there is a directed hyperedge e = (T, H) such that T ? X and u ? H . That is, each node u ? S?X is covered using only directed hyperedges whose tail set is from X .

The algorithm computes a dominator for any set S of vertices in an association hypergraphH = (V,E). It is an adaptation of the greedy O(log n)-approximation algorithm for computing a minimum cardinality dominating set in graphs. In order to minimize the size of the dominator, the following greedy heuristic is applied: for every node u that is not part of the dominating set yet, the algorithm computes the node effectiveness ?(u) that reflects u?s covering ability. During each iteration of the algorithm (until all the nodes in the graph are covered), the node with the highest effectiveness value is added to the dominator set. The algorithm runs in time O(|S| ? |E|).

DomSet ? ?; CoveredSet ? ?; while CoveredSet ?= S do  foreach vertex u ? V ? DomSet do if u ?? CoveredSet and u ? S then ?(u) ? 1; else ?(u) ? 0; ?(u) ? ?(u) +  X  v ??CoveredSet?v?S L(u, v),  where L(u, v) ? maxe:u?T (e)?v?H(e) w(e)|T (e)?DomSet|; end-foreach Let u0 ? V be such that ?(u0) = max  u??DomSet ?(u);  DomSet ? DomSet ? {u0}; CoveredSet ? CoveredSet ? {u0} ? {v ? S | ?e ? E s.t. v ? H(e)  and T (e) ? DomSet}; end-while return DomSet;  B. Association-Based Classifier  Let S be the set of attributes A1, A2, . . . , At and let these attributes take values v1, v2, . . . , vt, respectively. Let T be another set of attributes, disjoint from S. The association- based classifier determines the values of all attributes in T given the values of attributes in S. For this problem, we will assume that S is a dominator for T in the association hypergraph H and so S can be computed using the algorithm presented in Section IV-A. This assumption stands on our earlier stated hypothesis that a dominator for any set of attributes is also a leading indicator for the set.

The algorithm described below takes an association hyper- graph H = (V,E) modeling attribute relationships, a set T of attributes, and a set S = {(A1, v1), (A2, v2), . . . , (At, vt)}, where A1, A2, . . . , At are attributes and v1, v2, . . . , vt ? V are their respective values, as input. It returns (a) an assignment of values that assigns each attribute Y ? T its best classified value y? and (b) the classification confidence val[y?] associ- ated with every such assignment y? to Y . Its running time is O(k2 ? |T | ? |E|).

foreach attribute Y ? T do  for y ? 1 to k do {val[y] ? 0}; foreach directed hyperedge e = (T, H) ? E with H = {Y } and  T ? {A1, A2, . . . , At} do Let T be {A1, A2} and let y be the most frequent  value of Y given ?A1 = v1? and ?A2 = v2?; val[y] ? val[y] + Supp({(A1, v1), (A2, v2)})  ?Conf({(A1, v1), (A2, v2)}mva=?{(Y, y)}); end Let y? ? {1, . . . , k} be such that val[y?] = maxy?{1,...,k} val[y]; val[y?] ? val[y?]/ Py?{1,...,k} val[y]; Output ?(Y, y?, val[y?])?;  end

V. EXPERIMENTATION  The Standard & Poor?s (S&P) 500 is a market-value-weighted index of the stock prices of some 500 large publicly held com- panies. We used Yahoo Finance to obtain stock information for all financial time-series in S&P 500. In this section, all analysis is based on the daily closing stock price information from Jan 1, 1995 to Dec 21, 2009. We had to restrict start date to Jan 1, 1995 since a number of financial time-series in the current S&P 500 started trading only from the mid 90s. As a result of this clean up of our data, the number of financial time-series in our analysis is 346. These time-series belong to the following industrial sectors: Basic Materials (BM), Capital Goods (CG), Conglomerates (C), Consumer Cyclical (CC), Consumer Noncyclical (CN ), Energy (E), Financial (F), Healthcare (H), Services (SV), Technology (T ), Transporta- tion (T P), and Utilities (U).

In the financial time-series data set, every observation corresponds to a reading taken at a particular time. For each time-series (stock) in the data set, we create a delta time-series, which is a list of real numbers whose i?th entry is the fractional change in the closing price of the (i+1)?th day relative to the closing price of the i?th day. We use equi-depth [2] partitioning     to discretize the delta time-series into a set of values. Time- series are attributes and each observation is a list of discretized values assigned to the time-series on a particular day.

For the construction of the association hypergraph H, we try two configurations of parameters as follows: configuration C1 sets k to 3, ? (in Definition 5) for directed edges (say ?1?1) to 1.15, and for 2-to-1 directed hyperedges (say ?2?1) to 1.05 and configuration C2 sets k to 5, ?1?1 for directed edges to 1.20, and ?2?1 for directed hyperedges to 1.12. The choice for these values in both C1 and C2 are based on the following reasoning: (a) these values of ?2?1 lead to higher ACV values of 2-to-1 directed hyperedges when compared to the constituent directed edges and a similar remark can be made for the values of ?1?1, (b) these are the stable values of ?1?1 and ?2?1; that is, slight perturbations to these values do not result in significant changes to the numbers of directed edges and 2-to-1 directed hyperedges in H, and (c) the values in C1 and the values in C2 result in comparable numbers of directed edges and 2-to-1 directed hyperedges in H. The configuration C1 leads to 106, 475 directed edges with a mean ACV of 0.436 and 157, 412 2-to-1 directed hyperedges with a mean ACV of 0.437 and the configuration C2 leads to 109, 810 directed edges with a mean ACV of 0.288 and 274, 048 2-to-1 directed hyperedges with a mean ACV of 0.288.

A. Association Characteristics of Financial Time-Series  Table I shows, for each configuration, the directed edge and the 2-to-1 directed hyperedge with the highest ACV for each selected financial time-series from different sectors.

For example, the best prediction directed edge for GT (The Goodyear Tire & Rubber Company) shown in Row 3 for C1 and C2 represents a relationship between GT and PPG (PPG Industries, Inc.). This relationship may be interpreted in terms of GT procuring raw materials (e.g., precipitated silicas) from PPG for the manufacturing or processing of rubber. A more interesting many-to-one relationship is represented by the best prediction 2-to-1 directed hyperedge for GT shown in Row 3 for C1. Here, the 2-to-1 directed hyperedge for GT represents a relationship of GT with DOW (The Dow Chemical Company) and F (Ford Motor Company). This relationship may be interpreted in terms of GT procuring raw materials (e.g., polyurethane polymer) from DOW, whereas the relationship with F may be attributed towards F utilizing the products (e.g., tires) from GT. Thus, we see that 2-to-1 directed hyperedges provide meaningful and more interesting information than directed edges do.

Table II shows, for each configuration, the 2-to-1 directed hyperedge with the highest ACV and the constituent directed edges for each selected financial time-series from different sec- tors. For example, in Row 5, for C1 the accuracy of HES (Hess Corporation) predicting XOM (Exxon Mobil Corporation) is 0.55 and SLB (Schlumberger Limited) predicting XOM is 0.54, but both of them together predicting XOM is 0.58. This shows that the combination of two financial time-series leads to a better predicting 2-to-1 directed hyperedge.

B. Association-Based Similarity  Figure 1 compares in-similarity and out-similarity values with Euclidean similarity for configuration C1. A similar figure for configuration C2 is omitted due to lack of space.

Here, the Euclidean similarity between any two financial time-series A and B is computed as follows. Let ?(A) = (a1, a2, . . . , an) and ?(B) = (b1, b2, . . . , bn), where ai and bi are the values that A and B take in the i?th observa- tion. The Euclidean distance between A and B is defined as ED(A, B) = ||normalized(?(A)) ? normalized(?(B))||, where for any vector V = (v1, v2, . . . , vn), normalized(V ) = (v1/||V ||, v2/||V ||, . . . , vn/||V ||) and ||V || = (  ?n i=1 v  i )  2 .

Now, the Euclidean similarity ES(A, B) between A and B is defined as ES(A, B) = 1? 12?ED(A, B). Note that ES(A, B) is a real value in the range [0, 1] such that a higher value indicates a greater similarity. Figure 1 shows that Euclidean similarity does not differentiate between time-series pairs as distinctly as our similarity measures do. This could be due to the fact that Euclidean similarity accounts for pair-wise differences in price variations on a day-to-day basis whereas the similarity measures account for the closeness in being associated with common sets of time-series on an average basis.

Figure 2(a) shows a clustering of financial time-series for configuration C1 obtained using approach in Section III-A. A similar figure for configuration C2 is omitted due to lack of space. The collection S is the set of all time-series in our data set. The value of parameter t in the t-clustering algorithm is set to 104, which is the total number of sub-sectors over the entire sectors (as pointed out at the beginning of Section V). The first cluster center is picked from the sector T as this sector has the maximum number of time-series in our data set. We experimentally verified that the weight function in Definition 7 satisfies the triangle inequality property, and hence the factor 2-approximation of the optimal diameter of the t-clustering is in fact achieved by the algorithm. For clarity of display, Figure 2(a) shows clusters of size greater than 6. The edges that connect all cluster centers to other nodes in their clusters and the edges that interconnect the cluster centers are also shown in the figure. This partial similarity graph consists of 256 nodes and 298 edges. To show the quality of the clustering obtained, the following information is relevant: (i) the mean diameter over all clusters obtained is 0.83 and the overall mean distance in SGS is 0.89 and (ii) the largest cluster of size 29 contains all time-series from the sector T .

C. Leading Indicators of Financial Time-Series  We find the leading indicators for a collection S of financial time-series using the approach in Section IV-A. In order to obtain a dominator set that covers the rest of the time-series via directed edges and 2-to-1 directed hyperedges of high ACV , we set a threshold for ACV (ACV -threshold) and discard all directed edges and 2-to-1 directed hyperedges below this threshold during the computation of the dominator. For the computation of dominators for configurations C1 and C2, we consider the following choices for thresholds: (i) top 40%     Row Time-series Configuration Top directed edge Top 2-to-1 directed hyperedge 1 EMN (BM) C1 PPG (BM) ? EMN (BM) AVY (BM), GT (CC) ? EMN (BM)  C2 PPG (BM) ? EMN (BM) BLL (BM), IFF (BM) ? EMN (BM) 2 HON (CG) C1 TXT (C) ? HON (CG) CAT (CG), ITT (T ) ? HON (CG)  C2 UTX (CG) ? HON (CG) BA (CG), ROK (T ) ? HON (CG) 3 GT (CC) C1 PPG (BM) ? GT (CC) DOW (BM), F (CC) ? GT (CC)  C2 PPG (BM) ? GT (CC) ETN (T ), FMC (BM) ? GT (CC) 4 PG (CN ) C1 CL(CN ) ? PG (CN ) CLX(CN ), K (CN ) ? PG (CN )  C2 CL(CN ) ? PG (CN ) ABT(H), CPB (CN ) ? PG (CN ) 5 XOM (E) C1 CVX (E) ? XOM (E) HES (E), SLB (E) ? XOM (E)  C2 CVX (E) ? XOM (E) COG (E), PEG (U ) ? XOM (E) 6 AIG (F ) C1 C (F ) ? AIG (F ) BEN(F ), PGR (F ) ? AIG (F )  C2 C (F ) ? AIG (F ) AON (F ), CI (F ) ? AIG (F ) 7 JNJ (H) C1 MRK (H) ? JNJ (H) IFF (BM), SYY (SV) ? JNJ (H)  C2 MRK (H) ? JNJ (H) CL (CN ), PEP (CN ) ? JNJ (H) 8 INTC T ) C1 LLTC (T ) ? INTC (T ) EMC (T ), QCOM (T ) ? INTC (T )  C2 XLNX (T ) ? INTC (T ) CTXS (T ), QCOM (T ) ? INTC (T )  TABLE I THE DIRECTED EDGE AND THE 2-TO-1 DIRECTED HYPEREDGE WITH THE HIGHEST ACV FOR EACH SELECTED FINANCIAL TIME-SERIES FROM  DIFFERENT SECTORS (IN PARENTHESIS) AND FOR EACH CONFIGURATION CHOICE ARE SHOWN.

Row Time-series Configuration Top 2-to-1 directed hyperedge Directed edge 1 Directed edge 2 1 EMN (BM) C1 AVY, GT ? EMN (0.52) AVY ? EMN (0.49) GT ? EMN (0.49)  C2 BLL, IFF ? EMN (0.37) BLL ? EMN (0.32) IFF ? EMN (0.33) 2 HON (CG) C1 CAT, ITT ? HON (0.53) CAT ? HON (0.5) ITT ? HON (0.49)  C2 BA, ROK ? HON (0.38) BA ? HON (0.33) ROK ? HON (0.33) 3 GT (CC) C1 DOW, F ? GT (0.51) DOW ? GT (0.48) F ? GT (0.47)  C2 ETN, FMC ? GT (0.37) ETN ? GT (0.33) FMC ? GT (0.33) 4 PG (CN ) C1 CLX, K ? PG (0.53) CLX ? PG (0.5) K ? PG (0.49)  C2 ABT, CPB ? (0.36) ABT ? PG (0.32) CPB ? PG (0.32) 5 XOM (E) C1 HES, SLB ? XOM (0.58) HES ? XOM (0.55) SLB ? XOM (0.54)  C2 COG, PEG ? XOM (0.37) COG ? XOM (0.33) PEG ? XOM (0.31) 6 AIG (F ) C1 BEN, PGR ? AIG (0.54) BEN ? AIG (0.51) PGR ? AIG (0.51)  C2 AON, CI ? AIG (0.37) AON ? AIG (0.33) CI ? AIG (0.33) 7 JNJ (H) C1 IFF, SYY ? JNJ (0.48) IFF ? JNJ (0.45) SYY ? JNJ (0.45)  C2 CL, PEP ? JNJ (0.36) CL ? JNJ (0.32) PEP ? JNJ (0.31) 8 INTC T ) C1 EMC, QCOM ? INTC (0.55) EMC ? INTC (0.52) QCOM ? INTC (0.52)  C2 CTXS, QCOM ? INTC (0.4) CTXS ? INTC (0.35) QCOM ? INTC (0.35)  TABLE II THE 2-TO-1 DIRECTED HYPEREDGE WITH THE HIGHEST ACV AND THE CONSTITUENT DIRECTED EDGES FOR EACH SELECTED FINANCIAL TIME-SERIES  FROM DIFFERENT SECTORS AND FOR EACH CONFIGURATION CHOICE ARE SHOWN.

Row Configuration ACV- threshold  Dominator Size  Percent Covered  Mean Classification Confidence  In-sample Out-sample (top % hyper- edges)  Association- Based Classifier  Association- Based Classifier  SVM Multilayer Perceptron  Logistic Regression  1 C1 0.45 (40%) 14 98.6 0.648 0.725 0.537 0.723 0.534 0.46 (30%) 16 96 0.650 0.721 0.524 0.717 0.519 0.47 (20%) 22 94 0.652 0.726 0.510 0.719 0.506  2 C2 0.32 (40%) 20 96 0.647 0.718 0.433 0.635 0.262 0.33 (30%) 30 94 0.648 0.716 0.415 0.631 0.218 0.34 (20%) 32 88 0.650 0.717 0.413 0.630 0.215  TABLE III THE SIZE OF A DOMINATOR FOR ALL FINANCIAL TIME-SERIES AND THE MEAN CLASSIFICATION CONFIDENCE OF DIFFERENT CLASSIFIERS FOR EACH  CONFIGURATION ARE SHOWN.

(a) (b)  Fig. 1. (a) In-similarity (for configuration C1) vs Euclidean similarity and (b) Out-similarity (for configuration C1) vs Euclidean similarity.

The in-similarity (out-similarity) values are on the x-axis and the corresponding Euclidean similarity values are on the y-axis.

directed hyperedges w.r.t. ACV s?this sets ACV -threshold to 0.45 for C1 and sets ACV -threshold to 0.32 for C2, (ii) top 30% directed hyperedges w.r.t. ACV s?this sets ACV - threshold to 0.46 for C1 and sets ACV -threshold to 0.33 for C2, and (iii) top 20% directed hyperedges w.r.t. ACV s?this sets ACV -threshold to 0.47 for C1 and sets ACV -threshold to 0.34 for C2. Table III shows the size of dominator for almost all the time-series in our data set. Here, in row 1, for the case when ACV -threshold is set to 0.45, our algorithm (Section IV-A) finds a dominator of size 14 that covers 98.6% of all time-series in our data set.

D. Association-Based Classifier  We evaluate the accuracy of the assignments given by the asociation-based classifier on several test data sets. For each training data set, we construct an association hypergraph H = (V,E) using the procedure described in Section II- B. Next, in the corresponding test data set, all the financial time-series are converted to their respective delta time-series and then discretized. We choose a small collection S of financial time-series, usually a dominator for all financial time-series in our data sets. The values of every financial time-series A in the dominator are already known from the discretized representation of A. The best predicted value of all other financial time-series in our data sets is computed using the association-based Classifier presented in Section IV- B. We define the classification confidence for any financial time-series Y on a particular test data set as the fraction of days on which the value assigned by our classifier matches the value in Y ?s discretized representation, obtained from the same data set. We also compute the accuracy of value assignments given by other data mining classifiers such as the support vector machine (SVM), multilayer perceptron, and logistic regression. For experiments here, the classifiers provided by Weka [24] are used. The following methodology is used to predict the values of any financial time-series Y by constructing a training data set whose feature set is the set S of financial time-series: Consider a directed hyperedge  e in H such that e = ({A1, A2}, {Y }) and A1, A2 ? S. The training data set is built by using each row in AT (e) as a data point. Here, the particular value assignment A1 = v1 and A2 = v2 is the feature value, and the corresponding value y?  of Y (defined in Definition 4(1)) is the class value.

Table III lists, for each dominator, the mean classification confidence over all financial time-series for configurations C1 and C2. Here, in-sample indicates the training data set for constructing the directed hypergraph model and out-sample indicates the test data set for evaluating the value assignments made by the classifiers. The in-sample contains financial time- series data from Jan 1, 1996 to Dec 31, 2008 and the out- sample contains financial time-series data from Jan 1, 2009 to Dec 31, 2009. From this table, it is clear that the association- based Classifier outperforms SVM, Logistic Regression, and Multilayer Perceptron for both configurations C1 and C2.

Also, its mean classification confidence is consistent regardless of k, whereas the mean classification confidence of other classifiers decreases as k increases.

Figure 2(b) shows the distribution of the mean classification confidence over various in-sample and out-sample data for configuration C1 where the dominator having ACV -threshold of 0.45 is chosen to be S. In this figure, the mean classification confidence for each in-sample data has been computed by increasing the training data set incrementally one year at a time, starting from Jan 1, 1996 and ending at Dec 31, 2008.

The corresponding out-sample contains financial time-series data for one year immediately following the last day in the training data set. For instance, if the training data set is from Jan 1, 1996 to Dec 31, 2001, then the corresponding test data set contains financial time-series from Jan 1, 2002 to Dec 31, 2002. From this figure, it is evident that the association- based classifier achieves mean classification confidence in the range 0.60 to 0.75 on both in-sample and out-sample data.

The higher classification confidence values for out-sample data compared to in-sample data may be attributed to the fact that the out-sample data is substantially smaller than the in-sample data.

(a) (b)  Fig. 2. (a) Clusters of financial time-series for configuration C1. Note that this figure is made up of multiple colors and the colors correspond to sectors in financial domain. The big circles represent cluster centers and the small ones represent other nodes. The size of the big circles is directly proportional to the number of nodes assigned to them. The small circles are attached to their respective cluster centers, and (b) Classification confidence distribution of the association-based Classifier for in-sample and out-sample data for configuration C1. The start year for training data set is 1996.



VI. CONCLUSION  We proposed a directed hypergraph based model to capture attribute-level associations and their strength in any database.

We tested this model on a financial time-series data set (S&P 500). The clustering method based on our similarity notions allowed to find clusters of financial time-series. The association-based classifier coupled with the leading indicator of all the financial time-series exhibited a methodology to use mva-type association rules and predict values of financial time- series. We also demonstrated the consistency of our model by varying k throughout our experiments.

Our work raises interesting questions on the applications of association rule mining. It might be fruitful to explore associations by applying the directed hypergraph model on data sets such as gene databases and medical databases. It would be useful to understand how the different parameters (k, ?, and the sizes of head and tail sets) affect the model.

