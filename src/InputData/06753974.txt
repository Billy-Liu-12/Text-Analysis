Accelerating Frequent Itemsets Mining on the Cloud: A MapReduce -Based  Approach

Abstract? Frequent pattern mining has a critical role in mining associations, sequential patterns, correlations, causality, episodes, multidimensional patterns, emerging patterns, and many other significant data mining tasks. With the exponential growth of available data, most of the traditional frequent pattern mining algorithms become ineffective due to either huge resource requirements or large communications overhead. Cloud computing has proved that processing very large datasets over commodity clusters can be performed by providing the right programming model. As a parallel programming model, MapReduce, one of most important techniques for cloud computing, has emerged in the mining of datasets of terabyte scale or larger on clusters of computers. Converting a serial mining algorithm into a distributed algorithm on the MapReduce framework is not necessarily difficult, but the mining performance can be unsatisfactory. In this paper, we propose a method which finds all frequent itemsets by using just two MapReduce phases in a time and communication efficient manner. We demonstrate experimental results to corroborate our theoretical claims.

Keywords- Big Data Mining; Frequent Itemset Mining; Cloud Computing; MapReduce

I.  INTRODUCTION  Data Mining is a process of finding new, interesting,  previously unknown, potentially useful, and ultimately understandable patterns from very large volumes of data [1].

The regularities or exceptions discovered from databases through data mining have enabled human decision makers to better make better decisions in many different areas [2].

One critical topic in data mining research is concerned with the discovery of frequent itemsets in a wide range of applications including market basket analysis, web click mining, traffic signals analysis, ATM transactions analysis, etc. To further motivate readers, we cite a special application case from market basket analysis [3].

Suppose, as manager of an AllElectronics branch, you would like to learn more about the buying habits of your customers. You can then use the results to plan marketing or advertising strategies, or in the design of a new catalog. For instance, if customers who purchase computers also tend to buy antivirus software at the same time, then placing the hardware display close to the software display may help increase the sales of both items. In an alternative strategy, placing hardware and software at opposite ends of the store  may entice customers who purchase such items to pick up other items along the way. Market basket analysis can also help retailers plan which items to put on sale at reduced prices. If customers tend to purchase computers and printers together, then having a sale on printers may encourage the sale of printers as well as computers. Therefore, in one strategy, items that are frequently purchased together can be placed in proximity to further encourage the combined sale of such items.

Frequent pattern mining has a critical role in mining associations, sequential patterns, correlations, causality, episodes, multidimensional patterns, emerging patterns, and many other significant data mining tasks. Apriori is the most classic and most widely used algorithm for mining frequent itemsets in a transactional database, proposed by R.

Agrawal and R. Srikant in 1994 [4]. The algorithm works within a multiple-pass generation-and-test framework, consisting the joining and the pruning phases to reduce the number of candidates before scanning the database for support counting. The algorithm terminates when no more candidate itemsets can be created.

With the advent of Internet and the exponential growth of data volume towards a terabyte or more, it has been more difficult to mine them on a single sequential machine. Researchers attempt to parallelize these frequent itemset mining algorithms to speed up the mining of the ever-increasing sized databases [5, 6, 7, 8, 9, 10, 11, 12 and 13].

While the parallelization may enhance the mining performance, it also produces several issues for solution including load balancing, jobs assignment and monitoring, data partition and distribution, parameters passing between nodes, etc. To overcome this problem, the MapReduce framework [14], one of most important techniques for cloud computing, has been introduced. The novelty of the cloud computing is that it supplies unlimited cheap storage and computing power. Therefore, cloud computing provides a platform for the storage and mining of mass data.

MapReduce is a programming model reintroduced by Google in 2004 to support distributed computing on large datasets in a massively parallel manner on clusters of computers [14]. MapReduce hides the problems like parallelization, fault tolerance, data distribution and load balancing, and allows user to focus on the problem without worrying about parallelization details. For this purpose,   DOI 10.1109/ICDMW.2013.106    DOI 10.1109/ICDMW.2013.106     First, MapReduce splits data into equal sized blocks with some replicas and distributes them to the distributed file systems automatically so that users are free from the locations and the distributions of data. Second, MapReduce re-performs a crashed task and acquires good fault- tolerances. Third, MapReduce enhances total throughputs by re-assigning un-finished tasks of slower nodes to idle nodes in a heterogeneous cluster. In a MapReduce cluster, one node acts as the master, who schedules tasks for execution among nodes, and other nodes are the workers. The master and the workers can be placed at the same node.

Computations in the MapReduce framework are done by the two main functions, map and reduce. The master schedules the map tasks and the reduce tasks when a job is initialized.

The map function in each node takes the input data as a <key, value> pair and outputs a list of <key, value> pairs in a different domain. The reduce function takes the output of the map function as <key, list-of-values> and outputs a collection of values. Both functions can be performed in parallel. Developers write the map function that reads one block from the file system and produces a set of intermediate key/value pairs. The MapReduce library organizes together all intermediate values related to the same intermediate key and sends them to the reduce function [14]. The Reduce function, also written by the user, obtains an intermediate key and a set of values for that key.

The Reduce function merges together these values to form a smaller set of values. This merging allows us to handle lists of values that are too large to fit in memory. Thus, MapReduce can be an efficient platform for mining frequent itemsets from huge datasets of tera- or peta-bytes.

Hadoop is an open source implementation of Google MapReduce architecture, sponsored by the Apache Software Foundation. The architecture is built on Hadoop Distributed File Systems (HDFS), for efficiently writing applications which process huge amount of data in-parallel on large clusters of commodity hardware in a reliable and fault- tolerance manner [15]. The era of Hadoop means that the sequential computing algorithms need to be redesigned to MapReduce algorithms.

Converting a serial Apriori-like mining algorithm into a distributed algorithm on MapReduce framework might not be difficult, but the mining performance can be unsatisfactory. To redesign a serial Apriori-like mining algorithm into the MapReduce framework, the multiple-pass feature of the Apriori algorithm demands multiple MapReduce phases. The master node must schedule jobs to initialize each MapReduce phase. The map function of a MapReduce phase can start after all the reduce functions of its previous phase complete. Nodes that finishes their reduce functions must wait for all the unfinished nodes to done.

The waiting and the scheduling are pure overheads to the core mining task [16]. Thus, the challenge lies in proposing an algorithm which decreases the number of phases in a communication efficient and time efficient manner.

To achieve these objectives, we propose a method which finds all frequent itemsets by using just two MapReduce phases that is more efficient than the previous  works in the terms of the execution time and the communication.

The rest of the paper is organized as follows. Section 2 briefly describes the related work in mining frequent itemsets on MapReduce framework. Section 3 introduces the problem definition. In Section 4, the proposed algorithm is presented. Section 5 describes the experimental results.

Section 6 concludes the paper.



II. RELATED  WORKS Frequent itemset mining task is to find all subsets of  items which frequently occur. Apriori algorithm [4] is the most widely used algorithm for frequent itemsets mining.

The algorithm uses a multiple-pass generation-and-test method to generate the candidtate (k + l)-itemsets from the frequent k-itemsets. As we mentioned in the section I, the sequential Apriori algorithm performance is inefficient, especially when the data sets volume grows towards a terabyte or peta-bytes of data. Therefore, parallel Apriori algorithms were proposed [5, 6, 7, 8, 9, 10, 11 and 12].

However, parallel mining reveals new problems that did not exist in sequential computing, such as workload balancing, data partitioning and distribution, jobs assignment, and parameters passing between nodes [16].

Thus, significant time and effort are required to solve these problems. Cloud computing uses the MapReduce [14] programming model. Because of the benefits of the MapReduce model, some Apriori based algorithms [16, 17, 18, 19, 20, 21] and some FP-growth based algorithms [22, 23] using Hadoop-MapReduce model were implemented.

Mahout has a Parallel FPGrowth Implementation based on PFP algorithm [22]. In a single MapReduce phase the transactions are used to generate group-dependent transactions. Each group is assigned to a Reducer and the corresponding transactions are sent to this Reducer which then builds the local FP-tree and the conditional FP-trees, computing the frequent patterns. The PFP algorithm has a data replication problem that result a slower communication phase, which is usually the most expensive in a MapReduce algorithm. The PFP algorithm also does not consider load balance, which is significant when the dataset is extremely huge. A solution to this load balancing problem is suggested in [23], by computing the groups outside the MapReduce environment with no guarantees.

The algorithms in [16, 19, 20] require k MapReduce phases to find all frequent itemsets (k is maximum length of frequent itemsets): phase one to find frequent 1-itemset, phase two to find frequent 2-itemset, and so on. Therefore, the waiting and the scheduling are pure overheads to the mining task. The algorithm in [18] requires only one MapReduce phase to find all frequent k-itemsets. But the amount of data generated in the map phase grows exponentially with the length of the transactions in the dataset. Therefore, its performance is inefficient. MRApriori algorithm in [21] only needs two MapReduce phases to find all frequent itemsets. In the phase one, each Mapper takes the whole split as an input, and run the traditional Apriori algorithm on that split. The map?s output is a list of intermediate key/value pairs; where the key is an element of     partial frequent itemsets and the value is its partial count.

When all map tasks are finished, the reduce task is started.

The output of reduce function is a list of key/value pairs, where the key is an element of partial frequent itemsets and the value equal one. In phase two, one extra input is added to the data of the previous phase, which is a file that contains all partial frequent itemsets. The map function of this phase counts occurrence of each element of partial frequent itemset in the split and outputs a list of key/value pairs, where the key is an element of partial frequent itemset and the value is the total occurrence of this key in the split.

The reduce function outputs a list of key/value pairs, where the key is an element of global frequent itemsets and the value is its occurrence in the whole data set. Although, MRApriori algorithm outperforms the other algorithms but because of huge number of partial frequent itemsets, the execution time in each node and communication between Mappers and Reducers are very high. In [17], an efficient pruning technique, to enhance the performance of MRApriori algorithm, has been proposed. But, as we observed in our experiments, the number of partial frequent itemsets in Phase II is still problematic that causes high execution time of Mappers and high communication volume across the network.

In this paper, we propose an efficient technique to decrease the number of partial frequent itemsets in Phase II.

Subsequently, we compare our technique with the IMRApriori algorithm. We concluded that the proposed algorithm outperforms IMRApriori algorithm.



III. PROBLEM DEFINITION The problem of mining of frequent itemsets in the cloud  environment is defined as follows: Let DB = ???? ???? ??? ? be a sequence of transactions, where each transaction ??? DB, j ? ???????? is a subset of I. I= {??, ??, ?,??} is a set of n distinct literals, called items which are units of information in an application domain. An item set X is a subset of items. Assume that there are M sites  Msss ,...,, 21 in a cloud environment and the database DB is partitioned automatically into M equal sized blocks {  },,..., 21 MDBDBDB respectively. Let the size of the split iDB be ,iD  for Mi ,...,2,1= . An item set X in DB is called a globally frequent item set if

X.SupportCount Ds ??                                                  (1)   where ? is a user defined minimum support threshold in the range of [0, 1] and X.supportCount is called the global support count of X. Therefore, given the dataset DB distributed in M sites and the user-specified minimum support threshold s in the range of [0, 1], our objective is to develop an efficient algorithm that discovers globally frequent itemsets using MapReduce framework. The challenge is to decrease execution time and communication load.



IV. MAPREDUCE BASED FREQUENT ITEMSET MINING MapReduce based algorithms must reduce the number of  MapReduce phases in a time and communication efficient manner. As we mentioned in the previous paragraph, the MRApriori algorithm [21] only needs two MapReduce phases to find all frequent itemsets; but, the huge number of partial frequent itemsets in Phase II may overload nodes of map functions. IMRApriori [17] algorithm offers better performance in relative to MRApriori algorithm [21] by pruning some partial frequent itemsets. We briefly describe the IMRApriori [17] algorithm in the following.

A. IMRApriori algorithm The task of map function, in Phase I of IMRApriori, is  employing Apriori algorithm on the whole split as an input and finding a set of partial frequent itemsets. Therefore, the Mapper?s output is a list of intermediate key/value pairs, defined as <itemset, supportCount>, where itemset is an element of partial frequent itemsets and the supportCount is its partial count. When all the map tasks are finished, the reduce task is started.

The reduce task adds up the supportCount of the same partial frequent itemsets and gets the partial support count of them. Reducer also computes for each partial frequent itemset ? how many Mappers output it. Based on Property1 in [17] maximum support count for an infrequent itemset in the split of ,iDB with size ?? , would be equal with ?? ? ??? ? ?; where s is minimum support threshold. Then, the reduce task estimates a maximum global supportCount for each partial frequent itemset X as follows.

?? ??? ?!??"#$%?#?&!''$()*$!+)  ,?? &!''$()*$!+) - ./?& ? ?0? ? ?1 ? ?2 ? 34?56?????????????????????????????????????????????????????????????  where 78  is the number of Mappers outputting X as a frequent itemset and 9 is the total number of Mappers. If;   ?? ??? ?!??"#$%?#?&!''$()*$!+) : & ? ??????????????????????????????;?   therefore, partial frequent itemset X may be a global frequent itemset and is inserted in output list, denoted by L  partial . Otherwise, itemset X is not a global frequent itemset  and not considered in Phase II.

In Phase II, a file that includes all itemsets in L  partial is added  to the input data of the previous phase. The map function of Phase II counts occurrence of each element of L  partial in the  split and Reducer adds up the support count of the same itemsets, obtains the actual support count of them in the whole data set and determine the set of global frequent itemsets.

Therefore, in the IMRApriori algorithm, Reducers find some global infrequent itemsets in Phase I and remove them from partial frequent itemsets list. But, as we observed in our experiments, the number of partial frequent itemsets in Phase II is still problematic that causes high execution time     of Mappers and high communication volume across the network.

We extend IMRApriori algorithm to mine frequent itemsets in a time efficient and communication efficient manner.

B. Accelerating the Frequent Itemsets Mining Process We present a framework of mining frequent itemsets  based on MapReduce as shown in Fig. 1. The original database DB is the input of Phase I and Phase II, which is split by MapReduce library and sent to M nodes executing the map tasks.

In the Phase I, the task of map function, like IMRApriori algorithm, is to employ the Apriori algorithm on the entire split as an input and finding a set of partial frequent itemsets with minimum support s.

Each Mapper has an identification number corresponding with the split number. For example, 9<==>?? corresponds with ?@?.

9<==>?? ?s output is a list of intermediate key/value pairs, defined as <itemset, ?A==B?CDBAEC?>, where itemset is an element of partial frequent itemsets and the ?A==B?CDBAEC?  is its partial count in iDB . When all the map tasks are finished, the reduce task is started.

The reduce task adds up the supportCount of the same partial frequent itemsets and gets the partial support count of them in some splits. Reducers recognize some global infrequent itemsets by using (2) and (3) and remove them from partial frequent itemsets.

As we mentioned before in this paragraph, in our proposed method, each Reducer receives input data from Mappers in the format of <itemset, ?A==B?CDBAEC?>. So, Reducer knows which Mappers have sent the count of itemset X in their split. Therefore, there is no need that itemset X to be counted again in those splits in Phase II. In Phase II, the itemset X has to be counted in the splits in which itemset X has not been frequent.

For example, Let DB be partitioned in three blocks ?@?, ?@? and ?@F  and each block be assigned to a Mapper. If itemset ?abc? has been sent by Mapper 1 and Mapper 3 to Reducer, then Reducer has the total count ?abc? in the split 1 and split 3 and needs ?abc? to be counted just in the split 2.

For this purpose, each Reducer outputs partial frequent itemsets generated by Phase I into M different partitions, L  partial = { GHIJK?ILM??GHIJK?ILN? ? ? GHIJK?ILO . GHIJK?ILM  includes  itemsets which have to be counted in ?@?, GHIJK?ILN? includes itemsets which have to be counted in ?@? , and so on.

Therefore, each partition including some itemsets will be assigned to a specified map function in the Phase II to be counted in the split.

Fig. 2 illustrates the Phase I of the proposed algorithm.

Figure 1.  A framework of mining frequent itemsets based on MapReduce   Map task: Input:? iDB ? Output:? PQRS?? T?#!RU? '? (&?? VRW +RV? ?&? P )R?&R)??  ?A==B?CDBAEC? U?? XYR(R?  )R?&R)?  &? ?+? R#R?R+)? $W? '?() ?#?W(RZ!R+)?  )R?&R)&??+V? )YR?&!''$()*$!+)?  &?  )&?'?() ?#? [$!+)?  +? ?@?? Method Begin  1. GH?\?]!+^'( $( ^#"$( )Y??_?@???``?a?EbA==?\?? ????c?XYR(R???? &?& dR?$W??@?? 2. Foreach? )R?&R)?X? +?GH?do? 3. ??e!)'!)?_?? ?? ?A==B?CDBAEC?fc? 4. End Foreach End  Reduce task: Input: PgRS? T?#!RU? '? (&?? XYR(R? gRS?  &? ?+? R#R?R+)? $W?  '?() ?#?W(RZ!R+)? )R?&R)&??+V?T?#!R? &? )&?$[[!((R+[R? +???&'# )?? Output: h i>j? k<lA> m??'? (&??XYR(R?gRS?  &??+?R#R?R+)?$W?"#$%?#?[?+V V?)R? )R?&R)&??T?#!R? &? )&?)$)?#?[$!+)? +?&$?R?&'# )?  ?+V?i? &?2?''R(?n?? Method Begin  ?? Foreach?gRS?X do? ?? ??Foreach?T?#!R?k??do?;? ????78?o\?c? ``? XYR(R??78? &? )YR? +!?%R(? $W? Mappers $!)'!)) +"?X ?&???W(RZ!R+)? )R?&R)c? + ) ?##S?78\p?q? ?????? ?A==B?CDBAEC o\ k? c? ``?  + ) ?##S??? ?A==B?CDBAEC\p? r? ???st u\?c?``?XYR(R?st2u? &??+??((?S?X )Y?#R+")Y?$W?2? v? ??End Foreach w? If ?78?\\2 x?   e!)'!)?_?? ?? ?A==B?CDBAECfc?``?collected in GyLz{IL |? Else If /?? ?A==B?CDBAEC - /??? ? ??? ? ?? ?  ?9 ? ?78?1 : ? ? ?1c? ``?XYR(R? M?  &? )YR? )$)?#?+!?%R(? $W?Mappers??     ?p? ????Foreach?  do // XYR(R? \p????????2? ??? ??????If?st u\p? ??? ???????e!)'!)?h ?? ?? ?A==B?CDBAEC? m?c? ``? [$##R[)RV?  +?GHIJK?IL}??;? ?????End If? ?q? ???End Foreach ?r? ?End If ?v? End Foreach End  Figure 2.  Phase I of ourAlgorithm  As shown in Fig. 1, in Phase II, for each Mapper i, a file that includes all itemsets in GHIJK?IL}  is added to the input data of the previous Phase. The 9<==>??  counts occurrence of each element of GHIJK?IL}  in the split and outputs a list of key/value pairs, where the key is an element of GHIJK?IL}  and the value is the count of this key in the split and the generated count of this key in Phase I.

Reducer adds up the support count of the same itemsets and the generated count from previous phase, to get the actual support count of them in the whole data set and determine the set of global frequent itemsets; then outputs a list of key/value pairs, denote by L  global , where the key is an  element of global frequent itemset and the value is its actual occurrence in the whole data set.

Since, a large number of itemsets have been counted in Phase I, the number of itemsets which have to be considered by the map function in the Phase II and sent to Reducers is substantially less than IMRApriori algorithm. This achievement causes to decrease of execution time of Mappers in Phase II and communication load between Mappers and Reducers in Phase II, that both of them are necessary in a MapReduce based algorithm. Fig. 3 shows the pseudo-code of Phase II.

Map task: Input: ?@?  , GHIJK?IL}  // where GHIJK?IL}  is output of Phase I for 2?''R(0 Output: <key, value> pairs, key is an element of GHIJK?IL}  and value is {k?= ?? ?A==B?CDBAEC~,?k?=?? ?A==B?CDBAEC~~} Method Begin  1. Foreach itemset X in GHIJK?IL} do 2.   ?? ?A==B?CDBAEC~~ = Count (X, ?@?); 3.   Output (X, { k? = ?? ?A==B?CDBAEC~,?k?=?? ?A==B?CDBAEC~~}) 4. End Foreach End   Reduce task: Input: <key, value> pairs, where key is an element of partial  frequent itemsets and value is { k? = ?? ?A==B?CDBAEC~,?k?=?? ?A==B?CDBAEC~~} Output: <key, value> pairs, where the key is an element of global frequent itemsets and the value is its actual count in the whole data set  Method Begin  1. Foreach key X do  2.   Foreach value v do 3.        ??? ?A==B?CDBAEC o\ k? c? ``?  + ) ?##S??? ?A==B?CDBAEC?\?k? 4.   End Foreach 5.   If ???? ?A==B?CDBAEC : ? ? ??; // where D is the total  number of transactions.

6.    Output <X,??? ?A==B?CDBAEC>; // collected in GyLz{IL 7.   End If 8. End Foreach End  Figure 3.  Phase II of our Algorithm

V. EXPERIMENTAL RESULTS We conduct several experiments to evaluate the performance of our algorithm based on the IBM synthetic dataset [2]: T10.I5.D1000K. The parameters of synthetic data generated by IBM synthetic data generator [2] denote the average transaction size (T), the average potentially maximal frequent item set size (I), and the total number of transactions (D), respectively. We used Hadoop 1.1.2 running on the pseudo distributed mode on a single machine which has 3.4 GHz Intel(R) Core ? i7-2600 CPU with 8 GB main memory running Ubuntu 12.10 operating system.

Since IMRApriori algorithm outperforms other Apriori based algorithms, in this experiment study, we compare our algorithm with IMRApriori algorithm in the terms of the execution time and communication. Both of algorithms are implemented in java and the JDK version is 1.6.0_43.

Fig. 4 shows a runtime efficiency comparison between IMRApriori and our algorithm using synthetic dataset T10I5D1000k based on changes in the minimum support threshold from 1 to 0.2%. As shown in the graph, the execution time for both algorithms grows with variations in the minimum support from 1 to 0.2%. A considerable increase in the running time of IMRApriori in comparison with our algorithm with further decreasing of the minimum support is observed, because of handling a huge number of partial frequent itemsets in IMRApriori algorithm. Since in our algorithm, a substantial number of the partial frequent itemsets that are the input of each Mapper of the Phase II are pruned, total execution time is diminished in compare with IMRApriori algorithm.  From this point of view, our algorithm is a better solution, compared to IMRApriori in mining big data by using MapReduce framework.

Figure 4.  Runtime comparison by varying the minimum support threshold  Fig. 5 depicts the runtime of our approach in comparison with IMRApriori algorithm as the data size grows from 200000 to 1000000. The x-axes show the variation in data size. We set the minimum support threshold s to 0.4%.

As shown in Fig. 5, for IMRApriori, a considerable increase in the execution time with increasing dataset size was observed. Mappers in our algorithm avoid considering the whole partial frequent itemsets in each transaction. The overall runtime required by our algorithm is therefore less than IMRApriori especially in larger datasets. This result demonstrates an advantage of our algorithm to handle larger datasets.

Fig. 6 shows the total produced communication load for sending Mapper outputs to Reducers in the Phase I and the Phase II as the minimum support threshold decrease from 1 to 0.2%. As can be seen from the figure, as minimum support decreases, because the number of frequent itemsets in each split increases, the communication load for both algorithms grows. As shown in Fig. 6, in our algorithm in each the minimum support threshold, the entire generated communication load  is less that of IMRApriori algorithm.

This achievement, that is important for MapReduce based algorithms in which applications challenge over bandwidth, is caused due to decrease of the number of global candidate itemsets in each Mapper of Phase II especially in the low minimum support thresholds. Since IMRApriori algorithm counts each itemset repeadly in phase II for generating the global frequent itemsets, re-communication load is established among Mappers and Reducers for the large number of itemsets on system. In our algorithm, all Mappers in Phase II count only itemsets that has not been counted in their split in the Phase I that leads to significant reduction in the communication load. The results in this experiment show that our algorithm reduces the communication cost in comparison with IMRApriori algorithm especially in the low minimum support thresholds.

These results show that our algorithm outperforms IMRApriori in terms of both runtime and communication, when used to mine an exact set of frequent patterns from big data.

Figure 5.  Runtime comparison by varying Data size    Figure 6.  Total communication in megabytes with varying the minimum support threshold

VI. CONCLUSION Cloud computing has proved that processing very large  datasets over commodity clusters can be done by providing the right programming model. As a parallel programming model, MapReduce, one of most important techniques for cloud computing, has emerged in the mining of datasets of terabyte scale or larger on clusters of computers. In this paper, we extended IMRApriori algorithm which is a MapReduce based algorithm for mining frequent itemsets.

For this purpose, we preprocessed the input of each Mapper in Phase II that leads to a significant reduction in the communication load and execution time in comparison with IMRApriori algorithm. The experimental results prove the theoretical claims.

