Distributed Association Mining on Message Passing Systems

Abstract Association mining in finding relationships between  items in a dataset has been demonstrated to be practical in  business applications. Many companies are applying  association mining on market data for analyzing consumers  purchase behavior. The Apriori algorithm is the most  established algorithm for association mining in finding  frequent itemsets. However, the time complexity of the Apriori  algorithm is dominated by the size of candidate itemsets.

Research to date has focused on the efficient discovery of  itemsets in a large dataset. Those improvements include the  optimizations of data structures, the partitioning of datasets,  and the parallelism of data mining. In this paper, we propose a  distributed association mining algorithm in finding frequent  itemsets. The work is different from many existing distributed  algorithms where most of existing algorithms center on the  reduction of the size of the dataset. Our distributed algorithm  focuses on the reduction of the size of candidate itemsets. The  work of candidate k-itemsets generation is evenly distributed  to the nodes for workload balancing among processors. The  complexity analysis of the distributed algorithm is also  presented.

Keywords- Apriori, Association Rules, Data Mining,  Frequent Itemsets, Distributed Computing

I.  INTRODUCTION  Association mining in finding correlations between items  has been shown to be useful in business applications. Retail  stores routinely use association tools to learn about  purchasing habits of its customers [1]. Healthcare  organizations use association tools to improve the health care  to the patients and reduce the costs also [2]. Many  implementations of association mining have been proposed  to find frequent itemsets in a very large dataset. They are    broadly classified into two categories: Apriori [3] and FP-  growth [4]. The Apriori algorithm discovers large frequent  itemsets by iteratively performing the two steps. The first  step consists of finding large itemsets. The second step  consists of finding associations with a user specified  confidence among the large itemsets discovered in the first  step. The Apriori algorithm discovers the correlations  between items by generating maximal frequent itemsets  iteratively. One major bottleneck of the Apriori algorithm is  its time complexity of finding all large itemsets. The FP-  growth algorithm discovers frequent itemsets in a dataset  without candidate generation process which is a very time  consuming one in the Apriori algorithm. Although the FP-  growth algorithm outperforms the Apriori algorithm in  performance, several variants of the Apriori algorithm have  been made to speed up the performance of the Apriori  algorithm.

In this paper, we present a distributed Apriori algorithm  on message passing systems to improve the performance of  finding frequent itemsets. We study the problems of the  Apriori algorithm causing the bottleneck of the performance  in finding large frequent itemsets. We then study the degree  of parallelism and synchronization issues in parallelizing  association rule mining. We also present a set of  optimizations for the Apriori algorithm for parallelism.

The paper is organized as follows. Related work is  surveyed and presented in Section II. Section III introduces  the Apriori algorithm and our improved Apriori algorithm  for distributed processing is presented in Section IV. The  complexity of our presented algorithm is discussed in  Section V. An optimized tree data structure is suggested in  Section VI to reduce the sizes of the trees for candidate  itemsets generation. Section VII presents the performance of  our parallel algorithm. Finally, we summarize the paper in  Section VIII.



II. RELATED WORK  The Apriori algorithm was proposed by R. Agrawal and  R. Srikant in 1994 [3]. Basically, the Apriori algorithm first  finds all frequent itemsets and then generates association  rules from the frequent itemsets. The process is iterative until  no more frequent itemsets can be found. However, the  performance of the Apriori algorithm is a major concern in    finding frequent itemsets. Much of the time is spent in  dealing with the creations of candidate itemsets. The  algorithm has linear dependence on the size of the dataset but  the exponential growth on the size of the itemset. Thus, the  time spent for the algorithm is considerable.

Many variants of the Apriori algorithm have been  proposed that focus on improving the efficiency of the  original algorithm. Several of these improvements are hash-  based technique, transaction reduction, partitioning, and  sampling. The algorithms using hash table or hash tree to  reduce the number of the candidate itemsets examined.

When scanning the transactions in the dataset to generate the  International Symposium on Parallel and Distributed Processing with Applications  978-0-7695-4190-7/10 $26.00  2010 IEEE  DOI 10.1109/ISPA.2010.35   candidate itemsets, the itemsets are distributed into the  different buckets of a hash tree or tree. Examining the bucket  count of the corresponding itemsets against the support for  the removals of infrequent itemsets improves the efficiency  of the algorithms. However, the algorithms are not primarily  designed to reduce the size of a candidate itemset, thus, the  algorithms might still need to spend considerable amount of  time in generating them.

The partitioning algorithms divide the transactional  dataset D into n non-overlapping partitions, D1, D2, , and  Dn. The algorithms reduce the number of dataset scans to  two. During the first scan, the algorithm finds all itemsets in  each partition. Those local frequent itemsets are collected  into the global candidate itemsets. During the second scan,  these global itemsets are counted to determine if they are  large across the entire dataset. The partitioning algorithms  improve the performance of finding frequent itemsets and  also provide several advantages. Small partitions might be fit  into main memory than large one. Because the size of each  partition is small, the algorithms might reduce the size of  candidate itemsets. In addition, the algorithms require only  two scans on the dataset. However, the partition algorithms  reduce the size of the dataset, they might still need to deal  with the large size of itemsets.

Many algorithms apply the partitioning technique for  association rule mining in parallel. The algorithms mainly    partition datasets or candidate itemsets for parallelism.

Partitioning datasets for parallel association mining (count  distribution algorithms) divides a dataset into small  partitions. Partitions are distributed to processors where each  processor creates its local candidate itemsets against its own  dataset partition. The processors are then exchanging their  local dataset partitions and candidate itemsets for the global  candidate itemsets. Each processor removes its infrequent  itemsets from its local candidate itemsets against the global  one. The resulting candidate itemsets become the frequent  itemsets for the  next candidate itemsets [5]. The extraction  of the frequent itemsets is done on all the processors in  parallel. However, the extraction can also be done on one  master processor. By doing so, the master processor need to  broadcast the result to the other processors. Partitioning  datasets for parallelism has a weakness that it requires many  synchronizations among processors.

Partitioning candidate itemsets for parallel association  rule mining divides datasets and itemsets into partitions. The  partitions are distributed to processors. Each processor  creates its local candidate itemsets against its own local  dataset partition. To determine the global candidate itemsets,  processors are exchanging their local candidate itemsets and  dataset partitions. Each processor then determines the global  itemsets and generates the next local candidate itemsets.

Unlike partitioning datasets, it also partition itemsets to  reduce the size of the candidate itemsets. The algorithm  suffers from heavy for data exchanges between processors.

Parallel association rules play an important role in  mining very large datasets for efficiency. However, parallel  processing requires the availability of parallel processors.

Sampling [6] for association rule mining does not require  parallel processors. A sample is drawn from the dataset  which it can fit into the computer core. The Apriori  algorithm is applied to find the potentially frequent itemsets  from the sample with a support threshold lowered so we are  unlikely to miss any truly frequent itemsets. During the first  scan, sampling obtains the candidate itemsets from the  sample. It then adds the itemsets to the candidate using the  negative boarder. These itemsets are not found as frequent in  the sample, but they might be frequent in the dataset. The  resulting itemsets become a candidate itemsets. The    algorithm makes a scan over the dataset to determine the  next frequent itemsets. If no itemsets in the negative border  is frequent in the entire dataset, then the frequent itemsets are  exactly those candidate itemsets that are above the threshold.

However, if there is an itemset of the negative border is  found frequent, then the sampling algorithm repeatedly  applies the negative border until the candidate itemsets do  not grow further.

There are several papers for the tutorial of the Apriori  algorithm. Dunham et al. [5] present their finding in  comparing various association rules algorithms. Bodon  conducted a similar examination on the efficiency of  frequent itemsets mining algorithms [7]. Hegland [1] gives a  tutorial on the Apriori algorithm. The tutorial reviews basic  concepts of association rule mining and the corresponding  algorithms.



III. THE APRIORI ALGORITHM  In this section, we briefly introduce the concepts of  association rule mining for frequent itemsets. Along with the  definitions of association rule discovery, the Apriori  algorithm is presented. The time complexity of finding  frequent itemsets in the Apriori algorithm is addressed. The  algorithm is also illustrated with examples.

A. Notation  Definition 1. Let T = {t1, t2, t3, , tn} be a transactional  dataset consisting of a finite set of business transactions  where ti is a business transaction and 1 ? i ? n. Let I = {i1, i2,  i3, ., im} be a finite set of items (itemset) where ii is an item  and 1 ? i ? m. A business transaction ti = <TID, X> typically  includes a transaction identity (TID) and an itemset X ? I.

Definition 2. An itemset in Ti is called a k-itemset where  k = |X|. The support of an itemset X in D denoted as  support_count(X) indicates the number of transactions in D  containing X. An itemset X is frequent if its support count is  greater than a support count threshold called minimum  support count.

Definition 3. Let U = <D, T, I> be the association mining  context, where T and E are finite sets of transactions and  items, respectively in a dataset, D. D ? T  I, is a binary  relation where D = {<ti, X> | ti ? T and X ? I}. In D, an  association rule is of the form X ? Y where X ? I, Y ? I, X  ? Y = ?, X ? ? and Y ? ?. The rule holds in D with support s    where s is the percentage of transactions in D that contain X   and Y. It means that the probability of finding X and Y  denoted as support(X ? Y) = probability(X ? Y) =  support(X ? Y) = support_count(X ? Y). In general, we are  only interested in a high probability of finding Y in high  support. This level of high support is called the minimum  support threshold.

Definition 4. The rule X ? Y also has confidence c  where c is the percentage of transactions in D containing X  that also contain Y. It means that if we find an itemset X,  then we have a chance of finding an itemset Y denoted as  confidence(X ? Y) = probability(Y|X) = support(X ? Y) /  support(X) = support_count(X ? Y) / support_count(X).

The following example reiterates the terms and  definitions of association mining for frequent itemsets.

Example 1. Suppose we have T = {t1, t2, t3, t4, t5} and I = {a,  b, c, d, e} in the following dataset D where D = {<t1, {a, b,  c}>, <t2, {d, e}>, <t3, {a, c, e}>, <t4, {b, c, d, e}>, <t5, {a}>}.

Figure 1 presents an example to illustrate frequent itemsets  mining.

Figure 1.  An example of the transactional dataset    Figure 2.  An example to illustrate frequent itemsets  The association rules are illustrated below.

confidence({a, c} ? {e}) = support_count({a, c} ? {e})  / support_count({a, c}) = 1 / 2 = 50%  confidence({e} ? {a, c}) = support_count({e} ? {a, c})  / support_count({e}) = 1 / 3 = 33%  Definition 5. Large Itemsets Property. All nonempty  subsets of a frequent itemset must also be frequent. In other  word, if an infrequent itemset is added to an itemset I, then  the resulting itemset is not frequent.

Proof: Given an itemset A such that support(A) <  min_support is added to an itemset I, show that support(I ?

A) < min_support.

support(I ? A) = frequency(I  ? A) / |D| =  min{frequency(I) / |D|, frequency(A) | |D|} where  frequency(I) = {t in D| t contains I}. Since  frequency(I) ?

frequency(A), thus frequency(I) / |D| ? frequency(A) | |D| <  min_support. Therefore, support(I ? A) < min_support    which is also not a frequent itemset.

With Definition 5, the Apriori algorithm [3] was  proposed to find all significant association rules. The Apriori  algorithm first determines the large frequent 1-itemsets. This  frequent 1-itemsets is then used to generate the candidate 2-  itemsets. The process is repeated to find k-itemsets until the  candidate k-itemsets is empty. The following we present the  Apriori-Join algorithm used in the Apriori algorithm. The  Apriori-Join algorithm takes a set of large k-itemsets, Lk-1 as  an input argument and returns a superset of the set of all  large k-itemsets an output argument.

Algorithm 1. Apriori-Join Algorithm(Set of Large (k-1)Itemsets: Lk-1, Set of  Candidate k-Itemsets: Ck)  1. Ck = ?;  2. for each X ? Lk-1 do  {  for each Y ? Lk-1 and Y ? X do  {  if (X ? Y) ? Ck then  Ck = Ck ? (X ? Y);  }  }    Using the same example 1, the Apriori-Join algorithm is  illustrated below.

Figure 3.  Illustration of the apriori-join algorithm  Algorithm 2. The Apriori Algorithm(Itemsets: I, Transactional Dataset: D,  Support: s, Large Itemsets: L)  1. k = 0; // scan number  2. L = ?;  3. C1 = I; // Initial set of candidate itemsets  4. repeat   {  4.1 k = k + 1;  4.2 Lk = ?;  4.3 for each X ? Ck do  {  frequency(X) = 0; // initialize each count of the itemsets  }  4.4 for each t=<ID, I> ? D do    {  4.4.1 for each X ? Ck and X ? I do  {  frequency(X) = frequency(X) + 1;  }  }  4.5 for each X ? Ck do  {  if ((frequency(X) / |D|) ? s) then  Lk = Lk ? X;  }  4.6 Ck+1 = Apriori-Join(Lk);  4.7 L = L ? Lk;  } until Ck+1 = ?; // repeat until no more set of candidate k-itemsets      Figure 4.  Illustration of the apriori algorithm  The Apriori algorithm spends time in dealing with large  itemsets. Let |I| be the size of the initial itemsets, d, |Ck| be  the size of generating candidate k-itemsets Ck, n be the  number of transactions in D, and k be number of scans in D.

We also assume k? is the k time units spent in searching k  items in a transaction. The timing complexity is  approximately C = ?

=  d  k 1  (k? ? |Ck| ? n) time units = n ? (1  ? |C1| + 2 ? |C2| + 3 ? |C3|  +  + k ? |Ck|) ? ? = n ? d ? 2d-1 ?

? = n ? d ? 2d-1 ? ? where  ?

= ???

?

???

?d  k k  d  k   = d ? 2d-1. As we  know that timing complexity(d2) ? timing complexity(2d-1),  thus C = O(2d) which indicates the Apriori algorithm has a  weakness in performance with the exponential growth. The    size of the itemsets is a key factor in determining the  performance of finding significant association rules in  knowledge discovery. A faster algorithm is proposed in this  paper to speed up the performance of the Apriori algorithm.



IV. THE IMPROVED APRIORI ALGORITHM  The algorithm presented in this section focuses on the  size of candidate itemsets, d. The algorithm distributes  frequent itemsets to distributed processors. For example, in  Figure 5, we have an initial itemsets I for finding association  rules. The itemsets I is divided into four sub-itemsets I1, I2,  I3, and I4 and assigned to four processors P1, P2, P3, and P4  dispersed in different locations. All the sub-itemsets are  disjoint. Each processor is generating its own candidate k-  itemsets locally. Since each processor has only the local view  of the complete candidate k-itemsets, it needs to broadcast its  local k-itemsets to the leading processor. A processor  finishing its k-itemsets earlier needs to wait until the leading  processor collects all the k-itemsets from the other  participating processors. The leading processor whose rank is  0 then generates a complete itemsets, finding candidate  (k+1)-itemsets, dividing the (k+1)-itemsets evenly, and  assigning each sub-(k+1)-itemsets to the processors. At the  synchronization point, each processor continues. The  algorithm terminates as the candidate itemsets are empty.

The leading processor should have the frequent k-itemsets in  it.

Figure 5.  Broadcast itemsets partitions  Algorithm 3. The Apriori Broadcast Itemsets Partition Algorithm(Itemsets:  Ii, Transactional Dataset D, Support: s, Processors: P1, P2, P3, , Pn, Large  Itemsets: L)  // Perform in parallel at each processor Pi  1. k = 0; // scan number  2. L= ?;  3. Ci = Ii; // Initial set of candidate itemsets  4. repeat  {  // Leading Processor  4.1 if (process rank = = 0)  {  4.1.1 if (iteration = = 0)  k=k+1;    4.1.2 Lk= ?;  4.1.3 for each Xi ? Ci,k+1  do  {  frequency(Xi) = 0; // initialize each count of the itemsets  }  4.1.4 for each t=<ID, Ii> ? D do  {  for each Xi ? Ci,k+1 and Xi ? Ii do  {  frequency(Xi) = frequency(Xi) + 1;  }  }  4.1.5 Ck+1 = Apriori-Join(Lk);  4.1.6 Lk+1  =Apriori-Prune(Ck+1, Lk);  4.1.7 divide Lk+1  into n partitions, Lk+1,1  Lk+1,2    Lk+1,n  4.1.8 send Lk+1,i  to Processi  }  else  {  4.1.9 receive Ci,k+1 from Processi  4.1.10 Lk+1  =Apriori-Prune(Ci,k+1, Li,k);  4.1.11 divide Lk+1  into n partitions, Lk+1,1  Lk+1,2    Lk+1,n   4.1.12 send Lk+1,i  to Processi  }    // Non-Leading Processor  4.2 if (process rank != 0)  {  4.2.1 receive new Lk from Process0  4.2.2 Ci,k+1 = Apriori-Join(Lk);  4.2.3 send Ci,k+1 to Process0  }  } until Ci,k = ?; // repeat until no more set of candidate k-itemsets    The Apriori-Prune algorithm is shown below. Based on  the Apriori property that all subsets of a frequent itemset  must also be frequent, we can determine a frequent k-  itemsets by checking if its (k-1) subsets are frequent. The  Apriori-Prune algorithm employs the Apriori Broadcast  Itemsets Partition algorithm to remove candidates that have a  subset that is not frequent.

Algorithm 4. Apriori-Prune Algorithm(Set of Candidate k-Itemsets: Ck, Set  of Large (k-1)Itemsets: Lk-1, Set of Large k-Itemsets: Lk)  1. Lk = ?;  2. for each X ? Ck do  {  for each subset Y ? X and Y ? Lk-1 do  {  Lk = Lk ? X;  }  }    The Apriori algorithm suffers the performance of finding  frequent itemsets as the size of the itemsets increases. Our  algorithm improves the performance by reducing the size of  the itemsets. However, a fundamental issue that highly  influences the performance of our algorithm is the inter-  processor communications between the local processors and  the leading processor. Since the global knowledge about the  support of any itemset is not available at local sites, such  communication overhead is needed for the leading processor  to compute the global support of an itemset. Therefore, the  communication overhead increases as the number of frequent  itemsets and processors increase.



V. PARTITIONING ITEMSETS FOR WORKLOAD  BALANCING AMONG PROCESSORS  Recall that the Apriori algorithm spends much time on  the generations of candidate itemsets. The computation time  grows exponentially. Our algorithm takes the workload issue  on each processor into the consideration. Suppose we have 3  processors P1, P2, and P3 and L1 = {a, b, c, d , e , f, g}. From  L1, the 1-itemset {a} contributes six 2-itemsets {ab, ac, ad,  ae, af, ag}. The 1-itemset {f} contributes one 2-itemset {fg}  only. Zaki et al. [8] point out that there are several ways of  partitioning itemsets among processors. A simple block  partitioning and an interleaved partitioning suffer from a load  imbalance problem. They propose a new partitioning scheme  called bitonic partitioning for load balancing. The scheme  computes the work load Wi due to the itemset i. All the Wi  are sorted. The itemset with the maximum value of Wi is  assigned to the least loaded processor. This scheme is better    than the simple block partitioning and the interleaved  partitioning scheme and results in almost no imbalance  among processors.



VI. OPTIMIZED TREE DATA STRUCTURE  The Apriori algorithm and its variations including  parallel association mining generally use tree-based data  structures for candidate itemsets generation. Three common  data structures are generally adopted in them including has  trees, enumeration set trees and tries [9]. All the tree  structures suffer a same problem. For a very long frequent  itemsets, the trees can be very large. The trees corresponding  to a long itemset with the size of n will create O(n2) nodes in  the trees. We improve the tree structure from the trie [10] to  the suffix tree where all internal nodes of outdegree 1 are  removed, and the label of an edge is now allowed to be an  itemset. Several applications of suffix trees include on-line  string matching, longest repeated substring, and substring  identifiers [11]. Ye and Chiang implemented a fast Apriori  algorithm using the trie structure. All itemset information is  accrued during traversal to the node. The application of  suffix tree substantially reduces the number of internal nodes  in the tree which leads to the efficient use of the memory.

For example, in [10], the same structure using trie now  becomes more condensed using the suffix tree. Although all  itemset information is not explicitly represented in the tree,  nevertheless the tree structure does not loose all itemset  information. The following definition proves this Apriori  property.

Definition 6.  If Y id s frequent k-itemset and if X is a  subset of Y then X is a frequent |X|-itemset.

The following example adapted from [10] demonstrates  the improvement of the data structure used in the algorithm.

In Figure 6, a trie used in Ye and Chiangs candidate  generation algorithm and a fundamentally equivalent suffix  tree used in our algorithm are presented to show the  improvements.

Figure 6.  Two different data structures  These two tree structures are structurally equivalent. The  difference is that, while within the trie, all itemset  information is accrued during traversal to the itemset, in the  suffix tree, all itemset information is contained at the node.

For example, {AB} is shown a frequent 2-itemset in the  suffix tree. Based on Definition 6, it derives {A} and {B} are  both frequent 1-itemset also. Another example, {BCD} is a  frequent 3-itemset, then we can derive that {BC, BD, CD}  should also be a frequent 2-itemsets. This Apriori property   shows that the suffix tree wont loose any itemset  information corresponding to its trie.



VII. EXPERIMENTAL EVALUATION  We have discussed the performance of the Apriori  algorithm in Section III. The size of the itemsets, d, plays a  key role in determining the performance of the algorithm.

This is the main reason we present the Broadcast Itemsets  Participation algorithm in this paper to improve the  performance. When long itemsets are likely, the algorithm is  a suitable one for discovering frequent itemsets. Of course,  the other algorithms could be better for another dataset.

Table I shows the performance of our parallel algorithm  relative to the Apriori algorithm for a dataset on the number  of iterations from 1 to 5. The results are plotted on the graph  in Figure 7.

TABLE I.  EXECUTION TIME OF THE APRIORI ALGORITHM AND OUR  ALGORITHM ON THE NUMBER OF ITERATIONS FROM 1 TO 5  Time on Each  Iteration Apriori Parallel Apriori  gen time 1 0.516666667 0.412  prune 1 2.633333333 1.287  gen time 2 147.2666667 0.18  prune 2  N/A 5.02  gen time 3  N/A 0.27  prune 3  N/A 2.39  gen time 4  N/A 0.15  prune 4  N/A 0.23  gen time 5  N/A 0.12  prune 5  N/A 0.25    Figure 7 shows the performance of the Apriori algorithm  and our parallel algorithm. Apparently, as the number of  iterations increases, the performance of our algorithm  performs better than the Apriori algorithm. The Apriori  algorithm even cannot handle the dataset as the number of  iterations is greater than or equal to 3.

Figure 7.  Performance comparison based on the number of iterations  Table II shows the actual running time of our parallel  algorithm for mining association rules from T25I10D10k and  T25I20D100k with respect to the datasets for the number of  processes. T25I10D10K and T25I20D100K are synthetic  data resembling market basket data with and were  downloaded from  http://miles.cnuce.cnr.it/~palmeri/datam/DCI/datasets.php. T  indicates the number of items, I indicates the average length  of items, and D means the size of dataset.

TABLE II.  EXECUTION TIME OF FROM THE TWO DATASETS  Parallel Apriori Cost of Time (? secs)  Number of  Processes T25I10D10k.data T25I20D100k.data  1 21 513  2 18 357  3 14 271  4 12 245  5 12 218  6 11 204  7 11 189  8 10 153    The execution time for T25I10D10k and T25I20D100k  in terms of the number of processes is plotted on the graph in  Figure 8. As the size of a dataset increases, the execution  time of the algorithm increases. The algorithm performs  better if the number of processes increases from 1 to 8. Thus,  since the nature of data mining is to operate on a large  amount of data, with the datasets become larger and larger,  parallel computing seems cost effective.

Figure 8.  Performance comparison of T25I10D10k and T25I20D100k

VIII. SUMMARY  Data mining deals with huge volume data for knowledge  discovery. Association rule discovery is one of the areas in  data mining. The Apriori algorithm was developed in the  early years of data mining. However, the Apriori algorithm  has a weakness in performance.  Various variations are  proposed to improve the performance of the Apriori    algorithm. Among the existing algorithms, two major  algorithms including data distribution and task distribution  are using the power of distributed computing. In this paper,  we present an improved algorithm, Broadcast Itemsets   Partition, for association rule discovery. The algorithm  focuses on the distribution of frequent k-itemsets, d, not on  the partitions of the transactional dataset, D. Therefore, the  transactional dataset may not be able to fit into main memory  of each processor. The algorithm leads to a straight-forward  distributed for parallel processing algorithm. The weakness  of the algorithm is that it requires many synchronization  points. The communication overhead may outweigh the  advantage of the parallel processing. The algorithm can be  extended to have the transactional dataset partitioned and  distributed to the processors also. The partition of the  transactional dataset allows all the partitions to be able to fit  into main memory.

