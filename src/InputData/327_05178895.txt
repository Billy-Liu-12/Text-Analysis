Proceedings of International Joint Conference on Neural Networks, Atlanta, Georgia, USA, June 14-19, 2009

Abstract-The objective of this study is to introduce the con- cept of evolving granular neural networks (eGNN) and to develop a framework of information granulation and its role in the online design of neural networks. The suggested eGNN are neural models supported by granule-based learning algorithms whose aim is to tackle classification problems in continuously changing environments. eGNN are constructed from streams of data using fast incremental learning algorithms. eGNN models require a relatively small amount of memory to perform classification tasks. Basically, they try to find information occurring in the incoming data using the concept of granules and T-S neurons as basic processing elements. The main characteristics of eGNN models are continuous learning, self-organization, and adaptation to unknown environments. Association rules and parameters can be easily extracted from its structure at any step during the evolving process. The rule base gives a granular description of the behavior of the system in the input space together with the associated classes. To illustrate the effectiveness of the approach, the paper considers the Iris and Wine benchmark problems.



I. INTRODUCTION  One important aspect of the intelligence of systems lies in their ability to adapt to new situations [1]. To achieve adaptation, systems must be equipped with learning algorithms to continuously improve or, at least, no degrade the system performance in changing environments. The approaches dis- cussed in this paper are classification oriented models that need incremental adaptability. In particular, researchers in the pattern classification area are facing the challenge of classify streams of data using models that adapt or evolve their structures and parameters based on possible new information contained in the data. These are online models often referred to as evolving classifiers.

A classifier is said to be evolving if it has the following characteristics:  ? Ability of online learning from data streams. Once data are processed, there is no way to look back to historical data;  ? No a priori knowledge about the system structure is needed (structural online adaptation);  ? No a priori knowledge about the statistical properties of data is required (nonparametric classification);  ? No a priori knowledge about the number of classes and prototypes per class is needed;  ? No prototype initialization is required.

Moreover, it is very desired that evolving classifiers possess fast knowledge assimilation, small memory requirements, and nonlinear separation ability.

The design of evolving classifiers is mainly concerned with the construction of learning mechanisms to induce new knowledge without 'catastrophic forgetting' and/or to refine the existing knowledge. The whole problem is viewed as how to accommodate new data in an incremental way while keeping the system in use.

Classification processes often require simultaneous model construction and testing in environments which constantly evolves over time. If a static classifier is used to handle data streams, then the accuracy of the underlying classification process is likely to decrease when, for example, there is a sudden burst of exemplars belonging to a particular class. Off- line pre-trained classifiers may be good in certain contexts, but they need to be redesigned for new circumstances.

Examples of evolving classifiers that have been widely discussed in the literature include the following: i) Fuzzy ARTMAP (FAM) [2] is one of many adaptive resonance network models. Its incremental learning ability naturally suggests its use in real-time problems solving; ii) Growing Neural Gas (GNG) [3] tries to incrementally generate prototypes within the data space by constructing a graph that consists of a set of interconnected neurons; iii) Unsupervised evolving connectionist systems [4] perform classification from fully unlabeled data streams. They develop their connectionist structure to model the incoming data.

One of the major examples of classifiers in this category is the Evolving Self-Organizing Map (eSOM) [5]. eSOM uses the principles of self-organizing maps (SOM). It allows the prototype neurons to evolve in the input space, and at the same time, acquires and keeps topological representations; iv) The evolving clustering method (ECM) is a one-pass algorithm for dynamic clustering of an input stream of data [6] when there is no predefined number of clusters. ECM is a distance-based clustering method where a threshold value determines the maximum radius a cluster can assume to accommodate input exemplars; v) EFuNN [7] is an evolving fuzzy neural network model which adapts its structure and parameters through incremental, hybrid supervised/unsupervised online learning. EFuNN can accommodate new input data, including new features or new classes, through local element tuning; vi) Evolving systems called eClass (evolving Classification) [8] were specifically designed to address dynamic classifica- tion problems. eClass can operate in online mode using non- iterative and recursive calculations. Several architectures for eClass models were introduced in [9], [10];     Fig. I. Alternative realizations of nullnorms  (0,0) a (1,0) (0,0) a (1,0)  and-dominated or-dominat ed  In virtue of the above restriction , we can observe in Fig. 1 that the choice of the value of e defines the size of the region nand deliver some flexibility to nullnorms realization. Note also that the description of a nullnorm has the T-norm "above" the S- norm (T-norm for larger values and S-norm for small values).

There is no duality requirement between any of the assumed triangular norms. The discrimination of and-dominated and or-dominated nullnorms refers to the value of the absorbing element e E [0, 0.5[ and e E ]0.5, 1], respectively.

satisfies the boundary conditions F(a ,O) = T(F(I ,O),a) and F(a , 1) = S(F(I ,0), a) 'V a E [0, 1].

Nullnorms may be viewed as a way to generalize triangular norms. Nullnorms relax the assumption about the neutral element because they allow to choose e E [0, 1]. When the element e is equal to 0, a nullnorm turns into a T-norm, whereas when e = 1, the nullnorm becomes as-norm.

Formally, a nullnorm is a binary relation V : [0,1]x [0,1] ----t [0,1] satisfying the following properties :  ? Commutativity: V(a, b) = V(b, a), ? Monotonicity : V(a , b) ::::: V(a ,c), if b ::::: c, ? Associativity: V(a ,V(b ,c)) = V(V(a , b),c), ? Absorbing element e E [0, 1] : V(a , e) = e,  and that satisfies V(a ,0) = a 'V a E [0, e] and V(a ,l) = a 'V a E [e, 1], where a, b,c E [0, 1]. Nullnorms are a special class of T-S aggregation functions . In this work, we confine ourselves to the following family of constructs of nullnorms that seems to be highly interpretative [14]:  if a, b E [0, e], ifa,b E [e,l ],  otherwise.

) (1,1)  e Tvnorm  S-norm e   (a-e b-e)I -e ' l -e  b  (0 ,1  Nullnnrrn  ) (1,1)   .. Tvnor m  S-norm I'  (0,1  {  e S (? Q) e' e  V(a,b) = ~e + ( I -e) ) T  II . BRIEF INTRODUCTION TO T-S NEURONS  T-S neurons are neural implementations of T-S aggregation functions . In this paper we emphasize nullnorms [12], a special class of T-S operators. Nullnorms include T-norms and T- conorm s (S-norms) as boundary cases. Nullnorms extend T- norms and S-norms in the sense they allow a flexible choice of a neutral element, called the absorbing clement, of aggregation operations on fuzzy sets [13]. Nullnorms bind T-norms and S- norms constructs. As a result , we can switch between pure and and or properties of the logic operators occurring in this construct. T-S neurons inherit triangular norms logic connective processing as a conjunction, which make them flexible and logically appealing .

A. Nullnorms  vii) The fuzzy min-max neural network (FMM) is a nonlinear classifier that uses fuzzy sets as pattern classes [11] . The fuzzy min-max learning algorithm is based on the expansion- contraction paradigm, and can be used in an one-pass through data scheme . Thus, FMM reveals itself as being a natural candidate for real-time applications. Actually, the FMM model is a particular case of eONN models in the sense that gran- ules/prototypes can individually assume intermediate values between the min and max operation s. eONN generalizes min- max using nullnorms and T-S neurons . Moreover, there are considerable differences and improvements in the learning algorithm that make eONN models more effective and com- petitive . The intuitive idea of contraction-expansion learning paradigm remains .

In common, the learning mechanism s mentioned above generate prototypes when the incoming data are sufficiently dissimilar to the existing prototypes. Otherwise, an adjustment of some of the existing prototypes is conducted. The decision to assign new data to either an existing prototype or to a new prototype is based on control parameters like the size of granules/hyperboxes/clusters, error threshold or dissimilarity threshold .

After this introduction, the paper proceeds introducing T- S neurons in Section II. Next, Sections III and IV detail the eONN modeling approach and its associated learning procedure. Section V present s results on how eONN behaves when solving benchmark classification problems and compares its performance against alternative approaches. The paper concludes with Section VI summarizing the main contribution and suggesting issues for further investigation .

Triangular norms T are commutative, associauvc and in- creasing binary operators on the unit square whose boundary conditions are T(a ,0) = a and T(a ,1) = a, a E [0, 1] . Simi- larly, T-conorms S are commutative, associative and increasing binary operators on the unit square whose boundary conditions are S (a,0) = a and S(a ,1) = 1. The neutral clements of T and S norms are e = 1 and e = 0, respectively.

Consider a continuous triangular norm T and a continuous triangular conorm S . A binary operator F is called a T-S aggregation function if it is increasing and commutative, and  A motivation for using nullnorms is deduced from the fact that T-norms describe situations when both conditions a and b are absolutely necessary, so if one of these conditions is not satisfied, we completely reject the corresponding alternative.

There are many such situations, but there are also many other situations in which, although we want the first condition to be satisfied and the second condition to be satisfied, etc ., but if one of these conditions is not satisfied, we may still consider the corresponding alternative .

B. T-S Neurons  T-S neurons are neural implementations of nullnorms. For- mally, let X == {Xl, X2, ... , x n } be an input vector in the unit hypercube, X E [O,l]n, and W == {WI,W2, ... ,Wn } its corresponding connections (weights), W E [O,l]n. The nullnorm aggregation of the weighted inputs produces the output y E [0,1]' namely  ? Granulation of numeric data. At this level a collection of information granules is formed.

? The construction of the neural network. Now any learning that takes place with the neural network is based on the information granules rather than original data. As a consequence, the neural network is not exposed to the original data of a far higher granularity and far more numerous as the information granules.

Fig. 3. Two-phase design of granular neural networks  Original Granules of data set information  This relation emphasizes the parametric flexibility residing with the connections that are necessary to support learning.

We adopt the notation y == V (x, w) for short. Fig. 2 shows a schematic view of the neural processing. The absorbing ele- ment e can be adjusted through learning. What is remarkable is the diversity of nonlinear characteristics of the mapping produced by such neurons can assume depending on how W and e are chosen.

0 o 0 0 (10o 0 0 0 00 Data Networko 00 0 0 o goo granulation design0 0 <,00 o t t  0 0  0 0 0  ~0 0 0 0  ( Neural network)  ?:\"1 1-l'1  ,Yj l.'j J'  '\'1\ 1.'1\  e  Fig. 2. Schematic view of a T-S neuron: the synaptic processing uses algebraic product and the neuron employs nullnorm aggregation  Given this structure, the processing for each input can be done independently and because of the adjustable value of the absorbing element, we encounter a different way of aggregation processing that permits intermediate assumption between the operations and (T-noms) and or (S-norms) [14]. A possible drawback of this structure is that although the neuron exhibits a high level of flexibility, its interpretation could be more complicated given the semantics of nullnorms. Likewise, the learning approach might cause more difficulties.



III. EVOLVING GRANULAR NEURAL NETWORK - EGNN  Granular Computing (GrC) has emerged as a new infor- mation processing paradigm in the last ten years [15], [16], [17]. The foundation of GrC is the fact that precision is an expensive and often unnecessary goal in modeling complex systems. In particular, human reasoning does not appear to operate at a numeric level of precision, but at a coarser, more abstract level of detail. GrC is a framework for expressing these abstractions in computational processes.

The concept of granular neural networks (GNN) was first established in [18] emphasizing artificial neural networks capable of processing granular data. These granules of data are inputs and outputs of GNN. For example, environmental data are inputs and human actions are outputs of biological neural networks. Basically, the development of GNN involves two main phases, refer to Fig. 3:  Since the conception of GNN, studies suggesting variations and extension of the original content have been developed [19], [20]. In this paper we introduce evolving versions of GNN. The aim is to develop granule-based learning algorithms to tackle classification problems in continuously changing, dynamic environments. The evolving granular neural networks suggested here are constructed by processing streams of data using fast incremental learning algorithms. eGNN modeling requires a relatively small memory amount to perform classi- fication tasks. Basically, eGNN try to find information granules occurring in the incoming data using known granules and T-S neurons in the processing steps. The main characteristics of eGNN modeling include continuous learning, self-organization and adaptation to unknown environments. Association rules can be easily extracted from its structure and parameters at any time during the evolving process. The evolved rule base offers a granular description of the current behavior of the system considering the input space and the associated classes.

Generally, eGNN models develop and work with a small amount of granules. The number of granules needs not to be pre-defined. No granules exist prior to learning; they are created by the evolving process. Predominantly, the evolving granular neural models use constructive (bottom-up) model- ing mechanisms and decomposition-based (top-down) mecha- nisms. Association rules in the form of IF-THEN statements are easily extracted from the eGNN models at any step.

Fundamentally, eGNN models use class exemplars in the form of fuzzy hyperrectangles, the granules, to perform classi- fication tasks. During evolution, granules are created, updated or shrunk along one or more dimension of the input space.

More specifically, the newest input is first matched against the existing granules. Three possibilities may happen: i) the new input fits more than one existing granule. Thus, the one presenting highest membership degree given by a nullnorm- based aggregation operation is selected to accommodate the input. The selected granule must, however, be associated with the same class label of that of the incoming data. Otherwise, the granule with the second highest membership degree is     selected to accommodate the input, and so on. The accom- modation basically consists in adjusting the parameters of the membership functions of the granules and/or the parameters of the T-S neurons to better fit similar inputs at future steps; ii) the input falls into only one existing granule. Then, this granule and its corresponding T-S neuron are updated to accommodate the input; and iii) none of the existing granules accommodates the input. Thus, a new granule is created perfectly matching the input. Shrunk is a refinement mechanism of the collection of granules. It is applied when the granule that matches an input is assigned to a different class label, the following decomposition mechanism proceeds: i) splitting the granule into two finer ones; ii) creating a new granule perfectly matching the input; and iii) assigning this new granule to the correct class label. Details and characteristics of eGNN models are addressed in next sections.



IV. THE EGNN LEARNING ApPROACH  eGNN learn from a stream of data (x,C)[h], h == 1,2, ... , where the class label C[h] is assumed to be known, given the corresponding input vector x[h]. The model accommodates new knowledge contained in the incoming data either creating a new information granule or adapting the parameters of existing granules and that of their associated T-S neurons. Each information granule ~/ of a finite collection of information granules , == {,I, ,2, ..., ,e}, defined in the input space X ~ ~n, is associated with a class k of the finite collection of classes C == {Classl, Class2, ... , Classm } in an output space y < Nm. The granule ,i may assume different geometric forms and sizes, but here we emphasize multidimensional fuzzy intervals or, alternatively, fuzzy hyperrectangles or fuzzy hyperboxes.

The structure of the proposed eGNN is illustrated in Fig. 4.

The network has 5-layers. The input layer basically distributes n-dimensional data vectors X)h] , j == 1, ... , n, into the network.

The evolving layer consists of granules of information ,i, i == 1, ... , c, extracted from the data stream. Granules ,i are de- fined by membership functions A;, j == 1, ... , n, with different dimensions and universes, each one representing an attribute of the input vector. The aggregation layer encompasses the T-S neurons T Sn i, i == 1, ... , c. They process standardized ( 1? d)? ~ i [h] . - 1 . - 1 .norma Ize Inputs x j ,J - , ... , n, 't - , ... , c, proveruent from the granulation process, and aggregate data to generate a single output oi, i == 1, ... , c. The output may be viewed as a measure of compatibility between a data pair and the existing granules. The decision layer compares the aggregation values of the granules, and the granule with the highest activation (winner) produces a binary output vector clh ], k == 1, ... , m, with 1 in the entry corresponding to the class label assigned to the winner granule, and 0 in the remaining ones. The output layer compares the m-dimensional binary output of the network clh ] with the desired output vector clh ], k == 1, ... , m. This operation may result in the network estimation error Ek, k == 1, ... , m. In eGNN models, the number of granules in the evolving layer needs not to be pre-defined.

In fact, no granules and T-S neurons exist prior to learning,  they are created and adapted during the evolving process as new information is found in the data stream. The connection weights w;, j == 1, ... , n, i == 1, ... , c, assign different degrees of relevance to different input attributes, while weights 8i , i == 1, ... , c, depends on the amount of data within each granule ,i. Generally speaking, 8i is a mechanism to indicate denser and sparser data regions in the input space.

X~h]----IKH-+---+-tI  x~h]----I~---+-tI  x~h]----I*+t------fI  Dectsion Iayer output layer  Fig. 4. Evolving granular neural network structure  Let p E X ~ ~n be the maximum size that an information granule can assume in the input space. Suitable choices of p are very important as it is directly related to the model trans- parency and accuracy. Any granule larger than the maximum size may result in losing some desirable regions. The value of p dictates the granularity (coarser, finer) of the granulation process and a possible control over the shape of the granules.

Generally, the larger p, the fewer granules are created and, despite being easier to interpret, the less is the ability to capture nonlinear boundaries between classes. On the other hand, smaller p values may lead to data overfitting and loss of interpretability. In the limit, the input may become a single numeric quantity. Then, the attempt is to find an acceptable compromise between the extreme situations. An example of the granular classification we are interested in this paper is depicted in Fig. 5. The figure shows a collection of granules ,i, i == 1, ... , c, constructed in light of the data available for the purpose of identifying the five classes (1 to 5) that appear in the input space.

A. Creating and Updating Granules  No granules exist until eGNN learning starts. They are cre- ated during the evolution process. The membership functions A;, j == 1, ... , n, associated with a granule ,i are defined in each of the corresponding input variable domain. They represent class attributes. We assume trapezoidal or triangular membership function for each A; of the granule ,i. Thus, they are defined by four parameters: the j - th lower bound, lj; the j - th upper bound, L;; and intermediate values of the function, A; and A;. For trapezoidal functions A; < A;, and     Fig. 6. Process of granules creation and adaptation  Fig. 5. Information granulation and five classes in input space  w; (new ) = j3w; (old) ,  where j3 is a decay constant with values in the open interval ]0, 1[. The update procedure is justified becau se A~ does not satisfactorily helps to differentiate classes. After steps i) and ii), a granule ,c+1 is created adding normal triangular mem- bersh ip functions A j +1 centered at the input x y,], j = 1, ... , n.

, c+1 is then assigned to the correct class C [hJ.

Note that, because different degrees of importance to each attribute of the input vector are allowed, the procedure re- sembl es techniques such as principal component analysis and wrappers in the sense that it searches relevant subsets from the current set of attributes. However, differently from principal component analysis and wrappers, the procedure does not eliminate less relevant attributes of the model as a whole, but it reduces the degree of importance of less relevant attributes as perceived by the local processing units .

,i, then the parameters >.~ and A~ , j = 1, ... , n, are updated to accommodate X)h+ .6.] , j = 1, ... , n . Basically, adaptation  consists of setting either>'i = x [h+ .6.] if x [h+ .6.] is in [Ii. >.i ].

J J 'J J' J '  or A~ = X)h+ .6.], if X)h+ .6.] is in [A~, L~]. Fig. 6(b) illustrates the adaptation approach.

It should be noted that there are certain situat ions where more than one granule could be selected to be updated to accommodate new data (x , C ) [h]. In these situations, the granule to be updated should be the one most compatible with the current data . A way to compute the compatibility between a data and candidate granul es is to use T-S neurons.

B. Adju sting Evolving Layer Weights and Compressing Gran- ules  The weights between the evolving and aggregation layer, namely w~, j = 1, ... , n, i = 1, ... , c, aim at capturing the relevance with which each attribute j of granule i differentiate data classes . Initially, learning starts setting all w~ to 1. During evolution, some wj may reduce their values depending on the data stream.

Similarly to other evolving models , eGNN suffer from the problem of sensitivity to the order in which data arrives. New data pairs (x, C) [h] may cause revision of ,i if this granule is the one most compatible with x [h], but C [h] is different from the class assigned to ,i.The following procedure is used to compress granule ,i to reduce its compatibility with x [h]. Two situations are of interest: i) if A~ , i = 1, ... , c, for any j results in a xi. [h] E ]0 1[ then set I i = x [h] if x [h] < >.i or set L i =  J " J J J - J ' J  X)h] if X)h] ~ A~ . This procedure compress the membership function A~ of the granule ,i ; and ii) if A~, i = 1, ... , c, is  such that x~[h] = 1 for any j, then A~ parameters are kept the same and the weight associated with the j - th attribute of the granul e i , wj, is reduced as follows  x: 1 - -- ----- ----  I x: 1 --- ---- - - ---  (b) Adaptation of "Ii to accommodate the current input x 1h+ Ll. ]  ~ I~__---=  l',----- -----------------------, A~ A~  l',----------------------------, ~ ~ I  - I Ix.

1 --------  (a) Creation of "Ii to accommodate the current input x 1h]  for triangular functions >.~ = A~ . Granules ,i are associated with corresponding class labels Ck. k = 1, ... , m .

The procedure to create granules is run whenever an in- . d [h] . . [I i L i ] ? 1 . 1coming ata x j IS not In j ' j' J = , ... , ri, 2 = , ... , c,  where c is the number of currently existing granules . It is also run when, although X)h] E [ l~, L~ ] \ij , the class label of the incom ing data C [h] differs from that assigned to ,i.

The new granule created, denoted by , c+ l, is constructed using triangular membership functions A j +l, j = 1, ... , n, with center x [h] E ~n . Therefore, the parameters of the membership fucntions are l c+ l = x [h] _ os . >.c+ l = Ac+1 = x [hJ. and  J J 2' J J J '  L j+l = x )h] + Pi ; \ij. Fig. 6(a) illustrates the procedure considering a generic granule ,i.

If a new input data vector x [h+ .6.], where D. is a positive integer, is within the current bounds of ,i for some i , and its respective class C [h] is the same as the one assigned to  C. Adjusting the T-S Neurons ' Absorbing Element  The eGNN addre ssed in this paper uses and-dominated T-S neurons to process the attributes of the granules. The i - th T-     S neuron, rs?, processes the n attributes of the normalized input x; [h], j = 1, ... , n, associated to the granule ,i through  V( x; [h],wJ) "Ij . The result is a single output value Oi which may be viewed as the compatibility between x [h] and ,i . A procedure to initialize and adjust the absorbing element of T-S neurons is as follows . First, at the beginning of the learning process, choose a T-norm and a S-norm for the T-S neurons .

Create a granule ,c+l and set the absorbing element of its corresponding T-S neuron as ec+1 = O. This induces aT-norm.

During evolution, some of the e" , i = 1, ... , c, may increase their values depending on the data arrival. For instance, if  O?lr ~~ or a small number ~f attribute~ of th~ i~put vector x j ,J - 1, ... , n, does not activate functions A j , J = 1, ... , n, for any i , we still consider the granule ,i as candidate to accommodate (x, C) [h] increasing its respective T-S neuron absorbing element ei as follows  D. Adjusting Decision Layer Weights and Deleting Granules  The connections between the T-S neurons and the max- neuron - see Fig. 4 - are weighted by s, i = 1, ..., c, which encode the amount of data assigned to the corresponding granules ,i . Generally speaking, the weights Oi are a way to identify denser and sparser data regions in the input space. In general , the higher the value of s. the bigger the probability to activate the i - th granule in subsequent steps.

Learning starts setting all Oi to I. During evolution, the values of some Oi may reduce or increase depending on a priori established criteria . For instance, a simple criteria is the following : if the i - th granule does not activate after a certain number of steps, then reduce Oi as  oi(new) = (o i(old) ,  where ( is in ]0, 1[. Otherwise, if , i activates often, then increase Oi using  El d h] _ c [h]  1 1  E = Ek d h] _ C [h]k k  Em C~] - C~]  A natural consequence of this procedure is that in contin- uously changing environments if granules are inactive for a number of steps, then it may be suppressed . This means that the actual process has changed and deleting granules means Oi ----; 0+, which is justified to maintain a reasonable amount of information and updated knowledge available. Clearly, if the application requires memorization of rare events, then the deletion procedure becomes prohibitive . Notice that the learning procedure detailed in this section enhances the eGNN ability to track the actual process evolution.

E. The Role of the Max-neuron  The single max-neuron of the eGNN model of Fig. 4 computes the highest value of its c-dimensional input vector.

The entries of the input vector are the degrees of compatibility between , i, i = 1, .. , c, and x [h] . Using the degrees of compatibility, the max-neuron determines the class assigned to the granule that presents the highest compatibility to the current input. The max-neuron implements a winner-takes-all scheme and outputs a m-dimensional vector with value 1 in the winner entry and 0 in the remaining entries .

Let clh ], k = 1, ... , m, be the binary m-dimensional vector computed by the network for the input x [h]; and let Ek be the error between clh] and the expected/desired class C [h] . The error Ek for all k is  X2  x x  / 2=0 e '2 = 0.1  yi l  e i2=0.2  e i2= 0.25  pI yi2  0 e~I=O 0 e~l = 0.1  e~l = 0.2 e'l =0.25  PI Xl  where X is a growth constant with values in ]0, 1[.

To adjust ei , i = 1, ... , c, during evolution, low membership  values are compensated by high values using S-norm-based processing only for low membership values. If the actual input does not fit ,i by a small number of attributes , then it could be made compatible to a certain degree with this granule through increasing e i . This procedure generally avoids the creation of very similar granules and helps to keep a small number of granules in the model structure. Fig. 7 illustrates the idea of the procedure, which can be seen as a form of adaptive adjustment of the granules size. Note that, depending on the T- and S- norm chosen e.g. maximum S-norm and Lukasiewicz T-norm; bounded sum and nilpotent minimum, then the granules with their respective T-S neurons can be viewed in the input space as assuming different geometric forms .

Fig. 7. Higher values of the absorbing element of a T-S neuron increases the corresponding granule size When E = 0, the learning procedure refines the network  parameters. When E i=- 0, the mechanisms of granules com- pression and weights updating proceed .

For classification performance comparison we consider the following models: multi-layer Perceptron (MLP), Elman neu- ral network, fuzzy C-Means (FCM), EFuNN [21], FMM [11], eClass [8], GFMM [22], and eGNN.

Dataset characteristics:  ? Number of instances (observations): 150; ? Number of variables: 4 quantitative input variables  namely Sepal Length, Sepal Width, Petal Length and Petal Width, all in em; and 1 output variable representing 3 classes of Iris plant: Setosa, Versicolour and Virginica;  ? Training/Test data: 60/100 %.

The results show that eGNN is the most accurate approach, similarly as GFMM, but eGNN uses a considerably lower number of granules (21 against 43). Interestingly, with only 5 granules the eGNN reaches a 98% accuracy, a performance slightly better than eClass using a similar compact struc- ture. We noticed that the learning algorithm, the structural assumptions, the neural flexibility and the granular view of eGNN models are the major ingredients to achieve higher performance.



V. COMPUTATIONAL EXPERIMENTS  A. Example A: Iris Classification  Basic description: The Iris benchmark dataset, obtained from the DCI Machine Learning Repository, is a widely used dataset in the pattern recognition context. Data contains 3 classes of 50 instances each, where each class refers to a type of Iris flower. One class is linearly separable from the other two.

The latter are not linearly separable from each other.

96.6 97.3 93.3  unavailable unavailable unavailable  96.0 96.0 96.3 97.7 99.5  % Accuracy (Avg.)  97.3 98.0 96.0 95.3 97.3 96.0 98.7 100.0 98.0 99.3 100.0  % Accuracy (Best)   Training epochs   TABLE I IRIS CLASSIFICATION PERFORMANCE  No. granules/neurons/clustersModel MLP  Elman FCM  EFuNN FMM eClass GFMM GFMM eGNN-l eGNN-2 eGNN-3  Results: To evaluate the effect of different parameterizations, the eGNN approach considers three sets of parameters: eGNN- 1 parameters are PI == [1.8 1.2 2.95 1.2], 131 == (1 == 0.95, Xl == 0.05; eGNN-2 uses P2 == 0.6p1, 132 == (2 == 0.95, X2 == 0.05; and eGNN-3 adopts P3 == 0.4p1, 133 == (3 == 0.95, X3 == 0.05. Each experiment was run five times with the same parameters. In each run the data were shuffled to induce differ- ent orders in the data stream. Data is presented sequentially to each eGNN model only once to emulate a stream. eGNN start learning with an empty rule base and no pre-training. This is to simulate the true evolving process. Table I shows the performance of the eGNN models and that of the remaining classification models.

G. The eGNN Modeling Procedure  The eGNN modeling procedure is summarized below:  F. Choosing Model Granularity  Optimal granulation is a difficult and challenging problem whose solution is still open in the current literature. Estimation of granules sizes is done using heuristic and knowledge-based approaches. For instance, after an initial guess and preprocess- ing a small amount of incoming data, the spatial distribution and statistical properties of data may suggest values for the granules size p. The size of the granules can also be estimated using a priori knowledge about the problem in hands. The main idea to be kept in mind is that granules larger than p may cause loss of information about some desirable regions, specially when handling nonlinear mappings. If p assumes very small values, then the number of granules created can be very high, what may not be of practical interest.

Another mechanism to handle granularity is to learn values for p itself. For instance, consider a procedure that takes into account the amount of granules being created during a certain number of evolution steps. If the number of granules grows fast, then p could be increased during the next steps.

Otherwise, if the number of granules grows slowly, then p could be decreased. Combinations of preprocessing and learning could be considered at the expense of additional computational effort.

BEGIN Initialize p, {3, x, (, c = 0; Select a type of T-norm and S-norm; Do forever  Read (x, C)[h], h = 1, ... ; If (h = 1) I/First iteration  Create "c+l and TSn c +\ Assign it to class C[h]; c = c + 1; Else  Feed x [h] into the network; Compute the number of granules G with compatibility to x[h] higher than 0; If (G = 0)  Adjust e i, i = 1, ... , c; Create "c+l and TSn c +\ Assign it to class C[h]; c = c + 1;  Else  For 9 = 1, ... , G Compute the 9 - th winner granule (g - th place) for x[h], namely ,,1/; Extract the resulting output error Ek, k = 1, ... , m; If (E k Vk = 0)  Adjust Aj, j = 1, ... , n, of ,,1/; Break;  Else Compress Aj, j = 1, ... , n; Adjust wj, j = 1, ... , n, of ,,1/; If (g = G) IILast winner  Adjust e i, i = 1, ... , c; Create "c+l and TSn c +\ Assign it to C[h]; c = c + 1;  End End  End End  End IIAfter a number of iterations Adjust s', 8 = 1, ... , c; Delete inactive granules;  END     eGNN reaches the best performance if it uses 17 granules.

An 8-granule structure achieves the best average accuracy.

These results illustrate the potential of eGNN-based models to solve classification problems that demand incremental adapt- ability.

Results: As in the previous experiment, to evaluate the in- fluence of the parameters, eGNN models employ two sets of parameter: eGNN-l uses PI == [2.3 3.1 1.1 11.6 55 1.7 2.8 0.3 1.9 7 0.7 1.6 840], (31 == (1 == 0.96, Xl == 0.04; and eGNN-2 adopts P2 == 0.6p1, (32 == (2 == 0.96, X2 == 0.04.

Each experiment was run five times with the same parameters.

Each run presents data sequentially in different orders. Table II shows the performance of the eGNN models and of the remaining classification models.



VI. CONCLUSION  The concept of evolving granular neural models has been introduced in this paper. A novel evolving granular neural network, eGNN, was suggested as a mechanism to develop evolving models of systems. The eGNN approach evolves models from information granules and from T-S neurons associated with granules. This provides eGNN models the ability to develop highly flexible structures and strong mecha- nisms to track the evolution. Experiments with Iris and Wine benchmark classification problems have shown that eGNN is competitive when compared against alternative nonlinear clas- sification techniques. Further work shall consider refinement  B. Example B: Wine classification  Basic description: The wine dataset, also obtained from the DCI Machine Learning Repository, has 3 classes representing the chemical analysis of wines derived from three Italian regions. From the classification point of view, this is a well behaved example in the sense that classes are clearly separated.

Dataset characteristics:  ? Number of instances (observations): 178; ? Number of variables: 13 quantitative input variables  namely Alcohol, Malic Acid, Ash, Alkalinity of Ash, Magnesium, Phenols, Flavanoids, Nonflavanoid Phenols, Proanthocyanins, Color Intensity, Hue, OD280/0D315 and Proline; 1 output representing 3 different cultivars;  ? Training/Test data: 60/100 %.

For classification performance comparison we consider the following models: MLP, Elman, FCM, eClass [8], eClassB [9], eClassM [9], FAM [2], FMM [11], GFMM [22], and eGNN.

[1] A. Bouchachia, B. Gabrys, Z. Sahel, "Overview of some incremental learning algorithms", IEEE Int. Fuzzy Systems Conf, Jul. 2007, pp: 1-6.

[2] G. Carpenter, S. Grossberg, N. Markuzon, 1. Reynolds, D. Rosen, "Fuzzy ARTMAP: A neural network architecture for incremental supervised Networks, Vol. 3-5, Sep. 1992, pp: 698-713.

[3] B. Fritzke, "A Growing Neural Gas Network Learns Topologies", Ad- vances in Neural Information Processing Systems, 1995, pp: 625-632.

[4] N. Kasabov, Evolving Connectionist Systems: The Knowledge Engineer- ing Approach, Springer, 2n d edition, 2007, 451p.

[5] Da Deng, N. Kasabov, "ESOM: an algorithm to evolve self-organizing maps from online data streams", Proceedings of the IEEE International Joint Conference on Neural Networks, Jul. 2000, pp: 3-8.

[6] N. Kasabov, Q. Song, "DENFIS: Dynamic Evolving Neural-Fuzzy Infer- Vol. 10-2, Apr. 2002, pp: 144-154.

[7] N. Kasabov, "Evolving fuzzy neural networks for super- on Systems, Man, and Cybernetics - Part B, Vol. 31-6,2001, pp: 902-918.

[8] P. Angelov, Z. Xiaowei, F. Klawonn, "Evolving Fuzzy Rule-based Clas- sifiers", IEEE Symposium on Computational Intelligence in Image and Signal Processing, Apr. 2007, pp: 220-225.

[9] P. Angelov, Z. Xiaowei, D. Filev, E. Lughofer, "Architectures for evolving Man and Cybernetics, Oct. 2007, pp: 2050-2055.

[10] P. Angelov, X. Zhou, "Evolving Fuzzy Rule-Based Classifiers from Data Streams", IEEE Tran. on Fuzzy Systems, Vol.16-6, Dec.2008, pp:1462-75.

[11] P. Simpson, "Fuzzy min-max neural networks. Part I: Classification", IEEE Trans. on Neural Networks, Vol. 3-5, Sept. 1992, pp: 776-786.

[12] T. Calvo, J. De Baets, J. Fodor, "The functional equations of Frank and Alsina for uninorms and nullnorms", Fuzzy Sets and Systems, vol. 120, 2001, pp: 385-394.

[13] M. Hell, F. Gomide, P. Costa Jr., "Neurons and Neural Fuzzy Networks Based on Nullnorms", Brazilian Sym. on Neural Net.,Oct.2008,pp:123-28.

[14] W. Pedrycz, "Logic-Based Fuzzy Neurocomputing With Unineurons",  [15] W. Pedrycz, F. Gomide, Fuzzy Systems Engineering: Toward Human- Centric Computing, Wiley-IEEE Press, 1st edition, 2007, 526p.

[16] A. Bargiela, W. Pedrycz, Granular Computing: An Introduction, Kluwer Academic, 1st edition, 2002, 452p.

[17] W. Pedrycz, A. Skowron, V. Kreinovich, Handbook of Granular Com- puting, Wiley-Interscience, 2008, 1148p.

[18] W. Pedrycz, W. Vukovich, "Granular Neural Networks", Neurocomput- ing, Vol. 36, 2001, pp: 205-224.

[19] S. Dick, A. Tappenden, C. Badke, O. Olarewaju, "A Novel Granular Neural Network Architecture", Annual Meeting of the North American Fuzzy Information Processing Society - NAFIPS, Jun. 2007, pp: 42-47.

[20] Y. Zhang, B. Jin, Y. Tang, "Granular Neural Networks With Evolutionary 2008, pp: 309-319.

[21] N. Kasabov, B. Woodford, "Rule insertion and rule extraction from evolving fuzzy neural networks: algorithms and applications for building adaptive, intelligent expert systems", IEEE International Fuzzy Systems Conference Proceedings, Vol. 3-1, 1999, pp: 1406-1411.

[22] B. Gabrys, A. Bargiela, "General fuzzy min-max neural network for Vol. 11-3, 2000, pp: 769-783.

mechanisms and the development of evolving granular neural networks to handle unlabeled data and a mixture of labeled and unlabeled data.

