Elastic Algorithms for Guaranteeing Quality  Monotonicity in Big Data Mining

Abstract? When mining large data volumes in big data  applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g.

prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon?s entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used.  We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a na?ve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.

Keywords?elastic data mining algorithms; quality monotonicity; entropy; R-tree

I.  INTRODUCTION  A. Context and Motivation A key practical challenge facing the analysis of large data  volumes in many applications is that the computation is always required to be conducted under varying resource constraint and time constraints. To deal with such problems, users are usually willing to accept a useful approximate result from their computation that can be produced under the available  budget (whether time or resources). In the context of data mining tasks, approximate results can be produced typically either by restricting the size of the input data fed to exact algorithms, or by using approximating algorithms over full datasets. Within this context, it is appealing to organize the data mining process to support a ?Pay-as-You-Go? model, where users are guaranteed not only a useful approximate result on a tight   * Corresponding author # This author is supported by the Strategic Priority Program of Chinese Academy of Sciences (Grant XDA06010400).

budget but also guaranteed results of better quality if more budget is used. We call algorithms that meet this property elastic algorithms. Informally, an elastic algorithm is one that takes a dynamically changing time (or resource) budget as input and produces a result whose quality, based on some metric, improves monotonically with the budget used.

Several issues need to be addressed when developing budget-aware data mining algorithms. The first is being able to define and use a quality measure to evaluate how good the approximate results are. The second is being able to reason effectively about how using the budget affects the quality of results. The third, and more challenging issue, is to design the data mining algorithm itself so that the quality measure improves as more budget is used. This monotonic improvement should indeed bring consistent increase in observable quality, such as prediction accuracy in classification or recommendation algorithms, as more budget is used.

Various paradigms exist for developing time-adaptive algorithms that operate under time and budget constraints. For example, a wide class of incremental learning algorithms [1, 2, 3] can be used when an entire dataset is not accessible or cannot be processed under limited resources, e.g. memory.

Another wide class of algorithms is known as anytime algorithms [4, 5, 6]. These are interruptible algorithms that are designed to return a valid result whenever the algorithm is interrupted, e.g. when the time allocated for the computation expires. However,  little work has been conducted towards guaranteeing that by conducting more computation (i.e. using more budget) the result quality would indeed improve.

B. Contributions In this paper, we propose a generic approach for developing  elastic data mining algorithms that guarantee quality monotonicity with respect to allocated time budgets. The concrete contributions of the presented work are:  A generic framework for designing quality-monotonic elastic algorithms: We present a generic framework for designing quality-monotonic elastic algorithms. Such algorithms are suitable for a wide class of data mining problems that are based on finding an approximate result by solving an optimisation problem over a dataset. Our framework for designing such algorithms comprises two components: a coding component and a mining component. The coding component applies compression techniques to proactively map a dataset into a set of codes with smaller lengths, i.e., ones that require shorter processing time. By processing a code with suitable length, the mining component can then produce a useful approximate result within a specified time budget. We define two properties for these two components, entropy-      monotonicity for the coding component and entropy- preservation for the mining component. We provide proofs that any data mining algorithm whose components satisfy both properties guarantees quality monotonicity.

Case studies of developing two elastic mining algorithms: We demonstrate the validity of our approach by designing two elastic mining algorithms. The algorithms use standard na?ve k-Nearest-Neighbour (kNN) classification [7] or neighbourhood-based collaborative filtering (CF) [8] over an R-tree coding component [9]. The codes produced by the R- tree are the nodes at different depths that successively approximate the training set at different levels of granularity.

We present experimental evaluation on real datasets to demonstrate that both algorithms exhibit consistent increase in observable quality, i.e., prediction accuracy, in practice.

C. Paper Layout The paper is organized as follows. Section II reviews  related work and discusses its relationship with our work. In Section III, we formulate the elastic mining algorithms targeted by this paper by formally defining their quality measure, two components and associated properties. In Section IV and V, the elastic kNN and CF algorithms are introduced respectively.

Finally, we conclude the paper in Section VI.



II. DISCUSSING RELATIONSHIP TO RELATED WORK At present, many techniques have been developed to  produce approximate results under resource constraints by restricting data size or computations.

Incremental learning is a well-known methodology for machine learning under resource constraints.  The basic idea of incremental learning algorithms is to sequentially process more data to continually update existing acquired knowledge. An incremental learning algorithm benefits from new raw data and integrates it with previously acquired data to refine a learning result. Many incremental learning algorithms such as SVM [1], decision tree [2] and k-means clustering [3] have been proposed. In general, most of existing incremental learning algorithms apply a simple data accumulation strategy (e.g., random sampling from entire data) to accept new data. This means guaranteeing quality monotonicity of results is difficult.

Anytime algorithms provide a generic approach for producing approximate results within time constraints. The basic idea of an anytime algorithm is that once an initial approximate result is produced, the algorithm can be interrupted to output a result at anytime. If the algorithm is not interrupted it continues to incrementally improve its result [4].

The use of the framework has led to implementations of anytime of k-means clustering [5] and kNN classification [6].

In general, anytime algorithms can hide quality decrease by storing and returning the best result obtained so far. They, thus, do not guarantee effective use of extra budget.

Discussing relationships: The approaches discussed above focus on producing approximate results under specific constraints, such as inaccessibility of data or insufficiency of resources in incremental learning algorithms, and the requirement of returning a valid result when interrupted in anytime algorithms. Little work has been done in such methodologies towards guaranteeing quality monotonicity with respect to time (or resource) budget. Our proposed approach  offers a promising methodology for these paradigms by providing a systematic way to guaranteeing such monotonicity.



III. ELASTIC DATA MINING ALGORITHMS  A. Overview Informally, we define an elastic mining algorithm as one  that produces a result whose quality, based on some metric, improves monotonically with the allocated time budget.

Specifically, an elastic algorithm can adapt to a given time budget to produce an approximate result. If extra budget is allocated, the algorithm can build on the acquired result to produce a refined result with improved quality. In this sense, the key property of elastic algorithms is that the quality of results is elastic with respect to the computations. However, in practice, many data mining algorithms do not have such natural structure encapsulating the inherent elasticity. In this section, we provide a formal definition of a theoretical quality measure that can be used for a wide variety of data mining tasks. We then provide a more formal definition of elastic data mining algorithms. We then describe the conditions that must be met by any data mining algorithm to guarantee improving quality with more computations.

B. Definition of Quality Measure We can formulate many data mining problems (tasks) as  optimisation problems. Assume that a dataset  to be mined is drawn randomly from a prior distribution . Given an objective function  where the optimisation variable  , a data mining problem aims at finding a value of  in its domain  that maximises the value of optimisation function.

We call the value of  that gives the maximal value of objective function as the Optimal Result ( ):    Given a fixed objective function  and a domain  in a data mining problem, we define  as the function of dataset , denoted as . Thus, an  is a random variable depending on variable . For example, in the kNN classification problem, the target is to find the  nearest training points of a test point  to predict its class label. Thus, is the dataset to be searched, objective function  is the total distance between these points and , and the  is ?s closest points in .

The execution of an elastic data mining algorithm is constrained by the specified time budget. Given a fixed time budget  and a dataset , the above optimisation problem can be solved approximately to produce a useful result. We define such result as Approximate Result: . Note that similar to an , an  is a function of dataset  and time budget  is also a random variable.

Assuming the prior distribution  of dataset  is known, the prior distribution of the optimal result  is fixed. The uncertainty of an  can be described as the entropy  of the . In other words, we have to obtain at least bits of information to recover a value of  exactly.

Given an algorithm and a value  of  produced by the algorithm, we can calculate the conditional entropy  . This entropy represents the number of bits     required to recover the value of  exactly given an observed value . Thus, we can use the difference between  and  , known as information gain [10, 11], to measure of the quality of value . This quality can be explained as the benefit of the value , i.e., the decreased number of bits for exactly recovering the value of  from a prior state to a state that takes a given value .

Definition 1 (Quality measure of a value  of ): Given a value  of , its quality is defined as follows:    C. Definition of Elastic Data Mining Algorithms A data mining process can be viewed as a process of two  stages. At the first stage, an observation dataset is coded using an assumed base (representation) for the following mining.

Coding components range from transformation (such as wavelets based transformation) or other feature based representation (feature extraction) techniques. The coded data is then inputted into a mining component to solve a specific data mining problems/tasks, such as classification or recommendations. The mining component outputs some results that are meaningful to users such predicted class labels or predicted values of test points. In elastic mining scenario, a large-scale dataset needs to be processed within a limited time (resource) budget to produce some useful approximate result.

The key idea to solve this challenge is to represent the observational dataset in a proper hierarchical base using the coding component so that the mining can be elastic with respect to the time budget, as shown in Figure 1.

Fig. 1. The two components of an elastic algorithm  A coding component is a map from the space of the dataset to a code space. A coding component is lossy if the corresponding map is not injective. The basic idea of lossy coding is to preserve the important information of dataset and remove unimportant parts. The remaining information is then organised using a suitable data structure. A code  is the output of a coding component and is a function of dataset  and length budget , where  denotes the maximal length of a code value that can be processed by the elastic algorithm within the time budget . Thus, a code is a variable depending on variable  and . Given a fixed time budget  and fixed resources, a fixed length budget can be estimated. A code value is selected such that it has the maximum length value smaller than the length budget.  Note that the coding component is only applied once for a value  of dataset to produce a set of code values with various lengths to be used in the elastic algorithm.

Typically, there are two ways of performing compression of a particular dataset : instance reduction and feature reduction. In instance reduction, the number of data points in is reduced. A typical component to index these points in a hierarchy using tree data structures such as quadtree [12], cone  tree [13], and R-tree [14]. At different levels of the tree, the information of the data points can be statistically summarised.

In feature reduction, the feature number of data points is decreased. Many feature reduction techniques such as wavelet, principal component analysis (PCA) and hashing functions [15] can be applied in this type of compression.

A mining component  is designed to solve a specific data mining problem. A mining component  needs to take a code  as input and output an approximate result. Given a mining component and a code value , we assume that the running time of the mining component can be primarily determined by the length of the code value, and this running time should be smaller than or equal to the given time budget  . Hence, the bound of an elastic data mining algorithm?s running time can be controlled by tailoring the length of the inputted code value.

D. Properties of Quality-Monotonic Elastic Algorithm We first define the resolution measure of a code value.

Since the prior distribution  of dataset  is known, the uncertainty of  can be described as the entropy  of the , i.e., the minimal number of  bits information needed to recover the value of  exactly. Given a coding component and a code value  produced by this coding component, the conditional entropy  represents the number of bits required to recover the value of  exactly given an observed code value. Similar to the quality measure, we use information gain, namely the difference of  and , to measure of the resolution of the code value .

Definition 2 (Resolution measure for a code value ): Given a value  of code , its resolution is defined as follows:    To guarantee quality monotonicity, we need a coding component producing codes whose resolution is increasing with their length budget.

Definition 3 (Entropy-monotonicity of a coding component): Fixing a coding component and a value  of dataset , a coding component is entropy-monotonic if for any two length budgets ,  , equivalently,  The entropy-monotonicity property can be explained simply as follows: if a larger budget is given, a coding component will output a more accurate code value.

Furthermore, we define a property of the mining component: if the input code value is of higher resolution, then the output of the mining component will have a higher value of quality.

Definition 4 (Entropy-preserving mining component): Let and  be any two code values. Assume that  (equivalently, ). A mining component  is entropy-preserving if  , equivalently,   Satisfying these two properties, the quality monotonicity of a data mining algorithm is guaranteed.

Theorem 1: Let a data mining algorithm consist of an entropy- monotonic coding component and an entropy-preserving mining component. Fixing a value of dataset, the quality of     approximate value outputted by this algorithm increases monotonically with the time budget.

Proof. Given a value  of dataset , a set of corresponding code values with various length budgets is fixed. Given two time budgets , let  and  be two code values selected according to  and . We have the length of  is shorter than the length of . The coding component is entropy-monotonic,  according to Definition 3. Using the entropy- preserving mining component to process the two code values and , the corresponding two values of approximate results will satisfy  according to Definition 4.



IV. EXAMPLE 1: AN ELASTIC KNN ALGORITHM Based on the above generic framework, we develop an  example elastic kNN algorithm. The core of the algorithms is to use standard na?ve kNN classification (i.e., mining component) [7] over an R-tree coding component [9]. The codes produced by the R-tree are the nodes at different depths that successively approximate the training set at different levels of granularity. In this section, we introduce the two components of the algorithm in Section IV.A and Section

IV.B, and explain how an approximate result is produced in Section IV.C. Finally, we experimentally evaluate the elastic kNN algorithm in Section IV.D.

Note that due to the limit of space, the proofs that the R- tree coding component satisfies the property of entropy- monotonicity and the kNN mining component has the property of entropy-preservation are not presented. In addition, since no random variable is discussed in this section, we omit ?value? for simplicity. For example, a code  denotes a specific code and an approximate result  represents a specific result.

A. The R-tree Coding Component In an R-tree, a node consists of a set of n entries e1,?,en}  where . In a leaf node, each entry  refers to a data point . In a non-leaf node, each entry  refers to one of its child  nodes.  Each R-tree nodes has a Minimal Bounding Rectangle (MBR), which is the minimal bounding rectangle bounds its corresponding data points. Given a dataset , the R-tree coding component indexes all the data points in a set  in a hierarchical way by grouping points with similar attribute values to the same node in order to preserve data similarity. For example, Figure 2(a) shows a dataset  consisting of 12 data points in two-dimensional space and Figure 2(b) shows an R- tree constructed to index these points with three levels (depths).

It can be observed that at depth 0 of the R-tree, the root node represents all 12 points at the coarsest level of granularity. In contrast, the nodes at depth 2 only enclose two or three points.

Hence these nodes represent set  at a finer level of granularity.

Fig. 2. An Example R-tree Coding  For simplicity, we discuss binary kNN classifier with positive class  and negative class  and our result can be extended to multiple classes. Thus, given a training set, the R- tree coding component indexes all training points hierarchically using two R-trees. One positive R-tree indexes training points from the positive class and another negative R-tree indexes training points from the negative class. Thus, one R-tree node summarizes a set of training points from the same class. The output of the R-tree coding component is a set of codes, where each code corresponds to all the nodes at one depth of the two R-trees. These nodes represent the dataset, at specific granularity, to be used as input to the mining component.

B. The Na?ve kNN Classification Algorithm The na?ve kNN classification algorithm [7] classifies a test  point  by linearly scanning all data points in the training set with known class labels and finding the  ones whose distances are the closest to  as its  nearest neighbours. The algorithm then decides ?s class label according to the majority vote of the  nearest neighbours, i.e., assigning  to the class with more nearest neighbours.

C. Calculation of an Approximate Result Before delivering classification services, the elastic kNN  algorithm first applies the R-tree coding component to take a training set and generates a set of codes. Once a time budget is given, the algorithm estimates the length budget of code, i.e., the maximal number of nodes that can be linearly scanned by the na?ve kNN classification method within the time budget. A code  is then selected that it has the maximum length value smaller than the length budget. Using  and a test point , an approximate result  can be produced, which is the  nodes in the code that have the smallest distances to . Using these nodes, ?s class label can be predicted.

D. Experimental Evaluation In this section, we evaluated the property of quality  monotonicity by producing a list of approximate results and demonstrated how their qualities gradually improve when more computations were conducted. The prediction accuracy is applied as the quality measure. Briefly, in a binary kNN classifier, the prediction accuracy denotes the proportion of test points that are correctly classified. We also discussed the connection between an approximate result?s theoretical quality measure (information gain) and observable quality measure (prediction accuracy).

Experimental platform and datasets: We ran all the experiments on a Linux based workstation with four Intel Core i7 processors with a speed of 2 GHz and 4GB of RAM. Two     large datasets selected from the UCI machine learning repository [16] were tested: the skin dataset with 245,057 three- dimensional data points and the gas dataset with 13,910 128- dimensional points. To provide a common benchmark to assess different datasets, we randomly removed 100 points from both datasets to form their test sets and leave the remaining points to form the training set.

Evaluation settings: In our experiments, we constructed two R-trees for each dataset that can provide five code values  to  , which corresponds to nodes at depth 1 to 5 of the R-trees and code value  consists of leaf nodes. The R-tree construction time is 46.65 and 119.86 seconds for the skin and gas datasets, respectively. In all evaluation, we set the value of nearest neighbours =5 and produced five approximate results  to  using code values  to .

Prediction accuracy of optimal result. For both dataset, we first used the na?ve kNN algorithm to linearly scan all the training points to produce the optimal result. The prediction accuracies of the optimal result for the skin and gas datasets are 1.00 and 0.92, respectively.

Discussion on the connection between an approximate result?s theoretical quality measure (information gain) and observable quality measure (prediction accuracy). Since the elastic kNN algorithm guarantees quality monotonicity: the five approximate results  to  have decreasing information gain, i.e., increasing level of approximation to the optimal result. Hence, the observable qualities of these results should gradually approach the qualities of the optimal result.

Figure 3 shows the experiment results, where the x axis lists computational time of producing results  to  and the y axis displays their prediction accuracies. The results display that the elastic kNN algorithm guarantees the monotonicity of quality in all cases: once users conduct more computations, they can get a result with higher accuracy. We can also observe that these values stepwise approach the accuracy of the optimal result.

Fig. 3. Experimental result of the elastic kNN algorithm on the skin and gas datasets.



V. EXAMPLE 2: THE ELASTIC CF ALGORITHM In this section, we introduce another elastic collaborative filtering (CF) algorithm that applies the R-tree data structures as the coding component and the basic neighbourhood-based CF as the mining component.

A. The Basic Neighbourhood-based CF Algorithm Neighbourhood-based CF is one predominant type of CF  algorithms that has been applied in many recommendation systems such E-commerce websites [8]. Formally  given a  training set of  user-item rating matrix with  users and items, the entry  denotes user ?s rating on item  (  and ). The neighbourhood-based CF algorithm has two major phases. Given an active user ?s target item  to be rated, at the first phase, the algorithm calculates the similarity or weight  between user  and any other user  who has rated item  in the matrix. The Pearson?s correlation coefficient is used as the weight metric in this work. At the second phase, the algorithm generates the prediction  of user ?s rating on item  by taking a weighted average of all ratings on item  from user ?s  neighborhood users, , where  is the set of users that have rated item ,  denotes the absolute value of weight .

B. The Two Components in the Elastic CF Algorithm The elastic CF algorithm first applies feature reduction  using incremental SVD [17] to generate a reduced feature matrix of the user-item space, where each user is represented by a vector of a small number of features. The R-tree coding component then hierarchically indexes the user vectors in this reduced where each node summarizes a group of users. All nodes at a given depth of the R-tree represent a code value, based on the original user-item matrix[18], that can be used by the mining component.

Since the neighbourhood-based CF needs users? ratings on different items and their average ratings to generate prediction, each R-tree node is extended to store aggregated rating information of the users enclosed by this node from the original user-item rating matrix. Specifically, in an R-tree node whose  encloses a set  of users? aggregated ratings. For each item , suppose a subset  of users have rated  ( ), node ?s rating  on  and average rating  depending on  can be calculated as: /  and  / , where user ,  is ?s rating on item , is ?s average rating on all items, and  is the number of users in set . Node  has a distinct average rating  for each item  according to the set of users in  that have rated .

The neighbourhood-based CF component takes two inputs: a code value and a target item , and the component outputs an approximate result: all the nodes in code value  that have rated item . Using these nodes, item ?s rating can be predicted.

C. Experimental Evaluation Experimental platform and datasets: We ran all the  experiments on a Linux based workstation with four Intel(R) Xeon(R) processors with a speed of 2.50 GHz and 8GB memory. The MovieLens 1M dataset [19] with 6040 users, 3953 items and over 1 million ratings was tested. We randomly selected 20% users for as test users. For each test user, we withhold a randomly selected set of 20% ratings to form a test set, and tried to predict values of these ratings based on this user?s other observed ratings. Thus, there are 1208 test sets/users, where users? test ratings range from 4 to 370, and observed ratings range from 16 to 1480. In experiment, five code values at depth 1 to 5 of the R-tree were used to produce five values of approximate results  to .

Assessment metric: We use the root-mean-square error (RMSE) [8], a weighted average error, to measure the prediction accuracy of a user ?s test set :  . where  represents the number of items in  set  ,  is item ?s predicted rating and  is its actual rating. Since different users? test sets may have different numerical scales of  values, we employ the relative error (RE) to measure the quality of an approximate prediction. An approximate prediction ?s RE  is the difference between  and  divided by the  [20]: . Since  is fixed for each test set, a lower value of  denotes higher prediction accuracy, i.e., better quality of prediction .

Experiment results: For each test set, five approximate results  to  were produced using codes  to . By testing the 1,208 test sets in the MovieLens dataset, Figure 4(a) displays the distributions of the  for five results using the five basic descriptive statistics in the box plot. Experiment results display that most of  range between -0.10 to 0.10 for each result in both datasets. This means both the mean and variance values of these  are very small. Furthermore, Figure 4(b) shows that the  of the five approximate results  to  gradually reduce to 0. This verifies that as the information gain of results  to  increases, these results stepwise approximate the optimal result.

Fig. 4. Distribution and average  on the MovieLens dataset.



VI. CONCLUSIONS In this paper, we have proposed a generic approach to  guarantee quality monotonicity for big data mining in a "Pay- as-You-Go" computing fashion. This quality monotonicity enables an elastic mining algorithm to produce approximate results whose quality, based on some metric, improves as the allocated time budget increases. We presented a framework for developing such algorithms using two components: coding component and mining component.  We also presented two detailed case studies for designing elastic kNN classification and CF algorithms to demonstrate the applicability of the framework. Our experimental results showed that the two  elastic algorithms indeed guarantee the steady increase of observable quality metrics to time budget.

The framework presented in this paper provides a foundation for developing a wide class of elastic data mining algorithms that operate on large data volumes. Its successful application requires employ appropriate coding components that satisfy entropy-monotonicity. Our future work focuses on investigating other potential coding components including clustering methods such as k-means, and projection techniques such as Locality-sensitive hashing (LSH).

