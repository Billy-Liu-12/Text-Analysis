An Efficient Algorithm for Mining Association Rules for  Large Itemsets in Large Centralized Databases

ABSTRACT  The proposed algorithm is derived from the conventional Apriori approach with features added to improve data mining performance.

These features are embedded in the encoding and decoding mechanisms. It has been confirmed by the preliminary test results that these features can indeed support effective and efficient mining of association rules in large centralized databases. The goal of the encoding mechanism is to reduce the 1/0 time for finding large itemsets, and to economize memory usage in a predictable manner. The decoding mechanism contributes to speed up the process of identifying different items in a transaction.

The performance of three different decoding methods will be compared to demonstrate the potential gain delivered by any ingeniously devised decoding approach.

Keywords: Apriori algorithm, pe$ormance bottleneck, encoding, decoding, association rules  1. INTRODUCTION  In this paper an efficient algorithm for mining association rules [ 1,2] in large centralized databases is proposed. The new algorithm is derived from the conventional Apriori approach [3,4] by adding the encoding and decoding mechanisms. The encoding mechanism reduces the U 0  time in tallying the large itemsets, and naturally economizes memory usage in a predictable manner. The decoding mechanism helps speed up the process of identifying specific items in a transaction. In order to demonstrate the impact by any ingeniously devised decoding approach on data mining performance, three different decoding methods will be empirically compared. The data used in the comparison were test results collected from different experiments performed on the same platform for sequential computation. This platform consists of a Sun Ultra machine, and a centralized database generated by the public IBM package described in [ 5 ] .  The new algorithm can be represented conceptually by the eight steps as follows:  1. LI=[frequent-I itemsets] & /* Read the whole database to identify Level 1's large itemsets L1 and encode every transaction by the rule Vt=Z2' ; X=TRUE*/  2. For(k=2; Lk-l!={};K+ +){ /*Loop until no more large itemset */ 3 Ck=apriori_gen(Lk-l); /*Generate itemsets CK from Lk-1 for level k*/ 4. For(m=O; rn<no-transactions-in-encoded-database; m+ +){ 5. decode-transaction-mfbr_level-k (encoded-database); /*Work on V, value*/ 6 .  count-itemsetsJor-Ck (transaction-m);} /*For finding Lk 's large itemsets*/ 7. Lk=[itemsetsfor-Ck 2Minsup];} /*Identify large itemsets (=insup) for Level k+Lk)*/ 8. Find-association-rulesfor-large-itemsets (Lk); /* From large itemsets at all levels */  encode (whole-database);  0-7803-5731-0/99/$10.00 01999 IEEE lII -905    The highlighted portions in step 1 and step 5 are the encoding and decoding mechanisms respectively. The decoding approach adopted in the latter mechanism is normally dictated by the philosophy implemented in the former. The proposed new algorithm can have many variants, which are characterized by the particular decoding methods incorporated. If the highlighted portions in step 1 and step 5 are deleted, then the new algorithm is reverted back to the conventional Apriori. The operational difference between the two algorithms is that in the new one the database is read only once for encoding purpose, as shown in Figure 1. No reading of the database is required again for the  rest of the data mining life cycle. In the traditional Apriori, however, the whole database must be read once again when a deeper level denoted by Lk+l is mined for large itemsets. The encoding process (encode (whole-database)) transforms a large centralized database into a miniaturized and manageable form. It the transformation process, it produces the encoded value V,=X2' for every transaction in the database, where X marks the physical position of the item in the transaction.

The basic logic for the decoding mechanism (the highlighted code segment) in step 5 is: '( if (V, zZx)P, = V, -2'; transaction9s-x-item = TRUE} 99.

I Very large 1 database  Read database b only once for the encoding  ( Proposed ) f Trac  The database is read for mining large itemsets at  every level f i  3 tiona I algorithm Qpriov  Figure 1. The main difference between the two algorithms is in the U0  An itemset is large because its chance of X ,  Y and X u Y  are large itemsets, and occurrence is greater than or equal to S (b) (support_count_of-XuY/support-count-of_X (minimum support count or Minsup). An 2 minimum confidence C. The symbol 3 association rule X+Y holds provided that: (a) abstracts the relationship R between X and Y.

Ill -906    2. THE NEW ALGORITHM b) c)  divided into three stages: d) The development of the proposed algorithm is  a) evaluate the I/O performance of the conventional Apriori approach,  devise the encoding method, devise efficient decoding methods for the encoding measure above, and test the overall data mining efficacy of the algorithm when the encoding and decoding methods are combined.

2.1 YO Performance Evaluation experiments in which the conventional Apriori algorithm was executed without the code  The aim is prove that an appropriate encoding segment for computation. This deletion of the method is necessary for improving the I/O code segment is illustrated by the absence of performance of the traditional Apriori program statements after step 4 in the following approach. The evaluation was performed with pseudo-program:  1.

2. For(k=2; Lk-]!={};K++){ 4 For(m =O; m <no-transactions-in-encoded-database; m + +){ }; 7.

L~=[frequent-l itemsets] & /*Read whole database to identify Level 1's large itemsets L1 encode (whole-database);  Ck = ap rio ri_gen( Lk-1 );  and encode every transaction by the rule V,=D" ; x=TRUE*/  /*Generate itemsets CK from Lk-1 for level k*/  } /* The computation code segment is deleted from the traditional Apriori */ The results from many evaluation experiments bottleneck of the traditional approach is in the with different database sizes indicate that the I/O operation. Therefore, the bit-encoding conventional Apriori approach consistently method is proposed for inclusion in the new spends more than 70 % of its runtime on I/O algorithm to reduce the number of U 0 accesses. Obviously, the performance accesses.

2.2 The Bit-Encoding Method two objectives: (a) to reduce the number of YO accesses in data mining, and (b) to speedup the  Encoding here means reorganizing and decoding process. Generally speaking, such a transforming a large database into a data encoding process is an essential part of any miniaturized and manageable structure to fulfill typical knowledge discovery approach [6].

2O 2' 22 23 X +  1 2 4 8  Encoded 4ariable (V,=C2')  Table 1. Every item in a transaction is represented by its 2' value (X is a bit's physical position)  IJI -907    The are two mandatory requirements for the position in the transaction. The whole bit-encoding method: (a) the database should be transaction is then represented by its unique read only once within the whole life cycle of encoded value. The bit-encoding concept that data mining, and (b) memory utilization should transforms a large database into its miniaturized be maximized. In this encoding method every and manageable form is exemplified by Table item in a transaction is represented by a 2' 1, where the encoded value for the value, where X marks the item's physical transaction is equal to 2l+ 22 + 23 = 14.

d 500 g400 Q) f300 3  e200  n  r   I E 100  0 1 I I I  + Proposed algorithm  +Other algorithm (4 bytes  zh Other algorithm (2 bytes  for 1 item)  for 1 item)  10 32 64 128 Average no. of items per  transaction  Figure 2. Efficient predictable memory utilization by the proposed encoding method  first  The bit-encoding method also maximizes memory utilization in a predictable manner.

This is achieved for the following reasons: (a) memory usage is economized by encoding items in bits instead of bytes, and (b) bit representation is linear. On the contrary, it is necessary in the conventional Apriori to read and decode items that are encoded in a predefined number of bytes. For verifying the  3. THREE DECODING METHODS  The decoding method can seriously affect the performance of the data mining process. For demonstration of this point, the test results from  claim that memory usage is linear and economical, many experiments were performed with different database sizes generated by the public D3M package [4]. Figure 2 demonstrates the trend pinpointed by the results from these experiments. For the presented case the database had 100,000 transactions (D100K) constructed out of 1000 (N1000) possible items.

three different methods will be compared. The three methods are as follows: a) Basic decoding: This algorithm of the  following logic is executed repeatedly until  III - 908    i, which initialized to the encoded value V,, is reduced 0:  if (i 2 2x) then {i=i-2"; Yh-item-in-transaction = TRUE;  In this approach the encoded value of 14 in Table 1 would represent the "TRUE states" for those items in the 2'=', 2x=2 , and 2x=3 positions of the first transaction. During the decoding operation the encoded transaction is scanned bit by bit starting from the most significant bit.

b) Binary decoding: This approach is based on the basic decoding approach except that the first step is to find "the most significant TRUE position for X7 by binary search. The  X=X-I} else X=X-1  I U  3.1 Test Results  I  The three aforementioned decoding methods were evaluated in different experiments performed on a Sun Ultra machine running on Unix. It was found that the logarithmic  aim is to slash the scanning time of the basic decoding approach by half.

c) Logarithmic decoding: In this approach, which saves decoding time by eliminating the entire scanning process, the following logic is executed repeatedly until i (initialized to the encoded value V,) is reduced to 0:  i = i - 2 integer(log2W .

[integer(logz(i))]'h-item = TRUE The integer(Zogz(i)) operation converts Zogz(i) into an integer by truncating whatever after the decimal point. For example, it yields the integer values of 3,2 , and 1 from the encoded value of Vt = 14 successively.

approach is consistently the most efficient among the three. The databases used in these experiments were generated by the same IBM package [4]. Each test case involved a specific combination of database size and number of items.

Time(sec)  Figure 3. Comparison of four algorithms (T10.14.D100K.N200)  m -909    Figure 3 demonstrates the difference in performance among the four algorithms, namely, the conventional Apriori (4 bytes is used in this case for encoding a data item), and three variants of the new algorithm. Each variant is backed either by the basic, the binary, or the logarithmic decoding method. The database for producing the test data for the comparison was generated by the public IBM package with the following statistics: a) Total number of transactions in the database  (D) is 1 00,000 (100K).

4. CONCLUSION  The aim of in this project is to improve the performance of the conventional Apriori algorithm that mines association rules. The approach to attain the desired improvement is to create a more efficient new algorithm out of the conventional one by adding the encoding and decoding mechanisms to the latter. In order to demonstrate the importance of efficient decoding to high data mining performance, three methods, namely, basic, binary, and logarithmic were evaluated. These three decoding methods were devised with respect to  5. REFERENCES  [I] P. Adriaans and D. Zantinge, Datu Mining, Addison Wesley, 1996.

[2] S.P. Jong, ?Using a Hash-Based Method with Transaction Trimming for Mining Association Rules?, ZEEE Transactions on Knowledge and Data Engineering, Vol. 9, No. 5, September/October 1997, pp. 8 13-825.

[3] R. Agrawal and J.C. Shafer, ?Parallel Mining of Association Rules?, ZEEE Transactions on Knowledge and Data Engineering, Vol. 8, No. 6, December 1996, pp. 962-969.

b) Average number of items per transaction (T) is 10.

c) Total number of items in the database (N) for forming transactions is 200.

d) For large itemsets: the number of transaction patterns is 1000; the average number items in a transaction (I) is 4.

The test programs for collecting the data for Figure 3 were written in C. In the data mining experiments, large itemsets were tallied up to the 8?h level (Lk=8).

the bit-encoding approach that maximizes memory utilization in a predictable manner.

The findings from different experiments have confirmed that the logarithmic decoding method is the most efficient among the three. It can speed up the data mining process significantly as demonstrated in the performance comparison. The imminent future work is to investigate how the logarithmic approach can be applied effectively and efficiently to large-scale distributed data mining, particularly in the Internet environment.

[4] R. Agrawal, T. Imielinski and A. Swami, ?Mining Association Rules Between Sets of Items in Large Databases?, Proceedings of Data (SZGMOD 93), May 1993, pp. 207-216 [5] IBM Almaden Research Center, ?Synthetic Data Generation Code for Association and Sequential Patterns?, http://www.almaden.ibm .comialmaden/projects.html, 1998 [6] ?From Data Mining to Knowledge Discovery, An Overview?, in Advances in Knowledge Discovery and Data Mining, U.M.

Fayyad, G. Piatetsky-Shapiro, P.Smyth, and R.

