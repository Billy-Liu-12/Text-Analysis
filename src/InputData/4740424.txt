Finding Explanations for Assisting Pattern Interpretation

Abstract  We present a novel approach for assisting pattern in- terpretation by data mining end-users: finding explana- tions for association rules based on probabilistic depen- dencies. In the approach, relevant variables are selected from rules and from other data sources to facilitate human- understandable interpretations. An explanation of a rule involves consideration of observable variables in the data and alignment with the conditional probability of the rule.

To build explanations involving multiple, interacting vari- ables, we use Bayesian networks to structure relationships.

We illustrate the benefits of our technique for assisting pat- tern interpretation using Internet use survey data. The novel technique has potential in various data mining scenarios such as computer aided pattern interpretation and interac- tive data mining.

1. Introduction  The task of finding a generic model of empirical data usually falls under the category of learning or induction.

On the other hand, the task of interpreting or explaining a specific pattern or observations is known as abduction ([3], p. 381). In the field of data mining, the task of explain- ing a pattern is usually left to the user; this is the pattern interpretation step in the knowledge discovery cycle.

From a previous study [2], we have seen that when a user seeks to understand a mined rule, they try to elaborate the rule with other possible factors in their mind, form expla- nations and then ?test? whether the data is consistent with their intuition. However, if they want to verify their con- jectures, they have to explore the data manually. The pro- posed method is designed to automatically process the tasks of forming and validating explanations for users.

There has been little research related to explaining a pat- tern computationally. Yao et al. (2003) have argued that the explanation facility of intelligent systems has not received sufficient attention in the data mining community, and they  proposed an explanation-oriented framework for data min- ing [4]. In this paper, an explanation of a rule is constructed as a small Bayesian network, where additional variables are located in between the variables of the left-hand and right- hand side of rules. For example, the rule A? B may have an explanation A ? X ? B where knowing the proba- bility of P (X|A) and P (B|X) can approximate the confi- dence, P (B|A), of the rule. We present our finding of form- ing an explanation, its definition and a simple algorithm for searching for it in Section 2. We test the technique by find- ing explanations for rules mined from Internet use survey data and demonstrate (Section 3) that relevant variables can be identified for interpreting rules.

2 Finding Explanations  The idea of the format of an explanation comes from our experiences in discussing the output of a medical data min- ing project with a domain expert [2]. The medical dataset (from the Australia and New Zealand Dialysis and Trans- plant Registry [ANZDATA1]) a collection of 217,083 clin- ical treatment records of 19,220 kidney disease patients spanning 12.6 years. There are 96 categorical or numeri- cal variables in the dataset.

We illustrate our idea using a mined association rule that states that when a kidney failure patient is not diagnosed with cancer, they are more likely to die of cardiovascular disease:  ?Cancer ? CardiovascularDeath (1)  The domain expert interpreted this rule by giving the fol- lowing explanation: when patients have cancer, they usu- ally die of cancer; in other words, when patients are not diagnosed with cancer they surely do not die of cancer and therefore are more likely to die of cardiovascular disease.

We can formulate the explanation as:  ?Cancer ? ?DieOfCancer ? CardiovascularDeath (2)  1http://www.anzdata.org.au/   DOI 10.1109/WIIAT.2008.330    DOI 10.1109/WIIAT.2008.330     By assuming the data supports the expert?s expla- nation, we expected the inferred condition probability, P (CardiovascularDeath|?Cancer), in Eq. 2 should be able to approximate the conditional probability (confidence) of Eq. 1. We then verified the assumption in the data. The conditional relative frequency in the data is:  P (CardiovascularDeath|?Cancer) = 0.557. (3)  The DieOfCancer attribute was acquired by data trans- formation; we split the CauseOfDeath attribute into DieOfCancer and other attributes. If we apply the chain rule to infer the probability of CardiovascularDeath by setting DieOfCancer as the intermediate variable, we can get a similar conditional probability:  P? (CardiovascularDeath|?Cancer) = P (CardiovascularDeath|DieOfCancer) ?  P (DieOfCancer|?Cancer) + P (CardiovascularDeath|?DieOfCancer) ?  P (?DieOfCancer|?Cancer) = 0 ? 0.714 + 0.538 ? 1 = 0.538. (4)  Therefore, the probabilistic dependencies in the data are consistent with the expert?s explanation. Now we give a general definition of an explanation for an association rule in terms of probability.

Definition 1. ?-Explanations Given an association rule A ? B with confidence = P (B|A), we say a variable X is a factor in an explana- tion if X can approximate the confidence of the rule via the chain rule in probability:  |P (B|A)? P? (B|A)| < ?, (5)  where P? (B|A) = ? X  P (B|X)P (X|A).

The explanation is written A? X ? B.

Definition 1 deals with only three variables: A,B and X .

However, association rules sometimes have more than one variables on the left or right hand side. Furthermore, there is no reason to limit explanations to only one factor at a time.

Therefore we introduce the use of Bayesian networks for modeling explanations.

2.1 Modeling Explanations in Bayesian Networks  Because a Bayesian network is capable of modeling the dependencies amongst variables, and it is not limited to three variables, we introduce the idea of modeling an ex- planation using a Bayesian network.

A Bayesian network consists of a set of nodes and a set of arcs (or links) which forms a directed acyclic graph (DAG). Each node denotes one variable and an arc between two nodes represents the direct dependency between two variables. In the case of discrete variables, a child node2  of an arc is associated with a conditional probability table (CPT) representing its distribution under all values of the parent nodes. A skeleton of a Bayesian network is a graph of the network?s structure where the arcs are not directed.

Since association rules deal only with discrete variables, the Bayesian networks discussed in the paper consist only of nodes corresponding to variables with discrete values.

LetA = {a1, a2, ..., am} be a set of variables (attributes) at the left-hand-side of a rule and B be a variable (attribute) at the right-hand-side of the rule3; X = {x1, x2, ..., xr} be a set of variables (attributes) serving as the factors in an explanation. The definition of a Bayesian networks as an explanation is given below.

Definition 2. Bayesian Networks as ?-Explanations Given an association rule A ? B with confidence = P (B|A), a Bayesian network G with a set of variables X is a ?-explanation if it satisfies the following requirements.

1. X ? (A ?B) = ?. Every xi in X is not in A or B.

2. There is no direct connection between nodes in A and  node B.

3. Every node ai in A connects to one or more nodes in X . There are no arcs among the nodes of A.

4. There are no isolated nodes in X . Each node xi has degree at least two.

5. Let P?G(B|A) be the inferred conditional probability from the Bayesian network G. P?G(B|A) is within ? of the conditional relative frequency in the data:  |P (B|A)? P?G(B|A)| < ? (6)  Therefore, the problem of finding explanations is re- duced to finding a set of intermediate variables X = {x1, x2, ..., xr} and a Bayesian network G that can ap- proximate the conditional relative frequency P (B|A) when a1, a2, ..., am ? A is known. Fig. 1 shows a few examples of the skeletons of Bayesian networks as possible explana- tions; the orientation of arcs can be assigned freely as long as the network G remains acyclic. We define a simple score for explanations based on the closeness of the inferred con- ditional probability:  s =  |P (B|A)? P?G(B|A)| . (7)  An algorithm for finding explanations is proposed in the next Subsection.

2A child node is at the end point of an arc.

3In this paper, we consider only one variable on the right-hand-side.

A X B  A x1 B  x2  }}}}}}}}  A x1 B  x2  }}}}}}}}  a1 X B  a2  ||||||||  a1 x1 B  a2  |||||||| x2  }}}}}}}}  Figure 1. Examples of the skeletons of Bayesian networks as explanations.

2.2 Algorithm  Even though the complexity of searching explanations is exponential4, it is tractable in a small number of variables even using exhaustive search. The question of the suitable size of explanations needs further research; however in the early phase of our development, we have chosen to deal at most two variables in the left-hand-side of a rule |A| = 2 and two factors |X | = 2 at a time. Let G be the set of all possible structures of Bayesian networks for explana- tions. We state a simple algorithm for finding possible ex- planations for an association rule in Algorithm 1. The algo- rithm substitutes all possible combination of variables of X into a collection of possible structures (DAGs) of Bayesian networks G, then learns the conditional probability tables (CPTs) from data in the step LearnCPT(?, ?); finally the al- gorithm estimates whether the Bayesian network satisfies Eq. 6.

3 Experiments  In this section we describe an empirical test of our method of finding explanations on Internet Use survey data.

The data comes from Graphic, Visualization, and Usability Center?s (GVU) 8th WWW User Survey 5 which is avail- able online for analysis. The dataset is a tabular data con- sists of 10181 instances of 71 categorical variables. Sup- pose we are interested in the reason why users are not will- ing to pay to access a site; we select 10 variables (Table 1) as a subset of data for mining association rules. Two mined rules are shown in Table 2.

We select rule (1): NotPayReason = PreferFreeSources -> Gender=Male and  4The complexity of finding explanations is no worse than exhaustive structure learning of Bayesian networks ([1], chap. 6).

5http://www.gvu.gatech.edu/user surveys/survey-1997-10  Input: D: data, G: A collection of the structure of Bayesian networks, r: An association rule, ?: threshold for acceptance  Result: Er: A set of explanations Er ? ?; A ? r.A; B ? r.B: Attributes of the rule; a? r.a; b? r.b: Values of variables; forall G ? G do  forall X ? X ?X /? A ?X 6= B do Build a Bayesian network G; LearnCPT(G,D); Compute PD = P (B = b|A = a) from data; Compute P? = PG(B = b|A = a) from G; if |PD ? P? | < ? then  Append G to Er; end  end end return Er  Algorithm 1: Algorithm for finding explanations.

find its explanation to see possible factors for this rule. The probability distribution of the rule is listed below.

NotPayReason P(Male) P(Female) PreferFreeSources 0.637 0.363  One explanation for the rule is plotted in Fig. 2 that ever ordering products or services using the web (OrderViaWeb) is a factor. The conditional probabilities of the explanation are listed below.

NotPayReason P(OrderViaWeb=Yes) P(OrderViaWeb=No) PreferFreeSources 0.78 0.216  OrderViaWeb P(Male) P(Female) Yes 0.662 0.338 NO 0.513 0.487  At first pass, our interpretation of the explanation is: if users are unwilling to pay for content because of other free content sources, they are also likely to have the experience of ordering products or services using the web; and when the users have experience in web ordering, they are likely to be male. Altogether, the factor ?OrderViaWeb? could be an intermediate factor when we reason the confidence of the rule.

The second result (Fig. 3) is an explanation for rule (2) that when male users? occupation is computer-related, they are more likely to find free sources of information than pay for it. The method finds that the user?s actual job and his experience in creating web pages can be the intermediate     Table 1. Selected Attributes for data mining.

Attribute Meaning Gender Female or male.

Income User?s annual income.

MaritalStatus Current marital status.

NotPayReason The main reason that a user would not  be willing to pay to access a site.

YearsOnInternet For how long the user has been using  the Internet.

Occupation Primary occupation.

Job Actual Job.

Education Highest level of education completed.

OrderViaWeb Ever ordered a product/service by fill-  ing out a form on the web.

WebPageCreation Ever created a web page.

Table 2. Two mined rules.

rule support confidence (1) NotPayReason = PreferFreeSources -> Gender=Male  0.101 0.637  (2) Gender=Male, Occupation=Computer -> NotPayReason = PreferFreeSources  0.102 0.539  variables for explaining the rule. We omit the tables that display the distribution among variables in the explanation for brevity.

4 Conclusion  We have introduced a new approach for assisting pattern interpretation: finding explanations. The proposed method is designed to automate the tasks of forming and validating explanations for users. Note that the proposed approach is limited to recover associations rather than causation. One of the reasons is in the statistically equivalent structures of Bayesian networks (see [1], chap. 6.3.1). For brevity, we have not discussed the interactions between the generated explanations and end-users, but we expect the generated ex- planations will help users associate related factors as expla- nations; users can then compare the possible explanations with their knowledge.

To summarise, the idea of modeling an explanation in probabilistic dependency is novel. We have reported a pre- liminary solution for the task in the paper. There are still many problems to address such as the suitable size of an explanation, explaining the conditional probability tables in an explanation, a theoretically sound measure for choosing the best explanation(s) and a heuristic method for speeding up the search of explanations.

NotPayReason // OrderV iaWeb // Gender  Figure 2. One explanation for the rule NotPayReason = PreferFreeSources -> Gender = Male. Score of explanation s = 976.46.

Gender // Job // NotPayReason  Occupation // WebPageCreation  55kkkkkkkkkkkkkkk  Figure 3. One explanation for the rule Gender=Male, Occupation=Computer -> NotPayReason = PreferFreeSources.

Score of explanation s = 192.6.

Acknowledgement Special thanks to Dr. Adrian Pearce who jointly discussed the critical ideas of the paper and proofread the manuscript.

We are also grateful to Dr. Kathy Paizis for her discussion on pattern interpretation from a domain expert?s point of view.

