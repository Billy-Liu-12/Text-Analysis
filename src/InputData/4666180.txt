An Efficient Data Structure for Mining Generalized Association Rules

Abstract   The goal of this paper is to use an efficient data structure to improve our earlier research. In the earlier research, we attempted to find the generalized association rules between the items at different levels in the taxonomy tree under the assumption that the original frequent itemsets and association rules were generated in advance.

In the paper, we proposed an efficient data structure called a frequent closed enumeration table (FCET) to store the relevant information using a well-known algorithm. It stores only maximal itemsets, and can be used to derive the information of the subset itemsets in a maximal itemset through a hash function. From experimental results, we found that the combinations of FCET and the hash function not only save spaces, but also speed up producing the generalized association rules.

1. Introduction   In the previous research, we have used a VTV table to replace the whole original database. Although a VTV table demonstrated superior speed in operations, it required a large amount of storage spaces, even much larger than the original database, and therefore it is unsatisfactory in real applications. In the paper, we proposed an efficient data structure which substituted for the VTV table and applied to the GMFI and GMAR algorithms in the previous research. The efficient data structure called the frequent closed enumeration table (FCET) could reduce considerable storage spaces, and cooperate with a hash function to speed up execution time.

Given a taxonomy tree, we aimed to obtain generalized association rules from the original frequent itemsets or the original association rules. Beforehand, the preprocessing was done to convert the original frequent itemsets into an FCET data structure, because the transaction information of those frequent itemsets should be recorded in the  FCET. In the taxonomy tree, the leaf nodes not appearing in the original frequent-1 itemsets are called infrequent itemsets. The infrequent itemsets must be identified to learn generalized information. Here, to construct the infrequent itemsets table, we used existing clustering technologies, and then used the same algorithm as constructing the FCET.

Finding generalized association rules in databases is the fundamental operation behind several common data- mining tasks [12]. Generalized association rule finding is a very important problem in the data mining field with numerous practical applications, including consumer market basket analysis, inferring patterns from web page access logs, and network intrusion detection. Currently, data mining is arising to understand, analyze, and use these data. Also, data mining is designed for finding samples of interest in a large database, and thus it is also a central part of knowledge exploration [1][2].

In data mining, the most well-known method is the Apriori algorithm [1]. Since the Apriori algorithm is costly when finding candidate itemsets, there are many variants of Apriori that differ in how they check candidate itemsets against the database. For example, Park et al. [4] enhanced the Apriori algorithm with a hashing scheme that can identify those candidates that will turn up infrequently when checked against the database. Zaki et al. [7] presented the algorithms MaxEclat and CHARM for identifying maximal frequent itemsets. These algorithms are similar to Max-Miner [11] which also attempt to look ahead and identify long frequent itemsets early, in order to help prune the spaces of considered candidate itemsets. Zaki et al. also used the diffsets concepts [10] to save storage space while checking against in dense databases. In addition, some researches have made use of transaction reduction techniques, including partitioning proposed by Savasere et al. [5], to generate frequent itemsets quickly. Furthermore, a data structure called FP-Tree was proposed by Han et al. to produce frequent itemsets directly, but not candidate itemsets [3].

DOI 10.1109/FSKD.2008.609     Up to now, all proposed research can mine all association rules only from scratch. In this paper, our goal is to mine generalized association rules without re- scanning the original database. Here, the original frequent itemsets generated in advance can be used to provide users with mine generalized association rules, instead of the original database. We hope to only store the maximal itemsets, rather than all the frequent itemsets; however only storing the maximal itemsets may lead to a loss of information, since all subset frequencies are not available again. Thus, here we propose an efficient data structure called frequent closed enumeration table (FCET) to store all information.

To be followed up, the problem of mining association rules is defined in Section 2. Then, we proposed an efficient data structure FCET and infrequent closed enumeration table (IFCET) in Section 3, where it not only saves a large amount of storage spaces, but also speeds up mining time. In Sections 4, we employed an FCET index tree to combine multiple FCETs. In Section 5, several experimental results show the superiority of FCET over other data structures. Finally, we make conclusions in Section 6.

2. Problem Descriptions   An association rule is an implication of the form X ? Y, where X ? I, Y ? I, and X ? Y = ?. If c% of the transactions in D support X and Y, we say that X ? Y holds in the transaction database D with confidence c% The definition of Support(X) is the number of transactions purchasing X, divided by the number of transactions in the database. The definition of Conf(X ? Y) is the support of both antecedent and consequent divided by the support of antecedent in the rule.

Definitely, we can find association rules from scratch, given a large transaction database D. However it is unavoidable to scan the original database once more and this is very inefficient. Thus, the efficient data structure proposed here makes use of the original frequent itemsets that have already been generated beforehand, to produce new association rules, rather than re-scanning the original database. In a very large database, rescanning the database to mine specified association rules is very inefficient and unnecessary. Some researches have employed the vertical-TID-vector table (VTV) to store the database, which uses bit vectors to record the transaction information. Each item is represented by a bit vector where the length of the bit vector is the total number of transactions in the database. If an item appears in the jth transaction, the jth bit of the corresponding bit vector is set to 1; otherwise, the bit is set to 0. Because it uses bit- wise operations, it is very efficient when calculating support counts. However, it is not realistic for large-size data sets. For example, a date set with 10,000 items and  1,000,000 transactions will require 10,000,000,000 bits.

This size is much larger than the original data set. Thus, we propose an efficient data structure which only requires a small amount of storage space, instead of re-scanning the original database. In addition, we used a perfect hash function [9] to help us quickly find support counts.

3. Mining Algorithms  3.1 Processing Flow of Mining Algorithms   As shown in Fig. 1, the original VTV table used in our previous studies has been replaced with more efficient data structures; i.e., FCET and IFCET. The components shown inside the dotted box, such as original frequent itemsets, original association rules, FCET, and IFCET are generated beforehand. Rather than scanning the original database, we make use of the FCET and IFCET transformed from the original database and original frequent itemsets to mine generalized association rules.

Fig. 1. Processing flow of mining algorithms   3.2 Frequent Closed Enumeration Table (FCET)  Here, we give a property used in the FCET as follows.

Property 1: The frequent itemset is called maximal if it is not a subset of any frequent itemsets; any nonempty itemset is a frequent itemset if and only if it is a subset of the maximal itemsets.

During frequent itemset generation, we can build a frequent closed enumeration table using generated frequent itemsets, in order to reduce required disk space and speed up the mining operation. According to Property 1, since all subsets of a maximal frequent itemset must also be frequent itemsets, we use an FCET to store all the frequent closed itemsets where an FCET is indexed by a maximal frequent itemset.

In an FCET, we not only use a bit (or employ the technique called diffsets) to reduce the memory footprint of intermediate computations, but also use a perfect hash     function to rapidly get the support count and the TID set of a frequent itemset. Since we can use the perfect hash function to easily calculate the position of each subset of a maximal itemset, no subsets need to be stored in an FCET. For an FCET, two extra sets are used; i.e., a maximal itemset, and the TID set which is the union of all TID sets for each item in the maximal itemset. For example, for each item in a given maximal itemset {ACTW}, if item A appears in transaction 3, 5, 6, 7, item C in transaction 3, 4, 5, 6, 7, 8, item T in transaction 3, 5, 7, 8, and item W in transaction 3, 4, 5, 6, 7, then we keep two extra sets for the FCET; i.e., the maximal itemset {ACTW} and the TID set {3,4,5,6,7,8} which is the union of {3,5,6,7}, {3,4,5,6,7,8}, {3,5,7,8}, and {3,4,5,6,7}.

In addition to the two extra sets just mentioned, an FCET still contains other information such as inverted bits, next pointers, and SUB_TID sets. For a subset of the maximal itemset, the inverted bit is used to express whether the transactions containing the subset are the complement of the corresponding SUB_TID set. The next pointer is used to point to the location of the next superset of the current subset while calculating the support count.

The SUB_TID set stands for the transactions containing the subset or not, based on the inverted bit. For the transaction database and the generated frequent itemsets as shown in Fig. 2(a) and. 2(b), the maximal itemsets are {ACTW} and {CDW}, and the FCET of {ACTW} can be built as shown in Table 1. As a matter of convenience in dealing with redundancy conditions in FCET, we use a single sentinel to represent NIL. For a FCET, the sentinel is an object with the same fields as an ordinary structure in the FCET. Its inverted_bit field is 0, next_pointer field is NIL and SUB_TID field is ?. All the redundant itemsets are replaced by pointers to the sentinel nil[FCET]. Here, we do not store the repeat itemsets, for example, in the maximal itemsets {ACTW} and {CDW}, where [{C},{W},{CW}] are repeat itemsets, but we just store one copy in FCET of {ACTW} and the repeat itemsets in FCET of {CDW} then replaced by pointers to the sentinel nil[FCET].Hence, the repeat itemsets all are occupy the same memory. The purpose that we do it in this way is mainly to save the storage space and increase the search efficiency.

Fig. 2. (a) Transaction database (b) Original frequent itemsets with minimum support count 3 (c) Vertical-TID- database --------------------------------------------------------------------- Algorithm FCET: Set k the maximal length of frequent itemsets Set M the set of maximal frequent itemsets Set TS the set of transactions of all items in the maximal itemset MS Generate_Frequent_Closed_Enumeration_Table( ki 1=? = Fi) 1: j=k+1; M=?; i=0; 2: while j?0 3:{  j=j-1; 4:    do select S?Fj that S is not subset of M 5:   { MS=S; i=i+1; 6:     TS=TS?{ ki 1=? =Si.TID#_ Where S={s1,s2,s3,?sk},k=len(S)}; 7:     M=M?MS; 8:     create the corresponding FCETi ; 9:     initialize inverted_bit=0; next_pointer=NIL- SUB_TID=?; 10:   } 11:   else S?Fj is a subset of  k ji 1+=? =Fi  12:   {if the support of S is equal to the support of Fi then 13:      { get the index number k1 of S within the FCETi - using the  perfect hash function; 14:         get the index number k2 of superset of S within- the FCETi  where its next pointer=NIL; 15:         FCETi[k1].next_pointer=FCETi[k2]; 16:        } 17:       else 18:         { get the TID# of S from Vertical-TID-database; 19:            get the index number k1 of S within FCETi- using the  perfect hash function; 20:              if the length of S.TID# > 1/2 length of TSi then 21:              {   FCETi[k1].inverted_bit=1; 22:                   FCETi[k1].SUB_TID=TSi-S.TID#; 23:               } 24:              else 25:                       {   FCETi[k1].inverted_bit=0; 26:                            FCETi[k1].SUB_TID=S.TID#; 27:                         } 28:                    } 29:                } 30:         } 31:         return mi 1=? = FCETi, where m is the number of maximal itemsets ---------------------------------------------------------------------  Fig. 3. Building FCET from original frequent itemsets.

In the FCET1 for the maximal itemset {ACTW}  shown in Table 1, we create 15 subset entries according to line 8-9 in Fig. 3. N.B., its ms1 and ts1 can also be found according to line 5-6. Here, no frequent itemsets are     stored, but they can be found using the perfect hash function [9] as shown in equation (1).

+?+= ++?  ?  ?  =  ?  ?  ?  = ?? )CC(C )i(XT iL L  i  )i(XT  iL  L  i  T  i HF   (1)     The hash function used here can be expressed as follows: T: the length of a maximal itemset, L: the length of a subset itemset X(i): the position in a maximal itemset for the ith item of a subset itemset, where 1 ? i ? L and X(0)=0 HF: the position of a subset item in a maximal itemset.

For the maximal itemset {ACTW} shown in Table 1, if we want to find itemset {TW} with X(0)=0, X(1)=3, and X(2)=4, we use the hash function above to obtain HF=10. Because the support count of {TW} is equal to those of its supersets {ATW}, {CTW}, and {ACTW}, it is not a frequent closed itemset. Therefore, according to line 13-15, we should follow its next pointer and move to the location of its superset to find its support count. For another example to find itemset {T}, we obtain HF=3 using the hash function, and also follow its next point and move to the location of its superset, since {T} is also not a frequent closed itemset. Owing to FCET1[8].inverted_bit=1, the TID of {T} should be {3,4,5,6,7,8}-{4,6}={3,5,7,8} according to line 20-23.

Table 1. FCET1 for the maximal itemset {ACTW}, where ?=NIL   ms={ACTW}  ts={3,4,5,6,7,8}  index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  inverted _bit  0 1 0 0 0 0 0 1 1 0 0 1 0 0 0  next_poi nter  12 ? 8 9 12 15 12 ? ? 15 15 ? 15 15 ?  SUB_TI D  ? ? ? ? ? ? ? 4  8 ? ? 4  ? ? 3    3.2 InFrequent Closed Enumeration Table (IFCET)   Here, we also give a property used in the IFCET as follows.

Property 2: All nonempty supersets of an infrequent itemset must also be infrequent.

The infrequent closed enumeration table (IFCET) has the same skill as the FCET uses, but the itemsets in the IFCET are infrequent itemsets. The reason to store them is that we cannot calculate the support count of generalized itemsets only from the FCET, for a taxonomy tree. For the example as shown in Fig. 4, when we calculate the support count of item T5 (i.e., T5.TIDs=(M.TIDs)?(N.TIDs)) where item M and N are infrequent itemsets, we must construct the IFCET to store the information of infrequent itemsets.

Fig. 4 Taxonomy tree We can build up the IFCET based on Property 2. For  the example taken from Fig.1, item M, N, X, and Y are all infrequent items, and we can use the existing clustering technique to classify them according to the transactions where they appear. Then, the IFCET can be built up according to the clustering results. In the IFCET, we only store infrequent-1 items and maximal infrequent itemsets, since the information of infrequent-k (k>1) itemsets is helpless when counting the support of non-leaf nodes in the taxonomy tree. Thus, the required memory for the IFCET is not exponentially large. For example, the IFCET for the infrequent itemset {MN} is shown in Table 2 and the generalized association rules generated from Fig.2 and Fig. 4 are shown in Table 3.

Table 2. IFCET1 for the infrequent itemset {MN}, where ?=NIL  cluster1.ims={MN}  cluster1.ts={1,2} index 1 2 3 inverted_bit 1 0 0 next_pointer 3 ? ? SUB_TID ? 1 1      Table 3. The generalized association rules generated from above database and taxonomy tree  Generalized association rules  confidence Generalized association rules  confidence  A==>T4 0.75 C,W==>T1 1 A==>T3 0.75 C,T1==>W 1 C==>T1 1 W,T1==>C 1 C==>T3 1 T==>W,T1 0.75 T3==>C 0.8 W==>T,T1 0.75 D==>T4 0.75 T,W==>T1 1 D==>T3 0.75 T,T1==>W 0.75 T==>T1 1 W,T1==>T 0.75 W==>T1 1 C==>T1,T3 1 T4==>T1 1 T3==>C,T1 0.8 T1==>T4 0.83 C,T1==>T3 1 T1==>T3 0.83 C,T3==>T1 1 T3==>T1 1 T1,T3==>C 0.8 A==>C,T3 0.75 C==>T,W,T1 0.75 C==>A,T3 0.75 T==>C,W,T1 0.75 A,C==>T3 1 W==>C,T,T1 0.75 A,T3==>C 1 C,T==>W,T1 1 C,T3==>A 0.75 C,W==>T,T1 0.75 C==>T,T1 0.75 C,T1==>T,W 0.75 C==>T,T1 0.75 T,W==>C,T1 1 T==>C,T1 0.75 T,T1==>C,W 0.75 C,T==>T1 1 W,T1==>C,T 0.75 C,T1==>T 0.75 C,T,W==>T1 1 T,T1==>C 0.75 C,T,T1==>W 1 C==>W,T1 1 C,W,T1==>T 0.75 W==>C,T1 1 T,W,T1==>C 1 C,T1==>T,W 0.75 C,T1==>T,W 0.75   4. FCET Index Tree   In this section, we employ a signature tree called the SG-tree [16][15] to combine multiple FCETs. It is a dynamic balanced tree similar to R-tree. Here, we call it an FCET index tree. In the tree, each internal node represents a logical OR with all its children, and contains an entry of the form <sig, ptrs>. The root has a ?NULL? signature and contains all the items in maximal itemsets.

In the leaf node, sig is the signature of an FCET, and ptrs point to the FCETs sharing the signature. In other words, the signature of each internal node contains all signatures in the subtree pointed by itself. The index tree can help in finding maximal itemsets very quickly. As shown in Fig.

5, if we want to find itemset {BH} with corresponding bit string 0100000100, we start searching from the root. After comparing with the left child, we find (1111110000 and 0100000100)?0100000100, so we compares with the right child (0100001111 and 0100000100)=0100000100.

Then downgrading to the next level, we continue to  compare the left child (0100001100 and 0100000100)=0100000100, and finally we reach to the FCET of {BGH}. Then, we can use the hash function expressed in equation (1) to get the support count of {BH} in the FCET of {BGH}. The total time complexity is O(log2n)+O(1), where n is the number of maximal itemsets.

Fig. 5. FCET index tree    5. FCET Partition Tree for a Long Pattern   In this section, we also propose the partition-and- merge method to solve a long pattern problem [12]. If the length of a maximal itemset is 40, we may build 240-1 cells in an FCET; however, this is unrealistic. Instead, 240 cells are partitioned into 210+210+210+210, where it is feasible to fit 210 cells in the memory. Finally, we merge these cells using an intersection operation. For example, if we want to find the support count of {IJKL}, we reach to the node with signature {11111111111111110000000}.

Because the length of the maximal itemset is too large (here we assume the maximal length within a node is 10), the node is designed to point to a partition tree, as the dotted box shown in Fig. 6. Then the SUB_TID of {IJ} will be found in the FCET0_1, and the SUB_TID of {KL} found in the FCET0_2. Finally, if SUB_TID(IJ)=(1,3,5,7,9) and SUB_TID(KL)=(1,2,4,7,9), the return result will be {1,7,9} with support count=3.

Fig. 6 FCET partition tree   6. Performance Evaluations     6.1 Simulation Model   In the section, we evaluate the performances of four data structures for algorithm Apriori [1][2], MaxEclat [12], CHARM [10], and FCET on a DELL GX 270 with Intel Pentium 4 3.2Ghz and 1 GB main memory running Windows XP. All the experimental data are generated from a normal distribution. The relative simulation parameters are shown in Table 4. To make our dataset representative, we generate two types of databases in the experiments; i.e., DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300, whereas each item in the SPARSE database is randomly generated from a pool N (i.e., the set of all the items) with size 1000. Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support. In addition, we use the notations T for the average number of items per transaction, I for the average number of items in a frequent itemset, and D for the number of transactions. For example, the experiment labeled with T10.I3.D1K represents the simulation environment with 10 items on the average per transaction, 3 items on the average in a frequent itemset, and 1000 transactions in total.

Table 4 Simulation parameters with default values  D Number of transactions 100,000 T Number of the items per transaction 5-40 P Number of potentially frequent  itemsets  I Number of the items in a frequent itemset  2-5  N Number of items 1000 R Number of taxonomy trees 31-75 L Number of levels in a taxonomy tree 3  6.2 Experimental Results  Experiment 1: In this experiment, we explored the required storage spaces of Apriori, CHARM, MaxEclat, and FCET in the environment T10.I5.D10K under different minimum supports in SPARSE databases (Fig.

7). We found that the FCET saves almost 10 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET only stores maximal frequent itemsets, rather than the frequent itemsets.

T10I5D10K         0.02 0.01 0.007 0.005  minimum support  st or  ag e sp  ac e (b  yt es  ) FCET  MaxEclat  Apriori  CHARM  Fig. 7 Required storage spaces in SPARSE databases  Experiment 2: In this experiment, we also explored the required storage spaces of Apriori, CHARM, MaxEclat, and FCET in the environment T40.I20.D10K under different minimum supports in DENSE databases (Fig. 8).

We found that the FCET saves almost 20 times as much space compared with Apriori, especially under smaller minimum supports. The reason is that the FCET can store the maximal itemsets generated in DENSE databases more efficiently than those in SPARSE databases.

T40I20D10K            0.02 0.01 0.005 0.003 0.002  minimum support  sto ra  ge sp  ac e (b  yt es  ) FCET MaxEclat Apriori CHARM  Fig. 8 Required storage spaces in DENSE databases  Experiment 3: In this experiment, we explored the execution time of BASIC [17], Cumulate [17], GMFI [9], and GMAR [9] algorithms in the environment T10.I5.D10K/T40.I20.D10K under different minimum supports, as shown in Fig. 9 and 10, respectively. From Fig. 9 and 10, we found that GMFI/GMAR [9] employing the FCET perform better than the other ones in either SPARSE databases or DENSE databases.

3 2 1 0.75 minimum support (%)  ex ec  ut io  n tim  e (m  in ut  es )  GMFI GMAR BASIC Cumulate  Fig. 9 Mining time in SPARSE databases            3 2 1 0.75  minimum support (%)  ex ec  ut io  n tim  e (m  in ut  es )  GMFI GMAR BASIC Cumulate   Fig. 10 Mining time in DENSE databases   7.  Conclusions   Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules.

The time complexity to find the maximal itemsets is O(log2n) where n is the total number of maximal itemsets. For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results. Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases.

7.    Conclusions   Through several comprehensive experiments, we found that the FCET and IFECT can save a larger amount of storage spaces than Apriori, MaxEclat, and CHARM in both SPARSE databases and DENSE databases. Since the FCET stores fewer elements for a long pattern, when matched with GMFI/GMAR algorithm, it also revealed efficient execution time than BASIC and CUMULATE in mining generalized association rules.

The time complexity to find the maximal itemsets is O(log2n) where n is the total number of maximal itemsets.

For a long pattern, we used a partition tree to count the SUB_TID of itemsets, and then got their merged results.

Although the memory required for the FCET is still exponentially large, through limiting the size of maximal itemsets and the size of clustering to a reasonable memory requirement, we do save a large amount of storage spaces, especially in dense databases.

8. References  [1] R. Agrawal, T. Imielinski, and A. Swami,  ?Mining association rules between sets of items in large Management of Data (1993), pp. 207-216.

[2] R. Agrawal and R. Srikant,  ?Fast algorithms for mining association rules,? Proc. 20th International Conference on Very Large Data Bases (1994), pp.487- 499.

[3] Jia-Wei Han, Jian Pei, and Yi-Wen Yin, ?Mining frequent patterns without candidate generation,? Proc.

(2000), pp. 1-12.

[4] J.S. Park, M.S. Chen, and P.S. Yu, ? An effective hash-based algorithm for mining association rules,? Proc.

(1995), pp.175-186.

[5] A. Savasere, E. Omiecinski, and S. Navathe, ?An efficient algorithm for mining association rules in large Large Data Bases (1995), pp.432-443.

[6] Yin-Fu Huang and Chieh-Ming Wu, ?Mining generalized association rules using pruning techniques,? (2002), pp.227-234.

[7] M.J. Zaki and C.J. Hsiao, ?Efficient algorithms for mining closed itemsets and their lattice structure, ?  IEEE Transactions on Knowledge and Data Engineering, vol.

17, no. 4, April (2005), pp. 462-478.

[8] D. Burdick, M. Calimlim, and J. Gehrke, ?MAFIA: a maximal frequent itemset algorithm for transactional Engineering, (2001), pp.443-452.

[9] M.J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li, ?New algorithms for fast discovery of association rules,? Discovery in Databases and Data Mining, (1997), pp.

283-286.

[10] M.J. Zaki and K. Gouda, ?Fast vertical mining using Knowledge Discovery and Data Mining, Aug. (2003).

[11] N. Mamoulis, D.W. Cheung, and W. Lian, ?Similarity search in sets and categorical data using the Data Engineering, (2003).

[12] R. Srikant and R. Agrawal,  ?Mining generalized Very Large Data Bases, (1995),  pp.407-419.

