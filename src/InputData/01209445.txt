

A Fast Association Rule Algorithm Based On Bitmap and Granular Computing  T.Y.Lin Xiaohua Hu Eric Louie Dent. of Comnuter Science Colleae of Information Science IBM Almaden Research Center  I  San Jose State University Drexel University San Jose, California 95192 Philadelphia, PA 19104 tylin@cs.sjsu.edu thuOcis.drexel.edu  Abstracf-Mining association rules from databases is a time- consuming process. Finding the large item set fast is the crucial step in the association rule algorithm. In this paper we present a fast association rule algorithm (Bit-AssoRule) based on granular computing. Our Bit-AssocRule doesn?t follow the generation-and-test strategy of Apriori algorithm and adopts the divide-and-conquer strategy, thus avoids the time- consuming table scan to find and prune the itemsets, all the operations of finding large itemsets from the datasets are the fast bit operations based on its corresponding granular. The experimental result of our Bit-AssocRule algorithm with Apriori, AprioriTid and AprioirHybrid algorithms shows Bit- AssocRule is ?. to 3 orders of magnitudes faster. Our research indicates that bitmap and granular computing can greatly improve the performance of association rule algorithm, and are very promising for data mining applications.



I. INTRODUCTION An association rule in a transaction database is an expression X+ Y, where X, and Y are sets of items, XnY = 4 [1,2]. Given a set of transaction D, the problem of mining association rules is to generate all association rules that meet certain user-specific minimum support and confidence. The problem can be decomposed into two subproblems: (1) finding all combinations of items that have transaction support above the minimum support, (2) use the large itemsets to generate the desired rules. A lot of association rule algorithms have been developed in the last decades [1,2,3,4,6,9,11,12,13,14], which can be classified into two categories: (1) candidate-generation-and-test approach such as Apriori [2], (2) pattern-growth approach [6,12,13]. The challenging issues of association rule algorithms are multiple scan of transaction databases and huge number of candidates.

In this paper we present a novel association rule algorithm Bit-Assoc based on bitmap and granular computing approach. Traditional Apriori algorithms require full table scan and multiple passes of the itemsets in order to finding association rules from large database. Our Bit-AssocRule avoids these time-consuming operations and relies on the fast bit operations of its granular to find the large itemsets.

With bitmap techniques, we can greatly improve the performance of the association rule algorithms.

The rest of the paper is organized as follows: we discuss granular computing and bitmap techniques in Section 2. In Section 3 we present the granular-based association algorithm Bit-AssocRule and the comparison results of Bit- AssocRule with various Apriori algorithms (Apriori,  650 H a n y  Road San Jose, C A  95120 ewlouie@almaden.ibm.com  AprioriTid and AprioriHybrid). Section 4 concludes the paper with some discussions  11. GRANULAR COMPUTING AND BITMAP TECHNIQUES Granular computing was first proposed by TY Lin [7] and bas become a very important tool in data mining since then.

A granule is a clump of objects drown together by indistinguishability, similarity, proximity or functionality.

The equivalent relations are the granules of the relation.

Each unique attribute value in the relational table is a granule, and each granule is a list of tuples that have the same attribute value. Only the tuple name, or the reference to the tuple, is stored in the granules.

The bitmap technique was proposed in the 1960?s [SI and bas been used by a variety of products since then. Bitmap technique delivers far superior query performance on unselective (low cardinality) data than traditional B-Tree indexing techniques. Bitmap represents each distinct value as arrays of bits where a 1 or 0 in each relative position in the array represents True or False for that value for the corresponding relative record within the database relational table. This approach is sometimes referred as inverted-list.

But the real benefit of the bitmap index is the processing speed. Combining the process of performing a logical operation (AND, OR or NOT) on a serious of bitmaps is very efficient, particularly compared with performing similar processes on lists of tuple-Ids.

The granular concept and bitmap index are very closely related to each other. Other than a list, the granular can be represented in bitmap. Each tuple in the relation has one unique offset position in the bitmap. The bits are set to 1 for those tuples having the attribute value of the granule, and the bits are set to 0 for those tuples not having the attribute value of the granule. This is the bitmap representation of the granules of the lists.

111. AN ASSOCIATION RULE ALGORITHM BIT-ASSOCRULE The most influential algorithm Apriori (as described below) developed by Rakesh etc [1,2] generates the k-candidate by combining two (k-I)-itemset that have the first k-2 attribute values in the two (k-I)-itemset the same the last pair does not. A new k-candidate becomes a k large itemset if every fi-I)-subset of the k-candidate is a large itemset otherwise it is removed. This algorithm need to do table scan of the   mailto:tylin@cs.sjsu.edu http://thuOcis.drexel.edu mailto:ewlouie@almaden.ibm.com   whole data set and examine the item sets multiple times, the process is very time consuming.

Algorithm Apriori Ll = {large I-itemset} For (k=2; Lk.l#$ ; k t + )  do begin Ck = apriori-gen(Lk.,) ; //new candidate  Forall transactions t E D do begin C, =subset(Ck, 1); // Candidates contained in t Forall candidates c E C,do  end Lk = (c E Ck I c.count >= minsup]  c.count ++  End Answer = ut Lk  (Lk: Set of large k-itemsets (those with minimum support).

Each member of this set has two fields: (1) itemset, and (2) support count. ck: Set of candidate k-itemsets (potential large itemsets). Each member of this set has to fields: ( I ) itemset and (2) support count) The aprior-gen function takes as an argument Lk.,, the set of all large (k-1)-itemsets. It returns a superset of the set of all large k-itemsets. First, in the join step, Lk.] join with Lk., to obtain a superset of the final set o candidates Ck. The union p U q of itemset p, q E Lk.] is inserted in Ck if they share k- 2 first items.

Recently, many attempts have been given to applying bitmap techniques in the association rule algorithms in [7,8,10,15,l8,19]. The use of bitmaps improves the performance to find association rules. The bit representation of bitmaps offers efficient storage while the intersection of bitmaps offers fast computation in finding association rules.

The AND, SHIFT, and COUNT operations among bitmaps are extremely fast. Unlike the traditional Apriori algorithm which generates k-candidate by combining two (k-I) large itemsets. Our Bit-AssocRule algorithm generates k- candidate by intersecting the hitmap of 1 attribute value with bitmaps of other (k-1) attribute values. Below we describe the algorithm in details.

A.  The Generation of Combinations Creating combinations, or pattems, is a computational process of forming sets of attribute values. The number of combinations for one selected attribute is the number of distinct values of the selected attribute. Each candidate contains one attribute value from that selected attributes is the Cartesian product, q1 x q2, where q, and q2 are the number of distinct values the first and second selected attributes respectively. Each combination contains two attribute values: one attribute value form the first and one attribute value form the second. For the general case, the number of k-candidates is q1 x q2 x q3 x . . . xqk, where qi is the number of distinct values of the i* attribute. Each k- candidate contains one attribute value from each selected attribute. The more general case is to create k-candidate among the n attributes in the relation. Since no columns are  selected beforehand, k unique attributes are chosen from the n attributes in the relation, and from those chosen k attributes, all combinations of attribute values form the domains of those attributes are formed. There are C(n.k) possible ways to choose k attributes form n attributes, and each possible way has its own set of k-candidates among the k attributes. In short, the number of k-candidate is the total sum of the each subtotal combination from each possible way to select k attributes.

The general equation for the total number of k-candidate on n attributes in the relation is the following: Number of candidate of Length k = &??k-l) ( q g  ( Zh=g+l?*-?) q h  ~ i = h + l n ? k - 3 )  qi (...&+lo q z  )))I so far, the equation deals with various number of distinct  values on the columns in the relation. For the special case, suppose all attributes in the relation have the same number of distinct values, F, the equation simplifies to C(n,k)F*.

Generally, the number of distinct values of each attribute in the relation are not the same. Nonetheless, taking the average number of distinct values of all the attributes may he useful to estimate the number of combinations of length k: c(n,k)(( &p=l?qs)Wk  For a relation with many attributes or attribute values, the number of combinations can he very large. Each combination requires a count of the number of tuples in the relation that the combination contains. The combinations with the count greater than the minimal support are association rules. The next subsection demonstrate methods to reduce the number of combinations. In doing so, the number of comparisons between the combinations and the tuples in the relations are saved.

The number of combinations is an important factor to consider in the process. Each combination has the bitmaps in the combinations intersected and the result counted. If a combination does not have the potential of becoming a large itemset, the Combination should not be undergoing this process. So only potential combinations are generated in the process.

The algorithm starts with a list L,, which contains all the 1- itemset (all the counts of the hitmaps of these 1-itemset are greater than the minimal counter number). When making k- candidates, all the elements in the list L, are verified if they exist as elements in any (k-I)-itemset. If an element does not exist in any (k-1)-itemset, it is removed from the list.

Next, new k-candidates are created from the (k-l)-ite.mset and the list L by joining a @-])-itemset with element in L that has an attribute index greater than all attribute indexes of elements in that (k-1)-itemset. Only the new k candidates that have every (k-1)-subset a large itemset are kept.

Algorithm Bit-AssocRule LI = (bitmaps of large I-itemset} For ( k=2; Lk., #$ ; k++) do begin     Remove those elements in Ll which are not included in  Ck = Bit-apriori-gen(L,.,, L,) ; //new candidate Lk = (c E ck 1 bitmap count of c >= minsup)  any itemset of Lk.1 //prune the LI  End Answer = uk Lk  The algorithm starts with a list L,, which contains attribute values (also called I-itemset, all the counts of the hitmaps of these 1-itemset are greater than the minimal counter number). The k-candidates consist of k attribute values (X,,,,, .._, Xk.,,,k.,, &,I from k attributes. Using bitmap techniques, the candidate is a large itemset if the hit count on the intersection of all the bitmaps B , d 2 n  ... n B r (suppose 9; is the hitmap of the Xk,cl) is equal or greater than the minimal count. The bit count is the numher of 1's in the bitmap indexes from the result of the intersection of the hitmaps.

Below is an example of creating 4-combinations from 3- itemsets.

Suppose the following are the current 3-itemsets (x6.0 xll.0 x12.0), (x6.0 xll.0 xl4,2)), ( x ,  0 XI], 0 XI5 ,2}, (X6.OX11.1 x12.3), (x6.Oxll,l x14,2), {&,a xl1.l x15,2), (x6.0 x14.2x1S.2}, (&,I xll.0 XlZ,O), (x6.l xll.2 XI,,}, (xII.Ox14.2 Xl5,21, ~XII,IXI4,2X15.2).

And suppose the following are the 1-itemset in the list  First, since &,o, &,lo are not in any 3-itemsets, thus and are removed from the list L, Any 4-combinations that include these attribute values would not he an large itemset. Below are the remaining attribute values in the list  Next each 3-itemset is combined with attribute value in L to create 4-candidates, if possible, This 3-itemset (X6.0 Xll,o X12,o] is combined with these two attribute value in L X14.2. and this 3-itemset is combined with one attribute value in L XI~ .2  and so on with each 3 itemset. All the 4-combinations are shown below:  Ll= ~ & , 0 . x 4 , 1 0 . ~ 6 . 0 . ~ 6 . 1 , ~ l l . 0 , ~ I I . I . ~ 1 2 . 0 . ~ 1 2 , 3 ,  x14,2.xI5.2)~  ~~{~6.0.~6.l,~ll,0,~ll,l.~I2.0.~12,3. x14,2,XI5,2).

(X6.0 x11.0 xIZ.Ox14.2], (x6.0 xll,O Xl2.0XI5,Z)r (x6.0 x11.0 x14.2 x15.2). (x6.0 xI1.1 x12.3 X14,Z)r fX6.0 xll.1 x12.3 xI5.2), (x6.0 x l l , l  x14.2 x152), (x6.l xll.1 xl2,3 xI5.2), (&,O xII.1  x14.2 x15.2), (&,I Xll.0 XI20 x14,2), (x6.1 Xll.0 x12.0  XI5,2)>~&,1 x11.1 X12,3X14.2), {&,I XII.1 XI2,3Xl5.2~ All the 4-combinations are generated and their bitmaps are counted. If the bitmap counter of the 4-candidates is greater or equal to the minimum support, then it becomes a 4 large itemset, otherwise it is deleted.

B. The Storage and Management of Bitmaps The bitmaps are stored on disk in data pages. The data pages offer flexibility in accessing and processing the bitmaps. Portions of the hitmaps called slices, are stored in the data pages and the data pages are connected by links.

Below shows an example of several data pages holding three bitmaps. Each bitmap has its initial page on a different  page,, The initial pages for each hitmap are not necessarily consecutive. Su pose there are 3 different values xj.~,  Xlkl from the I .fl , J 4h and 1' attributes in the data set, their hitmaps are Bj,u, B1.u .

Bitma  Each bitmap requires the same number of data pages to store the binary representation of the list, and for this example, let's assume that each hitmap has 5 data pages.

For the combination, ( X ~ . L ~ .  X,.U, Xlkl), the bitmaps Bisl Bj,u, Bl,k3 are intersected and the result is counted. Since the bitmaps are stored in data pages, the data pages 1,2,and 5 are intersected first, and the bits in the result are counted.

Next pages 3, 4, and 6 are intersected, and the bits in this intersection are counted. This subtotal is added to the previous subtotal. This continues on for the third, forth and fifth pages of the bitmap Bicl B,,u and Bl,u. After the last page, the combination (Xi.kI. Xi,u,Xlk3] is a large itemset if the total counter is greater than the minimal support.

Through the intersection and bit counting, many data pages are read among the k-combinations in each cycle. Even though the bitmaps in each k-combination do not repeat within the k-combination, the bitmaps in one k-combination may appear among other k-combinations. This means that data pages read for one combination may be needed for other combinations. So to reduce the physical data pages reads for bitmaps, all the nL data pages on each hitmap from each k-combination are processed at the same time.

In other words, all the nL data pages on each hitmap are intersected, the results of the intersections are counted, and the counts are added to each combination's subtotal before continuing to the next data pages on each combination, As a result, the data pages for each bitmap are read once from disk to memory for k-combinations in determining which are large item sets.

The total storage cost for bitmaps is based on the number of bitmaps in the relation and the number of data pages per bitmap. The number of pages per bitmap is dependent on the number of tuples in the relation and the number of tuples that can he stored per pages. The equation for the total storage cost is the following: Storage-cost = Number of bitmaps * [Number of tuples / Max bits per pages 1 * Datagage-size  For example, the relation has 1000 bitmaps and 1000,000 tuples. Each data page is 4096 bytes, and 4080 out of 4096     bytes are available to store data for bitmaps (the last 16 byte is used as a pointer to point the next page in the bihnap).

Then, the following is the total storage costs: Storage cost = 1000? * rioooooo I (4080?8) 1 * 4096 = 125Mbyte  Data set  DSI  DSZ  DS3  C. Comparison Between Apriori, Its Variations and Bit.

AssocRule Algorithm The ,traditional Apriori algorithm, its variations and bitmap-based Bit-AssocRule are compared based on their key operations. For the Bit-AssocRule, .the number of AND operations between the bitmaps determines the cost. For the Apriori algorithm, the number of comparisons between the attribute values in the candidates and the tuples determines the cost. A ration is used to compare the two algorithms Apriori and Bit-AssocRule X= Number of Comparison operations in Apriori I  Number of AND operations in Bit-AssocRule  For the Aprioir algorithm if there are p candidates and q tuples in the relation, p*q comparisons are needed to determine the count for the p candidates. A hash-tree table is used to reduce the comparisons. The node of the bash- tree table is either a leaf node containing some candidates or an interior node containing a hash table [2]. The hasb- table consists of references to the interiors nodes or leaf node. The bash-tree table reduces the candidates to compare on each tuple by didiving the p candidates among the leaf modes. All the p candidates have the same length, k. Each tuple in tbe relation has C(n,k) subsets, and each subset is hashed on the hash-tree table. For subsets that hash onto a non-visited leaf node, each candidate in those leaf node is compared to the tuple. The subsets that hash to a visited leaf nodes or interior node are skipped. This is the subset function that is applied each tuple in the relation.

The cost of the subset function is determined by two factors: the number of subsets per tuple and the number of candidates for all visited-once nodes per tuple. The equation is: m(c(n.k)) + Z;.,?(v,*t). The first term is the costs of calling the bash function for m tuples in the relation. The second term is the sum cost of each tuple?s comparisons. The vi is the number of visited-once leaf nodes per tuple, and t is the average number of candidates per leaf node, n is the number of attributes in the relation and k is the current length of the candidates in the hash-tree table.

Apriori is improved further by short-circuiting the comparisons on each candidates to each tuple and by comparing the k-candidates to a &-?)-candidates bar table consisting of (k-I)-candidates ids. The first improvement bas the comparisons stop once the first element in the k- candidate is not contained in the tuple. The second improvement uses the (k-J)-candidate bar table to compare only the (k-I)-candidate ids of the tuples. Basically, when a k-candidate is compared to a tuple in the (k-I)-candidate bar table, the two (E-I)-candidate ids, that generated the k- candidate, is compared to the (k-])-candidate ids in the  Ro Colu #of Table Bitm Mini ws mn ite size ap mal  ms size Supp ort  400 16 199 25.6MB 10.6 20K K MB 800 20 247 64MB 25.0 40K K MB 1.0 30 709 120MB 90M 50K  tuple. If both (k-1)-candidate ids exist for that tuple, the k- candidate id for this k-candidate is inserted into the k- candidate bar table for that tuple. For more detail, refer to [2,9]. The Apriori and ApriorTid algorithms generate the candidate itemsets to be counted in a pass by using only the itemsets found large in the previous pass without considering the transaction in the databases. The AprioriTid algorithm has the additional property that the database is not used at all for counting the support of candidate itemsets after the first pass. AprioriHybrid uses Apriori in the initial passes and switches to AprioriTid when it expects that the set of candidate itemsets at the end of the pass will fit in the memory [1,2].

The worst case for the Apriori algorithm is m*(k*q) comparison operations, where m, k defined above and q is the average number of attribute values per candidates. Eacb candidate compares with each tuple to determine if it is contained in the tuple. The worse case for Bit-AssocRule algorithm is (m/32)*(k-l)*q AND operations. So, the ratio is the following: X= m * (r*q) / (m/32*(r-l)*q) = 32 * k I (k-1) The Bit-AssocRulemethod can execute 32 times faster or more than Apriori in theory. The subset function in Apriori lowers the costs if the subset function reduces the number of r-candidates to compare to each tuples in the relation.

However, the cost varies per tuple in the relation and is affected by the number of attribute in the relation.

D. Experimental Runs: Apriori, AprioriTid, AprioriHybrid and Bit-AssocRule Three different data are generated to compare the run time on these algorithms to find association rules. The data varies in the number of tuples, the attributes per relation and the minimal support. The program for Apriori, AprioriTid and AprioriHybrid are our honest implementations of the algorithms in [2. In the implementation, we use some buffer scheme to speedup readwrite for all algorithms. In the implementation, we use some buffer scheme to speedup readwrite for all algorithms. The tests were conducted using an IBM PC with 933Mhz CPU, 512MB memory under Window 2000.

The program is coded in C++.

l M  I I B  e L #  # BitAsso Aprio Apriori Apri  Cand item Rule ri- -Tid ori     n set g t  2 16333 103 18.426s 1402. 1669.18 1403  Hybr id  Total time  I I , _I Table 2: Experimental Run of 3 Data Sets  Here are some observations and explanations on the results The total time of our comparison includes the time to write the association rules to a file; Bit-AssocRule is 2 to 3 orders of magnitude faster than the various Apriori algorithms (64- 221 times faster). The big the test data set, the big the time difference between the Bit-AssocRule and the various Apriori algorithms. We haven?t compared our algorithm with some of the other association rule algorithms such as VIPER [15], CHARM [19], CLOSE [6] (CHARM and CLOSE are based on the closed frequent itemsets concept), but based on their published comparison results with Apriori, our Bit-AssocRule is very competitive compared to them and a direct comparison will be conducted and reported in the near future.

Bit-AssocRule takes the same or litter longer time than the various Apriori algorithms in constructing the I-itemsets because of the extra cost of building the bitmaps for the 1- itemsets. But after the 1-itsemtset is done, Bit-AssocRule is significant faster than the Apriori algorithms in constructing large frequent itemsets because it only uses the fast bit  S os 673.288 68286 68267.5 5144 S s 3 4 s  34s 2.56  I Q  operations (AND, COUNT and SHIFT) and doesn?t need to test the subsets of the newly candidate  Bit-AssocRule only stores the hitmaps of the frequent items, and the bitmap storage (uncompressed) is less than the original data set (U2 to 1/4 of the original data size).

The main reasons that Bit-AssocRule algorithm is significant faster than Apriori and its variations are  Bit-AssocRule adopts the divide-and-conquer strategy, the transaction is decompose into vertical bitmap format and leads to focused search of smaller domain. There is no repeated scan of entire database in Bit-AssocRule.

Bit-AssocRule doesn?t follow the traditional candidate-generate-and test approach, thus saves significant amount of time to test the candidates In Bit-AssocRule, the basic operations are hit Count and hit And operations, which are extremely faster than the pattern search and matching operations used in Apriori and its variations

IV. CONCLUSION We present a bitmap based association rule algorithm using granular computing technique and introduce the bitmap technique to the data mining procedure and develop a bitmap based algorithm (Bit-AssocRule) to find association rules. Our Bit-AssocRule avoids the time-consuming table scan to find and prune the itemsets, all the operations of finding large itemsets from the datasets are the fast bit operations. The experimental result of our Bit-AssocRule algorithm with Apriori, AprioriTid and AprioirHybrid algorithms shows Bit-AssocRule is 2 to 3 orders of magnitude faster. This research indicates that bitmap and granular computing techniques can greatly enhance the performance for finding association rule, and bitmap techniques are very promising for the decision support query optimization and data mining applications.

Bitmap technique is only one way to improve the performance data mining algorithm. Parallelism is another crucial aspect of DSS and data mining performance. We are currently working on paralleling the hitmap-based algorithms and hope to report our fmdings in the near future.

