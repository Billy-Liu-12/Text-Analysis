Finding Correlated item pairs through efficient pruning with a given threshold  Bo Wang  Liang Su  Aiping Li   Peng Zou

Abstract  Given a minimum threshold in a massive market- basket data set, an item pair whose correlation above the threshold is considered correlated. In this paper, we provide a randomized algorithm SERIT?a Searching-corrElated-pair Randomized algorithm for dIfferent Thresholds?to find all correlated pairs effectively, which adopts the Pearson?s correlation coefficient [11] as the measure criterion. In their CIKM?06 paper [2], Zhang et al. address the same problem by taking the relation of Pearson?s coefficient and Jaccard distance into account. However, it is inefficient when the threshold is small. We propose a new probability function to prune uncorrelated item pairs based on [2]?which can cover the shortage of the former one. Experimental results with synthetic and real data sets reveal that with a given threshold, even if it is small, SERIT algorithm can prune the item pairs unwanted efficiently and save large computational resources.

1. Introduction   Finding correlated attribute pairs plays an important role  in many applications, such as market-basket analysis, climate studies, public health, and bioinformatics [3]. We focus on market-basket itemset, which is a data model for many datasets if they involve two concepts: a set of items and a set of relations (baskets) [2]. Furthermore, items can always be transformed to binary attributes. In such a market-basket model, finding correlated attribute pairs is to find correlated item pairs, which is a quintessential and one of the most fundamental problems.

In a massive data set, we may face a new problem of finding all correlated item pairs effectively. When an appropriate measure criterion is identified,  a traditional brute-force approach needs to compute all item pairs and select the pair whose correlation is above a user-specified threshold. The approach is easy to implement, but not applicable. Since n items will combine to 2 ( 1)  2n n nC ?=  pairs,  the computation complexity is at least 2( )O n . Thus, we need a more efficient method especially when the main-memory capacity is limited. A natural thought is to mine the essential character of the measure criterion, which may speed up the pruning process.

In this work, we propose a new algorithm SERIT (A Searching-corrElated-pair Random algorithm for dIfferent Thresholds) based on [2] which takes Pearson?s coefficient ?  [11] as correlation measure. According to a new  probability function, uncorrelated item pairs are pruned and the remains are added to the candidate set. This the first step of SERIT called searching step. In the second refinement step, the correlation is computed exactly for each candidate pair to generate the final results. As demonstrated by our experiments on both real and synthetic data sets, with different thresholds, SERIT can prune the unwanted item pairs effectively bounded with a small false-negative tolerance. Moreover, our algorithm also deals with the shortage of the algorithm mentioned in [2]: the efficiency is too low with a small threshold.

1.1. Related work There already exist some methods to identify correlation  (association) patterns in market-basket itemsets. Two measurements are often considered when evaluating a method: one is the power of pruning which can be reflected through execution time. The more powerful the pruning rule, the smaller number of candidate pairs need to exactly compute,  and the more execution time can be saved. The other is the accuracy. Some method may cut off the item pair truly correlated leading to false negative. Hence, the selection of methods is the result of balancing between pruning power and accuracy.

This paper mainly relates with two kinds of literatures:  one category is about statistical correlation measure. The other is association-rule mining.

In the framework of data mining, association-rule  mining has been an active research field. There are many algorithms for efficient mining under kinds of constraints [4, 6, 10, 13]. The support of one association rule is often used to judge the rule?s importance. It can be interpreted as the fraction of baskets contained a item set with the total baskets.

Most of the former work in identifying correlation  (association) statistically is primarily about the measures of correlation [5, 8, 9, 12, 15, 16]. However, how to find the correlated pairs efficiently has not been concerned deeply, which is often encountered in massive-data-set tasks.

Previously, in paper [1], Xiong et al. propose TAPER algorithm to find strongly correlated item pairs. It chooses Pearson?s coefficient (the ? correlation coefficient is the computation form for binary variables) as the correlation measure. Then an upper bound of ?  is provided, which can be computed as a function of the support of individual item.

Furthermore, the good monotone property of this upper bound allows the elimination of many item pairs directly.

TAPER is a two-pass algorithm and can gain the set of correlated item pairs exactly. Xiong again exploits a 2-D monotone property of the upper bound of ?  in [3]. Its main   DOI 10.1109/WAIM.2008.84    DOI 10.1109/WAIM.2008.84     contribution is the development of TOP-COP query algorithm to find the top-k correlated item pairs without a given threshold, which may be difficult for users to decide.

The 2-D monotone property decides the next item pair to be considered in a diagonal traversal direction. These two algorithms both need to sort the items by support values in non-increasing order. Moreover, TOP-COP also needs to construct a correlation matrix. In order to enhance the pruning efficiency and save more execution time, Zhang et al. introduce a randomized method adapted from TAPER in [2]. Through analysis of the relation between Jaccard distance and the upper bound of ? , according to the probability property showed by a min-hash function [7, 14], the algorithm is more effective than TAPER. Because of the randomicity, it only selects the correlated pair in high probability and may filter out some truly wanted ones.

However, the false negative can be controlled. The shortage of this algorithm, just as mentioned in [2], that the efficiency lows down when the threshold is small, is also obvious. That is to say, it is more suitable to find the strongly correlated item pairs.

We can draw a conclusion from the above analysis, that  is, there are two key considerations: (1) the choice of the beginning point which searching starts from. The beginning point can be chosen randomly, or following some property.

For example, [1] chooses the item having the biggest support value. (2) the choice of searching strategy or searching direction. We can choose the next point sequently such as the sequence of support values in non-increasing order, or according to some special probability distribution.

1.2. Overview of the paper The rest of the paper is organized as follows: in section 2,  we introduce the basic concepts, and the theories of TAPER and the algorithm in [2]. For simplicity, we call the algorithm in [2] BTH (Big THreshold) in this paper. In section 3, our SERIT algorithm is presented, with proposal of the new probability function. In section 4, we provide experimental results comparing with TAPER and BTH. The results show that our algorithm is efficient and scalable.

Section 5 summarizes the whole paper and shows some possible future work.

2. Basic concepts To simplify the discussion, table 1 firstly summarizes the  notations that we will use throughout this paper.

TABLE 1 NOTATIONS  T A relation table?namely the market-basket data set. A column in T represents an item? and a row represents a shopping basket. If basket i contains item j?it is recorded as T(i,j)=1.

I the set of items, such as { }, , , , , , ,a b c d e f g h  n the number of items, namely the number of columns in T.

m the number of baskets, namely the number of rows in T.

S(i) the set of baskets containing item i, namely the set of rows satisfying { ( ) }| , 1r m T r i? = .

sp(i) the support value of item i ?  a given threshld?if the correlation is bigger  than ? , the item pair is correlated.

For an item pair ( ),a b , if we know ( )sp a and ( )sp b , the ? coefficient (namely Pearson correlation coefficient) can be expressed in Equation 1:  ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )  , ,  1 1  sp a b sp a sp b a b  sp a sp b sp a sp b ?  ? =  ? ?  (1)  Hence the problem of finding all correlated item pairs can be described as follows: For a given threshold ? , it needs to find all ( ),a b  satisfying ( ),a b? ?? .

Without loss of generality, let ( ) ( )sp a sp b? . Xiong et al.

reveal a upper bound of )( ,a b? in [1] as we show in Equation 2. Specifically, they also prove the monotone property of this upper bound. In this paper, we use this property as Lemma 1.

( ) ( )( ) ( )( ) ( ) ( )  , ,  sp b sp a  a b upper a b sp a sp b  ? ? ?  ? = ?  (2)  LEMMA 1. Given an item pair ( ),a b , without loss of generality, let ( ) ( )sp a sp b? . If we fix item a and change b, the upper bound of )( ,a b?  is monotone decreasing with the decrease of support value of item b. Moreover,  ( )( )0 , 1upper a b?< ?  is satisfied.

The TAPER algorithm is based on Lemma 1. Firstly, it  sorts all the items by support values in non-increasing order, like { }1 2, ,..., mi i i . If there is the fact of ( )( )1,l kupper i i? ?< and ( ) ( )1 2k ksp i sp i? , according to Lemma 1, we can get  ( )( ) ( )( )2 1, ,l k l kupper i i upper i i? ? ?< < . Hence, the pair ( )2,l ki i can be removed. After the first pass, TAPER gets pairs whose upper bound are no smaller than ? , namely candidate pairs. Whether they are truly correlated needs further computation with Equation 1. This is the work during the second refinement step.

The main contribution of TAPER algorithm is to propose a  method of computing the upper bound of ?  with individual item. Although it can remove many uncorrelated pairs, there is great potential for improvement. Firstly, ? coefficient is relevant with ( ),sp a b  (this fact can be easily derived from Equation 1). In other words, this correlation measure considers the frequency of co-occurrence of the two items statistically, while TAPER only takes the frequency of individual item into account. For instance, for item pair  )( ,a b  satisfying ( ) ( )sp a sp b? , we get ( )( ), 1upper a b? ? with Equation 2. That is to say, if item a and b have close support values, pair )( ,a b  will be chosen into candidate set.

In addition, [2] has discovered that item pairs removed by TAPER coincide with this feature: one item has small support value, and the other has a relatively big one. Those both having small (or big) values are remained as candidate pairs. Hence, Zhang et al. design a randomized algorithm BTH, which reflects the frequency of co-occurrence of item a and b by introducing Jaccard distance. The Jaccard distance of )( ,a b  equals ( ) ( )  ( ) ( )  S a S b S a S b  ? ?  ? , where ?  is the  number of elements in a set. Let ( )( ),R upper a b?= . BTH reveals that Jaccard distance holds an inequality (shown in Lemma 2) with R and? .

LEMMA 2. For any given item pair ( ),a b and threshold ? , we have the inequality ( ) ( )  ( ) ( ) 1 S a S b S a S b R R  ? ?  ? ?  ? + ?  . What?s  more, on the premise of that R is no smaller than ? (1 R ?? ? ), ( ) ( )  ( ) ( ) S a S b S a S b  ?  ?  achieves its minimum value of 2?  when R ?= .

Hence, ( ) ( ) ( ) ( )  S a S b S a S b  ?  ?  is bigger than 2?  when  ( )( ),upper a b? is above? . In this case, pair ( ),a b  can be removed if ( ) ( )  ( ) ( ) 2S a S b  S a S b ?  ? <  ? [2]. In addition, Zhang et al.

discover that the min-hash function has the following property:  ( ) ( )( ) ( ) ( )( ) ( )min min S a S b  h a h b S a S b  ? ? = =  ?  . BTH defines an  equivalence relation ? ?. When ( )a b x? =  (According to Lemma 2, if 2x ?> , we can get ( )( ),upper a b? ?> ), BTH repeats the process t times. Each time a set of k min-hash functions is chosen and pair ( ),a b  is mapped. Finally, the probability that item pair ( ),a b  (satisfying a b ) put into the candidate set is ( )1 1 tkx? ? . The algorithm expects to select the pair having 2x ?> . Experimental results in [2] show that BTH is more efficient than TAPER under a small false-negative tolerance, by judging parameter k and t.

3. The algorithm of SERIT 3.1. The description of SERIT  Figure 1 shows the probability distribution of ( ) ( ) ( ) ( )  S a S b x  S a S b ?  = ?  . Given a threshold ? , if pair ( ),a b  has  ( ) ( ) ( ) ( )  2S a S b S a S b  ? ?  > ?  (as mentioned above, we can get  ( )( ),upper a b? ?> .), ( ),a b  can be added to candidate set. As we know, the probability of a pair ( ),a b  added to candidate  set in BTH is ( )1 1 tkx? ? . Note that with a relatively big threshold ? , the probability value of a candidate pair centres in the ?northeast? area of the ?S? shaped curve, that means an item pair who has ( )( ),upper a b? ?>  would be more likely chosen as candidate pair. This is exactly the consequence BTH expects. But when ?  is small, some candidate item pairs locate in the middle of the curve. These candidate pairs might be missed because of low probability, inducing the increase of false negatives. To bound the false negative with a small tolerance, BTH needs to raise k ti which may low down efficiency. From the experimental results in [2], when false-negative tolerance equals 0.005, k ti  needs to be adjusted to 740 when 0.3? = , while only 94 with 0.5? = .

To resolve this problem, we firstly analyze the probability function ( )f x . If only one min-hash function is used, ( )f x  has the form of x, illustrated in Figure 2(a). Obviously,  this distribution won?t meet our expectation, which would be like the curve in Figure 2(b). We expect that the item pair selected must have ( ) ( )  ( ) ( ) 2S a S b  S a S b ?  ? >  ? , namely  ( )( ),upper a b? ?> . To close to the ideal probability distribution with different ? , we give the definition of a b? .

0.0 0.2 0.4 0.6 0.8 1.0 0.0  0.2  0.4  0.6  0.8  1.0  Thresholdsmall Thresholdbig   1- (1  -x k )t  x    Fig. 1 Shape of function ( )( ) 1 1 tkf x x= ? ?  0.2 0.4 0.6 0.8 1  0.2  0.4  0.6  0.8     2?    Fig. 2(a) Shape of function ( )f x x=  Fig. 2(b) Ideal probability distribution  DEFINITION 1. For two items a and b, a b?  if and only if ( ) ( )min minh a h b?  is satisfied for all of the k min-hash functions.

Still, when ( ) ( )( ) ( ) ( )( ) ( )min min S a S b  h a h b x S a S b  ? ? = = =  ? , with one  min-hash function ( ) 1a b x? ? = ? , then with k independent functions, ( ) ( )1 ka b x? ? = ? . The process is repeated t times , and each time we require k independent min-hash functions.

As a result, the probability that ( ),a b  satisfying a b?  put into the candidate set is ( )( )( ) 1 1 1 tkg x x= ? ? ? . Figure 3 describes the shape of g(x) with different k and t. According to the feature of g(x), we can select item pairs that satisfied  2x ?<  with high probability, then the remain requires further computation as candidate item pairs. In this way, the main process of SERIT is: We repeat t times, and each time item pairs satisfying a b?  are selected and put into item set ? . The final candidate set is I I? ?? . We can prove that k ti  is reasonable with different ? , no matter it is big or small, by experimental results. If there is an item pair ( ),a b  having ( ) ( )  ( ) ( ) 2S a S b  S a S b ?  ? =  ?  , the probability that it is  removed and never added into candidate set is  ( )( )2 2( ) 1 1 1 tkg ? ? ?= ? ? ? = . While the probability that other pairs having ( ) ( )  ( ) ( ) 2S a S b  S a S b ?  ? >  ? are removed would be smaller  than ? . With the fixed values of t and false-negative tolerance ? ,  ( ) ( )21 1 1tk log ? ??= ? ?  can guarantee the probability that a candidate item pair missed is no bigger than ? .

TABLE 2 THE COLUMN VALUES MAPPED BY H1  1? 2? 1k? ? k?    0.0 0.1 0.2 0.3 0.4 0.5 0.0  0.2  0.4  0.6  0.8  1.0  K:10,T:2 K:10,T:5 K:30,T:5 K:8,T:7  X  1- (1  -(1 -x  )k ) t    Fig. 3 Shape of function ( )( )( ) 1 1 1 tkg x x= ? ? ?  Here, how to single out item pairs we wanted also need considerations. The brute-force alternative is to pass through all the item pairs and judge whether it satisfies a b? . This  method is definitely slow and costly. [2] finds the equivalence class by a hash bucket while both the time and space complexity are ( )o n . But this data structure is not suitable for our SERIT any more. We design a hash structure as the bitmap shown in Figure 4. Furthermore, we can receive the set ( ) ( ){ }, | , '   a b a b doesn t satisy a b?  directly, without computation of ? .

After the first pass of data set, we can get k ti  min-hash function values for each item. There is n k ti i  vakues totally, which can be devided into t groups of size k. We choose a hash function H. For each item, H maps one of its k ti min-hash function values as a key into a lattice in the bitmap.

The i-th (1 i k? ? ) min-hash function value will be mapped into the lattice in i-th row. Suppose there are p columns.

More importantly, each lattice needs to maintain a linked list, recording the items whose min-hash function value is mapped into the lattice. We consider the case that the i-th min-hash value of item x is mapped into lattice  ,i jP (in i-th row and j-th column). If this is the first time that  ,i jP  is mapped, we label  ,i jP  with ?1? and add x into the corresponding linked list ,i jL ; Otherwise, some other item(s) has been mapped into  ,i jP  before and ,i jP  is already ?1?.

What we need is to add x into the linked list  ,i jL . To simplify the discussions,  ,i jL  also stands for the item sets contained in the linked list. Clearly, the i-th min-hash function of each item in  ,i jL  has the same value. We give an example that shows how the candidate set is generated. In round 1, we choose  1H  as the hash function for the min-hash function values contained in Group 1. Table 2 shows the values of columns mapped by  1H  for each item, where { }1 2 3, , ,..., kv v v v  represents the k min-hash function values. Figure 4(a)~(c) illustrate the bitmap after the mapping of item a, b and c respectively. Particularly, we can get Set  { }{ }, ,  | k  c i j i j i  L c c L =  ? = ? ? =? { },a b ? { }b ? { }... ,a b???? = .

Note that with each item in c? , item c has at least one equal min-hash value. The pair generated by c and an item from  c?  is exactly the pair we wanted, because it doesn?t satisfy a b? . As a result, item pairs ( , )a c and ( , )b c  are added into the candidate set. By the same token, other items (such as a,b,?) can also obtain a set like c? . In round 2, a new hash function 2H  is introduced. At the end of round t, we can get the final candidate set and start the refinement step.

The whole algorithm is described in Algorithm 1.

Fig4(a). The bitmap after a is mapped by H1  ...

Fig4(b). The bitmap after a,b are mapped by H1   Fig4(c). The bitmap after a,b and c are mapped by H1  Algorithm 1?SERIT  finding correlated pairs in massive itemset for a given threshold  Variable description S:  the set of correlated pairs C:  the candidate set Ti:  the item that has at least one same min-hash value as item i is added to set Ti  [ ][ ]minh i l :  the l-th min-hash value of item i  [ ][ ]L j u :  the set linked to the lattice in j-th row and u-th column in the bitmap table  vH : vH (1 v t? ? ) is used to map [ ][ ]minh i l  to a lattice in l-th row in Round v Input:   data set T k ti  min-hash functions Output:  S Compute k ti  min-hash values for each item i and u from 1 to k ti  do  [ ][ ]minh i l ?? end for for each item i and each j in S(i) do for u from 1 to k ti  do if [ ][ ] ( )min uh i l h j>  then  [ ][ ] ( )min uh i l h j? end if end for end for Generate candidate set for i from 1 to t do set the linked list to each lattice to? for each item j from 1 to n do   // n is the  //Num of items for l from 1 to k do  [ ][ ]( )i minp H h j l=    // map to p-th // column in row l  [ ][ ]minh j l if [ ][ ]L l p ??   then [ ][ ]Ti Ti L l p= ? end if [ ][ ] [ ][ ] { }L l p L l p j? ?  // add item j to  // the set end for          // Ti of item j is ready ( ){ }, |C C j x x T? ? ?         // update C Ti ?? end for          // consider the next item end for Refine to find the truly correlated pairs for each pair ( ),a b  in C do   ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )  , ,  1 1  sp a b sp a sp b a b  sp a sp b sp a sp b ?  ? ?  ? ?    if ( ),a b? ??  then ( ),S S a b? ? end if end for output S  3.2. The complexity of candidate-generation method in  algorithm SERIT  First we analyze the time complexity. From [2] we learn that the n k ti i  min-hash function values can be computed in ( )O n  time. After that, for n items and t rounds, we need  map k min-hash values into the bitmap and union at most k subsets. That can be done in ( ) ( )O k t n O n? ? =  time (Because k and t are two constants determined only by the threshold ?  and the false-negative tolerance ? ). From the experimental results, bounded with a small false-negative tolerance, BTH needs to choose a reasonlessly big t when ? is relatively small. However, our SERIT has reasonable k and t to reduce computing time distinctly.

Then we consider the space complexity. For each item, we store k ti  min-hash values. The bitmap totally has k pi lattices. And the length of a linked list is at most n. Note that in the i-th row, an item can only be mapped into one lattice.

In other words, an item would only appear at most one time in the i-th row during a single round. In this way, we have few linked lists including n items if the hash function H is sufficiently uniform. The total memory required is again ( )O n .

In summary, the time and space complexity of the  candidate-generation process in our SERIT are asymptotically the same as BTH?s.

4. experiment results  In this section, we report the results of testing our SERIT on several data sets. Comparing with algorithm TAPER and BTH, SERIT is proved to be effective. Bounded with a small false-negative tolerance ? as 0.005, SERIT can not only prune the uncorrelated pairs efficiently to generate a small candidate set and save computing resources; but also cover the shortcoming of BTH, still achieving small running time when  ?  is relatively small.

The experiment bases on both synthetic and real data sets.

We choose the famous ?retail? data set, which contains data from a retail store and can be downloaded from FIMI website. The tool mentioned in [2] is also used here to generate three synthetic data sets S1, S2 and S3. Table 3 lists the characteristics of these data sets. And table 4 shows the values of parameter k and t in algorithm SERIT and BTH for different ? .

TABLE 3 THE CHARACTERISTICS OF DATA SETS  Data Set Num of Items Num of Records  retail 16470 88163 S1 20589 51316 S2 33052 51292 S3 42522 51337  All the experiments were performed on a personal computer, with a 2.0 GHz CPU and 512 Mbytes of memory running the windows XP operating system.

In section 5.1, we give the executing time of the three algorithms, including candidate-generation running time and overall running time. We then show in section 5.2 the scalability of SERIT.

TABLE 4  THE VALUES OF PARAMETER K AND T  Algorithm SERIT Algorithm BTH ?  k t k t 0.8 7 8 2 10 0.7 11 6 2 18 0.6 14 6 2 38 0.5 17 6 2 47 0.4 28 4 2 204 0.3 30 4 2 370 0.2 110 4 2 2940  0.1 410 2 2 50000 4.1. The executing time  All the three algorithms need two steps: 1) generation of the candidate set 2) refinement. As a result, there are two types of executing time, candidate-generation running time and overall running time, which may present different properties. Although an algorithm executes efficiently during candidate-generation stage, it may produce a large candidate set inducing the increase of overall executing time.

The coefficient thresholds are split to two parts with 0.4 as a dividing point. Figure 5 illustrates the candidate-generation time, while Figure 6 shows the overall executing time, both based on ?retail? data set. The corresponding executing time on S1 is presented in Figure 7 and 8. Note that when 0.1? = , we find that the executing time (including the two types) of BTH is too long, much longer than that of other algorithms.

Moreover, it is also much longer than the time when 0.2? = .

For the rationality of experiment and brevity to present, we don?t illustrate the executing time of BTH with 0.1? = .

From Figure 5~8(a), SERIT achieves small executing time when 0.4? ? . And the advantage becomes more obvious when ?  is smaller. Oppositely, the running time of BTH increases sharply with the decrease of ? . This is mainly due to the very large numbers of min-hash values caused by large t. Although BTH merely chooses two items in the same equivalence class to generate a candidate pair, and the method in SERIT seems a little complex, SERIT is still faster due to the reason mentioned above.

When 0.5? ? , BTH computes less min-hash values, and consequentially it has smaller running time. SERIT is slightly slower than BTH in most cases, but always faster than TAPER. This is determined by the property of the probability function used in SERIT: With a big ? , the item pair whose correlation coefficient is smaller than ?  would be chosen into the candidate set in a relatively high probability (compared with BTH). In this way, SERIT may have a slightly bigger candidate set than BTH.

From further analysis, the other two is much powerful than TAPER to remove pairs unwanted. As a result of the large candidate set generated in TAPER, in the refinement step more time is required. As shown in Figure 7 (b) and 8 (b), the execution of TAPER can be one order of magnitude slower.

Moreover, comparing the experimental results on ?retail? with S1, we can conclude that the size of item set has greater influence on TAPER than on SERIT and BTH. The running time increases obviously with the growth of item set.

0.10 0.15 0.20 0.25 0.30 0.35 0.40        BTH TAPER SERIT  Ex ec  ut io  n Ti  m e  (s ec  )  Threshold   Fig 5(a) candidate-generation time on ?retail? with 0.4? ?  0.50 0.55 0.60 0.65 0.70 0.75 0.80       Ex ec  ut io  n Ti  m e  (s ec  )  Threshold  BTH TAPER SERIT   Fig 5(b) candidate-generation time on ?retail? with 0.5? ?  0.10 0.15 0.20 0.25 0.30 0.35 0.40       600  BTH TAPER SERIT  Ex ec  ut io  n Ti  m e  (s ec  )  Threshold   Fig 6(a) overall executing time on ?retail? with 0.4? ?  0.50 0.55 0.60 0.65 0.70 0.75 0.80            Ex ec  ut io  n Ti  m e  (s ec  )  Threshold  BTH TAPER SERIT   Fig 6(b) overall executing time on ?retail? with 0.5? ?  0.10 0.15 0.20 0.25 0.30 0.35 0.40       600  BTH TAPER SERIT  Ex ec  ut io  n Ti  m e  (s ec  )  Threshold   Fig 7(a) candidate-generation time on S1 with 0.4? ?  0.50 0.55 0.60 0.65 0.70 0.75 0.80          Ex ec  ut io  n Ti  m e  (s ec  )  Threshold  BTH TAPER SERIT   Fig 7(b) candidate-generation time on S1 with 0.5? ?  0.10 0.15 0.20 0.25 0.30 0.35 0.40          BTH TAPER SERIT  Ex ec  ut io  n Ti  m e  (s ec  )  Threshold   Fig 8(a) overall executing time on S1 with 0.4? ?  0.50 0.55 0.60 0.65 0.70 0.75 0.80           Ex ec  ut io  n Ti  m e  (s ec  )  Threshold  BTH TAPER SERIT   Fig 8(b) overall executing time on S1 with 0.5? ?  4.2. The scalability of algorithm SERIT We use synthesized data sets list in Table 3 to examine the  scalability of SERIT from the view point of execution time.

They have different sizes of item set. Figure 9 shows that the  execution time increases with the number of items. This conclusion is consistent with the complexity analysis shown in section 3.

2.0 2.5 3.0 3.5 4.0 4.5         Ex ec  ut io  n Ti  m e  (s ec  )  Num of items              X 10  4  Thre0.1 Thre0.3 Thre0.5 Thre0.7   Fig 9. scalability of algorithm SERIT  5. Conclusion and future work In this paper, given a coefficient threshold, we design and  implement the algorithm SERIT, which can find the correlated item pairs through efficient pruning in a massive market-basket data set. The method proposed by Zhang et al.

in [2] is more suitable when ?  is relatively big. Hence, a new problem is generated when ?  is small. Our SERIT introduces a new probability function to select candidate pairs.

Experimental results show that the space and time complexity are asymptotically the same as BTH?s. In addition, with a relatively small ? ( 0.4? ), SERIT needs a much smaller and more reasonable k ti  than BTH. Therefore, SERIT achieves much larger saving of computational resources.

There are several potential directions in future research.

First, we will examine whether it is more efficient to combine SERIT and TAPER. However, the two different pruning methods need different pre-processings. In particular, we have to consider the trade-off between a more complex pre-processing and efficiency. Second, we only considered pairs of items in this paper. How about a correlated item set?

Also, there exist several other criterions to measure correlation (association), which may have peculiar properties.

