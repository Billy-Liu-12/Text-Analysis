Large Scale Nearest Neighbors Search  Based on Neighborhood Graph

Abstract?Large scale approximate k-nearest neighbors search is an important and very useful technique for many multimedia retrieval applications. Most of existing search algorithms used the centralized indexing approaches and thus cannot meet the needs to search upon large scale datasets. This paper proposes an efficient and distributed approximate k- nearest neighbors search algorithm over a billion high- dimensional visual descriptors. We propose a randomized partitioning strategy and then design a two-layer distributed indexing scheme based on a neighborhood graph for large scale k-nearest neighbors search. The experimental results show that our method achieves excellent performance and scalability.

Keywords?approximate k-nearest  neighbors; neighborhood graph;  large scale search; distributed indexing

I.  INTRODUCTION Finding the k-nearest neighbors (KNN) of a single query  point in an extremely large collection of high-dimensional vectors is a fundamental problem in multimedia retrieval such as image retrieval [1, 2, 3], computer vision [4], music retrieval [5], etc.

A naive solution to the KNN search problem is to compute the distance from the query to every single point in the dataset and return the closest ? ones. This approach is called the linear search method and is guaranteed to find the exact k-nearest neighbors. However, the computational complexity of the linear search method is ????? , where ?  is the size of the dataset and ? is the dimensionality.The linear complexity may work for small to intermediate sized datasets; however, it will become so expensive for relatively large datasets. The difficulty of finding the exact k-nearest neighbors over relatively large datasets has led to the development of the approximate k-nearest neighbors (AKNN) search algorithms.

A key issue for fast AKNN search is to develop an efficient indexing mechanism over high-dimensional feature vectors.

A number of centralized indexing approaches [6, 7, 8, 9, 10, 11, 12, 13] for the AKNN search have been proposed in the past few decades. The centralized approaches may work well for relatively small or intermediate size of datasets. However, for large datasets, these approaches will come with severe performance problem due to limited memory storage and computing capability from the centralized computing resource.

Recently, with multimedia content growing rapidly and the scale of datasets of feature vectors becoming very large, large scale AKNN indexing and search have attracted more and more interests in multimedia retrieval. A number of recent studies [14, 15, 16] work on this using millions of images or billions of visual descriptors. However, they still fall into the centralized category without ability to scale to fit and efficiently process very large datasets that often appear in the Big Data era today. To provide efficient and scalable AKNN solutions for large high-dimensional datasets, we need to consider the distributed approach for the AKNN search.

Recently several studies on the distributed approach have been proposed [17, 18, 19].

One of major technical problems raised from the distributed approach is how to partition and balance data load across servers to build efficient index. In this paper, we use a flat randomized partitioning scheme to partition the data space into subspaces by using pivots, a set of data points selected from the dataset. We design a two-layer distributed indexing scheme based on a neighborhood graph for large scale AKNN search. The first layer of our index is a neighborhood graph of pivots, called pivot neighborhood graph. The second layer is the subspaces which are distributed over a set of machines.

The experimental results show that our method achieves excellent performance and scalability.



II. RELATED WORK A number of indexing methods [6, 9, 10, 12, 13] for AKNN  search have been proposed recently. In the following, we briefly explain three widely-used indexing methods for AKNN: KD-tree, Locality Sensitive Hashing (LSH) and the neighborhood graph method.

KD-tree: A KD-tree [9, 10] partitions the space by hyper- planes that are perpendicular to the coordinate axes. At the root of the tree a hyper-plane orthogonal to one of the dimensions splits the data into two halves according to some splitting value. Each half is recursively partitioned into two halves with a hyper-plane through a different dimension. The splitting values at each level are stored in the nodes. The query point is then compared to the splitting value at each node while traversing the tree from root to leaf to find the nearest neighbors in some order, e.g., depth-first or best-first. It is   DOI 10.1109/CBD.2013.20     mainly for low-dimensional cases and improper for AKNN search over high-dimensional visual descriptors.

Locality Sensitive Hashing (LSH): LSH [6] is a very popular approximate method for KNN search. The key idea is to hash the points using several hash functions to ensure that for each function the probability of collision is much higher for points that are close to each other than those far apart. At query time, the query point is mapped using the same hash functions and all the data points in the same hash bucket as the query point are returned as candidates. Nevertheless, LSH suffers from the required post-processing step to filter out false positives. It may achieve good precision and recall performances but has relatively poor query efficiency.

Neighborhood graph method: The neighborhood graph method organizes the data with a graph structure to connect data points, for example, Delaunay graph [20], relative neighborhood graph [21], and KNN graph [22].The intuition of its workability is that a neighbor of a neighbor is also likely to be a neighbor. When querying a new point, this method usually performs hill-climbing starting from a randomly sampled node of the graph. The main drawback of the method is that it takes a lot of time to construct the graph.

These three indexing methods have been used in a number of centralized indexing and AKNN search algorithms.

However, centralized indexing is obviously not able to work well for large scale datasets. As a result, these years several studies [17, 18] have tried to extend LSH to achieve distributed and scalable nearest neighbors search. They consider mappings from the multi-dimensional LSH bucket to the linearly ordered set of peers. The indexed data are maintained by the peers jointly. They try to achieve high quality search results and reduce the number of network accesses. However, the sizes of the evaluation datasets used in their experiments are less than the scale of ten million, which is not big enough to evaluate the effect toward the large scale datasets in real life. Recent studies [12, 13] show that the neighborhood graph method performs much better than the other two indexing methods both in recall and query speed.

The work in [12] simply used a distributed neighborhood graph method to index the datasets. One drawback in this study is that, in the case of large datasets, it will suffer from the huge time cost to construct the graph and the large communication cost at the query stage.

In this paper, we extend the neighborhood graph method to enable efficient and distributed nearest neighbor search. We adopt a flat randomized partitioning scheme to partition the data space into subspaces. Moreover, we propose a two-layer distributed indexing scheme based on a neighborhood graph for large scale AKNN search. Experimental results demonstrate that our approach achieves excellent performance and scalability.



III. OUR APPROACH  A. Basic Idea and Algorithm for Our Approach Data partitioning is a basic strategy to achieve a distributed  computation for large scale datasets. We adopt a flat randomized partitioning scheme to partition the data space  into subspaces by using pivots, and each data point is assigned to the subspace with its closest pivot.

Based on the data partitioning scheme, instead of building a global neighborhood graph for each data points in the dataset, a neighborhood graph is built for the pivots, called pivot neighborhood graph. Each node in a pivot neighborhood graph represents a subspace of the dataset.

Let ? be the global space of all data points in a datasets.

Each data point represents a multi-dimensional vector.

The basic idea here is that we partition the global space of data points ? into subspaces with the size of each subspace limited by a lower bound ? and an upper bound ?, that means:  ? ? (1)   ? 	? ? ?? ? ? ? (2)  ? ? ? ? ? ? (3)  The upper bound ? is to ensure that finding k-nearest neighbors in each subspace costs constant time. The lower bound ?  is to ensure that every single subspace at least contributes ? data points when a query comes.

Obviously the partitioned subspaces will be good for parallel processing by assigning these subspaces over distributed machines. Furthermore, each subspace can be loaded into memory for faster speedup.

For further performance improvement, given a query ?, to find the approximate k-nearest neighbors, there is no need to search all of subspaces in ?. Instead, by searching the most promising subspaces (MP), we obtain much faster performance with slight loss of precision. Based on this idea, the approximate k-nearest neighbors search problem can be solved by searching the approximate N nearest subspaces (???? ? ?).

Based on partitioning strategy and the promising subspaces ( ?? ), we can obtain the candidate approximate k-nearest neighbors set (?	) of q by:  ?	 ? ?? ? ? ! "##??? ?? ? $%& '()* ! ?? + (4)  Further we can obtain the final result by:  ,"##??? ?? ? ? -%."??? ?	? (5)  The partitioning strategy and the k-nearest subspaces method form main ideas for achieving our two-layer distributed indexing scheme for AKNN search. However, we still face several important issues for concrete processing. The first issue is how to partition the data space to achieve efficient search. The second issue is how to find the most promising subspaces efficiently. The third issue is how to implement our two-layer distributed search algorithm with proposed approach.

The following subsections introduce these key issues.

B. Partition the Space: Sample-based Method There are different ways to partition the data space. In our  approach, we use the sample-based method to learn topology (pivots) of the space.

First of all, we estimate the total number ? of partitions that we want to have. Then dividing the size of the dataset by ?, we obtain the expectation / of subspace size. On the basis of the expectation / , we set the lower bound ?  and upper bound ? . After that, we randomly sample ? distinct data points as our initial pivots and then partition the total space ? into subspaces. We remove the corresponding pivots of subspaces that violate lower bound ? and upper bound ?. In addition, we generate new pivots from subspaces that violate upper bound ?. We repeat this process to refine pivots until eq.

(1-3) are satisfied. The detailed procedure is described in the Algorithm 1. Basically around twenty thousand pivots will be generated by our sample method.

There are two advantages of this method when we have a lot of pivots. First, most of our pivots stay unchanged after the first sampling iteration completed. Thus it is much faster to get converged compared with other methods like k-means. Second,  this method is compatible with generic similarity measure since it only depends on distance measure. It enables to retrieval with non-metric distance measure, e.g. 3D model retrieval [13].

To speed up our partitioning process, we use a randomly selected subset of the total dataset as the learning dataset.

After obtaining pivots using algorithm 1, we will use these learned pivots to partition the total space ?.

C. Search most promising subspaces Since our method uses pivots to partition the data space,  given a query q, searching its most promising subspaces is converted to searching the approximate N nearest pivots. To support the search, we use the neighborhood graph to index our pivots.

To improve the accuracy for promising subspace search, we use two methods to approach global optimum. First, we use reverse neighbor information in neighborhood expansion stage. Second, in our approach, reaching an optimum means we need to select N nearest pivots so that the distances between the query point and the N nearest pivots are top N minimal ones. In our search algorithm, a max-heap with fixed size N is used to store current N nearest pivot candidates and a priority queue is used to decide neighborhood expansion order.

When a query q comes, we randomly pick N pivots from the pivot neighborhood graph as our initial candidates. Then according to the priority queue, the best candidate is extracted out, and the pivots in its neighborhood are discovered as new N nearest pivot candidates and then pushed into the priority queue. The resulting search path always attempts to move closer to the query point. When the best candidate pivot extracted from the priority queue is farther than any pivot from the max-heap, our algorithm terminates. The basic procedure is outlined in Algorithm 2.

D. Two-layer Distributed K-Nearest Neighbor Search In our method, since the size of pivots (around twenty  thousand) is relative small, we replicate our pivot neighborhood graph on all machines. This way a query can be initiated by any machine.

Fig. 1 shows the architectural overview of our distributed algorithm. The top layer is our pivot neighborhood graph, which is used to find the most promising subspaces. The second layer is the subspaces that are assigned to a set of distributed machines. Currently, we assign subspaces to distributed machines by their corresponding node id in the pivot neighborhood graph. In this way, each machine only takes charge of a small portion of the total dataset (space), thus the system achieves good load balance and scalability.

When a query q comes, we use pivot neighborhood graph to find its ? nearest subspaces and send KNN request to all related subspaces stored in the same or different machines.

Each machine received the KNN request performs a KNN search using the linear search method in the corresponding subspaces, then sends the KNN results back to the caller. After all responses from subspaces received, we collect and rank the partial results. The top k nearest neighbors will be final result.

Algorithm 1 Sample Pivots Algorithm PIVOTS (?? /? ?? ?? Input dataset ? ; lower bound L and upper bound U of the subspace size; expectation E of the subspace size Output pivots set pivotsSet // subspace: a map from pivots to data points // randomly pick N data points as initial solutions.

1: ? 0 ???1/; 23 4?5678	97 0 &(:;%<=.>)???? ??; @3 4A889? 0 $(BC' D3 E*>B' 4A889? ?? $(BC' ;% 5:     $%& '()* 4 ! 4?5678	97 ;% 6:         8FG84AH9I4J 0 ?; 7:     ':; $%& 8:     $%& '()* ? ! ? ;% // find d?s approximate nearest neighbor.

9:         ?K  0 L????? 4?5678	97?; 10:         8FG84AH9I?KJ 0 8FG84AH9I?KJ M ??+; 11:     ':; $%& // check subspaces 12:     4A889? 0 -&N'; 13:     $%& '()* 4 ! 4?5678	97 ;% 14:         >$ ?8FG84AH9I4J? O ? -*': 15:             4?5678	97 0 4?5678	97 P ?4+; 16:             8FG84AH9I4J 0 ?; 17:             .(CC'; 0 $(BC'; 18:         end if 19:         >$ ?8FG84AH9I4J? Q ? -*': 20:             4?5678	97 0 4?5678	97 P ?4+; 21:             ?4?5678 0 RST?UV?8FG84AH9I4J? /? ?? ??; 22:             4?5678	97 0 4?5678	97 M ?4?5678; 23:             8FG84AH9I4J 0 ?; 24:             4A889? 0 $(BC'; 25:         ':; >$ 26:     ':; $%& 27:     >$ 4A889? ?? -&N' -*': 28:         return pivotsSet; 29:     end if 30: end while         Fig. 1. Architectural overview of distributed

IV. EXPERIMENTS In this section we present our experiments  one billion SIFT (Scale Invariant Fea descriptors.

Algorithm 2 Most Promising Subspaces Search MPSEARCH (q, G, N)  Input query q; neighborhood graph G; number o search  N Output neighbor set R // R: a max-heap with a fixed size N; Q: a priority // V[G]: the node set of graph G; n: #accessed poin 1: $%& '()* F ! WIXJ ;% 2:     ())'CC';IFJ 0 $(BC' 3: end for 4: ? 0 Y; // choose k different node from G as initial s 5: for each ? ! IZ? ?J ;% 6:     v0 &(:;%<=:%;'?X?[ 7:     ?'\I5J  0 ;>C-?5? ??[ ())'CC';I5J 0 -&N 8:     Q0 5[ ] 0 5[ 9:     n 0 ? ^ Z[ 10: end for 11: while Q != ? do 12:     F 0 _`Ua,bU=cS#?d?[  // neighborhood expansion.

13:     for each5 ! #'>e*f%&CIFJ ;% 14:         if accessed[v] = false do 15:             ?'\I5J 0 ;>C-?5? ??[  ())'CC';I5J 0 16:             if MAX(R)>key[v] then 17:                 d 0 5[ ] 0 5[ 18:             end if 19:         end if 20:     end for  // MAX(R)<MIN(Q) means arriving at an 21:     If MAX(R)<MIN(Q) then 22:         break; 23:     end if 24: end while 25: return R;   d algorithm.

s on a collection of ature Transform)  A. Experimental Setup Datasets: Our descriptor  million images Flick1M from by the Hessian-Affine detector the software [25] with the de total of 999,290,745 128-dime occupies 160GB disk space. T scalability on different sizes of sizes of datasets to perform exp  Hardware and software: of machines connected by 1G has 4 Core Intel Xeon 2.4GH only use 1 core on each machin  Evaluation benchmark: W 100 data points as queries.

determine the ground truth of measured by averaging the que experiment is repeated 5 times  B. Partitioning Performance Algorithm 1 is an iterative  hard to converge. In our ex dataset with 100,000,000 visua number M of subspaces to 200 for subspace size is about 5 expectation E we set the lower bound to 1500. After several i pivots. Fig. 2 shows the com distribution and distribution af that the size of most subspaces which means our flat randomi well.

Fig. 2. Distribution of subs  C. Retrieval Performance We use NNDES library pro  neighborhood graph.

The number N of promis  accessed is a key parameter in better understanding of the  of neighbors to  queue; nts;  solutions.

'[  -&N'[  n optimum.

collection is obtained from 1 [24]. The images are processed  r and the SIFT descriptor, using fault parameters, resulting in a  ensional SIFT descriptors which To evaluate the performance and f datasets, we generate different periments.

The evaluation is done on a set bps network and each machine  Hz CPUs and 24GB RAM. We ne to perform our experiments.

We use an independent set of We run a sequential scan to  f the queries. The query time is ery time over 100 queries. Each and the average is recorded.

e process, sometimes it may be xperiments, we use a learning al descriptors. We set the initial 0,000 so that the expectation E 00. Then on the basis of the r bound to 10 and set the upper iterations, we obtained 237,629  mparison of final subspace size fter first iteration. It can be seen lies around the expectation 500, ized partitioning scheme works   space sizes using our method.

ovided by [23] to generate pivot  sing subspaces which will be n our approach. In order to get a  impact of this parameter, we     compare the accuracy and time cost with different settings (k=1, 10, 100) over 1 billion dataset using 10 machines.

Fig. 3. The number of most promising subspaces with corresponding  accuracy.

Fig. 4. The number of most promising subspaces with corresponding time  cost (ms).

Fig. 3 and Fig. 4 show that when the number N of most promising subspaces is 300, which means that the query is compared to approximately 1/1000th of the indexed vectors, our approach achieves best performance (96% when k=1) on this scale in about 155ms. However, in [15], their search quality is measured by the recall@r measure, i.e., the proportion of queries whose nearest neighbor is ranked in the first r positions.

Thus, their methods cannot tell which one in the first r positions is the true nearest neighbor. Another thing to be noted is that our approach also achieves a high recall when k=10 and k=100 which is not reported by [15, 16] on this scale.

To evaluate the scalability of our approach, we compare the time cost under different data sizes and different numbers of machines. In these experiments, we use the same learned pivots to partition the dataset.

Fig. 5. Time cost(ms) with different data sizes when access 500 most  promising subspaces using 10 machines.

Fig. 5 shows that with the growth of data sizes, the time cost of our approach increases slightly in almost linear trend.

The increase of time cost is mainly caused by the increased size of each subspace.

Fig. 6. Time cost (ms) of different numbers of machines when access 500  most promising subspaces.

Fig. 6 shows that, with the growth of the numbers of machines, the time cost of our approach will decrease and then slightly increase after a curtain time point. With the growth of the numbers of the machines, there are two factors which influence the time cost. First, the workload of subspaces KNN search will distribute to different machines. Thus the total time cost will decrease. Second, the communication cost will increase, thus the total time cost will increase. Fortunately, even the communication cost would increase, there exists an upper bound because we only need to access a curtain portion of the total subspaces. In a large scale nearest neighbor search system, this cost is negligible.



V. CONCLUSIONS This paper presents an efficient and scalable solution to the  distributed approximate k-nearest neighborhoods search     problem. The dataset for experiments is over 1 billion of high dimensional visual descriptors generated from flick1M. By using a flat randomized partitioning scheme and a two-layer distributed indexing scheme, we extend the existing neighborhood graph method to enable efficient and scalable k- nearest neighbor search for large scale AKNN search.

Experimental results show that our algorithm achieves high precision and recall and excellent scalability as well.

As a future work, we are preparing experiments using an order of magnitude larger collection. We are also planning to implement a near-duplicate image detection system by using our method.

