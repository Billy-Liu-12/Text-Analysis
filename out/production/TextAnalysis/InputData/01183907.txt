Mining Generalized Association Rules Using Pruning Techniques

Abstract  The goal of the paper is to mine generalized association rules usingpruning techniques. Given a large transaction database and a hierarchical fmonomy tree of the items, we try tofind the association rules between the items at dxerent levels in the faronomy free under the assumption that original frequenf itemsets and association rules have already been generated beforehand. In the proposed algorithm G M R ,  we use join methods and pruning techniques to generate new generalized association rules. Through several comprehensive experiments, we find that the G M R  algorithm is much better than BASIC and Cumulafe algorithms.

1. Introduction  In recent ten years, with the developments of information and advances of Internet, vast data have Seen propagated and recorded in the databases of different applications. Currently data mining is arising to understand, analyze, and use these data. Data mining is designed for finding interesting samples in a large database, and thus it is also the knowledge exploration center part [3, 51. One of the major tasks of data mining is to find association rules for helping retail industries to understand the consumers? behaviors [ I ,  21.

In data mining, the most well-known method is the Apriori algorithm [Z]. Since the Apriori algorithm is costly to find candidate itemsets, many researches have been trying to improve it; i.e., reducing the size of candidate itemsets. For example, DHP algorithm is one using a hash-based skill to reduce the size of candidate itemsets [9]. Besides, someone used transaction reduction techniques, including partitioning proposed by Savasere et al. [IO] and sampling proposed hy Toivonen 1141, to generate frequent itemsets quickly. What?s more, a kind of data structure called FP-Tree (Frequent Pattern Tree) was proposed by Han et al. to directly produce frequent itemsets, not candidate itemsets [6].

The motivation of our research is initiated with two  situations. One is that although mining association rules has completed, but it did not consider the taxonomy tree at that time. Another is that although the taxonomy tree was considered, it may need to be adjusted as time goes by, and thus the generalized association rules mined before are not useful any more. Till now, all proposed researches can only mine all association rules from scratch. In the paper, our goal is to find generalized association rules without re-scanning the original database. Here the pre-knowledge including the taxonomy tree, the original frequent itemsets and association rules both generated beforehand, can be used to mine new generalized association rules. The data type used in the paper is a transactional data form [7]. At first, we create maximal itemsets [S, 131 from the original frequent itamsers. Next, given a hierarchical taxonomy tree [4, E], we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table [S, 1 I]. Then, we use the original frequent itemsets L, and L2, and non-leaf items with the supports 2 the minimum support to create an association graph. In our paper different from those ones [S, IS], we assume the minimum supports of all the items are the same. Finally, in our algorithm GMAR (Generalized Mining Association Rules), we use join methods and pruning techniques to generate new generalized association rules. In the experiments, we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms [12], since it generates fewer candidate itemsets, and furthermore prunes a large amount of irrelevant rules based on the minimum confidence.

The remainder of the paper is organized as follows. In Section 2, the mining problem for generalized association rules is defined. We propose the GMAR algorithm using the original frequent itemsets and association rules to find generalized association rules in Section 3. In Section 4, several experiments are undertaken and the results show the superiority of the GMAR algorithm over BASIC and Cumulate algorithms. Finally, we make a conclusion in Section 5.

2. Problem descriptions  227 0-7695-1754-4/02 $17.00 Q 2002 IEEE  <-? . .

The problem investigated here is to mine generalized association rules according to a large transaction database D and a hierarchical taxonomy tree H of the purchased items. Transaction database D is a set of transactions where each transaction T consists of a set of items, called an itemset, such that T L 1. Here 1 =(i,, i2, i,, ..., im) is a set of the items purchased by all the transactions. Besides, hierarchical taxonomy tree H is a directed acyclic graph on the items where an edge in H represents an "is-a" relationship between items. For example, if there is an edge from p to c in H, we call p a parent of c and c a child of p. In other words, we can say p represents a generalization of c. Further, if there is a path from d to e in the transitive-closure of H, we call 3 an ancestor of e, and e a descendant of 2. Since W is acyclic, any item cannot be an ancestor of itself. Besides all the leaf items in H must come from I.

Recent researches are getting interested in finding the association rules spanning several different levels in the taxonomy tree, here called generalized association rules. A generalized association rule is an implication of the form X + Y, where X E I ,  Y GI, X n Y = 0, and no item in Y is an ancestor of any item in X. If c?? of the transactions in D that support X also support U, we say that X Y holds in the transaction database D with confidence c. The definition of Support(X) is the number of transactions purchasing X, divided by the number oftransactions in the database, as shown in formula ( I ) .  The definition of Conf(X 3 Y) is the support of both antecedent and consequent divided by the support of antecedent in the rule, as shown in formula (2).

Support(X) = X . C O ~ ~ ~ ~ ~ ~ ~ ~  ................................... (1)  conf(x y )  = support(X Y) ~uppor t (x ) . . . . - . . . ( 2 )  Definitely, we can find generalized association rules from scratch, given a large transaction database D and a hierarchical taxonomy tree H of the purchased items.

However it is not avoidable to scan the original database once more, and this will be very inefficient. Thus an efficient mining algorithm proposed here is to make use of the original frequent itemsets and association rules that have already been generated beforehand, to produce new generalized association rules, rather than re-scanning the original database. Besides we also propose some pruning techniques to speed up the mining process.

3. Mining algorithms  3.1. Processing flow of mining a lgor i thms  The processing flow of our mining algorithm for finding generalized association rules is shown in Figure 1.

Supposed that the components shown inside the dotted  box, such as vertical-TID-vector table, frequent itemsets, and association rules, were generated before our mining algorithm is executed. Rather than scanning the original database, we make use of the vertical-TID-vector table transformed from the original database to mine generalized association rules. Since the bit storage is used in the vertical-TID-vector table, the memory space and processing time to get the database information will be saved. Besides the original frequent itemsets and association rules generated beforehand should not be thrown away, since we need them to produce generalized association rules.

Figure 1. Processing flow of mining algorithms  At first, we create maximal itemsets from the original frequent itemsets, and then store them in an array to reduce disk space and speed up the mining operation.

Although no subsets of a maximal itemset are stored, we can use a hash function to calculate the position of each subset in a maximal itemset and find its support. Next, given a hierarchical taxonomy tree, we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table. Basically, for a non-leaf item I being the ancestor of {i,, i2, ..., i.} where items i,,  i2, ..., and in are leaf-items, the support of item f can found in the array ofmaximal itemset I if {i,, iZr ..., in) is a subset of maximal itemset I. Then, we use the original frequent itemsets L, and L2, and non-leaf items with the supports 5 the minimum support to create an association graph. It facilitates check whether k-itemsets (k L 3) involving non-leaf and leaf items are frequent or not.

Finally, in our algorithm GMAR, we use join methods and pruning techniques to generate new generalized association rules. The join methods used in the GMAR algorithm can directly produce generalized association rules from the original association rules, and the pruning techniques are used to prune irrelevant rules, thereby speeding up the production of generalized association rules.

3.2. T h e  vertical-TID-vector table  The vertical-TID-vector table was generated before the     mining algorithm is executed. It uses bit vectors to record the transaction information. Each item is represented with a bit vector where the length of the bit vector is the total number of transactions in the database. If an item appears in the j" transaction, the j" bit of  the corresponding bit vector is set to 1; otherwise, the bit is set to 0. As shown in Figure 2 and 3, Since item i3 appears in transactions 1, 8, 9, and IO, the bit vector for item i, can be expressed as [1000000111]. The bit vector facilitates the support computation of an itemset { i,, iz. i3, ..., ik).

Figure 3. Vertical-TID-vector table  3.3. Creating maximal itemsets  A maximal itemset is defined as a frequent itemset not contained in any other frequent itemset. For example, given a set offieqnent itemsets {{ill,  {iz), {i3), (i4), {is),  itemsets are {&), {i2, is) and (il, i3, is}. As shown in Figure 1, the procedure to find the maximal itemsets from a set of frequent itemsets is described as follows:  Gen-Max-ltemset: Step 1: Initialize all frequent itemsets unmarked.

Step 2: Set k the maximal length of frequent itemsets.

Step 3: For each unmarked frequent k-itemsets,  Step 3.1: Generate all the proper subsets.

Step 3.2 If any frequent h-itemset p (h < k) is one  ofthe subsets, markp.

Step 4 k=k-l and then repeat from Step 3 until k=l.

Step 5 :  The maximal itemsets are those not marked so far.

An example, as shown in Figure 4, illustrates how to find the maximal itemsets from a set of frequent itemsets  {it, i3), {il, is), {iz, is), (i3, id. {it, i3, is)), the maximal  where the unmarked itemsets are the maximal ones; i.e., ( id ,  {iz, id and {il, i3, id.

Figure 4. Finding maximal itemsets  Here we only need to store the maximal itemsets, as shown in Figure 5, to reduce disk space and speed up the mining operation. In the figure, the supports of all non-empty subsets of maximal itemset (il, i3, is) are 0.5 for ( i l ) ,  0.4 for {i3), 0.7 for {is), 0.3 for {i,, i3), 0.5 for (i,, is), 0.3 for {i3, is), and 0.3 for {il,  i3, is), respectively.

Instead of storing all non-empty subsets of a maximal itemset, we can use the hash function to calculate the position of any subset in a maximal itemset and find its support. For example, through the hash function, we can compute the value 5 for the position of {i,, is} in maximal itemset {i,, i3, is}, and find its support value 0.5. The hash function used here can he expressed as follows:  T: the length of a maximal itemset, L: the length of a subset itemset X(i): the position in a maximal itemset for the ith item of a  HE the position of a subset item in a maximal itemset.

subset itemset, where 1 < i < L and X(O)=O  14 10.3 I i,, is 10.4 10.7 10.4 I i , ,  i3, is 10.5 10.4 10.7 10.3 10.5 10.3 10.3 I  Figure 5. T h e  maximal itemsets  3.4. Calculating the supports of non-leaf items  Given the original frequent itemsets and association rules already generated beforehand, and a hierarchical taxonomy tree, we can produce new generalized association rules, rather than re-scanning the original database. However the items, except those at the leaf level in the taxonomy tree, do not appear in the original frequent itemsets and association rules. Therefore we must calculate the supports of the non-leaf items in the taxonomy tree from the maximal itemsets or VTV table.

For a non-leaf item I being the ancestor of {it, i2, _.., i.) where items i,, i2, ..., and i. are leaf-items, the support of     item f can be calculated as follows:  Support(r)=I[HF((i,,i2, ..., in))],if(i,,i2, ..., in) i s a subset of maximal itemset I  or Support(1) = Ivector(i,) OR ._. OR vector(i,)l/no. of trans.

Here we take an example to illustrate how the supports of non-leaf items are calculated. Given the VTV table and the taxonomy tree as shown in Figure 3 and 6, we can calculate the supports of non-leaf items 1001, 1002, and 1003 as follows:  Support( 1001) = Ivector( 1) OR vector(2)l/no. of trans.

= ~[Ol1001111 l]i/lO = 0.7  Support(1002) = Ivector(1) OR vector(2) OR vector(3)l /no. of trans.

= I[ 11 lOOlllll]~/lO = 0.8 Support(1003) = jvector(4) OR vector(5)l/no. of trans.

= /[011111 I 1 1  l]l/lO = 0.9  A A 1 2  Figure 6. Taxonomy t ree  3.5. Creating association graphs  In our mining algorithm, different from the Apriori algorithm, we only use the original frequent itemsets L, and L2 to generate k-itemsets (k b 3) involving non-leaf and leaf items and check whether they are frequent or not.

Our method is based on a graph called association graph to search all these k-itemsets. The association graph is defined as follows:  A graph AG(V, E), called association graph, consists a set of vertices or items V and a set of edges E where 1) V = L, U T where T = (v I v is a non-leaf item and  support(v) 2 the minimum support), and 2) E = (w I (U, v) in Lzf U (x-y I at least one item  of (x, y) is in T, x and y are not in the ancestor-offspring relationship, and support((x, y))  b the minimum support)  Given the VTV table and the taxonomy tree as shown in Figure 3 and 6, the minimum support 0.3, and the original frequent itemsets LI and Lz, we can obtain the  frequent itemsets T, and then create the association graph as shown in Figure 7.

LI = (1,2,3,4,51, Lz= { ( I ,  31, { I ,  51, (2, 51, (3,511, T =  (IOOl, 1002,1003)  Support(( I ,  1003)) = 0.5, Support((2, 1003)) = 0.4 Support((3, IOOI}) = 0.3, Support((3, 1003)) = 0.3  Support((5, lOOl))=O.7, Support((S,lO02))= 0.7 Supp0rt({1001,1003}) = 0.7, Support(( 1002,1003})= 0.7  Support((4, I O O I ) ) = O . 1 ,  support((4,1oo2))=o.I    Figure 7. Association graph  3.6. Pruning techniques  In the GMAR algorithm to be discussed in the next section, we use pruning techniques to generate new generalized association rules since pruning irrelevant rules can speed up the production of generalized association rules. Here we have six pruning techniques described as follows:  PT 1: For a frequent itemset I where i,," is the item with the minimum support wunt within it, if support((i,,.))/support(subset(l)) is less than the minimum confidence, then we can prune any rule subset(1) =) I - subset(1).

Rationale: For any rule subset(1) 3 I - subset(l), the confidence check is support(l)/support(subset(l)). Since support(l) must be less than or equal to support((i,,.)), support(I)/support(subset(I)) is definitely less than the minimum confidence as long as support({i,,.))/support(subseqI)) is less than the minimum confidence. In other words, we can use support((i,,.}) instead of support(1) in the confidence check.

PT 2: For a subset P in a frequent itemset I, if the rule P =) I - P holds, then any rule whose antecedent containing P, such as superset(P) I - supenet(P), holds as well.

Rationale: If the rule P a I - P holds, it implies that     support(l)/support(P) is more than or  equal to the minimum confidence. Nevertheless, since support(superset(P)) is less than or equal to support(P), support(l)/support(superset(P)) is definitely more than or equal to the minimum confidence, and thus the corresponding rule holds as well.

PT 3: For two rules with the same antecedent, if 1) the rule P = QI holds and 2) the rule P =) 42 does not hold due to support(P U Q2) < the minimum support, then all the rules corresponding to the itemset 1 (= P U QI U Q2) do not hold.

Rationale: If one of the rules corresponding to the itemset I holds, then I must be a frequent itemset and all the subsets are frequent itemsets as well.

However we know P U Q2 is not a frequent itemset since support(P U Q2) is less than the minimum support. Thus all these rules corresponding to the itemset I do not hold. For example, if 1) the rule i, i2 holds and 2) the rule i ,  3 i3 does not hold due to support((i,, i3}) c the minimum support, then all the rules, such as i, =r i2h i3, i2 3 i, A i3, i3 3 i, A i2, i2 A i, 3 i,, i ,  A i3 3 i2, and i, A i2 3 i3, do not hold.

PT 4: If the rule P 3 Q, holds, then the rule P 3 4 2 always holds where Q2 is derived from Q1 by replacing some items in QI  with their ancestors.

Rationale: If the rule P 3 Ql holds, it implies that support(P U Q,)/support(P) is more than or equal to the minimum confidence. Since support(Q2) is more than or equal to  definitely more than or equal to the minimum confidence and thus the rule P a Q2 always holds.

support(QJ, support(P U Q z ) / s u P P o ~ ~ ( ~ )  is  PT 5 :  For two rules with the same antecedent, if 1) the rule P 3 Q, holds and 2) the rule P * Q2 does not hold due to not satisfying the minimum confidence although it satisfies the minimum support, then the rule P a  Q, U Q2 does not hold.

Rationale: For the confidence check support(P U Q2)/support(P) < the minimum confidence, support(P U Q, U Q2)/support(P) is definitely less than the minimum confidence since support(P U QI U Q2) is less than or equal to support(P U Q2). Thus the rule P QI U Q2 does not hold.

PT 6: If any item in a frequent itemset 1 does not appear in association graph AG or all the items in I do not  form a complete connection in AG then, for a subset P of I, the rule P 3 I - P does not holds.

Rationale: If the rule P 3 I - P holds, support(l)/support(P) is not only more than or equal to the minimum confidence, but also support(1) should be more than or equal to the minimum support. In other words, all the items in I would appear in AG and form a complete connection. This contradicts the assumption, and therefore the rule P 3 I - P does not holds.

3.7. GMAR algorithm  The Apriori algorithm proposed by Agrawal and Srikant is a two-step process which consists of join and pruning actions to find frequent itemsets, and then uses the frequent itemsets to derive association rules. In the section, an algorithm GMAR (Generalized Mining Association Rules) is proposed, which generates generalized association rules not directly based on the raw data from the database, but based on the original frequent itemsets and association rules. Here an association rule is called weak when it satisfies the minimum support threshold, but not minimum confidence threshold. In the GMAR algorithm, we need both strong and weak association rules in the current level ( k - l )  to generate the generalized association rules for from the next level k to the next 2k-3 level, as shown in Figure 8. Therefore, the generation of the generalized association rules is not based on a step-by-step manner. The detailed algorithm is described as follows:  GMAR Algorithm: Input: VTV table, original association rules (RJ,  taxonomy tree, min-sup, min-conf Output: new generalized association rules Method Step 1: Generate new frequent I-itemsets L., using VTV  Step 2: Create association graph AG using original table and taxonomy tree:  frequent itemsets Lo, and Lo2, and new frequent I-itemsets LnI;  {x, y) is not in Lo2 Step 3: For each edge x--y in association graph AG where  Add the rules ?x 3 y ? and ?y 3 x ?  to k: If support((x, y})/support(x) 2 min-conf  Set the rule ?x 3 y ?  strong; If support((x, y})/support(y) 2 min-conf  Set the rule ?y 3 x ?  strong: Step 4: For(k = 3; Rn(k.,l# 0; k++)  Generate weak (k-])-association rules using the original frequent (k-1)-itemsets LNk.1) and add them to &cl,; GMAR-Genkk.1 1, &k.i 3;  23 1    GMAR-Gen(R,,w.t), R, , (x - i ) ) ; Step 5: R,, = uk {r  1 r E & and r is a strong rule];  Procedure GMAR-Gen(R,, R2) 1. For each rule rl E RI  For each rule r2 B R2 { I? Using PT 5 */  If (r,.antecedent = r,.antecedent) and (r,.consequent f r2.consequent) and (conf(rl) t min-con0 and (conf(r2) 5 min-con0  I* Using PT 3 and PT 6 *I Ifall the items in (r,.antecedent) U  (rl.consequent) U {r,.consequent) can form a complete connection in association graph AG  I* Using PT 2 and PT 4 */ r = ?r,.antecedent 2 r,.consequent  i = length(r); A r2.consequent ?;  /* Using PT 1 */ Add the ruler to ki; If con@) t min-conf  Set the ruler strong; If (r,.antecedent # r,.antecedent) and  (r,.consequent = r,.consequent) I* Using PT 3 and PT 6 */  If all the items in (r,.antecedent} U (r2.antecedent) U (r,.consequent] can form a complete connection in association graph AG  I* Using PT 2 and PT 4 */ r = ?rl.antecedent A rz. antecedent  i = length(r);  Add the rule r to Kj; If conf(r) t min-conf  =1 r, .consequent?;  I* Using PT 1 */  Set the rule r strong;  Figure 8. GMAR algorithm to generate new generalized association rules  Given the VTV table (a5 shown in Figure 3), the original frequent itemsets Lo, and Lo2 (as shown in Figure 4), the original association rules & (including the strong rules in and h3), the taxonomy tree (as shown in Figure 6),  the minimum support 0.3, and the minimum confidence 0.5, new generalized association rules R,, (including the strong rules in Rd and R?,) can be generated using the GMAR algorithm as follows. Among the rules, a rule is marked if it is a strong one.

R o 2  Rules Confidence Rule? Confidence 1-3 0.6  3=1@3? . OL7s s ~ l o o l - ~  1 1003a2 0.4444444 lool5s- -1? 1001=3 0.4285714 . - 1003-3 0.3333333  Ro3  Rules Confidence Rules. Confidence  :1,3-5 .1r--- 5=1,3 0.4285714  &= (&z join R d  U (R,,, join KO) Rules ~l=3,1003 0.6 3a1,1003 0175 1,321003 ,. 1: J ; I O O ~ ~  ~. . 0.6 100 1 2 3 , 5  0.42857 I4 ~ , 1 0 0 3 s I  . I 5,100123 0.4285714 3=S5,1001. 0.75~ 523,1001 0.4285714 !,5*1001 . 1 t 00 1,1003=.3 0.42857 14 3,1001~S1 .. I 1001 -3,1003 0.4285714 3=>l001,1003 0.75 . , :1003~3,1001 0.3333333  -, . - ... Confidence Rules-. . . . Confidence  %4=0  K4 = ( r  I r E (4, join %,) U (&,join Rn3) and length(r) =4) =0  4. Performance evaluations     4.1. Simulation model  In the section, we evaluate the performances of the three algorithms, including BASIC 1121, Cumulate [12], and GMAR, on a DELL PowerEdge 4400 Server with Intel" Xeon Processor and 756MB main memory running Windows 2000 server. All the experimental data are generated randomly and stored on a local 30GB SCSI Disk (Ultra 160) with a RAID controller. The relative simulation parameters are shown in Table 1. To make our data representative, we generate two types of databases in the experiments; i.e., DENSE databases and SPARSE databases. Each item in the DENSE database is randomly generated from a pool P called potentially frequent itemsets with size 300, while each item in the SPARSE database is randomly generated from a pool N (i.e., the set of all the items) with size 1000. Since the items in the DENSE database are more clustered than those in the SPARSE database, larger frequent itemsets will probably be produced in the DENSE database for the same minimum support. Besides, we use the notations T for average number of items per transaction, I for average number of items in a frequent itemset, and D for number of transactions. For example, the experiment labeled with 7lOB.DIK represents the simulation environment with IO items on the average per transaction, 3 items on the average in a frequent itemset, and 1000 transactions in total.

Table 1. Simulation parameters with default values ID INumber of transactions ~1000-500,000~ 7' INumber ofthe items per transaction 15-15 P INumber of potentially frequent 1300 . .

litemsets I I [Number of the items in a frequent 12-5  4.2. Experimental  results  Experiment 1:  In the experiment, we explore the execution time of BASIC, Cumulate, and GMAR algorithms for the environment 7lO.L3,DIK under different minimum support and minimum confidence pairs, as  shown in Figure 9. In the figure, we find that our algorithm GMAR is almost faster 2-16 times than BASIC, especially for larger minimum support and minimum confidence pairs, whereas Cumulate is only faster 1.3-1.5 times than BASIC, although R. Srikant and R. Agrawal claimed that Cumulate runs faster 2-5 times than BASIC 1121. In general, the larger the minimum support and minimum  confidence pair is, the faster the execution time of the three algorithms becomes. To be fair to all algorithms, we have added the extra time of generating original frequent itemsets and association rules for GMAR. However the time is helow 1% of total execution time; thus we do not show it in the figure.

h n  \-  B a- 3 i.m.26 i . m a  1.~1.3 i.mm 1.740.3 I . w . ~  Figure 9. Execution time for different pairs minimum support (%)hinimum Confidence  Experiment 2:  In the experiment, we extend Experiment 1 by fixing the minimum support IS%, and observe their variations.

For the minimum support 1.5%, all the algorithms except GMAR are not sensitive to the changes of the minimum confidences, as shown in Figure IO. The reason is that larger minimum confidences will make GMAR prune more irrelevant rules. Nevertheless, GMAR is still in the first rank.

-D  ~~~  .2 4M - c m 22% -=-CUlIIUlStt .- gm +-BASIC 8 Irn  0.26 0.28 0.3 0.32 0.34 0.3  minimum confidence  Figure 10. Execution time for different minimum confidences  Experiment 3:  In the experiment, we explore the execution time of the three algorithms for the environment ZlO.I3.DxK (i.e., different numbers of transactions) generated in the SPARSE database and in the DENSE database, as shown in Figure Il.(a) and (b), respectively. Both cases have the same minimum confidence 0.3. However, to get comparable number of frequent itemsets, we set a smaller minimum support 1% in the SPARSE case, and a larger     minimum support 2% in the DENSE case. As expected, GMAR is still the hest one among them in the SPARSE and DENSE case, especially when there are a huge amount of transactions. From the both cases, we find that much more frequent itemsets are generated in the DENSE database than in SPARSE database, so that BASIC and Cumulate are not practicable candidates there.

B  Ixa 8 I m o ? i x a 8 0 0 I m o  3033 m m loo30  number of transactions  Figure 11 .(a) Execution time for different numbers of transactions in the SPARSE database  -cumh?t  .- 6 loo30 $ rma ? 0  lorn m 5m 7 c w  IwD3 number of transactions  Figure 1 l.(b) Execution time for different numbers of transactions in the  DENSE database  5. Conclusions  In the paper, we try to find the association rules between the items at different levels in the taxonomy tree under the assumption that original frequent itemsets and association rules have already been generated beforehand.

The primary challenge is how to make use of the original frequent itemsets and association rules to  directly generate new generalized association rules, rather than rescanning the database. In the proposed algorithm GMAR, we use join methods and pruning techniques to  generate new generalized association rules. Through several comprehensive experiments, we find that the GMAR algorithm is much better than BASIC and Cumulate algorithms, since it generates fewer candidate itemsets, and furthermore prunes a large amount of irrelevant rules based on the minimum confidence.

6. Acknowledgments  This research was supported in part by the National Science Council, Taiwan, under contract NSC-90-22 13-E-224-026.

7. References  [I] R. Agrawal, T. Imielinski, and A. Swami, ?Mining Association Rules between Sets of Items in Large Databases,? 1993, pp. 207-216.

121 R. Anrawal and R. Srikant ?Fast Alaorithms for Mmine Adsociation Rules,? Pmc. 2Vh Internationaiconference on Ve; Large Data Bases, 1994, pp. 487-499.

131 Yong-Jian Fu, ?Data Mining,? IEEE Potentials. Yol. 16, No.

4, 1997, pp. 18-20.

141 Jia-Wei Han and Yong-Jian Fu, ?Mining Multiplelevel Knowledge and Data Engineering, Yo/ .  11, No. 5, 1999, pp.

798-805.

[5] Iia-Wei Han and Micheline Kamber, Data Mining; Concepts and Techniques, Morgan Kaufmann Publishers, 2001.

[6] Iia-Wei Han, lian Pei, and Yi-Wen Yin, ?Mining Frequent Patterns without Candidate Generation,? Pmc. ACM 1-12.

[7] Mon-Fong Jim& Shian-Shyong Tseng, and Shan-Yi Lia, ?Data Types Generalization for Data Mining Algorithms,? Pmc.

Cybernetics, 1999, pp. 928-933.

[8] Bing Liu, Wynne Hsu, and Yi-Ming Ma, ?Minin Association Rules with Multiple Minimum Supports,? Pmc. 5  B  DataMining, 1999, pp. 337-341.

191 J. S. Park, M. S. Cben, and P. S. Yu, ?An Effective  -  H&h-based Algorithm for Mining Association Rules,? Pmc.

ACM Internotional Conjerence on Mamgement o/ Data, 1995, pp. 175-186.

[IO] A. Savasere, E. Omiecinski, and S. Navathe, ?An Efficient Algorithm for Mining Association Rules in Large Databases,? P& 21? lnternationk Conference on Very La&. Data Bases, 1995, pp. 432-443.

I l l ]  Pradeep Shenoy, layant Haritsa, S .  Sudarshan, Gaurav Bhalotia, Mayank Bawa, and Devavrat Shah, ?Turbo-charging Vertical Mining of Large Databases,? Pmc. ACM International Conference on Management of Data, 2000, pp. 22-33.

[I21 R. Srikant and R. Agrawal, ?Mining Generalized Large DataBases, 1995, pp. 407-419.

[I31 S .  Y. Sung, K. Wag, and L. Chua, ?Data Mining in a Large on Systems, Man, and Cybernetics, 1996, pp. 988-993.

[I41 H. Toivonen, ?Sampling Large Databases for Association Bases, 1996, pp. 134-145.

