G.Kesavaraj#, Dr.S.Sukumaran*

Abstract: Data mining is a process of inferring  knowledge from such huge data. Data Mining has  three major components Clustering or Classification,  Association Rules and Sequence Analysis. By simple  definition, in classification/clustering analyze a set of  data and generate a set of grouping rules which can  be used to classify future data. Data mining is the  process is to extract information from a data set and  transform it into an understandable structure. It is  the computational process of discovering patterns in  large data sets involving methods at the intersection  of artificial intelligence, machine learning, statistics,  and database systems. The actual data mining task is  the automatic or semi-automatic analysis of large  quantities of data to extract previously unknown  interesting patterns. Data mining involves six  common classes of tasks. Anomaly detection,  Association rule learning, Clustering, Classification,  Regression, Summarization. Classification is a major  technique in data mining and widely used in various  fields. Classification is a data mining (machine  learning) technique used to predict group  membership for data instances. In this paper, we  present the basic classification techniques. Several  major kinds of classification method including  decision tree induction, Bayesian networks, k-nearest  neighbor classifier, the goal of this study is to  provide a comprehensive review of different  classification techniques in data mining    Keywords- classification algorithms, Decision  tree, KNN, Support vector machine, Data Mining.



I.INTRODUCTION  Classification is used to classify each item in a set  of data into one of predefined set of classes or  groups. The data analysis task classification is where  a model or classifier is constructed to predict  categorical labels (the class label attributes).

Classification is a data mining function that assigns  items in a collection to target categories or classes.

The goal of classification is to accurately predict the  target class for each case in the data. For example, a  classification model could be used to identify loan  applicants as low, medium, or high credit risks. A  classification task begins with a data set in which the  class assignments are known. For example, a  classification model that predicts credit risk could be  developed based on observed data for many loan  applicants over a period of time. In addition to the  historical credit rating, the data might track  employment history, home ownership or rental, years  of residence, number and type of investments, and so  on. Credit rating would be the target, the other  attributes would be the predictors, and the data for  each customer would constitute a case.

Classifications are discrete and do not imply order.

Continuous, floating-point values would indicate a  numerical, rather than a categorical, target. A  predictive model with a numerical target uses a  regression algorithm, not a classification algorithm.

The simplest type of classification problem is binary  classification. In binary classification, the target  attribute has only two possible values: for example,  high credit rating or low credit rating. Multiclass  targets have more than two values: for example, low,  medium, high, or unknown credit rating. In the model  build (training) process, a classification algorithm  finds relationships between the values of the  predictors and the values of the target. Different  classification algorithms use different techniques for  finding relationships. These relationships are  summarized in a model, which can then be applied to  a different data set in which the class assignments are  unknown. Classification has many applications in  customer segmentation, business modeling,  marketing, credit analysis, and biomedical and drug  response modeling. Data classification is defined as  two-step process shown in figure 1.

Figure: 1 illustrating classification task.

A Study On Classification Techniques in Data  Mining  IEEE - 31661  4th ICCCNT - 2013 July 4 - 6, 2013, Tiruchengode, India    Step 1: A classifier is built describing a  predetermined set of data classes or concepts.

(This is also known as supervised learning).

Step 2: Here, the model is used for classification.

First, the predictive accuracy of the classifier is  estimated. (This is also known as unsupervised  learning).

The commonly used methods for data mining  classification tasks can be classified    into the  following groups.1.Decision tree  induction methods,  2.Rule-based methods, 3.Memory based learning, 4.

Neural networks, 5.Bayesian network, 6.Support  vector machines.



II.DECISION  TREE  INDUCTION  Decision tree learning is a method commonly used  in data mining. The goal is to create a model that  predicts the value of a target variable based on  several input variables. Each interior node  corresponds to one of the input variables; there are  edges to children for each of the possible values of  that input variable. This is illustrating in Figure 1.

Each leaf represents a value of the target variable  given the values of the input variables represented by  the path from the root to the leaf.

Decision tree induction algorithms are function  recursively. First, an attribute must be selected as the  root node. In order to create the most efficient (i.e.,  smallest) tree, the root node must effectively split the  data. Each split attempts to pare down a set of  instances (the actual data) until they all have the same  classification. The best split is the one that provides  what is termed the most information gain.

Fig 2: Decision tree induction    The basic algorithm for decision tree induction is a  greedy algorithm that constructs decision trees in a  top-down recursive, divide-and-conquer manner. The  Tree induction algorithm, summarized as follows.

Step1: The algorithm operates over a set of  training instances, C.

Step2: If all instances in C are in class P, create a  node P and stop, otherwise select a feature or  attribute F and create a decision node.

Step3: Partition the training instances in C into  subsets according to the values of V.

Step4: Apply the algorithm recursively to each of  the subsets C.

These algorithms usually employ a greedy strategy  that grows a decision tree by making a series of  locally optimum decisions about which attribute to  use for partitioning the data. For example, Hunt's  algorithm, id3, c4.5, cart, sprint are greedy decision  tree induction algorithms.

A. Hunt's Algorithm  Hunt's algorithm grows a decision tree recursively  by partitioning a training data set into smaller, purer  subsets. This algorithm contains two steps in order to  construct a decision tree.

Step 1: which is the terminating step for the  recursive algorithm, checks if every record in a node  is of the same class. If so, the node is labeled as a leaf  node with its classification the class name of all the  records within.

Step 2: If a node is not pure then selects/creates an  attribute test condition to partition the data into two  purer data sets. From here a child node is created for  each subset.

The algorithm recurses until the all leaf nodes are  found. A common means of deciding which attribute  test condition should be used is the notion of the  "best split." This concept boils down to choosing the  test condition that results in subsets that are purer  (where purity is richer when the set contains records  of the same class). Three common formulas for  calculating impurity is entropy, gini, and  classification error.

B. (Iterative Dichotomiser 3) Algorithm  ID3 (Iterative Dichotomiser 3) is an algorithm  invented by Ross Quinlan used to generate a decision  tree. ID3 is the precursor to the C4.5 algorithm.

The ID3 algorithm can be summarized as follows:  Take all unused attributes and count their entropy concerning test samples  Choose attribute for which entropy is minimum (or, equivalently, information  gain is maximum)  Make node containing that attribute  C. C4.5 Algorithm  C4.5 is an algorithm used to generate a decision  tree developed by Ross Quinlan.[1] C4.5 is an  extension of Quinlan's earlier ID3 algorithm. The  decision trees generated by C4.5 can be used for  IEEE - 31661  4th ICCCNT - 2013 July 4 - 6, 2013, Tiruchengode, India    classification, and for this reason, C4.5 is often  referred to as a statistical classifier.

In pseudocode, the general algorithm for building  decision trees is:  1. Check for base cases 2. For each attribute a 3. Find the normalized information gain from  splitting on a  4. Let a_best be the attribute with the highest normalized information gain  5. Create a decision node that splits on a_best 6. Recurse on the sublists obtained by splitting  on a_best, and add those nodes as children of node  D. RndTree (Random Forest)  Random Forests grows many classification trees.

To classify a new object from an input vector, put the  input vector down each of the trees in the forest. Each  tree gives a classification, and we say the tree "votes"  for that class. The forest chooses the classification  having the most votes (over all the trees in the forest).

Each tree is grown as follows:  If the number of cases in the training set is N, sample  N cases at random - but with replacement, from the  original data. This sam t split on these m is used to  split the node. The value of m is held constant during  the forest growing.



III.RULE-BASED METHOD  Rule-based classifiers classify data by using a  collection of ?if . . . then . . .? rules. The rule  antecedent or condition is an expression made of  attribute conjunctions. The rule consequent is a  positive or negative classification. In order to build a  rule-based classifier we can follow a direct method to  extract rules directly from data. The advantages of  rule-based classifiers are that they are extremely  expressive since they are symbolic and operate with  the attributes of the data without any transformation.

Rule-based classifiers, and by extension decision  trees, are easy to interpret, easy to generate and they  can classify new instances efficiently.

Classify records by using a collection of  ?if?then?? rules  Rule: (Condition) ? y  Where Condition is a conjunction of attributes and  Y is the class label  LHS: rule antecedent or condition  RHS: rule consequent  Examples of classification rules  (Blood Type=Warm) ?  (Lay Eggs=Yes) ? Birds (Taxable Income < 50K) ?  (Refund=Yes) ?  Evade=No  rule-based classifier (example)  r1: (give birth = no) ?  (can fly = yes) ? birds  r2: (give birth = no) ?  (live in water = yes) ? fishes  r3: (give birth = yes) ?  (blood type = warm) ? mammals  r4: (give birth = no) ?  (can fly = no) ? reptiles    iv. memory based learning  memory-based learning is a family of learning  algorithms that, instead of performing explicit  generalization, compare new problem instances with  instances seen in training, which have been stored in  memory.  it is called instance-based because it  constructs hypotheses directly from the training  instances themselves. this means that the hypothesis  complexity can grow with the data. in the worst case,  a hypothesis is a list of n training items and the  computational complexity of classification a single  new instance is o(n). one advantage memory-based  learning has over other methods of machine learning  is its ability to adapt its model to previously unseen  data.a simple example of an instance-based learning  algorithm is the k-nearest neighbor algorithm.

A.k-nearest neighbor and memory-based reasoning  (mbr)  when trying to solve new problems, people often  look at solutions to similar problems that they have  previously solved. k-nearest neighbor (k-nn) is a  classification technique that uses a version of this  same method. it decides in which class to place a new  case by examining some number ? the ?k? in k-  nearest neighbor ? of the most similar cases or  neighbors. it counts the number of cases for each  class, and assigns the new case to the same class to  which most of its neighbors belong.

fig 3: k-nearest neighbor. n is a new case. it would be assigned to the class x because the seven x?s within the ellipse  outnumber the two y?s  k-nn models are very easy to understand when  there are few predictor variables. they are also useful  for building models that involve non-standard data  types, such as text. the only requirement for being  able to include a data type is the existence of an  appropriate metric.

IEEE - 31661  4th ICCCNT - 2013 July 4 - 6, 2013, Tiruchengode, India    IV .NEURAL NETWORKS  In more practical terms neural networks are non-  linear statistical data modeling tools. They can be  used to model complex relationships between inputs  and outputs or to find patterns in Data. Using neural  networks as a tool, data warehousing firms are  harvesting information from datasets in the process  known as data mining. The difference between these  data warehouses and an ordinary database is that  there is actual manipulation and cross-fertilization of  the data helping users makes more informed  decisions.

A.Feed forward Neural Network  One of the simplest feed forward neural networks  (FFNN), such as in Figure 4, consists of three layers:  an input layer, hidden layer and output layer. In each  layer there are one or more Processing Elements  (PEs). PEs is meant to simulate the neurons in the  brain and this is why they are often referred to as  neurons or nodes. A PE receives inputs from either  the outside world or the previous layer. There are  connections between the PEs in each layer that have a  weight (parameter) associated with them. This weight  is adjusted during training. Information only travels  in the forward direction through the network - there  are no feedback loops.

Fig 4::FIFO  in Neural Networks

V.BAYESIAN NETWORK  A Bayesian network,  Bayesian networks  are directed acyclic graphs(DAG) whose nodes  represent random variables in the Bayesian sense:  they may be observable quantities, latent variables,  unknown parameters or hypotheses. Edges represent  conditional dependencies; nodes which are not  connected represent variables which are conditionally  independent of each other. Each node is associated  with a probability function that takes as input a  particular set of values for the node's parent variables  and gives the probability of the variable represented  by the node. For example, if the parents  are  Boolean variables then the probability  function could be represented by a table  of  entries, one entry for each of the  possible  combinations of its parents being true or false.

Similar ideas may be applied to undirected, and  possibly cyclic, graphs; such are called Markov  networks.

Bayesian approaches are a fundamentally  important DM technique. Given the probability  distribution, Bayes classifier can provably achieve  the optimal result.     Bayesian method is based on the  probability theory. Bayes Rule is applied here to  calculate the posterior from the prior and the  likelihood, because the later two is generally easier to  be calculated from a probability modelOne example  of five attributes Bayes net is shown in figure 5.

Fig 5 : A Bayes net for 5 attributes.

A general model can be followed below for  build Bayes net:  Step1: Choose a set of relevant variables.

Step2: Choose an ordering for the variables.

Step3: Assume the variables are X1, X2, ..., Xn  (where X1 is the first, and Xi is the ith).

Step4: for i = 1 to n:  Step5: Add the Xi vertex to the network  Step6: Set Parent(Xi) to be a minimal subset of  X1, ..., Xi-1, such that we have conditional  independence of Xi and all other members of X1, ...,  Xi-1 given Parents(Xi).

Step7: Define the probability table of P(Xi=k |  Assignments of Parent(Xi)).

There are many choices of how to select relevant  variables, as well as how to estimate the conditional  probabilities. If the network as a connection of Bayes  classifiers, then the probability estimation can be  done applying some PDF like Gaussian. In some  cases, the design of the network can be rather  complicated. There are some efficient ways of getting  relevant variables from the dataset attributes. Assume  the coming signal to be stochastic will give a nice  way of extracting the signal attributes. And normally,  the likelihood weighting is another way to getting  attributes.

IEEE - 31661  4th ICCCNT - 2013 July 4 - 6, 2013, Tiruchengode, India

VI.SUPPORT VECTOR MACHINES    Support Vector Machines (SVMs, also support  vector networks) are supervised learning models with  associated learning algorithms that analyze data and  recognize patterns. The basic SVM takes a set of  input data and predicts, for each given input, which  of two possible classes forms the output, making it a  non-probabilistic binary linear classifier  SVM models have similar functional form  to neural networks and radial basis functions, both  popular data mining techniques. However, neither of  these algorithms has the well-founded theoretical  approach to regularization that forms the basis of  SVM. The quality of generalization and ease of  training of SVM is far beyond the capacities of these  more traditional methods.The support vector machine  (SVM) is a training algorithm for learning  classification and regression rules from data, for  example the SVM can be used to learn polynomial,  radial basis function (RBF) and multi-layer  perceptron (MLP)classifiers.

Fig 6:SVM algorithm  [Koliastasis C] [D.K. Despotis ][6] In the  classification task, data characteristics tend to  discriminate better the algorithms. However, in the  regression task, things tend to be more similar  referring to the error rate. On the basis of the analysis  of the experimental results, some conclusions can be  drawn for the classification task, as follows: The  deeper is the C5.0 tree the higher is the relative error  of the NN to the error of the C5.0; the lower is the  number of nominal variables the higher is the relative  error of DA to the error of C5.0 tree; the higher is the  number of classes the higher is the relative error of  LR to the error of C5.0 tree.

[Matthew N. Anyanwu][Sajjan G. Shiva ][7] The  experimental analysis also shows that SPRINT and  C4.5 algorithms have a good classification accuracy  compared to other algorithms used in the study. The  variation of data sets class size, number of attributes  and volume of data records is used to determine  which algorithm has better classification accuracy  between IDE3 and CART algorithms.

[p.nancy] [R.geethamani ][8] presents a  ?comparative study of various data mining  classification algorithms ? on the dataset  ?social side  of the internet?  for two set. For sub set 1, the  features selected by Feature ranking and for sub set 2  reliefF filtering. The features selected by feature  reduction techniques are chosen as input attributes  with necessary class variables as target attribute and  various classifications algorithms were executed for  all selected features one by one and there error rates  are tabled  below. In this research their conclusion  was Rnd Tree performed well for their taken dataset.

The following table 1 describes the comparison of  various classifications algorithms with error rate  Table 1. Comparisons on classification algorithms  with error rate  Classification  Algorithms  Error rates  Face book  Twiteer  C4.5 0.0860 0.1042  C-RT 0.1798 0.1976  CS-CRT 0.1798 0.1976  CS-MC4 0.1059 0.1107  ID3 0.1650 0.2084  KNN 0.1871 0.2097  RND 0.004 0.004    [Aman Kumar Sharma]  [Suruchi Sahni ] [10] in  the study of classification algorithms they compared  above said algorithms and came out the following  results and Table I demonstrate the classification  accuracy results of four classification decision tree  algorithms.

Table 2. Classification Accuracy    Algorithms    Correctly  classified  instances  Incorrectly  classified  instances  ID3 4100(89.1111%) 433(9.41%)  J48 4268(92.7624%) 333(7.23%)  ADTree 4183(90.915%) 418(9.08%)  SimpleCART 4262(92.63) 339(7.36%)    # Total number of instance 4601  It is evident from the table I that J48 has the  highest classification accuracy (92.7624%) where  4268 instances have been classified correctly and 333  instances have been classified incorrectly. The  Second highest classification accuracy for CART  algorithm is 92.632% in which 4262 instances have  IEEE - 31661  4th ICCCNT - 2013 July 4 - 6, 2013, Tiruchengode, India    been classified correctly. Moreover the ADTree  showed a classification accuracy of 90.915%. The  ID3 algorithm results in lowest classification  accuracy which is 89.111% among the four  algorithms. The ID3 was also not able to classify 68  instances. So the J48 outperforms the CART,  ADTree and ID3 in terms of classification accuracy.

[P. Bhargavi ][ S. Jyothi  ][9] In this paper, we  have compared the effectiveness of the classification  algorithms Genetic algorithm, Fuzzy classification  and Fuzzy clustering.

Table 3.  The Average Accuracy Rates Of Supervised  And Unsupervised Algorithms  Method  Average Accuracy  Rate  Genetic  Algorithm  Rules  46.67%  Fuzzy Classification 099%  Fuzzy Clustering  090.90%    The achieved performances are compared and  analyzed on the collected Supervised and  Unsupervised Soil data shown in Table 3. GATree  and Fuzzy Classification rules were used for  supervised learning. However, classification based on  Fuzzy rules gives much performance than GATree.

For Unsupervised learning Fuzzy C-Means algorithm  was used for classifying the soil data. This helps one  to classify soil texture based on soil properties  effectively, which influences fertility, drainage, water  holding capacity, aeration, tillage, and bearing  strength of soils.

[Bendi Venkata Ramana1] [M.Surendra Prasad  Babu][ Venkateswarlu ][4] in their study, popular  Classification  algorithms were considered for  evaluating their classification  performance in terms  of Accuracy, Precision, Sensitivity and Specificity in  classifying liver patients dataset. Accuracy,  Precision, Sensitivity and Specificity are better for  the AP Liver Dataset compared to UCLA liver  datasets with all the selected algorithms.

Table 4: Performance of Classification Algorithms  with all features of UCLA Liver Dataset Classifica  t-ion  Algorithm  Accuracy Precision Sensitivity Specificit  y  NBC 56.5 48.9 77.93 41  C 4.5  68.6 65.8 53.1  80  Back  propaga  tion  71.5 69.7 57.24 82  K-NN   62.8 55.7 56.51 67.5  SVM  58.2 1 68.9 1  This can be attributed to more number of useful  attributes like Total bilirubin, Direct bilirubin,  Indirect bilirubin, Albumin, Gender, Age and Total  proteins are available in the AP liver dataset  compared to the UCLA dataset. The common  attributes for AP liver data and Taiwan data are Age,  Sex, SGOT, SGPT, ALP, Total Bilirubin, Direct  Bilirubin, Total Proteins and Albumin are crucial in  deciding liver status.  With the selected dataset,  KNN, Back propagation and SVM are giving better  results with the entire feature set combinations.



VII. CONCLUSION  The goal of classification algorithms is to generate  more certain, precise and accurate system results.

Numerous methods have been suggested for the  creation of ensemble of classifiers. Classification  methods are typically strong in modeling interactions.

Several of the classification methods produce a set of  interacting logic that best predict the phenotype.

However, a straightforward application of  classification methods to large numbers of markers  has a potential risk picking up randomly associated  markers. But still it is difficult to recommend any one  technique as superior to others as the choice of a  dataset.  Finally, there is no single classification  algorithms is best for all kind of dataset.

Classification algorithms are specific in their problem  domain.

