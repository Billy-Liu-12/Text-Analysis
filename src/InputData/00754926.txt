Mining Optimized Support Rules for Numeric Attributes?

Abstract  In this paper, we generalize the optimized support  association rule problem by permitting rules to contain  disjunctions over uninstantiated numeric attributes.

For rules containing a single numeric attribute, we  present a dynamic programming algorithm for com-  puting optimized association rules. Furthermore, we  propose a bucketing technique for reducing the input  size, and a divide and conquer strategy that improves  the performance signi?cantly without sacri?cing opti-  mality. Our experimental results for a single numeric  attribute indicate that our bucketing and divide and  conquer enhancements are very e?ective in reducing  the execution times and memory requirements of our  dynamic programming algorithm. Furthermore, they  show that our algorithms scale up almost linearly with  the attribute's domain size as well as the number of  disjunctions.

1 Introduction  Association rules, introduced in [AIS93], provide a useful mechanism for discovering correlations among the underlying data. In its most general form, an association rule can be viewed as being de?ned over attributes of a relation, and has the form C1 ! C2, where C1 and C2 are conjunctions of conditions, and each condition is either Ai = vi or Ai 2 [li; ui] (vi, li and ui are values from the domain of the attribute Ai). Each rule has an associated support and con?- dence. Let the support of a condition Ci be the ratio of the number of tuples satisfying Ci and the num- ber of tuples in the relation. The support of a rule of the form C1 ! C2 is then the same as the support of C1 ^ C2, while its con?dence is the ratio of the sup- ports of conditions C1 ^ C2 and C1. The association rules problem is that of computing all association rules that satisfy user-speci?ed minimum support and min-  ?This work is part of the Serendip data mining project at Bell Labs. URL: http://www.bell-labs.com/project/serendip/.

imum con?dence constraints, and schemes for this can be found in [AIS93, AS94, SON95].

For example, consider a relation in a telecom ser- vice provider database that contains call detail infor- mation. The attributes of the relation are date, time, src city, src country, dst city, dst country and duration.

A single tuple in the relation thus captures informa- tion about the two endpoints of each call, as well as the temporal elements of the call. The association rule (src city = NY) ! (dst country = France) would sat- isfy the user-speci?ed minimum support and minimum con?dence of 0.05 and 0.3, respectively, if at least 5% of total calls are from NY to France, and at least 30% of the calls that originated from NY are to France.

The optimized association rules problem, motivated by applications in marketing and advertising, was in- troduced in [FMMT96b]. An association rule R has the form (A1 2 [l1; u1]) ^ C1 ! C2, where A1 is a nu- meric attribute, l1 and u1 are uninstantiated variables, and C1 and C2 contain only instantiated conditions (that is, the conditions do not contain uninstantiated variables). The authors then propose algorithms for determining values for the uninstantiated variables l1 and u1 for each of the following cases:  ? Con?dence of R is maximized and the support of the condition (A1 2 [l1; u1]) ^ C1 is at least the user-speci?ed minimum support (referred to as the optimized con?dence rule).

? Support of the condition (A1 2 [l1; u1]) ^ C1 is maximized and con?dence of R is at least the user- speci?ed minimum con?dence (referred to as the optimized support rule).

Optimized association rules are useful for unravel- ing ranges for numeric attributes where certain trends or correlations are strong (that is, have high support or con?dence). For example, suppose the telecom ser- vice provider mentioned earlier was interested in of- fering a promotion to NY customers who make calls to France. In this case, the timing of the promotion    may be critical { for its success, it would be advanta- geous to o?er it close to a period of consecutive days in which the maximum number of calls from NY are made and a certain minimum percentage of those calls from NY are to France. The framework developed in [FMMT96b] can be used to determine such periods.

Consider, for example, the association rule (date 2 [l1; u1]) ^ src city = NY ! dst country = France. With a minimum con?dence of 0.5, the optimized support rule results in the period during which at least 50% of the calls from NY are to France, and the number of calls originating in NY is maximum.

A limitation of the optimized association rules dealt with in [FMMT96b] is that only a single optimal inter- val for a single numeric attribute can be determined.

However, in a number of applications, a single inter- val may be an inadequate description of local trends in the underlying data. For example, suppose the tele- com service provider is interested in doing upto k pro- motions for customers in NY calling France. For this purpose, we need a mechanism to identify upto k peri- ods during which a sizeable number of calls from NY to France are made. If association rules were permitted to contain disjunctions of uninstantiated conditions, then we could determine the optimal k (or fewer) pe- riods by ?nding optimal instantiations for the rule: (date 2 [l1; u1])_? ? ?_(date 2 [lk; uk])^src city = NY !

dst country = France. This information can be used by the telecom service provider to determine the most suitable periods for o?ering discounts on international long distance calls to France.

In this paper, we generalize the optimized as- sociation rules problem for support, described in [FMMT96b]. We allow association rules to contain upto k disjunctions over one uninstantiated numeric attribute. We present a dynamic programming algo- rithm for computing the optimized association rule and whose complexity is O(n2k), where n is the num- ber of values in the domain of the uninstantiated at- tribute. We also present two optimizations that signif- icantly improve the computational complexity of our algorithm. The ?rst is a bucketing algorithm that coa- lesces contiguous values { all of which have con?dence greater than minConf. The second is a divide and con- quer strategy that enables us to split the original prob- lem into multiple smaller subproblems and then com- bine the solutions for the subproblems. This not only drastically reduces the computation to be performed but also reduces storage requirements, thus permit- ting our algorithm to execute in main-memory even for large databases. Our experimental results indicate that the two optimizations enable our dynamic pro-  gramming algorithm to scale almost linearly with both the domain size n and the number of disjunctions k.

Proofs of theorems presented in the paper can be found in [RS97].

2 Related Work Association rules for a set of transactions in which  each transaction is a set of items bought by a customer, were ?rst studied in [AIS93]. These association rules for sales transaction data have the formX ! Y , where X and Y are disjoint sets of items. E?cient algorithms for computing them can be found in [AS94, PCY95, SON95].

The optimized association rule problem was intro- duced in [FMMT96b]. The authors permit associa- tion rules to contain a single uninstantiated condi- tion A1 2 [l1; u1] on the left hand side, and propose schemes to determine values for variables l1 and u1 such that the con?dence or support of the rule is max- imized. In [FMMT96a], the authors extend the re- sults in [FMMT96b] to the case in which rules contain two uninstantiated numeric attributes on the left hand side. However, their schemes only compute a single op- timal region.

In [RS98], we extended the optimized support and con?dence problems to compute the k optimal regions, and showed that the problems are NP-hard even for the case of one uninstantiated numeric attribute. We proposed search algorithms that employed branch and bound pruning techniques to compute k optimal re- gions. The optimized support problem described in [RS98] required the con?dence over all the optimal re- gions, considered together, to be greater than a certain minimum threshold. Thus, the con?dence of an opti- mal region could fall below the threshold and this was the reason for its intractability. In this paper, we re- de?ne the optimized support problem such that each optimal region is required to have the minimum con- ?dence. This makes the problem tractable for the one attribute case.

In [ORS98], the authors study the problem of dis- covering cyclic association rules, that is, association rules that display regular cyclic variation over time.

3 Preliminaries  In this section, we de?ne the optimized association rule problem addressed in the paper. The data is as- sumed to be stored in a relation de?ned over cate- gorical and numeric attributes. Association rules are built from atomic conditions each of which has the form Ai = vi (Ai could be either categorical or nu- meric), and Ai 2 [li; ui] (only if Ai is numeric). For the atomic condition Ai 2 [li; ui], if li and ui are values    from the domain of Ai, the condition is referred to as instantiated; otherwise, if they are variables, we refer to the condition as uninstantiated.

Atomic conditions can be combined using operators ^ or _ to yield more complex conditions. Instanti- ated association rules, that we study in this paper, have the form C1 ! C2, where C1 and C2 are arbi- trary instantiated conditions. Let the support for an instantiated condition C, denoted by sup(C), be the ratio of the number of tuples satisfying the condition C and the total number of tuples in the relation. Then, for the association rule R: C1 ! C2, sup(R) is de-  ?ned as sup(C1) and conf(R) is de?ned as sup(C1^C2) sup(C1)  .

Note that our de?nition of sup(R) is di?erent from the de?nition in [AIS93] where sup(R) was de?ned to be sup(C1 ^C2). Instead, we have adopted the de?nition used in [FMMT96b] and [FMMT96a]. Also, let min- Conf denote the user-speci?ed minimum con?dence.

The optimized association rule problem requires op- timal instantiations to be computed for an uninstanti- ated association rule that has the form: U ^C1 ! C2, where U is an uninstantiated atomic condition involv- ing a numeric attribute, and C1 and C2 are arbitrary instantiated conditions. For simplicity, we assume that the domain of the uninstantiated numeric attribute is f1; 2; : : : ; ng. For the one attribute case, the re- gion [l1; u1] is simply the interval [l1; u1] for the at- tribute. Suppose, for a region R = [l1; u1], we de?ne conf(R) and sup(R) to be conf and sup, respectively, for the rule A1 2 [l1; u1] ^C1 ! C2. In addition, for a set of non-overlapping regions, S = fR1; R2; : : : ; Rjg, Ri = [li; ui], we de?ne conf(S) and sup(S) to be the conf and sup, respectively, of the rule _ji=1A1 2 [li; ui] ^ C1 ! C2. Note that, since R1; : : : ; Rj are non-overlapping regions, the following hold for set S.

sup(S) = sup(R1) + ? ? ?+ sup(Rj)  conf(S) = sup(R1) ? conf(R1) + ? ? ?+ sup(Rj) ? conf(Rj)  sup(R1) + ? ? ?+ sup(Rj)  Having de?ned the above notation for association rules, we present below, the formulations of the opti- mized association rule problems for support.

Given k, determine a set S containing at most k regions such that for each region Ri 2 S, conf(Ri) ? minConf and sup(S) is maxi- mized.

We refer to the set S as the optimized support set.

In the remainder of the paper, we shall assume that the support and con?dence for every point in a region are available { these can be computed by performing  a single pass over the relation. The points, along with their supports and con?dences, thus constitute the in- put to our algorithms. Thus, the input size is n.

4 Computing Optimized Support Sets In this section, we tackle the problem of computing  optimized support sets when association rules contain a single uninstantiated numeric attribute. Thus, the uninstantiated rule has the form: (A1 2 [l1; u1])^C1 !

C2, where A1 is the uninstantiated numeric attribute.

We propose a dynamic programming algorithm for computing optimized support sets in Section 4.2. But ?rst, in Section 4.1, we present preprocessing algo- rithms for collapsing certain contiguous ranges of val- ues in the domain of the attribute into a single bucket, thus reducing the size of the input n. We also present a divide and conquer optimization in Section 4.3 to im- prove the computational complexity and memory re- quirements of our dynamic programming algorithm.

4.1 Bucketing  For the one attribute case, each region is an interval and since the domain size is n, the number of possible intervals is O(n2). Now, suppose we could split the range 1; 2; : : : ; n into b buckets, where b < n, and map every value in A1's domain into one of the b buckets to which it belongs. Then the new domain of A1 becomes f1; 2; : : : ; bg and the number of intervals to be consid- ered becomes O(b2) { which could be much smaller, thus reducing the time and space complexity of our algorithms. Note that the reduction in space complex- ity also results in reduced memory requirements for our algorithms.

There are two considerations that we must take into account when assigning values in A1's domain to buck- ets. The ?rst is that the bucketing procedure must not adversely impact the optimality of the optimized set { that is, the optimized set computed on the buck- ets must be identical to the one computed using the raw domain values. In order to ensure this, the buck- eting procedure must be such that no interval in the optimized set computed prior to bucketing contains a subset of the values assigned to a bucket.

The second consideration has to do with the com- plexity of the bucketing procedure. The input to the bucketing algorithm is the support and con?dence for each of the n values in A1's domain, sorted in increas- ing order of the values. We would prefer for the time complexity to be linear in n and the procedure to per- form a single pass over the input data. This way, even if n is large or if the information on supports and con- ?dences for the n points does not ?t in memory, the b buckets can still be computed e?ciently in a single scan of the data.

In the following, we present bucketing algorithms that meet both of the above-mentioned requirements { that is, they do not compromise the optimality of the optimized sets and have time complexity O(n). The output of the algorithms is the b buckets with their supports and con?dences, and this becomes the input to our dynamic programming algorithm in Section 4.2.

For optimized support sets, contiguous values in the domain of the uninstantiated attribute for which the con?dence is greater than or equal to minConf are col- lapsed into a single bucket by our bucketing algorithm.

Each other value j whose con?dence is less than min- Conf is assigned to a separate bucket { the bucket con- tains only the value j. In other words, if values in interval [i; j] are assigned to a bucket then either  1. for all i ? l ? j, con?dence of [l; l] is at least minConf, or  2. i = j and con?dence of [i; i] is less than minConf.

For example, let the domain of A1 be f1; 2; : : : ; 6g and con?dences of 1, 2, 5 and 6 be greater than or equal to minConf, while con?dences of 3 and 4 be less than min- Conf. Then, our bucketing scheme generates 4 buckets, the ?rst containing values 1 and 2, the second and third containing values 3 and 4, respectively, and the fourth containing values 5 and 6. It is straightforward to ob- serve that assigning values to buckets can be achieved by performing a single pass over the input data.

Furthermore, the following theorem can be used to show that the above bucketing scheme preserves the optimality of the computed optimized sets.

Theorem 4.1 : Let S be an optimized support set.

Then, for any interval [i; j] in S, it is the case that conf([i?1; i?1]) < minConf and conf([j+1; j+1]) < minConf.

From the above theorem, it follows that for any set of contiguous values, each of which has con?dence at least minConf, either all of the values are contained in the optimized set or none of them are. The reason for this is that if the optimized set contained some of the values and not all the values, then for some interval [i; j] in the optimized set, either i ? 1 or j + 1 would be one of the values with con?dence at least minConf { thus violating Theorem 4.1. Thus, each interval in the optimized set computed prior to bucketing, either contains all the values assigned to a bucket or none of them. As a result, the optimized set can be computed using the buckets instead of the initial domain values.

4.2 Optimized Support Algorithm  In this subsection, we present a dynamic program- ming algorithm for the optimized support problem.

The input to the algorithm is the b buckets generated by our bucketing scheme in Section 4.1 along with their con?dences and supports. The problem is to determine a set of at most k (non-overlapping) intervals such that the con?dence of each interval is greater than or equal to minConf and support of the set is maximized.

4.2.1 Intuition  Suppose optSet[i; j; l] denotes the optimized set for in- terval [i; j] containing at most l (non-overlapping) in- tervals. Thus, for every interval [p; q] in the set, 1) con?dence is at least minConf, and 2) i ? p ? q ? j.

It is fairly straightforward to observe that optSet[i; i; l] for all l = 1; : : : ; k is [i; i] if conf([i; i]) ? minConf.

Otherwise, it is ;. Similarly, for i < j, if conf([i; j]) ? minConf, then optSet[i; j; l] for all l = 1; : : : ; k is [i; j]. On the other hand, if conf([i; j]) < minConf, then optSet for the interval bounded by i and j can be computed from the optSet values for its subintervals.

Theorem 4.2: If, for i < j, conf([i; j]) < minConf, then the optimized set optSet[i; j; l] is  ? l = 1: optSet[i; j ? 1; 1], if sup(optSet[i; j ? 1; 1]) > sup(optSet[i+ 1; j; 1]).

optSet[i+ 1; j; 1], otherwise.

? l > 1: optSet[i; r; 1] [ optSet[r+1; j; l? 1], where i ? r < j is such that sup(optSet[i; r; 1] [ optSet[r + 1; j; l ? 1]) is maximum.

The above theorem enables us to use dynamic pro- gramming in order compute optSet[1; b; k].

4.2.2 Dynamic Programming Algorithm  The dynamic programming algorithm for computing optSet[i; j; l] is as shown in Figure 1. Before the al- gorithm is invoked, we assume that conf([i; j]) and sup([i; j]) is precomputed for every interval [i; j]. The array optSet is used to store the optimized sets for the various intervals, and the entries of the array are initially set to ;.

Procedure optSup1D ?rst checks if optSet[i; j; l] has already been computed (by invoking the function computed in Step 1). If so, then the optimized set stored in optSet[i; j; l] is returned. Otherwise, the algo- rithm calculates optSet[i; j; l] using the results of The- orem 4.2. It ?rst checks the con?dence of [i; j] to see    procedure optSup1D(i; j; l) begin  1. if computed(optSet[i; j; l]) = true 2. return optSet[i; j; l] 3. if conf([i; j]) ? minConf f 4. optSet[i; j; l] := f[i; j]g 5. return optSet[i; j; l] 6. g 7. tmpSet := ; 8. if i < j f 9. if l = 1 10. tmpSet := maxSupSet(optSup1D(i; j ? 1; 1),  optSup1D(i+ 1; j; 1)) 11. else 12. for r:= i to j ? 1 do 13. tmpSet := maxSupSet(tmpSet,  optSup1D(i; r; 1) [ optSup1D(r + 1; j; l? 1)) 14. g 15. optSet[i; j; l] := maxSupSet(optSet[i; j; l], tmpSet) 16. return optSet[i; j; l] end  Figure 1: Algorithm for computing optimized support set  whether it is at least minConf. If this is the case, then optSet[i; j; l] is the interval [i; j] itself since it has the maximum possible support in [i; j]. However, if the con?dence of [i; j] is less than minConf, then the two cases are when l = 1 and l > 1. For l = 1, the in- terval we are interested must be in either [i; j ? 1] or [i+1; j]. The functionmaxSupSet in Step 10 takes two or more optSets as arguments and returns the set with the maximum support. On the other hand, if l > 1, then one of the intervals we are interested in must lie in [i; r] and the other l?1 must lie in [r+1; j] for some i ? r < j. Thus, optSet[i; j; l] is set to the union of the optSets for the pair of intervals with the maximum support. Note that if optSup1D was initially invoked with parameters 1, b and k, then when Step 12 of the algorithm is executed, the value of j is always b.

In [RS97], we show that the time and space com- plexity of our dynamic programming algorithm is O(b2k) and O(b2 + bk), respectively.

4.3 Divide and Conquer  The dynamic programming algorithm for comput- ing optimized support sets presented in the previous section had time complexity O(b2k) and space com- plexity O(b2), where b is the number of input buckets.

In this section, we propose a divide and conquer algo-  rithm that partitions the range consisting of b buckets into subranges and uses the dynamic programming al- gorithm in order to compute optimized sets for each subrange. It then combines the optimized sets for the various subranges to derive the optimized set for the entire range of buckets. Since the input to the dy- namic programming algorithm is a subrange whose size is smaller than that of the entire range, the divide and conquer approach reduces the time and space complex- ity of computing the optimized set for the entire range.

The result is reduced execution times and memory re- quirements, thus allowing our algorithms to execute in main-memory. Our experiments, in Section 5, indicate that with the divide and conquer optimization, execu- tion times and storage needs of our algorithms increase linearly (as opposed to quadratically) with the number of input buckets.

In the following subsections, we ?rst present the in- tuition underlying our divide and conquer approach.

We then present a scheme with linear time complex- ity for splitting the range of buckets into subranges { this makes the scheme practical even for large b and for the case in which the support and con?dence infor- mation for the b buckets does not ?t in main-memory.

Finally, we show how the results for the subranges can be combined to yield the optimized support set.

4.3.1 Intuition  We ?rst describe the intuition underlying the opti- mization. Suppose for a bucket p, 1 ? p ? b, it is the case that for every interval [i; j] containing p, we have conf([i; j]) < minConf. Then, since every in- terval in the optimized support set must have con?- dence at least minConf, we can conclude that p does not occur in the optimized set. Consequently, every interval in the optimized set must either be to the left of p or to the right of p (none of the intervals can span p). We can thus independently compute optSet[1; p?1; l] and optSet[p+1; b; l], for all 1 ? l ? k, and then set optSet[1; b; k] to be optSet[1; p ? 1; l] [ optSet[p+ 1; b; k ? l] for the value of l between 0 and k that results in maximum support (optSet[1; p? 1; 0] and optSet[p + 1; b; 0] are both trivially ;). Since the optimized set must have 0 ? r ? k intervals on the left and at most k?r intervals on the right of p, by consid- ering all values of l between 0 and k for optSet[1; p?1; l] [ optSet[p+1; b; k? l], optSet[1; b; k] is guaranteed to be an optimized set.

A generalization of the above idea is to ?rst com- pute partition points { a partition point is a bucket with the property that every interval containing it has con?dence less than minConf. Thus, no interval in    procedure computePartition() begin  1. partPoints := ; 2. earliest := b+ 1 3. j := b 4. for i := numE?ective downto 1 do 5. while j ? e?ective[i] do 6. if conf([e?ective[i], j] ? minConf)f 7. earliest := e?ective[i] 8. break (out of while-loop) 9. g else f 10. if j < earliest 11. partPoints := partPoints [ fjg 12. j := j ? 1 13. g 14. return partPoints end  Figure 2: Algorithm for generating partition points  the optimized set can contain a partition point. If p1; : : : ; pm?1 are the partition points in increasing or- der, then these partition interval [1; b] into m intervals { [1; p1?1], : : :, [pi?1+1; pi?1], : : :, [pm?1+1; b]. The m partitioned intervals have the property that every interval in the optimized set is wholly contained in a single partition. The reason for this is that between any two adjacent partitions, there is a partition point.

optSet[1; b; k] can then be computed by computing for every partition [i; j] and for 0 ? l ? k, optSet[i; j; l], and then choosing the combination of optSets for the various partitions, that has the maximum support.

4.3.2 Computing Partition Points  In Figure 2, we present an O(b) algorithm for comput- ing the partition points p1; : : : ; pm?1. The key idea un- derlying the algorithm is as follows. For every bucket j, we ?rst compute the largest interval [i; j] ending at j and with con?dence at least minConf. A point pi is a partition point if for all j > pi, the largest interval ending at j does not contain pi and no interval end- ing at pi has con?dence that exceeds minConf. The largest interval with con?dence minConf for all buck- ets can be computed in linear time using recent re- sults from [FMMT96b]. In [FMMT96b], the authors introduce the notion of e?ective points { a bucket s is e?ective if for all i < s, conf([i; s ? 1]) < minConf.

It is fairly straightforward to observe that if [s; j] is the largest interval with con?dence exceeding min- Conf and ending at j, then s must be an e?ective  point. Also, for an e?ective point s, if [s; j] has con- ?dence less than minConf, then for every other i < s, conf([i; j]) < minConf.

We are now in a position to describe how procedure computePartition (see Figure 2) utilizes the e?ective points in order to compute the partition points. In [FMMT96b], the authors show how e?ective points can be computed in linear time in a single forward pass over the b buckets. We do not repeat this here and assume that there are numE?ective e?ective points that are stored in increasing order in the array e?ective. The above-mentioned properties of e?ective points make them useful for e?ciently computing the largest in- terval ending at bucket j and with con?dence at least minConf { only e?ective points preceding j need to be scanned in reverse order until one is encountered, say s, for which conf([s; j]) decreases below minConf.

Procedure computePartition simultaneously scans both the input buckets as well as the e?ective points in the reverse order. The variable j keeps track of the current bucket being scanned while e?ective[i] is the e?ective point currently under consideration. Finally, the variable earliest stores the earliest e?ective point such that there exists an interval with con?dence min- Conf begining at earliest and ending at a bucket greater than or equal to j. Thus, if for bucket j, j < earliest, then j is a partitioning point (see steps 10 and 11) since no interval ending at or after j has con?dence minConf.

When scanning the buckets and e?ective points in reverse order, for bucket j, only e?ective points pre- ceding it are candidates for the longest interval with con?dence minConf and ending at j. Furthermore, if conf([e?ective[i], j]) ? minConf (see Step 6), then we next consider the e?ective point immediately before it (by decrementing i by 1) for the longest interval end- ing at j. Also, earliest is set to e?ective[i]. On the other hand, if conf([e?ective[i], j]) < minConf, then we simply consider the bucket preceding j (by decre- menting j in Step 12) since the earliest bucket for the longest interval with con?dence minConf ending at j cannot be before earliest (if there are e?ective points between e?ective[i] and j, earliest stores the e?ective point immediately following e?ective[i]).

The algorithm performs two passes over the data { one forward pass to compute the e?ective points and a reverse pass during which the partition points are computed. Thus, the algorithm is e?cient and can be used even if the data is too large to ?t in main-memory.

procedure divideConquerSup() begin  1. for i := 1 to m do f 2. optSetPart[i; 0] := ; 3. for l := 1 to k do 4. optSetPart[i; l] := optSup1D(lower(i); upper(i); l) 5. g 6. for i := 0 to k do 7. optSet[i] := optSetPart[1; i] 8. for i := 2 to m do 9. for j := k downto 1 do 10. for p := 0 to j do f 11. tmpSet := optSet[j ? p] [ optSetPart[i; p] 12. if sup(tmpSet) > sup(optSet[j]) then 13. optSet[j] := tmpSet 14. g 15. return optSet[k] end  Figure 3: Divide and conquer algorithm for optimized support  4.3.3 Divide and Conquer Algorithm  Once the m partitions are generated (from the parti- tioning points), then procedure divideConquerSup (see Figure 3) can be used to compute the optimized sup- port set. Let lower(i) and upper(i) be the boundary buckets for partition i, and let bi be the number of buckets in partition i. First, in steps 1 through 5, the optimized set for each partition containing at most 0, : : :, k intervals is computed using optSup1D. Proce- dure optSup1D(i; j; l) returns the optimized support set whose size is at most l for the interval consisting of buckets i through j. The optimized set of size l for partition i is stored in optSetPart[i; l]. Even though, in the for-loop in steps 3-4, optSup1D is invoked k times with the number of intervals ranging from 1 to k, the complexity of the for-loop is still O(b2i k) because the optimized sets for the various intervals in partition i (that are computed during an invocation of optSup1D) can be stored and shared between the k invocations of optSup1D for the partition. Thus, for an arbitrary in- terval in the partition and a maximum size l for the optimized set, the optimized set is computed only once (the ?rst time it is required).

The optimized sets computed for the various parti- tions are then merged in steps 6{14. The merging pro- cess is carried out in successive steps { in step i, the optimized set for partition i is merged with the result of the merge of partitions 1, : : :, i?1 (that is stored in  optSet). Thus, at the end of step i, optSet[j] stores the optimized set containing at most j intervals belonging partitions 1; : : : ; i. We need to compute optSet[j] for all 1 ? j ? k since the optimized set (containing at most k intervals) for the ?rst i + 1 partitions is ob- tained by combining, during step i + 1, optSet[j] and optSetPart[i+ 1; k ? j] for the value of j that causes optSet[k] to be maximized. The complexity of each step of the merge is thus O(k2) since we need to com- pute optSet for all values between 1 and k. Since the number of partitions is m, the complexity of steps 6{ 14 becomes O(mk2). Thus, the overall complexity of the algorithm becomes O((b21 + : : : + b  m)k + mk  2) where b ? b1 + : : : + bm. If bmax denotes the largest value among b1; : : : ; bm, then the complexity becomes O(b2maxmk+mk  2). In our experiments, we found that for a large number of cases, bmax << b and since k << b, the divide and conquer results in substan- tial reductions in the computational complexity of our dynamic programming algorithm (whose original com- plexity is O(b2k)). In addition, it also reduces the storage and memory requirements of our dynamic pro- gramming algorithm from O(b2) to O(b2max).

5 Experimental Results In this section, we study the performance of our  algorithms for computing optimized support sets for the one attribute case. In particular, we show that the bucketing and divide and conquer optmizations make our dynamic programming algorithm highly scaleable.

For instance, we can tackle domains of sizes as high as 100,000 in a few seconds. Furthermore, mining as many as 250 intervals for 100,000 domain values can be achieved in a matter of a few minutes. We also study the sensitivity of the above two optimizations to the minimum con?dence threshold.

In our experiments, the data ?le is read only once at the beginning of each algorithm in order to compute the support and con?dence for every point. The time for this, in most cases, constitutes a tiny fraction of the total execution time of our algorithms. Thus, we do not include the time spent on reading the data ?le in our results. Furthermore, note that the performance of our algorithms does not depend on the number of tuples in the data ?le { it is more sensitive to the size of the attribute's domain n and the number of intervals k.

Our experiments were performed on a Sun Ultra-2/200 machine with 512 MB of RAM and running Solaris 2.5.

Synthetic Datasets: The association rule that we experimented with, has the form U ^ C1 ! C2 where U contains 1 uninstantiated attribute (see Section 3) whose domain consists of integers ranging from 1 to n.

0.01  0.1       1000 10000 100000  E xe  cu tio  n tim  e (s  )  Size of domain (n)  minConf-0.75, k-50  No optimizations Only bucketing  Bucketing, Divide & conquer  (a) Scale-up with n n b m bmax  500 195 80 6 1000 354 140 8 2000 738 293 8 5000 1829 743 10 7500 2759 1123 8 10000 3711 1518 12 50000 16321 6847 10 100000 32266 13512 12  (b) Values of b, m and bmax for di?erent input sizes  Figure 4: Varying input size  Every domain value (that is, point in one-dimensional space) is assigned a randomly generated con?dence be- tween 0 and 1 with uniform distribution. Each value is also assigned a randomly generated support between 0 and 2  n with uniform distribution; thus, the average  support for a value is 1 n .

5.1 Bucketing and Divide and Conquer  In this subsection, we study the improvements in execution times that result due to the bucketing and divide and conquer optimizations. In Figure 4(a), we plot the performance of three variants of our algorithm as the domain size is increased from 500 to 100,000 { (1) with no optimizations, (2) with only bucketing, and (3) with both bucketing and divide and conquer.

We use a log scale to represent values along both axes.

Also, in our experiments, we ?x the number of intervals k at 50 and use a minimum con?dence threshold of 0.75.

For optimized support, we found that with no opti- mizations, our dynamic programming algorithm took times excessive of 30 minutes for as few as 5000 val- ues in the domains of the attributes. On the other hand, with both bucketing and divide and conquer, our algorithm took less than 15 seconds for domain sizes as high as 100,000. From the graphs, it follows  that the major portion of the performance improve- ment results due to divide and conquer. Even though bucketing does reduce input size, these reductions are fairly small for optimized support (about 5-6%). Di- vide and conquer, on the other hand, partitions the original problem of size b into m subproblems of size at most bmax and has complexity O(b  maxmk +mk  2).

For n = 100; 000 and the optimized support case, di- vide and conquer splits the b = 95000 buckets into m = 13000 partitions each of whose size is less than bmax = 20. Obviously, since bmax << b, k << b and m < b, it follows that the computational complexity of divide and conquer is much smaller than O(b2k), the complexity with only bucketing. The e?ectiveness of bucketing and divide and conquer depend on minConf values. We discuss this in more detail in Section 5.3.

5.2 Scale-up with n  Figure 4(a) also shows how the algorithms with and without optimizations scale with the input size n. Due to the quadratic complexity of our algorithm, we found that as n doubles, without optimizations, the execu- tion time increases almost four-fold. This is in line with what we expected. The same holds when we use only the bucketing optimization.

With the divide and conquer strategy, the complex- ity of our algorithm is O(b2maxmk + mk  2), where m is the number of partitions and bmax is the size of the largest partition. Thus, if with increasing n, bmax stays approximately the same and m increases linearly with n, then we can expect the divide and conquer algo- rithm to exhibit a linear scale-up with increasing n.

This is exactly what we observe for the optimized sup- port case. The table in Figure 4(b) contains the values of m and bmax for increasing values of n. For opti- mized support, bmax stays in a narrow range between 15 and 20. Furthermore, m increases almost linearly with respect to n from 80 (for n = 500) to 13500 (for n = 100; 000). Thus, as n increases, the cost of com- puting the optimized sets for each partition stays ap- proximately the same, and the algorithm only incurs a linear increase in the cost for computing optimized sets for all the partitions and then combining them.

5.3 Sensitivity to minConf  We next study the senstivity of our bucketing and divide and conquer optimizations to minimum con?- dence values. In Figure 5, for n = 5000 and k = 50, we plot execution times for our algorithms as minConf is varied from 0.2 to 0.9.

Figure 5(a) plots the performance of our algorithms with and without optimizations for computing opti- mized support sets. First, we ?nd that without any op- timizations, the performance of our dynamic program-    0.1       0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  E xe  cu tio  n tim  e (s  )  Minimum confidence (minConf)  n-5000, k - 50  No optimization Only bucketing  Bucketing, Divide & conquer  (a) Sensitivity to minConf minConf b m bmax  0.2 1620 1 1619 0.3 2166 1 2165 0.4 2489 1 2488 0.45 2553 1 2552 0.5 2539 1 2538 0.55 2451 136 172 0.6 2355 395 70 0.7 2063 708 14 0.8 1537 686 8 0.9 829 399 2  (b) Values of b, m and bmax for di?erent con?dences  Figure 5: Varying minConf  ming algorithm becomes worse as con?dence increases.

The reason for this is that, for optimized support, if the con?dence of an interval is minConf or higher, the optimized set for the interval is the interval itself, and further recursive calls for the interval are not needed.

When minConf is small, there are a large number of such intervals with con?dence at least minConf.

We next turn our attention to the bucketing and divide and conquer optimizations. In the table in Fig- ure 5(b), for a given con?dence, we present the number of buckets generated by the bucketing algorithm b, the number of partitions generated by the divide and con- quer algorithmm, and the max size of a partition bmax.

From the table, we can make the following three obser- vations { (1) as the con?dence increases, the number of buckets input to our algorithm increases, (2) for val- ues of minConf that are lower than 0.5, the divide and conquer optimization has no e?ect, and (3) as the con- ?dence increases beyond 0.5, for increasing con?dence values, the max partition size decreases. The reason for Point (1) is that the bucketing algorithm coalesces contiguous values with con?dence more than minConf and at lower minConf values, there are a larger number            50 100 150 200 250  E xe  cu tio  n tim  e (s  )  Size of optimized set (k)  n-100000, minConf - 0.75, Bucketing, Divide & conquer  "optSup.75"  Figure 6: Scale-up with k  of such values. Points (2) an (3) can be attributed to the fact that the number of partition points increases as the con?dence increases (and the number is 0 when minConf is less than 0.5). The above three points com- pletely explain the behaviour of our optimizations in Figure 5(a). With only bucketing, at lower minConf values, due to the smaller number of buckets and conse- quently the smaller input sizes, the performance of our algorithm with bucketing is much better than without bucketing. However, as the con?dence increases, the reductions in input size become smaller and smaller, and thus the bucketing optimization has very little ef- fect for large con?dence values. The performance of our algorithm with both bucketing and divide and con- quer enhancements is more interesting. First, since for con?dence values below 0.5, divide and conquer does not generate any partitions, the performance of our algorithm with and without divide and conquer is the same. However, beyond 0.5, divide and conquer kicks in and the execution times decrease as con?dence in- creases { the smaller sizes of partitions is primarily responsible for this.

5.4 Scale-up with k  In order to determine, how our algorithm with both the bucketing and divide and conquer optimizations scales for increasing values of k, we varied k between 10 and 250 with a domain size of 100; 000 and minConf = 0.75. The results of our experiments for the op- timized support case are as shown in Figure 6. From the graphs, it follows that the execution times increase slightly more than linearly as k is increased. The rea- son for this is that the complexity of our divide and conquer algorithm is O(b2maxmk + mk  2). The ?rst term, which is the complexity of computing optmized sets for the m partitions, increases linearly with k.

However, the second term, which is the cost of combin-    ing the results for the partitions to get the ?nal opti- mized set has complexity that is quadratic in k. Thus, we ?nd that the increase in execution times for our algorithms is somewhere between linear and quadratic for increasing k. On an average, we found that com- putation times increase about 3-fold every time k dou- bles. Note that for values of k much larger than bmax, a majority of the time is spent in combining the results for the various partitions. Thus, subsequent increases in the value of k could result in quadratic increases in execution times when the divide and conquer opti- mization is employed.

6 Concluding Remarks  In this paper, we generalized the optimized support association rule problem by permitting rules to con- tain upto k disjunctions over one uninstantiated nu- meric attribute. We presented a dynamic program- ming algorithm for computing the optimized associa- tion rule and whose complexity is O(n2k), where n is the number of values in the domain of the uninstan- tiated attribute. We also presented two optimizations that signi?cantly improve the computation time and memory requirements of our algorithm. The ?rst is a bucketing algorithm that coalesces contiguous values { all of which have con?dence either greater than the minimum speci?ed con?dence or less than the mini- mum con?dence. The second is a divide and conquer strategy that enables us to split the original problem into multiple smaller subproblems and then combine the solutions for the subproblems. We experimentally showed that the two optimizations enable our dynamic programming algorithms to execute in main-memory for large values of the domain size n. With the op- timizations, our algorithms scale almost linearly with both the domain size n and the number of disjunctions k.

Acknowledgements: We would like to thank Narain Gehani, Hank Korth and Avi Silberschatz for their en- couragement. Without the support of Yesook Shim, it would have been impossible to complete this work.

