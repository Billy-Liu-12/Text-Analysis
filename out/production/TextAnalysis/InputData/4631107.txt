

Abstract The problem of discovery association rules in large databases is considered. An encoding method for converting large databases to small one is proposed. Significant efficiency is obtained by applying some modified known algorithm on our proposed database layout. In addition, a new algorithm based on the proposed encoding method is introduced. Using some properties of numbers our database converts itemset to numerical domain. Our implementation indicates that the proposed layout made the size of database significantly smaller.

Also the time to find association rules is reduced.



I. INTRODUCTION  HE amount of data stored in databases continues to grow fast. Intuitively, this large amount of stored data contains valuable hidden knowledge, which could be used to  improve the decision-making process of an organization. For instance, data about previous sales might contain interesting relationships between products and customers. The discovery of such relationships can be very useful to increase the sales of a company. Thus, there is a clear need for (semi-)automatic methods for extracting knowledge from data. Data mining specially discovery association rules try to find interesting patterns from databases that represent meaningful relation between products and customers or other relations in some other applications.

Let I be a set of items. A set X= {i1, .,ik} I is called itemset or a k-itemset if contain k items. Each transaction T in the database that is denoted by T= (tid, X), is composed of two parts. First part is transaction identifier and second, subset of the itemset occurs in I. An association rule between a set of item X ? I and another set Y ? I and X ? Y= {} is expressed as X=>Y, and indicate that the presence of the itemset X in the transaction also indicate a strong possibility of presence of the itemset Y. The measure used to evaluate  Manuscript received December 13, 2007 Manuscript received December 14, 2007.

Huimin HE is with the Department of computer science, Handan College,  Hebei Handan, 056005 CHINA. He is an associate professor now and the research fields is Computer Structure and Embedded System (phone: 86-15930805817; e-mail: Hehuimin1967@gmail.com)  Haiyan DU is with the Department of computer science, Handan College, Hebei Handan, 056005 CHINA. She is a lecturer now and the research areas: are Computer Networks.

Yongjin LIU is with the Department of computer science, Handan College, Hebei Handan, 056005 CHINA. He is an associate professor now and the research areas are Computer Application.

Fangping LI is with the Department of computer science, Handan College, Hebei Handan, 056005 CHINA. Her research areas are Multimedia Technology.

Yi XIE is with the Department of Electrical and Electronics Engineering, Nanyang Technological University, Singapore. His research areas are Computer Networks, Optical Communications.

the level of presence of an itemset in the database is the support. The support of itemset X is equal to the fraction of transaction containing X. The measure that demonstrates the possibility of presence Y in the transaction, when X occurs in it, is confidence. The confidence of the rule X=>Y is the fraction of transaction containing X which also contains Y. In association rule mining we want to find all rules with the frequent and confidence above minimum threshold for frequent and confidence.

The problem of finding association rules first was introduced by Agrawal and his cooperators in. The frequent itemset and association rule mining problem have received a great deal of attention and many algorithms have been proposed to solve this problem.

Discovering association rules in these algorithms usually done in two phase. In the first phase frequent itemset generated and then interesting rules extract from frequent itemset. The rules that have a support and confidence above minimum threshold are interest. The task of discovering all frequent itemset is quite challenging especially in large databases. The database could be massive, containing million of transactions. A fast algorithm called Apriori was proposed in, which generates (k+1)-candidates using joins over frequent k-itemset, all subset of one itemset must be generated by the algorithm. Although many of these frequent itemset may be not useful and may not exploit for finding association rules because some of these frequent itemset haven t any interestingness antecedent or consequent in rules but we had to generate them to find superior frequent itemset.

Additionally size of database was main problem of this algorithm. Some modified algorithm of Apriori algorithm proposed to solve this problem but those algorithms also have the database size problem. For discovery association rules in short time some algorithms proposed such as sampling algorithm. Sampling algorithm proposed by Toivonen is an attractive approach to find association rules in tiny time with lost a little accuracy. In this paper we wish to propose new encoding database method and new algorithm to find only frequent itemset that useful and have interestingness antecedent or consequent and with higher probability generate association rules.

This paper is organized as follows. In the next section we propose a method for encoding databases and converting it to new layout. In section III we will discuss about key step in discovery association rules (itemset mining) and we ll propose new algorithm for generate frequent itemset. We propose our strategy for finding interesting association rules in section 4.

In section VI, we discuss some optimization on the  New Coding Method to Reduce the Database Size and Algorithm with Significant Efficiency in Association Rules  Huimin HE, Haiyan DU, Yongjin LIU, Fangping LI, Yi XIE  T      algorithm and we propose some technique to solve some drawback of the algorithm. In section 6 we present some empirical result, while last section contains the conclusion and summery.



II. DATABASE ENCODING  Database presentation is important consideration in most algorithms. The most commonly used layout is the horizontal database layout and vertical database layout. In both database layout set of items in transaction is denoted by binary value.

The size of database in both presentations is large. Encoding database layout to new presentation can reduce the database size and improve the efficiency of algorithms. We want to convert large databases into smaller database layout, which have all of property of prior layout. Since instead of maintain large table for database transactions, one table is created only with two columns. First column is for transaction identifier and second column for entire items that occur in the transaction. All items in one transaction are converting to only one number that has all property of those items. In this new state our database is too smaller than previous database layout and bringing it to memory is very simpler and its cost is order of magnitude less than bringing prior layout to memory. With this assumption any itemset as represented by only one number. For convert any itemset to number we must define some measures. First we define measure attribute. A measure attribute (MA) is a numerical attribute associated with each item in each transaction in database layout. A numerical attribute can have binary type. Those items that are occurring in one transaction are depicted with 1 and other items represent with 0.The transaction measure value, denoted as tmv(Ip, Tq), is the value of a measure attribute associated with an item Ip in a transaction Tq . If tmv(Ip,Tq)=0 this means item Ip not occur in transaction Tq but if occur item Ip in transaction Tq, tmv(Ip,Tq)equals to 1.For example in table 1 tmv(D,T1)is equals to 1.Any item Ip in set of items is encode to one prime number denoted as E(Ip). We use prime numbers for this property that prime numbers aren t divided by any number except 1 and themselves. For any item Ip in transaction Tq, we assign new measure that denoted as M(Ip, Tq)equals to multiply tmv(Ip, Tq)to it s Encoding number(E(Ip)). This value is denoted by equation 1.After this for all Ip and Tq that its M(Ip, Tq)value equals to 0,M(Ip, Tq)are converted to 1.This operation described in equation 2.Value MTq for any transaction is equal to multiplication of all M(Ip, Tq).Value of M(Ip, Tq)represented in equation 3.

( ) ( ) ( ), , .P q P q PM I T tmv I T E I= (1) ( ) ( ) ( )all ,  if M , 0 , 1P q P q P qFor I T I T M I T= ? = (2)  ( ),Tq P qM M I T= ? (3) For any itemset I, that is represented in schema  I=(Ip1,Ip2, .Ipn)there is one value denoted as MI is equal to multiplication of all E(Ip)that its Ip occur in I. Value MI is show the number correspond to itemset I. after this we can use this number instead of itemset I.

( ) p  I P I I  M E I ? ? ? (4)  With this encoding, instead of maintain all tmv(Ip, Tq)for every item and transaction, we can store value MI for every transaction. You can see using this technique in table 1.

TABLE I CONVERT VERTICAL DATABASE LAYOUT  TID A B C D  (a) IP EIP A 7 B 5 C 3 D 2  (b) TID M 1 14 2 2 3 10  4 6  (c) In table 1 our database has represented in (a).We have 1  column for Tid and 4 columns for items. In (b) we can see the set of items in left column, and prime numbers correspond to them in right column. By applying equation 1, 2, 3 we have (c) that has only two columns and every transaction have MTid instead of some binary numbers for itemset. In this situation if we want checking for example is there itemset I= {B, C} in 3th transaction or not, we can divide MTid=3=105 to MI=5*3=15. In this condition 105 divided by 15 then we can say itemset I has occured in transaction (Tid=3).This means for verifying presence any itemset Ip in transaction Tq, dividing value MTq to MIp is sufficient. Only one drawback may be occur in this encoding, when the number of items in database be large then the last prime numbers is been high.

This caused the value M of itemset and transaction increased rapidly. We propose a technique to solve this problem in section 5.using this encoding method and applying modified known algorithm such as apriori and apririTid and aprioriHybrid can significantly improve efficiency of algorithms.



III. FREQUENT ITEMSET MINING  Almost all of algorithms for itemset mining start finding frequent itemset with single itemset in first iteration and in next iteration with combining generated frequent itemset create 2-itemset candidate and continue to discover larger frequent itemset. In these techniques some frequent itemset create that may have no significant value such as 1-itemset that has no value independently. In other words in these algorithm finding frequent itemset is done in bottom up manner. In our algorithm, discovery frequent itemset is done in up to down style. It means that, first large frequent itemset are found and then all of their subsets (that are certainly frequent) are extracted. This property of frequent itemset is described in. Instead of computing small frequent sets and  2324 2008 IEEE Congress on Evolutionary Computation (CEC 2008)    combining those to find larger frequent itemset, in this algorithm frequent large itemset is discovered and we can extract all of subsets without more computing because if an itemset is frequent all of its subset could be frequent as well.

In this technique is supposed that any frequent itemset must be at least one time occurs in database lonely (without any other item than not member of that itemset) in transactions. In other words if itemset (A, B, C) frequent we must have this itemset at least in one transaction without any other item. That transaction in prior layout must be in this schema:  TABLE II FREQUENT ITEMSET PRESENTATION  A B C  Tid 1 1 1 000000000  In this method for every transaction Tq, greatest common divisors (GCD) between MTq and MT correspond to other transactions computed and these greatest common divisor and frequent of them are stored in GCD set. GCD set is candidate for frequent set. If there is any GCD in GCD set, that it s frequent above the required threshold, it will be selected and added to frequent GCD(FGCD)corresponds frequent itemset.

For every transaction is maintained a set denoted GCD Tid, composed of GCDs and frequency of any GCD. For example if GCD set and frequency between first transaction and other transactions is equal to GCD1= {(42,3),(6,8),(21,2) (15,4), (105,1)}and required threshold for support is equal to 7 then set(6,8)has a frequency greater than 8,then 6 add to frequent GCD(FGCD).The itemset mining phase of this algorithm is given in Algorithm 1.

We use notation M[i], to represent the value M for ith transaction and for required threshold for support. CF consists of Candidate frequent set (GCDs and their frequency).And those GCDs have a frequency greater or equal to be adding to FGCD.

----------------------------------------------------------- Algorithm 1:itemset mining ----------------------------------------------------------- M: array [1...DBsize] of longint {maintain M of  transaction} FGCD= {} For i: =1 to DBsize do Begin CF= {} For j: =1 to DBsize do Begin a:=gcd(M[i],M[j]); if a in CF then inc(a.f)//addition frequency of a else add (a,1)to CF;//insert a with frequency=1 to CF end for if there is any a in CF that a.f>=then add a to FGCD; end for -----------------------------------------------------------

IV. ASSOCIATION RULE MINING  Discovering association rules based on all frequent GCD  which have been found in previous phase. Measure M corresponds to any frequent itemset is maintained in FGCD.

Every measure M in FGCD is decomposed to multiplication of prime number and each prime number corresponds to one item. The itemset that correspond to M is identical and frequent then any subset of it must be frequent.

Every M in FGCD decomposed into a candidate head Y and a body X=M/Y. algorithm iteratively generates candidate heads Ck+1 of size k+1, starting with k=0.If head and body are interesting and valuable, the confidence c of the rule X Y is computed as the quotient of the supports for the itemset c=support (M)/support(X)(support(X)is computed by counting the number of MTid that divided by X).If any rule have a c greater than or equal to given threshold for confidence, that rule adds into association rules. The association rule mining phase of this algorithm is given in Algorithm 2.

We use notation M[i], to represent the value M for ith member of FGCD and ? for required threshold for confidence.

------------------------------------------------ Algorithm 2: association rule mining ------------------------------------------------- Association rules= {} For i: =1 to FGCD-size do Begin Decompose M[i]into prime number set M[i] = {t1, t2, tn} K: =1 While k<n do Begin Ck= {all subset with k member of set M[i]}; For any t Ck Begin X=1 For i: =1 to k X: =X*ti//ti t Y=M[i]/X If X and Y are interesting and c(X=>Y)> ? Add (X=>Y) to association rules; End for K++; End while End for -----------------------------------------------------------

V. OPTIMIZATION  Encoding database by using prime number to quantity number at least reduce the size of database to half size. But in large database with large number of item, encoding any item to one prime number may be caused some problems specially in computing the value M for nay transaction. The M value for any transaction with number of item greater than 30 may be excess than number that can be shown with two byte.

To solve this problem can divide the database in smaller part vertically and put correlated items in one part.

(Correspond to previous discovery of association rules).for  2008 IEEE Congress on Evolutionary Computation (CEC 2008) 2325    example put book items in one part and food items to another part. Then every part of database is encoded to one column. In this situation frequent itemset mining is done independently for any part and association rules in every part are discovered.

After this encoding, database has little columns and all algorithms for itemset mining can use it to achieve better performance and efficiency. Another approach to solve problem of growing M is assigning smaller prime numbers to items with higher frequency.



VI. EXPERIMENTAL RESULT  Our implementation on some benchmarks such as T40I10D100K and mushroom and BMS-Webview-1 denoted that by applying our encoding method size of database was reduced at least half. The sizes of databases have been shown in figure 1 in our encoding layout and prior layout.

Fig.1 Database size in prior and new encoding  Also our implementation showed that the speed of our algorithm after this encoding increased especially in sparse databases. Our improvement in speed of discovery association rules are at least 25%.The compromise between our algorithm and Apriori and FP-growth showed in figure 2.

Fig.2 Required time for generate association rules in synthetic data set.



VII. CONCLUSION  In this paper an encoding method was proposed to reduce the database size and after it by applying new algorithm on new layout we can achieve significant efficiency in association rules discovery. This efficiency in algorithm is achieved by lost 5%of association rules. In situation that size of database is bottleneck of discovery frequent patterns in databases, using encoding method can significantly improve the efficiency of known algorithms. In some condition, the efficiency of discovery of association rules mining is more important that accuracy we can use this algorithm to find  association rules with consuming short time.

