April 24-26, 2015, Changsha, Hunan, China

Abstract-Apriori algorithm usually generates a lot of  candidate sets in data m ining, and needs to repeatedly scan  all the data of the database. Making it in the implementation process efficiency is greatly reduced. This paper proposes  the establishment of a number of linear lists recording each  transaction in order to achieve the purpose of reducing  transaction. Setting value of 'up' to raise the threshold of combination of frequent I-item sets. It finds 2-item candidate for setting to achieve the purpose of pruning. The  experimental results show that the impro ved algorithm  performance has been significantly improved.

1.lNTRODUCTlON  D ATA mining is to fmd potentially useful data relationships from the seemingly disorgani:-:ed da?a.

Data mining can be used to fmd the relatIOnships  which are hidden under the law of large databases. Data  mining makes data into knowledge in order to provide  effective help for the people in the data analysis process.

Knowledge Discovery process consists of data cleaning, data integration, data selection, data mining and pattern  recognition.

Data mining is a meaningful exp loration and has huge  potential on hidden patterns in the information of the  migrant workers' emp loyment, because hidden patterns  and relationships are often untapped. In the process of data mining many types of algorithms can be used.

Association rule algorithm is one of them. Association rules [ 1,21, first introduced in 1993,are used to identifY  relationships among a set of items in databases. These  relationships are not based on hereditary properties of the data themselves, but rather on co-occurrence of the data  items [31.



II. APRIORI ALGORITHM  Apriori algorithm uses the support and confidence of pruning techniques to effectively control the exp losive  growth of candidate sets. Apriori algorithm is the most  classical algorithm of association rules discovery of frequent sets [41, and its function is to find all frequent sets in a given database. Based on a priori principle, if an  itemset is frequent set, any one of its subset must have  been frequent. Example: If a 2- itemsets {ij} is frequent itemsets[5,61, its 1- item subset {i} and {j} must be a frequent itemset.

Apriori algorithm is the core of all the data in the  database, using an iterative method to traverse the  This work was supported in part by the revitalization project of Higher Education of Anhui Province under Grant BS 2014zc!iyOSO and AnhuiKey Research Base Project (No.SK2012B625).

Tianpeng Han is with Scho ol  of Computer and Info rmation Engineering, Fuyang Teachers College, Anhui, China (e-mail: dudu20110102@163.com).

Daimu Wang Scho ol of Computer and Information Engineering, Fuyang Teachers College, Anhui, China (e-mail: wangdaimu@sina.com).

database and to find that frequent K- itemsets, and then  use a combination of frequent K-term to generate a  (K+I)-itemsets, this method is called layer by layer  search. At the operational level, first of all, you need to  find all 1- frequent sets in the database, only one item in  the collection, and its ratio in the database to exceed the threshold of support [81, denoted L1. By a priori principle  known, using frequent 1- itemsets, it is combined to give  2-itemsets. Itemsets are candidate 2-itemsets. Then  traversing the database and computing support of each  candidate 2- itemsets. When support of a candidate 2-  itemsets was on the threshold of frequent items, and then  we could find all 2- item frequent sets. Thus, according to  the above steps, continue to find all frequent 3- itemsets,  frequent 4- itemsets until no new frequent K- itemsets appears.

Frequent itemsets I'll of the Apriori algorithm produces the following features: First, layer by layer algorithm that all frequent itemsets are generated from frequent  I-itemsets, and so produce the next level frequent  itemsets until only frequent itemsets are empty; Second, the steps which the frequent itemsets are discovered are  strategy which are firstly produced candidate sets, and  then to test. Each candidate is calculated. Then we could  make the obtained results compare with the minimum  support threshold. If it's greater than the support of  candidates, it is frequent.



III. APRIORI ITEM SETS PRUNING  Apriori algorith m can be seen from the characteristics of its own inherent defIciencies:(I) When using iterative  methods to generate new frequent itemsets, it needs to re-scan the database, which means that if you want to  generate all frequent itemsets, the database needs to be  repeatedly scanned. ( 2) A large number of candidate sets  are generated. These candidates need to be judged and  pruned again. Park et al proved by experiments that most  of the time by Apriori algorithm was used to generate 2-  frequent itemsets in the process [ 101, so if you speed up the  rate of formation of 2- frequent itemsets in some way, we  can make the efficiency of the algorithm improved. This  paper proposes the establishment of a linear list of items  in the number of records for each transaction, on the other  hand proposes to prune the candidate sets to reduce them.

The following proposes an improved method for the detailed description:  A. Establish linear lists  Creating a dynamic list L_Count, T= {tl, t2, ... , tN). The number of items is used to record the corres ponding  transaction. For example L_Count; is used to record the number of items in the transaction. According to the basic  principle of frequent itemsets, if L _ Count; :S (K - 1), t; does not contain frequent K- itemsets. Therefore, when  the frequent K- itemsets are generated, it is not necessary to visit t;. With the increase in the number of frequent    items, the items which would not require to access are exponentially increased.

B. Candidate set pruning methods  Aiming Improvement of Apriori algorithm defects  follows: 1) First scanning the database and then reading  transaction items, / = {ii, i2, i3, .. . , id} is the set of all the items in the database, T={tl' tZ,t3, ... ,tN} the set of all transactions in the database. Calculate meet no less than  the minimum support values of frequent all 1 - itemsets,  denoted Fl , And recording support of every frequent 1-  itemsets as Sup [ill 2) The frequent l-itemsets through a combination  generate all candidate sets of 2-items. Then traversing the database to fmd the all exactly frequent 2-itemsets. It was inefficient in a large database, and did not break through  the previous pruning strategy. Analyzing the frequent l-itemsets can be seen, when the Sup [i;] and Sup [i)] are equal to or slightly greater than the support. In order to 2- candidate sets are frequent, all i; and i/ must be concentrated in a transaction entry.

As can be seen from the above description, if we only  allow a range of frequent 1- itemsets which are greater  than the min imum support are combined, and in the scope  of the following frequent 1- itemsets can't be combined, thus the resulting 2- item candidate sets are reduced. of  course, and this may not contain all of the candidate set,  but sometimes in big data we hope to get the result of an error can be tolerated in a short period of time, and less  willing to spend a very long the time to wait for an  accurate result.

3) Repeat steps ( 2) to generate frequent K-itemsets F K (K 2: 3), until no new frequent item sets to generate.

The improved CSP-Apriori algorithm is described as follows:  k = 1  Fk = {i I iEI /\ N x minsup} L_ Count ={L_CountJ ,L_Count2, ... ,L_CountN } While (k < N && Fk != C/J ) { k=k+l Ck = CSP-apriori-gen (Fk -I ) for(i = 0; t; E T; i++) {if (k < L_Count;)  if(Ck E ti) o(Ck)++; } Fk={clc E Ck /\ o(cYN2:minsup} } CSP-apriori-gen (Fu, up) { input: frequent (k-I)-itemsets  output: After pruning the candidate itemsets CP-Ck for each IE Fk  for Each K is not equal to I, but E Fk if (sup (1) >= 1 + up && sup (K) >= 1 + up )  CP- Ck+1 =/U K ;  CSP-Apriori Flowchart As follows:  N  N  N  END  Fig. I CSP-Apriori Flowchart

IV. ANAL YSIS AND VERIFICATION ALGORITHMS  Compared with the classic Apriori algorithm, the  advantages of CSP-Apriori algorithm is as follows: I) In order to reduce the number of reading and writing  the database, establishing a list in the algorithm was used to record each transaction entry item number. Because  when L _ Counti < t), the current candidate can't be contained in t/, so no need to go to read the data of database. It could make I/O of the database more efficient,  reducing the execution time of the algorithm. This may  lose some of the accuracy.

2) A large number of candidate sets are to be generated in Apriori algorith m, so that during operation it increases  the complexity of the space. CSP-Apriori algorithm can  be used to generate candidate sets pruning. It can optimize the storage and reduce these candidate sets  which are produced under a small probability. But we can  use the values of 'up' to control the accuracy of the candidate's. Make the algorithm to reach the balance  between accuracy and efficiency.

To verifY the efficiency of the CSP-Apriori algorithm,  following on Apriori, CSP-Apriori and FP-tree in  different 'up' value to validate comparisons. Experimental data was got from the database of the information of  emp loyment of migrant workers, and which was provided  by the colleges and universities in Anhui Province Key  Research Base. Database contains 21,694 items including  19 properties transaction which are the name, gender, age,  place of birth, education, type of work, place of work.

Some individual attributes are 5-37 enumeration values; the workplace is 2583 samples defaults. After data  preprocessing, the fmal division of property obtained 85, and then test the data.

Experiment 1: VerifY that in the case of comparing different 'up' value with the Apriori, CSP-Apriori and  FP-tree running time. Under different degrees of support,  when up = 0, up = 5%, up = 10%, shown in Fig. 2, 3, 4.

10000 ...--...-------------- ?Apriori  !::.

:3 (1l 6000 +---...... """"::OIE-?------?......

:3 rJJ -'  4000 +----?????._--  2000 +-----??????-  0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  Minimum support  Fig. 2 \\hen up = 0   !:".

3 6000 (1)  '"""'  g 4000 '-'   0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  :Minimum support  Fig. 3 \\hen up = 5%   ... 8000 _CSP-A riori 3? (1)  ,.-., 6000 Ul "-'    o 0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  Minimum support  Fig. 4 WIen up = 10%  It can be seen from the experimental conclusions.

CSP-Apriori which used the time is always better than  Apriori algorithm. When up=O or ( up=5% and support <1 .5%), CSP-Apriori used time more than FP-tree. When  up=10% or ( up=5% and support? 1.5%), CSP-Apriori  used time less than FP-tree.

The values of 'up' more higher, the frequent items will  be more pruned when pruning process. The advantage of CSP-Apriori is more obvious. But it may also be a problem, when the value of 'up' becomes more lager, the  algorithm accuracy will be worse. It requires the 'up' at a reasonable value in the range. These ranges of values are  determined by the different situations of frequent items  pruning accuracy. That is to say Experiment 2 data to  determine it.

Experiment 2: VerifY that in the case of comparing  different 'up' value with the Apriori and CSP-Apriori it  generates the number of frequent sets. Under different degrees of support, when up = 0, up = 5%, up = 10% ,  shown in Fig. 5,6,7.

? 30000 .., (1) 27000 ..Q C (1) 24000 a- U) 21000 ? Ul 18000 ? 15000 :3 cr- 12000 q     ? 30000 "2 27000 (1) a- 24000 ? 21000 ? 18000 ? 15000 g- 12000 .., 9000   o  0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  :tvfinimum support  Fig. 5 WIen up = 0  0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  :tvfinimum support  Fig. 6 \\hen up = 5%     >'Tj 30000 ..8 27000 ? 24000 (Il 21000 '" 1;]'" 18000 ? 15000 g. 12000 ? 9000   o 0.5% 1.0% 1.5% 2.0% 2.5% 3.0%  Minimum support  Fig. 7 \\hen up = 10%  As can be seen from Figure 5,6,7 at different values of  'up', Frequent sets which are produced by CSP-Apriori  algorithm is different, when the higher the value of 'up',  the less the number of frequent sets are produced.

Analysis derived from the figure, when up = 0, the  frequent itemsets with increasing support, in terms of the  total datasets remains unchanged, CSP-Apriori algorithm wh ich produces frequent itemsets less than A priori  algorithm 11%-41%; when up 5%, less than  19%-58%;when up = 10%, less than 27%-65%. We can analyze the data obtained from the above, when the  support, the greater the value of the 'up' frequent episodes  will be smaller. This condition is frequently ignored  which maybe set more frequent. Collection of accurate  collection will  not be enough. Frequent collection set accuracy is not enough. From the experiment, while  frequent item sets by 30% or less, it has little effect on the  final results produced. So when the support becomes larger, in order to ens ure the accuracy of the frequent sets, ranging up to be between [0, 0.05], support should be less  than 30%, so as to ens ure a relatively accurate data.



VI. CONCLUSION  Through the classical Apriori algorithm analysis found  that the algorithm will produce a lot of eventually proved  useless candidate set, which may also generate  unnecessary database I/O operation. This paper proposes  the establishment of the list to reach the effect of  compressing the transaction. By raising the threshold  frequent items combined to reduce the probability that a  combination of s mall production, to achieve the purpose  of the candidate sets pruning. Whether it is to establish line which records number of each transaction or set candidate of pruning, the goal is to reduce the running  time of the algorithm  Of course, we should pay attention to the range of 'up' when set to reduce the frequent itemsets. Because of the  different relationships between the data in each database, the required values of , up' is different. However, it is must to ensure that the error data in the acceptable range.

According to the paper, the experiment can be seen when  'up' value in [0, 0.05] error, itemsets generated is acceptable.

In this paper, we have proposed the CSP-Apriori algorithm for processing transaction data with 'up' values  and for mining frequent itemsets from the transactions.

We assume the database is static. In real-world applications, data may be dynamically inserted into a database. In the future, we will attempt to handle the maintenance problem of data mining. How to further  improve the CSP-Apriori is another interesting topic.

