

Abstract?Applied Remote Sensing to Disaster Management requires just-in-time delivery of custom data products to unsophisticated end-users such as fire fighters and first responders.  These requests are wide-ranging, from wild fire hot spots detection, smoke, fire suppression and rehabilitation, flood coverage to erupting volcanoes, and require coordination of many assets such as satellites, UAV?s and ground sensors.  They also require predictive models to complete the SensorWeb.

Using Open Geospatial Consortium (OGC) standards, distributed web services from many organizations have been geo-enabled (or sensor-web enabled) to process the data and distribute it to end-users using Web 2.0 technologies such as Atom feeds and KML.

The next challenge is to flow-enable these services to facilitate automated orchestration for on-demand requests coming from various communities in times of need.  With our desire to provide the ability to create quick mash-ups for our end-users using a simple web browser, we have implemented a RESTful1 architecture and applied it to workflow management with the help of the Workflow Management Coalition (WfMC) and the OGC to support interoperability across our SensorWeb Community.  These workflows must operate on behalf of a wide range of users and access services that may or may not be restricted.  At a minimum, data consumers and providers need to communicate over http using some level of authentication that can be easily implemented in a RESTful manner.

This paper also presents our Decision Support System used to manage our various Communities of Interests (COI) and give  1 REST ? Representational State Transfer  them transparent access to the SensorWeb assets and relevant custom data products when the needs arise.  An Analytical Hierarchy Process (AHP) is used to automate the daily selection of requests to be scheduled for imaging and processing based on themes of interest2,3.

TABLE OF CONTENTS  1. INTRODUCTION ............................................................ 1 2. FLOW ENABLEMENT AND ROA FOR SENSORWEB..... 3 3. RESTFUL WORKFLOWS ....................................... 3 4. SECURE WORKFLOWS ......................................... 4 5. INTEGRATING MODEL FORECASTS AND USER REQUESTS USING SENSORWEB...................... 4 6. CONCLUSION........................................................... 6 7. REFERENCES ........................................................... 7 8. BIOGRAPHY ............................................................. 7  1.  INTRODUCTION Many OGC standards have given a wider community of users access to remote sensing assets and data.  The Sensor Planning Service (SPS), Sensor Observation Service (SOS), Web Processing Service (WPS) and many others can now be orchestrated with workflows to provide just-in-time relevant data to end-users such as first responders or personnel from disaster management agencies.  KML files distributed via GeoRSS feeds allow those users to quickly visualize multiple layers of data and information on the desktop using Google Earth.

These users are not necessarily sufficiently trained to task a particular instrument onboard a satellite such as the  3 IEEEAC paper#1335, Version 4, Updated Dec 12, 2008       hyperspectral or multispectral imagers onboard EO-1.  A higher-level interface is necessary to allow them to post a request for a specific area-of-interest and a theme. Disaster management themes include: flooding, fires, oil spills, drought, earthquakes, tsunamis, and hurricanes following the NOAAWatch themes4. Users are now disconnected from the sensors, instruments and data processing requirements.  Under the hood, an infrastructure powered by dynamic and/or concrete workflows manages the complexity transparently to the user.

For a specific example, we can go back to the recent flood in Kenya on Nov 13th, 2008.  The Regional Centre for Mapping of Resources for Development (RCMRD) in Kenya posted a request for imagery in the flood area. The system picked up the request, tasked the EO-1 satellite, acquired the data and processed it using various web processing services such as the Jet Propulsion Lab Web Processing Service (WPS) for Hyperion data and the Vightel processing service for ALI data.  Some of the Level 2 products were generated and made available as atom feeds.  These feeds can be published to an aggregator such as Feedburner (www.feedburner.com) and viewed with a standard web browser like FireFox with the Sage Feed reader Add-on or Google Reader.  An example feeds can be found here5.  This relieved the end-users in Kenya to run the data through ENVI and painfully generate the products themselves.  Feed entries contain links to the data and KMZ files that allow for direct ingest and visualization using Google Earth.

4 http://www.noaawatch.gov/themes/severe.php 5 http://feeds.feedburner.com/EO1A1700602008318110KF    Figure 1 ? Product Feed in Reader               2. FLOW ENABLEMENT AND ROA FOR SENSORWEB  Targeting a wider community, dubbed as Mass Market, requires a gentler architecture that can scale in a global and federated environment such as GEOSS.  A service-oriented architecture (SOA) based on SOAP/WSDL is out of reach for most of our users and disadvantaged organizations in third world countries.  However, it is critical to reach these users.  Quick mashups within a standard browser or Google Earth application on the desktop could make a difference in the field during emergencies.

OGC services originated circa 2000 in a REST world heavily influenced by RPC software technologies from ONC RPC (1980) to DCE and CORBA (1990-2000).

Publishing service operations or methods was the de-facto norm but proved to be difficult to scale as different types of web services increased in numbers.  Over the years, a natural web service evolution caused a multiplication of services, which increased the number of operations to be mastered by the users and successfully implemented by developers.  The size of the specifications has become an issue and a formidable obstacle for new entrants.  This is, unfortunately, not unlike the evolution of the Human genome6 over the years.

In 2007, Leonard Richardson and Sam Ruby finally codified[8] RESTful Web Services.  Using a reduced operation set such as GET, POST, PUT and DELETE mapped to the traditional Create/Read/Update/Delete or CRUD transactions on resources, ROA (or Resource Oriented Architecture) was born with a solid framework that could scale.  There are additional services that complement REST7 such as AtomPub, OpenSearch, Google GData, GeoRSS and KML. A new and consistent way of accessing web resources by casual users finally emerged.

3. RESTFUL WORKFLOWS The Workflow Management Coalition (WfMC) was created in 1993 to focus on Business Process Management.

Orchestration of web services using workflows was enabled with the BPEL standard in 2003 and a generic API developed by the Workflow Management Coalition (WfMC) called WfXML, which was SOAP-based.

In 2006, we developed WfXML-R, a RESTful version of that specification during the Open Geospatial Consortium interoperability demonstration #5.  Following a Resource Oriented Architecture Approach and the AtomPub standard, resource collections are exposed via a service document that can be easily mined by the users.

6 http://www.nature.com/embor/journal/v7/n8/full/7400756.html 7Roy Fielding, PhD Dissertation (2000), http://www.ics.uci.edu/~fielding/pubs/dissertation/top.htm  Far from being optimum, Workflow Chaining Service (WfCS) resources include workflow metadata information, workflow definitions, participants, workitems, processes which are running instantiations of specific workflow definitions and so on.  From a user perspective, these workflow resources appear strangely similar to the RPC operations that have been exposed since the 1980?s.  Users are not as interested in these high level operations or workflows.  They are more interested in the products that these workflows will create on their behalf.  These products that do not exist are virtual but, from the user?s perspective, as real as any other products that have already been generated.  Virtual products are the actual resources that need to be published to the external world.  Extended GeoRSS feeds are used for publication.  Users can instantiate them on-demand by following the links that will eventually trigger workflows with the parameters necessary to generate the requested products.

As workflows can orchestrate services on many sites on behalf of the users, secure access to the services and delegation of user authorization is becoming an important issue to address properly.  This capability has to be fairly easy to implement to be adopted by the various Communities of Interest.

Critical to a Workflow Chaining Service is the capability to include one or more users as participants to selected activities of the flow.  Workitems or user tasks can be created and queued to specific stores.  Users have to retrieve the workitems from their respective stores, complete the allocated task and return the workitem to the workflow to continue the process.  When workitems accumulate in a store, an RSS feed can be generated to warn the user monitoring that feed that workitems are ready to process.

The user can follow one of the links within a specific entry to be redirected to the right store and start the processing of the workitem.

The main advantage of a WfCS is the inherent capability to support transactions: adding or removing workflows or definitions to the system.   A workflow can be viewed as a Web Processing Service (WPS).  It is a black box where inputs and outputs are specified.  A WPS can be viewed as a WfCS with one workflow.  A WPS-T or WPS with transactions is really a full-fledged WfCS.  Is a new specification needed for the WPS to support transactions?

A WfCS is a better way to design a WPS.  Beyond the inputs and outputs, the flow diagram acts as documentation for the process to be instantiated.  Standardization of the diagram notation is essential to allow analysts a better comprehension of the process.  The business Process Modeling Notation (BPMN) is used to support technical and business users with an intuitive notation that can represent complex process semantics independently of the underlying execution language.

This notation is similar to the musical notation that allows us to play musical pieces created several hundred years ago.

Any musician can understand what the notation means.

This standard notation does not imply that any instrument can play the music on the sheet.  Your instrument may be range limited.

Figure 2 ? BPMN diagram example  BPMN has the same goal and limitation.  Consideration to the targeted execution engine is key.  This is not quite a universal language.  The complementary standard to BPMN is XPDL.  XPDL is used for serialization of the diagram.

XDPL can be used as an interchange document format between the modeler and the execution engine.  Some engines can execute XPDL natively; others will translate XPDL to a native format.  This format allows for the separation of the modeler and the engine itself.  Users can pick what modeler works best for their own needs.

4.  SECURE WORKFLOWS The last breakthrough came with new security protocol standards such as OpenID and OAuth for RESTful services.

The major issue that hinders acceptance of a SOAP/WSDL architecture is the difficulty to successfully implement security.  The WS-* stack layer requires a higher level of expertise than our neo-geographers have.

Working in a federated environment of trusted organizations, user authentication using OpenID is becoming a de-facto standard in the Web 2.0 world.  Single sign-on is a must for the SensorWeb to succeed and scale.

It would be unconceivable to require users to have accounts  on every machine accessed during the various activities required to task sensors, process and distribute the data leveraging web services available across the country.

OpenID allows for authentication of users and services leveraging the support of a trusted network of Identity Providers.  The delegation of user authority to workflows was the final milestone reached last year during the OWS-5 interoperability demonstration.  Workflows that have been instantiated on behalf of a specific user may need to access services that may require restricted access privilege.  User information and granted role may need to be accessible by one or more services in a flow.  OAuth has been devised as a simple secure API authorization that allows services to exchange information in a trusted manner on behalf of a user.  Since users may not be available at run-time, a modification has been made to the protocol to allow for pre- approved transactions.  Users can, off-line, grant or revoke specific services access to resources they may have privileged access to.  User and security information can be safely exchanged within the http header.  Recent demonstrations have shown that developers can successfully implement that protocol within a day or so.  This is a significant improvement over other more exotic methodologies, such as WS-*.

Using the OpenID of the requestor, the web service can easily request and obtain user profile information if granted by the user.  Profile information contains email, organization and a few other pieces of information related to user identity.  In our case, a critical piece of information is missing: the user role granted by the organization to the user.  Once the user role is known, access right can be inferred assuming that the role has not been tampered by the user and has been granted by a trusted organization.  The attribute exchange protocol can be used to exchange extended attributes outside the current scope of the protocol.

At the moment, custom OpenID providers must be used to exchange role information using a previously agreed upon type identifier.

A secured, flow-enabled SensorWeb is now in place to allow end users such as first responders to request custom data products just-in-time to meet their special needs in an area of interest without requiring a deep knowledge of remote imaging sensors, data acquisition and complex data processing provided by organizations spread across the entire nation.

5. INTEGRATING MODEL FORECASTS AND USER REQUESTS USING SENSORWEB  In many cases, models can predict potential floods or other disasters.  This can give us enough notice to task imaging satellites and capture the event as it happens.  Many       organizations such as CATHALAC8, RCMRD9, and the International Federation of the Red Cross (IFRC) can now generate requests for imagery using a simple API to our Decision Support System or Campaign Manager.  The next challenge for us is to prioritize the incoming requests against our current mission science imagery requirements.

When an emergency task is granted, a science task has to be bumped.  This may cause some significant problems for a science investigator.  However, it would also address a major societal benefit that needs to be fairly weighted against the loss of a critical science task.

As a proof-of-concept, the EO-1 Mission Science Office agreed to give us up to two imaging slots per day (out of 25) to support international emergencies or disaster relief efforts and to demonstrate the concept.

Ideally, the goal would be to achieve complete operations autonomy.  Long-term mission science goals are decided months in advance and inserted in the schedule once a week.

Emergency requests and other short-term missions are by nature unplanned and can potentially be requested up to 6 hours in advance.  The system needs to prioritize the incoming requests and generate daily replacement records for the scheduler to manage and upload to the spacecraft.

This is a multi-stage decision making process that must take into account many qualitative and quantitative criteria which can be weighted offline and other criteria such as predicted cloud forecast and number of images already acquired for that campaign that contribute to the decision score at the last minute.  The cloud forecast is obtained from NOAA?s National Climatic Data Center (NCEP) Global Forecasting Service (GFS).  A similar and complementary capability from the Air Force is being integrated as part of another Earth Science Technology Office grant effort.

A user can log on to the Campaign Manager using an OpenID.  The system can quickly authenticate that user, access the user profile (if permission is granted) from the inferred Identity Provider to get access to the user email, parent organization, grade/rank and role granted by the organization to that user (future capability of attribute exchange called AX).  The user role could be inferred from the user grade/rank if not specified in the profile.

The user creates a campaign for a particular theme and associates one or more imaging requests with that campaign at potentially different locations.  The system automatically generates the imaging feasibilities for the next seven days by accessing the asset Sensor Planning Service (SPS) GetFeasibilities operation.

8 http://www.cathalac.org/ 9 Regional Center For Mapping of Resources for Development, http://www.rcmrd.org  At noon every day, the system prioritizes the requests for the next day and generates an email to the Review Board members.  The first two requests will be submitted unless a member vetoes a request for a specific reason that has to be documented online and review-able for accountability.

Prioritizing the requests is derived from the Analytic Hierarchy Process, developed by Thomas L. Saaty in the 1970?s to help people deal with complex decisions.  The problem is decomposed into a hierarchy of criteria and sub- criteria that can be weighted with the objective of ranking alternatives submitted by the various communities of interest.

Using this method, request alternatives ought to be ranked against applicable sub-criteria using a weighting method such as pair-wise comparison, direct weighting or value function. This was deemed to be too much work for the review board to be done on a daily basis.

Campaigns, however, can be scored at creation time using the logical framework of AHP.   The score is re-computed daily based on the predicted cloud coverage or other additional criteria (e.g., how many images have we already taken of that area?).

Remote sensing imagery is driven by NASA Strategic goal 3A: ?Advance scientific understanding and meets societal needs?.  The long-term science goal ranking is not currently managed by our decision making process, just short-term goals for societal benefits.  Four criteria are currently defined in the decision hierarchy tree.  The first one is the submitter?s organization.  Is it NASA or a NASA affiliated organization, DOD, a non-governmental organization or others?  The user grade or rank is also taken into account as a criterion.  A higher-ranking submitter would certainly get a higher score than a lower ranking submitter would.

Societal benefits have been broken down in different themes: flood, fire, drought, volcanic, hurricane, tsunami, oil spill, algal bloom and a recently added national theme for DOD customers.

A final criterion has been added to rank national disasters based on potential scope or disaster category levels.  This criterion takes into account the potential population risk, economic and infrastructure risk, environmental risk and finally a national risk for DOD interests.

A direct weighting tool is provided on the GeoBPMS portal to allow the allocation of weights to criteria and sub-criteria.

The review board is tasked to review this monthly.  A sensitivity analysis tool is also provided to assess the impact of a weight change to a potential decision reversal.

6. CONCLUSION A RESTful Resource Oriented Architecture approach is quickly becoming a best practice to address the mass market needs.  User Authorization and web services authentication are now possible with open standards such as OpenID and OAuth.  Delegation of user authority to workflows can be successfully implemented in a short period of time by developers with very little experience in the security domain.  This is critical for integration and the scaling of SensorWeb to GEOSS.

Secure Workflows can now be easily used to orchestrate web services on behalf of the users and provide new products on demand.  Virtual products are the resources to be published for users to find and instantiate when necessary.

AHP is proving to be extremely helpful in automating the decision-making process for imagery campaigns.  This has been an area of intense conflicts between groups for many years.  A clear model allows users to understand how decisions are made or rejected.  This opens the door for automated negotiation between organizations once the ranking has been calculated.

As mentioned earlier, science goals are not ranked using this decision framework.  This has been requested by many organizations including JPL to allow for a fairer imaging allotment of science campaigns.  This would also allow a potentially increased number of societal benefit scenes if their rankings were high enough on a given day.

Figure 3 ? Analytic Hierarchy For EO-1 Campaign Manager         7. REFERENCES [1] ?An Updated Status of the Experiments with Sensor Webs and OGC Service Oriented Architectures to Enable Global Earth Observing System of Systems?, Mandl D., Frye S., Chien S., Sohlberg R., Cappelaere P., Derezinski, L., Ungar S., Ames T., presented at Ground System Architecture Workshop 2007, Manhattan Beach CA, March 2007.

[2] Alameh, Nadine; Bambacus, Myra & al. ?Leveraging Open Standard Interfaces in Providing Efficient Discovery, Retrieval and Integration of NASA-Sponsored Observations and Predictions, 2006 AGU Spring Assembly proceedings (Baltimore, May 2006).

[3] Alameh, Nadine; Bambacus, Myra & al. ?NASA?s Earth Science Gateway: A Platform for Interoperable Services in Support of the GEOSS Vision?, 2006 IGARSS conference proceedings (Denver, Aug. 2006).

[4] ?GEOBLIKI: Geo-Data + Blog + Wiki, An OGC Sensor Web Enabled Data Node?A Data Publisher for Community Collaboration around Geo-Spatial Data?, Cappelaere P., Frye S., Mandl D., Derezinski L., presented at the Free and Open Source Software and Geoinformatics (FOSS4G2006) conference in Lausanne, Switzerland, September 2006.

[5]  ?Open ID 2.0  How does it work? URL:http://openid.net/about.bml[Cited 12 March 2007]  [6]  Hart, Dick ?Etech 2006 -- Who Is the Dick on My Site?? URL: http://identity20.com/media/ETECH_2006/[Cited 12 March 2007]  [7]  OpenID specifications URL: http://openid.net/specs.bml[Cited 12 March 2007]  [8] Leonard Richardson, Sam Ruby. RESTful Web Services. O?Reilly Media, Inc, 2007.

8. BIOGRAPHY Pat Cappelaere is a Software Architect working for NASA and other DOD agencies developing SensorWeb using many of the Open Geospatial Consortium Standards and Web 2.0 technologies.  Pat?s current focus is  on Resource Oriented Architecture for lower cost Mass Market applications.  As an OGC Member, Pat has been involved in many OGC standards and developed a generic open source Sensor Web Enabled data node capability called GeoBliki with eo1.geobliki.com being one of the first  instantiations for NASA.  This SensorWeb Architecture has received the 2008 R&D 100 award from the prestigious R&D magazine (The ?Oscars of Invention?).  After delivering onboard satellite automation with Clementine, EO-1 and FUSE, end-to-end secure automation with workflows and web 2.0 technologies is the next challenge.

Pat is also member of the OpenID foundation, Workflow Management Coalition and one of the developers of the WfXML-R standard used for web service orchestrations.

Pat has an Master?s degree from HEI, France and an Executive MBA from Loyola College in Maryland.

STU FRYE is a Systems Engineer working for NASA on satellite development, launch, and on-orbit operations.  On the Earth Observing One (EO-1) spacecraft, Stu is responsible for implementing service oriented architecture approaches for  autonomous satellite tasking and on-board processing using Sensor Web approaches.  Stu worked to convert the EO-1 mission from a NASA technology pathfinder spacecraft to an international hyperspectral remote sensing research testbed for developing Sensor Webs and autonomous systems in concert with unmanned aerial vehicles, robotic surface vessels, and in-situ sensors.  He was co-winner of the 2005 NASA Software of the Year award for his role in the Autonomous Sciencecraft Experiment on-board the EO- 1 satellite and won the R&D 100 Award in 2008 for the Sensor Web 2.0.  Stu has a Bachelor?s in Mathematics from the University of California and an M.S. in Operations Research from the George Washington University.

DAN MANDL is the EO-1 Mission Manager and the Principle Investigator for the NASA Earth Science Technology Office research award on SensorWeb 2.0, which is a multi-NASA center effort.  The SensorWeb 2.0 team consists of the three authors of this paper, other team members  who reside at NASA GSFC, NASA JPL, NASA Ames and the University of Maryland.  Furthermore, other collaborators on this effort span many other organizations internationally, such as Committee on  Earth Observing Satellites (CEOS), Group on Earth Observations (GEO) and the United Nations  Space-based Information Disaster Emergency Response(UN-SPIDER) group.

Previously, Dan managed the development of ground components on ten other missions.  Dan has a MS degree in Engineering Management from the George Washington University.

