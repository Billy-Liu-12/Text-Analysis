Maintaining Implicated Statistics in Constrained Environments ?

Abstract Aggregated information regarding implicated entities is criti-  cal for online applications like network management, traffic char- acterization or identifying patters of resource consumption. Re- cently there has been a flurry of research for online aggrega- tion on streams (like quantiles, hot items, hierarchical heavy hit- ters) but surprizingly the problem of summarizing implicated in- formation in stream data has received no attention. As an ex- ample, consider an IP-network and the implication source ? destination. Flash crowds, ?such as those that follow recent sport events (like the olympics) or seek information regarding catastrophic events? or denial of service attacks direct a large volume of traffic from a huge number of sources to a very small number of destinations. In this paper we present novel random- ized algorithms for monitoring such implications with constraints in both memory and processing power for environments like net- work routers. Our experiments demonstrate several factors of im- provements over straightforward approaches.

1. Introduction Keeping track of ?interesting? data trends by evaluating var-  ious relations between values of different attributes is the focus of a lot of research. For example, decision support systems are trying to evaluate efficient ways of accomplishing that in an of- fline fashion. The problem is computationally challenging even for such offline algorithms but is exacerbated for the case of data streams with high throughputs that are encountered in con- strained, streaming environments.

However, environments like communication and sensor net- works, security and monitoring applications need accurate and up-to-date statistics in real-time in order to trigger certain actions.

The class of Distinct Count[17] statistics are very useful for such applications since it provides at any moment the distinct number of values or species in a population[7]. For example a typical statistic, for a network router is to maintain the distinct number of sources and destinations or even (source,destination) pairs that the router handles. The distinct count problem has been exten- sively studied in the database literature(see [17] for a survey) and has found applications in other areas like selecting a good query plan in query optimization[23].

? This material is based upon work supported by, or in part by, the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number DAAD19-01-1-0494. Prepared through collaborative participation in the Communications and Networks Consortium sponsored by the U. S. Army Re- search Laboratory under the Collaborative Technology Alliance Program, Coop- erative Agreement DAAD19-01-2-0011. The U. S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon.

?Work performed while the author was with University of Maryland  Source Destination Service Time  S1 D2 WWW Morning S2 D1 FTP Morning S1 D3 WWW Morning S2 D1 P2P Noon S1 D3 P2P Afternoon S1 D3 WWW Afternoon S1 D3 P2P Afternoon S3 D3 P2P Night  Table 1. Example network traffic data In this paper we focus on the problem of maintaining dis-  tinct implication statistics in constrained environments even un- der the presence of noise. Some items imply other items in the sense that they either always, or a given percentage of the time, appear together. We use the term implication to denote such prop- erties between sets of attributes and the term implication count to refer to the number of items that exhibit such implication proper- ties. The statistics we collect not only generalize the distinct value statistics we mentioned but also aggregate and complement infor- mation gathered from data-mining techniques such as association rules or frequent itemsets[22]. Such techniques return the set of frequently encountered associated itemsets, while the implication statistics we consider here, return aggregated information (counts, averages, ratios) without returning the involved itemsets. In con- strained environments (like sensor networks, where aggregation is important for bandwidth conservation and energy consumption) frequent itemsets and association rules techniques cannot be ex- tended in order to provide real-time aggregates and error guar- antees for the implicated queries we are considering. The same stands for the class of ?heavy hitters?[10] which identifies the set of objects whose frequency of appearance is above a given thresh- old. The cumulative effect of many objects, whose frequency of appearance is less than the given threshold, may overwhelm the implication statistics, although these objects are not identified.

To help clarify the meaning and the extend of the statistics that we address, consider a simplistic data stream called Network Traffic, a window of which is presented in Table 1. The stream is comprised of the attributes Source, Destination, Service and Time and is obtained by the traffic a router observes. Table 2 contain examples of real-time statistics which are essential for monitoring purposes and that our framework supports.

A security administrator would like to maintain in real-time the statistic ?how many destinations are contacted by just a sin- gle source?? in order to identify possible intrusion attempts. We consider the Destinations with the implication property: Desti- nation ? Source. In our example, we have that ?D2,S1? and ?D1,S2? have the implication property, D2 appears only with S1 and D1 only with S2, and therefore the returned implication count is two. Furthermore, one might want to consider destinations that 80% of the time are contacted by one single source. In that case D3 qualifies and the returned count is three.

Another similar query for this specific dataset is: ?how many      Example Class How many sources have we seen so far? Distinct Count  How many destinations are contacted by only one source? Implication one-to-one How many sources contact more than ten destinations? Implication one-to-many  How many destinations are contacted by only one source 80% of the time? Implication one-to-one with noise How many sources do not use only the WEB service? Complement Implication  How many sources contact only one destination during the morning? Conditional Implication How many sources contact only one target per service? Compound Implication Average number of destinations that 90% of the time  are contacted from more than ten sources for the P2P service over a sliding window of 1h  Complex Implication  Table 2. Classification of Example Implication Queries  services are being requested from only one source??. The re- turned aggregate in our case is again two (because the correspond- ing implications are ?WWW,S1?, ?FT P,S2?).

A small set of all possible implication statistics that someone can keep track for our toy data set in Table 1 is classified in Ta- ble 2 based on the definition of the implication on each instance.

The apparent wide range of implication statistics is the motiva- tion behind this paper. In our example, such statistics help to keep tracking, in real-time, various traffic parameters. More so- phisticated statistics address either conditional or involving one- to-many implications. A security-expert may want to keep track of the answer to the following questions: ?How many sources connect to only one destination during the morning??, and ? How many destinations are connected from more than ten sources for the P2P service 90% of the time? ?.

Similar aggregate queries are very important to users of deci- sion support systems and data warehouses, where we assume that all the data can be stored and aggregated and that a lot of com- putation can be performed offline with bulk updates during down time. Even in this case however, the problem of maintaining dis- tinct implication statistics is complicated and requires too many computational and storage resources. Although we concentrate on data streams, our methods can be applied to offline query sce- narios, since our algorithm does not require repeated rescans over the entire database. It can run with input the incremental updates to maintain the implication counts as it does for a data stream. The complication behind maintaining implication statistics is partially common with the problem of Distinct Value queries where there is a huge number of duplicates that cannot be accommodated by either available memory or processing power.

In this paper we describe a framework that can be used to es- timate a rich variety of distinct implication statistics in the con- text of streaming environments under constraints on storage and processing power, as for example in the case of communication routers or sensor networks. In addition our techniques can be ap- plied directly to decision support systems and data warehouses, enriching the collection of aggregates that an analyst can use. For a data stream that is logically divided into two sets of attributes A and B, we calculate implication statistics of itemsets ai of A that imply some itemsets of B. The problem is more difficult than identifying frequent itemsets in a data stream, because the contri- bution of a large number of infrequent implicated itemsets can be very significant, overwhelming the aggregate count. We actually  show that current streaming algorithms for frequent itemsets[22] cannot be extended and provide error guarantees in the aggregate statistics.

The context of the environments we are considering, forces the following assumptions: ? There is not enough memory to accommodate the cardinali-  ties of the attributes participating in the query. For example, consider the case where one attribute is the network address of a client which in IPv6 has an address space of O(2128).

? Although some itemsets may not appear frequently enough  and therefore may pass ?undetected? by some technique, they can seriously affect the total count. This is the case for first hop routers[25] in distributed denial of service attacks where the counts are very small at the first hop but signifi- cantly contributing to the cumulative effect on the last hop routers.

? The exact meaning of the implication depends on the nature  of the application. In most situations a analyst will need to allow some tolerance to avoid noise in the data.

The work in this paper concentrates on how such implica- tion counts can be accurately estimated, in the context described above, by using a small amount of memory that holds a ?sum- mary? data structure which makes possible the estimation of the answer. The key issue is that the data structure can be kept up to date with a small amount of effort. Our technique is based on using certain properties of hash functions. We also investigate the error bounds of the estimation and how one can improve those bounds.

The main contributions of our work are summarized as fol- lows:  1. We describe a generalization of implication aggregate queries that frequently arise in the data stream model of data processing and also in other fields of database research.

2. We provide memory and processing efficient algorithms for estimating such aggregates, within small error bounds (typi- cally less than 10% relative error).

3. We prove that the complement problem of estimating non- implication counts can be (?,?)-approximated under most conditions.

4. We extend online algorithms[22] that estimate frequent item- sets and prove that they cannot applied directly to the prob- lem of estimating implication counts.

5. We demonstrate the accuracy of our methods, through an extensive set of experiments on both synthetic and real datasets.

In Section 2 we review applications that can benefit from our approach. In Section 3 we describe formally and in detail the queries our algorithms can estimate. Section 4 explains how one can estimate answers to such queries, with bounded error estima- tion and also presents the memory and processing requirements.

Section 5 describes why existing algorithms that identify frequent itemsets over a given support cannot be applied directly to the problem of estimating implication counts. Section 6 evaluates ex- perimentally the accuracy of the proposed algorithms with both synthetic and real data sets. The related work is presented in sec- tion 7 and the conclusions are in section 8.

2. Applications The following applications can benefit from such implication  statistics. We briefly describe them and how the statistics can be applied to solve important problems in them.

Network traffic monitoring and characterization: Accurately measuring aggregate network traffic from one router to another is essential not only for monitoring purposes but also for traf- fic control (rerouting), accounting (pricing based on usage), security[24] (detecting denial of service attacks). Certain char- acteristics of the traffic like router bottlenecks or patterns of re- source consumption[13] or even flash crowds[20] and denial of service attacks can be modeled as implication queries. One can associate triggers when such implication counts exceed certain thresholds and could for example reroute traffic.

Approximate Dependencies: Functional dependencies have a very ?strict? meaning. They leave no room for exception. On the other hand, association rules are essentially probabilistic and al- low room for exceptions which is typical in large databases. Ap- proximate dependencies[21] attempt to bridge the gap by defin- ing functional dependencies that ?almost hold?. Such approx- imate dependencies can be ?validated? during updates or on a data-stream by conditions on the aggregate implication counts.

Multi-dimensional histograms or models: A fundamental problem in scenarios like query optimization or query approx- imation is creating an accurate and compact representation for a multi-dimensional dataset. Typical models used are histograms or probabilistic graph models[15], or other original approaches[6].

In [12] a methodology is proposed where the independence as- sumption between attributes is waived. The histogram synopsis is broken into one model that captures ?significant? correlation and independence patterns in data and a collection of low-dimensional histograms. Estimations of implication counts can be used in a preprocessing step to provide information about significant de- pendent or independent areas among certain attributes. These counts can then be used to more efficiently and accurately con- struct the model part of the synopsis.

3. Problem Definition In this section, we formally describe the problem and the nota-  tion. We assume that a node in a distributed environment receives  a stream of data and wants to maintain a series of statistics about various implicated attributes. More specifically, we are interested in approximating the answer to the general query, written in SQL- like format1 : ?select count (distinct A) from R where A implies B?, where R is a relation that models a data stream, A, B and C are sets of attributes(dimensions) of the relation R. We assume -without loss of generality- that A?B= /0.

In order to fully define the meaning of the predicate implies, we first introduce the notion of an itemset and then proceed by defining certain implications between itemsets. Then we proceed by defining the aggregation of such implications over a distributed environment.

3.1. Itemsets and definitions The projection of a single tuple ? of R on the attributes of A is  defined as an itemset a, and we denote: a = ?[A]. For example, in the case of the data set in Table 1 if A={Source,Destination} the itemset of the first tuple is {S1,D2}.

At any given moment the number of tuples seen so far is de- noted by T . The compound cardinality |A| of the set of attributes A is the product of the cardinalities of the attributes of A. In our ex- ample the compound cardinality is: |A|= 3 ?3 = 9, because there are three different sources and three different destinations.

For an itemset a of A and b of B, we denote as ?a,b? their association and we define the following:  Implication Set ?(a,B): An itemset a of A may appear with more than one itemsets of B. We define as implication set of an itemset a of A w.r.t. B, the set of different itemsets of B it appears with: ?(a,B) = {bi} ,(?[A] = a)? (?[B] = bi),? ? R.

The cardinality |?(a,B)| of the implication set is called multi- plicity of a w.r.t to B. For example, the itemset a = {S1,D3} of A={Source,Destination} has a multiplicity with B={Service}, |?(a,B)| = 2 since it appears with two different services (WWW and P2P).

Support ?(a): An itemset a of A is said to have support ? when it appears at ? tuples out of T . For example, itemset a = {S1,D3} of A={Source,Destination} has a support of four, since it appears in four tuples. In the literature (for example association rules) the minimum support is expressed in terms of a ratio over all the tuples. In the case of streams, which are potentially unbounded in size, we chose to define it in terms of an absolute number of tuples. Additionally the relative minimum support has some in- teresting side-effects that are discussed in Section 5.1.1.

Confidence Level ?(a,b): An itemset a of A and an item- set b of B have a confidence level ? = ?(a?b)?(a) , where ?(a? b) is the number of tuples where itemsets a and b appear together.

For example, the itemsets a = {S1,D3} and b = {WWW}, of A={Source,Destination} and B={Service} respectively, have an confidence level ?(a,b) = 2/4 since itemsets a and b appear to- gether in two tuples over the four tuples of itemset a.

Top-Confidence Level ?c(a,B), c ? |?(a,B)|: Assume the sequence of all the confidence levels of a, i.e the sequence of ?i = ?(a,bi), bi ? ?(a,B). We define as: ?c(a) =  1We are not trying to extend SQL but rather use it to describe the class of queries we are addressing      ?[topc( {  ?1,?2, ...,?|k(a,B)| }  )], where the topc(X) operator re- turns the c-biggest items in sequence X . This metric can be used to keep track of approximate one-to-c implications, where one itemset a of A appears with at-most c different itemsets of B in ?c(a,B) percent of the tuples where a appears. For exam- ple, for the itemset a = {P2P} of A={Service} and B={Source} the confidence levels with sources {S1,S2,S3} it appears with are: {2/4,1/4,1/4}. The top-confidence level for c = 2 is: ?2(a,B) = 2/4 + 1/4 = 75%. This means that P2P appears with at most c = 2 sources in 75% of all the tuples where P2P appears.

The top-confidence level with c = 3 in this specific case is 100%, i.e. service P2P appears with at most three different sources in all the tuples of P2P. Similarly the top-confidence level with c = 1 is 50%, i.e in half the tuples, P2P appears with only one source.

3.1.1. Implications between itemsets and attributes An implication of an itemset a of A to B, denoted by a? B, holds for a given maximum multiplicity K, a given minimum support ? and a given minimum top-confidence level ?c, when all of the following implication conditions are met:  1. Maximum Multiplicity K: |?(a,B)| ? K 2. Minimum Support ?: ?(a)? ? 3. Minimum top-Confidence Level ?c ?c(a,B)? ?c The cardinality S of the set of itemsets ai of A such that ai? B  is the implication count for the general query of Section 3. When an itemset ai satisfies implications conditions (1)-(3), then we say that the itemset contributes or participates in the implication count.

Note that by definition, we are interested in counting itemsets that satisfy the implication conditions throughout the life of the stream (we lift this restriction using sliding windows and incre- mental counts in Section 3.2). This has a direct effect on how the confidence level is interpreted: When an itemset satisfies the minimum support and maximum multiplicity but does not satisfy the minimum top-confidence level then we immediately discard that itemset from the implication count. It is possible that later in the stream, the same itemset may satisfy the top-confidence level condition. However, since the itemset at least once did not satisfy all the implication conditions then by definition we do not count its contribution to the implication count.

3.1.2. Example The above parameters describe a very flexible framework for fil- tering out noise and defining one-to-many implications.

Consider the network traffic data set described in Table 1 and assume that an analyst is interested in identifying how many ser- vices are being used at most two different sources 80% of the time. The user may also want to consider all services even if they appear for just one tuple but does not want to consider services that are being used at more than five sources.

The corresponding implication conditions are: Maximum multiplicity is set to five; a service that is being  used by more than five different sources does not contribute in the returned count.

Minimum Support is set to one; meaning that we take into ac- count services even if they appear in just one tuple of the dataset.

Top-Confidence Level is set to 80% for c = 2; That means that a service P that contributes in the implication count, appears with at-most c = 2 different sources in at-least 80% of all the tuples where P appears.

Let?s go over all services in Table 1 to see how the above pa- rameters affect the returned count. Service WWW appears in two tuples with only S1 and therefore participates in the count.

FTP appears in only one tuple (with S2) and also participates.

P2P appears in four tuples with three different stores. The con- fidence level of ?P2P,S1? is 2/4, of ?P2P,S2? is 1/4 and of ?P2P,S3? is 1/4. Therefore the top-confidence level for c = 2 is 2/4+1/4 = 75% and service P2P doesn?t satisfy the minimum top-confidence level condition. The returned count is two (for services WWW and FTP).

The minimum top-confidence level corresponds to the fact that the user needs to consider only the services that appear with at most c = 2 different stores. The value of 80% corresponds to the gravity of this constraint. If we change to minimum top- confidence level to 75% then P2P is valid and participates in the count.

The minimum support is used to filter out implications that hold for a very small fraction of the data set. For example, if the user increases the minimum support to two tuples, then the pair ?FT P,S2? is not valid since it appears in only one tuple.

3.2. Incremental and Sliding Queries  t 1 start ic  t 2 stream  Figure 1. Incremental maintenance  Our framework provides for implication counts given a refer- ence point in the stream where the counting begins and the im- plication conditions must hold w.r.t to that reference point. We relax that constraint by using two techniques. The incremental technique can answer queries like: How many new sources with some given implication conditions have appears in the last 1h.

The sliding window technique generalizes the incremental tech- nique and provides the support for more general aggregates like moving averages.

In this section we briefly describe the ideas behind the tech- niques -due to the paper size constraints- and we refer the reader to the full version of the paper. In Figure 1 we demonstrate a count (ic) at two points t1 and t2. In many cases the user is interested in the incremental implication count, which is the distinct count of new itemsets that appeared and satisfy the implication conditions  t 1 start ic  stream t 2  start ic? stop ic  Figure 2. Sliding Windows      between t1 and t2. This can be derived by ic(t2)? ic(t1).

On the other hand sliding queries where we want to retire old  implication counts or compare implication counts with different origins can by supported by maintaining a vector of implications counts with different origins and appropriately retiring old ones as depicted in Figure 2.

4. Algorithm for Implication Counts In this section we describe an algorithm that can be used to  efficiently estimate implication counts. The algorithm uses selec- tive sampling driven by hashing techniques and is based on ideas that are used to estimate the count of distinct elements on a stream or relation using limited memory and processing per data item.

We evaluate analytically the accuracy and describe techniques that can be used to increase the accuracy. Finally we describe the complexity requirements of the algorithm both for total space required and time per data item.

4.1. Counting Distinct Elements The presence of duplicates in the data can traditionally be han-  dled using sorting, sampling or using hash tables. Sorting and using hash tables do not scale well under both memory and time constraints, as those encountered in a streaming environment.

Sampling appears as an attractive alternative mechanism. Tak- ing a simple random sample and then extrapolating the answer, however may introduce an arbitrarily large error or require in- dexed access to the data[18]. The alternative is to approximate the answer using properties of hash functions [14, 26, 2, 5, 17].

4.1.1. Basic Probabilistic Counting One can probabilistically estimate[14, 2] the number of distinct itemsets in a large collection of data, denoted as zeroth-frequency moment F0, in a single pass using only a small additional stor- age of space complexity O(log |A|), where |A| is the compound cardinality of A.

The basic counting procedure assumes that we have a hash function that maps itemsets into integers uniformly distributed over the set of binary strings of length L. The function p(y) represents the position of the least significant 1-bit in the binary representation of y.

For each itemset ai that appears in the stream we keep track of the maximum pi = p(hash(ai)). Let?s consider an initially empty bitmap and define that an itemset ai is hashed in position pi of the bitmap. By assigning the value one to the corresponding bit in the bitmap, the maximum pi can then be determined by the position of the most-significant one bit in the bitmap. If the number of distinct elements in M is F0, then bm[0] (least significant bit) is accessed approximately F0/2 times, bm[1] approximately F0/4 times etc. This leads to Lemma 1.

Lemma 1 The expected number of values that hash in the cell i of the bitmap is F02i+1 , where i = 0 corresponds to the least significant bit of the bitmap.

At any given moment, bm[i] will almost certainly be zero if i? logF0 and one if i? logF0 with a fringe of zeros and ones for i? logF0. The position R of the leftmost zero value in the bitmap  is an estimator of logF0 with expected value: E(R) ? log(F0) [14, 2].

4.2. Counting Implications The basic probabilistic counting procedure can be extended in  a straightforward (but inapplicable) manner in order to count im- plications. The idea is that the basic procedure can be thought of as recording events. When we are counting distinct elements, the recorded event is the existence of an itemset that hashes in a cell of the bitmap and it is recorded by assigning one to the value of that cell. Note that we only record events and never erase them.

When counting implications, the recording event is the ex- istence of an itemset that satisfies the implication conditions.

Whenever we discover such an itemset we must assign the value of one to the corresponding cell. The problem is that we don?t know if an itemset will keep on satisfying the implication condi- tions in the future. However we can postpone the assignment of one to a cell for the time when the user requests the implication count.

We extend the cells of the bitmap so that we can store itemsets in them. When an itemset ai hashes in a cell, we keep track of all the itemsets of B it appears with, postponing the assignment of one or zero to the corresponding cell. When the user asks for the count of itemsets ai of A with the property ai? B, we check each cell to see if there is at least one ai such that ai? B and we assign a value of one to the corresponding cells. Then -as described in Section 4.1.1- the position of the leftmost zero is an estimator for the implication count.

One obvious optimization is that whenever we can determine that some itemset ai ?? B we can remove it from the cell. However the memory requirements of this algorithm is still O(K|A|), where K is the maximum multiplicity, since we must keep track of every single itemset ai and the K different itemsets of B it appears with.

4.3. Counting non-implications Assume that instead of counting the itemsets ai : ai ? B we  consider the complement problem of estimating the count of item- sets a?i : a?i ?? B. Let?s call this problem Non-implication Count- ing. More specifically an itemset ai has the property a?i ?? B with respect to the implication conditions in Section 3.1.1 when it sat- isfies the minimum support requirement but does not satisfy the maximum multiplicity or the minimum top-confidence level. In this section we describe how we can bound the required memory and still get an estimate of the non-implication count.

The recording event is the existence of an itemset a?i : a?i ?? B.

Unlike the case when counting implications , we can now assign the value of one to a cell as soon as we discover such an itemset.

Once an itemset does not satisfy the implication conditions we know that it will never satisfy them in the future. Below we define the fringe zone and we show that for all non-implication counts - except for very small counts- the size of the fringe zone is quite small.

We can observe in Figure 3 that the general format of the bitmap while performing the probabilistic counting has three ?zones?.

b,ai j  ......1 1 ...0 0 Zone?0  Fringe Zone  Zone?1  ? ?

bm[0]  p(hash(a ))i  Figure 3. Fringe Zone  The bitmap consists of cells bm[i], where the leftmost cell bm[0] is the least significant one. Zone-1 is always filled with ones -because we found an itemset a?i : a?i ?? B, while Zone-0 is al- ways filled with zeros -because the corresponding cells are empty- . Note that these are the only two cases where we can assign a value of one or zero to a cell.

The fringe zone lies between Zone-1 and Zone-0. In its bound- aries (at least) we cannot determine the existence of an itemset a?i, i.e. all itemsets in the zone so far imply B and therefore we must keep track of all the itemsets ai and the corresponding itemsets of B until we determine there is at least one a?i.

4.3.1. Size of the Fringe Zone In order to calculate how big the fringe zone is, lets assume that we are interested in estimating the non-implication count: a?i ?? B for some set of attributes A and B. Let F0(A) be the number of distinct elements of A and let S? be the non-implication count.

The size in cells of the fringe zone is quantified with very high probability by the following lemma:  Lemma 2 The size F of the fringe zone is: F = ? logq, where q = S?F0(A) .

Proof: This is a direct effect of the way the function p(hash(ai)) distributes itemsets ai of A to cells. We expect 2qF0(A)?1 itemsets a?i : a?i ?? B in the first (less significant) cell, 2qF0(A)?2 in the second etc. and therefore there are log(qF0(A)) such cells -that correspond to the Zone-1 of the bitmap-. The whole bitmap (ignoring Zone-0 ) holds log(F0(A)) cells and there- fore the size of the fringe zone is log(F0(A))? log(qF0(A)) = ? logq.

This observation demonstrates that the size of the fringe zone is quite small for almost all non-implication counts and that it logarithmically increases when the ratio q? 0. For example, all non-implication counts greater than 1/16 of F0(A) correspond to a fringe zone of only four cells.

We must point out that the bounds given in Lemma 2 are very pessimistic and that the size of the fringe zone is actually smaller.

For example in the case where all distinct elements satisfy the implication condition (q = 0) then the counting procedure de- generates to the basic probabilistic counting described in [14], where the fringe zone size is quantified with high probability by O(log logF0(A)).

4.3.2. Bounding the size of the Fringe Zone This gives rise to the idea of bounding the size of the fringe zone to a specific size. This limits the amount of itemsets we must keep in memory.

In section 4.1.1 we mention that the index of the cell in the bitmap is determined by function p(hash(ai)), which represents the position of the least significant 1-bit in the binary represen- tation of hash(ai). From Lemma 1 we know that as we move to higher-order bits the number of ai?s that get hashed in the corre- sponding cells decreases exponentially. For example, if the num- ber of distinct ai?s is 128, we know that about 64 will get hashed to the first -from left to right- cell, 32 to the second, ..., 2 to the 6th and only 1 to the 7th cell. Note that this distribution is the same regardless the original distribution of ai values or the frequency that ai?s appear. In [2], there is a discussion about using linear hash functions in order to accomplish that.

Assume that we arbitrarily choose to define that the fringe zone has a fixed size of four cells. We expect that in the rightmost cell of the fringe zone only one ai will get hashed in, in the immediate left cell two, ..., and to the leftmost cell of the fringe we expect 23 = 8 different ai?s. We keep track of every single itemset ai that gets hashed in the cells of the fringe zone, as well as all the itemsets of B there itemsets appear with. This allows us to check if there is at least one a?i : a?i ?? B in a cell and therefore assign it a value of one. If this happens for the leftmost cell in the fringe zone, then we ?float? the fringe zone to the right, by increasing the size of Zone-1 by one. By ?limiting? the size of the fringe to four cells we bound the amount of memory required to make a decision. Note that for each different itemset that hashes in a cell we need to keep at most K different itemsets of B it appears with.

Therefore, in our case where the fringe has a size of four cells, we need at most (20 +21 + ...23)?K (i.e O(K)) itemsets to be stored in the corresponding cells. Note that the actual memory required is much less since we can free all the memory required by cells in the fringe that have been assigned a value of one.

We can also double the allocated memory (keeping the asymp- totic requirements unaffected) to accommodate deviations from the expected distributions due to inefficiencies of the hash func- tion.

4.3.3. Estimation Error due to Fringe Size Fixation When there is not enough space in a cell to accommodate an item- set ai we arbitrarily assign a value of one to the cell and shift the floating fringe to the right. This can happen under two different situations. In the first one, it just happens to hash an itemset to a cell in the fringe zone that already accommodates the expected number of itemsets. In the second situation, an itemset is hashed to Zone-0 and the fringe zone must float to the right to accom- modate that itemset. Remember that -by definition- the rightmost cell of the fringe is always the rightmost cell where an itemset has been hashed. As the fringe floats to the right, the leftmost cells of the fringe now belong to Zone-1. This step is that ?fixates? the length of the fringe zone. The only effect that it has, is the intro- duction of an error for small non-implication counts that cannot be ?managed? by the chosen fringe zone size.

Note that no error is introduced when the fringe zone has a size of at least F = ? log(q). By limiting the size of the fringe, we essentially limit the minimum non-implication count we can estimate. If the fringe size is F then the minimum      non-implication count we can estimate with the basic probabilis- tic counting algorithm is 2?F ?F0(A). Smaller non-implication counts are ?mapped? to that specific value.

For example, with a fringe size F = 4 we can estimate an implication count accurately if that count is bigger than than 6.25% ? F0(A), (2?4 ? 6.25%). Without changing the asymp- totic memory requirements one can increase the size of the fringe to eight, in order to estimate accurately very small counts, > 0.4% ?F0(A), (2?8? 0.4%). Smaller counts than that, are mapped to the same value: 2?8 ?F0(A).

4.3.4. Tracking Non-Implication Conditions In this section we describe how we can keep track if an itemset a?i a?i ?? B given the implication conditions.

For each itemset ai that hashes in the fringe zone we keep track of all itemsets of B it appears with. Therefore the implication count ?(ai) is known at any moment. The support ?(ai) of that itemset can be represented by a counter that is increased every time itemset ai is hashed in the cell. For the confidence level ?(ai,b) with an itemset b of B we use a counter that represents the support ?(ai?b). Every time the itemset ai appears with b in the stream we increase the corresponding counter.

At any moment the corresponding confidence level is: ?(ai,b) = ?(ai?b)?(ai) , which can be determined just by dividing the corresponding counters. The top-confidence level of an itemset ai can therefore be determined by summing the biggest ?(ai,b) at any given moment.

Whenever an itemset ai satisfies the minimum support condi- tion but does not satisfy the rest of the implication conditions we assign a value of one to the corresponding cell.

4.4. Deriving Implication Counts So far we have described how to get an estimate of the non-  implication count S? of A to B. The implication count can be de- rived by subtracting the non-implication count from the distinct count Fsup0 (A) of itemsets ai ? A that satisfy the minimum support requirement, i.e: S = Fsup0 (A)? S? .

We can have an estimate of distinct count Fsup0 (A) without us- ing any additional memory from the bitmap used to estimate S? , by virtually assigning a value of one to each cell in the fringe zone where at least one itemset of ai, that meets the minimum support condition, is hashed in. The cells in Zone-1 by definition have at least one itemset that satisfies the minimum support condition.

4.5. Algorithm NIPS/CI Algorithm 1 referred to as NIPS (Non-Implication Probabilis-  tic Sampling) gives the complete algorithm using a floating fringe for performing the probabilistic sampling for non-implication counts with given implication conditions. This algorithm is de- signed to sample a small O(K) number of pairs (ai,b j), based on the hash representation of ai over a data stream. Any time a tuple arrives, the bitmap is updated accordingly.

In line 2 we project the tuple to the attributes of A and B respec- tively. Then we calculate the position i of the cell where itemset a is hashed. If the position i is in Zone-0 -right of the fringe zone, where no item has been hashed yet- then we ?float? the fringe  Algorithm 1 NIPS: Non-Implication Probabilistic Sampling Input: M:stream of tuples Input: A,B: set of attributes of M Input: K,?,?C: Implication Conditions State: bm: bitmap of L cells 1: for each tuple t do 2: a? t[A];b? t[B];i? p(hash(a)) 3: if i in Zone-0 then 4: float fringe by making i its rightmost cell 5: end if 6: if i in fringe zone and bm[i].value=0 then 7: bm[i].supp[(a,b)]? bm[i].supp[(a,b)]+1 8: bm[i].supp[a]? bm[i].supp[a]+1 9: curConf? ?C  10: if bm[i].supp[a]> ? then  11: curConf? Sum[ topc bm[i].supp[(a,b)] ]bm[i].supp[a] 12: end if 13: if (curConf < ?C)  or (bm[i].overflowed) then 14: bm[i].value=1 15: free all the memory allocated for the cell bm[i] 16: if bm[i] is the leftmost in the fringe then 17: float fringe one cell to the right 18: end if 19: end if 20: end if 21: end for  zone to the right by making position i its rightmost cell. In the process of floating, the leftmost cells of the fringe zone that be- come part of Zone-1 are cleared of all itemsets inside and are set to value 1. As explained in Section 4.3.3, this process introduces an error only when counting very small non-implications and the size of the fringe zone is not appropriately set. In lines 7 and 8 the counters that represent the current support of the itemsets a and a?b are increased. Line 11 calculates the top-confidence level of itemset a. In lines 14 to 17 a cell is assigned the value one, if we have found an itemset a that either does not imply B or if there is no room in the corresponding cell. Additionally the fringe zone is shifted to the right if necessary.

Algorithm 2 CI:Counting Implications Input: K,?,?C: Same implication conditions used in NIPS Input: bm: bitmap of L cells used in NIPS 1: RFsup0 (A)? 0 2: while exists ai in bm[RFsup0 (A)] s.t.

supp[ai] > K and RFsup0 (A) < L do 3: RFsup0 (A)? RFsup0 (A) +1 4: end while 5: RS? ? 0 6: while bm[RS? ].value=1 and RS? < L do 7: RS? ? RS? +1 8: end while 9: return 2  RFsup0 (A) ?2RS?  Algorithm 2 referred to as CI (Counting Implications) returns an estimate of the implication count S and is designed to work with the bitmap used in (and in parallel with) algorithm NIPS.

Whenever the user wants an estimate of the current implication count she runs CI on the bitmap of NIPS.

Lines 1 to 3 find the position RFsup0 (A) that corresponds to the number of distinct elements Fsup0 (A) that satisfy the minimum      support requirement. Lines 5 to 7 similarly locate the position RS? that corresponds to the non-implication count. Line 9 returns the estimate of the implication count as described in Section 4.4.

4.6. Space and Time Complexities  The basic probabilistic counting algorithm has a space com- plexity (in bits) of O(log |A|) where |A| is the compound cardinal- ity of A.

In order to estimate the implication count S , Algorithm NIPS , in addition to the memory O(log log |A|) required for the counter for Zone-1 and the O(log |A|) memory required for the hash func- tion, requires enough memory to accommodate all the counters for the pairs (a,b) that hash in the cells of the fringe zone. The distinct number of a?s that hash in the fringe zone is bounded by the fringe zone (for example for a fringe size of eight we ex- pect about ?7i=0 2i = 255 different a?s). We can actually double or even triple the expected number of a?s, without affecting the asymptotic complexities, in order to accommodate more a?s due to inefficiencies of the hash function.

The number of distinct b?s that corresponds to each a is bounded by the maximum multiplicity K, i.e. there are at most K different such b?s. The number of counters for each cell then is O(K). In general we need O(logT ) bits to represent each counter, where T is the number of tuples of the dataset or the data stream.

Therefore the total space (in bits) complexity of algorithm 1 is: O(K ? logT + log log |A|+ log |A|) ,where K is the maximum mul- tiplicity, T is the number of tuples and |A| is the compound cardi- nality of A. We do not include the 2F term since the size F of the fringe zone is fixed and usually a value of four is sufficient to esti- mate very large implication counts as described in Section 4.3.3.

By using hash tables to locate a counter in a cell given a pair (a,b) or an a, and a priority queue to handle the topc operator, c ? K counters for a cell, the time complexity of the algorithm per data item is: O(K ? logK)  The number of entries (ai,b j) that NIPS holds in memory is bounded by the fringe zone size and the maximum multiplicity condition. For example, for F = 4, the number of entries in the bitmap is at most 15 ?K. The above complexities demonstrate the scalability potential of algorithm NIPS. One can estimate accu- rately any implication counts for arbitrarily big attribute cardinal- ities or number of tuples.

4.7. Approximation  In this section we discuss how an algorithm approximates a value and we show how existing techniques can be used in order to get more accurate results based on algorithm NIPS/CI.

A probabilistic algorithm (?,?)-approximates a value A if it outputs a value A? such that: P[  ??A? A??? ? ?A] ? 1? ? The pa- rameters ?, ? are called approximation parameter and confidence parameter respectively. For example, if a user requests ? = 10% and ? = 1% then the algorithm should return an estimate A? that is at most 10% relatively off the actual value A with probability at least 99%.

4.7.1. Approximating Non-Implication Counts The basic probabilistic algorithm is shown[2] to approximate the number of distinct elements F0 (zeroth frequency moment) in a different manner: P[ 1c ? F?0F0 ? c] ?  c?2 c ,?c > 2 by using a linear  hash function. In [5] techniques are presented that can be used to (?,?)-approximate F0 based on the algorithm in Section 4.1.1.

NIPS approximates the non-implication count in exactly the same manner with the basic probabilistic algorithm under the con- dition that the non-implication count is large enough for the cho- sen size of the fringe zone. The same techniques used in [5] can be applied to get an (?,?)-approximation of the non-implication count.

4.7.2. Approximating Implication Counts The implication count is determined by subtracting two (?,?)- approximations, namely the number of distinct itemsets, that sat- isfy the minimum support condition, and the non-implication count. This operation however does not maintain (?,?)- approximation, since the relative error can grow arbitrarily large, when the non-implication count is very close to the number of dis- tinct elements. This essentially means that the relative error for very small implication counts (close to zero) can be unbounded.

For a pragmatic approach however this is not an issue. We have already made the assumption that the user is not interested in very small non-implication counts in order to fixate the size of the fringe zone. We can make the assumption that the user is not interested in very small implication counts (very close zero) as well and in the experiments section we demonstrate that for a wide range of implication counts the estimates returned by algo- rithm 2 are very accurate.

5. Using Frequent Itemsets In this section we extend the algorithms ?Lossy Counting? and  ?Sticky Sampling? introduced in [22] so that they identify item- sets that satisfy given implication conditions. Then we discuss the advantages and disadvantages compared to NIPS/CI and point out why they cannot be applied successfully to the problem of es- timating implication counts.

5.1. Implication Lossy Counting The Implication Lossy Counting(ILC) algorithm is determinis-  tic and requires at most: K? log(?T )(see [22]) sampling entries in order to compute an (?,?)-synopsis2, that can be used to iden- tify the implicated itemsets, for given implications conditions K,?rel ,?. It is important to point out that the minimum support condition is required to be specified relatively to the current num- ber of tuples T in the stream, and that the approximation parame- ter ? must satisfy: ?? ?rel . These requirements have some very interesting side-effects discussed in Section 5.1.1.

The stream is conceptually divided into buckets of width w = ? 1? ?. The current bucket is denoted by bcurrent . The algorithm samples entries of the form [ai,support,?] and [(ai,b j),support,?], where ? is the maximum possible error in the support. For each pair (ai,b j) that arrives in the stream we check if there is an  2? = 1 in this case      entry for ai and if it is we update the support of both ai and (ai,b j). Otherwise we create two new entries [ai,1,bcurrent -1] and [(ai,b j),1,bcurrent ]. The supports of the itemset ai and the pairs (ai,b j) allow us to check if the itemset ai satisfies the implica- tion conditions, as explained in Section 4.3.4. If we determine that an itemset ai satisfies the minimum support requirement but not one of the remaining implication conditions then we mark the corresponding sample entry as dirty and delete all the pair entries for that itemset ai. At bucket boundaries we prune all non-dirty entries of the form [ai,support,?] where support + ? ? bcurrent .

For each non-dirty itemset ai that is deleted we also remove the corresponding entries [(ai,b j),support,?]. When the user re- quests the implicated itemsets we output all non-dirty ai?s with support? (?rel? ?)T .

Algorithm ILC has two differences with the original Lossy Counting algorithm, first we sample supports and errors for both itemsets and pairs of itemsets and second we mark an itemset as dirty -and remove all the corresponding pairs from the samples- as soon as we determine that the itemset does not satisfy the im- plication conditions.

It is possible to make the same modifications to the ?Sticky Sampling?[22] algorithm in order to identify implicated itemsets, but the issue with the relative minimum support remains.

5.1.1. Relative Minimum Support Issues The ILC algorithm returns the actual itemsets that satisfy any given implication conditions and not just their count. Although the complexity appear quite small (actually, in [22] it is shown that the required memory is much less than the worst case, which corresponds to a rather pathological situation), it does not tell the whole truth.

More specifically, one problem is that every single itemset that satisfies the minimum support ?rel has to stay in memory (marked dirty even if it doesn?t satisfy the rest of the implication condi- tions). This property, although desirable for certain applications, limits the applicability of ILC due to the amount of memory re- quired. In the worst case the number of entries that need to be sampled is in the order of the number of different itemsets in the stream. For conditional implications, the compound cardinality of the participating attributes can be quite high. The only way to limit the memory used, is by increasing the ?rel minimum support condition, which implies that the contribution of many implica- tions that hold for a smaller amount of tuples is totally lost (see [25] for an example application).

The major problem however, is the relative nature of the min- imum support itself. As the stream evolves, the number of tuples that correspond to the given implication conditions implicitly in- creases. The side-effect is that the contribution of ?small? im- plications to the implication count is lost, although these specific implication may hold for a quite large (and continuously increas- ing) number of tuples.

The relative nature of the minimum support in the ILC algo- rithm cannot be removed since the approximation parameter ? must remain constant (the size of the buckets is a function of ?) and satisfy: ?? ?rel , throughout the execution of the algorithm.

The same holds for the extended sticky sampling algorithm.

On the other hand the NIPS/CI algorithm returns an accurate  estimation of the implication count by keeping only O(K) sam- ples in memory, regardless of how small the minimum support requirement is, capturing the cumulative effect of small implica- tions throughout the life of the stream.

6. Experiments In this section, we present an extensive empirical study on the  accuracy of the estimation for implication counts. The first sec- tion demonstrates the results using artificially generated datasets, while the second section describes the result obtained from real- world datasets. For the synthetic datasets we imposed implica- tion patterns ?of known count? on the datasets and used both the bounded and the unbounded fringe estimator to estimate the strength of the correlations and report the relative error and the deviation. For the real datasets, we used an exact method (based on hash tables) for calculating the implication count and com- pared with the estimation of both the bounded and the unbounded version of our estimator.

6.1. Synthetic Dataset One We conducted a series of experiments to verify the error  bounds of NIPS/CI with a fixed fringe size of four. We used a varying cardinality for attribute A and the imposed implications had a variable count between 10% and 90% of |A|, for various one-to-c implications, where c = 1,2,4. We increased the ac- curacy of our estimation to approximately 10% using stohastic averaging([14]). More specifically for the accuracy of 10%, we used 64 bitmaps with a fringe zone of size four (i.e. there was available space for 1920 itemsets in memory).

We chose a minimum top-confidence level ? of 90% while the itemsets a?s that should participate in the count, were imposed to have a top-confidence level of 92%. The chosen minimum sup- port was 50 tuples. The maximum multiplicity was chosen to be equal to c.

To test the accuracy of the algorithms with respect to the im- plication conditions we also imposed a ?noise?. Some itemsets ai where created in a way that breaks at least one implication condi- tion and therefore they should not participate in the count.

For example, itemsets that did not participate because of the maximum multiplicity condition were imposed to appear with a number u of different itemsets of B that was uniformly distributed as: c+1? u? c+10.

Each combination of these parameters was tested one hundred times. The experiments were performed by generating random numbers using a random number generator to simulate the item- sets. Specifically the experiments were organized as follows: Pick a cardinality size |A|, an implication count S and a c. Generate S different itemsets ai. For each ai create at most c (uniformly dis- tributed in the range [1,c]) different b j. For each combination (ai,b j) write 50 tuples. Then for each ai create four b?j different than all b j?s created before. And write the four tuples (ai,b?j).

This step creates S itemsets ai such that ai? B with a minimum support of 54 tuples and a top-confidence level of 50/54 = 92%      10 20 30 40 50 60 70 80 90 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  100 200 300 400 500 600 700 800 900 Implication Count  0.06  0.08  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  10 20 30 40 50 60 70 80 90 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  100 200 300 400 500 600 700 800 900 Implication Count  0.06  0.08  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  |A|= 100 |A|= 1,000 |A|= 100 |A|= 1,000  1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  20000 40000 60000 80000 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  1000 2000 3000 4000 5000 6000 7000 8000 9000 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  20000 40000 60000 80000 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  |A|= 10,000 |A|= 100,000 |A|= 10,000 |A|= 100,000  Figure 4. DataSet One with c = 1 Figure 5, DataSet One with c = 2  and therefore these itemsets participate in the implication count.

The number of tuples created by this step is: S ?50((c+1)/2+4).

The rest of the steps create itemsets that should not participate in the count. We create three different kind of tuples that break one implication condition. The relative weight of each kind is 1/3.

Generate (|A| ? S)/3 pairs (ai,b j) where each ai is different than all itemsets of A created before. As in the previous step, for each ai create at most c different b j. For each combination (ai,b j) write 50 tuples. Then for each ai create eight b?j different than all b j?s created before. And write the eight tuples (ai,b?j).

This step creates (|A| ? S)/3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum top-confidence level, although they satisfy both the minimum support and the maximum multiplicity constraint. The number of tuples created by this step is: (|A| ? S)/3 ? 50((c + 1)/2+8).

Generate (|A| ? S)/3 pairs (ai,b j) where each ai is different than all itemsets of A previously generated and each one appears with u different b j, where: c+1? u? c+10. Write 50 such tu- ples. This step creates (|A|?S)/3 new itemsets of A that should not participate in the implication count because they do not satisfy the maximum multiplicity condition. The number of tuples cre- ated by this step is: (|A|?S)/3 ?50(c+5.5) Generate (|A|?S)/3 pairs (ai,b j) where each ai is different than all itemsets of A pre- viously generated. For each pair (ai,b j) write 40 tuples. This step creates (|A|? S)/3 new itemsets of A that should not participate in the implication count because they do not satisfy the minimum support requirement. The number of tuples created by this step is: (|A|?S)/3 ?40.

Shuffle the output file. This step just demonstrates that the operation of the algorithm is independent to the ordering of the tuples. Estimate the implication count using algorithm NIPS/CI with a fringe size of four and also without a bounded fringe. Per- form one hundred such experiments and calculate the mean and the standard deviation of both estimations.

The total number of tuples for each experiment can be de-  rived by adding the partial number of tuples created in each step. For example, for |A| = 10000, S = 5000 and c = 4 the average number of tuples for the corresponding experiment was ? 3,108,333. A minimum support of 50 tuples for this case cor- responds to only ? .001% of the tuples, demonstrating that in the implication count contribute even implications that hold for a very small number of tuples. Figures 4,5 and 6 show the re- sults for c = 1,2,4, for varying cardinalities |A|. The x-axis cor- responds to the actual implication count of the dataset as that was imposed by the creation process. The y-axis denotes the mean rel- ative error as it is calculated by running one hundred experiments.

We used the following formula to estimate the mean relative er- ror: relative error = |Actual S?Measured S |Actual S . Graphs ?Bounded Fringe? express the experimental results for the case of a fringe with size F = 4, while graphs ?Unbounded Fringe? demonstrate the result for the case of an arbitrarily large fringe. The error bars correspond to the statistical deviation of the mean error as that was computed by one hundred such experiments.The deviation is generally negligible, which means that the error of the estimated S is always very close to the mean error. We also observe that the difference between the estimation using a bounded fringe of size four and a unbounded one, is negligible for a very wide range of implication counts and therefore a size of four for the fringe zone is sufficient to provide very accurate results for most applications.

10 20 30 40 50 60 70 80 90 Implication Count  0.05  0.06  0.07  0.08  0.09  0.1  M ea  n E  rr or  Bounded Fringe Unbounded Fringe  Figure 6. Dataset One with c = 4  6.2. Real-world datasets & Algorithmic comparison We compare our estimates with the results taken using Dis-  tinct Sampling (DS)[17] which has been shown provide highly-      accurate estimates for distinct value queries and event reports.

This algorithm outperforms other estimators that are based on uniform sampling[8, 9] even when using much less sample space.

We also provide comparison with our Implication Lossy Count- ing (ILC) algorithm (described in Section 5.1) which is based in the Lossy Counting algorithm introduced in [22].

For this series of experiments we used a real dataset of eight dimensions which was given to us by an OLAP company, whose name we cannot disclose due to our agreement. The cardinalities of the dimensions are presented in Table 3. The parameters of the algorithms are presented in Table 5. For NIPS/CI, we used 64 concurrent bitmaps with a fringe size of four thus requiring mem- ory enough to hold (24? 1) ? 64 ?K = 1920 itemsets. We expect that the ?averaging?([5, 14]) over these many bitmaps will result in an error less than 10%. We used the exact same sample space for DS. The bound parameter t for DS was set to ?1920/50? fol- lowing the suggestion in [17]. For ILC we used an approximation parameter ? = 0.01 which increases the memory requirements of ILC relative to those of NIPS/CI or DS. On the average, ILC used more than twice the memory that NIPS/CI and DS used. For ex- ample for the experiment in Figure 7(B) it used more than 8,000 entries.

We evaluate the results of the algorithms with respect to the number of tuples, the cardinality of the participating dimen- sions and the implication conditions. To simulate a real data stream scenario we ?tracked? the conditional implication counts of A ?B ?E ? F and the unconditional B? E using the afore- mentioned algorithms. The first workload corresponds to quite large compound cardinality while the second to very moderate cardinalities. Table 4 presents the actual aggregates for various instances of the stream for ? = 5 and ?1 = 60%. We believe that most workloads fall somewhere in the middle with respect to the complexity of the wanted implications and the size of the returned counts.

Figure 7(A) depicts the relative error as the stream evolves for workload A, using the algorithms DS, NIPS/CI and ILC for differ- ent implication parameters. In Figure 7(A)(a) we show the results for minimum support ? = 5 and ?1 = 60% or ?1 = 80%. The different ?1 are encoded in the parentheses next to identification of the algorithm in the legend of the graph. In Figure 7(A)(b) we increased the minimum support to ? = 50. We observe that the behavior of DS varies widely while NIPS/CI remains always be- low the expected 10% error. DS actually keeps a sample of the distinct elements seen so far and tries to ?scale? the implication count that holds for that sample to the whole set of distinct ele- ments. In most cases the data in the sample is not representative of the implication. The situation for DS is exacerbated when the minimum support increases, where quite a lot of samples do not participate in the count making the ?scaling? even more error- prone. Algorithm ILC in all cases returned very erroneous results although it used much more space than NIPS/CI and DS, since it tries to store not the implication counts but the actual implicated itemsets. In these workload the implicated itemsets overwhelm its available memory (which is actually larger than the amount given to NIPS/CI and DS).

In figure 7(B) we present the results of the algorithms for work- load B. The situation is still in favor of NIPS/CI whose relative error remains always close to the expected 10% unlike DS who returns highly skewed errors even though the domain cardinalities are much smaller and therefore keeps in the sample space much more data. As expected from the analysis the error guarantees of NIPS/CI are virtually unaffected by changes in the cardinalities or the number of tuples seen so far in the stream. ILC returns very erroneous results although now the cardinalities and the im- plicated items are much smaller compared to those of workload A. The reason is not only because it keeps too much information in memory (i.e. all the implicated itemsets) (while both NIPS/CI and DS only hold a ?mantissa? for the count) but also because the constraint ?? ?rel is broken as the number of tuples increases.

7. Related Work  There are unique challenges in query processing for the data stream model. Most challenges are the result of the streams being potentially unbounded in size. Therefore the amount of the stor- age required in order to get an exact size may also grow out of bounds. Another equally important issue is the timely query re- sponse required although the volumes of data the need to be pro- cessed is continually augmented at a very high rate. Essentially the amount of computation per data item received should not add a lot of latency to each item. Otherwise any such algorithm wont be able to keep up with the data stream. In many cases access- ing secondary storage ?such as disks? is not even an option. In [3] there is a discussion of what queries can be answered exactly using bounded memory and queries that must be approximated unless disk access is allowed. Sketching techniques([14, 2]) have been introduced to build summaries of data in order to estimate the number F0 of distinct elements in a dataset. In [5] three al- gorithms that (?,?) approximate the F0 are described with vari- ous space and time requirements. Distinct Sampling[17] is driven by hashing functions similar to those studied in [14, 2] and pro- vides highly accurate results for distinct value queries compared to those taken by uniform sampling by using only a fraction of their sample size. In [22] the algorithms ?Sticky Sampling? and ?Lossy Counting? are introduced that estimate frequency counts with application to association rules and iceberg cubes. In [16] a framework for performing set expression on continuously up- dated streams based on sketching techniques is presented. In [27] a general framework over multiple granularities is presented for both range-temporal and spatio-temporal aggregations. In [10] a framework for identifying ?hierarchical heavy hitters?, i.e. hier- archical objects (like network addresses whose prefixes defines a hierarchy) with a frequency above a given threshold, is de- scribed. The effect of impications between columns has been emphasized in the CORDS[19] system that identifies correlated pairs of columns and soft-dependencies and has been proved very useful in query optimization.

8. Conclusions We have presented a generalized and parameterized framework  that can accurately and efficiently estimate implication counts and can be applied to many scenarios. To the best of our knowledge, this is the first practical and truly scalable approach to the problem      Dimension Cardinality A 1557 B 2669 C 2 D 2 E 3363 F 131 G 660 H 693  Table 3. Cardinalities  Workload A Workload B Tuples A?B?E? G E? B  134,576 608 50 672,771 12,787 125  1,344,591 34,816 152 2,690,181 84,190 165 4,035,475 132,161 182 5,381,203 187,584 188  Table 4. Impl. counts w.r.t tuples  NIPS/CI #bitmaps 64 NIPS/CI K 2  DS sample size 1920 DS bound t 39  ILC ? 0.01  Table 5. Algorithm Parameters  0 1 M 2 M 3 M 4 M 5 M 6 M  #Tuples            R el  at iv  e E  rr or  (% )  NIPS/CI (.6) NIPS/CI (.8) DS (.6) DS (.8) ILC (.6) ILC (.8)  0 1 M 2 M 3 M 4 M 5 M 6 M  #Tuples            R el  at iv  e E  rr or  (% )  NIPS/CI (.6) NIPS/CI (.8) DS (.6) DS (.8) ILC (.6) ILS (.8)  0 1 M 2 M 3 M 4 M 5 M 6 M  #Tuples            R el  at iv  e E  rr or  (% )  NIPS/CI (.6) NIPS/CI (.8) DS (.6) DS (.8) ILC (.6) ILC (.8)  0 1 M 2 M 3 M 4 M 5 M 6 M  #Tuples            R el  at iv  e E  rr or  (% )  NIPS/CI (.6) NIPS/CI (.8) DS (.6) DS (.8) ILC (.6) ILC (.8)  (a) ? = 5 (b) ? = 50 (a) ? = 5 (b) ? = 50  (A) Workload A (B) Workload B Figure 7. Relative Error vs stream size  of online estimation ?within small errors? of complex impli- cation (and non-implication) counts between attributes of a data stream under severe memory and processing constraints and even in the presence of noise. We prove that the complement problem of estimating non-implication counts can be (?,?) approximated, when the size of the fringe zone is fixed appropriately. We demon- strate that existing algorithms for estimating frequent itemsets or sampling cannot be applied to the problem since they lose the cumulative effect of small implications. In addition, through an extensive set of experiments on both synthetic and real data, we have shown that NIPS/CI always remains very close to the actual implication count, capturing even very small implications whose total contribution is significant.

