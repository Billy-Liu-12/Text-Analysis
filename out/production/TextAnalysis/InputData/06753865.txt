Safer@Home Analytics: a Big Data Analytical  Solution for Smart Homes

Abstract?The vast amounts of data generated from sensors in smart homes, can give valuable insights about social and behavioral patters on households and their residents. The goal of the project is investigation & implementation of mechanisms to capture/store vast continuous streams of time-series data from optical movement sensors, analyze & mine for anomalies/changes enabling preventive care with mechanisms for presentation/visualization of meaningful information to target user groups (next of kin, care providers, professional services), while ensuring that the privacy of participants are preserved.

Keywords?big data; data analysis; data mining; aging-in- place; smart homes

I. INTRODUCTION The demographic trend across the world is moving towards  an increasing pool of older population and is expected to double in coming years. The concept of aging-in-place (AIP) is creating an environment using modern ICT technologies, to enable elderly individuals lead an autonomous life in the comforts of their homes, reducing cost involvement, resource requirements and maintaining self-esteem, independence, care.

In order to maintain, improve the standard of healthcare services and quality of help, such technologies [1]?[5] would play a crucial role. Traditional healthcare services to residential homes could be extended using sensor networks supported by data analytics to deliver assistive services. One specific initiative being, the Safer@Home [6] project at the University of Stavanger, that aims at creating such an environment to preserve personal control, dignity and quality of life, through development of smart integrated systems offering AIP services.

The smart home concept developed through the Safer@Home project includes twenty model homes for participating elderly, furnished with a network of sensors. In order to provide appropriate services through data analytics, sensor data has to be collected centrally to effectively perform knowledge discovery algorithms. The need of real time as well as historical analysis leads to special infrastructure/processing requirements of volume, velocity and variety beyond the scope of existing data management solutions [7]?[9]. Big Data solutions like Hadoop [10], provides a distributed scalable storage and processing framework for handling frequent and large amounts of unstructured data. Also the nature of data, being extremely personal & sensitive could disclose the complete living behavior of individuals. It is necessary that the  privacy of the individuals be protected while providing the needed care.

Our Contribution: The aim of this project is to establish an analytical solution focusing on decision-making towards life- style/health of elderly people, with need of being highly responsive, available, scalable, secure and data intensive. A framework for securely, scalability collecting and storing data from smart homes, a statistical analytical platform for data intensive processing and a model for preserving privacy of shared results is to be established. Through a prototype case study, the capacity of the solution for preventive care would be demonstrated based on non-invasive movement sensor data.

Paper Structure: An overview of related works is given in section II. Background of different technical solutions used for our framework is provided in section III. The research methodology is discussed in section IV. The research areas and expected contributions are in section V. Section VI concludes the paper.



II. RELATED WORKS Smart home systems provides unique data analysis and  mining opportunities to identify regular user activities and patterns to help maintain independence and quality of life for an aging population. There have been numerous general research in this area along with their benefits [1], [2]. In this section, the focus is upon some of the projects that analyses data gathered from different sensors (context aware) and/or similar past activities (case based reasoning) to provide intelligent services.

Georgia Tech through their Aware House [3] introduced a ?track-stitching? algorithm for measuring activities in an environment based on data collected from RFID tags, optical and pressure sensors. They extended upon behavior recognition, through inferences drawn from information about various activities, interaction with objects and locations. The Independent Lifestyle Assisting (ILSA) and the MavHome project introduced a behavior-learning model based on sequential patterns [1], [4], to predict activities of residents of a smart home. The CASAS Smart Home Project [11] as well as the Smart Hospital Project [12] used hidden Markov probabilistic models, to recognize activities in complex situation where multiple individuals within the same environment were performing parallel activities. The University of Washington Assistive Cognition [13] (ACCESS)   DOI 10.1109/CloudCom.2013.129     project investigated machine learning and ubiquitous computing algorithms to support intelligent way finding systems based on travel patterns. It included modules to create user profiles, transportation plan learner and significant place identifier to predict likely destinations and detect unusual variations. MIT Intelligent Home [14] presented a predictive framework for location-aware resource optimization in smart homes. The Neural Network House [5] from the University of Colorado based on neural nets, focused upon providing adaptive control of home environments to anticipate needs of its inhabitants.

Although there have been major contributions towards analyzing data from smart homes, they remain a lab-based initiative. The use of traditional RDBMS solutions for storage and processing, constrains their practical applicability and scalability. In a real world scenario, sensor data from smart homes would generate enormous and continuous streams of data beyond the capabilities of existing data management solutions. Further the research focus was more towards improvement of services offered through smart homes, rather than about preventive care where data from sensors can provide insights about health/lifestyle anomalies. Our work addresses a complete different dimension of establishing preventive care through a data intensive solution. The goal is to create an analytical solution for privacy preserving data analytics. We propose a framework for securely collecting and storing sensor time-series data and a platform for statistical analysis. Analyzing movement data of a household to recognize behavioral patterns, detect incidents and realize syndromes at early stages can enable preventive care. In this respect, our work is novel and different from existing research, where opportunities for preventive care through smart homes utilizing new No-SQL based solutions is validated.



III. TECHNOLOGY OVERVIEW The various technologies that would facilitate creation of  our big data analytics solution are provided in the following subsections.

A. Hadoop Hadoop is an open source framework for distributed storage  and data-intensive processing first developed by Yahoo [15]. It has two core projects: Hadoop Distributed File System (HDFS) and MapReduce programming model. HDFS is a distributed file systems that splits and stores data on nodes throughout a cluster, with a number of replicas. It provides an extremely reliable, fault tolerant, consistent, efficient and cost effective way to store large amounts data. The MapReduce model consists of two key functions: Mapper and Reducer. The Mapper processes input data splits in parallel through different map tasks and sends sorted, shuffled outputs to the Reducers that in turn groups and processes them using reduce tasks for each group.

B. HBase HBase is a distributed column-oriented and NoSQL  database built on top of HDFS and modeled after Google's BigTable [16]. HBase provides real-time read/write random-  access to very large datasets [17] In HBase, a table is physically divided into many regions, which are in turn served by different Regional Servers. One of its biggest utility is of combining real-time HBase queries with batch MapReduce jobs, using HDFS as a shared storage platform.

C. OpenTSDB OpenTSDB [18] is an open source distributed and scalable  time-series database, developed by StumbleUpon. It supports real time collection of data points from various sources. It stores, indexes and serve metrics at a large scale in HBase.

D. R and RHIPE R [19] is a language and environment from the open source  community widely used among statisticians and data scientists.

It provides a wide range of libraries for analysis and visualization. R Hadoop Integrated Processing Environment (RHIPE) is a library for R integrating with the Hadoop DFS.

Using MapReduce programming model it computes massive data sets across different nodes in a cluster. It works from the R environment using standard R programming idioms [20].



IV. RESEARCH METHODOLOGY The key research focus of this project is creating an  analytical solution to support assisted decision making enabling preventive care from analysis of behavior pattern based on movement data. A design science [21] research method reflects strive to solve practical problems [22].

Activities in design science follow an iteration of build and evaluate [23]. This is particularly helpful if the desired target state of a system is not known a priori. First of all, requirements for a novel analytical engine to recognize arbitrary movement patterns and bring meaning to behavior, needs to be compiled. After implementing an initial prototype based on the requirements, it could be evaluated and refined [24] in subsequent steps. This enables a process of ?learning via making? [25]. In other words: with each iteration more profound understanding of the problem can be gained, leading to improved prototypes that would help to further track down design issues.

Design science is particularly suited for designing healthcare/lifestyle related systems. In most cases, no technical breakthroughs are needed. Rather, existing technology has to be adopted, reconfigured, and recombined in creative ways to target yet unsolved problems. This helps to focus on the research objective - improving the quality of life of elderly using modern technology as an enabler.

Requirements Engineering (RE) needs adherence to well- established practices [26]. In particular, requirements for movement data analysis has to be derived through literature review as well as working closely with domain experts for the care of elderly such as municipality and health-care professional. Designing and implementing the prototype will follow common procedures from Software Engineering (SE) [27]. All phases will have an incremental character. Progress will be checked with the domain experts with feedback incorporated along the new requirements for the next iteration cycle.

Fig. 1. Research Areas

V. RESEARCH AREAS & CONTRIBUTIONS The key research of our work would be focused towards  data analysis and mining. An end-to-end evaluation of the complete project leads to classification of the research questions into three categories. As represented in Fig. 1, the categories are data privacy & security, data collection and data analysis & mining. The following subsection provides an overview of proposed contributions in creating our analytical solution. As mentioned earlier, a scalable framework for securely collecting and processing data from smart homes is needed. In the due course of our work, we worked on a smart home data analysis framework for collecting vast amounts of sensor data, storing them into a distributed Hadoop cluster while maintaining data security. As continuation of the framework we propose a k-anonymization based privacy model for preserving privacy of shared results and a data intensive platform to enable distributed statistical analysis. Our future work would focus more towards establishing preventive care mechanisms demonstrated mainly through analysis of movement sensor data.

A. Smart Home Data Analysis Framework The data collection process should be independent of the  source and structure of the data. It should embed the security & privacy concepts while extracting data from sensors and pushing them into our distributed big data environment. The workflow should ensure data consistency, confidentiality and integrity. It should be capable of enabling scalability on both ends (number of sources and size of cluster). Proven concepts for storage and processing of continuous time series data without losing their properties in a distributed environment should be investigated.

In an earlier work [28], we had presented a framework for securely collecting and storing sensor data from smart homes in a distributed cluster using the Hadoop framework. The  presented approach Fig. 2, maintained the data utility by not transforming the stored data. Rather based on cryptographic techniques, it replaced the primary and quasi- identifiers1 of collected sensor data with hashed values before storing them into a de-identified storage. A separate encrypted identifier dictionary storage, with hashed and actual identifier values was also maintained as a point of reference for re-introduction of identifiers. The identifier dictionary stored only the unique sets of primary/quasi- identifiers. The de-identified storage contained all data collected from each home, with sizes moving upto tera-bytes of information. We proposed using heuristic- based k-anonymization algorithms [29] based on the end-users privacy level, requirements and authorization on the identifier dictionary storage. The hashed identifiers from outputs of any data processing job on the de-identified store would be replaced with their respective k-anonymized value, thus preserving privacy of any presented/shared results.

Fig. 2. Secure Data Framework Architecture  B. Data Privacy Lifestyle data collected from smart homes are very  sensitive. Although analysis of such data can enable establishing supportive and preventive care, there are major privacy regulations that have to be complied in order to share results across the value chain. The code of conduct for data privacy and security from Helsedirektoratet Norway, prescribes guidelines for data from healthcare and social sectors. Using k-anonymization, privacy of micro data can be preserved depending on actors with whom data is being shared.

A transformed dataset satisfies k-anonymity if every combination of values in personally identifiable columns cannot be matched to fewer than k rows. However, heuristic based k-anonymization algorithms lack scalability to data spread across various nodes in a cluster and are built for centralized storages [30]. A novel distributed MapReduce based k-anonymization solution is needed to address the   1 Personal and quasi- identifiers describe personally identifiable  information. These attributes can directly or in-directly reveal personal information     requirements of our smart home data analysis framework. K.

LeFevre, et. al. [31] demonstrates the optimal multi- dimensional anonymization is NP-hard and introduce a simple, scalable, greedy approximation algorithm for several general- purpose quality metrics, having results better than those produced by more expensive optimal algorithms [29], [32]? [34].

Our k-anonymization solution realizes the Mondrian algorithm in a distributed setting using the MapReduce programming paradigm. As illustrated through our privacy preserving data analysis framework, sensitive/personal information was stored separately in HDFS. The goal is to partition the sensitive data set into subsets called equivalence classes and recursively further partition them until they satisfy the k-condition. The k-condition can be described as the size of partitioned equivalence class, greater than equal to the specified k or smaller than equal to 2k-1 values. The k value represents the number of records that should be indistinguishable from each other. The anonymization workflow is represented in Fig. 3. The anonymization process is carried out iteratively in two phases until the complete dataset is anonymized. First data from HDFS is read through an initializer mapper. All records consist of a primary identifier as key and a set of quasi-identifiers as values. A partitioner groups all key-value pairs into the same reducer as a single equivalence class. A stat reducer counts the total number of records. It also calculates the max, min and cardinalities for each quasi-identifier column of type numeric and characters.

The outputs are stored back into HDFS. Completion of this phase initiates the second phase of partitioning the data. A partition mapper reads the data from HDFS that is to be anonymized. The whole process being performed iteratively, data read by the partition mapper is outputs from a previous phase. Outputs of the mapper consist of the quasi-identifiers as values and a composite key containing the primary identifier and a set of equivalence class identifiers of itself and its parents. For the first iteration the input data set is the raw data set with the same equivalence class identifier as keys for all records. Next a comparator sorts inputs for the reducer on a quasi-identifier chosen based on the statistics from the earlier phase. It uses median partitioning rules as used in kd-trees [35] to determine the most suitable sorting/partitioning attribute.

The partition reducer loads the statistics from previous phase as local distributed cache and partitions the output data set into two equivalence classes based on values on left hand side (lhs) and right hand side (rhs) of its median row. Depending on the partitioning rules it either includes all rows after the median row for the most suitable sorting/partitioning attribute with the same value as itself into the lhs equivalence class and the rest to the rhs equivalence class (strict partitioning) or partitions records until the median record into lhs and remaining to rhs (relaxed partitioning). If an equivalence class satisfies the k- condition it is anonymized based on specified recording rules [35], else the equivalence classes are stored as intermediate results back into the HDFS. The equivalence classes that do not satisfy the k-condition are read using a stat mapper and a respective stat reducer processes each group. The process is continued until all the remaining rows are partitioned into equivalence classes satisfying the k-condition.

Fig. 3. Distributed K-Anonymization Workflow  C. Data Intensive Processing To interpret, analyse and visualize large time-series data, it  is essential to store them while preserving their properties of time dependence, quantity, frequency and speed. HBase provides a multi-dimensional storage through usage of unique composite keys. OpenTSDB an open source distributed and scalable time-series database that use such keys consisting of timestamps, metrics and tags for storing and indexing time- series metrics in HBase. R is an environment for statistical explorations and data visualization with a wide variety of analytical functionalities including time-series analysis. RHIPE enables the extension of R functionalities through MapReduce on data in HDFS & HBase, but remains incompatible in processing composite keys used by OpenTSDB. We propose a framework called R2Time to fill this gap, integrating these solutions together enabling distributed processing of large time-series data across a Hadoop cluster. It provides methods for distributed handling of composite keys, allowing analysis of massive amount of time-series data in an effective and scalable manner. The platform enables statistical and data mining operations using the MapReduce programming model,     through user defined map and reduce tasks for different analytical requirements.

D. Data Analysis & Mining The subsections earlier, described a solution to securely  collect, store and share data from smart homes. It also enabled a platform for distributed statistical analysis. In this section, we propose establishing preventive care, demonstrated through analysis of movement sensor data.

Each smart home comprises of a network of optical sensors detecting movement of residents within a household. Using wristbands with embedded identifiers, multiple individuals within a household can be recognized. Profiles for each household could be established, through daily movement patters. Behavioral anomaly can be detected from significant variation in their patterns or co-relation with other households with similar behavior, leading to heath risks in the future. An example would be behavior of using the toilet, wherein unusual patterns could be detected and care be furnished. This is especially essential for elderly residents, where it is critical to provide right care at the right time. A generic analytical model can provide opportunities for ad-hoc analytics along with knowledge discovery and data mining methods to detect behavioral anomalies. Investigation of various data mining methods and techniques [36] has to be carried out for identifying, evaluating, and choosing correct algorithms to recognize movement patterns, create profiles, establish behavior knowledge bases and perform co-relation on time- series data. Focus would be towards implementation of scalable data mining algorithms to work on a distributed environment and to create knowledge in an accurate & meaningful way.



VI. CONCLUSION Through this paper the motivation and a prototype towards  creation of a data analytical solution for the Safer@Home project is presented. Majority of research in analyzing data from smart homes are much of a lab-based initiative and lack adaptation in the real world. Unlike other projects, our focus is on establishing preventive care based on analysis of movement sensor data to contribute in the welfare of the elderly. Using modern Big Data solutions and proper statistical tools the aim is providing real-time as well as historical analysis of continuous stream of time series data. The key area of research through this project is data analysis and mining. Data security, privacy, collection and visualization are other areas that are also investigated, in order to create and develop a comprehensive & complete solution with real-world applications.

