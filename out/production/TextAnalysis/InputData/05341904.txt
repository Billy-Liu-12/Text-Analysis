2009 2nd Conference on Data Mining and Optimization   27-28 October 2009, Selangor, Malaysia

Abstract?This paper aims to propose a weighted linguistic  associative classification model for uncertainty data analysis  using rough membership function. Transformation of  quantitative association rules into linguistic representation can  be achieved in discretizing the numerical interval into rough  interval described with respective rough membership values.

Transformation of linguistic information system is suggested  prior to the frequent pattern discovery. Neither pruning of  association rules nor classifier modelling is needed. The rough  membership values of the each linguistic frequent item are  composited to form the weighted associative classification rule.

Simulated results on Iris Plant dataset were shown in the paper.

The future work of the research will focus on implementing the  model with more experimental dataset.

Keywords? associative classification, rough set theory, rough  membership function, uncertainty.



I. INTRODUCTION  Human tend to employ natural language when describing a certain situation when communicating with others, while the same situation is not easily explainable when using numerical values. Quantitative rules that convey in numbers are sometimes less practical in conversation. Thus, describing the interval data in natural language has lead to the emergence of the usage of linguistic terms in association rule which are later known as linguistic association rule.

Linguistic association rules mining is derived from the original concept of association rules mining [1]. The term ?Linguistic association rule? was coined in order to replace the numerical values with linguistic terms in quantitative association rules mining. With the introduction of associative classification task in [2], linguistic association rule mining can be further explored and transformed into associative classification rule mining. Besides fuzzy set theory, rough sets suggested by Zdzislaw Pawlak is another technique in soft computing that is able to deal with ambiguous boundary data sets [3]. Rough sets theory uses the upper and lower approximation as well as the rough membership function to determine the items in a set which can be obtained through self discovery in learning data. They are similar but different from the uncertainty handling  proposed by fuzzy theory.  Hence, it has gained the interest of researchers in the development of rough sets techniques as a complement to fuzzy techniques.

Currently, most researchers focus only on the concept of approximation. Hence, this paper attempts to investigate the opportunity and strength of rough sets theory in linguistic association rules mining from the perspective of rough membership function and embed the proposed model into associative classification task. The expected output of the model is the weighted associative classification model for uncertainty data analysis. The organization of the paper starts with the brief explanation on the theoretical concepts of associative classification in section II-A and section II-B, capturing uncertainty in linguistic association rule in section II-C, the related rough set concepts in section III, the proposed associative classification model and the detailed discussion on every phases in section IV before discussion on the simulation results in section V and lastly the conclusion and future work of this research in section VI.



II. ASSOCIATIVE CLASSIFICATION  The task of associative classification (AC) was born from  combination of two frequently used data mining tasks, i.e.

the association rules discovery and the classification task. Its  main purpose is to model the classifier for prediction akin to  the goal of classification [4]. AC has been initiated to  complement whatever inadequacies in both the association  rules mining and classification tasks [5-8]. The task of AC  starts from the frequent itemsets discovery, association rules  generation, pruning and rules selection to the regular  classification with AC rules.

A. Association Rules Mining  Association rule mining was first proposed by Agrawal,  Imielinski, and Swami [9]. It is a technique to extract  patterns from huge data sets to obtain useful information  generally from the transaction data. Earliest example of  association rule mining is presented by the market basket  analysis, which forms the foundation of today?s association  rule mining. Later, the emergence of the categorical and  quantitative association rules mining has widened the         discussion of association rules mining. The conventional  quantitative association rules mining with A priori algorithm  proposed by Agrawal and Srikant [10]. It has become the  foundation of many new techniques in the later work of  association rules mining. The A priori algorithm consists of  two major tasks, i.e. the frequent pattern discovery to scan  for frequent occurring itemsets that satisfied the predefined  support and confidence thresholds and the association rules  generation.

B. Classification  Classification is one of the most important tasks in data mining. The classification task models a classifier from the data records, with the intention of categorizing objects into different decision classes as precisely as possible according to the values of each attributes in the dataset. Classification is commonly carried out in two phases, i.e. the training and the testing. The training phase aims to generate classification patterns where each of the rules links a decision class while the testing phase is to verify the generated classification rules on the unseen data record. The higher the accuracy of generated classification rules set on the test data, the better the classifier is.

C. Capturing Uncertainty with Linguistic Association Mining  Linguistic association mining describes data attributes in a human understandable language. It conveys the message in linguistic terms instead of numerical values in quantitative attributes. Linguistic association mining task is generally being divided into two phases. The first phase is to discretize the continuous numeric data and represent those intervals with a suitable linguistic term [11] before proceeding to the second phase of frequent pattern mining. Partitioning the numeric values into interval sets will cause crisp boundary problems. This may create some contradiction and overlapping when these intervals are being presented in linguistics term. Hence, there is a need for a soft boundary instead of crisp boundary. Since the partitions are divided by linguistic terms, it is not easy to differentiate the boundary of the set [12]. There are several techniques in mining quantitative and categorical attributes in association rules.

Some of the attributes may not be suitable to be described by intervals. To make them more recognizable to humans, it is advisable to use the natural language terms. However, this introduces some other problems. The linguistic terms used may not be sufficient to embody the actual distribution of data [12]. Problems happen due to the difficulty of distinguishing the exact boundary of a particular interval set.

Thus, concept of uncertainty is used to soften the boundary of interval sets in data analysis [13].

Data discretization has arisen since the earlier work on quantitative association rule mining, while the linguistic terms representation is always being related to Fuzzy sets theory. Soft partitioning technique and some soft computing related techniques were introduced to create vague interval sets to handle this problem. Although typical analysis such  as binning [14, 15], statistical [16] and evolutionary techniques [17] have contributed to a certain extent in dealing with quantitative attribute partitioning, it seems that soft computing methods especially involving fuzzy theory or its hybrids techniques with A priori algorithm and genetic algorithm are more promising techniques in managing soft partition problems. Fuzzy algorithm has been improved from time to time to more efficiently and accurately mine the linguistic terms used in association rules mining. Since then, fuzzy sets theory has become the more prominent technique in dealing with linguistic representation on soft boundary interval.

On the other hand, rough sets theory introduced by Pawlak also deals with vagueness and uncertainty. With embedded foundation of classical theory, rough sets theory has its own perspective towards vagueness which is similar to fuzzy set theory [18]. According to Pawlak, it is a unique implementation of Frege?s idea of vagueness. The rough membership function is a special case of fuzzy membership function [19]. Hence, it is expected that rough sets theory can be used to describe linguistic knowledge as an alternative to fuzzy set theory in a different view of vagueness and uncertainty.



III. ROUGH SET THEORY  Rough sets theory was introduced by Pawlak [5] to deal  with vagueness and uncertainty. According to [20] an  information system can be presented as a pair >=< AUS , ,  or a function VAUf ??: , where U is the nonempty  universe object set, A an nonempty attribute set, and V a  value set of attribute a such that V a Ua ?:  for  every Aa ? . A decision system is any information system of  the form >?=< }{, dAUS , where Ad ?  is the decision  attribute. The elements of A are called conditional attributes.

The universe, U can be partitioned into disjoint subsets  known as equivalence classes which are discernible among  one another. All the elements in one particular subset are  considered equal or indiscernible based on the selected  attribute subset. This gives the definition of indiscernibility.

Let A=(U, A) be an information system and each subset of  attributes AB ?  defines an equivalence relation, called an  indiscernibility relation as in (1):    ))}()((,|  ),{()( jxaixaBaUjxixBIND =???=?  (1)    The core of rough sets approach is the idea of  indiscernibility between objects. Thus, the equality operator  is needed to define every attribute domain. This makes the  method suitable to apply on any problem with discrete or  categorical values.  Therefore, continuous or real values have  to be combined or discretised in the pre-processing step in  order to take on a suitable level of granularity. Discretisation         on quantitative values will reduce and optimize the  determining input before the process of rules generation.

Rough membership function is another uncertainty  representation in rough sets theory. Let AB ?  a set of  attributes, the rough membership function ]1,0[: ?U B X?  is  defined as in (2):   |][|  |][|  )(  x B  Xx B x  B  X  ?  =?  (2)  Equation (2) is an unbiased estimation of conditional  probability ),|Pr( BxXx ? , the probability of object x  belongs to set X in terms of B. Similar to fuzzy membership  value, the operation of union and intersection of two RMVs  [19] is defined in (3) and (4) as follows:  (3)  (4)  Reference [21] shows the further details in the notation of  rough membership function. Rough membership derived  from accuracy of the rules will be used as the rough accuracy  value to generate membership reference in transforming  numerical values into linguistic variable. Instead of getting  the membership value from the expert, our method is able to  auto generate membership references through learning from  data. Similar to fuzzy membership values, the generated  rough membership reference will be applied in transforming  the instances value into rough approximation region in  linguistic rules mining.

According to [20], default rules generation framework  proposed by Mollestad to discover more general rules to  capture useful knowledge in incomplete dataset. The rules  generated have two advantages compared to the  deterministic rules. First, they are usually simpler in  structure because they cover larger classes defined over a  few condition attributes. Secondly, they are in many cases  proven to be better when handling unseen cases. The default  rules are generated through creating the indeterminacy in a  decision system. The generation of indeterminacy is done  through selection of projections over the condition attributes  allowing certain attributes to be excluded from  consideration. The removal of the information in terms of  condition attributes from a given decision system allows the  generation of rules from the resulting decision tables. The  underlying idea is to search for default rules by removing  reducts from the original system. The set of full reducts  computed from the decision system determines the set of  attributes to be retained in the projection. Refer to [20] for  further information on default generation framework.



IV. THE PROPOSED MODEL  The proposed associative classification model employs the  similar architecture to common AC technique. A new phase  of generating rough membership value is embedded prior to  the two major steps of original associative classification  model to capture and define uncertainty in dicretized  attribute. Some modifications have been made to the AC  phases to facilitate rough associative classification rules  generation. Fig. 1 shows the proposed associative  classification model. The proposed rough associative  classifier model comprises of three major phases serving  different functionalities as below:  A. Generating Rough Membership Value (RMV)  This phase serves the purpose of inducing rough  intervals with the corresponding rough membership values  for linguistic representation and transforms the linguistic  decision system. It is an automated membership generation  process similar to the automated fuzzy membership function  in fuzzy system. Once the rough intervals had been  determined from the training data, they can be used for  association rules mining process in any relevant testing data.

Basically, there were two major steps involved in the rough  variables generation, i.e. the discretisation and the rough  intervals induction.

The Boolean reasoning suggested as the discretisation  method in rough classification is implemented as a necessary  step in rough interval generation [22]. Rough intervals  induction was the second step in generating rough variables.

The purpose of rough interval generation is to obtain the  rough membership values (RMV) corresponding to each  discretised interval [23]. RMV can be derived from two  sources, either the right hand side accuracy of the rough  classification rules [23] or the probability of occurrence of  the respective intervals in the data [21]. In the proposed  model, default rules generation framework is used to obtain  the rough classification rules and the accuracy values. RMV  indicates the probability of an object being classified in the  set of a decision class [19, 21, 24, 25]. It is used as the  numerical weightage in AC. RMV for all output class of  every rough interval is determined according to the formula  defined in (5) as follows:    ;    repeat for every 0 < i ? x and 0 < j ? y  (5)   Let x denotes the number of rough interval and i is the i-th rough interval. Card (i) is the total instance in the i-th rough interval. Card (Oj) is the total instance from i-th rough interval that concludes to decision class Oj while j is the j-th decision class and y is the total decision class.

Fig. 1: The proposed Rough AC model    B. Association Mining  The association mining is the second phase in the proposed model. Modification on the conventional association rule mining has been made in the proposed model. Instead of generating association rules from the frequent itemset, the proposed model starts with transforming the decision system into linguistic information system (IS). The next step is to scan for frequent occurring linguistic itemsets that satisfy the user-defined support thresholds.

The linguistic IS transformation serves a two-fold purpose: i.e. replaces the decision classes in the original decision system with linguistic terms and transform the decision table into a linguistic IS table. This is an important step before the frequent pattern mining in order to generate linguistic frequent pattern instead of quantitative frequent pattern. The linguistic terms carry different RMV weightage towards the discretised RMR intervals. The decision classes will be replaced by user-defined linguistic terms. Replacing the decision classes with predefined linguistic terms will  change the decision classes into different rough linguistic sets [19].

User-defined linguistic terms were used in the experiments.

Therefore this process requires the intervention of expert knowledge. Rough membership function helps in capturing the uncertainty boundary of the rough linguistic set, but user understanding of the dataset used is still necessary in order to determine suitable linguistic terms to be replaced. Imprecise terms chosen may result in deviation of the meaning of the rules. However, this would not affect the process of rules generation and the performance of the association rules.

Choosing suitable linguistic terms with expert knowledge is not the focus of this paper. The second part of the linguistic decision system transformation is to convert the original decision system into a linguistic decision system. Similar to fuzzy decision system proposed in [26], the attributes in the linguistic decision system are the linguistic terms chosen previously while the attributes values are the RMV attained  from respective rough intervals. All the instances that fall in the same rough interval will be assigned the same RMV.

Iteration of the algorithm finds different large itemsets (frequent combination of items that appears together). The frequent pattern discovery algorithm [9] is referred in this phase. It starts with first scan pass the dataset to determine the occurrences to produce the large 1-itemsets. This is denoted as L1. L1 is then used to find L2, which is the large 2- itemsets in the 2-pass scanning. The process is carried on until no more frequent k-itemsets can be found. Since the associative mining process stops at frequent pattern discovery, only the measurement of minimum support is needed in this phase. The higher the minimum support thresholds the lesser the frequent pattern  discovered in the experiments but it can increase the accuracy since only strong frequent itemsets passed the threshold constraints [27, 28].

C. Weighted Linguistic Associative Classification Rules Generation  This is the last major phase in the proposed AC model.

Some modification has been made in this phase compared to  the conventional concept of AC model. This phase aims to  generate weighted associative classification rules from the  linguistic frequent pattern in the previous phase. Neither  pruning of association rules nor classifier modelling is  needed. The RMV values of the each linguistic frequent item  are composited to form the weighted associative  classification rule. Refer to section II.



V. SIMULATION RESULTS  The proposed model was simulated and discussed here.

The simulation results shown in this section is based on the Iris Plant (IP) dataset from UCI repositories [29]. The rough interval of IP dataset and their respective RMV is shown in Fig. 2. Three decision classes were replaced with linguistic terms describing the size of the Iris plant, i.e. short (Iris- setosa), medium (Iris-versicolor) and long (Iris-virginica).

Fig. 2: Example of rough intervals with respective RMV   The original decision system was transformed into  linguistic information system. Fig. 3 is an example of linguistic data table for IP dataset. The RMV was used as the linguistic attributes value in the new IS. No decision class is  Generating RMV:  Association Mining:  Discretisation  Induce Rough Intervals  Linguistic IS Transformation    Frequent Pattern Discovery    Weighted Linguistic Associative Classification Rules Generation           needed as each linguistic attribute itself embodies a rough decision class set. The associative relation between these rough decision class set need to be identified.

Fig. 3:  Example of linguistic data table  The frequent pattern mining was done on the new linguistic information system to get the large itemset. An example of the associative frequent pattern generated is shown in table I. Each of the generated items contains multiple relations between the rough decision classes. All the relations were composited into one AC rule.

TABLE I.  AN EXAMPLE OF LINGUISTIC FREQUENT PATTERN FOR IP DATASET  No. Linguistic Frequent Pattern  1 PL.Medium^PW.Medium^PW.Short   For example, the linguistic frequent L3 itemset consists of  three rough decision classes. The rough intervals and their respective RMVs generated in the first phase of the proposed model were referred. The composition of RMVs is shown as follows:                         Thus, weighted linguistic AC rules generated from linguistic frequent L3 itemset is as shown in table II.

TABLE II.  THE WEIGHTED LINGUISTIC ASSOCITIVE CLASSIFICATION RULES  No. Linguistic AC Rules Rule  Weightage  1 IF PL.Medium^PW. Medium^PW.Short   => Iris-setosa 0.51  IF PL.Medium^PW. Medium^PW.Short  => Iris- versicolor   3 IF PL.Medium^PW. Medium^PW.Short => Iris-virginica 0

VI. CONCLUSION AND FUTURE WORK  This paper has proposed an associative classification  model for uncertainty data analysis using rough membership  function. Rough membership value is suggested as the  weightage for capturing uncertainty in linguistic  representation of discretized numerical intervals. With the  new phase of generating rough membership values  embedded prior to the two major steps of original associative  classification model, the proposed model is able to handle  continuous real-value attributes to generate weighted  linguistic AC rules instead of crisp quantitative AC rules.

The linguistic terms carry different RMV weightage. The  weighted attribute sets will then go through the proposed  task of associative classification including linguistic IS  transformation, frequent pattern discovery, weighted  associative classification rule generation. The generated  weighted linguistic associative classification rules set can be  used to perform classification operation on respective unseen  data records. The future work of this research is to verify the  proposed model with real dataset. The results will be  reported in near future.

