Patterned Growth algorithm using Hub-Averaging  without pre-assigned weights

Abstract? The concept of finding frequent itemsets without pre- assigned weights is of great importance in Association Rule Mining (ARM). The prime advantage of this approach is that weights can be derived from the dataset itself rather than being given by domain expert. The modification of Apriori algorithm for Weighted Association Rule Mining (WARM) without pre- assigned weights using HITS algorithm has been attempted in the past. However, drift effect is a major limitation of HITS algorithm. In this paper, a new approach HAP-Growth (Hub- Averaging Pattern-Growth) has been proposed for WARM without pre-assigned weights. HAP-Growth algorithm generates frequent itemsets using Hub-Averaging in conjunction with pattern tree approach. Performance of the proposed algorithm has been compared with HITS algorithm in conjunction with pattern tree approach and the existing algorithm. Experimental studies have been carried out on large number of synthetic datasets of varying sizes (generated using IBM Synthetic Data Generator) and real life datasets (taken from UCI Machine Learning Repository and other sources). It is observed that for large datasets, there is drastic reduction in the computational time for the proposed algorithm and at the same time drift effect is reduced to a great extent.

Keywords ?data mining; hub-averaging;  link analysis;  weighted association rule mining

I. INTRODUCTION Association Rule Mining (ARM) has received a great deal  of attention in the recent past. ARM aims at generating association rules among the items by exploring large databases.

Measurement framework proposed in ARM is of support- confidence that reduces it to the discovery of frequent itemsets.

The problem of mining Association Rules over market basket data was introduced by Aggarwal, Imielinski and Swami [1].

The fast mining algorithm named Apriori, AprioriTID and AprioriHybrid were proposed by Aggarwal and Srikant[2].

Later on, FP-Growth (Frequent Pattern Growth) method was proposed by Han, Pei and Yin [8] for finding frequent itemsets that uses few database scans as compared to Aproiri algorithm.

Classical association rule mining algorithms are limited to finding associations among items within the same transactions.

But associations may span across different transactions and hence cross-object relationships are introduced by Feng and Tsang [7] in which association exists between the items  available in different transactions. Wong, Wu and Feng [13] proposed a modification of Apriori in which features were embedded for encoding and decoding mechanisms to reduce the I/O time for finding large itemsets, to use memory in economical manner and to improving the speed of identifying different items in the transactions. Ding, Ding and Perrizi [6] proposed PARM for mining association rules from spatial data using P-trees.

In classical Association Rule Mining, all items and transactions in market basket data are given equal importance.

But in real life different items and transactions may have different importance. Hence, Weighted Association Rule Mining algorithms were proposed in which weights were pre- assigned [5] [12] [14].

Cai, Fu, Chang and Kwong [5] proposed an algorithm in which weights were pre-assigned to items (by domain expert) in the transaction. This algorithm could not satisfy the downward closure property. Yet another definition of weighted support was proposed by Tao, Murtagh and Farid [12] where-in weights were assigned to both items and transactions and this satisfied the downward closure property. Another algorithm, named Weighted Frequent Itemset Mining (WFIM) was proposed by Yun and Leggett [14] in which pattern growth algorithm was used.

It is not always possible for a domain expert to give weights for items and transactions. Hence, HITS algorithm was proposed [11] for finding weighted support for items and transactions. Originally, HITS algorithm was used in the area of link analysis for page ranking, where-in, with every page two identities are associated [9]. The hub identity captures quality of page as a pointer to useful resources and the authority identity captures quality of page as a resource itself. Sun and Bai [11] extended HITS algorithm to Association Rule Mining (ARM) where they considered items as authorities and transactions as hubs. The weights were computed for items and transactions without pre-assigning any weight to them. However, HITS algorithm has certain drawbacks. It computes the weight of authority (item) as the sum of weight of transactions pointing to it and weight of transaction as sum of the weight of authorities (items) pointed to by it. Thus, the algorithm assumes that good transaction (hub) contains collection of links to good items (authorities) and good items (authorities) contain collection of links from good transactions (hubs), which is not true. There     may be some items (authorities) that are not good. As a result, bad items (authorities) in HITS become as important as good items (authorities) because of the mutual reinforcement relationship between the items and transactions. Hence, to neutralize the effect of bad items (authorities) to some extent on hub value, Hub-Averaging has been proposed in this paper.

In the proposed work, Hub-Averaging is used in conjunction with pattern tree approach to speed up the process of finding frequent itemsets and to reduce the drift effect caused by HITS algorithm. The efficiency of the proposed algorithm is compared with HITS in conjunction with pattern tree approach and the algorithm proposed by Sun and Bai [11] on synthetic datasets (generated from IBM Synthetic Data Generator [16]) and real life datasets (taken from UCI Machine Learning Repository [18] and other sources [15][17]). It is observed that proposed algorithm outperforms other algorithms in terms of computational time in addition to reducing drift effect. The remarkable feature of the proposed algorithm is that for very large market basket data there is appreciable decrease in computational time as compared to other algorithms.

The present paper is organized as follows: Section II presents Weighted Frequent Itemset Mining; Section III gives an overview of HITS algorithm; Section IV discusses Hub- Averaging algorithm; Section V describes finding frequent itemsets using Hub-Averaging; Section VI gives the details of comparative performance evaluation and in the last section concluding remarks are given.



II. WEIGHTED FREQUENT ITEMSET MINING The weight of an item is a non-negative real number that  shows the importance of each item. The problem of Weighted Association Rule Mining is to find the complete set of association rules satisfying a support constraint and a weight constraint in the database [5] [12] [14]. The notion of weighted items to represent the importance of individual items was introduced by Cai, Fu, Chang and Kwong [5]. This work could not make a balance between the support and the weight measure and hence downward closure property was not maintained. Weighted Association Rule Mining in which weights were assigned to items and were derived for transactions from the participating items was introduced by Tao, Murtagh and Farid [12]. It was proposed that an itemset is significant if its weighted support is above a pre-defined minimum weighted support threshold. This method was more meaningful than specifying relatively arbitrary weights only to items.  Yun and Leggett [14] introduced Weighted Frequent Itemset Mining (WFIM) where in pattern growth algorithm was used while maintaining the downward closure property. In this algorithm, a weight range and a minimum weight constraint were provided. Items were given different weights within a weight range. By adjusting a minimum weight and weight range, Weighted Frequent Itemset Mining generates important and concise Weighted Frequent Itemset (WFI) in large dense databases with low minimum support value. Sun and Bai [11] proposed Weighted Association Rules without pre-assigned weights where-in HITS algorithm is used in conjunction with modified Apriori to assign weights to the frequent itemsets.



III. OVERVIEW OF HITS ALGORITHM Hyperlink-Induced Topic Search (HITS) (also known as  Hubs and Authorities) proposed by Kleinberg [9] is a link analysis algorithm that rates web pages. It associates two values for each page: its authority value, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages. Retrieving the result set to the search query is the first step in HITS algorithm. The computation is then performed only on this result set and not across all Web pages. The authority and hub values (weights) are defined in terms of one another in mutual recursion. The algorithm performs series of iterations, each consisting of two steps: Update Authority: Update authority weight of each node as sum of the hub weights that point to that page. Update Hub: Update hub weight of each node as sum of the authority weights of the pages it points to. The computation of hub and authority values is given by equation (1) and (2). a h?B                                                      1  h a?A                                                    2 where  is the authority weight,  is the hub weight,  is the set of nodes that point to node  and  is the set of nodes pointed to by node .

Sun and Bai [11] extended the HITS algorithm to Association Rule Mining by considering items as authorities and transactions as hubs.



IV. HUB-AVERAGING ALGORITHM Hub-Averaging algorithm was proposed by Borodin,  Roberts, Rosenthal and Tsaparas [4] to associate the hub and authority weights. In case of HITS algorithm, the hub weight of the node is the sum of weight of all authorities pointed to by it and the authority weight of the node is the sum of weight of hubs that point to it i.e. all authorities that are pointed to by a hub contribute equally to the hub weight of the node and vice versa. Because of this, non-intuitive result is produced and hence quantity becomes quality, that is, if the hub points to many weak (bad) authorities its weight is more as compared to the hub that points to less but good authorities [3].

To overcome this drawback, Hub-Averaging is used in place of HITS. The intuition of the Hub-Averaging algorithm is that a good hub should point only to good authorities, rather than to both good and bad authorities. This has been achieved by updating the hub weight to the average authority weight. The pseudo-code for the proposed algorithm is given in Section V.

Hub-Averaging initializes all authority weights to 1. The process of calculating and normalizing hub and authority weights continues till the respective weights converge. The algorithm updates the authority weight like HITS algorithm, but sets the hub weight to the average authority weight of the authorities pointed to by hub. Thus, authority and hub weights are computed as given by equation (3) and (4) and are finally normalized.

a  h?B                                                          3 h  1|A t | a?A                                               4 where  is the authority weight,  is the hub weight,  is the set of nodes that point to node  and  is the set of nodes pointed to by node . Hub-Averaging algorithm is a hybrid of the HITS and SALSA algorithms [10]. The algorithm performs the operation of finding authority weights like HITS algorithm and hub weights like SALSA algorithm.



V. FINDING FREQUENT ITEMSETS USING HUB-AVERAGING Hub-Averaging computes weighted support for items  present in the database on the basis of their importance in the database and hence helps in finding important items in the database. The results computed by using Hub-Averaging helps in generating better set of frequent itemsets because items are given weights depending on their importance. The weighted support for an item is taken as summation of the weight associated with transactions that contain the item. The reinforcement relationship between the transactions and items is used to find weight for the items and thus helps in finding frequent itemsets in the database. The algorithm HAP-Growth, for mining frequent itemsets is given in Fig.2. An example database is taken to show how Hub-Averaging helps in computing the weighted support for the items and how the weighted support value is different from the support value computed without associating weights with items and transactions. Computing transaction weight by taking average item weight reduces the drift effect in finding important items that is the limitation of HITS.

Consider the following market basket data (given in Table I) [17]. The equivalent bipartite representation is shown in Fig.1.

The graph representation gives the insight of applying link based ranking model for transaction evaluation. Here, we use Hub-Averaging to find AW-support (authority weighted? support), to represent significance of itemset . The AW- support and support for an itemset  is defined as:  AW support X ? hub t:X? ?D? hub t: ?D                       5  support X ? 1:X? ?D? 1: ?D                                   6 where  is the hub weight of the transaction . An  itemset is significant, if AW-support is larger than the user specified minimum weighted support value. The hub weights for the market basket data (Table I) are given in Table II. The AW-support and support values are computed using equations (5) and (6) and are tabulated in Table III.

Figure 1. Graph Representations between Transactions (hubs) and Items (authorities)   TABLE I.  TRANSACTIONAL DATABASE OF MOVIE  TID Transactions  1 [Lord_of_the_Rings, The_Green_Mile]  2 [Lord_of_the_Rings, The_Sixth_Sense, Forrest_Gump, Con_Air]  3 [The_Green_Mile, The_Sixth_Sense, Forrrest_Gump, Twister]  4 [Lord_of_the_Rings,The_Green_Mile, The_Sixth_Sense, Forrest_Gump]  5 [Lord_of_the_Rings, The_Green_Mile, Forrest_Gump, Twister]  TABLE II.  HUB WEIGHT OF TRANSACTIONS  TID Transactions Hub Weight  1 [Lord_of_the_Rings, The_Green_Mile] 0.525  2 [Lord_of_the_Rings, The_Sixth_Sense, Forrest_Gump, Con_Air] 0.369  3 [The_Green_Mile, The_Sixth_Sense, Forrrest_Gump, Twister] 0.406  4 [Lord_of_the_Rings,The_Green_Mile, The_Sixth_Sense, Forrest_Gump] 0.473  5 [Lord_of_the_Rings, The_Green_Mile, Forrest_Gump, Twister] 0.446  TABLE III.  SUPPORT COMPUTED USING CLASSICAL ARM AND AW-SUPPORT COMPUTED USING HUB-AVERAGING (OF MOVIE DATABASE)  1-itemset Support (classical ARM) AW-support (Hub-Averaging)  {Lord_of_the_Rings} 0.8 0.82  {The_Green_Mile} 0.8 0.83  {The_Sixth_Sense} 0.6 0.56  {Forrest_Gump} 0.8 0.76  {Con_Air} 0.2 0.17  {Twister} 0.4 0.38   The AW-support evaluates itemsets in a different way. For  example, items {Lord_of_the_Rings}, {The_Green_Mile} and {Forrest_Gump} all have a support of 0.8. However, their AW- support is different which tells the importance of one item over the other.



VI. RESULTS Comparative performance of the proposed algorithm HAP- Growth has been done with the existing algorithm and HITS in conjunction with pattern tree approach for generating frequent itemsets. Several tests have been carried out on classical data sets generated by IBM Synthetic Data Generator [16] and on       benchmark datasets taken from UCI Machine learning Repository [18] and other sources [15]. The experiments were conducted on Intel Core 2 Duo processor machine with 160 GB hard disk and 1GB RAM.

A. Comparative Performance Evaluation on IBM Synthetic Datasets For various sizes, the performance evaluation of the  proposed algorithm HAP-Growth (Hub-Averaging with pattern tree approach) has been compared with HITS in conjunction with pattern tree approach and the existing algorithm proposed by Sun and Bai [11]. Datasets of varying sizes were generated using IBM Synthetic Data Generator [16]. The datasets have been grouped into 3 categories of sizes in the range 0-15MB (Small sized datasets), 16- 45 MB (Medium sized datasets) and 45-75MB (Large sized datasets). The comparative performance evaluation has been shown in the Tables IV-VI for various categories of sizes.

TABLE IV.  RUNTIME COMPARISON OF THE PROPOSED ALGORITHM WITH THE EXISTING ALGORITHM AND HITS IN CONJUNCTION WITH PATTERN TREE APPROACH ON DATASETS OF SMALL SIZES  Sr.

No.

Size (in MB)  HITS with modified Apriori (Time in Seconds)  Hub-Averaging with pattern  tree approach (Time in Seconds)  HITS with pattern tree  approach (Time in Seconds)  1 7.6 MB 18.65 2.53 7.98  2 9.4 MB 21.50 3.12 9.40  3 10.3 MB 23.47 3.51 10.51  4 11.3 MB 25.84 3.74 11.34  5 12.4 MB 27.78 4.03 12.01  6 13.1 MB 29.82 4.21 12.91  7 14.1 MB 33.96 4.72 14.27  TABLE V.  RUNTIME COMPARISON OF THE PROPOSED ALGORITHM WITH THE EXISTING ALGORITHM AND HITS IN CONJUNCTION  WITH PATTERN TREE APPROACH ON DATASETS OF MEDIUM SIZES  Sr.

No.

Size (in MB)  HITS with modified Apriori (Time in Seconds)  Hub-Averaging with pattern  tree approach (Time in Seconds)  HITS with pattern tree  approach (Time in Seconds)  1 16.8 MB 40.44 5.94 16.78  2 20.6 MB 68.23 6.63 19.73  3 25.1 MB 104.44 8.36 23.45  4 29.2 MB 152.49 9.42 27.32  5 33.9 MB 227.83 10.01 33.04  6 41.6 MB 290.56 12.46 38.39  7 44.9 MB 368.69 14.27 42.57  TABLE VI.  RUNTIME COMPARISON OF THE PROPOSED ALGORITHM WITH THE EXISTING ALGORITHM AND HITS IN CONJUNCTION  WITH PATTERN TREE APPROACH ON DATASETS OF LARGE SIZES  Sr.

No.

Size (in MB)  HITS with modified Apriori (Time in Seconds)  Hub-Averaging with pattern  tree approach (Time in Seconds)  HITS with pattern tree  approach (Time in Seconds)  1 46.6 MB   440.32    15.49    44.12  2 50.2 MB    516.70    16.43    46.74  3 54.3 MB   603.42    17.47    49.61  4 58.5 MB   707.93    19.02    54.29  5 63.5 MB   883.43    20.86    57.76  6 66.8 MB 1049.92    21.45    60.82  7 72.4 MB 1286.83    23.80    65.45  Given MBD with minimum weighted support count,  w_min_sup; Invoke Hub-Averaging(MBD); Compute AW-support for each item i; Steps for generating pattern tree: (a). MBD is read once (b). 1-frequent items for which AW-support > w_min_sup are computed; (c). 1-frequent items are then sorted in descending order of AW- support denoted by OL; (d). Let pattern tree be MFPTree, with root node labeled ?root?; (e). For each transaction t in MBD: (i). Ordered frequent items in t are selected in accordance to their order in OL and represented by [f/F], where f is the first element in the list F; (ii). Invoke tree_gen([f/F], MFPTree); (iii). H+=hub(t); For mining pattern tree invoke Pattern_gen_Hub  (MFPTree, null);  Procedure Hub-Averaging(MBD) { initialize auth(i) to 1 for each item i; for (l=0; l<iterations; l++) { auth?(i) = 0 for each item i for all transactions t ? D { hub(t)= ?i:i?t auth(i)/i; auth?(i) += hub(t) for each item i?t; } auth(i) = auth?(i) for each item i, normalize auth } }  Procedure tree_gen([f/F], MFPTree) { if ((MFPTree has a child C) && (C.item-name=  f.item-name)) increment C?s count by hub(t); else { create a new node C; increment its count by hub(t) for that item; } add to MFPTree parent link of C; add node-link of C to the nodes with the same item-name; if (F is not equal to empty) invoke tree_gen (F,C); }  Procedure Pattern_gen_Hub(MFPT,?) { if (P is a single path in MFPT) { evaluate AW-support = (minimum hub(t) of nodes in ?) / H for pattern (? ??) where  ? is every possible combination of nodes in the path P; } else for each item xi in the header of MFPT { evaluate AW-support = (xi.hub(t))/H for every pattern (xi ? ?) and construct its conditional pattern base and finally conditional tree CFPTree; if (CFPTree is not equal to empty) invoke Pattern_gen_Hub (CFPTree, ?); } frequent patterns = all patterns with AW-  support > w_min_support; }   Figure 2.  Pseudo-Code for HAP-Growth (for Mining Frequent Patterns)      Fig. 3 depict that HAP-Growth (Hub-Averaging in  conjunction with pattern tree approach) outperforms HITS in conjunction with modified Apriori irrespective of the size of the datasets of all categories. Fig. 4 depicts that Hub-Averaging takes computationally less time for finding weights of items and transactions and it reduces the drift effect in the results caused by HITS because it computes the weight of transaction as average weight of items participating in the transaction.

B. Comparative Performance Evaluation on Benchmark Datasets Comparative performance evaluation was carried out on  various benchmark datasets like Kosarak [15], Mushroom (taken from UCI Machine Learning Repository [18]), Pusmb-star [15], Retail [15] datasets. Description of each of the datasets is given below.

The Kosarak dataset comes from the click-stream data of a Hungarian online news portal. The Mushroom data set includes descriptions of samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. The Pumsb-star dataset is census data from PUMS (Public Use Microdata Sample). The Retail dataset contains information about the date of purchase, the receipt number, the article number, number of items purchased, the article  price and the customer number. Comparative performance evaluation of the proposed algorithm with the existing algorithm and HITS in conjunction with pattern tree approach for these datasets is given in Table VII and Fig. 5- 8. It is observed that for these benchmark datasets also HAP-Growth (Hub Averaging with pattern tree approach) outperforms HITS with modified Apriori and pattern tree approach.

TABLE VII.  RUNTIME COMPARISON OOF THE PROPOSED ALGORITHM WITH THE EXISTING ALGORITHM AND HITS IN  CONJUNCTION WITH PATTERN TREE APPROACH ON DATASETS OF LARGE SIZES  Sr.

No  Dataset HITS with modified  Apriori (Time in Seconds)  Hub-Averaging with pattern tree approach (Time  in Seconds)  HITS with pattern tree  approach (Time in Seconds)  1 KOSARAK     56.07        22.60      28.38  2 MUSHROOM       1.91          0.41         0.67  3 PUMSB STAR     27.15         6.16         9.19  4 RETAIL       7.40         1.80         2.25          Figure 4. Comparison of Hub-Averaging with HITS using pattern tree approach on small, medium and large sized datasets    Figure 3. Comparison of the proposed algorithm with the existing algorithm on small, medium and large sized datasets  70 Hub-Averagi  ng with pattern tree approac h (Time in Sec.)  HITS with pattern tree approac h (Time in Sec.)  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)  Comparison of  Hub-Averaging with HITS using pattern tree approach   16 Hub-  Averaging with pattern tree approach (Time in Sec.) HITS with pattern tree approach (Time in Sec.)  Comparison of  Hub-Averaging with HITS using pattern tree approach  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)    Hub- Averagin g with pattern tree approach (Time in Sec.)  HITS with pattern tree approach (Time in Sec.)  Comparison of  Hub-Averaging with HITS using pattern tree approach  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)   40 HITS with  modified Apriori (Time in Sec.)  Hub- Averaging with pattern tree approach (Time in Sec.)  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)  Comparison of proposed algorithm with the existing algorithm   400 HITS with  modified Apriori (Time in Sec.)  Hub- Averaging with pattern tree approach (Time in Sec.)  Comparison of proposed algorithm with the existing algorithm  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)   1400 HITS with modified  Apriori (Time in Sec.) Hub- Averaging with pattern tree approach (Time in Sec.)  Ti m  e ta  ke n  (in S  ec .)  Size (in MB)  Comparison of proposed algorithm with the existing algorithm

VII. CONCLUSIONS A novel framework has been presented here. Hub-  Averaging model is used to derive weights for items and transactions from the database. AW-support has been used to give significance to itemsets. AW-support takes into account the average value of authorities for calculating the hub weights and hence is different from traditional support.

Pattern tree approach has been followed here to extract frequent items whose AW-support is above some given threshold. Computational cost of link-based model presented here is much less than the work done earlier in this field. Through comparison, we found that our method addresses emphasis on high quality transactions and items for finding frequent itemsets.

