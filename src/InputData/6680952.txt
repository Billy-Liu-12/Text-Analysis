Real-Time Effective Framework for Unstructured Data Mining

Abstract?Today, the enterprise landscape faces voluminous amount of data. The information gathered from these data sources are useful for improving on product and services delivery. However, it is challenging to perform knowledge discovery in database (KDD) activities on these data sources because of its unstructured nature. Previous studies have proposed the hierarchical clustering methodology since it enhances human readability and provides clear dependency structure through topics, term and document organization.

But, the methodology can be resource intensive and time consuming. In order to improve on the terms extraction process, we propose a tool called RSenter that searches through interconnected Hyperlinks and NoSQL database (specifically, CouchDB). We evaluate the tool based on search algorithms such as parallelization, random walk (or linear search), pessimistic search, and optimistic search. The tool shows high accuracy and optimality in view of the search time.

Keywords-data mining; hierarchical clustering; unstructured data; information extraction; topics; terms; big data

I. INTRODUCTION The ?big data? era has received warm reception due to  ease of data accessibility that it has brought to the enterprise landscape. Today, there is ever growing demand for the consumption of high-dimensional data across different enterprises such as Biomedical Engineering, Telecommunications, Geospatial Data, Climate Data and the Earth?s Ecosystems, and Capital Research and Risk Management [1-4]. It is now more affordable to deploy recommendation-based services and products provided useful information can be extracted from the data. More so, enterprises are shifting focus from paper and hardcopy-based file and document management to digital data for various reasons that promote business progress.

However, we are now faced with the challenge of extracting useful information out of the voluminous digital data which is forming at an exponential rate. The data is becoming debris because it is in unstructured format (e.g., blogs, comments, emails, multimedia, document files, etc.) and this prevents previous data mining techniques and knowledge discovery in database (KDD) processes from being applied [20]. These previous data extraction and retrieval processes were deployed with structured and schema-based storage in mind [5]. Thus, the study of unstructured data retrieval and organization (especially textual data mining) has been the focus of most studies who propose wide-array of methodologies such as: information retrieval algorithms based on templates [6], association rules  [7-8], topic tracking and topic maps [9], term crawling (terms) [10], document clustering [2, 11], document summarization [12], Helmholtz Search Principle [13], re- usable dictionaries [14], and so on. On the whole, most of the methodologies are based on Natural Language Processing (NLP) which is inherited from Artificial Intelligence and Linguistics [15-19].

The choice of methodology is based on the need of the user. There has not been any claim as to which approach is better since some of the methodologies focus on document content while some focus on repositories and their organization. But, each of the methodologies has their setbacks which users (consumers) have to understand before using them. In order to merge data from multiple sources, there have been extensive studies on building mashup frameworks [23-25, 28]. However, the challenge today is not just dependent on how to merge the data from multiple sources but the capacity to merge the data; considering that Twitter alone generates 12 Terabytes of unstructured data daily [26].

In this paper, we aimed at building a more cost effective tool, called RSenter, which performs optimal document and content hierarchical clustering. The approach we adopt aimed at combining topics, terms, and clustering methodologies with the result being parsed to a visualization output. The reason our project seek to implement a new tool is because we found existing tools time consuming to go through the entire document repositories. Further, we could not find any tool that supports topics and terms extraction for our NoSQL storages. Also, existing tools either focus on only Web contents (e.g. HTML or XML) or document organization. This phenomenon requires the use of multiple tools to identify the granularity of linkages and interdependencies between content and repositories. The success of RSenter is the deployment of a single tool that provides both content extraction and hierarchical clustering of terms from textual and NoSQL sources. In order to optimize the search time, we explored different search algorithms such as parallelization, random walk (or liner search), pessimistic search, and optimistic search. The final delivery is based on parallelization since our preliminary evaluation finds the algorithm more reliable. RSenter will be made open source (plus the code set) for enterprise adoption or adaptation.

The upcoming sections are organized as follows. Section II describes some of the attempts that have been made to extract useful information from the unstructured data.

Section III expounds the RSenter architecture and deployment, and Section IV focuses on the evaluations of the   DOI 10.1109/TrustCom.2013.131     RSenter tool. The paper concludes in Section V with our contribution and future outlook.



II. UNSTRUCTURED DATA MINING TECHNIQUES Unstructured data mining is now receiving attention from  the research community.  In this paper, we present the efforts on information extraction, topics, terms, and clustering.

A. Information Extraction Information Extraction (IE) forms the basis of today?s  KDD process where the data source is unstructured. The KDD process [8] considers series of activities including the search through databases, data warehouse, and other repositories where data cleaning and integration procedures are performed to remove outliers and so on. Then the cleaned data can be stored in a single data repository where knowledge base activities can be performed using data mining tools for pattern evaluation. These processes can be iterative until the final output is given mostly in a graphical format. In most of the existing KDD systems, the assumption is that the databases have a schema so data cleaning and integration is straightforward; however, current data sources are unstructured so the IE process is required to transform the data sources into a structure or semi-structure before the application of the data cleaning and integration process [6, 8, 21, 29].

The IE process is community-based. Meaning, the information to be extracted is dependent on the need of the user and the available data source. Hence, there is no all- purpose IE tool but rather, community-based tools which support specific domain data extraction tasks. For example, Al-Zaidy et al. [22] focus on mining data from a criminal community. The problem in such systems is the identification of invisible group or individuals in the criminal network. Hence, the authors focus on extracting profile data from available criminal networks. The process flow is in this order: fetch documents from a storage repository (HDD), tag names of persons, perform normalization of names, extract corresponding prominent communities (i.e., city, contact, and text summarization on the extracted text), construct community profiles, detect names that have indirect linkages to the communities, and visualization of the criminal network. Also, the challenge of language barriers has compelled researchers to opt for the deployment of language specific IE tools [16, 30]. As already hinted, there are quite a number of community-based works; some reported by Zaiane et al. [31] who focus on DBLP database by trying to identify the social relationships between the researchers within the community, and Segall et al. [32] who employ SAS Text Miner and Megaputer Polyanalyst to analyze hotel customer comments in a Web content. Also, LIPTUS [33] is developed to merge structured and unstructured data in a banking community while MEDREADFAST [34] performs similar functionality in a clinical community.

Information extraction can also be based on templates.

Hsu et al. [6] propose a template-based approach for textual mining and argue that if unstructured text can be converted into a semi-structure then syntactically, meaningful information can be extracted. Templates depend on the  structure, content, and format of a document and further focuses on the logical structure of a class of similar documents [6, 27].

There are other IE applications that are implemented on algorithms such as regular expressions, dictionaries, statistics (e.g., Bayesian or Hidden Markov Models), Preprocessing, Tokenizing, Tagging, Chunking (entity detection), State Transition Models, Spatial Data Analytic, and Relation Extraction Stages. Further, what is also important for consideration is the processing demand of the various IE procedures. Some of the approaches may be time consuming (e.g., construction of processing trees) and some may be resource intensive. In this regard, query performance aspects and measures of unstructured text mining processes should focus on the process specificity, process similarity, and process requirement to achieve coherency [36].

B. Topics Topics tracking is generally employed to recommend  subjects of interest to a user. Most online subscription systems (e.g., hotel booking, flight, news articles etc) are based on topics methodologies where keywords are extracted from users? subscription to form a basis for the users? interests [9, 11]. Based on these key words, a mechanism of grouping the related keywords (called Lexical Chaining) can be employed to extract subsequent published messages [9].

Topic Maps (ISO/IEC 13250:2003) on the other hand focuses on the representation and knowledge interchangeability in a repository [9]. Topic maps deal on the following elements (commonly referred to as TAO) as representations: topics?refers to the entities being referred to which are mainly names, events, application component and modules etc.; associations?graphical links between the topics of interest; occurrences?refers to the relevant linkages of the information to topics. Abramowicz et al. [9] researched on the automation of topic map creation and outline four procedures which can be adopted such as: subject recognition, information extraction and preparing, RDF modeling, and mapping RDF model into a topic map.

The basic idea is that the information extraction is initialized after the topic of interest is identified. Information Extraction at this point from the unstructured data source can be done by exploring other techniques such Natural Language Processing and Shallow Text Mining. RDF models are triples of the format subject (e.g., resources), property (e.g.

property type of the subject), and value (e.g., URL). RDF models label the corresponding objects to topics in a map and their occurrences and associations. However, mapping RDF model into a topic map has various processing procedures to follow which include One-time Processing, Repeated Processing, and Continuous Processing [9].

C. Terms Terms are different from topics in the sense that the focus of term crawling is to establish a network of associative relationships between features [9]. The features in this case can be any artifact. The main benefit of term mining is that, it optimizes the search space vector thereby reducing processing time instead of going through the entire document     as required for topics extraction [18-19]. Further, term crawling indicates the relevance of information gathered in an unstructured document. The term extraction module proposed by [19] performs Linguistic Preprocessing, Term Generation, and Term Filtering. Also, the metrics proposed for terms evaluation is based on the estimation of term frequency, relative frequency variation of a term, and inverse document frequency (tf-idf) to assign a score for information retrieval.

D.  Document Clustering Document clustering as explained by [9] and [11] is a  way of sub-categorizing documents into closely related identities. This requires a degree of similarity between the documents, or topics, or terms. The degree of similarity is simplified mathematically by Han et al. [2] as follows:  where V is the vector of the document. This similarity search is actually based on the Cosine Angle Distance which means that the cosine score of i and j will be zero (0) if they have no commonalities and one (1) if they are similar. The case of similarity also means that key issues that are of interest in clustering should be considered which are sensitivity and specificity. By sensitivity, we mean the degree of identification of actually related documents while specificity is the degree of identification of non-related documents.

The concept of clustering can be divided into two: K- means clustering and Hierarchical clustering. The reader is referred to [9] for further reading on the details of the various methodologies. But, the K-means clustering is criticized for the formation of poor clusters and it can be resource intensive. Hence, the hierarchical clustering is proposed to overcome the bottleneck in the former methodology.

The hierarchical clustering further supports clustering tree formation for human readability. The hierarchical clusters can be formed following the bottom-up approach or the top-down approach. There have been some innovative proposals for the implementation of hierarchical clusters. For example, [9] explains the use of multiple dictionaries with  split algorithms to form clusters, contrary to earlier methodologies that use a single algorithm on a single verbose dictionary. Also, Mohanty et al. [35] proposed an algorithm for hierarchical clustering called Balanced Iterative Reducing and Clustering Using Hierarchies (BIRCH) which has a complexity of O(n). The BIRCH methodology is aimed at using parallel cluster generation to optimize the clustering formation process.



III. RSENTER ARCHITECTURE The core functionalities of RSenter are: 1) the facilitation  of topics and terms extraction in a single textual document (e.g., PDF, XML, and RTF), 2) facilitate topics and terms extraction over interconnected documents (e.g., link traversal), 3) extraction in NoSQL databases (e.g., CouchDB, Amazon S3, etc.), 4) generate a hierarchical tree clustering based on n-depth search, 5) document organization, and 6) provide visualization outputs for the extracted features.

At the current stage of development, we have achieved the successful deployment of four of the functionalities which are: 3, 4, 5, and 6.

A. Architectural Overview and Test NoSQL Database The generic architectural overview of RSenter is shown  in Fig. 1. The entire implementation is C# based and we also have a JavaScript version that can be used as a plug-in in a browser. In order to explain further to the reader the nature of the data, we provide a snapshot of records for the same employee in two NoSQL (CouchDB) databases in Fig 2. The record of an employee ?Richard Lomotey? is kept by the IT unit in Fig 2a while the Human Resource unit keeps record of the same employee in Fig. 2b. Presently, we cannot query the two CouchDB databases in a single join operation (as in the case of SQL databases) and as shown, the two databases have different record structure of the same person. This leads to challenges such as a) divergent view of records in silos, b) lack of standardized schema, and c) duplication of records, and so on.

File and Document Repository  Document Organization  Keyword Extraction  Thesaurus Engine  Topics Terms  Search Algorithm  NoSQL  Linear  Pe ss im is tic  O pt im is tic  Pa ral lel  Serializer  Hyperlinks  Clustring Data (JSON)  Search Criteria  Visualization  Topics Clustering  Terms Clustering  Document Clustering  Unstructured Data Source  Figure 1. Architectural overview of RSenter      Figure 2. Sample record of the same employee (a) in the database of IT Unit and (b) in the HR Unit database  Further, there are no links or pointers between records in the same CouchDB database so it is challenging to perform terms extraction within the same database without crawling the entire database. This is why it is important to build a tool that can optimize the search duration.

B. The Search Flow The purpose of the Search Criteria component is to  enable the user to specify whether the intended activity is keyword (artifact) extraction or document organization.

Currently, the latter is not implemented so we shall discuss the process flow of the former.

Keyword Extraction: We describe our IE process as keyword extraction. Technically, RSenter supports both topics extraction and terms crawling. When the intention of the user is to perform a keyword extraction activity, the Thesaurus Engine which is a dictionary is activated. Just as other related works, the thesaurus is pre-populated with features (keywords) of interest from the community that is using it. These keywords can include contract related terms, employee records, payment transactions, and so on. The reason for the thesaurus is also to avoid challenges with keyword identification that arises as a result of semantic issues in data mining. The thesaurus may contain a keyword before the time of extraction or the keyword may be added at the time of extraction. Topic extraction in RSenter assumes the user wants to extract a particular element or predefine jargon or word within the unstructured data source (i.e., the NoSQL database). Terms extraction on the other hand involves the extraction of words and their synonyms or other artifacts that are dependent on the search term. Thus, terms extraction facilitates the organization of topics into a structure and shows their dependencies. For instance, in the thesaurus, the term ?employee? may be represented as [employee: ?payroll?, ?profile?, ?employment contract?].

Hence, the extraction of employee data from the data source  will include all the information in the tuple as defined in the thesaurus. The importance of implementing the thesaurus is also to aid us perform topic tracking and repetition of topic and term extraction activities. Moreover, RSenter can be employed in other communities by populating the thesaurus with that community?s data. In the future, we will extend on the current system by building an adaptive learning component into the extraction process with the help of the thesaurus.

Document Organization: Though currently not implemented, we have finalized its design. This search is performed when specific documents and files are being searched in a repository. Document search is not dependent on the thesaurus; rather, RSenter directly calls the search algorithm component to perform the search for the specified documents.

Unstructured Data Sources: This is the data source that requires the information extraction process. The overall aim is that, RSenter will be able to search through multiple data sources such as NoSQL database (e.g., CouchDB, Amazon S3, MongoDB, Cloudata, etc), entire websites (with hyperlinks), file repositories, flat file storage (e.g., XML), and semi-structured textual sources in single files such as PDF and RTF. However at present, we have accomplished and reported only the NoSQL component.

Search Algorithms: The moment the search criteria are determined and the user chooses to perform topic extraction, term extraction, or document search, the search algorithm engine is activated. The search algorithm is applied directly to the unstructured data source based on the user?s search preferences. We have proposed the following search algorithms: parallel search, random (linear search), pessimistic search and optimistic search. We explored the proposed algorithms in order to examine the best methodology that meets our need; especially, optimization of information extraction time. Thus, the current RSenter     version which is in use employs the parallel search methodology since it?s the most reliable based on our evaluation indices (to be discussed later in the next section).

Further, all the search methodologies follow the directed graph approach.

Hence, the search methodologies determine the depth of the unstructured data and traverse the layers from top to bottom. For example, considering a hyperlink of HTML documents as illustrated in Fig.3, for each methodology, the extraction process starts from the root which is node 1 (or the first record in the NoSQL databse).

Figure 3. Directed graph  As already posited, there are no pointers to records in our use case NoSQL database so we have to go through the entire database to search for terms. This appears that the record is flat but technically, we have to design it as graph so that we can keep track of which terms or topics are dependent during the query process. For example, if we see that node 2 is related somehow to node 1, then we have to build a directed graph between the two nodes in our array.

Also, with the directed graph, we adopt the hyperlink search model from [5] where we keep two attributes (i.e., the Nodes and Link) after visiting each node. These attributes are represented as tuples in an array such that Nodes = [url, title, format, size, date, topic or term (text)] and Link = [source- url, target-url, label, link-type].

The Linear (Random) Search: This is the commonest algorithm on which most existing data mining tools are built; probably because it is straightforward to implement and it has a complexity cost of O(n). Our linear search methodology explores an entire vector (search) space starting from the current node (root) and going through the child nodes one after the other. So in Fig. 3, the linear search from the root will consider child nodes 2, 3, and 4. Then it will go to node 2 and will reach a dead-end. After which it will go to node 3, and will also determine that node 5 is a child of node 3. It will then visit node 5 and determine child nodes 6, 7, 8, and 9. It will further search through these individual child nodes and reach a dead-end in each case. After that, it will come back to node 4 and determine that node 8 is a child of node 4. But, in this case, it will not visit node 8 because it has already visited node 8 when it traversed node 3 and node 5 links. This measure of not visiting an already visited link makes the search not to enter into an infinite search loop.

Similarly, the algorithm can be described as a recursive search algorithm because of how it searches through the links and determines which node to visit next from the current node.

Further, we refer to the model as random because the system determines which child node to visit without user intervention. For instance, the choice of traversing node 2, 3, or 4 as the first choice follows no particular order. The only criterion is every child node must be visited.

Parallelization: Our proposed parallel search employs the linear search in two concurrent functions but randomness is not permitted. Since the search is in a directed graph, the search for topics, terms or document starts from the root node and depending on the number of child nodes, two concurrent functions are called that divides the search space by numbering nodes as even and odd. Assuming the entire vector space is N with layers such that N = {1, 2, 3? n}.

Then, for each layer two linear functions, X and Y are called such that, X = {search through odd numbered nodes of N} and Y = {search through even numbered nodes of N}. So, for every node that has more than one child node, the linear functions X and Y will be activated in order to halve the search space vector in the previous linear search algorithm.

After all the layers in N are visited and the topics, or terms, or document are extracted, the Serializer component is activated which combines all the search terms into a single extracted document structure just as the linear search. In the RSenter architecture, the serializer works only with the parallel search in order to integrate the results from the segregated functions. That is, combining the array values of the various search results into a single array with duplicates eliminated.

In order to explain the concept of the parallel search further to the reader using Fig. 3, we can say that N = 4 layers counting from the root node (i.e. node 1). The first layer which has the child nodes of the root are three nodes (i.e., nodes 2, 3, 4). So the two parallel linear functions will be activated and the function X will search node 3 and function Y will search node 2 and 4. Node 2 is a dead-end and node 4 has a child node 8 so the Y function will be called to search through node 8 (since its only one node and we treat one node as odd) and then we reach a dead-end.

Node 3 however will be searched by Y and then node 5 will be found as a child. The search space in node 5 will be divided among X and Y again since the child nodes are more than one. X will search node 6 and node 8, and Y will search node 7 and node 9. Now there are two cases to explain here.

There is the repetition of searching through node 8 because the initial search was done by Y and in the second case, the search is being carried out by X. So, during the serialization of the results, the duplicate nodes will be eliminated.

However, assuming the use case is different and Y initially searches through node 8 and then based on the search space division between X and Y at node 5 it happens that Y is assigned to node 8, the search will not be carried out all because Y already knows that it has visited 8. It works just as the linear search algorithm and this further saves cost (time).

Pessimistic and Optimistic Search Algorithms: Pessimistic and optimistic constants have been employed extensively in software engineering to accomplish tasks such as the detection of the existence of software artifacts in source code, traceability link detection, bug triaging and fixing, and so on [36]. The pessimistic search assumes that     the search term may not be found based on some probability while the optimistic search is the reverse. The reason we found the pessimistic and optimistic search relevant to our work is because we employ it to search for the existence of topics, terms, and documents in the unstructured data source but not necessarily the links and the dependencies of the keywords. In essence, the pessimistic and optimistic search is a linear search but with restrictions as to which sources to search or not. For example, looking for employee ?Richard Lomotey? from a pile of digital files, the pessimistic search will ignore all nodes (files) that have nothing to do with human resources (HR) because it assumes the specified employee record can only be found in HR related documents (i.e., if that is how the user search criteria settings are). In that case, the search is not going to go through all the nodes for example in Fig. 3 if the algorithm assumes that the topic cannot be found in node 2 or 4 or some other node. The search is user specified hence, the user has to provide the search term and also provide the document names or files or directories that should not be searched. The optimistic search on the other hand is when the user provides a search term; and then specifies files or directories in which the user thinks those search terms can be found.

Figure 4. Visualization clustering tree showing the result for the search term ?R. Lomotey? who is an emplyee. RSenter searched through all the unstructured data debris and classified the data based on the depth of the  space vector. The yellow label shows the link that is being traversed.

Hierarchical Clustering Data and Visualization: After the information extraction process is complete based on the adopted search algorithm, the extracted result is modeled as JSON data. The extracted information is an array so it can be saved in a NoSQL database (e.g., CouchDB), Relational Database, or File Systems. Depending on the search criteria, the JSON data provides topic clustering, term clustering, and document clustering. Currently, the RSenter output relies on the JavaScript InfoVis Toolkit [37] to display results. Fig. 4 shows an output of a search term through unstructured NoSQL and textual sources.

C. Current Limitations At the moment, RSenter is not generic enough because it  is limited only to the CouchDB NoSQL database or databases that are similar to CouchDB such as MongoDB.

Also, when we look at the databases, as shown in Fig. 2, they contain attachments which are textual documents which can require mining. For example, in extracting the topics and terms, it will be appropriate if those attachments can be searched for valuable information which is the case in most cases we run into.

Our plan is to target these limitations and advance on the RSenter framework. The advancement is under way and will be reported in a scientific forum soon.



IV. EVALUATION METRICS Using the C# version of RSenter, we conducted thorough  evaluation based on our desired metrics. We deployed RSenter on our Window based system with the following specifications: Windows 7 System 32, 3.20 3.12 GHz, 4GB RAM, 1TB HDD. We considered a training data set from our research partner of 12 million entries in a NoSQL database (CouchDB). The data is divided into 48 categories with subcategorized data of approximately 21000 data sets. For instance, we have categories and sub-categories organized as: [HR: Employee Records, Terminations, Promotions, Hiring], [IT: Employees, Duties, Deadlines, Deliverables], [Admin: Payments, Clients, Contracts], and so on. On the average, the hierarchical depth of the search space is 7 layers. The entire evaluation focused on term extraction because we considered it as an advanced form of topic extraction with dependencies. We evaluate the framework based on the training data set from the NoSQL and provide the overall overview of the experimental result in Table I.

True Positive (TF) refers to the extraction of desired terms.

False Positive (FP) refers to the extraction of desired terms from the unstructured data but in reality, the terms do not exist in the data source. True Negative (TN) is when the term is not found because it does not exist. False Negative (FN) is when the term could not be found but it exists in the data source.

TABLE I. EVALUATED METRICS OF THE PROPOSED ALGORITHMS  Linear Search  Parallel Search  Pessimistic Search  Optimistic Search  True Positive 100.00% 100.00% 81.40% 90.10%  False Positive 0.00% 0.00% 0.00% 0.00% True Negative 100.00% 100.00% 99.70% 99.70% False Negative 0.00% 0.00% 30.70% 31.20%  Precision 100.00% 100.00% 100.00% 100.00% Recall  (Sensitivity) 100.00% 100.00% 72.61% 74.28%  Specificity 100.00% 100.00% 100.00% 100.00% Accuracy 100.00% 100.00% 85.51% 85.88%  Type I Error 0.00% 0.00% 18.60% 9.90% Type II Error 0.00% 0.00% 0.30% 0.30%  Based on the four classification context, we calculate the Precision, Recall (a.k.a. Sensitivity), Specificity, Type I Error, and Type II Error. The TP value of the pessimistic search is 81.4%, less than the other algorithms because, sometimes the user specifies directories that should not be searched but in reality, those directories contain the required data. This has also affected the other metrics as shown in Table I. The optimistic search has a high TP because mostly,     we are certain of the directories to search based on previous usage. The user behavior has equally introduced the FN error in the pessimistic and optimistic search. This is because when the algorithm returns not found result, it is because the user specified directories that truly did not contain the search term but those results exist in non-specified areas. Further, we achieved 100% precision, recall, and specificity, and accuracy in the case of some algorithms because we have the FP and FN values as 0%.

Next, we evaluate the time for traversing the entire dataset for the NoSQL (Fig. 5). In Fig. 5 the approximate number of vocabularies in the thesaurus that we focused on extracting is 40000.

Figure 5. Document Search Time  In Fig. 5, it takes approximately 55.11 minutes on average to extract 40000 vocabulary-based terms from the 12 million unstructured NoSQL documents when the linear search is employed. The records are unstructured, semi-and semi-structured. For the same data set, our parallel algorithm accomplishes the search task averagely in approx. 27.19 minutes. This means the percentage cost saved in terms of search time is 102.7%.

We did not evaluate the optimistic and pessimistic search because the two are dependent on the user?s specifications.

So, a user can decide to search through a fraction of the document or the full document. Since there are multiple search preferences for users, it is difficult for us to capture results that can reflect a user?s behavior. But, since the two algorithms are also linear dependent, the points on the linear graph gives a hint of the approximate search cost in terms of the time required to search through the units of documents.



V. CONCLUSION This paper introduces RSenter; a framework for topics,  terms, and document extraction from unstructured data sources. The dawn of big data has left in its wake debris of data that are unstructured; a phenomenon that has rendered most previous data mining techniques non-applicable since they are designed for schema-based data sources.

Unstructured data comes in varying forms such as textual, NoSQL, multimedia files, documents, etc. Further, these documents are scattered across multiple platforms.

The aim of RSenter is to achieve data extraction from NoSQL sources, file systems, and network of hyperlinks.

Currently, we have reported the successful implementation regarding NoSQL database search. Specifically, we built a thesaurus that contains human resource information, contracts, procurement, and so on. Users can extract topics, terms, and documents; and the output is presented as a hierarchical clustering tree.

The RSenter framework is built and tested based on the following algorithms: parallel search, linear (random) search, pessimistic search, and optimistic search. Currently, the deployed version is running on the parallel algorithm because it is deemed by our research partners as the most reliable algorithm. When made open source, RSenter can be adopted by any community through the modification of the thesaurus. The future direction of the project is to incorporate an adaptive learning component that could lead to automated search and the deployment of a reasoning system. Also, we shall soon report the deployment of the other search requirements regarding documents and hyperlinks.

