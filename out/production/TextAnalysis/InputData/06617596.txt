Frequent Itemset Mining on Hadoop Ferenc Kova?cs

Abstract?One of the most important problems in data mining is frequent itemset mining. It requires very large computation and I/O traffic capacity. For that reason several parallel and distributed mining algorithms were developed. Recently the mapreduce distributed data processing paradigm is unavoidable and porting the current algorithms to mapreduce is in focus.

In this paper a substantial frequent itemset mining algorithms and their mapreduce implementations are introduced and investi- gated. An algorithm improvement is also proposed and analyzed.



I. INTRODUCTION  With the help of data mining and knowledge discovery it is possible to discover useful or interesting, non-trivial information in large and possibly noisy datasets. One of the most researched field of data mining is association rule mining (ARM) [2]. The basis of the ARM is the frequent itemset mining (FIM). There were many algorithms proposed for asso- ciation rule learning since the original [2] was described, some fundamental improvements can be found in [3] [4] [5] and [6]. Apriori algorithms [2] is one of the best known frequent itemset mining solution. The other substantial algorithm for FIM is the FP-growth that was introduced in paper [4]. It has significantly lower I/O cost than Apriori based solutions but it has huge memory needs. As it tries to compact the database into the memory using a special data structure, so-called FP- tree.

Several distributed FIM algorithm were were introduced as FIM is complex and I/O intensive task. These solutions are usually based on Apriori algorithms the fundamental Count Distribution (CD) and Data Distribution (DD) algorithms are introduced in [7]. The CD algorithm has better performance as it generates lower network traffic. As the mapreduce [12] paradigm and frameworks became popular several mapreduce based solutions were introduced in recent years [13] [14].

The usage of multi processor multi and core processors raises some questions in field of FIM. Paper [8] describes fundamental solutions on multi processor systems multi core solutions are investigated in [20] [10] [11]. Recently, the usage of GPUs is in focus of the parallel FIM solutions [18] [19].

This paper organized as follows: Section 2 and Section 3 give an overview of the fundamental FIM solutions, which are the basis of developed solutions. Next section provides an overview of mapreduce algorithms, Hadoop framework and FIM implementations on it. In Section 5 we introduce two  solutions for FIM on Hadoop and experimental results are in Section 6. Finally conclusions are provided.



II. PROBLEM STATEMENT  First we elaborate on some basic concepts of associa- tion rules using the formalism presented in [2]. Let I = {i1, i2, . . . im} be set of literals, called items. Let D = {t1, t2, . . . tn} be set of transactions, where each transaction t is a set of items such that t ? I . The itemset X has support s in the transaction set D if s% of transactions contains X , here we denote s= support(X). An association rule is an implication in the form of X ? Y , where X,Y ? I and X ? Y = ?.

Each rule has two measures of value, support and confidence.

The support of the rule X ? Y is support(X ? Y ). The confidence c of the rule X ? Y in the transaction set D means c% of transactions in D that contain X also contain Y, which can be written in S(X ? Y )/S(X) form. The problem of mining association rules is to find all the rules that satisfy a user specified minimum support and minimum confidence.

If support(X) is larger than a user defined minimum support (denoted here min sup) then the itemset X is called frequent itemset.

The association rule mining (ARM) can be decomposed into two subproblems:  ? Finding all of the frequent itemsets ? Generating rules from these large itemsets  Most of the ARM algorithms are mainly different from each other in frequent itemset mining step as the second sub problem is much easier than the first one.



III. BASIC ALGORITHMS  In this section two fundamental algorithms are introduced: Apriori and CD as these are the basis of the provided solutions that is introduced in Section 5.

A. Apriori algorithm  The Apriori algorithm [1] uses the following theorem to reduce the search space: if an itemset is frequent then all of its subsets are frequent as well. This means it is possible to gener- ate the next potentially frequent i+1 itemset, called candidate, using the current frequent i itemset. Each subsets of candidate i+1 itemset must be frequent itemset. Hereby it is possible to find all frequent itemset using database scan repeatedly. During the ith database scan it counts the occurrence of the i itemset  ? 241 ?      and the end of the pass i, it generates the candidates, which contain i + 1 item. The main disadvantage of this algorithm is the multiple database scan. Algorithm 1 shows the pseudo code of the Apriori algorithm. The notations, which are used in in the pseudo code of algorithms, are introduced in Table I.

TABLE I: Notations  k itemset Itemset having k item  Li Set of frequent i temset  L Set of frequen itemset  Ci Set of candidate i frequent itemset  Dlocal Local part of the dataset  Clocali Local copy of Ci |A| Number of elements in set A  Algorithm 1 Apriori algorithm L1 =countItems(D) C2=generateCandidate(L1) i=2 while Ci 6= ? do  for all t ? D do incrementCounter(Ci, t)  end for Li=generateFrequents(Ci,min sup) i++ Ci=generateCandidate(Li?1)  end while return  ? i  Li  B. Count Distribution Algorithm  The count distribution (CD) algorithm [7] is a fundamental distributed association rule mining algorithm. The basic idea of this algorithm is that each of the nodes keeps large itemsets and counters of candidates locally, which are related to the whole database. These counters are maintained in accordance with the local dataset and incoming counter values. The nodes locally execute the Apriori algorithm and after reading through the local dataset they broadcast own counters to the other nodes. Hence each of the nodes can generate new candidates on the basis of the global counter values. Algorithm 2 shows the pseudo code of the CD algorithm. A drawback of this algorithms is that the itemset counting phases are always synchronized to the slowest node.



IV. HIGH PERFORMANCE COMPUTING SOLUTIONS  As cheap hardware based clusters become more accessible, the need for more general and easier to use frameworks and models had rise. New programming paradigms appeared which make better use of the available computing power with the goal of making it easier to use for the end user.

Algorithm 2 Count Distribution Algorithm  Dlocal=reciveDataset() Clocal1 =countItems(D  local) broadcast(Clocal1 ) L1=gatherCounters() Clocal2 =generateCandidate(L1) i=2 while Ci 6= ? do  for all t ? Dlocal do incrementCounter(Clocali , t)  end for broadcast(Clocal1 ) Li=gatherCounters() i++ Ci=generateCandidate(Li?1)  end while return  ? i  Li  A. Mapreduce  MapReduce was introduced in 2004 by Google [12].

MapReduce is a programming model and the first framework which implemented it also called Mapreduce. Google?s im- plementation is proprietary but the underlying technological idea is not owned by them, they made it popular as being part of their search engine?s core technology. The paradigm was designed to work on really large datasets.

The main idea behind Mapreduce is to split the problem the computation into two subproblems: a ?map? and a ?reduce? phase. First, the input date get sliced into smaller chunks and every chunk is given to a machine that executes a mapper.

Mapper processes that chunk and provides key value pairs from it. Than mapreduce framework collects these pairs and passes them to reducers. A reducer receive a key and a a list of values that belongs that key and provides the results.

This approach hides the problem of data distribution and workload balance, the programmer can focus on the algorithm.

There are some limitations of course, the mapreduce paradigm works only on key-value pairs, input and output data has to be key-value pairs, as well as the intermediate data that moves between the map and reduce phases.

Some data processing tasks nicely fit to this paradigm, and the conversion of the sequential algorithm is straightforward.

Translating other problems and algorithms to the mapreduce paradigm may raise challenges or simply prove unrealizable.

The challenges of porting an algorithm and in using a mapreduce framework include the separation the algorithm into two, non-communicating map and reduce phase. Another issue is that a paradigm is consists two steps, the map and the reduce phase, the computation only gives a final result after the final computer in the reduce phase finishes its job. Porting a recursive or cyclical algorithm means that we have to restart the map-reduce loop many times before reaching the desired result.

F. Kov?cs and J. Ill?s ? Frequent Itemset Mining on Hadoop  ? 242 ?    B. Hadoop The original mapreduce implementation is proprietary, but  there are multiple implementations of this data processing paradigm.

Hadoop [15] is an open-source software framework, under the supervision of the Apache Foundation [16], which im- plements the mapreduce paradigm. Hadoop was implemented from the publicly available papers and it was designed to run on large number of machines. Hadoop itself was written in Java, but it provides range set of APIs for different languages and it can read plain-text and xml from the standard input so it can be programmed in basically any programming language.

One of the main component the Hadoop architecture is a distributed file-system called HDFS. HDFS transparently distributes the data across the machines in the Hadoop cluster through the network. HDFS uses data replication to make sure the data is available even after multiple computer failures.

The system is not disrupted by hardware failure, unfinished computations are moved to other machines in the cluster.

This simple, two stage separation and share-nothing archi- tecture can be used to implement large range of computa- tional and data mining problems. A collection of basic data mining and knowledge discovering algorithms available as Java libraries for the Hadoop ecosystem, packaged together and called Apache Mahout [17]. Mahout contains several algorithm implementations but they seem an algorithm heap and not a consistent framework.

C. FIM on Hadoop Mapreduce paradigm requires key-value pairs for input and  output, so first the dataset has to be converted to this format.

Transactional databases can be converted to key value pairs where the key is the transaction ID and value is the set of items from the transaction. The algorithms have to use key-value pairs to represent their intermediate state, which is passed to the reduce machines at the end of the map phase. The main problem is to construct the temporal data structures as key- value pairs.

The fault-tolerant and cheap nature of Hadoop based clus- ters led to a newfound interest in porting existing algorithms to the mapreduce programming framework. Such work include porting the classical Apriori algorithm to mapreduce [13] [14].

Algorithm 3 and 4 show the pseudo code of the mapper and reducer of the implementation [14].

Algorithm 3 Mapper of the Apriori Algorithms [14] Li?1=readFromHDFS() Ci=generateCandidate(Li?1) for all t ? Dlocal do  for all c ? Ci do if c ? t then  appendOutput(c,1) end if  end for end for  Algorithm 4 Reducer of the Apriori Algorithms [14] receive(c, valueList) for all v ? valueList do c.counter+ = v  end for if c.counter > min sup ? |D| then Li = Li ? c saveToHDFS(Li)  end if

V. PROPOSED SOLUTIONS  [14] proposes a simple solution where the mappers read through their local dataset and check if a candidate occurs in an itemset and the reducer counts the occurrences one by one.

The main problem is that this solution overloads the reducer and huge amount of network traffic is generated as well.

A. Porting Apriori Algorithm  The Count Distribution algorithm is a basic distributed implementation of the Apriori algorithm and it is easy to adopt to map reduce. Algorithm 5 and 6 shows the pseudo codes of the appropriate mapper and reducer. Figure 1 gives the communication overview of the algorithm. This solution reduces the network traffic and the load of the reducer.

Algorithm 5 Mapper of the Apriori Algorithms Ci=readFromHDFS() for all t ? Dlocal do  for all c ? Ci do if c ? t then  c.counter ++ end if  end for end for for all c ? Ci do  appendOutput(c,c.counter) end for  Algorithm 6 Reducer of the Apriori Algorithms for all c ? Ci do  receive(c, valueList) for all v ? valueList do  c.counter+ = v end for if c.counter > min sup ? |D| then  Li = Li ? c end if  end for Ci+1=generateCandidate(Li) saveToHDFS(Ci+1)  ? 243 ?     split_0  split_1  split_2  mapper  mapper  mapper  reducer  candidate support  candidate support  candi date s  uppo rt  Ck candidate item-sets  input dataset  Fig. 1: Data flow layout between mappers and reducers  B. Algorithm improvement  Several Apriori-based algorithms were developed but the modifications are focused on the reduction of the number of database scans. However the Apriori-based algorithms have some other drawback. The role of the 2 frequent itemsets is fundamental therefore it is useful to take care of the 2 candidates in special way. It is not necessary to generate 2 candidates let handle all 2 itemsets as candidates. In this case the itemset counters can be stored in a triangular matrix. This triangular matrix can be indexed by the itemset identifiers.

Hence this matrix contains counter values of each 2 itemsets and the diagonal of the matrix can be the placeholder of the 1 itemset counters. The advantages of this solution as follows:  ? It is not necessary to generate the 2 candidates ? It is possible to count the 1 and 2 itemset in one step ? Each the counters can be reached in O(1) time ? It is independent of the size of database, it depends only  on number of items  However the memory requirement of the triangular matrix is O(n2) but it does not cause problem for many applications, where the number of items are limited e.g: web log mining or census data analysis. The Alogithm 7 introduces the modified mapper pseudo code and Figure 2 explains the commination overview of the first step.

split_0  split_1  split_2  mapper  mapper  mapper  reducer2-itemsets with support  2-item sets w  ith su pport  2-itemsets with support  C3 candidate itemsets  input dataset  Fig. 2: The first step of the improved Apriori implementation  Algorithm 7 Mapper of the improved Apriori algorithm if i = 1 then C2 = createTrianguarMatrix(|Items|) for all t ? Dlocal do  for all i1, i2 ? ItemPairs(t) do C2[i1][i2] + +  end for end for for i = 0; i < |Items|; i++ do  for j = i; j < |Items|; j ++ do appendOutput(ij,C2[i][j])  end for end for return  else for all t ? Dlocal do  for all c ? Ci do if c ? t then c.counter ++  end if end for  end for for all c ? Ci do  appendOutput(c,c.counter) end for  end if

VI. EXPERIMENTAL RESULTS The experimentation took place in the PC laboratory of the  department using 10 uniform PCs having a 2.1 GHz Intel Core2 processor and 4 GB RAM, the nodes were connected through a GBit switching hub. The datasets used by the algorithms were generated by IBM synthetic data generator [1]. The provided diagrams are based on T20I7D500K dataset.

This means that dataset contains 500K transactions, the aver- age size of a transaction is 20, and the average size of the maximal frequent itemset is 7. The mappers and reducers of the introduced algorithms were implemented in Python and the used map reduce framework was hadoop 1.0.6. The simulation result is shown Figure 3 and 4.

Figure 3 shows the response time and the scale up capability of the Hadoop based CD algorithms and same parameters of the proposed algorithms are depicted on Figure 4. They show that the proposed algorithm has better response time.



VII. CONCLUSION In this paper mapreduce based frequent itemset mining  algorithms were investigated. We provide a solution for porting CD algorithm to mapreduce and introduced and improvement on it. for The bases of it that it handles the 2 itemsets in special way therefore it can significantly reduce the response time of the Apriori algorithm. The basic distributed association rule mining algorithms are based on the Apriori algorithm hence they inherit all of its problems. The introduced speed up solution can be used in any kind of Apriori-based algorithm.

