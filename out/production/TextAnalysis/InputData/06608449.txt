

A Novel Fuzzy Associative Classifier Based on Information Gain and Rule-Covering    Yue Ma, Guoqing Chen, Qiang Wei Research Center for Contemporary Management  Key Research Institute of Humanities and Social Sciences at Universities School of Economics and Management, Tsinghua University, Beijing 100084, China  may.10@sem.tsinghua.edu.cn  Abstract ? Fuzzy Associative Classification has attracted remarkable research attention for knowledge discovery and business analytics in recent years due to its merits in accuracy and linguistic modeling. Furthermore, it is deemed meaningful to construct an associative classifier with a compact set of rules (i.e., compactness), which is easy to understand and use in decision making. This paper introduces a novel fuzzy associative classification approach called GFRC (i.e., Gain-based Fuzzy Rule-Covering classification). Two desirable strategies are developed in GFRC so as to enhance the compactness with accuracy. One strategy is fuzzy partitioning for data discretization, in that simulated annealing is incorporated based on the information entropy measure; the other strategy is a data- redundancy resolution coupled with the rule-covering treatment.

Moreover, data experiments show that GFRC had good accuracy, and was significantly advantageous over other classifiers in compactness.

Keywords-Associative Classification, Information Gain, Fuzzy Partition, Simulated Annealing, Rule-Covering   1.  INTRODUCTION  Fuzzy associative classification has been an active area of research for recent decades in knowledge discovery and business analytics combined with extensions of fuzzy logic [1]. Discovering an understandable set of rules to form an accurate and efficient classifier is one of the most addressed data mining tasks [2]. The understandability depends on both the compactness of a rule set and the capability to build a linguistic model.

Most fuzzy associative classification approaches are fuzzy extensions to the classical ones. Among various directions [3, 19, 28], four key aspects include (1) fuzzy operators; (2) the measures to the certainty of fuzzy rules; (3) the classification output mechanisms; and (4) fuzzy partitioning of continuous attributes. They affect the performances of the classifiers in different ways.

As far as fuzzy partitioning is concerned, the existing efforts could be divided into two categories [4]: single-layer vs. multi-layer. In a single-layer method, the fuzzy partition is pre-specified with respect to the domains of continuous attributes, while, in a multiple-layer method, multiple fuzzy partitions can be obtained in the process of learning class associative rules. The CHI algorithm [10,11] is a typical single-layer method. It divides the data set into several fixed fuzzy grid cells, and mines a rule for each cell based on the  transaction with maximum membership degree to it. Data clustering algorithms [6-8] are also used by the single-layer methods for fuzzy partitioning. Another recent effort is FURIA [3], which is a fuzzy extension to the RIPPER algorithm [5], and fuzzifies rule antecedent intervals in a greedy manner attempting at maximizing the classification accuracy. However, the single-layer methods suffer from the strong impact of the chosen partitioning method and the fixed number of partitioned intervals. On the other hand, a multi- layer method proposed by Ishibuchi et al. [12] employs multiple fuzzy rule tables to solve several partitions with different sizes simultaneously, aimed at reducing the dependency of classification on the single-layer partitioning.

But a limitation is that the number of generated fuzzy rules would be usually enormous. Subsequently, related efforts [13,14] try to combine the multi-layer partitioning with several heuristic methods so as to optimize the rule sets.

Additionally, SLAVE [15,16] uses a genetic algorithm to generate a set of rules in order to sufficiently represent the training set. Since the rule set optimizing processes are generally complex, and different granularities of partitioning are used, the computational efficiency and understandability of multiple-layer classifiers are the issues of concern.

Furthermore, due to the fact that associative classification is of the association rule discovery nature, the number of discovered class association rules is huge. This leads to a difficulty in getting a full grasp of the data features as well as in resolving possible rule redundancy and conflicts. In this regard, an effective approach, namely GARC (Gain based Associative Rule Classification) [17] is proposed to produce a classifier with satisfactory classification accuracy as well as a much smaller size of generated rule set. The characteristic of GARC-type approaches [17-20] is threefold: first, applying the information gain measure to select the best-split attribute for 1-itemsets, which is to be included in the generation of candidate k-itemsets; second, integrating the process of frequent itemsets generation with the process of rule generation; third, defining rule redundancy and rule conflicts, and incorporating corresponding strategies for rule pruning into the mining process.

It is worth mentioning that these GARC-type approaches are crisp ones, and fuzzy extensions are considered possible and meaningful. One aspect of consideration is to extend its crisp data discretization (e.g., [21]) into a gradual partitioning     so as to solve the ?sharp boundary problem? to avoid misclassification. Another aspect of consideration is that in GARC the pruning strategies are executed at the rule level, while it is deemed necessary to measure the usefulness of rules from a global perspective in light of the whole dataset.

Moreover, the classification ability of the rules to the whole dataset is considered in terms of coverage degree so that the number of rules can be further reduced. With these considerations, this paper is aimed to present a novel fuzzy associative classification approach called Gain-based Fuzzy Rule-Covering classification (i.e., GFRC), which is a fuzzy extension to GARC. As will be discussed in detail in the next sections, GFRC employs fuzzy partitioning using a simulated annealing strategy to search for the number of global optimal partition intervals with the boundaries having highest information gains [21]. In addition, GFRC further reduces the number of class association rules in light of coverage degree by incorporating a rule-covering strategy to prune the redundant rules.

2.  GAIN-BASED FUZZY RULE-COVERING CLASSIFICATION  2.1.  Fuzzy Partitioning and Extensions  2.1.1. Fuzzy partitioning with simulated annealing Fuzzy partitioning [22, 23] is used for transforming  quantitative datasets to fuzzy datasets. Generally, suppose there are n transactions 1 2{ , ,..., }nt t t  in an original dataset D.

Given an attribute A in D, suppose Domain(A) is fuzzy- partitioned into a series of v attributes 1 2A ,A ,...,Av , where qA ( 1,2,..., )q v= is a fuzzy set. In this study, these fuzzy sets are defined by trapezoidal membership functions, which are concise, and commonly used in fuzzy partitioning methods [2, 3, 19].

For each of such fuzzy sets (intervals) with a trapezoidal membership function, it can be described by four parameters, i.e., : ( , , , )a b c d? ? , and the membership function is of the following form:      ( ) ( )  ( ) ( ) ( )   x a b a a x b b x cx  d x d c c x d others  ?  ? ?? ? < ? ? <?= ? ? ? ? <? ??  .     (1)    Notably, when a = b and c = d, the fuzzy set degrades into a crisp interval.

Thus, the fuzzy partitioning of Domain(A) can be described via an n v? matrix, where each element ijx means the membership grade of it  to jA . The matrix has the  following properties: (1) [0,1]ijx ?  ; (2)  1, v  ij j  x i =  = ?? ; (3)  0 ,  n  ij i  x n j =  < < ?? . Therefore, for the fuzzy partitioning with trapezoidal membership functions, we shall  have 1i ia c ?= , 1i ib d ?= ( 2,3,..., )i v= , and 1 1 inf{A}a b= = , sup{A}v vc d= = .

With this form of fuzzy partitioning, the next important question is how to determine the number of intervals, the degrees of interval overlapping, and the positions of interval boundaries [25]. In classical discretization such as [21], information entropy is used to select cut points, with the minimum description length principle (MDLP) for deciding the number of intervals. This method of sharp partitioning is considered effective; however, every binary cut point is a local optimum, resulting in a final partition that may hardly be of the global entropy minimization.

The global optimization methods such as simulated annealing (SA) can search the solution space for a global optimum. Numerous attempts have been made to apply such methods to the design and implementation of fuzzy systems [13-16]. They could provide viable solutions to finding optimal or near optimal configurations for large scale problems. In this paper, the simulated annealing is used for the purpose of global entropy minimization.

Concretely, in fuzzy partitioning, the fuzzy information entropy is defined as follows to measure the average amount of information needed to identify the class of a transaction in  fD [26]:  f ff 1 f f  ( ,D ) ( ,D ) (D ) log( )  D D  g p p  p  count C count C info  =  = ?? ?? .    (2)  where fD  is the number of transactions in fD , and  f( ,D )pcount C?  is the fuzzy cardinality of transactions in fD  with class label pC . In addition, given an attribute A in D,  without loss of generality, suppose Domain(A) is fuzzy- partitioned into a series of v attributes 1 2A ,A ,...,Av , and q  A fD  represents the fuzzy sub-dataset that has f{ ( ) 0, D } =1,2,... .qAt t t q v? > ? ?   then the fuzzy information  entropy of A can be calculated below:   i  i  i i i  A f A  A f f 1 f  A A A f f f  1 1f f f  D (D ) (D )  D  D ( ,D ) ( ,D ) ( log( ))  D D Di i  v v  p i  gv p p  A A i p  info info  count C count C  =  = =  = ? ?  = ? ? ?  ?  ? ?? ? .     (3)    The partition corresponding to the minimum A f(D ) vinfo  is the optimal v-interval fuzzy partition. Then, the information gained by v-interval partition in contrast to (v-1)-interval partition is as follows:    1A f A f(A) (D ) (D ) v v  vgain info info ?= ? .     (4)    where the 1A f(D ) vinfo ?  is the entropy of the optimal (v-1)-  interval fuzzy partition, i.e., the minimum 1A f(D ) vinfo ? .

Particularly, 0 f(A) (D ) /gain info n= , where n is the number of transactions in fD .

The performance of an associative classifier depends on the number of partitioned intervals.  If a fuzzy partition is too fine (e.g., with too many fuzzy sets on the domains), many fuzzy rules cannot be generated because of the lack of training patterns in the corresponding fuzzy subspaces [13]; furthermore, too many candidate itemsets may reduce the efficiency and understandability of the generated fuzzy rule set. Therefore, the maximal interval number K must be given for restricting the search space 0 v K? ? . The maximal  (A)vgain  is the global optimum of the simulated annealing strategy, and the corresponding fuzzy partitioned fD  will be applied in mining a compact set (classifier) of fuzzy class association rules.

2.1.2. Fuzzy Class Association Rules The fuzzy partitioned dataset can then be used to generate fuzzy association rules. In classification, the resultant rule set (i.e., classifier) consists of fuzzy class association rules (fuzzy CARs). For example, a fuzzy CAR in botanical data (e.g., Iris [29]) may look like ?short petal => Iris-setosa? (meaning that the samples whose petal lengths are short will be classified into ?Iris-setosa?), where short petal is of linguistic/fuzzy nature.

Generally, for a database D with schema R(A) , where A is a set of attributes, i.e., 1 2A={A ,A ,...,A }m , each Ak (1 )k m? ? can be associated with kq fuzzy sets defined on  kdomain(A ) , usually labelled as kq new attributes in fD .

That is, the new database fD  is with respect to the attribute  set 1 21 1 1f 1 1 2 2A ={A ,...A ,A ,...A ,A ,...A }m qq q  m m . Let G =  1 2{ , ,..., }gC C C  be a set of class labels, then a fuzzy rule item  is of the form X C? , where fX A? , and if j  kA X? ? ,  then ( ,and 1 ,1 )ik k kA X i j i q j q? ? ? ? ? ? , C G? .

In fuzzy context, the notions of degrees of support and confidence (i.e., Dsupp and Dconf) can be extended as follows [17,19]. For transaction t in fD , and any itemset  1 2{ , ,... } { }( 1,2,... )p f jX x x x A C j g= ? ? = ,   1 2( ) ( ) min( ( ), ( ),..., ( ))t X pDsu p pX t t x t x t x?= =      (5)   f f  ( ( )) ( )  D D  X t  count tX Dsupp X  ? = =  ? (6)  .

f f  ( ( )) ( ) ( )  D D i  X t t class Ci  i i  count t XC  Dsupp X C Dsupp XC ?  =? = = = ?  (7)  .

( ( ))  ( ) ( ( ))  i  X t t class Ci  i X  t  count t XC  Dconf X C X count t  ?  ? =? = =  ?  ? .     (8)  where ( )it x ( 1, 2,... )i p= is the corresponding membership degree of t for attribute ix , fD  is the cardinality of fD (i.e., the number of transactions in fD ), and X is the fuzzy cardinality of X calculated using count? operator.

. it t class C= means the transactions with class label iC . Here t-norm minimum is used in the definition of ( )Dsuppt X  due to its semantics. Other t-norms may also be considered under specific application contexts.

Then, given minimal support ?  and minimal confidence ? ( , [0,1]? ? ? ), a fuzzy rule item iX C?  is called frequent if ( )iDsupp X C ?? ? . Importantly, the fuzzy frequent itemsets also have a well-known anti-monotone property [24]: if a pattern is infrequent, all of its super patterns must be infrequent. Therefore, the pruning strategy of excluded itemsets in [17] is also appropriate in fuzzy context.

A fuzzy rule item iX C?  is called a valid fuzzy CAR if  ( )iDsupp X C ?? ? and ( )iDconf X C ?? ? .

Suppose the fuzzy CAR set has been learned from the  training data set. For a new transaction t, the weighted confidence of rule iX C?  to t is denoted as follows:    ( ) ( ) ( )t i t iWconf X C Dsupp X Dconf X C? = ? ?      (9)   The class predicted by GFRC is the class label of the rule with maximal weighted confidence.

2.1.3. Fuzzy Information Gain In GARC-type approaches, information gain is used to  reduce the search space in generating candidate itemsets and rules. The attribute with most information gain is defined as the best split attribute, which is to be included in the generation of candidate k-itemsets (i.e., the itemsets each containing k items, k ? 2). This will significantly help reduce the rule set without loss of accuracy [17]. In GFRC, fuzzy information entropy [26] is used to find the class best split attribute.

For one class label pC , all other classes can be considered as class pC?  . Hence, the data set has only two class labels, e.g. pC  and pC? . The fuzzy information entropy for class pC in fuzzy dataset fD is obtained as follows:      f f f  f f  f f  f f  count( ,D ) count( ,D ) (D ) log( )  D D count( ,D ) count( ,D )  ?log( ) D D  p p p  p p  C C info  C C? ?  = ? ?  ?  ? ?  ? ?     (10)    where |Df| represents the cardinality of fD  with class label  pC .  For the fuzzy partition of attribute A, 1 2A ,A ,...,Av  ,  the     fuzzy information entropy of A for class label pC  can be calculated below:    f f f  1 f  f f f  1 f f f  f f  f f  D (D ) (D )  D  D ( ,D ) ( ,D ) ( log( )  D D D  ( ,D ) ( ,D ) log( ))  D D  i  i  i i i  i i  i i  i i  Av A  Ap p i  A A Av p p  A A i  A A p p  A A  info info  count C count C  count C count C  =  =  ? ?  = ?  = ? ? ?  ? ?  ?  ? ??  ? ?  .

(11)  Hence, the fuzzy information gain for class label pC  of attribute A is obtained as:    f f(D ) (D )Ap Ap pgain info info= ? .     (12)   2.2. Data-Redundant Rule Pruning In GARC, some pruning strategies at the rule level are applied to avoid the redundancy/conflicts, giving rise to a compact rule set. This subsection further investigates another type of redundancy so as to largely improve redundancy resolution and classifier compactness.

The main idea of the pruning from the data perspective partly stems from the covering algorithm [27]. The rules are re-evaluated by not only their precedences, but also the rules? coverage degrees (which means the size of the transaction set they can classify correctly). If all the transactions covered (or satisfied) by a rule jX C? have already been covered by other rules with higher weighted supports in (9), then rule  jX C?  is called a data-redundant rule.

In the crisp context, a rule covers the transactions that contain the rule items. However, for a fuzzy classification rule set, the support of transaction t to ir  i ( )Dsupp t   is in the interval [0, 1]. The classification ability of ir  to t depends on the confidence of ir  and the i ( )Dsupp t  as in (9). The rules with low support of t cannot contribute to classify t correctly.

Therefore, only the transactions with sufficiently high  i ( )Dsupp t  to ir  should be considered when we calculate the coverage degree of ir .

For a transaction t, the rules can be ranked by i ( )Dsupp t .

The rule list is denoted as 1 2, ,...t lL r r r=< > , where l is the size of the rule set. Then, trank  ?  is defined as the rank number of the first rule whose class label is different from t, i.e.,  min{ . . , }t i i trank i r class t class r L ? = ? ? . Thus, the coverage is  defined as follows in Definition 1.

Definition 1.  A transaction t is covered by a rule ir  if and only if . .ir class t class= , and i ( ) ( )-  trank Dsuppt t Dsuppt t> .

For a rule ir , all transactions covered by ir  constitute a transaction set iD . Obviously, ir  has strong classification ability to every record in iD . Hence, the coverage degree of  ir  in fuzzy data set fD  can be described as follows:   iDf f  ( ) (D )  D  i t  i  Dsuppt t coverage ?=  ? .     (13)    The goal of data-redundancy pruning is to find a rule set with high coverage degree and make sure that all transactions are covered. The rule with the highest coverage degree in data set fD is defined as the optimal CAR of fD . Thus, the procedure of data-redundancy pruning is as follows: Step 1. Set training dataset as fD , the generated rule set as? , the compact rule set c? is empty; Step 2. Calculate the coverage degree of rules in ? , get the optimal CAR ir  of fD , c? = c? + ir , ? =? - ir , fD = fD - iD ; Step 3. If fD  is empty or ?  is empty, end the procedure; otherwise go to Step 2.

Normally, the size of compact rule set c?  could be considerably smaller than original rule set when these data- redundant rules are discarded.

Although the notion of rule-covering here is similar to that of the covering algorithm such as FOIL [27], there is a fundamental difference between them. Our procedure is carried out after the original rule set is generated by learning.

In other word, the whole training set is considered in the rule generating process, which is a global perspective; whereas FOIL deletes covered transactions at once after a rule is learned, in which the rules generated thereafter can only be measured in a local fashion.

2.3.  Algorithmic Details of GFRC As discussed in previous sections, GFRC is of extended  GARC-type with certain novel features. The main algorithmic details are shown in Table 1.

In Table 1, line 1 performs the fuzzy partitioning operation, so that the original dataset D is then transformed to the fuzzy-partitioned dataset Df. Lines 2-4 scan the dataset to measure all 1-itemsets rule items and then  get the class best split attribute by fuzzy information gain calculation with function fuzzy_gain ( fD ). Lines 5-11 perform the consecutive scans of Df. During each scan, function CandidateGen(Fexcluded, cla_bestattr, k) generates k- itemsets from Df, with each containing the cla_bestattr, and no super itemsets of ones in Fexcluded. Moreover, function pruneRules(cand) removes  the conflicting rules and redundant rules.  Finally, the data-redundant pruning is carried out in Line 12, to further reduce the generated rule set.

Notably, the algorithm will terminate if cand is empty, or otherwise, in at most m passes, where m is the number of attributes.

TABLE 1. THE ALGORITHM OF GFRC  Algorithm: Input: training dataset D Output: Fuzzy CARs Frule = {r1, r2, ?, rl} Begin: // Fuzzy partitioning using simulated annealing algorithm 1. fD =SA _ Fuzzy(D) 2. Frule ={r | r is an 1-item antecedent rule, ( )Dsupp r ?? and  ( )Dconf r ?? } 3. Fexcluded ={e | e is an 1-item antecedent rule, and ( )Dsupp e ?< } 4. cla_bestattr = Fuzzy_Gain ( fD ) 5. For k from 2 to m do 6.     kC =CandidateGen( Fexcluded, cla_bestattr, k);  7.     cand={r | kr C? , ( )Dsupp r ?? and ( )Dconf r ?? } 8.     pruneRules(cand) 9.     Frule= Frule? cand 10.     Fexcluded= Fexcluded? {e| ke C? , ( )Dsupp e ?< } 11. end for 12. RemoverRedundant (Frule)    3.  DATA EXPERIMENTS  To analyse the performance of our GFRC approach, this section shows several data experimental results. The evaluation of GFRC focused on accuracy and the number of rules, in comparison with GARC[17], a well-known benchmark classifier C4.5 [9], and three main fuzzy associative classification methods (i.e., FURIA[3], CHI[10,11], and SLAVE[15,16]).

The experiments were conducted in the environment with Windows 7, Intel Core i3, 3.1 GHz, 4 GB RAM and java with JDK1.7. All the experiments were tested based on datasets from a commonly used benchmarking database in the fields, namely the UCI Machine Learning Repository [29].

Concretely, 10 datasets were used in the experiments, which were well-known and widely used for the purpose of comparative assessments of the data mining methods [3,9,17].

In the experiments, thresholds ? and ? were 1% and 70% respectively. These settings were made through a series of tests in the parameters selection process, which is analogue to that in [17].

Firstly, classification accuracy, the basic performance measure for classification, was evaluated. For a classifier, its classification accuracy is the ratio of the number of cases correctly predicted by the classifier over the total number of cases in the test data set. Table 2 summarizes the classification accuracies of the six methods, where the best result for each dataset is highlighted in bold. Apparently, GFRC was seen superior to other methods in most datasets. Statistically, pair- wise Friedman Tests[30] between GFRC and others revealed that the accuracy of GFRC was significantly better than those of GARC, CHI and SLAVE at 95% level and that of C4.5 at 90% level, while the p-value(0.2059) indicated that there was no significant difference between GFRC and FURIA in accuracy, but the average rank of GFRC is better than FURIA.

Overall, the performance was generally in favour of GFRC.

We could say that the GFRC achieved an accuracy level higher than or at least as good as those of other methods.

TABLE 2.  CLASSIFICATION ACCURACY data GFAR GARC C4.5 FURIA CHI. SLAVE  breast 95.57% 94.85% 94.51% 95.68% 90.20% 95.49% cars 71.24% 70.02% 82.15% 79.08% 68.97% 70.68% heart-statlog 82.96% 80.57% 77.08% 79.75% 68.66% 78.44% ionosphere 91.72% 90.64% 88.72% 89.59% 66.40% 89.83% iris 95.33% 94.01% 94.25% 94.76% 92.27% 94.92% pima 75.16% 73.83% 73.43% 74.71% 72.55% 73.65% sonar 77.93% 74.30% 72.09% 77.01% 74.61% 68.50% wine 94.38% 83.46% 91.22% 93.25% 92.77% 92.46% haberman 73.53% 77.12% 71.75% 72.72% 73.08% 73.31% live-unsorted 59.42% 57.97% 63.00% 67.15% 58.75% 59.77%   Secondly, the compactness of resultant fuzzy CARs is considered by comparing numbers of generated rules. It is widely accepted that the less the number of rules there is, the higher understandability the classifier is. From Table 3, it is clear to see that the rule sets of GFRC reveal a remarkable reduction compared with those of GARC. Furthermore, Friedman Tests showed that GFRC was significantly better than GARC, FURIA, CHI in the number of rules with p- values 0.0114, 0.1317, 0.0016 respectively.  Additionally, there was no significant difference (p=0.527) between the rule set size of GFRC and SLAVE, whereas the GFRC outperformed SLAVE significantly in accuracy. The experiments revealed that GFRC was of a considerably high compactness, which largely stems from strategies to eliminate redundancy/conflicts and data-redundancy.

TABLE 3.  NUMBER OF RULES Data GFAR GARC FURIA CHI SLAVE  breast 10 21 12.8 172.4 5.8  Cars 3.2 9 12.9 54.4 12.5  heart-statlog 6.2 12 8.4 164.9 7  ionosphere 9.2 67 8.3 168.9 8  Iris 3 7 4.4 14.9 3.1  Pima 5.8 6 8.5 98.6 9.3  Sonar 9.2 16 8.1 137.1 6.9  Wine 7 16 6.2 101.2 3.8  haberman 2.2 13.7 4.4 15.8 4  live-unsorted 4 1 8.2 42.1 5.9  Finally, for illustrative purposes, the fuzzy CARs generated by GFRC and the crisp CARs by GARC in dataset Iris [29] are exemplified in Table 4. In brief, with good accuracy, the GFRC rules as a whole are of higher compactness and more linguistic nature, which is desirable as a classifier for understandability and use in decision making.

4.  CONCLUSION     This paper has proposed a novel fuzzy associative classification approach GFRC by extending a crisp one (namely GARC). GFRC has introduced fuzzy sets with trapezoidal membership functions on the attribute domains, via fuzzy partitioning using simulated annealing based on information gain. Additionally, with a measure on coverage degree of rules, a pruning strategy for reducing data- redundancy has been designed to select the rules with high coverage consecutively until all training data can be covered.

It has been demonstrated via experiments on benchmarking datasets that GFRC achieved an accuracy level higher than or at least as good as those of GARC, C4.5, FURIA, CHI, and SLAVE, and that  the GFRC classifier was significantly more compact in size than the other methods.

TABLE 4.  RULES IN THE CLASSIFIERS BUILT BY GFRC AND GARC  GFRC GARC (short petal) => Iris-setosa (long petal) => Iris-virginica (middle-width petal)=> Iris- versicolor  (petal length (-inf-2.45]) => Iris-setosa (petal width (-inf-0.8]) => Iris-setosa (petal width(1.75-inf)) => Iris-virginica (petal length(2.45-4.75]) => Iris-versicolor (petal width(0.8-1.75])=> Iris-versicolor (petal length(4.75-inf)) => Iris-virginica (sepal width(3.35-inf)) => Iris-setosa    Future research will center on other fuzzy and optimization strategies, as well as on large-scale real applications, for further enhancement of efficiency and effectiveness of the approach.

