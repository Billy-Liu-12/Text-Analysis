Parallel Implementation of Apriori Algorithm Based on MapReduce

Abstract?Searching frequent patterns in transactional databases is considered as one of the most important data mining problems and Apriori is one of the typical algorithms for this task. Developing fast and efficient algorithms that can handle large volumes of data becomes a challenging task due to the large databases. In this paper, we implement a parallel Apriori algorithm based on MapReduce, which is a framework for processing huge datasets on certain kinds of distributable problems using a large number of computers (nodes). The experimental results demonstrate that the proposed algorithm can scale well and efficiently process large datasets on commodity hardware.

Keywords-Apriori algorithm; Frequent itemsets; MapReduce; Parallel implementation; Large database

I. INTRODUCTION Data Mining has attracted a great deal of attention in the  information industry and in society as a whole in recent years. One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. Many algorithms have been proposed to find frequent item sets from a large database. However, there has not yet been published implementation performing the best under whatever conditions [14]. Apriori is one of the typical algorithms, which is a seminal algorithm proposed by R.Agrawal and R.Srikant in 1994 for mining frequent itemsets for Boolean association rules [5]. It aggressively prunes the set of potential candidates of size k by using the following observation: a candidate of size k can be frequent only if all of its subsets also meet the minimum threshold of support. Even with the pruning, the task of finding all association rules requires a lot of computation power and memory. Parallel computers offer a potential solution to the computation requirement of this task, if the efficient and scalable parallel algorithms can be designed.

MapReduce is a patented software framework introduced by Google in 2004. It is a programming model and an associated implementation for processing and generating large data sets in a massively parallel manner [2,6]. Some data preprocessing, clustering and classification algorithms have been implemented based on MapReduce [1,8,13].

In this paper, we implemented the parallel Apriori algorithm based on MapReduce, which makes it applicable to mine association rules from large databases of  transactions.

The rest of the paper is organized as follows. In  Section 2, we introduce the basic Apriori algorithm. Section 3 gives an overview of MapReduce. In Section 4, we present the details of the parallel implementation of Apriori algorithms based on MapReduce. Experimental results and evaluations are showed in Section 5 with respect to speedup, scaleup, and sizeup. Finally, Section 6 concludes the paper.



II. APRIORI ALGORITHM  A. Problem Statement The problem of mining association rules over market  basket analysis was introduced in [10]. It consists of finding associations between items or itemsets in transactional data [7].

As defined in [11], the problem can be formally stated as follows. Let 1 2{ , , , }mI i i i= ?  be a set of literals, called items. Let D  be a set of transactions, where each transaction T  is a set of items such that T I? . Each transaction has a unique identifier TID. A transaction T  is said to contain X , a set of items in I , if X T? . An association rule is an implication of the form ? X Y? ?, where X I? , Y I?  and X Y = ?? .

Each itemset has an associated measure of statistical significance called support. For An itemset X , we say its support is s  if the fraction of transactions in D containing X  equals s . The rule X Y?  has a support s  in the transaction set D  if s of the transactions in D contain X Y? . The problem of discovering all association rules from a set of transactions D  consists of generating the rules that have a support and confidence greater than given thresholds. These rules are called strong rules.

This association-mining task can be broken into two steps: Step1. The large or frequent itemsets which have support above the user specified minimum support are generated.

Step2. Generate confident rules from the frequent itemsets.

Computing  DOI 10.1109/SNPD.2012.31     B. Apriori Algorithm The name of the Apriori algorithm is based on the fact  that the algorithm uses prior knowledge of frequent itemset property which is that all nonempty subsets of a frequent itemset must also be frequent [5]. The main idea is to find the frequent itemsets.

The process of the algorithm is as follows.

Step1. Set the minimum support and confidence  according to the user definition.

Step2. Construct the candidate 1-itemsets. And then  generate the frequent 1-itemsets by pruning some candidate 1-itemsets if their support values are lower than the minimum support.

Step3. Join the frequent 1-itemsets with each other to construct the candidate 2-itemsets and prune some infrequent itemsets from the candidate 2-itemsets to create the frequent 2-itemsets.

Step4. Repeat the steps likewise step3 until no more candidate itemsets can be created.

The main steps consist of join and prune actions and the process is followed.

(1) The join step: To find kL , a set of candidate k -itemsets is generated by joining ( 1)k ? -itemsets. This set  of candidates is denoted as kC . Let 1l and 2l  be itemsets in 1kL ? . The notation [ ]il j  refers to the j th item in il .

The items within an itemset are sorted in lexicographic  order. The join 1kL ? ? 1kL ? , is performed, where members 1l and 2l of 1kL ? are joinable if their first (k-2) items are in  common. The resulting itemsets formed by joining 1l and  2l is 1 1 1 1 2[1], [2], , [ 2], [ 1], [ 1].l l l k l k l k? ? ?? The prune step: kC  is a superset of kL , its member  may or may not be frequent. According to the Apriori property, any ( 1)k ? -itemsets that is not frequent cannot be a subset of a frequent k -itemsets. Hence, if any subset with length ( 1)k ?  of a candidate k -itemsets is not in  1kL ? , then the candidate cannot be frequent either and so can be removed from kC .



III. INTRODUCTION TO MAPREDUCE MapReduce is a programming model and an associated  implementation for processing and generating large data sets. As the framework showed in Figure1, MapReduce specifies the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks.

Figure 1. Illustration of the MapReduce framework: the  ?map? is applied to all input records, which generates intermediate results that are aggregated by the ?reduce?.

Map takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key and passes them to the reduce function [6]. That is, a map function is used to take a single key/value pair and outputs a list of new key/value pairs. It could be formalized as:  map :: (key1, value1) ? list(key2, value2) The reduce function, also written by the user, accepts  an intermediate key and a set of values for that key. It merges together these values to form a possibly smaller set of values. The intermediate values are supplied to the users reduce function via an iterator. This allows us to handle lists of values that are too large to fit in memory. The reduce function is given all associated values for the key and outputs a new list of values. Mathematically, this could be represented as:  reduce :: (key2, list(value2)) ? (key3, value3) The MapReduce model provides sufficient high-level  parallelization. Since the map function only takes a single record, all map operations are independent of each other and fully parallelizable. Reduce function can be executed in parallel on each set of intermediate pairs with the same key.



IV. PARALLEL APRIORI ALGORITHM BASED ON MAPREDUCE  A. The main idea of the parallel Apriori algorithm  As described in Section2, the key step in Apriori algorithm is to find the frequent itemsets. In the k th iteration, it computes the occurrences of potential candidates of size k in each of the transactions. It is obviously that the occurrences counting of candidate itemsets in one transaction is irrelevant with the counting in another transaction in the same iteration. Therefore, the occurrences computation process in one iteration could be parallel executed. After this phase, all the occurrences of candidate itemsets are summed up. Furthermore, the join actions are done on the frequent k-itemsets and prune     actions are performed on the candidate (k+1)-itemsets.

Naturally, the frequent (k+1)-itemsets are found. Finally, according to the frequent itemsets, the rules that have a support and confidence greater than given thresholds are generated.

Figure 2 shows the flow chart of parallel Apriori algorithm, which is denoted as PApriori. The steps are as follows.

Step1. Use MapReduce model to find the frequent 1-itemsets.

Step2. Set 1k =  .

Step3.If the frequent (k+1)-itemsets cannot be generated, then goto Step6.

Step4. According to the frequent k -itemsets, use  MapReduce model to generate the frequent (k+1)-itemsets.

Step5. If k  is less than the max iteration times, then k + + , goto Step3; Otherwise, continue to the next step.

Step6. According to the frequent itemsets L , generate the strong rules.

k kL?  Figure 2. The flow chart of the parallel Apriori algorithm  B. The parallel implementation of the Apriori algorithm based on MapReduce  As the analysis mentioned above, PApriori algorithm needs one kind of MapReduce job. The map function performs the procedure of counting each occurrence of potential candidates of size k and thus the map stage realizes the occurrences counting for all the potential candidates in a parallel way. Then, the reduce function performs the procedure of summing the occurrences counts.

For each round of the iteration, such a job is carried out to implement the occurrences computing for potential candidates of size k.

Map-function The input dataset is stored on HDFS[1] as a sequence file of <key, value> pairs, each of which represents a record in the dataset. The key is the offset in bytes of this record to the start point of the data file, and the value is a string of the content of this record. The dataset is splitted and globally broadcasted to all mappers.

Consequently, the occurrence computations are parallel executed. For each map task, once the items in the candidate itemsets occur in the transactions, the <key?, 1> pair will be outputted, where key? is the candidate itemsets.

We use m_cycles to represent the maximum cycles of the PApriori. The pseudo-code of map function is shown in Algorithm 1.

Algorithm1. Map(key, value) Input: Global variable m_cycles, the offset key, the sample value Output: <key?, value?> pair, where the key? is the candidate itemsets and value? is the once occurrence of the key?, actually, it equals to 1.

1. if (m_cycles>1) /*for the case k>1*/ 2.   For each itemset kiC  in the candidate  k -itemsets  3.      If kiC  is a subset of value  4.           Output( kiC , 1); 5.      Endif 6.    End For 7. Else For each itemset iI  in value /*k=1*/ 8.          If 0iI ? 9.              Output( iI ,1); 10.         Endif 11.       End For  Reduce-function The input of the reduce function is the data obtained from the map function of each host. In reduce step, we sum up all the values with the same key and get the final result. In another word, we can get the total occurrences of potential candidates in the transactions. The pseudo-code for reduce function is shown in Algorithm2.

Algorithm2. Reduce(key, Value) Input: key is the element of the candidate itemsets, Value is once occurrence of the key Output: <key?, value?> pair, where the key? is identical to key and value? is total occurrence of the key?.

1. sum=0; 2. while( values.hasNext()){ 3.     sum+=values.next(); 4.     } 5.output (key, sum);

V. EXPERIMENTAL RESULTS In this section, we evaluate the performance of our     proposed PApriori algorithm in terms of sizeup, speedup and scaleup to deal with large scale dataset.

A. The datasets The transactional data for an AllElectronics branch and  T10I4D100K dataset are used in our experiments. As shown in Table1, there are nine transactions in the transactional data. We denote it as dataset1and replicate it to get 1GB, 2GB, 4GB, and 8GB datasets respectively.

They have many short transactions with few frequent itemsets. For the T10I4D100K dataset, we replicate it to 2 times, 4 times, 8 times and get 0.6G, 1.2G, 2.4G datasets, we denote them as T10I4D200K, T10I4D400K and T10I4D800K respectively. They have fewer larger transactions with many frequent itemsets. Performance experiments were run on a cluster of 10 computers, six with four 2.8GHz cores and 4GB memory, the rest four with two 2.8GHz cores and 4GB memory. Hadoop version 0.20.2 and Java 1.5.0_14 are used as the MapReduce system for all the experiments. Experiments were carried on 10 times to obtain stable values for each data point.

Table1. TRANSACTIONAL DATA FOR AN ALLELECTRONICS BRANCH  TID List of item_IDs T100 I1,I2,I5 T200 I2,I4 T300 I2,I3 T400 I1,I2,I4 T500 I1,I3 T600 I2,I3 T700 I1,I3 T800 I1,I2,I3,I5 T900 I1,I2,I3  B. The evaluation measure We use scaleup, sizeup and speedup to evaluate the  performance of PApriori algorithm.

Scaleup: Scaleup evaluates the ability of the algorithm  to grow both the system and the dataset size. Scaleup is defined as the ability of an m-times larger system to perform an m-times larger job in the same run-time as the original system. The definition is as follows.

1( , ) mm  TScaleup data m T  =               (1)  Where, 1T is the execution time for processing data on 1 core, mmT  is the execution time for processing m*data on m cores.

Sizeup: Sizeup analysis holds the number of cores in the system constant, and grows the size of the datasets by the factor m. Sizeup measures how much longer it takes on a given system, when the dataset size is m-times larger than the original dataset. It is defined by the following formula:   ( , ) mTSizeup data m T  =             (2)  Where, mT  is the execution time for processing m*data, 1T  is the execution time for processing data.

Speedup: Speedup refers to how much a parallel algorithm is faster than a corresponding sequential algorithm. It is defined by the following formula:   p  TSpeedup T  =                 (3)  Where, p  is the number of processors, 1T  is the execution time of the algorithm with one processor, pT  is the execution time of the parallel algorithm with p processors.

Linear speedup or ideal speedup is obtained when Speedup p= . When running an algorithm with linear speedup, doubling the number of processors doubles the speed. In practice, linear speedup is difficult to achieve because the communication cost increases with the number of records becomes large.

C. The Performance and Analysis We examine the scaleup, sizeup and speedup  characteristics of the PApriori algorithm.

To demonstrate how well the PApriori algorithm  handles larger datasets when more cores of computers are available, we have performed scaleup experiments where we have increased the size of the datasets in direct proportion to the number of cores in the system. For dataset1, the datasets size of 1GB, 2GB, 4GB and 8GB are executed on 4, 8, 16 and 32 cores respectively. For dataset T10I4D100K, T10I4D100K, T10I4D4200K, T10I4D400K and T10I4D1800K are executed in the same way.

Figure3 shows the scaleup performance of the datasets.

Clearly, the PApriori algorithm scales well, the scaleup fall shortly as the database and multiprocessor sizes increase. It always maintains a higher than 78% scalability for dataset1 and 80% for T10I4D100K.

To measure the performance of sizeup, we fix the number of cores to 4, 8, 16 and 32 respectively. Figure 4 shows the sizeup results on different cores. When the number of cores is small such as 4 and 8, the sizeup performances differ little. However, as more cores are available, the sizeup value for 16 or 32 cores decreases significantly compared to that of 4 or 8 cores on the same data sets. The results show sublinear performance for the PApriori algorithm, the program is actually more efficient as the database size is increased. Increasing the size of the dataset simply makes the noncommunication portion of the code take more time due to more I/O and more transaction processing. This has the result of reducing the percentage of the overall time spent in communication. Since I/O and CPU processing scale well with sizeup, we get sublinear performance.

To measure the speedup, we kept the dataset constant     and varied the number of cores. The number of cores varies from 4 to 32. We have performed four experiments, the size of the dataset increases from 1GB to 8GB for dataset1, and from 0.3GB to 2.4GB for T10I4D100K.

We have performed the speedup evaluation on datasets with different sizes and systems. Figure 5 shows the speedup for different datasets. As the result shows, the speedup performance does however not to be very good in the case of 1GB for dataset1 and 0.3GB for T10I4D100K.

This is an artifact of the small amount of data each node processing. In this case, communication cost becomes a significant percentage of the overall response time. This is easily predicted from our sizeup experiments where we notice that the more data a core processes, the less significant becomes the communication cost giving us better performance. Therefore, PApriori algorithm can deal with large datasets efficiently. Larger datasets would have shown even better speedup characteristics.

(a) Scaleup for dataset1     (b) Scaleup for T10I4D100K  Figure 3. Scaleup performance evaluation   (a) Sizeup for dataset1   (b) Sizeup for T10I4D100K  Figure 4. Sizeup performance evaluation   (a) Speedup for dataset1   (b) Speedup for T10I4D100K  Figure 5. Speedup performance evaluation To sum up, for the datasets either have many short  transactions with few frequent itemsets or fewer larger transactions with many frequent itemsets, PApriori algorithm has shown good performance.



VI. CONCLUSION Searching for frequent patterns in transactional  databases is considered one of the most important data mining problems. The task of finding all association rules requires a lot of computation power and memory. In this paper, we propose a fast parallel Apriori algorithm based on MapReduce, We use sizeup, speedup and scaleup to evaluate the performances of PApriori. The experimental results show that the program is actually more efficient as the database size is increased. Therefore, the proposed algorithm can process large datasets on commodity     hardware effectively.

