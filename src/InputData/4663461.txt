

Learning to enhance dependability of information systems  Marco D. Aime marco.aime@polito.it  Andrea Atzeni andrea.atzeni@polito.it  Paolo C. Pomi paolo.pomi@polito.it  Dip. di Automatica e Informatica, Politecnico di Torino Corso Duca degli Abruzzi, 24  Torino, Italy  Modelling and formal verification techniques have been successfully applied to complex software or hardware1, but their application to complex information systems and to their heterogeneous reliability and security constraints is still an open issue. The configuration and live management of these systems is essentially based on expert knowledge, with well accepted checklists, ad-hoc simulation tools, test- ing of prototype implementations, all trying to reduce ex- perts? time and error rate.

The DESEREC project[1] has developed an approach for system dependability that creates a portfolio of pre- validated system configurations, used to react and restore normal or acceptably degraded operational system state.

Major components of this approach are a set of modelling and analysis tools. The system manager describes the de- sired services and the available resources. The system au- tomatically generates and validates several configurations spanning various service and dependability levels. These configurations are then available for automatic response or manual selection when a problem (i.e.attack or failure) takes place. This approach makes possible system surviv- ability (at certain degree of functionality) while technical personnel identifies the source of the problem and finds a better solution.

This approach can be further empowered with machine learning techniques that track the live behaviour of the system and feedback the configuration generation process.

This opportunity becomes practically essential to customise and update the behaviour of system management method- ologies to complex and evolving system, as clearly stated in literature [3].

As far as we know, our proposal does not directly com- pare with any previous work in literature. The closest works lie in the relative new autonomic computing and self- healing fields [5] [2]  However, our work owes to several well established  1ACM Turing Award 2007, http://www.acm.org/ press-room/news-releases/turing-award-07/  fields, including ICT system management, risk analysis, and learning and planning from artificial intelligence liter- ature. In this section we discuss these ties and the general planning framework of the DESEREC project in which our work places.

The self-learning configuration generation and valida- tion process we have designed and experimented is outlined in the following phases  We first formally describe the system and its configura- tion requirements (including provided services). We dis- tinguish between the following parts in the system descrip- tion. The service model, which describes the structure of the business services and their security constraint. It is used to automatically identify the assets at the business service level, to derive their functional and non-functional (secu- rity and QoS mainly) dependencies, to characterise the as- sociated threats, and to compute the risks. The resource model which describes the system infrastructure (hardware and software elements and their interconnection) and asso- ciated capabilities (CPU / communication resources, secu- rity mechanisms). It is used to identify the assets at net- work level with their dependencies, the associated threats and risks, and to identify the countermeasures that may be configured in the system. The policy model which describes the constraints we must comply when configuring the sys- tem infrastructure to provide the target business services. It is used to identify constraints to respect during system con- figuration.

Based on its current knowledge (represented as a set of strategies), the COnfiguration Generation tool (COG) gen- erates a set of potential system configurations that comply with the requirements. The configuration generation tools semi-automatically compute how to configure the system infrastructure to provide the given business services, as pre- scribed by the service, system and policy models. It re- sults in a set of alternative system configurations, includ- ing where services components must be installed and run and how security devices (e.g.firewalls, authentication and   DOI 10.1109/SASO.2008.50    DOI 10.1109/SASO.2008.50     authorisation services, security protocols) should be config- ured. These configurations are formal description that spec- ify where every component of the business service must be installed and run, how security devices (e.g.firewalls, AAA services, security protocols) should be configured to al- low legitimate service interactions and prevent unauthorised ones, what reliability mechanisms (e.g.redundancy and di- versity of physical and logical elements) should be put in place. The COG is provided with an internal knowledge base that has the formalisation of different translation strate- gies that can be adopted to satisfy the requirements. Strate- gies are actually represented as model transformation rules and are prioritised to avoid conflicts.

The risk-based Dependability ANalysis Tool (DANT) verifies the configurations against a set of possible depend- ability problems. Our threat knowledge-base is actually an XML database with information of threats and how they af- fect system and service elements. Such database is popu- lated on the base of publicly available repositories, such as vulnerability databases (e.g. [6, 8]) and security advisory sites (e.g. CERT2). The DANT takes as input single vul- nerability and dependability problems and internally com- putes a ?risk graph? shaped similarly to an attack graph, where security and dependability problems are organised in causal sequences. The internal behaviour of the DANT is based on well known risk analysis process: MAGERIT([7].

The first phase is the system assets identification, the second is the threat identification and characterisation, the third is risk evaluation score, combining threats and assets critical- ity and importance. The risk score DANT assigns is based on the propagation of the faults in the system, based on the functional dependencies among structural system elements and service elements modelled previously.

Data-mining techniques are applied to correlate the de- pendability scores computed by DANT with distinguishing characteristics of the configurations. Direct comparison of formal configurations is complex and time spending. For- tunately, our architecture suggests a natural and effective change of prospective. We know that every configuration is generated choosing among the rules available in COG?s knowledge base. We can then assume the configuration de- pendability score to be the result of the choices taken by the COG. Score vectors and rules are joined in a common data, in order to analyse their cause and effect relations.

Data mining techniques [4] provide some well known ap- proaches to extract the association rules, like the Apriori [9] algorithm. The Apriori algorithm is composed by two main parts: find in a database the sets of elements whose occur- rence exceed the threshold and generate association rules from these sets having confidence above the threshold .

The results of the data-mining analysis are merged with the current knowledge-base of COG to customise and im-  2http://www.cert.org  prove its behaviour. In fact, selected association rules can correspond to two kinds of configuration strategies: known and unknown. The known ones are already present within the COG knowledge base. In this case, the only think to do is ?tuning? their priority, in order to favour their applica- tion in further runs of the configuration generation process.

The unknown are the combined effect of multiple strate- gies. Pre-existent strategies are aggregated in new ones, with an higher number of parameters, and inserted in the COG knowledge base. Their priority is tuned to favour the more specific (and complex) strategies over basic ones.

The whole process is repeated at the time of first deploy- ment and whenever changes occur in the system or in the knowledge on dependability problems. Merely for exper- imental reasons, we have simulated this step by manually introducing changes in the formal system description, in its configuration requirements, and in the set of dependability problems supported by DANT.

The main evolution path is now boosting the scale of our experimentation. The size of the target system directly im- pacts on several steps in our prototype process. Restricting to the data mining part, we are testing our approach against a broader set of configuration strategies and dependability threats. The more interesting future works will come how- ever from integration of our conceptual approach and proto- types within a system management infrastructure and their validation against a live system.

