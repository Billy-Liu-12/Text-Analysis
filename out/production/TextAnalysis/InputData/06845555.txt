2014 IEEE Workshop on Electronics, Computer and Applications

Abstract- This article explores the challenges in data privacy  within the big data era with specific focus on differential  privacy of social media data and its geospatial realization  within a Cloud-based research environment. By using  differential privacy method, this paper achieves the distortion  of the data by adding noise to protect data privacy.

Furthermore, this article presents the lOP k-means Aggregation Optimizing Method to decrease the overlap and superposition of massive data visualization. Finally this paper  combines lOP k-means Aggregation Optimizing Method with differential privacy method to protect data privacy. The  outcome of this research is a set of underpinning formal  models of differential privacy that reflect the geospatial tools  challenges faced with location-based information, and the  implementation of a suite of Cloud-based tools illustrating how  these tools support an extensive range of data privacy demands.

Keywords-differential privacy; aggregation optimizing; massive data; Data Visualization

I. INTRODUCTION With the rapid development of internet technology, it is  becoming increasingly easy to share data which results in concerns about privacy leakage. In recent years, social panics caused by data leakage have occurred at home and abroad from time to time. The protection of personal privacy is much more than hiding sensitive attributes (such as name, address, salary) in data records, but also to prevent the connection between sensitive attribute values and a particular entity or individual to avoid the individual's true identity being speculated by non-sensitive attribute information. Over the last decade, the rapid development of data mining technology also brings new challenges to the privacy information protection. Traditional database security measures like authentication, access control and so on can only prevent direct access to sensitive attributes, but data mining often get access to massive data and obtain sensitive information by indirect reasoning which makes traditional defense means entirely useless. Thus, privacy protection is a wide applied, multi domain crossing and complex system engineering with many aspects to be studied and developed.

Wang Shuo,Li Hui Agricultural Information Institute of Chinese Academy  of Agricultural Sciences Beijing, P.R. China 100081  wangsurecn@163.com

II. THE RESEARCH REVIEW OF THE DIFFERENTIAL PRIVACY PROTECTION  Privacy protection can be roughly divided into three categories according to its technical features: data distortion, data encryption and publication limitation. These three technologies have their own characteristics and different application situations. Besides, there are also many privacy protection methods combining several technologies above and it is hard to classifY them to a particular type. Also, there is no privacy protection technology suited for all applications.

To get E-differential privacy protection. Differential privacy protection is a kind of privacy protection technology based on data distortion, and the biggest advantage of which is that the amount of noise added is independent of the data set size. Therefore, only by adding a small amount of noise can we achieved a high level of privacy protection for large data sets. Differential privacy protection sets off a research upsurge at home and aboard because of its so many advantages.

For specific data types, Cormode et al solved the problem of the sparse data added with too much noise in the process of differential privacy protection by simplifYing procedures and reducing sensitivity to obtain good effect in the application of sparse data protection [1]. Sarathy et al pointed out some limitations in the application of differential privacy protection in numeric data [2]. Dwork et al proposed a stronger protection level concept pan privacy for convection and continuous observation differential privacy protection. Pan privacy is based on flow algorithm and each data will be immediately discarded after processed. It can defend continuous invasion even if the internal state of the data has been exposed to the invasion person [3]. Li et al combined differential privacy protection with k-anonymity algorithm for the first time and applied it to data publication under the micro-data privacy protection. Zhou et al proposed a compression algorithm applied in large database differential privacy protection and researched how to compress database by the way of random linear or mapping transformation with the help of Gaussian random variable matrix. They tried to achieve the differential privacy protection while maintaining the statistical features of the original data such as high-dimensional sparse regression, PCA and other statistical measures based on covariance method of the initial data.

2014 IEEE Workshop on Electronics, Computer and Applications  In applications, Duy Vu et al combined differential privacy protection with traditional statistical hypothesis testing modeling and used it in clinical trials data mining and concluded some regularities in how to adjust the number of samples to obtain a balance between statistical efficiency and privacy protection level. Gehrke et al improve the differential privacy protection algorithm and applied it to the social networks privacy protection modeling. Zhang et al proposed a differential privacy protection algorithm applied to distributed data mining to defend joint attack based on multiparty computation and random selection [4].

Differential privacy protection can guarantee that adding or deleting a data will not affect the investigation output and therefore even in the worst case, the attacker has known all the sensitive data except one record, we can also ensure that the sensitive information of this record will not be disclosed.

D 1 and D2 represent two sets with only one different record between each other, Range (K) represents the value range of random function K, Pr[Es] represents the disclosure risk of event Es . If a random function K provides E?  differential privacy protection, then for all S <;;; Range(K) .

Pr[K(DJ)E S]:S; Pr[K(DJE S]xexp(E) (1) The calculated disclosure risk depends on the value of the  random function K.The selection of random function K has nothing to do with the knowledge of the attacker, as long as K satisfies the definition in Theorem 1, it can protect any privacy of the data set, even if the attacker has known all the other data.There is a query function f, the data set X and the real query results f(X). K adds suitable random noises into f(X) to protect privacy.

The response of E-Differential privacy protection function K is f (x)+(LaAlf / E)t ?  For f: D----+Rk, sensitivity of f is defined as:  N = maxllf(DJ}/(D,)IIJ' Dl' D2  Let the noise function Lap(b )=exp( -lxi/b) as the symmetrical exponential distribution that standard deviation is Jib, b= N 1 E, so the formula of the probability density function is p( x) = exp(-I x I / b) / 2b and the formula cumulative distribution function is  D(x) = (1 12)(1 +sgn(x)(1-(exp(1 x lib?) (2) Added noise is inversely proportional to the value of ?f.

Privacy protection methods are all based on some kinds  of attack model, for example, the attack model of K? anonymity and L-diversity assumes that the attacker is absolutely ignorant of the sensitive attribute information otherwise the privacy could not be protected. The attack model defined by the differential privacy protection is that in the worst case that the attacker has obtained all the sensitive attributes of data except one record, the sensitive attribute information of this record can still be protected. From theorem 1 we can know that any query result will be similar when the difference between data set D 1 and D2 is only one record. So the attacker cannot infer any sensitive attribute information of the record with a certain probability.



III. RESEARCH OF DIFFERENTIAL PRIVACY PROTECTION CLUSTERING METHODS  With the increasingly strong of database system and reducing storage costs, personal data collection has no longer merely been the work of the government and statistical departments, various organizations like medical institutions, financial sectors, search engines, intrusion detection systems and social networks are holding numerous personal data including some privacy data. The data visualization can get a lot of valuable information quickly and intuitively, so how to visualize the data with privacy protection is a problem with great research value.

Data aggregation is a main way to solve problems such as images painted cross and overlap in big sample data visualization. Clustering is the basic method of data aggregation. The purpose of privacy protection clustering research is to control the impact of privacy protection method on the clustering results in a certain range and protect sensitive attributes of data at the same time. However, the current research of privacy protection methods in data mining are mainly focused on association rule mining and classification mining and the research based on clustering analysis is relatively uncommon.

This paper want to propose an IDP k-means privacy protection clustering method based on differential privacy protection. By improving the choice of the initial center, we can make the data distorted through adding random noise satisfied specific distribution in a strict attack model to achieve privacy protection. At last we can verifY the result and feasibility of IDP k-means by simulation experiments.

A. Dtfferential Privacy k-means clustering  Given a set of points Ptc [0, I ]d, The k-means algorithm is the clustering process of points in point sets that to make the each point in the cluster close to each other as possible.

Specially, the k-means algorithm calculates k center points in the [O,l]d space at first, and clusters those nearest points from the center. So k-means algorithm is an iterative process to obtain k center points and minimize the total distance of every point from the center point in the cluster. The specific implementation process is as follows:  k-means (?, ... ,Uk): 1) The point set Pt is divided into k sets (Sl, ... , S"j, and  each point is divided into its nearest division from the center uJ.

2) For 1:::;' j :::;, k, to update the center of each cluster uj , = LtEs; dJ I Sj I  To repeat the whole process until partitIOn does not change or iteration is limited. The process in Step 1 needs to calculate the distance of each point from the nearest center, which will obviously lead to privacy disclosure. So we can design differential privacy clustering algorithm based a SuLQ framework [5] to make the process meet the differential privacy protection. Firstly SuLQ frame is introduced:N (0, R) is the normal distribution random number with mid-value of O and variance R=R(E, 8 ,T).

SuLQ database algorithms A (R):    2014 IEEE Workshop on Electronics, Computer and Applications  Input: Input: A query (g:D---+[O,I],Sc[n]) Returns: LtES g(dt) + N(O,R) Firstly k points are selected, and then the k points that  adds noise are returned as the initial point of the center, forming k collections (St"", S,J ,  SuLQ k-means (Ut, " " u,J : 1) (k query ): For 1 ::::; j ::::; k, to calculate the  approximate number of points contained in the set Sj as .

Sf = SuLQ(f(p,) := {I p, E SI }) o Otherwise (3) 2) (kd query ) : For 1 ::::; j ::::; k, the approximate sum of  all points in the set Sj as s,  - . {Pi Pi E SI } mf = SuLQ(f(p,) := . ) o Otherwise (4) For I ::::; j ::::; k, to update each center point: :;; = ;; / ? . I .1 J  If the number of points in each cluster far exceeds RI /2,  the sf is a good estimate of s] = ISjl, and ;; is also an accurate estimate of u] under no privacy protection.

B. D!fferential Privacy clustering algorithm IDP k-means In the k-means clustering algorithm to calculate the  nearest point from the center of each sample will leak privacy. By analyzing k-Means we can find that it is necessary to divide the sum of points by total number when calculating the center point. So, only to release an approximation of the number and amount of points will not divulge privacy.

On this basis, this article has done some improvements that combine f.-Differential privacy and k-means algorithm, whose main steps are as follows:  Firstly, the n points (p!, ... , Pn) in d-dimensional space [O, I]d are inputted, and then k points (u!, ... , Uk) are randomly selected, returning k new points (ut], ? ? ?  , u\) that add noise as the initial center points, which follows the steps to update :  1) To divide each point pt to the nearest center ut], the sample set {Pt} is divided into k collections (S1,???, SiJ.

2) For I:Sj:Sk, the sum of points in set Sj is calculated:  sum = LIES, PI and total number: num = ISjl? After the noise  is added respectively, sum' and num' can be obtained and uj' = sum'/ num' is updated as the new center point.

The above iterative process continues until partition does not change or iteration is limited. The function of the noise added is Lap(b)=exp(-Ixl/b), where b= MIE  For this algorithm, when f. reduces to a certain value, the accuracy of clustering sharply decline, the important reason that results in too big error is that the choice of initial centers, randomly selected after the center, is often far from the center after adding the noise. Therefore, the selection method of initial center points should be considered to be improved.

We can try the improved algorithm that p], ... , Pn are divided into k subsets and center points in the k sets that add noise are calculated in step 2 of the above algorithm as   initial center points, and then to prove that improved methods can greatly improve the accuracy of clustering with maintaining the amount of noise added in the sets and the same level off. privacy protection.

o I and 02 represent two sets with only one different record between each other, the process of calculating the center point is equivalent to histogram query to divide the [0, I]d space, where the ?f of denominator is q. The maximal ?f of numerator is dm, because PtE [O, I]d. When adding or deleting a point in d-dimensional space [0, I ]d, the sensitivity of the sum of each dimension is 1. Therefore, the sensitivity of the entire query sequence is d + I .  Let Par (0 I ) and Par (02) correspond to results of 0 I and 02 clustering after adding noise, where partition represents any kind of clustering division So if Pr[Par(D1) = partition] E were . ,  . ?e  Pr[Par(D2) = partition] met, it could prove the IDP k-Means algorithm meet f.? Differential Privacy[6].

Based on the above analysis, it is assumed that number of iterations can be fixed as N by adding distribution Lap ((d + 1) N If.) to obtain f.-differential privacy.

The next job is to achieve the above algorithm based on the open source Weka machine learning framework [7], using the JAVA language algorithms, MyEclipse programming environment and DCI Knowledge Discovery Archive database as data source .

In order to verify that this differential privacy algorithm IDP k-means can better maintain the clustering availability on a strictly defined level of privacy protection, and this paper chooses a similar method NeSDO as contrast to do simulation experiments, using the same evaluation of the experimental results to compare them.

Availability of clustering uses the F-measure evaluation [8] to measure. Dataset with k-Means algorithm and differential privacy k-Means clustering algorithm respectively to calculate the F-measure results of the two clustering.



IV. DIFFERENTIAL PRIVACY FOR SOCIAL NETWORKING MULTIDIMENSIONAL DATA SAFETY VISUALIZATION  Visual analysis is a new discipline with a multidisciplinary field evolved, playing an increasingly important role in large-scale data understanding, mining and network data analysis. Through effective visualization technology users can participate in knowledge discovery and data mining process. Cluster analysis is an important knowledge discovery technique which is often able to achieve better results in Tweeter that contains wide variety of kinds and needs a large artificial training set to analyze. This paper aims to develop an improved K-means algorithm to complete the tweets clustering. However, tweets are typical multidimensional data sets, and this article designs and implement a framework for multidimensional data clustering analysis in the visual interactive environment.

To meet the growing demand for visualization of massive data, scholars have proposed a variety of data visualization methods. Meanwhile, databases with sensitive information in the real world are becoming more common, especially in the    2014 IEEE Workshop on Electronics, Computer and Applications  social network. These sensItIve information might leak of personal privacy information, therefore, to prevent privacy leaks become one of the hot issues of current concern of the scientific community. However, there are little researches in the sensitive data visualization. As data visualization techniques are widely used in massive data analysis and presentation, the scope of application of data visualization techniques becomes more widely, and sensitive data visualization has become an unavoidable problem. Privacy protection technology can be used for data visualization process to hidden sensitive information.

Privacy and data visualization has essential different purpose that privacy protection techniques research how to maximize the protection of sensitive information from being leaked and data visualization techniques study how to maintain the original data availability and improve visualization performance of images. Therefore, how to deal with data privacy in data visualization is a challenging task.

So, this article explores how can privacy and data visualization strike a balance between each other, which greatly improves the visual expression of the image while maintain the maximum protection of sensitive data.

A. Related Chanlenge  The main purpose of my future research is to solve large sample visualization of sensitive data.

Data security visualization methods usually protect privacy of the original data to get secure data sets, and then take the safety data to data visualization. Privacy protection methods can be anonymizing data, adding noise and so on to achieve data security visualization. So, we can use the large sample data as the study object, to do aggregation operation in the original data simultaneously to protect privacy, so that the data is aggregated with not disclosing any sensitive information.

The next work is to add noise to protect privacy based on the differential privacy lOP k-means algorithm mentioned before, combining with data aggregation algorithms without privacy protection, to use the differential privacy to protect data aggregation methods based on k? means clustering. There are three major difficulties:  a) Aggregation operation in Data Visualization usually requires a larger number of clustered, while the calculating speed of traditional clustering algorithms such as k-means clustering is very slow when k is large ( e.g. k=400), which is clearly unable to meet the aggregation operation of large sample data.

b) The data aggregation process needs to add noise to achieve the purpose of protecting privacy, while noise interference makes large number of clustering error and success rate is very low.

c) Adding noise and the big amount of clusters results in another consequence that uncertainty of the number of iterations, and as the number of iterations increase the amount of noise becomes larger, which in turn increases the clustering error.

Accordingly, to study a differential privacy equipartition k-means algorithm (OPE k-means) based on lOP k-means in the data aggregation for this particular problem, hoping to   solve the slow traditional problems that clustering speed of k-means is low when in large k, greatly improving the efficiency of the algorithm, reducing the amount of noise added and to meet the stringent ?-differential privacy while ensuring the data visualization image quality.

B. Differential privacy data aggregation method DPE k? means  In massive data visualization process, image overlay is necessary to solve, otherwise it will cause the low quality of image and a lot of data hard to be recognized. The objective of this task is to propose a data aggregation method based on differential privacy k-means, which can solve the aforementioned problem of overlapping images and can achieve the strict definition of privacy protection.

Differential privacy protection is based on data distortion achieved by adding random Laplacian noises that the amount of added noise is only related to the query function itself, regardless of the size of the data set.

Therefore, for a large data set, by adding only a small amount of noise, we can achieve a high level of privacy protection, which greatly ensures the availability of data.

The protection mechanism of differential privacy is denoted as Kf, the query function as f ' calculating value of f(X) and adding noises that meet Symmetry Index distribution that variance is 0'2 , distribution density function is as follows :  Pr[Kf(X) = a] oc exp(-II f(X)-alll /(J) (5) Each coordinate in this distribution is random variables  that meet exponential distribution. Privacy protection is achieved by adding exponential noise to f(X).

For f: 0 ---7 Rd , Kr satisfies (?f /O')-Differential privacy protection.

We apply exponential triangle inequality to Equation (5) and all the possible response of r satisfy:  Pr[KrCD,) = rl S Pr[KrCD,) = rlxexp( -II feD,) - feD,) II, fer) (6) According to definition of !J.j , the upper bound of  exp(-II f(DI)-f(D2) III /(J) is exp(N / (J). So:  Pr[Kr(D,) = rl S Pr[K j(D,) = rl x exp(e) (7) Formula (7) is suitable for single-element set S = {a},  which needs a unified community.

To obtain ?-Differential privacy protection, we must  select 0'2:? / Af .

F is the query strategy determined by a specified set of  query functions fp' so fp(X)i is the i-th query function , and the query results are the previous i-I responses PI, ... ,Pi?

I. If the previous i-I responses P were equal with p', fp(X), = 1;,.(X),.

We can define the sensitivity of this query strategy F = {fp D -HR+),'} the maximum sensitivity to any possible function, and ?F = sup p ?f p . For query strategy F = Up: 0 ---7 Rd} , KF can provide (M/O')-Differential    2014 IEEE Workshop on Electronics, Computer and Applications  privacy protection. For any P E (R + )d , the conditional probability rule as follows:  Pr[Kj(X) = p] = IIPr[KAX)i = Pi I PI ' Pz ... Pi-I ] (8) i"S,d  is the random variables that mid-point is f/X)i and the  variance of exponential noise is (}"2, there is: Pr[KF(X) = p] oc II exp(-II fp(X)i -Pi III /(J) i'S,d = exp(-II fp (X) -pill/a)  (9)  According to formula (6), the triangle inequality obeys (?F/?)-Oifferential privacy protection.

The previous proposed improved differential privacy lOP k-means algorithm can maintain a better clustering availability with ?-differential privacy protection. Actually, data aggregation operations in data visualization often require enough larger k of k-means algorithm to aggregation enough center points for clustered data visualization, however, the existing k-means algorithm with differential privacy protection will lead to a lot of empty cluster failures due to interference from added noises when k is large, which cannot really be applied to data visualization .

On the basis of lOP k-means, for data aggregation, OPE k-means algorithm can be proposed to minimize the distance of neighboring points and, at the same time, enable data aggregation to reflect the original data distribution well in the future. Assuming we will aggregate n points pI, ... , Pn in d-dimensional space [O,I]d into k clusters, then the k center points are Uj = sumlnum, l<::j?k. The greatest impact of adding or deleting a point in the d-dimensional space [O,I]d on the denominator num is 1 , therefore ,?fnum of num is l. For sum, due to PiE [o,l]d, so the biggest change in the numerator is d to add or delete a point in the d-dimensional space [O,l]d, therefore ?fsum of sum is d. Therefore the sensitivity of entire query sequence ?t;otal= d +1. If the maximum number of iterations is fixed to N and the total sensitivity of the query sequence id d+ 1, then the function that adds noise to get ?- Differential Privacy Protection is Lap((d + l)N/?).

Major steps of OPE k-means algorithm to obtain ?? differential privacy protection are as follows:  aj To input the n points PI, ... , Pn in d-dimensional space [O']/, and then divide those points into k subsets  Sf, . . .  , Sk by chronological order. For l?j?k, the number of points in set ? is ISjI9, a= ceil (n I kj, ceil Ois a ceiling fimction.

bj For l<::j?k, to calculate sum = LiES! Pi and  num=ISjl and add noises respectively to obtain sum' and num', so the center points ofSj become u/= sum'lnum'.

cj To re-classify the points set {pj according to the calculated center points in step 2, each sample point Pi will be classified into the set Sj where the new nearest center point u/ in. If ISjlo:::a, it indicates that the collection is full, and continue to find the set Sj where the secondary nearest center point tt/ in. The points set {pj will be divided into k clusters Sf, . . .  , Sk using the above method.

dj To repeat steps 2, 3 until demarcation does not change or iteration is limited.

OPE k-means clustering algorithm limits the number of points of each cluster to make the aggregation point uniform distribution in the original data set, therefore, the point obtained by aggregation can avoid some extreme cases, such as some aggregation points represent several data, and others represent hundreds of pieces of data.

Finally, From the result of verification test prove that OPE k-means algorithm can improve the visual quality of the image through some experiments. Specially, algorithm can be realized in Matlab, using XmdvTool as a data visualization tool to judge the quality evaluation OAL.

