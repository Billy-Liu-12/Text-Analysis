On Dual Mining: From Patterns to Circumstances, and Back

Abstract  Previous work on frequent itemset mining has focused on finding all itemsets that are frequent in a specijied part of a database. In this paper; we motivate the dual question of finding under what circumstances a given itemset satisfies a pattern of interest (e.g., frequency) in a database. Circum- stances form a lattice that generalizes the instance lattice associated with datacube. Exploiting this, we adapt known cube algorithms and propose our own, minCirc, for  mining the strongest (e.g., minimal) circumstances under which an itemset satisfies a pattern. Our experiments show minCirc is competitive with the adapted algorithms. We motivate min- ing queries involving migration between itemset and circum- stance lattices and propose the notion of Armstrong Basis as a structure that provides eficient support for  such migration queries, as  well as a simple algorithm for computing it.

1 Introduction  Extensive work has been done on efficient mining of interesting patterns like associations, correlations, etc.

and their variants, from large databases (e.g., see [AS94, MTV94, BMS97, LSW97, AS95, MTV97, DL99, HPYOO, PHMOO, Bay98, AAPOO, ZaHs991). At the core of extract- ing such patterns is the determination of frequent itemsets, to which many of these studies are devoted. All of these studies are concerned with answering the question ?which patterns hold in the database.? We argue that many a time, it is at least as interesting and useful to ask ?when a giv- en pattern holds in a database.? Here the word when can be interpreted to mean, but need not be restricted to, tempo- ral conditions, conditions involving locations, demographic- s, etc., and more generally, the ?underlying circumstances? under which the pattern of interest holds in the database.

Why Should we care about this question? Here are a few motivating example queries.

QI: Under what circumstances (say, on time of purchase) is  Qz: Under what circumstances (e. g. time, location) are in-  the itemset {beer, diaper} bought frequently?

terstate calls totalling over $50 made by a customer?

Q3: Under what circumstances (e. g. season, weather) are ice cream sales high? Which items(ets) have high sales under those circumstances?

Q4: Which itemsets are seasonal favourites, i.e. they are bought frequently in fall but not in summer or vice ver- sa?

Q5: For each itemset bought frequently in fall, find further circumstances under which they are bought frequently.

Just as associations, correlations, and other mined pat- terns are useful in sales planning etc., knowing the circum- stances under which one or more patterns hold can be useful in ?tuning? the plans around these Circumstances. E.g., if certain patterns hold for NJ locations and not for NY loca- tions, then a manager might want to incorporate this in his planning. The examples above illustrate that one might be interested in a specific pattern (e.g., a specific frequent item- set {beer, diaper} in 91). or in a set of patterns generated from another query (e.g., the set of frequent itemsets bought in fall in Q4) .  and ask for further circumstances under which they hold.

A second observation is that for certain patterns, if they hold under a given circumstance c, then they nec- essarily hold in a weaker circumstance c?, where weak- er means c implies c?. As an example, consider frequen- t itemset as a pattern. Let c be the circumstance city = ?westfield? & s t a t e  = ?nj? & month = ?October? and c? be state  = ?nj?. Clearly, c? is weaker than c in the sense that every transaction that satisfies c necessarily satisfies c?. E- quivalently, c is stronger than c?. Let S be any itemset. Sup- pose S is frequent under c. Then it must be frequent under c?. ? In the terminology of Ng et al. [NLHF?98, LNHP991, we call such patterns monotone. For these patterns, there is an intrinsic interest in determining the ?strongest? circum- stances under which they hold. E.g., suppose we know the set of strongest circumstances C under which an itemset s is frequent. Intuitively, this covers the space of all circum- stances under which S is frequent, since we know S is fre- quent exactly under those circumstances that are weaker than some circumstance in C. Another example of a pattern that  ?Throughout this paper, we use the absolute support count for frequen- cy. This count can come from a specified fraction, say 0.1%. of the mal database size, for example.

1063-6382/01 $lO.OOfiO 2001 IEEE    has this property occurs in (42): if interstate calls totaling over $50 were made under a given circumstance (e.g., over a week), then under a weaker circumstance (e.g., over a month containing that week), the total cannot be less.

Some patterns tend to be anti-monotone in that whenev- er they hold in a circumstance, they necessarily hold in a stronger circumstance. A natural example is the pattern ?in- terstate calls made total under $25.? Clearly, if this holds for a given circumstance, it must hold for any stronger circum- stance2. For such patterns, following the motivation similar to the previous paragrahph, there is an intrinsic interest in asking for the weakest circumstances under which they hold.

The rest of this paper is organized as follows. In Sec- tion 2 we formalize the notions item, transaction, and cir- cumstance. We show in Section 3, that for many appli- cations, the space of circumstances forms a lattice, which we call the circumstance lattice. This lattice generalizes the cube lattice, often employed by algorithms for comput- ing the data cube operator proposed by Gray et al. [G*96].

(See also [BR99], e.g.) In Section 4 we develop an algorith- m, called minCirc, for determining the strongest (resp., the weakest) circumstances under which patterns satisfying ei- ther the monotone or the anti-monotone property hold. We also establish a close relationship between the problem of finding all circumstances under which a given pattern holds and data cube computation, and we make obvious adapta- tions to two representative popular algorithms for cube, in- cluding BUC ([BR99]) and chunked array cube ([ZDN97]).

In Section 5 we experimentally evaluate the relative perfor- mance of all three algorithms. Our results show algorith- m minCirc is competitive in that there are conditions under which each algorithm (minCirc included) is the algorithm of choice.

Some of the motivating queries above go beyond mere- ly finding circumstances under which a pattern holds. To capture this class of queries, in Section 6, we propose a no- tion called Armstrong Basis and show that a variety of useful queries can be answered from this basis. Section 7 discusses related work, and summarizes the paper.

2 Problem Definition Traditional framework of association mining regards  each attribute-value pair as an ?item? and attempts to discov- er associations between sets of items using measures such as support and confidence. Given a set of transactions, not all attributes are alike. Some attributes may have a unique val- ue per transaction while others may have multiple values in a given transaction. As an example, items bought from a su- permarket naturally corresponds to an attribute that has a set of values per transaction. On the other hand, the timestam- p associated with a transaction typically has a unique value per transaction. While treating all attributes alike is certain-  ly.

?The strongest circumstace is false, which verifies any pattern vacuous-  ly possible, we contend that i t  misses out on the opportuni- ties of exploring and exploiting the different structures and properties associated with dissimilar attributes. E.g., for an attribute such as time, different values are mutually exclusive in that a transaction can only have one value for it. By dis- tinguishing among such attributes, we can formulate queries like ?what are the intervals during which there is a signifi- cant volume of sales on dessert items?. As another example, we can ask ?which are the locations where there is a strong association among a set of items being purchased?. We con- tend that mining queries of this form can be best facilitated by developing a framework where attributes that are inher- ently like items (we call them ?item attributes? below), i.e.

have a set of values per transaction, are distinguished from those that are more like time (we call them ?circumstance attributes? below), which have a unique value per transac- tion. The potential ?item attributes? need further to be distin- guished from ?descriptive attributes,? such as cost or quanti- ty. These considerations motivate the following definitions.

Consider a table over a set of attributes. We wish to regard tuples, or possibly sets of tuples, in this table as transactions. To this end, we can designate any set of at- tributes in the table as a transaction id, as long as this at- tribute set uniquely determines the transaction. E.g., the set customer I D ,  t i m e I D  might constitute the transaction id for the transactions for a table containing a supermarket sales data. As another example, l o c a t i o n I D ,  proaID, t i m e I D might serve as a transaction id for tuples in a fact table of a data warehouse containing sales data. In general, given a table T over a set of attributes R, we assume that the user specifies a subset of attributes, say K C R, as its transaction id. This induces a partition on the table with tuples sharing the same K-value being in the same cell of the partition.

We call an attribute A E R \ K circumstance attribute provided the functional dependency K + A  holds for T .  For other attributes A E R \ K it follows from the axioms of functional and multivalued dependencies that the multival- ued dependency K t> Y ,  where A E Y, will hold in T .  If in addition the functional dependency K A  + Y \ A holds we call A an item attribute, otherwise we call A a descriptive attribute.

As an example, consider a table s a l e s ( t i d ,  l o c a t i o n ,  t i m e , p r o d u c t ,  quant i ty)  with t i d  acting as the transaction id, in the sense that each tid-value uniquely determines a set of tuples, in which a set of products are sold at a given time and location.

We also have {t id}+{product ,  quant i ty},  and { t i d , p r o d u c t }  + {quant i ty} .  Thus product  is an item attribute and q u a n t i t y  is a descriptive attribute, while l o c a t i o n  and t i m e  are circumstance attributes.

An itemset is then a set of attribute-value pairs, where  ,3Each cell might be a singleton set.

41n this paper we do not focus on descriptive attributes. In the con-  strained data mining framework of [LNHP99, GLWOO]. constraints would typically be formulated over circumstance and descriptive attributes.

the attribute is an item attribute and the value comes from its domain. By a circumstance, we mean a conjunction of pred- icates of the form at tdvalue ,  where attr is a circumstance attribute, and value comes from its domain. Here, typically, we want to set 8 to be equality. When dealing with total- ly ordered circumstance attributes (e.g., time, distance, etc.), we want to allow 8 to be 5, since on such domains, intervals over which patterns hold are of interest. A transaction saris- fies a circumstance $, provided it satisfies the predicate $.

Given a support threshold s, we say that an itemset S is frequent in a transaction database provided at least s trans- actions contain S.  More generally, given an itemset S and a circumstance $, we say that S is frequent under the circum- stance $ provided s transactions among those satisfying the circumstance $ contain S. The traditional notions of sup- port and confidence in association mining can be relativized to circumstances: the support of S under 11, is the number of transactions satisfying 11, that contain S; for itemsets S and T, the confidence of a rule S 3 T  under $ is the proportion, among the transactions satisfying $, the number of transac- tions containing S U T over those that contain S.

In data mining, there are various notions of interesting patterns, such as frequent itemsets, associations, correlation- s, etc. In the bulk of this paper, we shall focus on frequent itemsets and address the following question: Given a spe- cific itemset S, a support threshold s, find all circumstances under which S is frequent.

The reason we pick frequency as a representative pattern for detailed study is that i t  is the simplest among those dis- cussed and yet epitomizes the issues that arise in studying other patterns. Also, as discussed previously, frequent item- sets are often used as a basis for finding other patterns like associations, correlations, etc.

Later, we will address how techniques developed for solving the above problem can be adapted for handling as- sociations.

3 Structure of Circumstances In general, the space of circumstances forms a lattice,  called the circumstance lattice. To see this, recall that cir- cumstances are conditions comparing attributes with values.

Define a partial order 5 on the set of all circumstances Q, by setting $ 5 q5 exactly when 11, logically implies 4. It is well-known that a collection of sentences ordered this way forms a lattice. In particular, true is the top element and false is the bottom element. Figure 1 shows an example cir- cumstance lattice for two circumstance attributes A, B each with two values in its domain. For simplicity, the figure only considers circumstances involving equality conditions.

Circumstance attributes sometimes come with hierar- chies. E.g., locations may have the hierarchy storeId- city-state. It is a straightforward matter to see that the  5Recall, we use an absolute number.

6Thus, if fewer than s transactions satisfy y!J, S is infrequent under y!J.

A=al& B=hl A=al&  B=b2 A=aZ & B=bI A d  & B=bZ  Figure 1. An Example Circumstance Lattice  notion of lattice extends to circumstance attributes with hi- erarchies, as well as to circumstances involving inequalities.

In the rest of the paper, unless otherwise specified, we on- ly consider circumstances involving equality conditions and no attribute hierarchies. More general conditions are left for future work.

Let (Z, C _ )  denote the itemset lattice for a given applica- tion and (C, 5)  the circumstance lattice w.r.t. the circum- stances pertaining to the application. We can construct a new structure C x Z by taking the direct product of the t- wo lattices. On this structure, we can define two alternative orders: the first order 5:; is defined as (I), S)<:;(I)', S') iff $ _< $' and S S'; the second order <:&,, is defined as ( $ J , S ) ~ ~ ~ ~ , , ( $ ' , S ' )  iff $ 5 $' and S' E S. A struc- ture (12, SI, 5 2 )  with two partial orders is called a bilattice [Fit91], provided each of (IC, 51) and (L, 5 2 )  is a lattice, and a certain condition relating the two orders hold. It can be shown that (C x Z, <:F7 is actually a bilattice, which we call the circumstance-itemset bilattice.

The bilattice formulation gives a semantic basis for known mining problems and inspires newer ones too. For example, the classical problem of mining frequent itemset- .

s from a given transaction database reduces to finding all itemsets S such that S is frequent under true. More gen- erally, if we wanted to find all frequent sets in a selection view (not involving aggregates) defined by conditions over the transaction database, this would amount to finding the set {S 1 S is an itemset frequent under $}, where $J is the conjunction of conditions defining the view. Let F denote the subset of the bilattice containing precisely those pairs ($, S) such that S is frequent under $. Then frequent set mining given 11, corresponds to finding {S I ($, S) E F}.

Circumstance mining for a given itemset S corresponds to finding the set (11, I ($ ,S)  E F}. The bilattice structure is instrumental in shaping the concept of an Amstrong Basis, the main object of Section 6.

In the rest of this section, we consider general patterns involving itemsets and classify them according to the prop- erties they satisfy. Since we want to do this in relation to circumstances, we need some notation first. We define apat- tem over the bilattice (C x 1) as any predicate over it, i.e.

any subset of the elements of the bilattice. E.g., frequency corresponds to the collection of elements ($, S) E (C x Z), such that S is frequent under $; correlation corresponds to the collection of elements ($,S) such that S is a cor- related set under +. As another example, consider the s- tatement minprchase(S)  that says the total dollar value     of the purchase on items in S must exceed a threshold, or prchasecap(S)  that says the total dollar value of the pur- chase on items in S must be below a threshold. Each of these defines a subset of the bilattice. We use the notation m($, S), or simply p($ ,  S), to say pattern p holds for the element ($, S )  in database 2). E.g., when p is purchasecap, this means over those transactions satisfying circumstance $, the total dollar value of purchase of items in S does not exceed the given threshold.

We say that a pattern p is c-monotone, provided whenever p($ ,  S) holds, p ( 4 ,  S) holds as well, for every $ 5 4. It is c-anti-monotone, provided whenever p($ ,  S) holds, p ( 4 ,  S) holds as well, for every 4 5 $. The notions i-monotone and i-anti-monotone are defined analogously and correspond ex- actly to what was referred to as monotone and anti-monotone in the frequent set mining literature [NLHP98]. In gener- al, a pattern may be monotone or anti-monotone w.r.t. ei- ther lattice. For example, frequency is c-monotone and i- anti-monotone. The pattern purchasecap is c-anti-monotone and i-anti-monotone, while minpurchase is i-monotone and c-monotone. Being correlated as defined in [BMS97] is i- monotone but neither c-monotone nor c-anti-monotone.

Many of these patterns satisfy interesting identities. The following hoposition is proved using the axioms of bilatti- cies.

Proposition 1 Let p be any pattern. Then we have the fol- lowing.

1. i f p  is i-anti-monotone, then {$ I p ( $ , S ) }  n {$ I p ( $ ,  T)} c {IC, I p($ ,  S n T ) } ;  a similar identify holds w.zt. union.

2. i f p  is c-monotone, then {S  I p($  A 4,S)} C {s 1 p ( + ,  S ) }  n { S  \ p ( 4 ,  S ) } ;  a similar identity holds w.r.t.

disjunction.

3. similar identities holds for  i-monotone and c-anti- monotone patterns.

As an example, frequency satisfies the two identities in the proposition. On the other hand, purchasecap sat- isfies the first identity as well as the identity: {S I purchasecap($,S)} n { S  I purchasecap(4, S)} c { S  I purchasecap($ A 4, S)}.

The identities above illustrate what kind of pruning op- portunities exist when processing complex queries. For in- stance, suppose we want to find itemsets that are frequent- ly bought in northeast and in fall, and that we already have itemsets corresponding frequent purchases in northeast and frequent purchases in fall. Then one way of processing this query is to verify each itemset in the intersection for fre- quency in the circumstance ?northeast and fall.? A strategy like this makes sense when we want to process queries cor- responding to circumstances at multiple granularities.

4 Algorithms for Circumstance Mining In this section, we address the problem of finding all cir-  cumstances in which a given pattern holds for a given item-  set, when the pattern satisfies some monotonicity properties.

For the sake of concreteness, we pick frequency as a proto- typical example. Since it is a c-monotone pattern, knowing an itemset is frequent under a circumstance tells us it is fre- quent under all circumstances implied by it. Thus, we really wish to find the minimal circumstances under which a given itemset is frequent, in the sense that the itemset is not fre- quent in any stronger circumstance. For c-anti-monotone patterns such as purchasecap, the set of maximal circum- stances in which the pattern holds w.r.t. a given itemset sum- marizes the whole space of circumstances where this pattern holds for that itemset. In the rest of this section, we focus on frequency, a c-monotone pattern.

We note that the circumstance lattice bears a strong sim- ilarity to the lattice used for data cube computation. Indeed, when we limit ourselves to circumstances involving only equality, the circumstance lattice is identical to the lattice corresponding to the (instances of) various group-bys. This suggests any of the algorithms developed for cube should be useful for mining circumstances. On the other hand, given the c-monotone nature of frequency, a traditional Apriori- style level-wise pruning strategy should be relevant as well, except we use the circumstance lattice in place of the itemset lattice. Rather than trying to be exhaustive, in this section, we will discuss simple adaptations to some existing cube al- gorithms, as well as propose a new algorithm, for finding minimal circumstances under which a given itemset is fre- quent.

The representative cube algorithms we pick are the BUC (bottom-up cube, which is actually top-down in our lattice formulation) algorithm proposed by Beyer and Ramakrish-  bottom-up in our formulation) due to Zhao et al. [ZDN97].

The main efficiency of chunk may-based algorithm comes from using a multi-dimensional address space for storing the base table and the cube itself. This affords great compres- sion. It uses the minimum weight spanning tree data struc- ture to optimize cube computation. In addition, i t  also us- es techniques for handling sparse data. Figure 2 shows the adapted chunk array algorithm for finding minimal circum- stances in which a given itemset S is frequent. The basic idea is we perform a search in the circumstance lattice from the strongest circumstances to the weaker ones. As soon as S is found to be frequent in a circumstance, we know it must be minimal. Clearly, all minimal circumstances can be found in this way. The correctness follows from these observations.

The BUC algorithm was also designed to incorporate constraints on groups such as count(*) > n, which says the group must have at least n elements. However, the ex- press goal of BUC was to find all groups satisfying given constraints (as opposed to minimal ones). Figure 3 shows our adaptation to the BUC algorithm for finding minimal circumstances. A minor modification consists in comput- ing the support of S in every group (i.e. circumstance). Just like original BUC, whenever S is found to be infrequent in  nan [BR99] and the chunk array-based algorithm (which is     Algorithm Modified Chunk Array; Input: an itemset S, a support threshold s. and a transaction database V ; Output: the set of minimal circumstances in which S is frequent; Run chunk array algorithm. with the following modifications:  - - 1. no aggregate computation (other than verifying the support constraint) is need-  ed;  2. when processing a circumstance $, compute the support of S in $; if it exceeds 3, add Q to the output; circumstances that are implied by $ are pruned;  Figure 2. Adaptation to Chunk Array Algorithm for finding minimal circumstances.

Algorithm Modified BUC; Input: an itemset S. a support threshold 3, and a transaction database 2); Output: the set of minimal circumstances in which S is frequent; - -  1. run BUC algorithm, with the following modifications:  (a) no aggregate computation (other than verifying the support constraint) is needed;  (b) when processing a circumstance $, compute the support of S in $; if i t  is below s, circumstances that are imply $,including $, are pruned; otherwise, Q is retained;  2. do post-processing to eliminate non-minimal circumstances;  Figure 3. Adaptation to BUC Algorithm for finding minimal circumstances.

a circumstance, we prune away all stronger ones. A final modification consists in post-processing: BUC (modified as above) finds all circumstances in which S is frequent; we need to efficiently detect and eliminate non-minimal ones.

Correctness of modified BUC is straightforward.

Post-processing: Circumstances are processed in in- creasing order of their length (e.g., A = a1 A B = bl is of length 2) in BUC. In addition, within each length, we can make sure they are processed in lexicographical order (by assuming some arbitrary, but fixed order on circumstance at- tributes and their domains). Let n be the number of circum- stance attributes. Then all circumstances of length n found by the modified BUC algorithm are minimal, by definition.

For each circumstance of length k < n, we check if i t  is im- plied by any circumstance of length k + 1. The implication test is syntactic and checking length (k + 1)-circumstances for implication can be done efficiently using binary search.

The last algorithm, called minCirc, that we propose in this section is in some sense inspired by Apriori. The ba- sic idea is since frequency is c-monotone, we can scan the transactions repeatedly and compute the support of S cor- responding to all circumstances of a given length in each iteration, starting from length 1. When S is infrequent in a circumstance, we prune all circumstances of which it  is a ?prefix?. E.g., when S is infrequent in A = a1 A B  = b l ,  we ignore A = a1 A B = bl A C = c1, etc. A literal implemen- tation of this idea, however, can lead to a poor performance.

The reason is when we have several circumstance attributes, each with a domain of reasonable cardinality, the effort re- quired for candidate generation (even with this pruning) is appreciable. Our algorithm instead imposes an order on the dimensions and exploits this order in quickly pruning away  Algorithm minCirc; Input: an itemset S, a support threshold s. and a transaction database V ; Output: the set of minimal circumstances in which S is frequent;  I .  choose some circ attribute order, say D1, . . ., D,; 2. scan 2, once and obtain counts of S in all atomic circumstances. i.e. in all  D1 -circumstances, _.., D,  -circumstances; 3. let FI be the set of all atomic circumstances in which S is frequent; 4. hash the circumstances and write the counts;  5. create a linked list of candidates as follows:  (a) for ( 1  <= i <= n ;  i + +) {  - -  let zi E Fl be any Di -value; create a linked list with head 5; ; f o r ( i <  j < = n ; j + + ) {  let z j  E Fl be any Dj-value; append a node containing zj to the linked list with head zi; } I  (b) k = 2 ; (c) while the current set of linked lists is non-empty {  scan V and obtain counts of all k-circumstances appearing in the linked lists; purge each node corresponding to a circumstance in which S is infrequent; construct linked lists for (k  + 1)-circumstances (candidates) as follows: for each linked list L with head H of size k {  if (length(L) >= 2) { for each node (containing a circ) 5 in L {  temporarily create a new list with head HI, which contains all nodes of L after node z, that correspond to later circumstance atmbutes; //since l e n g t h ( l )  >= 2, there will be at //least one such node; delete from this list those nodes y such that some k- subset of Hzy does not appear in any linked list with head size k; //this test can be performed quickly by sorting;  } ) ) I 6. eliminate non-minimal circumstances via post-processing;  Figure 4. A linked list based mining algorithm for finding minimal circumstances.

many candidate circumstances early. As an example, if S is found to be frequent in A = a1 A D = dl, where D is the last circumstance attribute, there is no need to check ?suffix- es? of this circumstance. Figure 4 shows the algorithm. It makes use of the following notions.

Suppose D1, ..., Dn is a chosen order on the circum- stance attributes and let c and d be values of any circum- stance attributes. Then c 4 d iff c is a Di-value and d is a Dj-value, for some i < j ;  c and d are said to be in- comparable otherwise. We sometimes find it convenient to write circumstances such as A = a1 A B = bl A C = c1, simply as alblcl, relying on the fixed chosen order of cir- cumstance attributes. We use linked lists with heads and n- odes (head is not a node). For a linked list, its length rep- resents the number of nodes it has. Each list represents a set of candidate circumstances of a given length. E.g., a list with head a1 bl and nodes c1, . . . c5, dz d6, d7 (where ci E dom(C) and dj E don(D)) represents the set of cir- cumstances a1 bl c1, al bl c5, . . . , a1 bl d7. We say a circum-     stance x1 . . . xk appears in a linked list provided its head contains z1 . . . zk-1 and one of its nodes contains the circ xk. In the algorithm, we refer to a circumstance in which s is frequent as a frequent circumstance.

The last step of the algorithm invokes post-processing, discussed earlier. In particular, since the circumstances are processed in increasing order of length, the same post- processing as used for BUC can be used. The correctness of the algorithm follows from the following facts: (i) a circum- stance is never explicitly pruned unless S is infrequent in it; (ii) circumstances that are suffixes of those ending in the last circumstance attribute value are never considered for count- ing, e.g., if we have the circumstance attribute order ABCD, then we do not consider suffixes of b l d l ,  for instance; every suffix of such circumstances are covered by some circum- stances that we do consider; e.g., the suffix bldlcl E blcldl, and one of its (not necessarily proper) prefixes, e.g., bl, is al- ways considered by the algorithm.

Relevance to Cube: A significant feature of the chunk array and BUC algorithms (with modifications) is that they can be used to compute not only (minimal) circumstances where a pattern holds for an itemset, but also any required aggregate measure value at each such circumstance. What can we say about the algorithm minCirc, inspired by Apriori- style intuition? It turns out by associating a field for com- puting the aggregate measure with each node, we can indeed compute the aggregate incrementally. This argument hold- s for all distributive and algebraic aggregate functions, for which most known fast cube algorithms have been develope- d. This establishes an interesting tight connection between frequent set mining and cube computation. Our framework is applicable not just to frequent sets but to more general itemset patterns with monotone or anti-monotone properties w.r.t. itemsets or circumstances.

5 Experimental Results We conducted a series of experiments to evaluate the ef-  fectiveness of the algorithms presented for mining minimal circumstances in which an itemset is frequent. More specifi- cally, we compared the three algorithms to find out how they perform under different conditions. For the given three al- gorithms, BUC is a top-down algorithm? and uses a ?depth first? principle; minCirc is also a top down algorithm but us- es level-wise principle; Chunk Array by nature is a bottom- up algorithm and it also uses the level wise principle. So these three algorithms represent three different approaches to mining minimal circumstance sets from different direc- tions of the circumstance lattice, and using different search strategies. We want to compare the performance of the algo- rithms not only by total CPU time, but also by the number of candidate sets the algorithms generated and tested.

A Pentium I1 200 processor with 64 KB memory is used for all the experiments. All the algorithms are implemented  ?See remark in the beginning of Section 4  in C++ running on a Windows NT platform. For test data we generated synthetic sets of transactions for various cases.

We used a Zipf distribution to control the skewness of the data, as is commonly used in experiments of this kind, see e. g. [BR99]. To be fair to all the algorithms, we tested both the cases where the transaction database fits in main memory and where it  is disk-resident. We used 1 to 7 circumstance at- tributes, and each attribute had a domain of cardinality 100.

The number of transactions varied from 10,000 to 500,000, the support threshold from 5 to 1000, and skewness was var- ied from 0, which is uniform distribution, to 3. In each of the experiments, the number of candidate sets generated, the peak memory allocation, the total time, the YO time, and the number of passes over the transaction database was record- ed for analysis. For lack of space, we only show the results from a representative subset of our experiments.

In the first set of experiments we used transaction databases which fit in main memory. All other parameters were varied as described above. We measured the total CPU time as a function of the number of transactions. The results were fairly uniform, and a typical representative of this set is shown in Figure A. There we can see that with the increase of the number of transactions, there is a linear increase in the total time for all three algorithms. But the slope for min- Circ is steeper than the slope for BUC and Chunk Array.

This is due to the fact that BUC and Chunk Array make on- ly one pass over the transaction database, whereas minCirc makes up to as many passes as the number of circumstance attributes.

The second set of experiments (Figures B - E) conduct- ed is for disk-resident transaction databases. When the data is uniformly distributed (skewness = 0), minCirc gives the best performance among the three algorithms. Figure B dis- plays the results for support threshold at 100, and number of circumstance attributes at 5. When the number of trans- actions increases, the total time increases rapidly for BUC.

When the number of transaction is equal to 500,OOO, the total time for BUC is SO times larger than for mincirc. Even for Chunk Array the total time is 5 times larger than minCirc.

For minCirc, the upper bound on the number of passes over the transaction database equals the number of circumstance attributes. On the other hand, for BUC the only upper bound is the size of the circumstance lattice. Furthermore, as the data is uniformly distributed, Chunk Array has to do many more computations for each of the atomic circumstance set- s, so minCirc also performs better than Chunk Array under these conditions.

However, with the increase of the skewness, as the data is not uniformly distributed, the performance of Chunk Ar- ray will improve quite drastically since some of the atomic circumstance sets will not appear in the transaction database.

Figure C gives the results for the same condition as Figure B, except that the skewness is equal to 1. Similar performance can be observed for BUC. Under these conditions, Chunk     Array is the algorithm of choice. * Another factor that affects the performance of these three  algorithms, especially Chunk Array, is the number of cir- cumstance attributes. In our case, when the number is 5, and the domain of each circumstance attribute has cardinal- ity 100, there is potentially 1O'O atomic circumstance sets appearing in the transaction database. When we increase the number of circumstance attributes to 6, there might be up to  atomic circumstance sets. This is a large increase for Chunk Array, but for minCirc this just implies at most one more pass over the transaction database. For these condi- tions minCirc becomes the best choice. Figure D shows the results for 6 circumstance attributes. When the number of transactions is smaller than 300,000, the total time for BUC and Chunk Array is similar, and both about twice the total time of minCirc. When the number of transactions increases to 500,000, the total time for minCirc is 5 times smaller than the other two algorithms.

Another interesting experiment to do is to observe the effect of varying number of circumstance attributes (Figure E). When the number is smaller than 5, Chunk Array has the best performance and the slope of the curve is very grad- ual. However, once the number of circumstance attributes is greater than 6, there is a dramatic decrease in the perfor- mance of Chunk Array and it  becomes the worst among the three algorithms. With the increase in the number of cir- cumstance attributes, the total time for both BUC and min- Circ increase much more slowly when compared to Chunk Array.

In the third and last set of experiments (Figures F - H) we tested the effect of the skewness of the data and the pos- sible reasons for an increase in the number of candidate sets generated. When the data is uniformly distributed and is disk-resident, minCirc has the best performance and BUC gives the worst performance. When skewness is greater than 1, the curve for all three algorithms become flat. Chunk Ar- ray shows the best performance while BUC still is the worst one. In another experiment, not shown here, we tested main memory resident transaction databases, while keeping the other parameters as in Figure F, we observed BUC almost unaffected by the skewness of the data, and gives the best performance among the three algorithms.

In the final experiments of this set, we tested the number of candidate sets generated. This measure is not affected by the residency of the transaction database. The skewness of the data does affect the performance, but not the relative dif- ferences between the three algorithms. We show two exper- iments, namely Figure G and H for the number of candidate sets generated vs. the number of transactions. Skewness is at 1, and the number of circumstance attributes is 5. Figure G shows the result for the threshold at 1000, and Figure H shows the result for the threshold at 5. When the threshold is high, the pruning strategies for BUC and minCirc are very effective, so at 500,000 transactions the number of candidate  'Note that the time scale is different in Figures B and C.

Dalabase'?

No. of CircW No. ofcirc's? ;r\ LwA DaIr diSVibutiO"? T I U C ~ ' ~ ?  mincirc  BUC  BUC ChumkArray mincirc Chunk Amay  Figure 5. Decision tree for choosing a mining algorithm.

sets generated by minCirc is over 500 times smaller than that of Chunk Array, and BUC generates 80 times fewer candi- date sets than Chunk Array. However, when the threshold is small, the pruning strategy for BUC loses its efficiency, and the number of candidate sets becomes the largest. minCirc also loses some of its pruning efficiency, but it still compa- rable to Chunk Array.

The prescriptions emerging from our experiments are de- scribed in the decision tree of Figure 5.

6 Beyond Circumstance Mining So far in the paper, we have been concerned with finding  circumstances where a pattern holds for a given itemset. In general, mining is exploratory in nature. An analyst might wish to find the circumstances under which a pattern hold- s for an itemset and then find out which other sets satisfy the pattern under those circumstances. (Query Q3 in Sec- tion 1 is an example involving such exploration.) Or she might start with a circumstance (say, fall in northeast), find the itemsets that satisfy a pattern, and then attempt to char- acterize the circumstances under which these latter sets sat- isfy the pattern (e.g., see query QS in Section 1). Queries of this kind require that the analyst be able to freely move between the worlds of itemsets and circumstances. The bi- lattice of itemsets and circumstances developed in Section 3 is ideal for supporting such "migration." In this section, we propose the notion of an Armstrong Basis as a basis for sup- porting a variety of queries involving such migration. Recall the bilattice (C x 1, <,U$, <lzwn) defined in Section 3. An element ($J, S) in the bilattice is 5,UE-minimul w.r.t. a prop- erty p provided it satisfies p and whenever (4, T) satisfies p and (4, T)L;F($J, S), we have ($J, S) = (4, T). Minimality w.r.t. the other order and maximality w.r.t. either order is defined analogously.

Definition 1 (Armstrong Basis) Let p be a pattern over cir- cumstances and itemsets, and 23 be a transaction database.

Suppose p is c-monotone and i-anti-monotone. Then the Armstrong Basis of p w.r.t. V is defined as A B b )  = {($, S) E (C x I) 1 ($J, S) is <&,, - minimal w.r.t. p}.

The significance of Armstrong Basis will be clear from the following proposition.

Proposition2 Let D a transaction database, p be a c- monotone and i-anti-monotone pattem, and AB (m) the Armstrong Basis o f p  w.Et. D. Then the following statements are true.

1. For every element (I), S) E A B b ) ,  pattem p does not hold for any proper superset of S at circumstance $; similarly, p does not hold for S at any circumstance that is strictly stronger (i.e. it implies, but is not equiv- alent to, $) than $.

2. Every element of the bilattice (C  x 1) which satisfies the above statement belongs to d a b ) .

3. For every element (4 ,T)  in the bilattice, p ~ ( 4 , T ) holds if and only if there is an element ($J, S) E AU(m), such that T S,  and 4 is logically implied by *.

The proposition above also explains the reason behind the terminology used for this notion. Indeed, Armstrong Ba- sis has a structure that is very similar to the notion of Arm- strong couples introduced for families of functional depen- dencies by Armstrong, almost three decades ago [W74].

The notion of Armstrong Basis is applicable to any pat- tern that satisfies some monotonicity properties w.r.t. item- sets and circumstances, either by defining minimality or maximality w.r.t. the appropriate order in the bilattice.

E.g., for minpurchase, a c-monotone and i-monotone pat- tern, the Armstrong Basis should be defined as the set of <:P,-minimal elements satisfying minpurchase. Note that for such a pattern the Armstrong Basis has the complete information about the space of circumstance-itemset pairs valid w.r.t. this pattern, as stated in part (3) of the proposi- tion.

In the rest of this section, we focus on algorithms for computing the Armstrong Basis. For a typical transaction database and a pattern such as frequency, the size of the Armstrong Basis can be substantial. So why should any- one want to compute it? The analogy we give is cube. For large databases, cube is a time- and space-expensive opera- tion. Indeed, a user may not be interested in seeing a cube in its entirety. Its utility comes from its ability to service a variety of aggregation queries involving multiple group-bys.

In a similar manner, if the Armstrong Basis for a pattern is available, queries (such as those in Section 1) involving migration between itemsets and circumstances can be an- swered efficiently. Thus, it is worth investigating efficient algorithms for its computation. We start with the follow- ing notions, defined for patterns p that are c-monotone and i-anti-monotone. Similar notions exist for other patterns.

Let p be a c-monotone and i-anti-monotone pattern. De- fine a function p;  : c 4 1  as p i ($ )  = {s I p($ ,  s) A vs' : S' 3 S + lp(@,S')}, and afunctionp, : 1 4  aspc(S)  = {$ I p(I+!J,s)  A V 4  : & < $ + ip ($ , s )} .  These functions  Algorithm ArmBase; Input: a transaction database D, a c-monotone and i-anti-monotone pattern p .

Output: A O ( P D ) - -  1. the Armstrong Basis is initially empty;  2. do a level wise sweep of the itemset lattice and do the following:  3. for each set S at the current level, if S is not a subset of S' for some ($, S') in the current Armstrong Basis, then find the collection of minimal circumstances circs at which the pattem holds for S;  4. for each circumstance $J E c ircs ,  find the collection of maximal set- s i tsets4 for which the pattem holds under $; let ITSETS be the union of i t se t sq ,  overall $ E c ircss ;  5 .  each pair in circs x itsets is an element of A B ( p = ) ;  6. terminate when all levels are processed, or when no itemset at a given level makes it to A B ( p n ) ;  Figure 6. Algorithm for computing the Arm- strong Basis of a pattern in a transaction database  have obvious extensions to collections of circumstances or sets: for a set of circumstances Q ,  p i ( e >  = U,,,pi($), and for a collection of itemsets S ,  p c ( S )  = u s E s p c ( S ) .

Note that with such extensions, the functions p;  and pc can be composed. For example, let p be frequency. Then for a collection of circumstances !I!, the composite function p ,  op; does the following: first, it finds all maximal itemsets fre- quent under any circumstance in Q ;  second for each such itemset, it finds all minimal circumstances under which it is frequent. Similar remarks hold for other patterns. We say a set of circumstances !I! covers another set @ provided V$ E @, 3I+!J E Q, such that I+!J implies &. Similarly, a col- lection of itemsets S covers collection T provided V T  E T , 35 E S ,  such that T C S. Notice that the Armstrong Ba- sis d B ( m )  is itself a cover of the set of all pairs ($, s) for which the pattern p holds. We have the following result.

Proposition 3 Let pi and p, befunctions as de$ned above, w.zt. a c-monotone and i-anti-monotonepattern p. Then the following holds:  I .  for every collection of circumstances Q,  pc(p;(Q)) cov- ers Q; furthermore, P, (pi (pc ( P i  ( Q ) )  1) = P, (pi  ( Q)) .

2. for every collection of itemsets S ,  pi(p,(S)) covers S ; furthermore, pi (pc (pi (pc (S)) 1) = pi (P, (S) 1.

The proposition can be proved by appealing to the defi- nitions ofp i  and p,. It immediately suggests the naive algo- rithm in Figure 6 for computing AB(m).

This algorithm relies on the following facts: circss = p , ( S )  and itsets = p;(p,(S) .  For each pair (q5,T) E (p,(S) x pi(p,(S), if p held for T at a stronger circum- stance, then it would hold there for S too, which contradicts the minimality of & w.r.t. S. Similarly, if p held for a prop- er superset of T at 4, this would contradict the maximality of T w.r.t. circumstances in p,(S). The correctness follows from this. Unfortunately, this algorithm can be prohibitively expensive. In particular, it does not share any effort between successive iterations. There is, of course, a dual algorithm     which works off the circumstance lattice, sweeping it level- wise, and is very expensive as well. In the full paper, we develop efficient algorithms for computing the Armstrong Basis, suppressed here for lack of space.

7 Discussion and Conclusions Recently, there has been interest in mining long pattern-  s. In this context, algorithms for mining maximal frequent sets have been proposed [Bay98, AAPOO]. This is relevant because of our interest in minimal circumstances. However, these algorithms operate in a different lattice and the mech- anisms of the algorithms appear different. Given our interest in the Armstrong Basis, these algorithms might offer yet an- other way to efficiently find maximal frequent itemsets as- sociated with a circumstance, in that context. The recent- ly proposed FP-tree method [HPYOO, PHMOO] has obvious relevance for circumstance mining and also for the compu- tation of the Armstrong Basis. We are currently exploring efficient algorithms for the Armstrong Basis based on the FP-tree method. Finally, as previously noted, mining for cir- cumstances in which a given pattern (like frequent itemset) holds has a close relationship with cube computation. For lack of space, we refer the reader to the recent paper [BR99] which surveys many of the cube methods, and which also describes the BUC algorithm.

In conclusion, basing pattern mining on a clear sepa- ration between item attributes and circumstance attributes pays its dividends: the close ties with the cube computa- tion paradigm, and between cube computation and mining of i-anti-monotone patterns open up new and promising di- rections for future work. As motivated earlier, finding mini- mal (for c-monotone patterns) circumstances under which a pattern holds for a specific itemset is useful in its own right.

The framework developed in this paper goes far beyondjust support-based mining of frequent itemsets. This is achieved using properties of patterns on the bilattice of circumstances and itemsets. Finally, we proposed a useful notion of Arm- strong Basis with which we can find (say, minimal) circum- stances where a pattern holds for an itemset, and then find which other itemsets satisfy the pattern under these circum- stances. This is in the spirit of mining the result of min- ing, one of the important problems identified by Imielinski and Mannila in their seminal paper [ImMa96]. Just as data cube is an important operator for supporting queries involv- ing multiple group-bys, for queries involving migration be- tween itemsets and circumstances, Armstrong Basis can pro- vide effective support. More work is needed for its efficient computation and its use in answering such mining queries.

[AS951  [AAPOO]  R. Agrawal and R. Srikant. Mining sequential pattem- s. (ICDE?95), pp. 3-14 R. Agrawal, C. C. Aggarwal, and V. V. V. Prasad.

Depth first generation of long pattems. KDD 2000, to appear.

R. J. Bayardo. Efficiently mining long pattems from databases. (SIGMOD ?98). pp. 85-93 K. S .  Beyer, R. Ramakrishnan: Bottom-Up Computa- tion of Sparse and Iceberg CUBES. SIGMOD ?99, pp.

S. Brin, R. Motwani, and C. Silverstein. Beyond mar- ket basket: Generalizing association rules to correla- tions. SIGMOD ?97, pp 265-276 G. Dong and 1. Li. Efficient mining of emerging pat- tems: Discovering trends and differences. In KDD?99,  M. Fitting. Bilattices and the semantics of logic pro- gramming. J. Logic Programming 11.91-1 16.

G. Grahne, L. V. S .  Lakshmanan, X. Wang. Efficient Mining of Constrained Correlated Sets. ICDE 2000,  J. Gray, A. Bosworth, A. Layman, H. Pirahesh. Data Cube: A Relational Aggregation Operator Generaliz- ing Group-By, Cross-Tab, and Sub-Total. ICDE 1996,  J. Han, J. Pei, and Y. Yin. Mining frequent pattems without candidate generation. In SIGMOD 2000, pp.

1-12  [lmMa96] T. Imielinski, H. Mannila: A Database Perspective on Knowledge Discovery. CACM 39(11): 58-64 (1996).

[LNHP99] L. V. S .  Lakshmanan, R. Ng, J. Han, and A. Pang.

Optimization of constrained frequent set queries with 2-variable constraints. In SIGMOD?99, pp. 157-168  [LSW97] B. Lent, A. Swami, and J. Widom. Clustering associ- ation rules. In Proc. ICDE?97, pp. 220-231  [MTV94] H. Mannila, H. Toivonen, and A. I. Verkamo. Effi- cient algorithms for discovering association rules. In  [MTV97] H. Mannila, H Toivonen, and A. I. Verkamo. Dis- covery of frequent episodes in event sequences. Darn Mining and Knowledge Discovery, 1 :259-289.1997.

[NLHP98] R. Ng, L. V. S .  Lakshmanan, J. Han, and A. Pang.

Exploratory mining and pruning optimizations of con- strained associations rules. In SIGMOD?98, pp. 13-24 J. Pei, J. Han, and R. Mao. CLOSET An efficien- t algorithm for mining frequent closed itemsets. In  [SVA97] R. Srikant, Q. Vu, and R. Agrawal. Mining association rules with item constraints. In KDD?97, pp. 67-73  [W74] W. W. Armstrong: Dependency Structures of Data Base Relationships. IFIP Congress 1974: 580-583.

[ZaHs99] M.J. Zaki and C. Hsiao. Charm: an efficient algorithm for closed association rule mining. Tech. Report., RPI, 1999.

[ZDN97] Y. Zhao, P. Deshpande, J. F. Naughton: An Array- Based Algorithm for Simultaneous Multidimensional Aggregates. SIGMOD?97, pp. 159-1 70.

[Bay981  [BR99]  359-370 [BMS97]  [DL991  pp. 43-52 [Fit911  [GLWOO]  pp. 5 12-52 1, [G*96]  pp. 152-1 59.

