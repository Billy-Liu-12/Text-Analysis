

Abstract- The FP-growth algorithm is currently one of the fastest approaches to frequent item set mining. Fuzzy logic provides a mathematical framework where the entire range of the data lies in between 0 and 1. The PSO algorithm was developed from observations of the social behavior of animals, including bird flocking and fish schooling. It is easier to implement than evolutionary algorithms because it only involves a single operator for updating solutions. In contrast, evolutionary algorithms require a particular representation and specific methods for cross-over, mutation, and selection. Furthermore, PSO has been found to be very effective in a wide variety of applications, being able to produce good solutions at a very low computational cost. In this paper, we have considered the fuzzified dataset and have implemented various frequent pattern mining techniques. Out of the various frequent pattern mining techniques it was found that Frequent Pattern (FP) growth method yields us better results on a fuzzy dataset. Here, the frequent patterns obtained are considered as the set of initial population. For the selection criteria, we had considered the mean squared residue score rather using the threshold value. It was observed that out of the four fuzzy based frequent mining techniques, the PSO based fuzzy FP growth technique finds the best individual frequent patterns. Also, the run time of the algorithm and the number of frequent patterns generated is far better than the rest of the techniques used.

Keywords-Fuzzy logic; Frequent pattern mining; Apriori algorithm; Vertical data format; FP growth tree; Particle swarm optimization

I. INTRODUCTION  Frequent pattern mining has been a focused theme in data mining research for over a decade. It is a core technique used in many mining tasks like sequential pattern mining, structured pattern mining, correlation mining, associative classification, and frequent pattern-based clustering [1] , as well as their broad applications [2][3][4]. So, a great effort has been dedicated to this research and tremendous progress has been made to develop efficient and scalable algorithms for frequent pattern mining [4][5][6]. All these algorithms deal with precise data sets [7] [8]. Such data is characterized by known and definite existence of the item  or events in the transactions.

As we know that microarray datasets may contain up to thousands or tens of thousands of columns (genes) but only tens or hundreds of rows (samples). Discovering frequent patterns from  microarray datasets is very important and useful, especially to discover association rules, which can not only reveal biological relevant associations between genes and environments/categories to identify gene regulation pathways but also help to uncover gene networks [9] and to discover bi-clustering of gene expression. However, these high-dimensional microarray datasets pose a great challenge for existing frequent pattern discovery algorithms. Though there are a large number of algorithms that have been developed for frequent pattern discovery and closed pattern mining their basic approaches are based on item enumeration where combinations of items are tested to search for frequent patterns. Because of this, their running time increases exponentially with increasing average length of the records.  Hence, the high dimensional microarray datasets render most of these algorithms impractical.

Apriori algorithm is one of the oldest frequent mining approach that is based on the property of Apriori that for an item set to be frequent. A k-item set is frequent only if all of its sub-item sets are frequent. This implies that frequent item sets can be mined by first scanning the database to find the frequent 1-itemsets, then using the frequent 1-itemsets to generate candidate frequent 2 item sets, and check against the database to obtain the frequent 2-item sets. This process iterates until no more frequent k- item sets can be generated for some k. This is the essence of the Apriori algorithm. But apriori algorithm suffers by scanning the database while finding the k-item sets in each and every step. Due to which the processing overhead is drastically increased. Because of this FP growth method came in which is an enhanced version of apriori algorithm.

FP-growth works in a divide-and-conquer way. The first scan of the database derives a list of frequent items in which items are ordered by frequency in descending order. According to the frequency-descending list, the database is compressed into a frequent-pattern tree, or FP- tree, which retains the item set association information.

The FP-tree is mined by starting from each frequent  Particle Swarm Optimization Based Fuzzy Frequent Pattern Mining from Gene Expression Data  Shruti Mishra, Debahuti Mishra, and Sandeep Kumar Satapathy  Institute of Technical Education & Research Siksha O Anusandhan University, Bhubaneswar, Odisha, India  shruti_m2129@yahoo.co.in, debahuti@iter.ac.in, sandeepkumar04@gmail.com      length-1 pattern (as an initial suffix pattern), constructing its conditional pattern base (a ?sub-database?, which consists of the set of prefix paths in the FP-tree co- occurring with the suffix pattern), then constructing its conditional FP-tree, and performing mining recursively on such a tree. The pattern growth is achieved by the concatenation of the suffix pattern with the frequent patterns generated from a conditional FP-tree. The FP- growth algorithm transforms the problem of finding long frequent patterns to searching for shorter ones recursively and then concatenating the suffix. It uses the least frequent items as a suffix, offering good selectivity.

Performance studies demonstrate that the method substantially reduces search time.

A. Goal of the paper  In our paper, we have used various frequent pattern mining techniques on the fuzzy dataset that we have obtained to find out various frequent patterns. Then using the frequent patterns we have applied the particle swarm optimization (PSO) algorithm considering the mean squared residue score as the fitness function to find out the individual perfect clusters with certain reasonable running time.

B. Proposed Model  Here, we have proposed two figures, fig.1 and fig.2. Fig.1 shows the general scenario where we have fuzzified the original dataset and have categorized them into high and low after which we have implemented various frequent pattern mining techniques. Also, we are calculating the mean squared residue for the frequent patterns and then we are starting with our PSO algorithm. Fig.2 shows the application of the entire particle swarm optimization algorithm on the frequent patterns or particles.

C. Paper Layout  This paper is arranged in the following manner, section I gives the introduction and a brief idea about the techniques, section II deals with related work on frequent pattern mining and particle swarm optimization algorithm. Section III describes about the FP growth algorithm, Fuzzy FP-growth algorithm and particle swarm optimization (PSO). Section IV gives the analysis of our work and finally section V gives the conclusion and future directions of our work.

Figure 1. Proposed Model  Figure 2. Structure of Particle swarm optimization algorithm (PSO)

II. RELATED WORK  Data mining can be categorized into several models, including association rules, clustering and classification.

Among these models, association rule mining is the most widely applied method. The Apriori algorithm is the most representative algorithm. It consists of many modified algorithms that focus on improving its efficiency and accuracy. However, two parameters, minimal support and confidence, are always determined by the decision-maker      him/herself or through trial-and-error; and thus, the algorithm lacks both objectiveness and efficiency.

Han et al. [7] designed strategies to mine top-k frequent patterns for effectiveness and efficiency. Cohen et al. [10] developed a family of effective algorithms for finding interesting associations. Roddick et al. [11] discussed the independent thresholds and context dependent thresholds for measuring time-varying interestingness of events. Hipp et al. [12] presented a new mining approach that postpones constraints from mining to evaluation. Wang et al. [13] designed a confidence- driven mining strategy without minimum-support.

However, these approaches only attempt to avoid specifying the minimum-support.

Cohen and Castro [14] presented a modified PSOA that featured self organization of the updating rule for clustering analysis. In their PSOA, it is not necessary to calculate fitness value. The results show that it is better than the K-means method. Kuo et al. [15] proposed a PSKO which combined PSO-clustering with K-means.

The PSKOA was evaluated in four datasets, and compared with the performance of K-means clustering, PSO-clustering and hybrid PSO. The experimental results show that the PSKO algorithms outperform other algorithms. In addition, Kuo and Lin [16] further used binary PSO to solve a clustering analysis problem and applied it to an order clustering problem. Chen and Ye proposed a PSO based clustering algorithm [17], which they called PSO-clustering. This method used minimal target function in PSO to automatically search for the data group center in multi-dimensional space.  Compared with traditional clustering algorithms, PSO-clustering requires fewer parameter settings and avoids local optimal solutions.

In 2003, Sousa et al. applied PSO to classification problems in a decision support system [18]. Three kinds of algorithms generated from PSO, discrete PSO, linear decreasing weight PSO, and constructed PSO, were compared with a genetic algorithm. The results showed that PSO has better convergence. Wang et al. employed PSO to solve a traveling salesman problem (TSP) in 2003 [19]. The results indicated the search space of possible solutions diminishes and convergence can be achieved very quickly under the condition that the PSO has the same optimal solution, as is the case with traditional methods.



III. PRELIMINARIES  A. FP-growth Algorithm  One of the currently fastest and most popular algorithms for frequent item set mining is the FP-growth algorithm. It is based on a prefix tree representation of the given  database of transactions (called an FP-tree), which can save considerable amounts of memory for storing the transactions. The basic idea of the FP-growth algorithm can be described as a recursive elimination scheme: in a preprocessing step delete all items from the transactions that are not frequent individually, i.e., do not appear in a user-specified minimum number of transactions. Then select all transactions that contain the least frequent item (least frequent among those that are frequent) and delete this item from them. Recurse to process the obtained reduced database, remembering that the item sets found in the recursion share the deleted item as a prefix. On return, remove the processed item also from the database of all transactions and start over, i.e., process the second frequent item etc. In these processing steps the prefix tree, which is enhanced by links between the branches, is exploited to quickly find the transactions containing a given item and also to remove this item from the transactions after it has been processed.

B. Fuzzy FP-growth tree  Briefly, fuzzy set theory allows an object to partially belong to a set with a membership degree between 0 and 1. For the construction of a fuzzy FP growth tree we first convert our raw dataset into a fuzzy set (that is with membership degree between 0 and 1). Now the procedure of FP growth tree is to be carried for constructing the fuzzy FP tree but with an exception of using only the fuzzy dataset. For example, in our earlier papers based on fuzzy frequent pattern mining we had constructed a fuzzy FP tree based on the parameters as shown in table 1. The global frequent list that we had obtained was by processing all the transactions of the database. The processing step is the thresholding of the fuzzy membership function with a threshold ?. Here, ?=0.7 and we obtain the frequencies like,   Freq.={(G1L:2.65),(G1H:0.9),(G2L:1.6),(G2H:1.7),(G3L:0) , (G3H:2.75),(G4L:0), (G4H:1.5)}  Minimum support required is at least 1.5 and hence the global frequency list is:  Freq.List= [(G3H:2.75), (G1L:2.65), (G2H:1.7), (G2L:1.6), (G4H:1.5)]  The fuzzy FP tree that was constructed from the below mentioned table 1 is shown in fig.3.

Table 1. Fuzzy frequent pattern mining  .

Figure 3. The Fuzzy frequent pattern tree  C. Particle Swarm optimization (PSO)  The particle swarm optimization (PSO) technique is a population based stochastic optimization technique first introduced by Kennedy and Eberhart (1995). It belongs to the category of Swarm Intelligence methods; it is also an evolutionary computation method inspired by the metaphor of social interaction and communication such as bird flocking and fish schooling. PSO is a bio-inspired search technique that has shown comparable performance to that of evolutionary algorithms [20] [21]. Like evolutionary algorithms, PSO is useful when other techniques such as gradient descend or direct analytical discovery are not applicable. Combinatory and real valued optimization problems, in which the optimization surface possesses many locally optimal solutions, are well suited for swarm optimization. The main concept of PSO originates from the study of fauna behavior.

PSO is initialized with a group of random particles (solutions) and then searches for optima by updating generations. During all iterations, each particle is updated by following the two ?best? values. The first one is the best solution (fitness) it has achieved so far. The fitness value is also stored. This value is called ?pbest?. The other ?best? value that is tracked by the particle swarm optimizer is the best value obtained so far by any particle in the population. This best value is a global best and is called ?gbest?. After finding the two best values, each particle updates its corresponding velocity and position according to the following equations (1) and (2):  )()()()( 21 idid old id  new id xgbestrandcxpbestrandcvv xcxcv  (1) new id  old id  new id vxx vx     (2)   The variable definitions are: vid is the particle velocity of the idth particle, xid is the is the idth, or current particle, i is the particle?s number; d is the dimension of searching space, rand ( ) is a random number in (0, 1), c1is the individual factor,c2 is the societal factor.



IV. EXPERIMENTAL RESULT  Step-1: Fuzzification of the original dataset  We have considered a gene expression dataset and applied the Gaussian membership method to fuzzify (shown in table 2) the set given as follows in (3):  Y=exp ^ (-(x- mean))   ^ 2 / 2* variance                       (3)  Table 2.  Original dataset converted into fuzzy dataset  Step-2: Subdivision of data into high and low & Implementation of frequent pattern mining algorithms      0 500 1000 1500 2000 2500 3000 3500        Time in millisecs-------------->  N o.

o f P  at te  rn s  ge ne  ra te  d- ---  --- ---  --- -->     Fuzzy Apriori Fuzzy Vertical Data Format Fuzzy FP PSO Fuzzy FP  2865      3010        3233       3455        Time in millisecs-------------->  N o.

o f P  at te  rn s  ge ne  ra te  d- ---  --- ---  --- -->     PSO based FP growth Fuzzy FP growth Fuzzy vertical data format Fuzzy Apriori  Here, the fuzzy dataset obtained (that is a combination of genes and conditions) is subdivided into two sets of category namely, ?high? and ?low?.

Step- 3: Calculation of mean squared residue score This step plays an important role in the entire algorithm as we use this as the selection criteria rather than any input threshold value. In fact, for finding the perfect clusters in genetic algorithm we had earlier used the same score and it proved to give us a better result.

Step 4: Implementation of Particle Swarm Optimization (PSO)  The basic procedure of this algorithm is shown as below:  a) Initialize ? The set of frequent patterns are considered as the initial populations.

b) Encoding ?Here, we consider a set of frequent patterns as initial population which we encode into an 8-bit binary string where, each single pattern is considered as a particle. Each particle is represented by a binary string of fixed length ?a+b?, where a and b are the number of genes and conditions of the gene expression data. If a bit is set to 1, it means that the corresponding gene or condition belongs to the bicluster; otherwise it does not  c) Evaluation ? The fitness function can be evaluated as :  fitness (fp) = (fp. mean residue score) / (no. of fp)  (4)  d) Find the pbest ? If the fitness value of particle i is better than its best fitness value (pbest) in history, then set current fitness value as the new pbest to particle i.

e) Find the gbest ? If any pbest is updated and it is better than the current gbest, then set gbest to the current value.

f) Update velocity and position ? Update velocity and move to the next position as equation (1) and (2).

g) Stopping criterion ? If the numbers of iterations or CPU time are met, then stop; otherwise go back to (c).

Analyzing the results obtained by the three fuzzified frequent pattern mining algorithms and the result obtained by the PSO based fuzzy FP growth algorithm (as shown in fig. 4 and fig.5), it was found that the later yields better results. It was also found that the number of frequent patterns generated is more and the runtime of the  algorithm tends to be much faster than the three algorithms (as shown is table 3).

Table 3. Comparison of the four algorithm based on the required parameters  Figure 4. Result of the four algorithms obtained  Figure 5. Graphical chart representation of the four algorithms obtained

V. CONCLUSION AND FUTURE WORK  Now, from the above discussions we are able to analyze that applying frequent pattern mining algorithms on a fuzzified set yields us better results. Also, when using particle swarm optimization algorithm on a FP growth method it even yields us much better results as expected to discover perfect patterns. In fact, other evolutionary algorithms can also deliver us much better results than this. Our future work would be dedicated to find out all those algorithms (preferably soft computing algorithms and evolutionary algorithms) that can deliver us much better results than the one obtained.

