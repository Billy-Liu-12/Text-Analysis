A Descriptive Big Data Model Using Grounded Theory

Abstract ? Big Data is an emerging research topic. The term remains fuzzy and jeopardizes to become an umbrella term.

Straight forward investigations are inhibited since the research field is not well defined, yet. This paper executes expert interviews to identify a common understanding. Hereby, the findings are coded and conceptualized until a descriptive Big Data model is developed by using Grounded Theory. It becomes evident that Big Data is use case driven and forms an interdisciplinary research field. By classifying several Big Data papers it gets obvious that not all of them belong to this research field. The paper contributes to the intensive discussion about the term Big Data in illustrating the underlying area of discourse. A classification to set the research area apart from others can be achieved to support a goal oriented research in future.

Keywords: Big Data, Grounded Theory, Interview, Qualitative

I.  INTRODUCTION The amount of various business data is growing  exponential while their accompanied storing and processing in a traditional way makes a task fulfillment complicated. In this context, the term Big Data occurs increasingly in scientific discussions and publications. [1] A common definition belongs to Gartner: ?Big data is high-volume, high-velocity, and high-variety information assets that demand cost-effective innovative forms of information processing for enhanced insight and decision making? [2], but considering others, existing definitions are diverse. For example, Bizer et al. [3] follow a semantic perspective and whereby others focusing on processing huge amounts of data [4]. As a consequence, two major issues are conspicuous. On the one hand side, the positioning of goal oriented publications to solve business administration related problems within a Big Data context is impeded. [1] On the other hand side, already established research areas such as High Performance Computing (HPC) tend to recoin themselves as Big Data to obtain more attention. [5] Thus, the possible research field of Big Data is fuzzy and the term seems to be only used as marketing buzzword. So, the demand of a theoretical base has already been stated since relevant topics and related theories are unknown [1]. In this context, it is the paper?s goal to isolate the inherent nature of the Big Data topic and to deduce a descriptive model as initial step. Thus, research areas are stronger defined to set  Big Data apart from and business administrated issues can be addressed.

A quantitative proof of the fuzzy term Big Data seems to be not possible since underlying factors are unknown. In addition, no research has been published, yet, which addresses a theoretical reprocessing of this topic. It is helpful that already Miles and Huberman [6] indicate that qualitative methods are useful in an early state of research to gain a professional perspective based on longstanding experience.

In this context, we accomplished expert interviews to gain the consent understanding [7]. Results are coded and conceptualized by using Grounded Theory [8]. A Big Data description model is derived that represents expert opinions.

The resulting categories can be used as patterns for future Big Data research to define the research area apart from others. Hereby, the paper contributes to the intensive discussion about the term Big Data in illustrating the underlying area of discourse.

The paper?s concourse is as follows: Chapter 2 provides the research design. Hereby, expert interviews and Grounded Theory as method are briefly shown to be able to create an appropriate basis. The proceeding of methods is illustrated, thus, a rigor research design is ensured. Chapter 3 provides the descriptive model. Single categorizes are presented and discussed. The model is used in Chapter 4. Hereby, several Big Data papers are classified whether they belong to Big Data or not. So the model application and its term specification of Big Data get evident. The paper is summarized in Chapter 5, where implications and further research topics are highlighted.



II. RESEARCH DESIGN The research design is subdivided into four stages (see  Figure 1).

DOI 10.1109/CSE.2013.132    DOI 10.1109/CSE.2013.132      Figure 1.  Research Design (based on [8, 9])  After stating the research goal and problem formulation, we are conducting a literature review [10] in a second step to arrange the existing knowledge about Big Data models.

Academic databases like IEEE xplore, AIS library, ScienceDirect, and EBSCOhost Web were used and relevant papers were identified through the search items Big Data Theory and Big Data Model in keywords, title, and abstract.

In fact, no article was identified. Thus, no published research exists that addresses a specification of the term Big Data through a theoretical approach. Due to this reason, we decided to use expert interviews to obtain initial insights.

Expert interviews are based on different phases. They address the problem and eligible experts as well as a guideline based interview conducting and evaluation. [7] Within the execution, 20 experts from international acting IT companies were interviewed. To ensure the expert?s competence, a minimum of one year Big Data experience was assumed. The telephone interviews varied between 30 and 60 minutes and were conducted between July and December 2012. Comparability of the results is ensured through interview guidelines. Thereby, pretests were conducted. As a result, characteristics, key drivers, definitions, distinctions, potentials, use cases, issues, strategies of Big Data, and expert statistics were gathered.

All interviews were recorded and transcribed.

Grounded Theory [8] collects and analyzes qualitative data (like expert interviews) and is accepted and widespread in information systems research [11]. The method aims the generation of conceptual properties, categories, and relationships through a combination of inductive, deductive, and abductive reasoning in iterative cycles. The usage in an early research status to describe a phenomenon is often applied [12]. Big Data belongs to a relative new research area without existing theoretical fundament, yet. Thus, Grounded Theory is used here since the generation and discovering of concepts and inherent relationships relies to the strengths of the method. [9]  The analysis in Grounded Theory consists of four steps [9]. Within the open coding, so called slices of data (transcribed interviews) are broken down into several categories. These categories are described in terms of their properties. Thereby, several categories were assigned to the transcribed interview passages, thus similar statements get apparent. This is followed by the theoretical coding, where relations between categories are established. Thereby, new slices of data are added and categorized until a theoretical saturation arises and no novel category emerges. Three researches were involved during the model development until all agreed in a theoretical saturation. The result is a substantive approach, which is applicable to the emerging particular area. It is recommended to use existing coding schemata within the theoretical coding, because the negation of them leads usually to confused and unclear theoretically coding [9]. An often used one belongs to [13], where the phenomenon (Big Data) belongs to the center. The original reasons of emergence, the context, possible strategies to face the phenomenon, and consequences are put into relation. As a result, all identified categories were shrunken until a substantive model emerged. Exemplary scientific Big Data publications are classified to illustrate how the model supports the academic sharpening of the term.



III. DESCRIPTIVE MODEL The used five categories and their subcategories are  described in this chapter and discussed. Numbers in brackets belongs to the amount of experts stating a category. E.g., (10/20) mean out of 20 probates, ten mentioned this category. Thus, the importance of a component is illustrated.

This supports the verification whether a Big Data paper belongs to the research area.

A. Phenomenon In general, Big Data can be seen as phenomenon and  emerges through context and causes. This gets apparent since popular terms volume, velocity, and variety [2] are implied.

This leads to the point that Big Data can be understood as circumstance, in which an increased data volume must be processed or/and stored. In fact, strategies to face this phenomenon will change in time. But they can always be subdivided into a functional and technology part. The consequences are resulting from all schema categories and are issue and advantage related. Considering this proposed approach, all mentioned categories are not novel. They are representing own and nowadays mostly well researched disciplines, which emerged by their own causes and context.

Based on gained experiences and use cases of this single disciplines in past emerged the phenomenon Big Data. It applies that a combination of the underlying disciplines achieves a higher value as a single discipline can obtain for its own. The combination of these concepts will only be useful, if a specific idea exists, which produces a positive value. Since the underlying disciplines are data-intensive the phenomenon Big Data is into place. In this context, Big Data represents an interdisciplinary research area and is use case driven. Figure 2 shows the describing Big Data model.

Figure 2.  Big Data descriptive Model  B. Causes In fact, most causes are well-known and defining Big  Data not by itself. The requirements can be summarized as a need for an extensive environmental understanding. Experts stated a particular demand on monitoring, prediction, and decision support as well as a need to explain circumstances in natural sciences. New possibilities are seen through context and strategy, which enable the achievement of an improved understanding as before. In a dynamic world, this knowledge must be increasingly produced in a timely manner. [14] Hereby, time adequate processing belongs as one part to velocity [2]. Concepts like decision latency or automatization [15] represent core aspects and are often mentioned. Hence, information technology tasks must accomplish user needs in time. Besides requirements are two more causes.

One often applied aspect belongs to marketing. Hereby, experts stated the possibility that Big Data is not novel and only driven by sales persons to maximum their revenue. In this case, Big Data is no new research area and should be ignored by academics. Comparable statements can be found for Business Intelligence (BI), which counts today as one of the most important fields for practice and academic in information systems. [16] This leads to the point that further work has to proof, whether Big Data is marketing driven or not.

Another cause belongs to market competition. It is slightly related to the requirements since it represents the source of them. In dynamic markets, companies have to reduce production cycles, safe costs, identify early trends, react fast, and maximize their profit in a sustainable way.

[14]  C. Context Context describes the circumstances in which Big Data  evolved. During our coding, we recognized that most of all context related expert statements were already shown the Knowledge Based Theory (KBT) of the Firm [17]. Here, experts stated: ?For the purpose of decision support, different data sources and structures must be consolidated to achieve a most extensive acquisition of information? or ?Formats, structures and sources must be integrated to acquire competitive advantages? or ?The novelty of Big Data consists besides huge data volume within the usage of heterogeneous sources and formats?. In this work, KPT is rather used as concept as a theory and should not interpreted as a mapping mechanism to Grounded Theory. Thus, the KBT of the Firm considers knowledge as unique and most strategically significant resource [17] by focusing on knowledge integration and combination to achieve a competitive advantage [18]. According to [19] information technologies are able to support firms within the KBT since it can be used to synthesize, enhance, and expedite large- scale intra- and inter-firm knowledge management. In this context, an increasing trend is noticeable that integrate various data sources, structures and formats to obtain a competitive advantages. On a practical view, this is also known as variety [2].

However, the KBT arises as well as through underlying reasons. An expert stated an increasing awareness of analyzing transaction data. Hereby, transactions are no longer the fundamental basis of analyses as we understand it in BI [14] but rather an explanatory variable. Hereby, the way why a transaction appears (e.g. interaction data analysis of Social Media, forum, web shop, GPS tracking etc. of a customer until the purchase) comes more and more into focus to gain a more extensive environment view.

Another subcategory is seen in machine generated versus user generated content. User generated content belongs to contributed data, information or media not created by the provider of the web service itself. Examples are hotel ratings, wikis, or videos. This kind of information has rapidly grown during the last years and provides an interesting analysis fundament. [20] In contrast, machine generated content relies to any data consisting discrete events that have been created automatically from a computer application, process or other machines in absents of any human intervention. [21] In this context, experts stated the increasing IT pervasion. Thus, research and practice find themselves in a world of radio- frequency identification (RFID) chips, log files, internet of thinks, GPS tracking (vehicles and mobile phones), and any kind of sensor technology that can be used to gain an extensive environment understanding. In most of the cases, this type of data occurs as stream.

In streaming, a continuous flow of data must be handled.

This data relies often to semi-structured time series and implies issues of storing and processing since the permanent transfer produces an enormous data volume. In contrast to static data, only a subset of data can be considered. [22] Experts stated that in most of the cases streaming data must be analyzed in real time. In addition, science application produces stream data where the storing of all features is not possible.

Other key drivers are seen in Internet, Web 2.0, and Social Media. Most of the Big Data phenomena occur through the Internet. One aspect belongs to the physical infrastructure, which enables a world spanning interconnection through all participants and consequently an exchange of information and data. Only through this infrastructure, the realization of concepts like cloud, grid, or distributed computing got possible. In addition, it acts as immense information storage for every participant. Besides that, experts stated the fact that the internet evolved as interactive platform where people collaborate and share information, also known as Web 2.0.

[23] Within this movement companies like Google, Yahoo!, Twitter, eBay, Amazon, or Facebook emerged. The rapidly growing content and users forced the companies to develop cost effective and scalable technologies. Here, technologies like MapReduce or Hadoop emerged and are now patterns for further applications. [24] Besides that, Web 2.0 enabled the extensive creation and exchange of various user generated content. One of the most important applications of Web 2.0 belongs to the Social Media. [25] In this context, most of the experts stated that the step into Social Media enables the most exciting possibility to gain a more advanced view about customer and environment as before.

Another aspect belongs to the data type itself. While in past, only structured data went into the analyzed focus, possibilities raised to interpret and gain information automatically from multimedia sources nowadays (image, video, audio, and text). Thus, human generated content gets machine-readable. Prominent examples are seen in text mining or video and image sentiment analysis.

This new types of data and sources implicate another context. Whereby traditional data was clean and precise the new ones are rather fuzzy. Text mining or sentiment analyses  are always a translation from human content to machine- readable data. During this procedure, information gets lost and diffuse. Even a tracking system with data from vehicle, mobile devices works not exactly. In addition, combining unknown sources means also unknown data quality. This aspect is already known as veracity [26], but applications have to deal with these uncertainties.

The increasing improvements in storage and processing technologies are also stated as one Big Data reason.

Especially, associated cost savings enables the implementation of scalable systems. Thus, companies and research organizations can fulfill their requirements within the budget limitation. Another aspect belongs to the open source movement. Thus, the generation of content within the internet remains in most of the cases free of cost. In addition, key Big Data technologies are usually open source tools.

Here, organizations observing saving possibilities through projects like Hadoop or MangoDB.

In addition, the legal framework represents a relevant context. Experts stated that besides all possibilities of Big Data future applications are situated between data privacy restrictions. Thus, it is not legal to combine different sources to gain a broader view over customer and environment, always. Legislation depends on the country and must be proofed in detail. Furthermore, users are developing awareness of data privacy. As a result, countries have to define a framework where users rights are guaranteed and new Big Data concepts are enabled.

D. Strategy As already stated, the inherent nature of Big Data is not  defined by strategy (shown as punctuate arrow). Strategy is necessary to overcome the phenomenon but is not an essential component. Hereby, strategy is stretched through technology and functional aspects. Technology addresses the phenomenon (caused by context and causes) and is not limited in future. Experts agree in three concepts.

HPC belongs to intensive calculation or storing tasks. The processing of those jobs through personal computer is not appropriate. Thus, concepts like cloud, grid, distributed, and parallel computing are implied and mentioned. The nature of these concepts allows the scaling of needed hardware to fulfill the task in an appropriate manner. [27]  Besides high performance hardware, efficient working algorithms were also mentioned. Thus, the time and computation complexity of traditional algorithms suffers. All steps of processing, storing, and analyzing are time relevant in Big Data and need to be supported. One possible solution refers to [28]. In addition, parallel programming models took place into Big Data technologies. Here, Googles MapReduce was often mentioned, where the framework specifies map and reduce functions to execute them in a parallel way [29].

An important part of technologies belongs to the database discussion. Some experts argue relational databases are not able to meet the requirements due the lack of scalability and query performance, where others allude proceedings in relational databases. Often stated NoSQL approaches are key-value, column, or document oriented databases. Besides the discussion between NoSQL and relational databases, the     movement to in-memory technologies was also often mentioned. Nevertheless, this kind of technologies enables the computation and storing of high volume data in a proper time. The processing of heterogeneous data sources cannot be solved by just technology.

Thus, the second strategy belongs to functional concept.

Expert believes that the Big Data phenomenon can be tackled through organizational approaches. Hereby, information management (IM) is mentioned. IM aims the efficient utilization of information in respect of the organizational goal. It manages information systems, overarching executive functions and information and communication technologies as well as the handling of information in an economic way.

The last one belongs to information logistic, information lifecycle, information demand, information source, information quality, information provision and usages management. [30] During the interviews experts stated several Big Data related aspects to the information management. Hereby, already common approaches like BI were mentioned. But, not all concepts are transferable to Big Data. In future, existing methods must be proofed of Big Data suitability. If not, new methods will emerge.

One core concept belongs to the mapping of structured and unstructured (image, text, audio, video) data. In this context, Big Data application designers must combine various data sources in a useful manner to gain a broader view about reality. Nevertheless, available concepts are rare to guide those activities [1]. Industry experts have to figure out what kind of data sources can support the target activity.

In addition, semantic concepts are seen as possible benefit within the combination of various sources.

Another aspect is seen in the task-oriented provision of information. This aspect shares similar aspects to information logistic. Both address the provision of the right information, at the right time, in the right amount and place in an adequate quality. Thereby, information logistics consider only the data flow. The flow of unstructured data is often uncertain. In addition, information logistic neglects the value of information itself. [30]  Information lifecycle management is also seen as one possible aspect within Big Data strategies. It aims the creation of an equal status between information demand and supply. Hereby, an update of both in an iterative cycle is necessary. Especially within Big Data information management is important, because various data sources and information technology are used whereby quality has to be ensured. Besides that, storage aspects have to be considered.

Thus, most of the information loses value in time and a continuous storing remains questionable.

The identification of relevant information can support this aspect. This can be done through management methods as well as through machine learning based techniques.

Experts stated that task relevant information must be automatically declared within data sources. This is essential, especially in Big Data.

Another field belongs to the data reduction. As in information identification this can be done through management or mathematical methods. Thereby, any kind of technique that allows a more efficient way of handling data  without losing relevant information has to be considered.

Possible works can be seen in [31].

17 of 20 experts labeled the analyses of Big Data as core concept. Through the stated context and content several kinds of analyze techniques evolved and now considered in Big Data. The most mentioned method belongs to the Knowledge Discovery in Database (KDD) also referred to as Data Mining. It represents the non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data [32]. There exist several mentioned subcategories of Data Mining. Text Mining transfers unstructured textual data into a machine-readable content, which is subsequently used by machine based learning techniques. [33] Another one belongs to Web Mining. This can be categorized into content, structure, and usage mining. [34] Similar approaches were stated for image, audio and video analyzes. A typical example can be seen by [35]. Even the growing Social Media brings new practices along. One often stated example is seen in social network analyses, where pattern of relationships and interactions between social actors are analyzed to discover underlying structures. [36] Predictive Analytics was often mentioned, which addresses the predictions of the future itself. [37] Certain participants remarked that the nature of Big Data prevents modeling and cleansing. Here, the data must be processed on-the-fly. Another analyses focus is seen within the detection of relationships between different data sources. Thus, analysts are supported in hypothesizing.

Besides the stated analyze concepts, experts highlighted the advanced analysis character of Big Data. Classical Online Analytical Processing (OLAP) cubes as known from BI are not included. Few experts stated the application of advanced visualization techniques. Possible examples of high volume presentation can be obtained from [38].

The final strategy belongs to simulation. Just some experts quoted this fact since it is more seen from an academic perspective. Nevertheless, in most of the cases, simulations handle voluminous data of various inputs. [16] Hereby, the processing time must be acceptable. It is defined as process, which drives a system model with suitable inputs an observing the resultant outputs, whereby simulation one process by another. [39] Examples for Big Data aided work is stated in [40].

E. Consequences According to [13], a phenomenon causes always  consequences. In context of Big Data, they are divided into issues, competitive advantages and research findings. The most stated problem belongs to missing Big Data experts, both technicians as well as functional oriented people. Thus, only a handful of specialists are experienced in new technologies like Hadoop or MapReduce, whereby the demand is huge. Here universes must adjust their study programs. [41] The requirements on functional experts are enormous. Experts stated that more analytical and domain specific knowledge is necessary as well as an understanding for underlying technical processes to be able to deal with this phenomenon. Especially the functional integration of various data sources is seen as complex, but a technical integration is     critical. Within the phenomenon data is stored in divers NoSQL approaches, most of the time schema-less. Hereby, the integration of traditional concepts remains difficult. This is aggravated by the fact data is received as stream and processed on-the-fly.

Considering veracity, a huge data quality issue arises.

This affects the quality of analytical and simulation results since the kind of data remains fuzzy. Other experts stated quality regarded problems, because data sources are often obtained from third parties. Last but not least, NoSQL is mostly inconsistent.

Another issue is seen in assigning relevant information to the task. Thereby, the spectrum of possible Big Data sources is broad and not any content achieves an advanced understanding of the environment. Hence, functional experts must be supported by organisational as well as by technical methods in order to discovery valuable relationships and items within data sources. So investigations can be performed in a goal oriented way.

The second consequence belongs to the competitive advantages. There are closely related to the discussed causes.

Thus, the most important fact is seen in gaining an extensive view about customer and market. Hereby, functional experts are supported through analytics and simulations as well as through organisational methods. Driven by the illustrated context, the expectation of new business ideas and markets emerged. Here, experts imagine the combination of various strategies since research enable it at the moment. In addition, the achievement of unsuccessful ambitious aims was also stated. At least, Big Data is not seen as end in itself.

Financial advantages must appear. Savings can be obtained by the usage of open source, saved memory, and computational capacity as well as trough an increased automatization. Depending on the analytical/ simulation  focus savings and returns are possible to achieve. This goes along with the KBF theory. Both, users as well as consultants stated this fact.

The final consequence addresses the academic application affected by Big Data. Thus, the context and current available strategies allows the exploration of new knowledge. New relationships and insights, especially within natural science, were quoted. Efficient algorithms, simulations, and analytics are highlighted to gain a broader view about the framing environment. Due to the reason that the expert?s common background is business, only two experts stated these concepts.



IV. CLASSIFICATION OF ACADEMIC PUBLICATIONS We developed a descriptive model to discover Big Data?s  inherent nature. The research area of Big Data is interdisciplinary and belongs to the need of storing and processing a huge amount of data, driven by context and content. Around the emerging phenomenon various research disciplines has been positioned. Thus, Big Data belongs to applied sciences. Progress in the undergraduate sciences is welcome, but not part of the research field of Big Data itself, e.g. text mining, HPC, etc. To demonstrate the model application, we have chosen several papers out of a Big Data state-of-the-art publication randomly [1] and have classified them whether they belong to Big Data or not. The results are illustrated in Table 1. Numbers within the tables are related to the concept abbreviations in Figure 2. State-of-the-art papers are excluded due to their informing character. Since not each single concept must be fulfilled to be Big Data, we left place for a small discussion part. The amount of mentioned times enables a statement how strong the field is related to Big Data.

TABLE I.  BIG DATA DESCRIPTIVE MODEL  Source Cause Context Phenomenon Strategy Consequence Big Data Publication  Discussion  [42]  C1; C1a; C1b;  K1; K1c; K1d; K1e; K1f, K2 P1  S1; S1a: S1b; S1c; S2a; S2aa; S2ab; S2b  Q1; Q1c; Q2c; Q3 Yes  The paper presents a pluggable prototype system to analyze and integrate high volume biometric data (fingerprints, iris image, facial image, voice, DNA) in real time. The system is cloud based underlying systems are HBase and ZooKeeper.

Various analyses and deduplication occurs.

[43] C1a; C1b  K1; K1b; K1c; K1d; K1f P1  S2ae; S2ab; S2b; S2c Q2; Q2a; Q2b Yes  Web forum discussions are used to predict stock returns. Here structured data like market return, volatility, or trading volume are combined with unstructured text. Using stepwise regression and sentiment analysis.

[44]  C1; C1a; C1b  K1; K1d; K1e; K1f P1 S2ab; S2b Q1c; Q3 Yes  Enormous amount of behavioral data is explored to identify spatiotemporal dynamics of criminal events with the hope of identifying patterns in their aggregation that may be useful to predict and prevent future crimes. Hereby, relationships in both space and time, cross- and auto-correlation measures are combined.

[45]  C1; C1a; C1b  K1; K1d; K2  S1: S1a; S1b; S2b Q2; Q2a; Q2b No  The article designs a MapReduce based computing model for an option price prediction. Only price data was used as input. The combination of various data sources took no place. Thus, no Big Data publications.

[46] C1; C1b  K1; K1d; K2  S1; S1c  No  The paper proposes the use of single level data stores. Thereby, consistent and durable data structures, that on current hardware, allows programmers to safely exploit the low-latency and non-volatile aspects of new memory technologies is introduced.

No application is illustrated only view concepts are fulfilled.

As illustrated in Table 1, the first three publications  belong to Big Data. Hereby most of the concepts are fulfilled. In addition, not all papers [1] are Big Data. The fourth [45] remains to an application and typical strategies like MapReduce were used, but a combination of various data sources is neglected. Thus, the paper is more related to HPC. Furthermore, [46] is not focused on a use case. So the paper supports Big Data research, but relies not on the research area itself.



V. CONCLUSION The paper addressed the stated issue of a missing descriptive fundament of Big Data to prevent the establishment of a new buzzword. Hereby, expert interviews were conducted, transcribed and used as basis for a Grounded Theory design to obtain the inherent nature. Hereby, emerging concepts where categorized into cause, context, phenomenon, strategy and consequences until a theoretical saturation was achieved.

As a result, Big Data is interdisciplinary, context and causes driven and belongs to applied sciences. The usefulness of the resulting Big Data model was exemplary tested. In this context, state-of the-art papers [1] were classified into the model and proofed, whether they belong to Big Data or not.

In fact, not all of them belong to this topic.

The resulting model contributes to the specification and classification of Big Data contents. Thus, the area of discourse is illustrated and the positioning of goal oriented publications to solve business administration related issues is assisted. So, Big Data is delimitated from existing research fields such as HPC. Nevertheless, there are limitations. In fact, the descriptive model was an initial step to contribute to the existing discussion and offering a possible consensus about Big Data. Based on the proposed model, a quantitative analysis will follow to clarify if all categories and relationships are significant. In this context, it has to be proofed, whether Big Data is marketing driven or not.

