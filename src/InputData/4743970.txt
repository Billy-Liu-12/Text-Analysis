Classification Association Rules Mining for Zhongjing Prescriptions

Abstract  Zhongjing prescriptions have experienced approximately two thousand years of Clinical verification. The prescriptions are commended by the later generation as classical prescriptions because of strict compatibility and remarkable treatment result.

So Zhongjing prescriptions are selected as data set in this paper. A classification association rules mining algorithm based on information gain feature selection CARM-IG is achieved, which is used to mine compatibility principles of Zhang Zhongjing for the treatment of cold zheng, heat zheng, deficiency zheng and excess zheng etc. Experiments show that the run time and memory cost of algorithm is reduced, at the same time, the concision and precision of classification compatibility principles is guaranteed because of the deletion of irrelevant Chinese medicines with classification.

1. Introduction  Zhongjing prescriptions, which are created by Zhang Zhongjing, have experienced approximately two thousand years of Clinical verification. The prescriptions are commended by the later generation as classical prescriptions because of strict compatibility and remarkable treatment result. Over the years, Zhongjing prescription research is more confined to personal summary about compatibility principle, clinical research, experimental study, classification etc.

Zhongjing prescriptions have not been collected completely to establish a thematic database, and mathematical statistical and data mining methods have not been used to sum up compatibility principle objectively for it so far.

Then, 268 prescriptions from ?Shang Han Lun? and ?Jin Kui Yao Lue? which are written by Zhang Zhongjing are selected from the prescription database of Traditional Chinese Medicine (TCM) information system through excluding duplicate prescriptions and deleting prescriptions without medicine in order to establish data set of Zhongjing prescription in this research firstly. Then Zhongjing prescriptions are classified by experts in the TCM field from two angles of cold-heat and deficiency-excess in eight principles.

Finally, a classification association rules mining algorithm based on information gain feature selection CARM-IG is applied to mining latent compatibility principle of Zhang Zhongjing, so that the compatibility principles of Zhang Zhongjing for the treatment of cold zheng, heat zheng, deficiency zheng and excess zheng etc. can be summed up objectively.

2. Related works  Currently, association rules mining[1], cluster[2], correspondence analysis[3] etc. are data mining methods commonly used to discover prescription compatibility principle. Furthermore, several improved algorithms based on association rules mining have emerged [4]. A prescriptions data mining system integrated data preprocessing, high frequent item sets and association rules mining, cluster, classification etc.

is developed in project DartGrid[5, 6]. New research way is explored because of the application of methods above for prescriptions compatibility principle mining, which is becoming one of the most important research directions for the interdisciplinary research of TCM in union with computer technology gradually.

Algorithm CMAR (Classification based on Multiple Association Rules) based on FP-growth method is proposed in 2001[7]. The objective of algorithm CMAR  Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education      is to solve the problem of classification through mining classification association rules. But it is assumed that decision attributes (or category attribute) are closely related any conditional attributes (the attributes except decision attributes) in algorithm CMAR. Higher memory cost and running time will be spend when super high dimension data set such as prescriptions data sets of TCM with hundreds dimensions is processed. The assumption above causes that irrelative attributes exist in mined classification association rules.

So the index of IF*ICF (the inverse class frequency) is proposed in paper [8], which is used to delete the attributes with higher frequent but lower contribution for classification from VCFP-Tree in order to improved the performance of algorithm. The index of IF*ICF usually is useless When the classification number is lower so that a Chinese medicine has appeared in each type of prescription. The method of feature selection based on information gain is introduced in algorithm CMAR. A classification association rules mining algorithm CARM-IG for TCM prescription high dimension data set is established, which can reduce the time and space cost and mine classification association rules with higher quality through the deletion of irrelevant Chinese medicines with classification. Finally, the compatibility principles of Zhongjing prescriptions are summed up from different angles of classification.

3. Feature selection based on information gain  The basic idea behind feature selection is to compute some measure that is used to quantify the relevance between an attribute and a given class label.

Information gain is one of effective measures for feature selection. Such a measure is referred to as an attribute selection measure or a measure of the goodness of split. Information gain is defined as the difference of the expect information needed to classify a give sample 1( ,..., )mI s s  and the expect information needed to classify a give sample based on conditional attributes . Specific definition is given by ( )E C  1( ) ( ,..., ) ( )mIG C I s s E C                    (1) Let S be a set consisting of s data samples. Suppose  the class label attribute has m distinct values defining m distinct classes Di (for i=1,?, m ). Let si be the number of samples of S in class Di. The expect information needed to classify a give sample is given by  1 2  ( ,..., ) log ( ) m  m i i  I s s p p  Where pi is the probability that an arbitrary sample belongs to class Di, and is estimated by si/s.

Let attribute C have v distinct values 1,..., vc c .

Attribute C can be used to partition S into v subsets 1,..., vS S , where Sj contains those samples in S that have value cj of C. Let sij be the number of sample of class Di in a subset Sj. The entropy, or expected information based on the partitioning into subsets by C, is given by    ...

( ) ( , ..., )  v j m j  j m j i  s s E C I s s  s (3)  Where 1 ...j mjs s s  acts as the weight of the jth  subset, and is the number of samples in the subset divided by the total number of samples in S.

1( ,..., )j mjI s s  is defined as 2  log ( ) m  ij ij i  p p , and  | | i j  i j j  s p  S is the probability that a sample in Sj belongs  to class Di.

Anyway, the smaller the entropy value , the  greater the information gain value ( )E C  ( )IG C , the greater the purity of the subset partitions, and the higher the correlation between conditional attributes and decision attributes. The lowest threshold of information gain is usually been set, and the attributes whose information gain value is higher than the threshold will be chosen as the distinguish attributes for classification. The threshold of information gain is set to 0.1 in this research refer book [11].

4. Classification association rules mining based on information gain feature selection  4.1. Variant of classification frequent pattern tree  A variant of classification frequent pattern tree (VCFP-Tree) is a compress storage structure of data set in order to mine classification association rules without candidate frequent item sets generation. A VCFP-Tree must satisfy condition below [9]:  1. A VCFP-Tree consists of one root labeled as "null", a set of item prefix subtrees as the children of the root, and a frequent-item header table F-list.

2. Each node in the item prefix subtree consists of five fields: item, child-link, parent-link, node-link, and class-list, where item registers which item this node represents, child-link links to child nodes of each node, parent-link links to parent node of each node, node- link links to the next node in the VCFP-Tree carrying  i                  (2)  Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education     the same item, or null if there is none, and class-list is a classification list which records every class label and its corresponding frequency.

3. Each entry in the frequent-item header list consists of three fields, (1) item-name; (2) start of node-link, which points to the first node in the VCFP- Tree carrying the same item; (3) information gain value of the attribute.

Table 1. Prescription data set T.

No. a b c d e classification tag 1 1 0 1 0 1 A  2 1 1 1 0 0 B  3 0 0 0 1 1 A 4 1 1 0 1 0 C 5 1 1 1 1 0 C  Figure 1. VCFP-Tree based on data set T  As shown in table 1 below, Chinese medicines usually is designated as attributes of prescriptions data set such as a and b etc. Whether a Chinese medicine appears in a prescription is been indicated by 0 or 1. Classification attributes often list in the end, such as classification tag. A VCFP-Tree (i.e., figure 1) is constructed based on the data set T. The order of items in the frequent-item header table F- list is a: 4-b: 3-c: 3-d: 3 with 3 as the threshold of support count. The information stored in the node indicated by deep color arrow is that there is one prescription containing Chinese medicines abd and belonging to class C.

4.2. Classification association rule  Class association rule is an implication of the form R: C D, where C D= , sup(R) min_sup, and conf (R) min_conf. C indicates conditional attribute (or item). D indicates decision attribute (or classification attribute). Sup (R) is the support of rule R, min_sup is the lowest threshold of support; Conf (R) is the  confidence of rule R, min_conf is the lowest threshold of confidence. The support and confidence of rule R is defined below:  Sup (R) = P (C D)                            (4) Conf (R) = P (D|C)                            (5)  P (C D) indicates the joint probability of C and D.

P (D|C) indicates the conditional probability that belongs to class D under the condition of Chinese medicine C appearance. Rule support and confidence are two important measures of rule interestingness.

They respectively reflect the usefulness and certainty of discovered rules. The rules with higher support and confidence are usually chosen as interesting pattern.

For example: {Ramulus Cinnamomi* Rhizoma Zingiberis  Recens* Radix Paeoniae Alba} {Cold Zheng} (Support count=15)  (The frequency of rule former piece=25) Where {Ramulus Cinnamomi*Rhizoma Zingiberis  Recens*Radix Paeoniae Alba} before  is the rule former piece; {Cold Zheng} after  is the rule rear piece. The support count indicates the frequency of three Chinese medicines above appearing in a same prescription and treating Cold Zheng, and is also called the frequency of rule; the frequency of rule former piece indicates the frequency of three Chinese medicines above appearing in a same prescription.

There are 268 prescriptions in Zhongjing prescriptions.

So the rule support is 0.056, which is computed through support count 15 divided by 268. The rule confidence is 60%, which is computed through support count 15 divided by the frequency of rule former piece 25.

4.3. Efficient mining of classification association rules based on information gain feature selection  The concrete steps of algorithm CARM-IG which are used to mine classification association rule effectively are as following.

Algorithm: CARM-IG Input: min_sup: the lowest threshold of support min_conf: the lowest threshold of confidence min_IG: the lowest threshold of information gain.

Output: CAR the set of classification association rules.

Begin (1) Scans Zhongjing prescription database once. The information of relevant frequencies is recorded, so that the information gain value of every conditional attribute can been computed completely.

Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education     (2) Conditional attributes which are higher than min_sup and min_IG are selected. The frequent-item header table F-list is constructed in attributes support descending order.

(3) Scans Zhongjing prescription database second, and a VCFP-Tree T is constructed according to section4.1.

(4) CAR=CFP-growth (T, min_sup, min_conf); (5) Return (CAR); End  Algorithm CARM-IG scans Zhongjing prescription database firstly. The frequency  ix f for every Chinese  medicine xi, the frequency jyf  for each type of  prescription, and the frequency ( | )j if y x  for each type of prescription containing Chinese medicine xi are recorded.

jy f  is used to compute 1( ,..., )mI s s , and  ( | )j if y x  is used to computer , so that information gain value IG of every conditional attribute can be computed according to  Eq.(1). The conditional attributes (or Chinese medicines) with  ( )E C  ix f >min_sup and IG>min_IG are chosen. The header  table F-list is constructed in accordance with ix  f descending. The objective of the second scan of prescription data set is to create a compress storage structure for Zhongjing prescription, namely a VCFP- Tree T. Finally, the algorithm of CFP-growth (Class- Frequent Pattern growth) is achieved as following [7].

1. Based on F-list in Figure 1, the set of class- association rules can be divided into subsets without overlap: (1) the ones having d; (2) the ones having c, but no d; (3) the ones having b, but no d  nor c ; and (4) the ones having only a. So algorithm CARM-IG finds these subsets of class-association rules one by one.

2. To find the subset of rules having d, algorithm CARM-IG traverses nodes having attribute value d and look ?upward? to collect a d-projected database, which contains {(a,b,c,d):C; (a,b,d):C; d:A} three tuples. It contains all the tuples having d. The problem of finding all frequent patterns having d in the whole training set can be reduced to mine frequent patterns in d-projected database.

3. Recursively, in d-projected database, a and b are the frequent attribute values, i.e., they pass support threshold. (In d-projected database, d happens in every tuple and thus is trivially frequent. We do not count d as a local frequent attribute value.) We can mine the projected database recursively by constructing VCFP- trees and projected databases. Please see [10] for more details.

4. It happens that, in d-projected database, a and b always happen together and thus ab is a frequent pattern. a and b are two subpatterns of ab and have  same support count as ab. To avoid triviality, we only adopt frequent pattern abd. Based on the class label distribution information, we generate rule abd C with support count 2 and confidence 100%.

5. After search for rules having d, all nodes of d are merged into their parent nodes, respectively. That is, the class label information registered in a d node is registered in its parent node. The VCFP-tree is shrunk as shown in figure 2. Please note that this tree- shrinking operation is done at the same scan of collecting the d-projected.

The remaining subsets of rules can be mined similarly.

Anyway, the information used to compute information gain value for every conditional attribute is collected completely through algorithm CARM-IG without increase in computing time basically. The scale of VCFP-Tree created in memory has been reduced moderately after dimension reduction based on information gain filter. Then the mining of classification association rule becomes more quickly and efficient, and experiments show that the run time and memory cost of algorithm CARM-IG has been reduced respectively.

Figure 2. VCFP-Tree after merging nodes d  5. Experiment result  Experimental platform configured to Intel 2G / 1G, Windows XP. Algorithm CARM-IG using java programming is achieved. The 268 Zhongjing prescriptions are chosen as data set, which are classified by experts in the TCM field from two angles of cold-heat and deficiency-excess in eight principles.

The lowest support, confidence and information gain threshold set 2%, 50% and 0.1 respectively.

Experiment results are summarized below.

5.1. The comparison of run time and memory cost  Run time and memory cost are compared between algorithms of classification association rules mining  Proceedings of 2008 IEEE International Symposium on IT in Medicine and Education     based (algorithm II) or not based (algorithm I) on information gain feature selection. Experiment results listed in table 2 show that algorithm II has lower run time and memory cost than algorithm I, and the reduction of memory cost is particularly conspicuous.

Table 2. The comparison of time and memory cost methods Classifications Attributes  Time  (s) Memory  (M) cold-heat  49 0.187 10.440 I  deficiency- excess  49 0.172 10.908  cold-heat 37 0.172 3.124 II deficiency-  excess 38 0.156 2.940  5.2. The classification compatibility principle of Zhongjing prescriptions  The classification association rules of Zhongjing prescription are mined by algorithm CARM-IG from two angles of cold-heat (21 rules) and deficiency- excess (29 rules) in eight principles. For example, {Ramulus Cinnamomi * Herba Ephedrae}-> {Cold Zheng} (Sup=3.36%, Conf=64.28%) , {Fructus Jujubae * Radix Paeoniae Alba}-> {Deficiency Zheng}(Sup=7.09%, Conf=65.51%) and so on.

To sum up, the run time and memory cost of algorithm is reduced, at the same time, the concision and precision of classification compatibility principles is guaranteed because of the deletion of irrelevant Chinese medicines with classification. The mined rules more follow with the prescription law directed by basic theories of traditional Chinese medicine by experts? confirmation preliminary. Follow-up study focuses on the careful and thoroughgoing explanation for classification compatibility principle of Zhongjing prescriptions through literature research and expert advice.

