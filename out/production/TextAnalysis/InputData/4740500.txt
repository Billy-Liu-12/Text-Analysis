An Ontology-based Framework for Knowledge Retrieval

Abstract  Retrieving accurate information from the Web is a great challenge to users. The existing information retrieval sys- tems are mostly term-based and thus need to be enhanced toward knowledge-based. User information needs need to be better captured in order to deliver personalized search results. In this paper, an ontology-based framework is pro- posed for capturing user information needs using a world knowledge base and the user?s local instance repository.

The framework aims to discover a user?s background knowl- edge for knowledge retrieval. The evaluation result is en- couraging, in which the proposed model achieved the same performance as a manual user model.

1. Introduction  In the past decades, the Web information has exploded rapidly. How to gather useful information from the Web nowadays becomes a challenging issue. Attempting to solve this problem, many information retrieval (IR) systems have been proposed and made great achievements. However, there is still not a solution to the challenge eventually [2].

The IR systems are mostly based on keyword-match tech- niques and suffer from the problems of information mis- matching and overloading. The information needs may be expressed in different queries because of different user perspectives, background knowledge, terminological habits and vocabulary. Thus, if a user?s background knowledge is discovered, more accurate information can be retrieved.

However, discovering user background knowledge through a given query is difficult. When users read through the content of a document, they can easily find out if it is interesting or not. This is because users implicitly have a knowledge system built based on their background knowl- edge [8]. By re-building this knowledge system, a user?s background knowledge can expect to be discovered, and thus better IR performance can be achieved. This route is suggested by [16] as future knowledge retrieval systems.

In this paper, we introduce a knowledge retrieval frame- work for ontology-based information gathering, and pro- pose an approach to rebuild users? knowledge systems. A user?s mental model and querying model are formalized, aiming to describe the process of how an information need is transformed into a query. In response to the query, the user background knowledge is discovered from the world knowledge base and user?s local instance repository. Based on these, a personalized ontology is constructed to sim- ulate the user?s mental model and capture the informa- tion need. The semantic relations of hypernym/hyponym, holonym/meronym and synonym are specified in the on- tology. The proposed approach is evaluated by comparing to a manual model that discovers knowledge by linguists, and the evaluation result is promising. The ontology-based knowledge retrieval framework is a novel contribution to knowledge engineering and Web information retrieval.

The paper is organized as follows. Section 2 presents re- lated work. In Section 3, we introduce the ontology-based knowledge retrieval framework. The evaluation of our pro- posal is described in Section 4, and the results are discussed in Section 5. Finally, Section 6 makes conclusions.

2. Related Work  Ontologies have been used by many groups for person- alized information retrieval. Tran et al. [15] introduced an approach to translate keyword queries to DL conjunctive queries and used ontologies to describe a user?s background knowledge. Gauch et al. [4] learned ontologies for users in order to specify their personalized preferences and interests in Web search. King et al. [6] developed an ontology based on the Dewey Decimal Classification for distributed IR sys- tems. However, these works have problems that either the volume of knowledge covered in ontologies are limited or the knowledge specified is not clear adequately.

Learning an ontology to specify knowledge is challeng- ing. Maedche [10] formally defined an ontology as a 5- tuple of concepts, hierarchical relations, plain relations, in- stances and axioms. He also proposed an ontology learn-   DOI 10.1109/WIIAT.2008.226    DOI 10.1109/WIIAT.2008.226     ing framework using semi-automatic ontology construction tools with human intervention. However, the human inter- vention increases the cost in this framework. Aiming to re- duce the cost, Liu and Singh [9] developed ConceptNet on- tology by smartly using free contributions from Web users.

However, as a trade-off, the knowledge specified in Con- ceptNet is not expert evaluated. These works need to be improved for knowledge acquisition.

Many other works attempted to learn ontologies auto- matically. Li and Zhong [8] mined patterns from Web contents and used association rules for ontology learning.

Web content mining techniques were also used by Jiang and Tan [5] to discover semantic knowledge from domain- specific documents for ontology learning. Dou et al. [3] pro- posed a framework for developing domain ontologies using pattern decomposition, clustering/classification, and asso- ciation rules mining techniques. However, as pointed out by [8], these works cover only a limited number of concepts and specify only simple ?super-class? and ?sub-class? rela- tions. They suffer from the problem of inadequate knowl- edge specification.

In summary, there still remains a research gap in learning an ontology to describe user background knowledge in IR.

This motivates our research work presented in this paper.

3. Ontology-based Retrieval Framework  The ontology-based knowledge retrieval framework con- sists of four models: a user?s mental model and querying model, a computer model, and an ontology model. A user?s mental model is her (his) background knowledge system.

A querying model is a user?s translation of an information need generated from her (his) mental model. The computer model constructs an ontology for the user. The constructed ontology is the ontology model aiming to simulate the user?s mental model in IR.

3.1. Mental Model  A search task starts from a user?s information need.

From observations, when a user was in need of some in- formation and commencing a search task, we found that the user usually fell into one of the following situations:  ? she (he) knew nothing about that information;  ? she (he) had tried but failed to infer that information from his (her) already possessed knowledge;  ? she (he) might know something about that information but was not sure, so she (he) needed to confirm it.

Based on the findings, apparently a user holds a repository in her (his) brain that stores the knowledge she (he) pos- sesses, that is why a user can check if knowing something  or not. The second finding suggests that the knowledge pos- sessed by a user may be linked each other, so that a user can perform an inference task from what is known to what is un- known. The last finding indicates that a user actually holds a confidence rate to the knowledge she (he) possesses, so that the user knows that she (he) is certain or uncertain about it. However, the confidence rate may be implicit because a user may not be able to express it clearly. Apparently, a user has an implicit knowledge system in brain for information search.

Thus, although the mechanism of a user?s brain-working in Web search has not yet been clearly understood, we can at least have the following assumption:  Assumption 1 A user has a knowledge repository, in which  ? the stored knowledge are embedded in an association structure;  ? the stored knowledge are associated with implicit con- fidence rates.

Based on the assumption, by calling a user?s implicit knowl- edge system as a mental model, we formalize it as follows:  Definition 1 A user?s mental model is a 3-tuple U :? ?K, B?,G?, where  ? K is a non-empty set of pairs {?k,wk?}, where k is a primitive knowledge unit possessed by the user and wk is the user?s confidence rate on k;  ? B? is the backbone of the mental model that frames the association structure of knowledge units;  ? G is a set of gaps {g1, g2, . . . , gi} existing on B?, in which each gap g is a knowledge unit that the user does not possess.

Note that we use :? instead of := because this definition is given under Assumption 1, which is based on observations and cannot be proved in laboratories currently.

3.2. Querying Model  Filling a knowledge gap in the mental model triggers a user?s search task and becomes the user?s information need.

This implicit g in U is expressed in an explicit short phrase by the user through her (his) own language, e.g. English. In IR, we call this phrase as a query, which is a set of terms.

We formally describe a user?s query as a querying model in our knowledge retrieval framework:  Definition 2 A user querying model Q is a set of terms {t|t ? Lu}, in which elements are primitive units in the user?s language Lu.

Generating a query means the process of translating an implicit knowledge gap, g ? G in a user?s mental model U , to a set of explicit terms in Q. In contrast, capturing an information need means the inverse process of translation from a Q back to a g ? G in U . However, users may use different terms to generate queries, even for the same infor- mation need, because of user perspectives, terminological habits and vocabulary. Thus, capturing a user?s informa- tion need through a given query only is extremely difficult.

However, if the user background knowledge can be discov- ered, capturing the accurate information need is possible. In the following sections we discuss how this can be done.

3.3. Computer Model  The computer model aims to simulate a user?s mental model. The computer model discovers a user?s background knowledge associated with an information need. For the sake of explanation, we first formalize the computer model:  Definition 3 A computer model C is a 3-tuple C := ?WKB,LIR,F?, where  ? WKB is a world knowledge base that frames a user?s background knowledge;  ? LIR is a user?s local instance repository, in which the elements cite the knowledge in WKB;  ? F is a set of functions, inferences and algorithms that build an ontology for a user using WKB and LIR.

To simulate a user?s mental model U , an ontology is constructed based on the WKB and personalized using the LIR, co-responding to a querying model Q for a g ? G.

3.3.1 Constructing Ontologies using WKB  The world knowledge base is a knowledge frame describ- ing and specifying the background knowledge possessed by humans. In this paper, we assume the existence of a world knowledge base and use a subject as a primitive knowledge unit in the WKB. Subjects in the WKB are linked by semantic relations. Two semantic relations are postulated existing in the WKB: hypernym/hyponym and holonym/meronym. Hypernym/hyponym (usually called is- a) relations describe the situation that the semantic range referred by a hyponym is within that of its hypernym, e.g. ?car? is a hyponym of ?automobile?, and ?auto- mobile? is a hypernym of ?car?, they are on different levels of abstraction (or concretion). In this paper, we treat hypernym/hyponym as one single semantic relation, as they are just the two sides of one coin. Differently, holonym/meronym (usually called part-of ) relations define the relationship between a (holonym) subject denoting the  whole and a (meronym) subject denoting a part of, or a member of, the whole, e.g. a ?tyre? is a meronym of its holonym ?car?. Again, we treat holonym/meronym as one relation. The world knowledge base is formalized as:  Definition 4 Let WKB be a world knowledge base, which is a directed acyclic graph consisting of a set of subjects linked by their semantic relations, WKB is formally defined as a 2-tuple WKB := ?S,R?, where  ? S is a set of subjects S = {s1, s2, ? ? ? , sm}, in which each element is a 2-tuple s := ?label, ??, where label is the name of s and label(s) = {t1, t2, . . . , tj |t ? Lu}; and ? is a signature mapping defining a set of subjects that directly link to s, and ?(s) ? S;  ? R is a set of relations R = {r1, r2, ? ? ? , rn}, in which each element is a 2-tuple r := ?r? , r? ?, where r? ? S ? S and r? is a relation type of hypernym/hyponym or holonym/meronym. For each (sx, sy) ? r? , sx is the subject who holds the r? of relation to sy , e.g. sx is a hypernym of sy .

The relevance of a subject to a query in WKB is deter- mined using the syntax-matching mechanism, because they are both represented by a set of terms in the user?s language Lu (see Definition 2 and 4). We use sim(s,Q) to specify the relevance of a subject s ? S to Q:  sim(s,Q) = |label(s) ?Q|. (1)  A subject with sim(s,Q) > 0 is a positive subject relevant to Q, otherwise a non-relevant negative subject.

Let S be a subject set dealing withQ,R be a relation set specifying the semantic relations existing in S. The positive subjects in S are first extracted for S. For each s ? S , the subjects in its ?(s) are also extracted for S, along with their associated semantic relations r ? R to s extracted for R.

The extraction is iteratively conducted for three times, as we believe that any subjects out of that range from a positive subject are no longer significant and can be ignored. We can then decompose S into to two sets S+ and S?, based on the extracted subjects? sim(s,Q) values:  S+ = {s|sim(s,Q) > 0, s ? S}; S? = {s|sim(s,Q) = 0, s ? S}; (2)  and have their semantic relations specified as:  R = {r|r ? R, r? ? S ? S}. (3)  The subjects in S specify the implicit knowledge k in K in a user?s mental model U , associated with an information need g ? G, which is expressed by the user as a query Q.

S+ specifies the knowledge relevant to g, and S? specifies the subjects paradoxical or non-relevant to g. The relations inR link the subjects in S, and thus provide the backbone of B? for U . By these, we have the user?s ontology constructed.

3.3.2 Personalizing Ontologies using LIRs  A user?s constructed ontology is personalized using the user?s LIR. An LIR is a collection of information items that are recently visited by the user. These items have tags assigned with subjects that cite the knowledge specified in the WKB. A user?s personal background knowledge related to an information need can be discovered from these cita- tions. The discovered background knowledge personalizes the constructed ontology.

The subjects assigned to items in an LIR are the ties con- necting the LIR to the WKB. We call an element in LIRs as an instance and denote it by i. Let I = {i1, i2, ? ? ? , ip} be an LIR, and S ? S be a set of subjects assigned to the instances in I . The relationships between S and I can be described as the following mappings:  ? : I ? 2S , ?(i) = {s ? S|s is cited by i}; ??1 : S ? 2I , ??1(s) = {i ? I|s ? ?(i)}; (4)  where ??1(s) is a reverse mapping of ?(i). These map- pings aim to explore the semantic matrix existing between the subjects and instances.

The beliefs of an instance to its referring subjects are varying. An instance cites multiple subjects, and these sub- jects are indexed by their importance to the instance. Thus, the belief of an instance to a subject can be defined by  bel(i, s) = priority(s, i)  n(i) ; (5)  where n(i) is the number of subjects on the citing list of instance i, priority(s, i) = 1index(s,i) where index(s, i) is the index (starting from one) of s on the citing list of i. The bel(i, s) increases when less subjects occur in the citing list and the higher index of s on the list.

A user?s personal background knowledge is discovered from the semantic matrix existing between subjects and in- stances. Based on the mappings in Eq. (4), we first define the coverset for a subject aiming to discover its synonym subjects. A subject?s coverset, referring to the extent of instances in an LIR citing the subject, is defined by:  coverset(s) = {i|i ? ??1(s)}. (6)  Assume subject s1 ? S+ and s2 ? S?, if coverset(s1) ? coverset(s2) 6= ?, they have something in common, be- cause s1 and s2 both refer to some common instances.

Thus, we may say that s2 is relevant to s1, and may further deduce that s2 may also be interesting to the user because s1 is a positive subject. Based on these, let S?(s) = {s?|s? ? S+, coverset(s?) ? coverset(s) 6= ?} be the synonyms of s ? S? in S+, we discover new interesting subjects from S? and personalize the S+ and S? by:  S+ = S+ ? {s|s ? S?, S?(s) 6= ?}; S? = S? ? {s|s ? S?, S?(s) 6= ?}. (7)  We call the positive subjects extracted by using syntax- matching mechanism as the initial positive subjects, and the subjects {s|s ? S?, S?(s) 6= ?} as newly discovered inter- esting subjects, for the sake of explanation in this paper.

3.3.3 Specifying Confidence Rates  Recall back to Assumption 1 and Definition 1, a knowledge unit k possessed by a user in the mental model U is associ- ated with a confidence rate wk. In this section, we call this confidence rate as the support value sup(s,Q) of a subject s to Q, and present how wk is re-produced for subjects.

The sim(s,Q) judges the positive or negative of a sub- ject to Q, and has impact to the sup(s,Q). For an initial positive subject, we have its sim value measured by Eq. (1).

We also need to define the sim values for those new inter- esting subjects discovered in Section 3.3.2. Because these new interesting subjects are discovered based on their syn- onyms in initial S+, these synonyms also have authority to determine a new interesting subject?s sim value. Thus, we measure the sim of a new interesting subject as:  sim(s,Q) = ? s??S?(s) conf(s  ? ? s)? sim(s?)  |S?(s)| . (8)  where s? is an initial positive subject and sim(s?) is deter- mined by Eq. (1). conf(s? ? s) is the confidence of s received from its synonyms and calculated by:  conf(s? ? s) = |coverset(s ?) ? coverset(s)|  |coverset(s?)| . (9)  As a result, a new interesting subject overlapping more ini- tial positive subjects would have higher sim value. Subjects with higher sim has greater sup(s,Q) values.

A subject?s locality in the backbone of ontology affects its sup(s,Q). Subjects located toward lower bound levels of backbone are more specific and so more focused on the knowledge they refer to. Thus, they should have higher sup(s,Q) than the subjects located toward upper bound (more abstractive) levels. For the study of a subject?s local- ity, we use an ontology mining method Specificity, which is introduced by [14, 13], and describes a subject?s seman- tic focus on its referring knowledge. Algorithm 1 presents how the specificity values are assigned to subjects in an on- tology. hyponym(s) and meronym(s) are functions that return the set of direct hyponyms or meronyms of s satisfy- ing hyponym(s) ? ?(s) and meronym(s) ? ?(s). ? is a coefficient defining the reducing rate of specificity for focus lost in each step up from lower bound toward upper bound levels (? = 0.9 in our experiments). A subject?s specificity depends on its child subjects, as described in Algorithm 1.

As WKB is a directed acyclic graph (see Definition 4), Al- gorithm 1 has complexity of only O(n), where n = |S|.

input : the subject and relation set (S,R); a coefficient ? between (0,1).

output: specificity spe(s) assigned to all s ? S.

let k = 1, get the set of leaves S0 from S;1 for (s0 ? S0) assign spe(s0) = k;2 get S? which is the set of leaves in case that we remove the3 nodes in S0 and the related relations from (S,R); if (S? == ?) then return;//the terminal condition;4 foreach s? ? S? do5  if (hyponym(s?) == ?) then spe1 = k;6 else spe1 = ? ?min{spe(s)|s ? hyponym(s?)};7 if (meronym(s?) == ?) then spe2 = k;8  else spe2 = ?  s?meronym(s?) spe(s)  |meronym(s?)| ;9 spe(s?) = min(spe1, spe2);10  end11 k = k ? ?, S0 = S0 ? S?, go to step 3.12  Algorithm 1: Analyzing Semantic Relations for Specificity  The instances in an LIR contain a user?s background knowledge, and should also count in the sup(s,Q) values of the LIR?s citing subjects. Considering the bel(i,Q) cal- culated by Eq. (5), the support value sup(i,Q) held by an instance i to Q is calculated by:  sup(i,Q) = ? s??(i)  bel(i, s)? sim(s,Q). (10)  Because the negative subjects have sim(s,Q) = 0, only the positive subjects cited by i count in sup(i,Q).

Finally, sup(s,Q) is calculated, based on the sim(s,Q) from Eq. (1) or (8), the spe(s) from Algorithm 1, and the related sup(i,Q) from Eq. (10):  sup(s,Q) = spe(s)? sim(s,Q)? ?  i???1(s)  sup(i,Q).

(11) For a subject in the final S?, the sup(s,Q) = 0 because its sim(s,Q) = 0. The support value sup(s,Q) specifies the wk of ?k,wk? ? K in U considering an information need, where s is for a k and Q is for a g ? G.

During the personalized ontology learning, the WKB in the computer model C constructs an ontology for a user, and the user?s LIR is used to personalize the ontology. Equa- tions (1) to (11) and Algorithm 1 are used for knowledge extraction. They are the F in C, as described in Definition 5.

3.4. Ontology Model  The ontology model is the product from the computer model, aiming to simulate a user?s implicit mental model dealing with an information need. An ontology model can be formalized as follows:  Definition 5 An ontology model associated with a Q is a 4-tuple O(Q) := ?S,R, taxS , rel?, where  ? S is a set of subjects (S ? S) consisting of a posi- tive subset S+ relevant and a negative subset S? non- relevant to Q;  ? R is a set of relations andR ? R;  ? taxS : taxS ? S ? S is a function defining the taxo- nomic structure of ontology containing two directed re- lations of hypernym/hyponym and holonym/meronym;  ? rel is a function defining non-taxonomic relation of synonyms, e.g. overlapping.

The ontology model simulates a user?s mental model U .

The knowledge K in U is specified by S, in which the S+ is relevant and S? is non-relevant to a Q representing an information need g ? G. The wk for k in K is re-produced by sup(s,Q) for the subjects in S. The B? in U is specified byR, taxS and rel inO(Q). The mental model U is rebuilt.

4. Evaluation  In the IR fields, a common batch-style experiment is to select a collection of documents (testing set), a set of topics associated with relevance judgements (training set) and then compare the performance of experimental models [12]. Our experiments follow this style, and use the standard testbed and topics as that used in the TREC-11 Filtering track1, which aims to evaluate IR methods using relevant and non- relevant training sets.

Our proposed model (called ?ONTO model? in the ex- periments) is compared with an implemented mental model (called ?TREC model? in experiments) in the experiments.

The experiment design is illustrated in Fig. 1. Against an incoming topic, the TREC model generates a training set manually, whereas the ONTO model builds a user?s person- alized ontology automatically and generates a training set from the user?s LIR. A training set consists of a set of pos- itive samples D+ and a set of negative samples D?. Each sample is a document d holding a support value support(d) to the given topic. The different training sets are used by the common information gathering system to retrieve infor- mation from the testing set. The performance of the infor- mation gathering system is then affected by the training sets input. Based upon this, we can compare the performances and evaluate our proposed model.

The Reuters Corpus Volume 1 (RCV1) [7] used in the TREC-11 is also used as the testbed in our experiments.

The RCV1 is a large XML document set (806,791 docu- ments) with great topic coverage. A set of 50 topics are also provided by the TREC-11. These topics are designed by linguists manually, and associated with relevance docu- ments judged by the same linguists [11]. All 50 topics are  1Text REtrieval Conference, http://trec.nist.gov/.

Figure 1. Experiment Design  used in our experiments, in order to maintain the high sta- bility, as suggested by [1]. The topics have title, description and narrative. However, only the titles are used as queries, because in real world users only use short phrases to express their information needs.

4.1. Information Gathering System  An information gathering system (IGS) is implemented for common use by all the experimental models. The IGS is an implementation of a model developed by [8], which uses user profiles for information gathering. The [8]?s model is chosen because not only it is verified better than the Roc- chio and Dempster-Shafer models, but also it is extensible in using support values of training documents. The input support values associated with documents would affect the IGS?s performance sensitively. The technical details and the related justifications can be referred to [8].

The IGS first uses the training set to evaluate weights for a set of selected terms T . After text pre-processing of stopword removal and word stemming, a positive document d becomes a pattern that consists of a set of term frequency pairs d? = {(t1, f1), (t2, f2), . . . , (tk, fk)}, where fi is ti?s term frequency in d. The semantic space referred by d? is represented by its normal form ?(d), which satisfies ?(d) = {(t1, w1), (t2, w2), . . . , (tk, wk)}, where wi (i = 1, . . . , k) are the weight distribution of terms and wi = fi?k  j=1 fj .

A probability function on T can be derived based on the normal forms of positive documents and their supports for all t ? T :  pr?(t) = ?  d?D+,(t,w)??(d)  support(d)? w. (12)  The testing documents can be indexed by weight(d), which is calculated using the probability function pr? :  weight(d) = ? t?T  pr?(t)? ?(t, d); (13)  where ?(t, d) = 1 if t ? d; otherwise ?(t, d) = 0.

4.2. TREC Model  The TREC model is the implementation of a user?s men- tal model. For a given topic, the TREC linguists read a set of documents, and marked each document ?positive? or ?neg- ative? against the topic. If a document d is marked ?posi- tive?, it becomes a positive document in the TREC training set and support(d) = 1|D+| ; otherwise, it becomes a nega- tive document and support(d) = 0. Since the linguists who marked the documents are also the people who generated the topics, following the assumption that only users know their interests and preferences perfectly, the TREC model makes a golden model to our proposed model to mark. The modelling of a user?s mental model can be proven success- ful if the ONTO model can achieve the same or close per- formance to this golden model.

4.3. ONTO Model  This model is the implementation of our proposed model. As illustrated in Fig. 1 and required by the IGS, the input to this model is a topic and the output is a train- ing set consisting of positive documents (D+) and nega- tive documents (D?). Each document is associated with a support(d) value indicating its support level to the topic.

The WKB described in Section 3.3.1 is constructed based on the Library of Congress Subject Headings2  (LCSH) system. The LCSH system is a categorization de- veloped for organizing the large volumes of library collec- tions and for retrieving information from the library. The subject headings in the LCSH are transformed into the sub- jects in WKB, and the LCSH structure is transformed into the backbone of WKB. Eventually, the constructed WKB contains over 400,000 subjects covering various topics.

The semantic relations in the WKB are transformed from the references, Broader term, Narrower term and Used-for, specified in the LCSH. The Broader term and Narrower term references are transformed into hy- ponym/hypernym relations. Used-for references are usually used in two situations: to describe an action or to describe an object. When object A is used for an action, A actually becomes a part of that action, like ?using turner in cook- ing?; when A is used for object B, A becomes a part of B, like?using wheels for a car?. Hence, we transform the  2http://classificationweb.net/.

Used-for references in the LCSH into holonym/meronym relations in our WKB.

In the experiments, we assume that each topic comes from an individual user. We attempt to evaluate our model in an environment that covers great range of topics. How- ever, it is not realistic to expect a participant to hold such great range of topics in personal interests. Thus, for the 50 experimental topics, we assume each one coming from an individual user and learn her (his) personalized ontology.

An LIR is collected through searching the subject cata- logue of Queensland University of Technology (QUT) Li- brary3 by using the title of a topic. Librarians have assigned title, table of content, summary, and a list of subjects to each information item (e.g. a book) stored in QUT library. The assigned subjects are treated as the tags in Web documents that cite the knowledge in the WKB. In order to simplify the experiments, we only use the librarian summarized in- formation (title, table of content and summary) to represent an instance in an LIR. All these information can be down- loaded from QUT?s Web site and are available to the public.

Once the WKB and an LIR are ready, an ontology is learned as described in Section 3.3.1, and personalized as in Section 3.3.2. The user confidence rates on the subjects are specified as in Section 3.3.3. A document di in the training set is then generated by an instance i, and its support value is determined by:  support(di) = ?  s??(i),s?S  sup(s,Q); (14)  where s ? S in O(Q) are as defined in Definition 5. As sup(s,Q) = 0 for s ? S? (according to Eq. (11)), the documents with support(d) = 0 go to D?, whereas those with support(d) > 0 go to D+.

4.4. Performance Measures  The performance of the experimental models are mea- sured by three methods: the precision averages at eleven standard recall levels (11SPR), the mean average precision (MAP), and the F1 Measure. They are all based on preci- sion and recall, the modern IR evaluation methods [1].

The 11SPR is reported suitable for information gather- ing, and is used in TREC evaluations as a performance mea- suring standard [11]. An 11SPR value is computed by sum- ming the interpolated precisions at the specified recall cutoff and then dividing by the number of topics:?N  i=1 precision? N  ; ? = {0.0, 0.1, 0.2, . . . , 1.0}. (15)  N is the number of topics and ? are the cutoff points where the precisions are interpolated. At each ? point, an aver-  3http://library.qut.edu.au/.

Figure 2. Experimental 11SPR Results  age precision value over N topics is calculated. These av- erage precisions then link to a curve describing the recall- precision performance.

The MAP is a stable and discriminating choice in infor- mation gathering evaluations, and is recommended for mea- suring general-purpose information gathering methods [1].

The average precision for each topic is the mean of the pre- cision obtained after each relevant document is retrieved.

The MAP for the 50 experimental topics is then the mean of the average precision scores of each of the individual top- ics in the experiments. The MAP reflects the performance in a non-interpolated recall-precision fashion.

F1 Measure is also well accepted by the information  gathering community, which is calculated by:  F1 = 2? precision? recall precision+ recall  . (16)  Precision and recall are evenly weighted in F1 Measure.

For each topic, the macro-F1 Measure averages the preci- sion and recall and then calculates F1 Measure, whereas the micro-F1 Measure calculates the F1 Measure for each re- turned result and then averages the F1 Measure values. The greater F1 values indicate the better performance.

5. Results and Discussions  The experiments attempt to evaluate our proposed model by comparing to an implementation of mental model. We expect that the ONTO model can achieve at least the close performance to the TREC model.

The experimental 11SPR results are illustrated in Fig. 2.

At recall point 0.3, the TREC model slightly outperformed the ONTO model, but at 0.5 and 0.6, the ONTO model achieved better results than the TREC model subtly. At all other points, their 11SPR results are just the same. For the MAP results shown on Table 1, the ONTO model achieved 0.284, which is just 0.006 below the TREC model (2%     TREC ONTO p-value Macro-FM 0.388 0.386 0.862 Micro-FM 0.356 0.355 0.896  MAP 0.290 0.284 0.484  Table 1. Other Experimental results  downgrade). For the average macro- and micro-F1 Mea- sures also shown on Table 1, the TREC model only outper- formed the ONTO model by 0.002 (0.5%) in macro-F1 and 0.001 (0.2%) in micro-F1. The two models achieved almost the same performance. The evaluation result is promising.

The statistical test is also performed on the experimen- tal results, in order to analyze the evaluation?s reliability.

As suggested by [12], we use the Student?s Paired T-Test for the significance test. The null hypothesis in our T-Test is that no difference exists in two comparing models. When two tests produce substantially low p-value (usually<0.05), the null hypothesis can be rejected. In contrast, when two tests produce high p-value (usually >0.1), there is not or just little practical difference between two models [12].

The T-Test results are also presented on Table 1. The p- values show that there is no evidence of significant differ- ence between two experimental models, as the produced p- values are quite high (p-value=0.484(MAP), 0.862(macro- FM) and 0.896(micro-FM), far greater than 0.1). Thus, we can conclude that in terms of statistics, our proposed model has the same performance as the golden TREC model, and the evaluation result is reliable.

The advantage of the TREC model is that the experimen- tal topics and the training sets are generated by the same linguists manually. They as users perfectly know their in- formation needs and what they are looking for in the train- ing sets. Therefore, it is reasonable that the TREC model performed better than the ONTO model, as we cannot ex- pect that a computational model could outperform a such perfect manual model. However, the knowledge contained in TREC model?s training sets is well formed for human beings to understand, but not for computers. The contained knowledge is not mathematically formalized and specified.

The ONTO model, on the other hand, formally specifies the user background knowledge and the related semantic rela- tions using the world knowledge base and local instance repositories. The mathematic formalizations are ideal for computers to understand. This leverages the performance of the ONTO model. As a result, as shown on Fig. 2 and Table 1, the ONTO model achieved almost the same perfor- mance as that of the TREC model.

6. Conclusions  In this paper, an ontology-based knowledge IR frame- work is proposed aiming to discover a user?s background  knowledge to improve IR performance. The framework consists of a user?s mental model, a querying model, a computer model and an ontology model. A world knowl- edge base is used by the computer model to construct an ontology to simulate a user?s mental model, and the on- tology is personalized by using the user?s local instance repository. The semantic relations of hypernym/hyponym, holonym/meronym and synonym are specified in the ontol- ogy model. The framework is successfully evaluated by comparing to a manual user model. The ontology-based framework is a novel contribution to knowledge engineer- ing and Web information retrieval.

