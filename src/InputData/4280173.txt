The Research of Improved Apriori Algorithm for Mining Association Rules  Sheng Chai1, 3, Jia Yang2, Yang Cheng1

ABSTRACT  The efficiency of mining association rules is an important field of Knowledge Discovery in Databases. The Apriori algorithm is a classical algorithm in mining association rules. This paper presents an improved Apriori algorithm to increase the efficiency of generating association rules. This algorithm adopts a new method to reduce the redundant generation of sub-itemsets during pruning the candidate itemsets, which can form directly the set of frequent itemsets and eliminate candidates having a subset that is not frequent in the meantime. This algorithm can raise the probability of obtaining information in scanning database and reduce the potential scale of itemsets.

Keywords: data mining, association rule, Apriori algorithm, frequent itemset    1. INTRODUCTION   Mining association rule is one of main contents of data mining research at present, and emphasizes particularly is finding the relation of different items in the database.

How to generate frequent itemsets is the key and core. It is an important aspect in improving mining algorithm that how to decrease itemsets candidate in order to generate frequent itemsets efficiently.

In classical Apriori algorithm, when candidate generations are generated, the algorithm needs to test their occurrence frequencies. The manipulation with redundancy result in high frequency in querying, so tremendous amounts of resources will be expended whether in time or in space.

In this paper, an improved algorithm is proposed for miming the association rules in generating frequent k-itemsets. Instead of judging whether these candidates are frequent itemsets after generating new candidates, this new algorithm finds frequent itemsets directly and removes the subset that is not frequent, which based on the classical Apriori.

2. ASSOCIATION RULES DESCRIPTION  The association rule mining were first proposed by Agrawal and etc. It can be formal defined as follow: Given a transaction database DB, I={il, i2, i3, ? , in} is a set of items with n different itemsets in DB, each transaction T in DB is a set of item (i.e.itemsets), so T?I.

Definition 1: Let I={il, i2, i3, ? , in} be a set of items, then D={<Tid, T>|T?I} is a transaction database, where Tid is an identifier which be associated with each transaction.

Definition 2: Let X?I, Y?I, and X?Y= ?, we called X?Y as association rule.

Definition 3: Suppose c is the percentage of transaction in D containing A that also contain B, then the rule X?Y bas confidence c in D. If s is percentage of transactions in D that contain A?B, then the rule X?Y has support s in D.

Definition 4: Hypothesis X?I, minsup is the smallest support. If the frequency of X is greater than or equal to minsup, then X is called as frequent itemset, otherwise X is called non-frequent itemset.

Classical Apriori algorithm:  (1) Cl = {candidate I-itemsets}; (2) L1 = {c? C1|c.count?minsup}; (3) FOR (k=2;Lk-1??;k++) DO BEGIN (4)  Ck=apriori-gen(L k-1); (5)  FOR all transactions t?D DO BEGIN (6)   Ct=subset (Ck,t); (7)  FOR all candidates c?Ct DO (8)   c.count++; (9) END (10) Lk={c?Ck |c.count?minsup} (11) END (12) Answer=?Lk;  The association rule mining is a two-step process:  1.Find all the frequent itemsets in the transaction database. If support of itemset X, support(X) ?minsup, then X is a frequent itemset. Otherwise, X is not a frequent itemset.

2.Generate strong association rules from frequent itemsets. For every frequent itemset A, if B?A, B??, and support (A)/support(B)?minconf, then we have       association rule B? (A-B). The second step is relatively easy, and its algorithm generation can be found in the reference.

The present focuses in research are to find highly efficient algorithms in the first step.

3. PROBLEM DESCRIPTION  In Apriori algorithm, from Ck to Lk have two steps : (1) Pruning itemsets according to Lk-1. (2) Pruning itemsets according to minsupport  First, the set of frequent 1-itemsets is found. This set is denoted L1. L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found, and then algorithm ceases. In the cycle k, a set of candidate k-itemsets is generated at first. This set of candidates is denoted Ck. Each itemset in Ck is generated by joining two frequent itemsets that belong to Lk-1 and have only one different item. The itemsets in Ck are candidates for generating frequent sets, and the ultimate frequent itemsets Lk must be a subset of Ck. Every elements in Ck should be identified in business database to decide wither to join Lk.

The question of Apriori algorithm is that the set of candidate itemsets Ck is generated from Lk-1. Every itemset in Ck is tested whether its all k-l subsets constitute a large k-1 itemset or not, if one of k-1 itemsets is not in Lk-1 itemsets, the super itemset of this k-l itemset can be deleted. That is, every time a k itemset is constituted, Apriori must scan all Lk-1 itemsets.

Because of may scan large database many times in this way, the identifying processes are the bottleneck of the Apriori algorithm.

Apriori algorithm may need to generate a huge number of candidate generations. Each time when candidate generations are generated, the algorithm needs to judge whether these candidates are frequent item sets. The manipulation with redundancy result in high frequency in querying, so tremendous amounts of resources will be expended whether in time or in space.

4. IMPROVED ALGORITHM  4.1 Principle of the Improvement  The improvement is mainly way of reducing query frequencies and storage resources. We design an improved Apriori algorithm that mines frequent itemsets without new candidate generation. For example, in this algorithm we compute the frequency of frequent k-itemsets when k-itemsets are generated from (k-1)-itemsets. If k is greater than the size of transaction T, there is no need to scan transaction T which is  generated by (k-1)-itemsets according to the nature of Apriori algorithm, and we can remove it.

4.2. Algorithm in Detail  To implement the improvement, the improved algorithm is described as follow steps:  (1) The function apriori-gen(Lk-1) is called to generate candidate k-itemsets by frequent (k-1)-itemsets.

(2) Judging whether C is joined into candidate k-itemsets. It is processed by calling function has_infrequent_subset(C, Lk-1). If the return value is true, it means the sets aren?t frequent itemsets and should be remove in order to raise efficiency. Otherwise, scan database D.

(3) The frequency of frequent k-itemsets is computed when k-itemsets are generated by (k-1)-itemsets. If k is greater than the size of transaction T, there is no need to scan transaction T which is generated by (k-1)-itemsets according to the nature of Apriori algorithm, and we can delete it.

(4) If the size of transaction T is greater than or equal to k, then function subset(Ck,t) is called, which finds frequent itemsets using an iterative level-wise approach based on candidate generation.

Algorithm1: Apriori. Find frequent itemsets  L1 = {large 1-itemsets}; FOR (k=2; Lk-1??; k++) do BEGIN Ck=apriori-gen(Lk-1,min_sup);  // new candidate generation END RETURN L=UkLk;   Algorithm2: apriori-gen(Lk-1).Generate candidate generation  FOR all itemset p? Lk-1 DO BEGIN FOR all itemset q? Lk-1 DO BEGIN IF p.item1=q.item1, ?, p.itemk-2=q.itemk-2,  p.itemk-1 < q.itemk-1 THEN BEGIN c= p?q; // Put the k-1 elements in q join to the back of q IF has_infrequent_subset(c, Lk-1) THEN delete c; // Delete the candidate generation that include  non-frequent subtests ELSE FOR all transactions t?D DO  BEGIN// Scan D and accumulate itemsets IF Count(t)<k THEN  delete t; ELSE BEGIN Ct=subset(Ck,t); // Take out the subsets of candidate generation  included in transaction t      FOR all candidates c? Ct DO c.count++; END END Lk={c? Ck |c.count?minsup} END END END END Return Ck;   Algorithm3: has_infrequent_subset(c, Lk-1). Judge the elements of candidate generation  FOR all (k-1)-subset s of c DO IF s?Lk-1 THEN Return TURE; Return FALSE;   Owing to the improved algorithm mines frequent itemsets without candidate generation, the query frequency falls to about half level comparing with Apriori algorithm. The size of database is reduced; meanwhile, the storage space and computing time are saved.

5. EXPERIMENT RESULTS  This is an example based on the transaction database, D, of Figure 1. There are nine transactions in this database, that is, |D|=9.We use the improved Apriori algorithm for finding frequent itemsets in D.

Figure 1 Generation of candidate itemsets and frequent  itemsets  1.Scan D for count of each candidate.

In the first iteration of the algorithm, each item is a member of the set of candidate 1-itemsets, C1. The algorithm scans all of the transactions in order to count  the number of occurrences of each item.

2.Compare candidate support count with minsup.

Suppose that the minimum transaction support count required is 2. The set of frequent 1-itemsets, L1, can then be determined. It consists of the candidate 1-itemsets satisfying minimum support.

3.Generate C2 candidates from L1 and scan D for count of each candidate.

To discover the set of frequent 2-itemsets, L2, the algorithm generates a candidate set of 2-itemsets, C2.

And then the transactions in D are scanned and the support count of each candidate itemset in C2 is accumulated, as shown in the table of C2 in Figure 1.

4. Compare candidate support count with minsup.

The set of frequent 2-itemsets, L2, is then determined, consisting of those candidate 2-itemsets in C2 having minimum support. Then D2 was determined from L2.

5. Generate C3 candidates from L2 and scan D2 for count of each candidate.

First let C3=L2?L2={{I1, I2, I3}, {I1, I2, I5}, {I1, I3, I5}, {I2, I3, I4},{I2, I3, I5},{I2, I4, I5}}. Based on the Apriori property that all subsets of a frequent itemset must also be frequent, it can determine that the four latter candidates cannot possibly be frequent, therefore remove them from C3, thereby saving the effort of unnecessarily obtaining their counts during the subsequent scan of D2 to determine L3.

6. Compare candidate support count with minsup.

The transactions in D2 are scanned in order to determine L3, consisting of those candidate 3-itemsets in C3 having minimum support.

7.The algorithm uses L3?L3 to generate a candidate set of 4-itemsets, C4. Although the join results in {{I1, I2, I3, I5}}, this itemset is pruned since its subset {{I2, I3, I5}}is not frequent. Thus, C4=?, and the algorithm terminates, having found all of the frequent itemsets.

6. CONCLUSIONS  In this paper, the improved Apriori algorithm is proposed to update the classical Apriori algorithm.

Through pruning candidate itemsets by the infrequent itemsets, the present algorithm can reduce the number of database scanning and the redundancy while generating subtests and verifying them in the database.

Validated by the experiments, it can obtain higher efficiency.

