

TEMPORAL ASSOCIATION RULES MINING IN T-DATABASES USING PIPELINE TECHNIQUE     Kanhaiya Lal        N.C.Mahanti Deptt. of Computer Sc. & Engg.     Deptt. of Applied Mathematics  BIT, Patna Campus               BIT Mesra Patna, India         Ranchi, India  klal@bitmesra.ac.in       ncmahanti@rediffmail.com     Abstract- Temporal data mining is rapidly evolving area of research that is at the intersection of several disciplines, including statistic, temporal pattern recognition, temporal database, optimization visualization, high performance computing & parallel computing. The presence of a temporal association rule may suggest a number of interpretations, such as  ? Past event(PE) ? Future event(FE) ? The event(E) ? PE and FE ? events ? coincidental (c)  Classical association rules have no notion of order, while time implies an ordering. If we could find the associability of time with event, nothing will be hidden to us as the events are associated to each other in the form PE?PtE(present event)?FE. In this study, we examine the association rules mining in temporal database. After partitioning the database, a time interval TI=[s,e] is allocated to each partition and sequentially put the partitions in an array, in reverse order.

Keywords- Association rules; temporal databases; timestamp model; pipeline; parallelization.



I. INTRODUCTION The increase in the affordability of storage capacity, the associated growth in the volumes of data being stored and the mounting recognition in the value of   temporal data and the usefulness of temporal databases and temporal data modeling has resulted in the prospect of mining temporal rules from both static and longitudinal/temporal data. Data mining can itself be viewed as the application of artificial intelligence and statistical techniques to the increasing quantities of data held in large, more or less structured data sets, such as databases [1], [2], [3] and temporal data mining is an extension of this work.

Depending on the size of the database, many efficient algorithms have been proposed for finding the frequent patterns in a database [4][5][6][7][8][9][10].

A. TEMPORAL DATABASE A temporal database is a database with built- in time aspects. More specifically the temporal aspects usually include valid-time and transaction-time. These attributes go together to form bi temporal data.

? Valid time denotes the time period during which a fact is true with respect to the real world.

? Transaction time is the time period during which a fact is stored in the database.

? Bi temporal data combines both Valid and Transaction Time.

DOI 10.1109/CSE/I-SPAN/IUCC.2011.74    DOI 10.1109/CSE.2011.196     It is possible to have timelines other than Valid Time and Transaction Time, such as Decision Time, in the database [11].

The Main Goal of Temporal Database:  ? Identification of an appropriate data type for time  ? Prevent fragmentation of an object description  ? Provide query algebra to deal with temporal data  ? Compatible with old database without temporal data [12][13].

B.  RELATIONSHIPS BETWEEN EVENT AND TIME  Time is represented by a calendar system that has multiple granularities such as hours, days, etc. We assume that there is an absolute time line represented by real numbers [14]. Clock ticks of a calendar are represented by natural numbers. Each clock tick corresponds to an interval of real numbers. Thus, clock ticks of a calendar system are mapped to consecutive intervals of real numbers that are disjoint. This definition allows representation of multiple time granularities, which is essential for modeling temporal data as well as mining useful information from it.

Generally, data is batched for a meaningful period of time and data mining techniques are applied to discover association rules.

However, in a temporal context, we can batch data by different time granularities, i.e. hour, day, month, quarter, etc. and this may reveal further useful information. Let?s assume that the absolute time line is represented by a time unit of seconds. A calendar based on minutes would see every consecutive 60 second interval as one minute. A calendar of hours would consider every 60 minutes interval, 3600 seconds, as one hour and so on. The figure 1 shows any event takes place in the world is associated with a clock tick. The classification and characterization rule discovery algorithms, discussed in a series of papers by Han and others [15], [16], [17], can be extended to accommodate time in a number of ways.

Minimally, time can simply be provided as a concept hierarchy, such as: Day ! Week !

Quarter ! Financial Year: Algorithms deriving concepts can use one of those concept hierarchies to generalize data, estimate their statistical support in the sample, and try to find trends in them.

However, time can be generalized in many ways. An alternative concept hierarchy could be: Day ! Month ! Calendar Year: Both hierarchies are equally valid for the generalization process. However, the statistics are computed on different groups of the same data as in [18]. In [19] an algorithm is developed which generates candidate patterns and then matches them against user supplied granularities. . Relative Time: The concepts of before and after in sequences are addressed in sequence analysis.  Richer Allen-style interval relationships, such as during, overlaps, contemporary of, etc., are more problematic.

Incorporating them into a mining algorithm remains an open problem. The problem of varying granularity is discussed in [20].

C.  TIME SERIES TEMPORAL DATA MINING Measure of temporal sequence is used  for measuring temporal characteristic element between temporal sequences[26].

Two methods can be used for this:  a) Characteristic distance measuring in time domain  b) Characteristic distance measuring in other than time domain  Predication of temporal sequence predicts some fields in a database based on time domain. The temporal classification model and temporal regression models.



II. PROBLEM DESCRIPTION  Let {I1 ,..., Im}be a set of binary attributes, called items. Let D be a database of transactions that contain a subset of items I.

Each transaction T is represented as a binary vector, with T[k]=1 if T bought the item Ik , and T[k]=0 otherwise. Let X be a set of items in I. We say that a transaction T satisfies X if for all items Ik in X, T[k]=1. By an association rule, we mean an implication of the form X  Y, where X ? I, Y ? I, and X  Y = ?. X is the antecedent of the rule, and Y is the consequent of the rule. The rule X  Y holds in D with confidence c if c% of the transactions in D that contain X also contain Y. The rule X  Y has support s in D if s% of the transactions in D contain X ?Y.

Confidence is a measure of rule?s strength while support corresponds to its statistical significance. An itemset X is called a k- itemset if it contains k items from I. X + Y is a l-extension of X if Y is a l-itemset and X Y = ?. Given a database DB, the problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified minsup and minconf, respectively.

In temporal data mining applications, it is necessary to search within a temporal  sequence database for those sequences that are similar to a given query sequence. The problem is known as similarity search problem. This kind of problem involves search on multiple and multidimensional time series sets in TDBs to find out how many series are similar to one another. It is one of the most important problems in TDM. A sequence (?) is represented as ?1??2??????q , where ?1 is an event. A sequence is called a k-sequence, if the sum of the cardinalities of ?1 is k. The sequence s = (?1??2??????q) is said to be subsequence of s1 = (?1??2??????r), if there exist indices t1 < t2 <??tq of s1 such that ?1 ? ?t1, ?2 ? ?t2 ???q ? ?tq. The problem is concerned with the temporal order of the events within a sequence of transactions. A more general problem is when we focus on the temporal distance between the events. That is, transaction should indicate the temporal gap between two transactions.



III. RELATED WORKS  Association rules typically find correlations between items in transaction data sets that record activity (such as purchases) on multiple items as part of a single transaction [25]. In association with rules mining, Apriori (Agrawal and Srikant, 1995), DHP (Park et al., 1997) and partition-based ones ( Savasere et al., 1995; Lin and Dunham, 1998) were proposed to find frequent itemsets. Various aspects of the temporal data mining and knowledge discovery were discussed. This has included a discussion of temporal rules and their semantics, the discovery of temporal rules (including temporal associations rules, time sequences and series, temporal classification, etc.), as well as, the issue of interestingness, and temporal mining (and meta-mining) environments. Many of the techniques we have addressed are applicable equally to     temporal data sets, series of static data sets, and temporally oriented data warehouses(Roddick and Spiliopoulou, 2002; Tansel and Imberman,2007;  chu etal, 2008).

In cases where client histories exist, temporal patterns on purchasing or other behavior over time can be discovered and used in strategic planning [26], [27].  The presence of a temporal association rule may suggest a number of interpretations. The earlier event plays some role in causing the later event. There is a third (set of) events that cause both other events. The confluence of events is coincidental.  The concept of association rule discovery is the same for temporal and non-temporal rules; algorithms designed for conventional rules cannot be directly applied to extract temporal rules.

The reason is that classical association rules have no notion of order, while time implies an ordering. This ordering affects the statistical properties of the data and the semantics of the rules being extracted from them [28, 29].

Although much research is being performed in this domain, there are still several open issues like concepts, algorithms, and supportive techniques (such as indexing) designed for mining over static data should be extended to take the temporal dimension into account. In particular, mining algorithms for static data are not directly applicable on temporal data. Concepts such as ?interestingness? should incorporate the temporal dimension of the data and of the rules extracted from them. Database systems should offer data structures and operators appropriate for efficient temporal data mining.

This motivates us to mine association rules in temporal databases in this research.



IV. TEMPORAL  ASSOCIATION RULES   One of the reasons behind maintaining any database is to enable the user to find interesting patterns and trends in data. The goal of data mining is to automate this process of finding interesting patterns and trends. Once this information is available, we can perhaps get rid of the original database. This goal is difficult to achieve due to the vagueness associated with the term ?interesting?. The solution is to define various types of trends in the database. One such type constitutes the association rules [30].

A set of items is referred to be as an itemset. An itemset that contains k items is a k-itemset. The occurrence frequency of an itemset is the number of transactions that contain the itemset. This is also known, simply, as the frequency, support count, or count of itemsets. An itemset satisfies minimum support if the occurrence frequency of the itemset is greater than or equal to the product of min-sup and the total number of transactions in the transaction set.

The number of transactions required for the itemset to satisfy minimum support is therefore referred to as the minimum support count. If an itemset satisfies minimum support, then it is a frequent itemset. The set of frequent k-itemsets is commonly denoted as Lk.

Algorithm Input:  Database of transaction (D) Items (I) Large itemsets (L) Support(s) Confidence (?)  Output: Association Rules satisfying s and ?   ARGen algorithm R=0; For each l?L do For each x? l such that x ? 0 do If support(l) / support(x)?? then     R= R? {x  (1 ? x)}; Let I=I1, I2, ? , Im be a set of m distinct attributes, T be transaction that contains a set of items such that T ? I, D be a database with different transaction records Ts. An association rule is an implication in the form of X Y, where X, Y ? I are sets of items called item sets, and X  Y =?. X is called antecedent while Y is called consequent, the rule means X implies Y. There are two important basic measures for association rules, support(s) and confidence(c). Since the database is large and users concern about only those frequently purchased items, usually thresholds of support and confidence are predefined by users to drop those rules that are not so interesting or useful. The two thresholds are called minimal support and minimal confidence respectively. Support(s) of an association rule is defined as the percentage/fraction of records that contain X  Y to the total number of records in the database. Suppose the support of an item is 0.1%, it means only 0.1 percent of the transaction contain purchasing of this item.

Confidence of an association rule is defined as the percentage/fraction of the number of transactions that contain X  Y to the total number of records that contain X.

Confidence is a measure of strength of the association rules, suppose the confidence of the association rule X Y is 80%, it means that 80% of the transactions that contain X also contain Y together. In general, a set of items (such as the antecedent or the consequent of a rule) is called an item set.

The number of items in an item set is called the length of an item set. Item sets of some length k are referred to as k-item sets.

Generally, an association rules mining algorithm contains the following steps: ? The set of candidate k-item sets is generated by 1-extensions of the large (k - 1)-item sets generated in the previous iteration.

? Supports for the candidate k-item sets are generated by a pass over the database.

? Item sets that do not have the minimum support are discarded and the remaining item sets are called large k-item sets. This process is repeated until no larger item sets are found.

[21,[22],[23]. With above rules a transaction can be viewed as <TID , CID, I1, In , ??. Im > Where  TID is the ID for the transaction, CID is the ID for customer, and I1, In , ??. Im are the items. When considered part of temporal database, a transaction could be viewed as <TID , CID, I1, In , ??. Im , ts, te>.

Where [ ts, te] is the valid time range for the transaction. If this were a grocery store transaction, ts = te could be the point in time that the transaction was completed.

Figure 2.  Association rules in sequence  for partition database

V. METHODOLOGY     Our approach discovers the rules within a certain period for each partition, and repeats this process for the whole database for consecutive time periods.

Let TI = [s, e] be a time interval. DBTI denotes the portion of a temporal relation that is valid during the interval [s, e].

Let ATI be the set of large itemsets derived from DBTI.  Let TI1, ?, TIn be consecutive, non-overlapping, fixed length time intervals, and ATI ,..., ATI n 1 be the corresponding large itemsets.

Figure 2, depicts temporal relations and the large itemsets in partition over the time line in. Let L be the length of a time interval, START be the start of TI1, END be the end of TIn,  and g be the time granularity of the temporal database finds the large itemsets, ATIi , for i= 1, ?,n. Changing L to n?L walks through the database in time intervals of [s, s+n?L].

For example, if L is one day, 365?L would be a year. This is particularly useful for observing seasonal/yearly/halfyearly, cyclical patterns, and changes in such patterns. Moreover, it is possible to walk through the database by sliding the starting points of intervals different than the interval length.

Algorithm for databases in different time granularities  1  s = START 2  e = START + L 3  while s  END do 4  begin 5  R[s, e] = R <X, fI>[s, e] 6  compute A[s ,e] 7  s = s + L + g 8  e = s + L 9  end [30].

Generation of Local Large Item sets:-   The procedure gen_large_itemsets takes a partition and generates all large itemsets ( of all  lengths ) for that partition . The procedure is shown in Figure 2 . Lines 3-8 show the candidate  generation process. The prune step is performed as follows:  Prune ( c: k-itemset) Forall  (k-1) subsets s of c do If s doesnot belongs to Lk-1 then Return ?c can be pruned"  The prune step eliminates extensions of (k- 1)-itemsets which are not found to be large, from  being considered for counting support . For example , if Lp3 is found to be { { 1 2 3 } , { 1 2 4 } , { 1 3 4 } ,{ 1 3 5 } , { 2 3 4 } } the candidate generation initially generates the itemsets { 1 2 3 4 } and  { 1 3 4 5 }. However , itemset { 1 3 4 5 } is pruned since  { 1 4  5 }  is not in Lp . This technique is same as the one described in [4] except in our case , as each candidate itemset is generated , its count is determined immediately.

The counts for the candidate itemsets are generated as follows. Associated with every itemset, we define a structure called as tidlist .A tidlist for itemset l contains the TIDs of all transactions that contain the itemset l within a given partition . The TIDs in a tid list are kept in sorted order. We represent the tidlist for an itemset l by l tid list .Clearly, the cardinality of the tid list of an itemset divided by the total number of transactions in a partition gives the support for that itemset  in that partition.

Initially, the tid lists for 1-itemsets are generated directly by reading the partition.

The tid list for  a candidate k-itemset is generated by joining the tid lists of the two (k-1)-itemsets that were used to generate the candidate k itemset. For example, in the above case the tid list for the candidate itemset { 1 2 3 4 }  is generated by joining     the tid lists of itemsets { 1 2 3 }  and   { 1 2 4 }.

Procedure gen_final_counts  (  CG: global candidate set, pr  database partition)  1) forall 1-itemsets do 2)  generate the tid list 3)  For ( k=2 ; CkG   ? ; k++) do begin 4)  Forall k ?itemset c belongs to CKG   do  begin 5)  Templist = C[1].tidlist ?  C[2].tidlist????c[k].tidlist 6)  C.count = count + | templist | 7)  End 8)  End   Generation of Final Large Itemsets :-  The global candidate set is generated as the union of all local large itemsets from all partitions. In phase II of the algorithm, global large itemsets are determined from the global candidate set; this phase also takes n (number of partitions) iterations. Initially, a counter is setup for each candidate itemsets and initialized to zero. Next, for each partition tidlists for all 1-itemsets are generated. The support for a candidate itemset in that partition is generated by intersecting the tidlists of all 1-subsets of that itemset the cumulative count gives the global support for the itemsets.

Discovering Rules Once the large itemsets and their supports are determined , the rules can be discovered in a straight forward manner as follows : if l is a large itemset , then for every subset a of l , the ratio support ( l )  / support ( a )  is computed . If the ratio is at least equal to the user specified minimum confidence, them the rule  a => ( l - a ) is output[22].



VI. CONCLUSIONS & FUTURE SCOPE  Knowledge discovery in general, and discovering association rules in particular, involves the extraction of useful information from large datasets. Temporal databases are naturally good sources for knowledge discovery. Moreover, temporal databases provide the opportunity to see how unearthed knowledge changes through time.

In this paper, we have developed a formalism to make temporal databases a good source for knowledge discovery. It is also possible to observe the seasonal or cyclical changes and fluctuations in the rules. Moreover, the formalism presented in this paper can be used to extract the association rules in a given time interval, beginning the discovery process in small time intervals, and combining the results of them to obtain a complete set of association rules in the specified time interval.

If we consider t1 as the time taken by partition algorithm to generate local large itemset and t2 to generate global large itemset i.e. total time taken = t1+t2.

Since the proposed algorithm uses m-stage pipeline to generate local large itemset therefore time taken is t1/m+t2.

This approach will yield important results for the decision makers in the fields where customer preferences are strongly interrelated with the time. For example, the application of our formalism to the market basket data will be beneficial while interpreting the results of the promotions, discounts, etc.

There are many problems for future research such as optimizing the enumeration operation and the discovery of association rules, evaluating the performance of different algorithms for discovering association rules, determining the right interval size, etc including incremental approaches. We are implementing our algorithm and comparing with other.

