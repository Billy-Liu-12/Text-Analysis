

Abstract  This paper focuses on the use of extremely large data sets in power system operation, control, and protection, which are difficult to process with traditional database tools and often termed big data. We will discuss three aspects of using such data sets: feature extraction, systematic integration for power system applications, and examples of typical applications in the utility industry. The following analytics tasks based on big data methodology are elaborated upon: corrective, predictive, distributed and adaptive. The paper also outlines several research topics related to asset management, operation planning, real- time monitoring and fault detection/protection that present new opportunities but require further investigation.

Introduction  By virtue of deploying a smart grid infrastructure the utility industry is facing new challenges in dealing with extremely large data sets, often called big data, and using them to improve decision-making. Work in [1] points out that big data in the electric power industry can be of large volume, high velocity, increasing variety, or all three. The volume of data has been growing significantly with introduction of new metering devices. Velocity refers to the temporal constraints on collecting, processing and analyzing data, which is the case with synchrophasor data.

For efficient condition-based asset management and outage prevention real-time operation is necessary, often requiring fast processing of large volumes of data. Variety refers to data coming from many different sources that are not necessarily part of the traditional electric utility data.

As mentioned, typical examples of big data encountered by the utility industry include streaming synchrophasor measurement system (SMS) data used for situational awareness, and similarly extensive data sets collected by poling automated revenue metering (ARM) systems for billing purposes. Another type of big data stems from asset management and may consist of condition-based measurements acquired by intelligent electronic devices (IED) as well as nameplate and maintenance data entered off-line. Other very large data sets, not directly obtained through the utility field measurement infrastructure but  widely used in decision making, such as weather data, data from the National Lightning Detection Network (NLDN), Geographic Information System (GIS) data, and electricity market data with different planning spans, are becoming more readily available. Several recent papers deal with the integration of GIS data [2-4], US NLDN data [5], and electricity market data [6], for power system applications. The new Hierarchically Coordinated Protection (HCP) approach is proposed in [7, 8]. This approach utilizes local and wide area measurements organizing them in three HPC levels: fault anticipation and prediction, adaptive fault detection, and relay operation correction in case of unwanted tripping. Big data plays an important role in the integration of variable renewable energy sources [9].

Data is typically not handled by utilities using a single, consistent, data management framework making ad-hoc utilization by novel decision making applications needlessly complicated. While data analytics may need to reach across diverse data sets, if the data is not correlated in time and space, if it does not have common data syntax and semantics to assure ease of use, and if it is not correlated to a unified and generalized power system model, such data analytics may not be easily implementable. Past experience, particularly from large blackouts that have affected the grid in recent years, has shown the need for better situational awareness about network disturbances such as faults and dynamic events, sudden changes of intermittent power from renewable resources such as wind generation, outage management tasks such as fault location and restoration, and monitoring of system operating conditions such as voltage stability. All of the mentioned tasks, and others, are handled reasonably well by existing solutions but improvements in decision-making are highly desirable in order to produce more cost-effective and timely decisions, facilitating more efficient and secure grid operation. This paper focuses on utilization of integrated data sets and systematic data processing to improve decision-making.

The paper first explains what the typical large data sets of interest in power system operation and protection are, then it discusses how they may be processed and managed, and finally examples of future applications are provided.

Types of Big Data Used in Power  Big data sets and their relationship to sam are depicted in Fig. 1.

Figure 1. Big Data and applications  The deployment of phasor measurement u provided operators the ability to measur values of voltages and currents with accu [10]. Since PMUs provide both magni measurements they can be imported to sta provide direct state measurement [11]. T relevant when system topology such as au operation changes rapidly. PMU data can protective relaying applications [12]. Th data can aid in faster and more accurate tr Improved communication and visualizati engineers to better analyze wide area even system oscillations may not be detected b slow-response SCADA system. Wide-a can be utilized to implement new to situational awareness and help operator best response to an event, or allow autonomously in cases where human respo be exceedingly slow [13, 14]. Su applications are increasingly significan variable renewable sources which are more unpredictable stress on the existin transmission lines connecting renewabl remote areas to the load centers.

The management of large data at the dist also important in order to facilitate appl demand response, integration of dist resources (DERs), and electric vehicles loads into dispatchable resources and all participate in electricity markets will re and secure two way communications as management of the associated big data.

r Systems  mple applications    units (PMUs) has re instantaneous urate timestamps itude and angle ate estimators to his is especially  utomated breaker also be used for  he use of PMU ripping response.

ion would allow nts. Many power by the traditional area information ools for better s determine the systems to act onse time would uch automated nt in light of likely to create  ng long distance e sources from  tribution level is ications such as tributed energy s [15]. Turning lowing DERs to equire improved s well as better  Data Processing  Data processing steps that will be are illustrated in Fig. 2 and are ba innovations: representation of mult the big data to be matched at an utilization of advanced data minin the most relevant data features, a multi-domain graphics analytics making. The technical approach illustrated by the flow of data analytics as shown in Fig 2. T indicates that as one moves p collection and data conversion laye key components of new data an information features, the use o knowledge, and the use of knowle visualization for facilitated decision   Figure 2. Data management steps  Data Correlation  To ensure that the relevant data application purposes, the paper w such a correlation may be establis The considerations included in thi in Fig. 3. For each relevant data set on how the data may be correlated or data model, and what the featur to be correlated are.

Figure 3. Spatiotemporal correlation of b  discussed in the paper ased on three important tiscale models to allow appropriate resolution,  ng techniques to select nd specification of the to enhance decision-  h discussion may be a and associated data The presented concept past the measurement ers, the focus is on three nalytics: extraction of  of features to expand edge to define the best n-making.

may be correlated for will now discusses how shed in time and space.

s discussion are shown t the discussion focuses d to the power systems res of interest that need   big data    With the use of a Geographical Informatio the collected data can be viewed in a geo An important source of information for G Positioning System (GPS) which provid time information thus giving the data temporal component.

In order to reduce the number of interrup the power industry is interested in catas conditions such as lightning, severe sto which can lead to outages in power sy information is collected as a set of mea weather radar systems located at various a region and it is commonly integrate indexed with GIS. Depending on metho locate lightning strikes the temporal re from 0.1 to 5 ?sec and the source position within a few hundred meters [5].

Corrective, Predictive, Distributed and Ad Making  The main focus of the decision-making f the exploration of innovative computatio allow novel applications:  a) Corrective: This concept facilita actions aimed at rectifying undesirable co as they develop. An example of such a risk-based asset management and optimization.

b) Predictive: This concept provides forecasting of system behavior allowin operations planning. An example of such a operations-planning convergence and renewable generation and mobile loads (E  c) Distributed: This concept demon to assess system state based on a distribute that fast control decisions can be execu example application is online assessm stability.

d) Adaptive: This concept enable monitor unfolding events very clo adjustments in operating strategy. An exa application is enhanced disturbance detec outage management.

The following sections illustrate research be pursued to get full use of big data in applications in the future.

Asset Management  This application combines large data se condition based monitoring of power sys large data sets representing the asset mode on the example of using condition based  on System (GIS) ographic context.

GIS is the Global des location and a a spatial and  ptions in service strophic weather rms and winds,  ystem grid. This asurements from locations across  ed and spatially odology used to esolution ranges n is uncertain to  daptive Decision  framework is on onal concepts to  ates just-in time onditions as soon an application is d maintenance  s a very detailed ng for enhanced an application is interactions of  EVs).

strates an ability ed processing so uted locally. An  ment of voltage  es operators to osely allowing ample of such an ction and on-line  topics that may n power system  ets from on-line stem assets with el. We will focus data from a CB  control circuit. The relevant measu in Fig. 4, including the trip initiate contact voltages, with associated [16]. The explanations of these ti Table 1.

Figure 4. Signal features for CB monitori  Records of timings, across many o the circuit breaker and across many system, may be used to form prob these events.

Table 1. Explanation of signal features f Event Event Description  1 Trip or close operation is inifrom LOW to HIG 2 Coil current picks 3 Coil current dips after s 4 Coil current drops  5 B contact breaks or makes ( LOW to HIGH or vice 6 A contact breaks or m   Together with actual conditions o historical records may be used to of circuit breaker operating cond particular timing to the adequate distribution related to that timing, may be used to predict when malfunctioning or classify those are close to malfunctioning (prop blue).

Figure 5. Historical distribution of timing  urements are illustrated e, trip coil current, and timings ?1 through ?6  imings are included in   ing signals  operations (tripping) of y circuit breakers in the bability distributions of  for CB monitoring signals n Signal tiated (change  GH) ?1  s up ?2 saturation ?3 s off ?4 changes from  e versa) ?5  makes ?6  f circuit breakers these build statistical models  ditions. Comparing any e historical probability as illustrated in Fig. 5, a circuit breaker is  circuit breakers which per operation shown in   gs ?2    As illustrated in Fig. 5 if a statistical model of a timing, in this case ?2 is well known, and the proper thresholds of failure, in this case the interval [11 s, 16.5 s], are available, it is possible to classify the operation of a circuit breaker according to the recorded timings.

Operations-Planning Convergence  This application refers to the utilization of unified models, facilitating massive data integration and use. The realized actual physical state of the power system is the result of a set of overlapping decisions that take place at different temporal scales, from long-term planning to operations planning to real-time control. This is illustrated in Fig. 6.

Figure 6. Planning, Operations Planning and Real-Time Operations Process  Planning provides long-term and mid-term plans, which are utilized by operations planning. Operations planning in turn provide week and day-ahead schedules to real-time operations, which are then delivered to the realized system. A post-operational analysis provides feedback in order to affect and enhance plans and schedules.

The term operations-planning convergence refers to the ability of a utility enterprise to realize the future conditions of the power system with high probability and high accuracy. This is difficult to achieve without systematic data management and unified models.

Inevitably, external conditions such as outages, errors in demand forecasting, and variability of renewable sources will result in a realized system that is different from the planned system. Besides external and intrinsic system deviations, there are several other reasons why the realized system is different from the planned system. Let us consider the historical records of the system for a certain time range [Tstart, Tend]. We select this range to be  large enough so that any decision-making uses information contained within the range. Within this large time range, let us consider the time range of one day:   dt0 = {t : Tstart << t0 < t < t0+24 << Tend}         (1)  In order to plan that day, there were a set of operational planning procedures that took place before hand. Let us denote the scheduled state of the system for a given time t by s(t), and the realized state of the system by r(t). The difference r(t) ? s(t) is the convergence gap. Let us denote by St0 = { s(t): t0 < t < t0+24} the sequence of scheduled system states for the day ahead that starts at t0, and by Rt0 the corresponding sequence of realized states. The day- ahead schedule St0 is a function of past realized system information and some forecasted information available at the time when the schedule was generated.

Assuming a deterministic scheduling procedure, we can utilize the realized information in the range [Tstart, Tend] to reproduce the scheduling process that took place to generate a day-ahead schedule St0. If the scheduling process was perfect, the scheduled states would be identical to the realized states, e.g., St0 = Rt0. However, records for most utilities suggest that there may be significant differences between what was planned and the realization, even under assumptions of deterministic scheduling and certainty of demand and output from renewable sources. There are various reasons for this convergence gap, which can be overcome with the proposed unified methods and systematic data management approach:  a) Models: Traditionally, it has been difficult to compare operational and planning cases even within the same utility, due to the use of diverse modeling approaches by different applications. Thus it has been difficult to determine the extent of the convergence gap, except when discrepancies in individual elements, such as transmission lines flow, become obvious.

b) Format: The formats used by different applications may be incompatible, making comparisons on large data sets very difficult.

c)  Data Management: Polling the data from the various subsystems necessary to reproduce planning procedures has been cumbersome. New systematic methods are required.

d) Computation: It has been difficult to set a simulation planning environment for convergence assessment.

e) Application Setting: Different applications that are fundamentally realizations of the same algorithm for operations and planning use different settings. For instance a power flow application may have different options for variable sharing in power plants for the automatic setting of LTC transformers.

The Role of Big Data in Convergence  Under a unified model approach the d setting is illustrated in Fig. 7, where access a unified model (or its database rep   Figure 7. Proposed Unified Topology Model  Convergence of operation and pl successfully managing the electricity in system was planned correctly, and the s took place as planned. Electric utility amounts of data from emerging sen operation of the power infrastructure that systematic and strategic manner, means ne to provide a richer feedback loop for op opportunities are identified at three levels:  a) Being able to provide an otherw feedback loop.

b) Being able to take actions to corr planning or operational processes.

c) Being able to realize the planned  Some of the features of big data that enab are:  a) Availability of more data: mor more robust statistical or data mining ana increased process accuracy and enhanced data requires better storage processes, vir systems and possibly distributed data cent  b) New data, which allows creatin loops for planning and operations. For PMU data can be used to enhance an estimator or help determine model errors.

be managed using an integrated and fle new data is generated as raw facts from s as by-products of more and enhanced d functions. For example smart meter da enhanced load forecasting. Enhanced l may enable forecasted prices, etc.

c) Better management of data, w clear, actionable information. Traditional been managed in a server-centric archite requires an information-centric architect storage and other IT resources are optim information centric approach allows mo  decision-making all applications  plica).

lanning means frastructure: the ystem operation big data, large sors about the is managed in a  ew opportunities perations. These :  wise non-existent  rect and enhance  system state.

ble convergence  re data enables alysis that allows d control. More rtualized storage ers.

g new feedback r instance, new n existing state . New data must exible platform; sensors, but also decision-making  ata may support load forecasting  which provides l utility data has ecture. Big data ture, where the  mally shared. An ore flexible and  adaptive management including data.

d) Advanced analytics, t decision-making.

Distributed Online Monit Instability  Big data could be used for imp awareness in large-scale system example, the monitoring of quasi- can be performed using SCADA da manner [17]. Theoretically, the collapse can be associated with the power flow Jacobian matrix o submatrices. There has been performance indices proposed in th collapse analysis and control, suc load voltage deviation, maximum l damping ratio. There have also b recently on using second-order saddle-node bifurcation surface detection.

In practice, online voltage stability function in modern control cent power systems such as PJM, NYIS operators in control rooms typica Calculator (TLC), which simula capability based on the latest system the state estimator, to assess the TLC calculates the voltage collaps of pre-defined interfaces, which voltage transmission lines. Ty simulated to assess the power tran the interfaces to the other, typi surplus regions to load centers. Th are typically based on the oper lengthy off-line studies. Howeve scheme of voltage instability m effective under a much broader set for the following reasons:  1) A reliable state estimation the TLC calculation. However, in estimator may not converge reliab close to stressed conditions/voltage  2) The pre-defined reactive necessarily be the actual weakest tr system due to the change of operat the transmission outages and/or from renewable sources.

3) In order to obtain a reli accurate equivalence of externa required. However, such accurate unavailable or results in a heavy co  emerging unstructured  to support complex  toring of Voltage  proving the situational m operations. As an -static voltage collapse ata in a fully distributed e quasi-static voltage e near singularity of the or its reduced order  several centralized he literature for voltage ch as minimization of loading point, and least been discussions more approximation of the for voltage instability  monitoring is a critical ters of interconnected O and ERCOT. System  ally use Transfer Limit ates the MW transfer m snapshot obtained by  voltage stability. The se points based on a set include multiple high- ypical scenarios are nsfer from one side of ically from generation he choices of interfaces rators? experience and er, such a centralized onitoring may not be of operating conditions  n is the prerequisite of n practice global state bly when the system is e collapse.

e interfaces may not ransfer interfaces in the ting conditions, such as  stochastic generation  iable transfer limit, an al systems would be e equivalence is either omputational burden.

In our preliminary work, with the decent matrix Jk, we propose a performance decentralized monitoring of voltage collap     This PI represents the highest sensitivity singular value (SSV) ?m of Jk with respec apparent load vector S = [S1, S2, ?, S justification of PI is provided in [17]. By small value of widely used SSV of the J the proposed PI holds a large valu singularity, which makes it easier for syst detect the voltage collapse.

Based on the theoretical justification of su PI, large-scale power system operators online distributed monitoring of quas collapse by use of SCADA data. Simulat 118 bus system show a promising distributed analytics for voltage monit entire interconnection can be decompos areas based on their electrical couplings, the most critical information of system-w Jacobian matrix and includes only tie-lin neighboring control areas. Then, each c solve a decomposed power flow Jacobia significantly reduces the computationa complexity while maintaining the monit Such data and online methods allow a f online monitoring of system-wide instabili  Stability Margin Prediction U Measurements  Several data mining tools such a Regression, Neural Networks, Support V and Regression Tree have been used system stability status [18-21]. We w regression tree-based approach to predic system stability margin and detection system events.

Voltage and oscillatory stability are monitoring. In the case of oscillator damping ratio (DR) of the critical oscillati as the stability margin indicator. The v margin (VSmargin) may be defin continuation power flow (CPF) techni occurs when the load attempts to st capability of the combined transmission system.

( )1/ m dkPI S  ?  ?  ? =  ?  tralized Jacobian index (PI) for  pse  (2)  y of the smallest ct to the regional Sn]T. Theoretical  overcoming the Jacobian matrix,  ue near system tem operators to  uch a distributed could develop  si-static voltage tion results on a opportunity of  toring [17]. An sed into control which preserves  wide power flow ne buses in both control area will an matrix, which al burden and toring accuracy.

fully distributed ity.

Using PMU  s Multi-Linear Vector Machine to evaluate the  will focus on a cting the power  n of impending  e targeted for ry stability the ion mode is used voltage stability  ned using the ique. Instability tep beyond the  n and generation  The framework for RT-based stab and event detection is shown in F measurements are collected and Phasor Data Concentrator (PDC), t Wide Area Measurement System ( at the central control facility. Th monitoring oscillatory and voltag and updated periodically. When corresponding thresholds indicate margin operators are alerted with preventive control strategies ca commercial software CART [22 regression trees used for evalu voltage stability margins respective   Figure 8. Framework of the RT-based sta and event detection  Fault detection/protection  One example of problem assoc protection schemes comes from th signal during a fault is non- fundamental frequency compone damping, harmonics, etc. In some will result in an inaccurate representation of the faulted sign misoperation in relays [23]. Und thresholds may have to be recal feasible in real-time.

To overcome such problems intell neural networks and fuzzy logic sh neural network learning process co for converting the power system fie which then can be processed to which is based on the inter-neuron or synaptic weights. In order to in networks with power system fie  bility margin prediction Fig. 8. After the PMU  d time-aligned by the hey are delivered to the WAMS) server located  he regression trees for ge stability are trained never the checking of es insufficient stability the possible event and  an be initiated. The 2] is used to develop uating oscillatory and ely.

ability margin prediction  ciated with traditional e fact that the transient -stationary, containing ents, DC offset with e extreme situations, it estimation of phasor nal, which may cause er such conditions the lculated, which is not  ligent methods such as hould be explored. The onsists of the procedure eld data to information,  form the knowledge, n connection strengths,  ntegrate artificial neural eld data the following    questions should be answered: what data is required for neural network based fault detection and classification; what kind of model should be used to convert the data to information; and how the translation from information to knowledge leads to improved decision-making.

Neural network training and testing environment  The learning techniques for neural networks can be classified into two broad categories: supervised learning and unsupervised learning [24]. In supervised learning, each input signal is associated with the labeled output.

The task of learning a correct mapping of inputs to outputs is that of adequately adjusting the synaptic weights to minimize the overall error between the output patterns and their corresponding input patterns, across the entire set of data designated for training. In unsupervised learning, the categories of the outputs are not known in advance. The network is used to self-organize data by clustering to identify concentrations of input patterns maximizing mutual similarity within a cluster and mutual differences between clusters. A neural network based on a combined unsupervised/supervised training scheme has proven to be more capable of handling large data sets of random fault scenarios than solely using supervised training schemes [25]. The input data vectors of samples from voltage and/or current signals are mapped into clusters that contain information about fault existence and type. Fig. 9 shows the training environment process.

During the testing procedure, distances between each test pattern and established clusters are calculated. The outcomes of testing are class labels assigned to test patterns according to the most common value among the K nearest prototypes.

Data From Power System Simulation  Data From Substation Historical  Database  ART Neural Network Training  Prototypes of trained clusters  Data From Real System , CVT, CT  Selection of K nearest neighbour  Clusters  Fuzzy K -NN Classification  Fault Detection, Type & Zone  Fuzzy ART Neural Network  Algorithm   Figure 9. Fuzzy ART neural network algorithm for fault diagnosis  The input into the neural network is in the form of a moving data window containing samples of voltage and current signals from local measurements and simulations.

Patterns are extracted from these data and placed together in one row to form feature vector components, as shown in Fig. 10. Then the pattern is arranged using the post- fault samples of three phase voltage and current signals.

The zero sequence values of voltage 3v0 = va + vb + vc and current 3i0 = ia + ib + ic are also included to precisely detect ground faults. In this case, all fault types can be differentiated very well [24]. Thousands of such patterns obtained from power system simulation or substation databases of field recordings are used to train the neural network offline, and then the recovered pattern prototypes are used to analyze faults on-line by using the Fuzzy K- NN classifier.

Figure 10. Simulations for fault and non-fault scenarios  Data Processing  The neural network algorithm requires a large number of fault and non-fault cases to complete the process of training and testing for neural network tuning. Those training and testing cases are quite different for various transmission lines due to the selection of different simulation parameters and settings. To perform comprehensive tests, two categories of data can be acquired: field signal measurements and data from simulated cases. As it is not possible to acquire enough fault cases from the field, the needed data can be generated by simulation of the fault and non-fault scenarios, based on ATP/ATPDraw [26] and MATLAB [27]. The power system of interest is first modeled in ATP/ATPDraw. A user can define the desired fault or non-fault cases by initializing the simulation setting parameters in MATLAB. The measured three-phase    voltage and current samples, which may time stamp, are extracted in the data form by a user.

Feature Extraction  One of the tasks that is often time co extreme extent when dealing with big distinguishing between important an attributes. It is often desirable to consideration measurements that are not decision-making process. Automated rea support of the decision-making proces severe penalties in both accuracy and t when applied to data with many unnece However the methodology of distinguis often large amount of measurements, which are useful and should be includ making, and those which are not, is difficu perform in ad-hoc fashion.

Let us describe one powerful method of consideration measurements that are decision-making. Starting from the measurements we remove those for which Blanket approximation can be made using The intuition behind this method i consideration only those measurement cannot be derived from other measuremen not used to derive other measurements, w to the task at hand. The Hilbert-Schmid Criterion may be used to determine depe measurements in kernel space for this p this method, solidly grounded in the successfully applied to select am measurements those which are useful.

Detecting and Classifying Faults by Ma Labeled Clusters  The purpose of the information processin knowledge, which could allocate the train homogeneous clusters by a grouping tech clusters are assigned to classes, which are expected fault events in the power system type, etc. The number of clusters is incr positions are updated automatically durin there is no need to define them in advance the raw training patterns as information on clusters after the training processing, as kn right. The typical types of classification detecting the fault type and fault zone. T of mapping data to labeled clusters is perf the K-nearest neighbor rule (K-NN) [25].

y also include a mat files defined  onsuming, to an data, is that of  nd unimportant remove from  t helpful in the asoning tools, in ss, often suffer time complexity essary attributes.

shing, within an  between those ded in decision- ult to objectively  f removing from not useful to  full set of h a good Market g other features.

