KIMAS 2003 BOSTON, USA

Abstract-This paper describes techniques to identify data records that have unique or novelfield value combinations.

These techniques allow the rapid ident$ication of unique records in data sets without a priori knowledge of field values orfield-value ranges. This results in the formulation and creation of new knowledge about the data, based on information that is not evident and is not otherwise easily obtained.

1. INTRODUCTION This paper describes techniques to identify data records that have unique or ?novel? field value combinations. We define a combination of values to be novel when un more than k records in a data set have the same combination of values.

As an example, records are singularly unique in the data set when k = l .  Our approach is based on research being conducted for the National Science Foundation to identify at-risk records in anonymous databases.

Several measures of novelty are considered. The first measure ranks records according to the minimum number of field combinations that make the record novel. The probability of a record becoming novel increases with the number of fields considered. Therefore, a record that is novel on only a few fields is potentially more interesting than a record that is novel on more fields. Other techniques are applied to fields and field combinations to determine their relative contribution to record novelty.

This problem is motivated by the need to assess re- identification risk in public-use microdata, such as data from the United States Census that has had personally identifiable information removed from the microdata (i.e., it?s been ?anonymized). Confidential information about specific individuals is often released to the public with the belief that the individual identities are anonymous. Public use data also exist with personally identifiable information, such as voter registration data. Data fields shared by both data sets can be used to link personal identity with confidential information, a direct violation of individual privacy [l].

Prior work in data mining relies on finding association rules of the form X -> Y, where X and Y are conjunctions of fields and values. Associations are sought that have both high confidence and high support. Confidence is the percentage of records having both X and Y, while support is the percentage of records that have X or Y [Z]. However, a  number of measures exist to quantify interesting or novel patterns that may not have high confidence or support [3].

While this technique is useful for finding relationships among fields. our interest is finding unique records within the data set based on combinations of field values.

The remainder of this paper is organized as fnllows. First, we describe the data structure used to store information about the records and search for groups of unique records.

Next, we define uniqueness. This is followed with a description and analysis of the algorithm. We conclude with an example and discussion.

2. DATA STRUCTURE There are 2?-1 combinations of m fields for each record in a database. Each combination may be represented by a Boolean function where each literal is a field in the combination [4]. For each combination and each record, we apply a one-way hash function, such as a checksum, using the values of each field in the combination. A one-way bash function provides a unique number that represents all data values as a single entity. Because the results are numeric, they provide a simplified representation that can be easily compared, examined, and manipulated. For example, let rj=(AGE=34, RACFkcaucasian, SEX=male) be the ith record. Let b,=101 he a Boolean function for tbe jtb combination of fields. Bj=lOl selects the fields AGE and SEX, but not RACE. Then we compute crcg=f(AGE=34, SEX=male) using the checksum function f on the field values of ri specified by bj.

The set of Boolean functions that select field combinations has a hierarchy that can be depicted in a Hasse diagram [5].

The Hasse diagram corresponding to the above example is shown in figure I .

RACE, SEX  AGE, RACE, SEX - Figure 1. Hasse diagram for the quasi-identifier  { AGE,RACE,SEX)   mailto:awheeler@3sigmaResearch.com   KIMAS 2003 BOSTON, USA  We represent each record by 2"-1 tuples of the form (hi, j). Since all values may be expressed as integers, we  use them as hash values for pattern searching.

3. UNIQUENSS We measure the novelty or interestingness of database records by the uniqueness of combinations of their field values. We consider the k-uniqueness of a record. A record is k-unique for a combination of field values when the combination matches k or fewer records in the database.

This definition of k-uniqueness is taken from the notion of k-anonymity described in [6]. Records that are k-unique may be of interest for many reasons. In our work on data privacy, it is important to find k-unique records because these records are at-risk for re-identification, and may result in the compromise of personal information privacy.

The measure is computed using Boolean functions where each literal corresponds to a record field. W e  compute a checksum using the values of a record specified by the literals in the Boolean function. An implicant is a Boolean function for which the record is k-unique. A prime implicant is an implicant that is no longer an implicant if any literal is removed [41.

A prime implicant represents the minimum combination of fields needed to make a record k-unique. If a particular combination of fields makes a record unique, then any combination containing this combination is also unique [ 5 ] .

By grouping k-unique records according to their prime implicants, we can easily identify which records are at risk and which field and field values contribute to  the at risk status.

4. ALGORITHM Figure 2 shows the steps required to identify k-unique records using the novelty measures we have described.

The first step is to compute the checksums crci, on each record j for each combination bi of fields. This can be done efficiently by using a hash table with the checksum as the key, and the list of records generating that checksum as the value. We have one such hash table for each Boolean function bi. To save space, we can replace the list of records with an integer count of matching records when the number of matching records exceeds k.

The next step is to find all of the combinations of fields bi that make each record k-unique. This is done by finding every record list having k or fewer records. Results are stored in a hash table with the record index j as the key and the list of implicants bi as the value.

The third step is to find the set of prime implicants for each k-unique record by removing non-prime implicant Boolean functions from the list of implicants for each record.

Finally, compute the fewest combinations of fields, uniqueness of the combination of fields, and the number of prime implicants for each record. Sort each record on each of these measures.

1.Compute (b,,crc,,, j) for each i and  2.Find the set of implicants [b%), f o r  each record j with Icrc2,1<=k.

3.Compute the prime implicants for each record j.

4.Compute novelty measures on each record using the prime implicants and sort records on each measure.

j.

Figure 2. Algorithm to find all k-unique records.

5. ALGOFUTHM ANALYSIS In Step 1 of the algorithm, for all records rj, 1 e j <= N, we must compute the checksum for each combination of fields and values. There are 2"-1 combinations for m fields. This gives a worst case space and time of O(2"N).

Step 2 also has time complexity of O(2"N). The order comes from the worst case where each record generates a unique checksum for each combination of fields. Space complexity is O(2'"N) because there are 2m-1 implicants for each record since each implicant generated a unique checksum.

The space complexity,of step 3 just reduces the number of implicants in step 2 so we can place an upper hound on the complexity of O(2"'N). A naive search for prime implicants compares each implicant against each other, resulting in 22"-2mt'+l comparisons for each record, and a worst-case time complexity of O(2'"N).

Step 4 of the algorithm has linear space complexity to store the measures for each record. If we assume an in-place sorting algorithm like QuickSon, then the space complexity is O(N). It takes linear time to compute the novelty measures for each record but, with sorting, time complexity is O(2").

Combining results from steps 1-4, we find that the algorithm has a worst-case space complexity of  d2")=OLZmN)f 4(2"'N)f 03(2")f O,( N)  and a worst-case time complexity of  Despite the poor theoretical space-time complexity of the algorithm, we expect the actual space-time complexity to be much better. We make this claim on the observation that records in most databases exhibit considerable similarity.

KlMAS 2003 BOSTON. USA  Data sets have records with many field values in common.

Values of fields do not always take all possible values, and are not evenly distributed across the range of all possible values. All possible field values and all possible combinations of values may not exist in the data.

Furthermore, not all available fields need to be used for finding k-unique records. It may be that only a small subset of fields is of interest for k-uniqueness. The result we expect in most situations is that the number of unique checksums is much less than N*(2"-1) for N records with m fields. It has been our experience that these are valid assumptions for medical data.

6. EXAMPLE Given an example data set, we compute the uniqueness measures for each record and show the records that are identified as most unique by each of these measures.

Table 1 contains data we use for our example. Twelve individuals are described by five fields: organization identifier (OID), SEX, RACE, TYPE, and ZIP CODE. We apply the techniques discussed previously to identify unique groups of records in the data. We assume that k=l so that a  record is unique on a combination of fields if it is the only record that has that combination of values.

Table 2 shows that three records (3, 7, 11) have unique combinations of values for the aid and sex fields (indicated by bold character values). In Table 3, records 3, 8, and 11 are unique on the OID and RACE fields. We can group two records (3, 11) on their k-uniqueness over the three fields RACE, TYPE and ZIP (Table 4). In Table 5, we see the grouping of records 3 and 12 by their k-uniqueness on SEX, RACE, and ZIP values. Records 3 and 11 also form a group of k-unique records on the OID and TYPE fields (Table 6).

Table 7 summarizes the results of the grouping. Records 7, 8, and 12 form k-unique groups of size one for a number of field combinations, meaning that no other record is unique on that combination of field values. For example record 8 is the only record with unique values for (RACE, ZIP),(TYPE,ZIP),(SEX,ZIP),(OID,ZIP). From this table we also observe that records 3 and 8 belong to the most k- unique groups (five), followed by record 11 with four groups, and records 7 and 12 with three groups  Table 1 .  Sample data.

Table 2. Records grouped by organization and sex.

KlMAS 2003 BOSTON, USA  Table 3. Records grouped by organization and race.

id 0.2 sex race 1 1 m Caucasian 71 1 m Caucasian  3 2 f Caucasian 4 3 f african 5 2 f 2frirsn  ____________ _______  -  Table 4. Records grouped by race, other, and zip.

type zip 32 122 1  1 32122 32110 1  1 32110 1 w i n  Table 5. Records grouped by sex, race, and zip.

- I 6 3 I 3 8 3 9 1 10 1 11 2 12 4  ~  .__..

f &Can 1 32110 m african 1 321 I O  321 11 f Caucasian I  m -  african 2 321 11 - m african 2 32110  m Caucasian 2 32110  _______ m african 2 321 1 1     KlMAS 2003 BOSTON, USA  Table 6. Records grouped by organization and other  Table 7. Records clustered by unique group

I. CONCLUSIONS We have presented a method for finding groups of unique records in data. The groups are defined by the combinations of fields that make the records unique in the data. This method of information discovery in data is being applied to identifying anonymous records in public use microdata that are at-risk for re-identification.

These techniques allow the rapid identification of unique records in data sets without a priori knowledge of field values or field-value ranges. These techniques can also be used to identify groups of records ( k  >I)  that have the same characteristics. This results in the formulation and creation of new knowledge about the data, based on information that is not evident and is not otherwise easily obtained. The same technique can be applied to other areas.

8. REFERENCES [l] D. Lambert. Measures of Disclosure Risk and Harm.

Journal of O p a l  Statistics, 9(2):313-332, 1993.

http://citeseer.nj .nec.com/25496.html [2] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. Proceedings of the 20th Intemational Conference on Very Large Data Bases, Morgan Kaufmann,  1994. http://citeseer.nj.ne.com/article/agrawal94fast.html [3] R.J. Hilderman and H.J. Hamilton. Knowledge discovery and interestingness measures: a survey.

Technical Report CS 99-04, University of Regina, October 2000.

http://citeseer.nj .nec.com/hilderman99knowledge.html [4] A. Ohm and L. Ohno-Machado. Using boolean reasoning to anonymize databases. Arfificial Intelligence in Medicine, 15(3):235-254, 1999.

http://citeseer.nj.nec.com/6 1688.html [5] A. Takemura. Minimum unsafe and maximum safe sets of variables for disclosure risk assessment of individual records in a microdata set. Journal of Japan Statistical Society, 32107-1 17,2002.

http://citeseer.nj.nec.com/4264lO.html [6] P. Samarati and L. Sweeney. Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. Unpublished, 1998. http://citeseer.nj.nec.com/samarati98protecting.html  Notice: %is material is based upon work supported by the Nodonnl Science Folrndotion under Grant No. (023I 783). Any opinions,findings, and conclusions or recommendations upressed in this material arc those of the author($) ond do not necesssody reflect the vicwr of the Notiom1 Science Foundnfion.

