Association Rule Mining using a Multi-Objective Grammar-Based Ant

Abstract?This paper presents a method for extracting as- sociation rules by means of a multi-objective grammar guided ant programming algorithm. Solution construction is guided by a context-free grammar specifically suited for association rule mining, which defines the search space of all possible expressions or programs. Evaluation of individuals is consid- ered from a Pareto-based point of view, measuring support and confidence of rules mined, and assigning them a ranking fitness.

The proposed algorithm is verified over 10 varied data sets and compared to other association rule mining algorithms from several paradigms such as exhaustive search, genetic algorithms and genetic programming, showing that ant programming is a good technique at addressing the association task of data mining as well.

Keywords-Association rule mining (ARM), data mining (DM), ant colony optimization (ACO), ant programming (AP)

I. INTRODUCTION  One of the most important techniques for the extraction  of close relations between patterns is the association rule  mining (ARM). Each relation mined is known as an asso-  ciation rule, referred to as A ? C, where A stands for the antecedent and C for the consequent. Both A and C are sets of conditions fulfilling the requirement of not having any  attribute in common. The meaning of an association rule  is that if the antecedent is satisfied, then there is a high  likelihood of satisfying also the consequent.

The first algorithm proposed for mining association rules  was Apriori [1], which works in two phases. First, it finds  all items whose frequency of occurrence is high, satisfying  a minimum frequency threshold. Second, it obtains reliable  rules basing on another metric called confidence. Another  important approach, based on Apriori, is the FP-Growth  algorithm [2], which is based on a frequent pattern tree  structure, minimizing the computational time. Nevertheless,  this approach may produce an increment of the memory  requirements with the size growth of the data set to be  explored.

Both algorithms fall into the category of exhaustive search  approaches. Therefore, when the search space is too large to  use deterministic search methods, other kind of methods are  preferred, such as evolutionary algorithms [3], [4] or particle  swarm optimization [5], which have demonstrated to obtain  good results in such situations.

An example of evolutionary algorithm for the associa-  tion task is ARMGA (Association Rule Mining Genetic  Algorithm) [4]. This algorithm encodes each individual as  an association rule in a single chromosome. Each gene  represents an item from the dataset and the first one indicates  the separation between items of the antecedent and the  consequent. As any evolutionary approach, this algorithm  uses genetic operators to obtain new individuals, evaluating  them by means of the relative confidence measure, which  establishes the degree of relationship between the antecedent  and the consequent.

Another natural computation metaheuristic is ant colony  optimization (ACO) [6], which is inspired in the behavior  and self-organization capabilities of ant colonies in nature.

ACO has demonstrated its success when dealing with clas-  sification rule mining problems [7], [8], and recently it  has been also applied to association tasks under multi-  dimensional constraints [9]. However, its application to  association rule mining is still a very unexplored area.

A variation of ACO algorithms is ant programming  (AP) [10], [11], [12], which enhances the powerful ability  of automatic programming for building computer programs  using ACO as search technique. The best-known type of  automatic programming is genetic programming, presented  by Koza [13]. A very recent ARM proposal based on this  metaheuristic is the G3PARM (Grammar Guided Genetic  Programming Association Rule Mining) algorithm [14],  which has demonstrated to obtain reliable and very frequent  rules, also covering a high percentage of instances of the data  set. In G3PARM, each individual represents an association  rule and each one is evaluated by using a fitness function  based on the frequency of ocurrence of the rule represented  by the genotype. This proposal allows to restrict the search  space by means of a grammar and it uses a pool of individ-  uals to keep the best ones obtained during the evolutionary  process.

ARM can also be visualized as a multi-objective problem,  considering several measures as objectives to be optimized.

Multi-objective optimization has become a popular research     topic in the last years, and limited works, such as Ishibuchi?s  et al. [15], have explored the use of multi-objective algo-  rithms for extracting association rules.

This paper addresses the extraction of association rules  applying AP, which has recently demonstrated to be a very  promising technique for addressing another data mining task  such as classification [16]. The algorithm proposed in this  paper follows a multi-objective approach, looking for the  simultaneous acquisition of very frequent and reliable rules.

It is also based on the use of a context-free grammar (CFG)  that limits the search space and ensures the creation of valid  individuals. Its effectiveness has been tested over several  data sets and compared to other well-known algorithms,  proving that AP is a good technique for ARM and also that  this task can be successfully tackled from a multi-objective  perspective.

This paper is arranged as follows. Section II describes  the proposed algorithm. Section III presents the data sets  used in the experiments and the experimental set-up. In  Section IV, the results obtained are shown and analyzed.

Finally, concluding remarks are made in Section V.



II. THE PROPOSED MULTI-OBJECTIVE AP MODEL FOR  ARM  This section introduces the fundamentals of the developed  AP algorithm for mining association rules, presenting its  particular features and a pseudocode of the model.

A. Environment and individual encoding  The algorithm proposed is founded on the use of a CFG  which defines all the possible states that individuals can visit.

This grammar plays the role of the set of terminals and  functions in automatic programming, restricting the search  space and ensuring any solution to be valid. The CFG is  expressed in Backus-Naur form notation and its definition  is given by G = (?N , ?T , P , S), as shown in Figure 1.

?N represents the set of non-terminal symbols, ?T is the set of terminal symbols, P is the set of production rules and S stands for the start symbol.

The environment defined by the grammar, which permits  ants to communicate indirectly with each other, adopts the  shape of a derivation tree. From the initial state, ants follow  a path over the environment until reaching a final state or  solution. Each individual stores the path followed, fulfilling  the properties of an artificial ant [17]. Therefore, each  individual encodes an association rule, and the last state of  the path directly corresponds to the evaluatable expression  of this rule, expressed in prefix notation.

The treatment of the environment generation is also  important. The algorithm follows an incremental build ap-  proach in which, as ants are created, visited states are  stored in a data structure. This way, there is no need of  keeping in memory the whole space of states, storing just the  necessary states, which is translated into less computational  requirements.

G = (?N, ?T , P, S)  ?N = {<Rule>, <Antecedent>, <Consequent>,  <Condition>}  ?T = {-->, AND, =, !=  attr1, attr2, ..., attrn,  value1,1, value1,2?, ..., value1,m,  value2,1, value2,2, ..., value2,m,  ..., valuen,1, valuen,2, ..., valuen,m}  S = {<Rule>}  P = {<Rule> := --> <Antecedent> <Consequent>,  <Antecedent> := <Condition> |  <Antecedent> <Condition>,  <Consequent> := <Condition>,  <Condition> := all possible valid  combinations of the ternary  operator attr value}  Figure 1. Context-free grammar  B. Multi-objective evaluation  The quality of individuals generated is assessed on the  basis of two objectives: support and confidence. As men-  tioned in the previous section, the last state of the path of a  given ant directly encodes the expression of the association  rule. The first step in the evaluation of an association rule is  to check whether this expression is consistent with a valid  rule, i.e., if there are no coincidences between attributes  in the antecedent and the consequent of the rule. If there  is a match, the individual is discarded. Once the previous  condition has been verified, support and confidence metrics  are calculated.

Support and confidence are two measures widely em-  ployed in ARM. The support of an association rule A ? C indicates the proportion of the number of transactions T including both the antecedent and the consequent in the data  set D, as shown in Equation 1.

sup(A ? C) = |{A ? C ? T, T ? D}|  |D| (1)  Thus, support involves frequency of appearing patterns.

In turn, confidence (Equation 2) represents the proportion  of the number of transactions including both the antecedent  and the consequent among that transactions in the data set  that include the antecedent. It is a measure of the strength  of implication of an association rule.

conf(A ? C) = sup(A ? C)  sup(A) =  |{A ? C ? T, T ? D}|  |{A ? T, T ? D}| (2)  Pareto dominance asserts that a given individual ant1 dominates another individual ant2, denoted as ant1 ? ant2, if ant1 is not worse than ant2 on all the objectives and better in at least one of them. The non-dominated set of solutions  of a population makes up the Pareto front.

By the time all individuals have been evaluated regarding  support and confidence, a fitness value is calculated for  each one using ranks, organizing the population in fronts  of non-dominated individuals, in a similar procedure to that  followed in the NSGA2 algorithm [18]. The fitness assigned  to a given individual corresponds to the inverse value of  the front to which it belongs. This way, individuals of the  Pareto front will receive a fitness value of 1, individuals  of the second front a value of 1/2, and so on. After each  individual in the population has been evaluated and has  a fitness value associated, reinforcement and evaporation  processes take place.

C. Heuristic measure  The AP algorithm proposed considers a heuristic function  with two complementary components. The first one is a  measure related to the cardinality of the production rules  involved in the transition, based on the cardinality measure  proposed by [19]. It is referred to as Pcard, and it increments the probability of choosing transitions that let ants to reach  more candidate solutions. This metric is only used in case  of intermediate transitions, which are those that do not  imply the selection of attributes of the problem domain.

This is because final transitions embrace the same number  of solutions, independently of the movement selected by the  ant. The value of Pcard suppossing a transition between a source state i and a destination state j, where there are k possible subsequent states, is computed as follows:  Pcardk ij =  cardinality(statej , d? 1)  ?k?allowed(cardinality(statek, d? 1)) (3)  where d is the number of derivations that remain available at the depth of the source state i. As can be observed, it is equal to the ratio between the number of candidate solutions  that can be successfully reached from the state j in d ? 1 derivations and the sum of all possible candidate solutions  that can be reached from the source state i in d derivations.

In case of final transitions, the metric used is the support  of the condition encoded by each destination state. The  support metric is computed for each possible combination  of operator-attribute-value before starting up the grammar.

Notice that, according to the grammar defined in Figure 1,  conditions involving the operators = and != are allowed.

The operator != favors to obtain a higher support in domains  where there are itemsets that are not very frequent.

D. Transition probability  Each step forward taken by a given ant during its solution  construction obeys a probability rule. This rule takes into  account both the pheromones spread over the environment  by ants of previous generations and the heuristic function ex-  plained before. It is called transition rule, and the probability  that a given ant located at the state i at some point moves to another state j is computed as shown in Equation 4:  P kij = (?ij)  ? ? (?ij) ?  ?k?allowed(?ik)? ? (?ik)? (4)  where k is the number of valid subsequent states, ? is the heuristic exponent, ? is the pheromone exponent, ? is the value of the heuristic function, computed as sup(Condition) + Pcard (having at least one of the two components equal to zero), and ? indicates the strength of the pheromone trail.

When computing the transition rule, the algorithm con-  trols that the movement to the state j allows to reach final states in the number of derivations that remain available at  that point. If not, the algorithm assigns a probability of zero  to this state and, therefore, it will never be chosen.

E. Pheromone evaporation and reinforcement  Two categories can be differentiated depending on how  pheromone information is treated by multi-objective ant-  based algorithms: algorithms that use a single pheromone  matrix, or those that employ a pheromone matrix per ob-  jective to be optimized. Our algorithm falls into the first  group, which involves a benefit with respect to memory and  computational time requirements.

Concerning the pheromone operations existent, two op-  erations are considered, reinforcement and evaporation. Re-  garding the former, all ants in the current population deposit  pheromones in their path according to Equation 5, propor-  tionally to its raking fitness:  ?ij(t+ 1) = ?ij(t) ? rankFitness (5)  where ?ij(t) indicates the existing quantity of pheromone in the transition from state i to state j, ?ij(t + 1) states for the new amount of pheromones that will be in the same  transition after the pheromone deposition, and rankFitness is the fitness ranking assigned to each individual according  to Pareto dominance, as explained in Section II-B. All  transitions in the path of a given individual are reinforced  equally.

The evaporation takes place over the whole space of  states. For a given transition, the amount of pheromone  after performing the evaporation is computed as shown in  Equation 6:  ?ij(t+ 1) = ?ij(t) ? (1? ?) (6)  where ? represents the evaporation rate.

Finally, a normalization process follows the other  pheromone maintenance operations, in order to limit  the pheromone levels in each transition to the range  [?min, ?max].

Algorithm 1 Pseudocode of the AP algorithm for ARM  Require: numGenerations, numAnts,maxDerivations, minSupport,maxRules  1: Initialize grammar and space of states  2: List ants ? {} 3: for i ? 0 to i < numGenerations inc 1 do 4: for j ? 0 to j < numAnts inc 1 do 5: ant ? Create new ant 6: Evaluate ant by objective functions, checking that  the support is greater than minSupport 7: Store ant?s path states in the space of states  8: Add ant to the list ants  9: end for  10: for each ant in ants do  11: rankFitness ? Assign a ranking fitness according to Pareto dominance  12: end for  13: List fronts ? {} 14: for each ant in ants do  15: if rankFitness ? 0.1 then 16: Add ant to fronts  17: end if  18: Update pheromone rate in the path followed by ant  proportionally to its rankFitness  19: end for  20: Evaporate pheromones along the whole space of states  21: Normalize pheromone values  22: ants ? fronts 23: end for  24: Sort ants by rankFitness and then by support 25: List associationRules ? {} 26: i ? 0 27: while i < size of ants and  size of associationRules ? maxRules do 28: if associationRules does not contain equivalent  rules to individual i in ants then 29: Add individual i in ants to associationRules 30: end if  31: i ? i+ 1 32: end while  33: return associationRules  F. Algorithm  The main pseudocode of the algorithm is diplayed in  Algorithm 1. It begins initializating the CFG and creating  an empty space of states, just containing the initial state.

It also creates an empty list of individuals: ants. The AP algorithm loops through the following processes until  performing numGenerations generations.

First, it creates the number of ants specified as parameter.

As they are created, each ant is evaluated by the objec-  tive functions (support and confidence), ensuring that its  support exceeds the threshold established by the parameter  minSupport (if not, a value of zero is assigned to both measures). The states visited by each ant are kept in the  data structure that represents the space of states, and each  individual is added to the list ants.

Then, all individuals contained in the list ants are ranked  according to Pareto dominance, so that individuals belonging  to front N receive a rankFitness of 1/N .

A list of individuals called fronts is initialized, and all  ants up to the tenth front and, therefore, whose rankFitness is greater or equal to 0.1, are added to the list fronts. This is because we consider the possibility that the user may  desire to obtain a greater number of association rules, even  if they do not belong to the Pareto front. Hence, not only  the solutions corresponding to the first front are maintained,  but also good solutions belonging to other fronts, ordered  by their ranking fitness.

All individuals reinforce the amount of pheromone in their  path in proportion to their rankFitness, each transition perceiving an equal amount of pheromones.

To conclude a generation, evaporation and normalization  processes takes place, and the list ants is replaced with the individuals contained in the list fronts.

After carrying out the numGenerations generations, for the sake of selecting an appropriate set of association  rules as result of the algorithm, the individuals contained  in the list ants are sorted in descending order by their rankFitness, and then by their support. Afterwards, start- ing at the first individual, the algorithm adds it to the list  associationRules, only if this set of rules does contain any other rule equivalent encoded by another individual inserted  previously, and if the maximum number of individuals that  this list can contain has not been exceeded. The maximum  number of individuals is given by the maxRules parameter.

Finally, the list associationRules is returned.



III. EXPERIMENTATION  This section presents the data sets and the algorithms  employed in the experimental study, with the parameter  configuration used.

A. Data sets and preprocessing  To analyze the effectiveness of the AP algorithm, ten  varied data sets from the well-known machine learning  repository of the university of California at Irvine (UCI)1  were considered.

Two preprocessing steps were carried out before executing  the algorithms, in order to do a fair comparison. The  first one involved the replacement of missing values with  the mode or the arithmetic mean, assuming nominal and  numeric attributes, respectively. The second one entailed  1UCI data set repository is available at http://www.ics.uci.edu/ml/datasets.html     the discretization of such data sets comprising numeric at-  tributes. The algorithm used for this purpose was the Fayyad  and Irani?s discretization algorithm [20]. Both preprocessing  actions were perfomed by using Weka2 [21].

B. Algorithms and experimental set-up  Our AP algorithm is evaluated against other ARM algo-  rithms, belonging to several paradigms: a grammar-guided  genetic programming algorithm, G3PARM3; a genetic al-  gorithm, ARMGA4; and two exhaustive search algorithms,  Apriori5 and FP-Growth6.

The parameter configuration for the AP algorithm was  adopted after performing several tests using different rank  values for each parameter. These executions were done over  several representative data sets, and then analyzing which  specific set-up globally yielded the best results. It is worth  mentioning that no single combination of parameter values  performed better for all data sets, as expected. Nevertheless,  notice that this adopted configuration should be tuned for  each particular data set.

This configuration corresponds to a population size of 20  ants, 100 generations, a maximum number of 10 derivations,  an initial amount of pheromone and a maximum amount of  pheromone of 1.0, a minimum amount of pheromone equal  to 0.1, an evaporation rate of 0.05, a value of 0.4 for the  alpha exponent, a value of 1.0 for the beta exponent, a 70%  of support threshold, and a maximum size for the set or rules  returned of 20. Notice that this algorithm does not require  a confidence threshold.

For the G3PARM algorithm, its optimal parameters were  used: a population size of 50 individuals, 100 generations,  probabilities of 70% and 14% for crossover and mutation  operators, respectively, a maximum derivation number of  24, an external population of size 20, a 90% of confidence  threshold and a 70% of support threshold.

In case of the ARMGA algorithm, its optimal parameters  are: a population size of 50 individuals, 100 generations,  100% selection probability, 90% of crossover and 1% of  mutation probabilities, a maximum rule length of 4 attributes  and a 90% of confidence threshold. ARMGA does not  employ support threshold.

For Apriori and FP-Growth, in order to perform a fair  comparison, same support and confidence thresholds used  in the other algorithms were considered.

2The Weka machine learning software is publicly available at http://www.cs.waikato.ac.nz/ml/index.html  3G3PARM algorithm was executed using the implementation provided by the authors  4ARMGA algorithm was run using the implementation available in the software KEEL, that can be reached at http://www.keel.es  5Apriori algorithm was run using the implementation accessible in Weka 6FP-Growth algorithm was executed employing the im-  plementation by F. Coenen (2003), university of Liverpool, http://www.csc.liv.ac.uk/ frans/KDD/Software/FPgrowth/ fpGrowth.html

IV. RESULTS  Experiments compare the average results of support  (Table I), confidence (Table II) and coverage (Table IV)  achieved by each algorithm over the data sets considered,  showing also the average number of rules returned (Ta-  ble III). It is worth noting that stochastic algorithms have  been run ten times with different seeds, in order to obtain  unbiased results. In contrast, Apriori and FP-Growth, which  are deterministic, were executed once.

The results obtained by every algorithm are summarized  in a separate table per each metric considered, where a single  row displays the result obtained per algorithm for a given  data set. Bold type values indicate the algorithm that attains  the best result for a particular data set. At a glance, it can  be observed that the AP algorithm achieves the best support  results in a 90% of the problems, and also overcomes the  other algorithms regarding coverage in a 90% of the cases.

Likewise, ARMGA obtains the best results with respect to  confidence metric for all the data sets.

On the other hand, Apriori and FP-Growth, are exhaustive  search algorithms, so that they find all the possible relations  that meet the support and confidence restrictions. Never-  theless, they are not able to find any association rule for  wine, vehicle and nursery data sets (indicated with n/a, not  available). This is because both algorithms only consider the  equal operator in conditions. In contrast, G3PARM and AP  also use the not equal operator, which allows to find high  support results in data sets having unfrequent itemsets. In  the case of ARMGA, it is capable of discovering rules in  the mentioned problems because it has no support threshold.

Apriori and FP-Growth algorithms return a large set of  association rules, as they perform an exhaustive search.

In contrast, nature inspired methods as ARMGA, AP and  G3PARM extract a reduced set of high quality rules, thus  denoting more interesting patterns. For instance, ARMGA  mines 100 rules having the maximum average confidence  and AP mines 20 association rules with very high average  support.

Looking at the instances covering results, AP and  G3PARM achieve best covering of the data sets, although  the number of rules is limited by the external population  size. Thus, both algorithms are capable of mining more  representative association rules.

To analyze statistically these results, Friedman?s method  was applied. This test computes the average rankings ob-  tained by k algorithms over N data sets regarding one metric. Table V sums up the average ranking values of each  algorithm for the three metrics studied. The computed value  for the Friedman?s statistic for average support distributed  according to the F-distribution with k?1 and (k?1)(N?1) degrees of freedom is 37.5116; for the confidence metric,  its value is 42.8732; and 8.0777 for coverage. None of them  belongs to the critical interval at the ? = 0.01 significance     Table I SUPPORT AVERAGE RESULTS  DATA SET AP G3PARM ARMGA APRIORI FP-GROWTH  Lymphography 0.9598 0.9301 0.1643 0.7605 0.7605 Wine 0.8456 0.8162 0.1675 n/a n/a Sonar 0.7965 0.7952 0.2142 0.7501 0.7501 Primary-tumor 0.9659 0.8435 0.0354 0.7568 0.7568 Soybean 0.9611 0.9198 0.0892 0.7301 0.7368 Vehicle 0.9448 0.9335 0.0090 n/a n/a Vowel 0.9272 0.8910 0.0347 0.7881 0.7881 Contraceptive 0.8758 0.8434 0.0075 0.7821 0.7821 Segment 0.9733 0.9754 0.0091 0.8828 0.8828 Nursery 0.7784 0.7606 0.0038 n/a n/a  Average 0.9028 0.8709 0.0735 0.5450 0.5457  Table II AVERAGE CONFIDENCE RESULTS  DATA SET AP G3PARM ARMGA APRIORI FP-GROWTH  Lymphography 0.9947 0.9989 1.0000 0.9722 0.9722 Wine 0.9522 0.9645 1.0000 n/a n/a Sonar 0.9477 0.9431 1.0000 0.9432 0.9432 Primary-tumor 0.9937 0.9964 1.0000 0.9420 0.9420 Soybean 0.9912 0.9990 1.0000 0.8856 0.9456 Vehicle 0.9959 0.9998 1.0000 n/a n/a Vowel 0.9869 0.9915 1.0000 0.9535 0.9535 Contraceptive 0.9766 0.9703 1.0000 0.9193 0.9193 Segment 0.9964 0.9955 1.0000 0.9773 0.9773 Nursery 0.9392 0.9921 1.0000 n/a n/a  Average 0.9775 0.9851 1.0000 0.6593 0.6653  Table III AVERAGE NUMBER OF RULES  DATA SET AP G3PARM ARMGA APRIORI FP-GROWTH  Lymphography 20.0 20.0 50.0 104 104 Wine 20.0 20.0 50.0 n/a n/a Sonar 20.0 14.1 50.0 98 98 Primary-tumor 20.0 20.0 50.0 209 209 Soybean 20.0 20.0 50.0 112650 47304 Vehicle 20.0 20.0 50.0 n/a n/a Vowel 20.0 20.0 50.0 5 5 Contraceptive 20.0 20.0 50.0 1 1 Segment 20.0 20.0 50.0 7 7 Nursery 20.0 20.0 50.0 n/a n/a  Average 20.0 19.4 50.0 11307.4 4772.8  Table IV AVERAGE COVERAGE RESULTS  DATA SET AP G3PARM ARMGA APRIORI FP-GROWTH  Lymphography 100.00 99.59 83.11 98.65 98.65 Wine 99.94 99.74 78.65 n/a n/a Sonar 97.69 96.68 71.15 98.07 98.07 Primary-tumor 100.00 99.99 22.86 100.00 100.00 Soybean 100.00 100.00 72.88 100.00 100.00 Vehicle 100.00 98.56 10.17 n/a n/a Vowel 100.00 100.00 43.29 98.89 98.89 Contraceptive 99.80 99.76 21.86 78.21 78.21 Segment 100.00 100.00 25.97 99.61 99.61 Nursery 100.00 97.81 4.49 n/a n/a  Average 99.74 99.01 43.44 67.34 67.34  level, C0 = [0, (FF )0.01,4,36 = 3.8903]. Thus, the null- hypothesis set out by the Friedman test that all algorithms  perform equally well can be rejected for the three metrics  considered. Therefore, to reveal the performance differences  it is necessary to proceed with a post-hoc test. Since all  classifiers are compared to a control one, it is possible to  perform the Bonferroni-Dunn test [22].

The Bonferroni-Dunn?s critical difference considering the  same significance level of ? = 0.01 is equal to 2.1383.

At this significance level, AP-ARM obtains statistically  better support results than ARMGA, Apriori and FP-Growth  algorithms.

Regarding the confidence measure, the algorithm with  the lowest ranking is ARMGA, which does not find sig-  nificant differences with AP-ARM, even if we consider a  less restrictive level of significance, ? = 0.05 (where the critical difference is 1.7663). In contrast, AP-ARM does  obtain significant differences with respect to Apriori and FP-  Growth when ? = 0.1 (the critical difference is 1.5846 at this level).

Finally, focusing on the covering ability of the algorithms  considered, AP-ARM is significantly better than ARMGA  with a 99% of probability, and so it is for a 95% of likelihood  with respect to Apriori and FP-Growth.

Table V AVERAGE RANKINGS OF THE ALGORITHMS  ALGORITHM RANKING Support Confidence Coverage  AP 1.10 2.70 1.55 G3PARM 1.90 2.50 2.35 ARMGA 4.39 1.00 4.39 Apriori 3.85 4.45 3.35  FP-Growth 3.75 4.35 3.35

V. CONCLUSION  In this article we have explored the application of AP for  discovering association rules with high support, confidence  and coverage. We have proposed a multi-objective AP al-  gorithm guided by grammar, carrying out experiments over  several varied data sets and comparing its results with those  obtained by other ARM algorithms. The main features of our  proposal are the use of a CFG that restricts the search space,  the use of a two-sided heuristic function, and the use of a  multi-objective approach to evaluate generated individuals  according to support and confidence measures.

The results obtained in the experimental study show that  AP is a promising technique to address the ARM task.

The AP algorithm is, with a likelihood of 99%, signifi-  cantly better regarding support than ARMGA, Apriori and  FP-Growth algorithms. Concerning the confidence of the  rules mined by the AP algorithm, the ARMGA algorithm,  which focuses on obtaining the highest confident rules at  the expense of obtaining lower support values, does not  present significant differences with our proposal, which also  obtains high confidence results. Moreover, our algorithm is  significantly more confident than Apriori and FP-Growth     with a probability of 90%. Finally, regarding the covering  of the data set by the set of association rules extracted,  the AP model obtains significant differences with respect  to ARMGA with a 99% of probability, and with exhaustive  methods with a 95% of likelihood.

Future work lies in studying the employment of other  heuristic metrics and other pheromone updating strategies  to improve the performance of the algorithm. On the other  hand, it would be interesting to extend the multi-objective  approach considering more objectives to be optimized.

