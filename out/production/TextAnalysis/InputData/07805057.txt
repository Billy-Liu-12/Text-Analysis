A novel Approach for mining frequent itemsets:

Abstract? The step of mining frequent itemsets in database is  the essential step and most expensive in the process of mining  association rules in data mining task, many algorithms of mining  frequent itemsets have been proposed to improve the  performance of Apriori Algorithm. In this paper, we have  introduced an optimization in the phase of generation pruning of  candidates by a new strategy for the calculation of frequent  itemsets based on approximate values of supports exact the  itemsets. We have evaluated our algorithm AprioriMin against  three popular frequent itemsets mining algorithms - Apriori and  FP-growth, Close using two data sets with a variety of minimum  support.

Keywords? Frequent itemsets; Support; Pruning of  candidates; Performance

I.  INTRODUCTION  Association rules mining is one of the most important and well research-conducted techniques of Data Mining, with goal of discovering the relevant knowledge. Association rules are used to identify relationships among a set of items in database trying to find interesting itemsets. An Itemset is frequent if the subsets in a collection of sets of items occur frequently. The step of mining frequent itemsets in database is the essential step and most expensive in the process of mining association rules; then the researchers concentrated in the generation of frequent itemsets which represent the main cost of treatment.

Apriori Algorithm is the most classical algorithm for mining frequent itemsets proposed by Agrwal [1] verifying if an itemsets is frequent. Any superset containing this itemsets can be frequent; but the most criticized drawback of this algorithm is scanning the databases more times frequently.

Many variant algorithms are proposed to improve Apriori, including DIC [2], Partition [3], and Sapling [4] .These algorithms have different techniques and is used for mining frequent itemsets. During the mining of frequent itemsets with Apriori, much frequent itemsets have the same support. This observation that is exploited by the Algorithm Close [5] which is based on the pruning of the closed itemsets lattice in order to find frequent itemsets. The FP-growth [6] (frequent pattern growth) was also the improvement of Apriori Algorithm which was employed to mine the complete set of frequent patterns by pattern fragment growth without using generation of candidate item sets.

Research works in the area of developing algorithms for mining frequent itemsets are several; the goal of which being to find interesting patterns from datasets. We can mention some works: Bhandari et al [7] have indicated some weaknesses of Apriori Algorithm. The main limitation is wasting time in holding vast number of candidate sets with much frequent itemsets and space for scanning the whole database. Predrag and Savo [8] have suggested a new procedure for large itemsets generation which is a modification of the Apriori Algorithm. To implement it, they have used the structural pattern composite of a candidate hash tree. The authors in [9] have provided an efficient analysis of frequent pattern mining by comprehensive survey of some algorithms, which exist in literature for the mining of frequent patterns. Additionally they have described a performance analysis of the approaches and algorithms for mining frequent pattern. The researchers in [10] have proposed an adaptive implementation case study of Apriori Algorithm for a retail scenario in a cloud environment. It aims at resolving the time- consuming problem for retail transactional databases by using the approach of mining the frequent itemsets.

The algorithms of frequent itemsets proceed iteratively by level in each iteration. A list of candidates? itemsets of size K is built and pruned later. In this paper, we present a novel approach for mining frequent itemsets based on approximate values of exact supports of the itemsets .We call the new algorithm the AprioriMin Algorithm ; it's an improvement of Apriori Algorithm in the phase of generation pruning of the candidates.

The rest of this paper is organized as follows: Section 2 presents our approach of mining frequent itemsets. The methodology of the proposed work is given in section 3.The empirical evaluations of our algorithm are described in section 4.Finally we will finish by a conclusion.



II. APPROACH OF FREQUENT ITEMSETS  The problem of mining frequent itemsets relies on global measures such as Support and requires an access to data- processing. Many algorithms of mining frequent itemsets have been proposed in the literature .These algorithms scan all itemsets iteratively and by levels at each level k; a set of candidate Itemsets of size k is generated by joining frequent Itemsets discovered during the previous iteration. However, this step is very expensive due to the exponential size of the     search space and the large number of scans of the entire database. This scan is necessary to estimate the supports of itemsets.

The principal basis of algorithms of mining frequent itemsets is iterative and consists of repeating the following actions:  - Use (k-1) -Itemsets to generate the k-itemsets candidates;  - Scan transactions to calculate Supports candidates.

The generation phase of the candidates in the algorithms of search frequent itemsets decomposes into two sub-phases: Construction the list of candidate itemsets and the pruning of the list of candidates? itemsets. Our approach consists modifying the phase of the generation-pruning itemsets of Apriori Algorithm and, it introduces a new method for generating candidate itemsets which is based on the concept of SupportMin.

We know that a k-Itemsets Y cannot be frequent if least one of its subsets is not frequent. Thus, we notice that for a K- Itemsets Y, it is enough to identify (k-1) - subset of Y which has the smallest value of support to deduce if Y is frequent or not. This value represents the minimal value of support which can take K-Itemsets Y; we have called it SupportMin of the k- Itemset Y. Therefore, we have introduced a new technique for mining frequent itemsets based on the SupportMin. On the one hand, we use this value  SupportMin in the pruning step of itemsets candidates to determine if a k-itemset cannot be frequent. On the other hand, we exploit the  SupportMin for mining frequent itemsets not based on the metric support that requires access to the database, but on  SupportMin which gives an approximate value of the real Support which is deduced from the properties of Itemsets without access to the database.

A. Techniques of SupportMin  Let D be a set of database of n transactions and hold as an example of D the 3- Itemset xyz. Consider that,  Txyz all the transactions of D which contain the triplet of items (x, z) and Card (Txyz ) the cardinality of the set Txyz .

Thus we have the Support of the 3-Itemset xyz: support (xyz) = Card (Txyz)/n. The 3-Itemset xyz has three subsets of size 2 (xy, xz, yz); either Txy ,Txz , Tyz the D transaction sets containing these subsets with the respective cardinalities Card (Txy ), Card (Txz), Card (Tyz) and the following Supports:  Support (xy) =Card (Txy)/n, Support (xz) =Card (Txz)/n, Support (yz) = Card (Tyz)/n.

The set  Txyz is a subset of Txy : all instances of items containing the triplet (x, y, and z) contain necessarily the couple of items (x, y).

On the other hand, all the transactions containing the couple of items (x, y) don?t necessarily contain the items Z and, thus don?t contain necessarily the triplet of items (x, y, and z). Therefore ,Card(Txyz) ?  Card(Txy), and by association ( n positive) : Card(Txyz )/n ?  Card(Txy )/n  , and thus Support (xyz) ?  Support(xy).

Consequently, we know that the support of the subset xy is superior or equal to the support of the superset xyz: support (xyz) ? Support (xy). Similarly, the subsets xz and yz, the supports of xz and yz are superior or equal to the Support of the superset xyz.

Both the 3-Itemset xyz and the 2-Itemsets xy, xz, yz are subset of xyz. They have the following relations:  Support (xyz) ? Support (xy)    (i)  Support (xyz) ? Support (xz)    (ii)  Support (xyz) ? Support (yz)    (iii)  By (i), (ii) and (iii) we deduce the following relationship:  Support (xyz) ? min {Support (xy), Support (xz), Support (yz)} (a.1).

This minimum represents the minimal value of support that the superset Y can get.

We suggest calling this value SupportMin.  Thus, the SupportMin  of 3-itemset xyz is defined as follows: SupportMin (xyz) = min {Supp (xy), Supp (XZ), Supp (yz)} (a.2).

The SupportMin  Of a k-Itemset Y gives us an estimation of the value of the Support Itemset Y. Its value is deduced from the minimum of its k-1 subsets found in the previous iteration and, thus, no access to the database is necessary for the calculation of this value.

This notion of  SupportMin  is used for mining frequent itemsets particularly for generation-pruning phase of itemsets candidates. To sum up from the already mentioned that (a.1) and (a.2) have got a relation (a.3):  Support (xyz) ?  SupportMin  (xyz)       (a.3).

By analyzing the relation (a.3) it is obvious that if SupportMin  of  the 3-Itemset xyz is inferior than the threshold  of Support (minsup )then we can deduce that the support of xyz is inferior to minsup and consequently, the 3-Itemset xyz is not frequent  : if  SupportMin   ( xyz) ? minsup then support(xyz) ? minsup.   (a.4).

According to the relation (a.4) an Itemset X is not frequent if its SupportMin is inferior to the threshold of support: SupportMin  (X) ? minsup => support(X) ? minsup.

The utility of  SupportMin   lies in the fact that the calculation of its value does not require access to data sources but can be simply deducted from the previous iteration and that  SupportMin   is used for pruning the candidates.

However, the calculation of  SupportMin   itemsets candidates is interesting only for Itemsets of size upper or equal to 3 (3-Itemsets or more). - 1-itemset has no subsets. - 2- Itemsets have exactly two subsets of size 1; each 2-itemset candidate is generated from two frequent 1-itemsets and there is no need to use  SupportMin   for the pruning because necessarily the minimum of the supports of its subsets is greater or equal to minsup.

B. Calculation of Frequent K-Itemsets(K=>3)  Following the iterative principle of mining frequent itemsets algorithm, we generate Itemsets candidate of a level k and we calculate their values by  SupportMin and we consider that a k-Itemsets is approximately frequent. If its  SupportMin is greater than the threshold of support exact. When the calculation of  SupportMin of Itemsets of the level k is completed, we eliminate Itemsets having  SupportMin lower than minsup (pruning by the SupportMin).The Itemsets which are not eliminated are used to generate Itemset of the level k+1.

However, the use of  SupportMin for the search the frequent Itemsets generates certain non-frequent Itemsets as being frequent Itemsets, because SupportMin gives an approximation of the real Support of Itemset. A validation phase is necessary to calculate the exact Supports of frequent itemsets and to eliminate the Itemsets which are not frequent that occur in the pruning phase. Two cases occur when the SupportMin is superior to minsup:  ? Case 1 : Minsup ? Support (Y) ?  SupportMin (Y)  The Y itemset is frequent; therefore it must calculate its exact support that will be used for the generation of association rules.

?   Case 2 : Support (Y) ? Minsup ?  SupportMin (Y)  The itemset Y is not frequent, but it is generated; and it must be eliminated from all frequent itemsets.

At the end of calculation of all the frequent k-Itemsets by using SupportMin , a validation phase is performed. Thus, one scan of the database is needed to refine all the set of frequent itemsets calculated. In this case, the number of accesses to data sources is reduced.



III. METHODOLOGY  Frequent Item sets are generated using the proposed Algorithm AprioriMin .The methodology of the proposed work is given in fig 1.

Fig. 1. Process of generation the frequent itemsets    Algorithm: AprioriMin algorithm  Input: Transactional Database, Minimum support value.

Output: Mining Frequent Itemsets.

Method:  1. Calculate the support of itemsets which its size superior or equal to 3 (3-Itemsets or more).

2. Verify that the SupportMin of itemsets is superior to minsup in this case; we have two possibly already mentioned in the section of 2.

3. Execute a validation by calculating the exact Supports of frequent itemsets and eliminating the Itemsets not frequent.

4. Mining Frequent Itemsets for the given datasets.



IV. EXPERIMENTAL EVALUATION  To evaluate the performance of AprioriMin, we have taken two sets of data (T10I4D100K and T25I10D10K).These data sets have downloaded from [11][12].The characteristics of selected data sets are shown in the table 1.

TABLE I.  DATA CHARACTERISTICS  Datasets Items Transaction  T10I4D100K 10 100,000  T25I10D10k 25 9,219   The result is performed using Intel? Core(TM) i7 CPU  with the speed of 2.80GHZ with 4G of RAM, running Windows 7. We have implemented the AprioriMin algorithm using java programming language, for comparison we have used Apriori, FP- growth, close algorithms implemented in java by the researchers in [12]. The performance analyses of algorithms are shown in the following figures.

